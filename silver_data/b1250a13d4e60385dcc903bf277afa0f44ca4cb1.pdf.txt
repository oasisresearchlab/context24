Selenite : Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models Michael Xieyang Liu ∗ Google Research lxieyang @ google . com Tongshuang Wu Carnegie Mellon University sherryw @ cs . cmu . edu Tianying Chen Carnegie Mellon University tianyinc @ andrew . cmu . edu Franklin Mingzhe Li Carnegie Mellon University mingzhe2 @ cs . cmu . edu Aniket Kittur Carnegie Mellon University nkittur @ cs . cmu . edu Brad A . Myers Carnegie Mellon University bam @ cs . cmu . edu ABSTRACT Sensemaking in unfamiliar domains can be challenging , demanding considerable user effort to compare different options with respect to various criteria . Prior research and our formative study found that people would benefit from reading an overview of an informa - tion space upfront , including the criteria others previously found useful . However , existing sensemaking tools struggle with the “cold - start” problem — not only requiring significant input from previous users to generate and share these overviews , but also that such overviews may turn out to be biased and incomplete . In this work , we introduce a novel system , Selenite , which leverages LLMs as reasoning machines and knowledge retrievers to automatically pro - duce a comprehensive overview of options and criteria to jumpstart users’ sensemaking processes . Subsequently , Selenite also adapts as people use it , helping users find , read , and navigate unfamiliar in - formation in a systematic yet personalized manner . Through three studies , we found that Selenite produced accurate and high - quality overviews reliably , significantly accelerated users’ information pro - cessing , and effectively improved their overall comprehension and sensemaking experience . KEYWORDS Sensemaking , Decision making , Large Language Models , Natural Language Processing , Human - AI Collaboration 1 INTRODUCTION Whether it is parents delving into the vast sea of baby stroller choicesordeveloperscomparingdifferentJavaScriptfrontendframe - works , people frequently find themselves having to make sense of unfamiliar domains and performing comparisons to make reasoned decisions . In these situations , individuals often have to iteratively find , read , collect , and organize large amounts of information about different options with respect to various criteria [ 15 , 54 , 55 , 66 ] , which can become a messy and overwhelming experience . One challenge lies within the initial reading process — due to unfa - miliarity with the topic , people may struggle to fully understand certain content as they read , or fail to recognize important aspects that should otherwise warrant their attention [ 58 , 85 ] , leading to a limited viewpoint or eventually misguided decisions [ 42 , 109 ] . For example , novice developers might not be aware of criteria crucial to the utility of a software library such as its stability , community ∗ This work was completed when the author was a Ph . D . candidate at Carnegie Mellon University . size and support , or ease of integration with existing codebases , resulting in making a sub - optimal choice . Another challenge arises when people have to sift through nu - merous online reviews and comparison articles but with limited time and cognitive bandwidth , making a complete understanding of the information space impractical or impossible . Instead , people often adopt a “selective” ( or non - linear ) reading strategy [ 104 ] , only reading the paragraphs that discuss information that they consider relevant or valuable and bypassing the rest [ 26 , 103 ] , e . g . , in a fo - cused session where they would like to compare different options with respect to the same criterion . However , it is challenging for people to gauge the potential value of articles or paragraphs , espe - cially long - winded ones , just by skimming and without performing a more thorough read [ 12 , 35 , 38 ] . For instance , information snip - pets about the maneuverability of different baby strollers may be dispersed throughout a review and appear in diverse variations ( e . g . , “agile enough to go through tight spots” and “easy to steer and navigate small corners” both discuss “maneuverability” ) , making it difficult for people to spot these variations effectively and navigate efficiently among such scattered details . In response to these difficulties , prior work on sensemaking and knowledge reuse has shown promising evidence that people would benefit from seeing an overview of the information space before they dive into the sensemaking process [ 16 , 36 , 76 , 81 , 95 ] — for example , Kittur et al . reported that having read an overview of the criteria that earlier users found useful can help people build intuition and understanding of the decision space upfront , leading to significantly improved digestion of the source material , better - structured mental models , and ultimately more well - informed decisions [ 54 , 55 ] . Sim - ilarly , our formative study found that people expressed a desire for such comprehensive overviews to help them more systematically read , understand , and strategically navigate unfamiliar information . However , existing sensemaking systems have struggled with the “cold start” issue — they often require substantial effort from the current user to personally go through the unfamiliar content and gather and structure information to obtain such an overview [ 6 , 53 , 55 , 67 , 92 ] , which defeats the premise of receiving one upfront . And even if a previous user has generated such an overview , it can often be incomplete [ 68 , 81 ] or in idiosyncratic formats that make it hard for the current user to readily understand and leverage [ 50 , 51 , 67 ] . To overcome these challenges and go beyond prior systems , we explore the idea of providing users with a comprehensive overview of the information space upfront to jumpstart as well as guiding their subsequent sensemaking processes in a novel system named 1 a r X i v : 2310 . 02161v2 [ c s . H C ] 13 D ec 2023 Liu and Wu et al . https : / / www . babylist . com / hello - baby / best - strollers Best Strollers of 2023 e Criteria that you looked at : Safety Comfort Price Weight and size Foldability Ease of cleaning Durability Criteria that you ignored : Maneuverability Storage space Design You might be interested in further searching for “Best strollers of 2023” based on : Ease of assembly Brake & locking system f a b c c 1 c 2 c 3 d c 4 Best baby strollers Figure 1 : The main user interface of Selenite , which provides users with a comprehensive overview of the information space in the sidebar ( a ) . When users encounter an unfamiliar topic ( b ) , Selenite offers them a global grounding based on commonly considered criteria ( c ) as well as the options encountered so far ( d ) , helping them develop quick intuitions of the topic . As users read new articles , Selenite provides local grounding through page - level and paragraph - level summaries and annotations ( e ) , enabling effective comprehension and efficient navigation between the content of their interests . Upon leaving a page , Selenite dynamically summarizes users’ progress and suggests avenues for finding additional new information ( f ) in subsequent searches . Selenite . 1 At a high - level , when users encounter an unfamiliar topic , Selenite leverages GPT - 4 , a large language model ( LLM ) developed by OpenAI , as a knowledge retriever to offer them a global ground - ing based on commonly considered criteria , helping users develop quick intuitions of the topic ( Figure 1c ) . As users read new articles , Selenite contextualizes that overview and uses it as an index to help users effectively comprehend and efficiently navigate among the content of their interest ( Figure 1e ) . Upon leaving a page , Selen - ite dynamically summarizes users’ progress and suggests unique search queries that would help users find additional information and expand their perspectives rather than duplicating existing knowl - edge ( Figure 1f ) . Through an evaluation of Selenite , we verified its feasibility to provide a sufficiently accurate and high - quality global overview to users . Furthermore , additional usability and case stud - ies revealed that Selenite accelerated users’ information processing , facilitated their comprehension , and improved their overall reading 1 Selenite is named after a soft and transparent gemstone , and stands for “ S mart E nvironment for L ogical E xtraction and N avigation of I nformation using T echnological E xpertise . ” and sensemaking experience . The contributions described in this work include : • a formative study showcasing people’s needs when reading and understanding information during online sensemaking and bar - riers to them , • Selenite , a novel system providing users with a upfront com - prehensive overview of the information space as well as in - teractively guiding their subsequent reading and sensemaking processes , • as part of Selenite , a novel user interface to interact with LLM - generated content and a novel user experience to contextualize that content into people’s existing sensemaking workflows , go - ing beyond the widely - used conversational interfaces in gener - ative AI research and applications [ 3 , 9 , 13 , 79 , 105 ] . • evaluation of the accuracy and coverage of Selenite that demon - strates the feasibility of our approach , as well as usability and case studies that offer preliminary insights into its usability , usefulness , and effectiveness , • adiscussionofdesignimplicationsfor futureLLMand AI - powered sensemaking tools . 2 Selenite : Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models 2 RELATED WORK 2 . 1 Making Sense of Online Information Building on theories of sensemaking as defined as developing a mental model of an information space in service of a user’s goals [ 28 , 57 , 91 ] , at a high level , a typical online sensemaking process involves first reading and understanding information from various data sources ( e . g . , online comparison articles , blog posts , video re - views , etc . ) , and then collecting and organizing such information to form a schema or representation ( e . g . , a comparison table , a decision tree , etc . ) to interpret the space [ 15 , 83 ] . Notably , there have been many research as well as commercial tools and systems developed to support this latter stage of sensemaking , i . e . , collecting and orga - nizing information . For example , tools like SearchPad [ 8 ] , Hunter Gather [ 92 ] , as well as commercial systems like the Evernote clipper [ 33 ] , allow a user to capture entire pages or portions of web content , categorize them , and later assemble them into a coherent document or structure for their own sensemaking , decision making , or sharing and collaboration . Furthermore , to reduce the disruption to users’ overall workflow , prior work has also explored using lightweight interactions [ 14 , 20 , 50 , 70 ] to streamline the process of collection and triaging information , or even automatically keeping track of content of interest on behalf of the user [ 17 , 31 , 48 , 69 ] . However , an inherent assumption that these prior systems make is that the user has a well - developed mental model that will allow them to effectively grasp the content that they are about to collect using the system . But , in many circumstances , building sufficient context about the information space can be both time and effort - intensive , requiring individuals to iteratively read through and understand unfamiliar content by themselves without expert guid - ance or systematic strategy , as has been discussed in the literature [ 78 , 94 ] and confirmed in our formative study . Despite being feature - rich , aforementioned sensemaking tools generally fail to address the “cold start” issue during the initial reading and comprehension stage where users need help deciding “what is important to read” [ 38 ] and “how to most effectively read it” [ 5 , 44 , 88 ] . Discovering new and unseen content after reading can also be surprisingly difficult , as prior work revealed that a generic search query could return many redundant and near - duplicate documents [ 30 , 82 , 86 ] despite the extensive use of duplicate detection algorithms in modern search engines [ 84 ] . Therefore , in this work , we focus on supporting users’ reading and understanding of unfamiliar content in the first place . 2 . 2 Tool Support for Reading and Comprehension One crucial way to address the “cold start” issue is to provide users with a summary of the information landscape . Previous research has investigated mechanisms that prompt domain experts to iteratively and collectively summarize a single document or discussion thread [ 39 , 111 , 112 ] . In addition , prior work has introduced tools and systems that facilitate finding , extracting , and summarizing relevant information from multiple documents and sources into cohesive themes and knowledge [ 15 , 16 , 67 ] . However , these methods are dependent on the significant investment of time and effort from previous users and are contingent upon their availability , and may still result in summaries that are limited in scope [ 32 , 68 , 81 , 111 ] or with a biased perspective [ 37 , 49 ] . More recently , researchers have been experimenting with ap - proaches to extract key information and build underlying knowl - edge structures automatically with machine learning [ 2 , 10 , 27 , 38 , 52 ] , lexical and HTML patterns [ 31 , 59 , 69 ] , or crowdsourcing [ 18 , 22 , 40 ] . However , many studies have demonstrated that these techniques can often produce structures that are incoherent and difficult for users to comprehend and contextualize [ 18 , 24 , 45 ] . To address these limitations , Selenite leverages the wealth of world knowledge embedded within large language models to gen - erate overviews of information spaces that aimed at being compre - hensive and high - quality , eliminating the need for manual effort . In addition , Selenite provides detailed explanations to the generated overview as well as contextualizes it within the original content of the source documents to further enhance user understanding and facilitate navigation . 2 . 3 Eliciting Knowledge from Large Language Models Recent advances in large language models ( LLMs ) like GPT - 4 [ 80 ] , PaLM [ 23 ] , and LLaMa [ 98 ] showcase impressive capabilities in answering user questions . These models are trained on large vol - umes of data , and as a result , their parameters might contain a significant body of factual as well as synthesized knowledge across a wide range of domains [ 25 , 101 ] , In this work , we leverage GPT - 4 as a knowledge retriever to retrieve a list of commonly considered aspects given an arbitrary topic , which is used to directly help users understand that topic at the beginning and systematically read about and explore that topic afterwards . However , LLMs face well - known challenges like hallucination and falsehood [ 7 , 97 ] , which could make their outputs uncertain and less trustworthy , often requiring manual inspection and verification before use [ 71 ] . In this work , we address these issues with a two - prong approach : 1 ) reducing hallucination through techniques such as Self - Refine [ 73 ] ; 2 ) grounding LLM generations with the content that users would actually read , enabling natural verification . 3 FORMATIVE STUDY & DESIGN GOALS To better understand the obstacles people encounter in their read - ing and sensemaking in unfamiliar domains , we first conducted a formative study . 3 . 1 Formative Study 3 . 1 . 1 Methodology . Participants were a convenience sample of 8 information workers ( 5 male , 3 female ) recruited through social media listings and mailing lists . To capture a variety of processes , we recruited 3 doctoral students , 2 professional software develop - ers , 2 researchers , and 1 administrative staff member . While we do not claim that this sample is representative of all information workers , the interviews were very informative and helped motivate the design of Selenite . We began by asking participants to recall experiences of con - ducting sensemaking tasks on topics that they were not familiar 3 Liu and Wu et al . with . 2 We then explored how they manage those situations . We asked participants to provide context by reviewing their browser histories to cue their recollections while retrospectively describing those tasks . We solicited their workflows , strategies , frustrations , and needs . Finally , we had participants use the open - source Unakite system [ 67 ] 3 to make sense of a topic that they were not familiar with ( e . g . , for people who have not yet had children to figure out the best baby strollers to purchase for their future child ) and externalize their workflows , strategies , and mental models . 3 . 1 . 2 Findings . Participants reported a total of 28 distinct topics that they encountered and explored , such as “ choosing a hybrid app framework , ” “ selecting the best time tracking tool , ” and “ picking an engagement ring . ” 4 Below , we report major findings from the study : People often find themselves feeling lost or unsure of where to begin and desire a big - picture understanding of important criteria ( or aspects ) of an information space be - fore diving deeper . When approaching an unfamiliar topic , one common strategy that participants reported employing is to find some sort of “ overview of different aspects ” ( P3 ) that would give them “ an intuition of what to care about and some guidance on what to look out for regarding each option ” ( P5 ) in their subsequent ex - ploration . For example , when investigating which time - tracking app to use , P5 was able to find a few articles that provided such overviews at the beginning , e . g . , under the section “ What makes the best time tracking software ? ” 5 , However , participants complained that such lists of criteria are often “ subjective , incomplete ” ( P1 ) , can contain aspects that they “ most likely don’t care about ” ( P2 ) , and worse yet , “ do not represent how the rest of an article would be struc - tured ” ( P6 ) . In addition , for certain topics such as “ best birthday gift ideas ” , such overviews of criteria are hard to find upfront , in which case participants would have to employ a bottom - up approach by reading through a series of articles back - and - forth , which is of - ten considered “ time - consuming ” ( P1 ) and “ hard to actually follow through ” ( P2 ) . Without these “ important criteria to keep in mind ” ( P4 ) upfront , participants reported feeling “ overwhelmed by large amounts of unfamiliar information ” ( P6 ) , lacking “ a sense of clarity and structure ” ( P7 ) , and can easily lose focus during sensemaking . These findings prompted us to generate an initial overview of the commonly considered criteria given an information space to pro - vide users with some global grounding and an anchor point for their subsequent reading and sensemaking . Identifying and consistently keeping track of criteria is challenging . Participants cited identifying and aggregating cri - teria while reading content as a “ significant cognitive load ” ( P4 ) . One challenge is that the same criterion can be discussed in various ways across different articles ( and even within the same article ) , making it hard for participants to recognize those variations and time - consuming to flip back and forth to make sure they are even - tually aggregated and consistently represented in their Unakite 2 We subsequently kept track of these topics and used them in our system evaluations . 3 Unakite is a Chrome extension that helps users collect and organize information into comparison tables in a lightweight fashion while searching and browsing web articles . We used Unakite since it has been shown to be easy to learn and use , and can support a diversity of web page styles and structures [ 67 ] . 4 For a complete catalog of these topics , please refer to Table 5 in the supplemental materials . 5 https : / / zapier . com / blog / best - time - tracking - apps / tables . For example , when investigating the topic of “baby strollers , ” P6 first saw a stroller should be “agile and nimble to be able to go through tight spots and sidewalks , ” and recorded it as “nimbleness” in Unakite ; later when she saw another segment that stated that a particular stroller is “easy to steer and handle and can smoothly navigate tight corners , ” she created another criterion called “steer - ing . ” It wasn’t until when she saw a segment in a third article that described a stroller having great “maneuverability and control” did she realize that all of these were practically describing the same aspect , “maneuverability , ” and she had to go back and readjust and combine those criteria and their associated evidence in Unakite , a common refactoring challenge for users in unfamiliar domains [ 54 ] . Additionally , P7 recounted a similar experience when searching for washers and dryers for his first house and admitted that “ oftentimes , coming up with the right keyword or jargon to summarize what I saw can be surprisingly hard , and I really wish someone would just do that for me . ” Selenite tackles this issue by providing a comprehensive list of frequently considered criteria upfront , reducing the need for individuals to haphazardly find and aggregate criteria themselves . People need reading and navigation guidance both at para - graph level as well as article level . Participants reported often having “ limited attention span ” ( P8 ) when reading online articles and can only focus on a certain amount of information , usually the first few paragraphs or the first few sentences within a para - graph , before getting distracted or lost . For example , P5 in her quest to find a suitable time - tracking app pointed to a typical situation where “ sometimes a paragraph , even a short one , could be quite con - voluted and have a lot of intertwined information , for example , and at first I thought this paragraph was just about money , but the rest of the paragraph was actually about lots of other things like plat - form compatibility . ” But , since participants tend to skim through content quickly , it often leads to potential misunderstandings or missing important details . In such situations , participants desired “ some simple metadata of what’s covered in a paragraph ” ( P4 ) to give them an intuition of what the paragraph is about and whether it is worth reading . These findings prompted us to provide in - situ per - paragraph summaries and the option for users to clarify convoluted paragraphs by “zooming in” on them in Selenite . The same applies to the page level , where participants wanted to be able to “ preview a page before investing time reading it ” ( P3 ) to understand whether it discusses detailed aspects that they care about . Additionally , such preview can also help them “ maximize the information gained from each page ” ( P5 ) , i . e . , help them avoid reading duplicate information and aspects without learning any - thing new . As P4 put it , “ if I’ve already learned about all the aspects from the other pages , I don’t have to read this one . ” However , as discussed previously , such previews ( even if they are in the form of an abstract or table of contents ) are not always available for each article , nor grounded in a person’s past reading and information collection activity . Selenite addresses the page - preview need by offering users a concise overview of what’s covered ( and not ) in a page to help users gauge its value . Additionally , Selenite tackles the issue of personalization by presenting users with a progress summary based on their previous sensemaking activities at the end of a page . As participants became more familiar with a topic , their reading patterns started to get increasingly selective and non - linear . For 4 Selenite : Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models “Best baby stroller” Encountered options Common criteria B Looked at criteria Unseen criteria Ignored criteria New search : best baby stroller on ease of assembly Find webpage - of - interest With keywords search Global grounding With criteria and options Local grounding With in - situ annotations Suggesting new searches Through behavior tracking best baby stroller 1 2 3 Figure 2 : Main stages and features of Selenite : After the user searches and finds an initial webpage - of - interest to read , Selenite provides : 1 ) global grounding with a set of common criteria as well as options encountered so far , 2 ) local grounding with in - situ annotations of criteria per paragraph , and 3 ) suggestions on what to search for next to gain new information . example , we have observed that participants use a combination of keyword searches and flipping back and forth in an effort to find relevant information about a particular criterion that they cared about ( with respect to different options ) , which they thought was “ haphazard ” ( P1 ) and “ inefficient ” ( P7 ) . This led us to suggest po - tentially fruitful search keywords to users for discovering more unseen information in the end - of - page progress summary . 3 . 2 Summary of Design Goals We postulate that an effective user interface / interaction paradigm for helping users find and read about key information during sense - making should support : • [ D1 ] As the user starts investigating a topic , provide a global grounding using common criteria as well as the options encountered to help users build intuitions of the information space and promote structured thinking ; • [ D2 ] During their reading , provide a local grounding using page - level as well as paragraph - level summary and an - notation to enable an accurate understanding of and effective navigation within and across articles ; • [ D3 ] Upon finishing , dynamically suggest next steps in sensemaking based on users’ existing reading and informa - tion collection activities to avoid missing important aspects after reading as well as maximize the information gain in future readings . 4 THE SELENITE SYSTEM Based on the design goals , we designed and implemented the Se - lenite Chrome extension prototype to help people read about and make sense of unfamiliar topics with the help of global grounding ( summarized in Figure 2 ) . We will first illustrate how an end - user , Adam , would interact with Selenite . 4 . 1 Example Usage Scenario Adam , an expectant mother , is seeking guidance in selecting a baby stroller for her upcoming child . As someone without prior experience in child - rearing , she decided to rely on Selenite to help her while going through review articles and product pages of baby strollers . Adam did a quick Google search and clicked on the first result page , which appeared to be a review article titled “The 10 Best Baby Strollers Put To The Test” . Upon opening the page , Selenite auto - matically recognized the topic of the page as “best baby strollers” ( Figure 1b ) , and then automatically presented an overview in a global sidebar available on every page ( Figure 1a ) . The overview contained a list of criteria ( Figure 1c ) that are commonly consid - ered by people when investigating the topic , such as maneuverabil - ity and durability . In addition , Selenite also automatically parsed the web page content and extracted the different baby stroller options and presented them under the “Options encountered so far” section ( Figure 1d ) in the sidebar . After quickly skimming the overview , Adam now felt that she has already built an intuition about what criteria she should care about when picking baby strollers before even delving into the article itself . Based on the global options and criteria , Selenite contextualizes the ones that are covered on the current page by highlighting them in the sidebar ( and conversely low - lighting the ones that are not present , for example , see Figure 1c & d ) , helping users better un - derstand and find specific information of interest while browsing . In addition , as Adam read the article , she noticed that Selenite provided in - situ annotations of mentioned criteria at the be - ginning of each paragraph ( Figure 1e ) . She quickly learned that she could just skim those mentioned criteria to get a rough idea of what a particular paragraph is about and decide if that paragraph is worth reading . When she came across information about the maneuverability of a specific stroller , Adam became interested in finding out if there were any details about the maneuverability of other stroller options as well . To facilitate this , she used the “previous / next” buttons ( Fig - ure 3a ) to quickly navigate among the paragraphs that discussed maneuverability . Here , the aforementioned annotations not only offer paragraph overviews during linear skimming but also act as bookmarks for non - linear navigation between distinct parts of the page that pertain to similar criteria . Later , when Adam encountered a particularly convoluted para - graph with multiple criteria and options that she couldn’t quite absorb after a first pass , she decided to leverage the “zoom in” feature that Selenite offers — querying for more comprehensive descriptions that clarify which sentences or phrases within the paragraph pertain to specific criteria and sentiments ( positive , neu - tral , or negative ) ( Figure 4 ) by clicking the “Analyze” button ( Figure 4a ) that appears when hovering the cursor over a paragraph . 5 Liu and Wu et al . While it is harder to push and a wee bit heavier than the smallest product , it is easier to use and earned a higher score for quality than most of the best umbrella strollers we tested… The wheels on the Cruz are relatively small , and their disappointing size makes it more challenging to traverse uneven surfaces than the larger tires on joggers . However , the Cruz offers excellent maneuverability on smooth surfaces… … a Maneuverability Wheel type Price Maneuverability This full - size stroller is relatively easy to push , but the plastic wheels make uneven terrain , gravel , and grass more challenging . While not the best in a group that includes rubber tires and joggers , the plastic wheels … … Wheel type Durability Price Customer review Maneuverability Weight and size Figure 3 : Selenite enables structured and efficient navigation by criterion through clicking the “previous / next” ( shown as “ < ” and “ > ” ) buttons ( a ) , after which Selenite will automatically scroll the page to reveal the previous / next mentioning of the target criterion . After Adam reached the end of the current article , Selenite pre - sented a summary block ( Figure 1f ) , automatically summarizing her research progress ( e . g . , criteria and aspects from the overview that she has actually read about ) . With the help of this summary , Adam realized that she hasn’t seen evidence related to “ease of assembly” or “brake & locking system” before . She then specifi - cally searches for them on Google , finding new articles that contain information about these previously unencountered criteria . 4 . 2 Detailed Designs We now discuss how the various Selenite features are designed and implemented to support the design goals . 4 . 2 . 1 [ D1 ] Providing Global Grounding using Common Cri - teria and Options Encountered . In Selenite , we explore the idea of having the system provide users with an initial overview of cri - teria that are typically significant and frequently considered by people when exploring a particular topic . Selenite also performs information extraction on each page to identify the options that a user has encountered during their sensemaking process . Naturally , users have the flexibility to reorder , pin , edit , add , or delete any options and criteria to tailor them precisely to their specific pref - erences . We discuss the relevant designs and the rationale behind those designs below : Automatically recognizing topics . Selenite goes beyond previous sensemaking systems [ 17 , 54 , 56 , 66 , 68 – 70 ] by autonomously iden - tifying and classifying web pages into broad topics based on their titles and content . Unlike previous systems that require users to manually determine the topic [ 15 , 41 , 67 ] , automatic topic recogni - tion further lowers the barrier for entry , enabling users to quickly begin reaping benefits from the list of commonly considered criteria based on that topic . To achieve this , we frame the topic recognition as a summarization task for a large language model — specifically , we asked GPT - 4 ( taking advantage of its generalizability to various domains [ 80 ] ) 6 to first summarize an arbitrary web page given its title and initial five paragraphs ( with the temperature set to 0 to 6 Please refer to section D . 1 in the supplemental materials for the detailed prompt design . The wheels on the Cruz are relatively small , and their disappointing size makes it more challenging to traverse uneven surfaces than the larger tires on joggers . However , the Cruz offers excellent maneuverability on smooth surfaces making it highly suitable for running errands and public venues . While the Cruz v2 is more expensive than most of the best full - size strollers and jogging competitors , many users feel its higher quality features more than justify the higher cost . The wheels on the Cruz are relatively small , and their disappointing size makes it more challenging to traverse uneven surfaces than the larger tires on joggers . However , the Cruz offers excellent maneuverability on smooth surfaces making it highly suitable for running errands and public venues . While the Cruz v2 is more expensive than most of the best full - size strollers and jogging competitors , many users feel its higher quality and easy - to - use features more than justify the higher cost . Durability Maneuverability Wheel type Maneuverability Price Zoom in 💡 Analyze a b c Price Maneuverability Wheel type Durability Figure 4 : When encountering a particularly convoluted paragraph ( e . g . , the paragraph on the left ) with multiple criteria and options that users can’t quite absorb in the first pass , they can click the “Analyze” button ( a ) and leverage the “zoom in” feature that Selenite offers to query for more comprehensive descriptions that clarify which sentences or phrases pertain to which specific criteria and sentiments . Selenite wraps phrases and sentences in colored boxes , with green denoting “positive” ( b ) , red denoting “negative” , and grey denoting “neutral” ( not shown ) . minimize LLM hallucination ) . 7 Selenite then clusters the semanti - cally similar topics ( note that each web page has an associated topic generated by GPT - 4 ) based on the cosine distances on topic seman - tic embeddings computed using SentenceBERT [ 90 ] . Therefore , for example , websites titled “React vs . Svelte : Performance , DX , and more , ” “Angular vs React vs Vue : Which Framework to Choose , ” and “What are the key differences between Meteor , Ember . js and Backbone . js ? ” would all be regarded as “Comparison of JavaScript frameworks . ” Naturally , users have the flexibility to manually cre - ate , edit , and remove topics , as well as reassign pages to different topics based on their personal opinions . Automaticallyretrievingcommonlyconsideredcriteria . Ifweadopt the “bottom - up” approach discussed in prior work [ 41 , 62 , 67 ] , an intuitive method for obtaining criteria would involve extracting them from individual paragraphs on a page . However , in our initial attempts , we found that this method faced significant challenges that limited its effectiveness . One of the main issues was the lack of uniformity among the criteria extracted from different paragraphs — each paragraph presented its own variations and nuances , making it difficult to establish a cohesive and standardized set of criteria ( similar to what was reported by our formative study participants ) . Additionally , the approach lacked a comprehensive global perspec - tive , failing to consider the broader context and overarching themes of the topic . As a result , manual review , correction , and unification of the extraction results were frequently necessary , making the process impractical and inefficient . To address these challenges , we instead explored an alternative “top - down” approach , where we directly query an “oracle” for a glob - ally applicable and comprehensive set of criteria . We were particu - larly inspired by recent research suggesting that the majority of peo - ple’s information - seeking needs are not novel [ 72 ] — “previous people” have experimented with most search needs and synthesized infor - mation into summarized knowledge such as review articles . While it is impractical for individuals to process and synthesize vast amounts of information online , LLMs excel at this . Recent studies suggest that LLMs can be highly effective in processing and integrating information , making them potentially valuable for tasks like knowl - edge graph querying and retrieving common sense information 7 Empirically , we found this step helped GPT - 4 to better engage with the context provided . It also aligns with the idea of Chain - of - thought prompting proposed by [ 102 ] , and then offer a search phrase that would help one to find similar web pages using a modern search engine , which we use as the topic . 6 Selenite : Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models Criteria Name Criteria Description Safety Ensuring the stroller has proper safety features such as a secure harness , sturdy construction , and reliable brakes . Comfort Providing a comfortable seat with adequate padding and support for the baby , as well as adjustable recline positions . Maneuverability Having easy & smooth maneuverability , with features like swivel wheels , suspension systems , and the ability to navigate tight spaces . Durability Ensuring the stroller is built to last , with high - quality materials and strong construction . Storage Offering ample storage space for carrying essentials such as diaper bags , snacks , and personal items . Folding and Portability Allowing for easy folding and compact storage , as well as being lightweight for convenient transportation . Versatility Providing features that allow the stroller to adapt to different terrains , weather conditions , and age ranges . Ease of Use Having user - friendly features like adjustable handles , intuitive controls , and easy - to - clean fabrics . Price Considering the affordability and value for money in relation to the features and quality of the stroller . Customer Reviews Taking into account feedback and recommendations from other parents who have used the stroller . Weight and size Considering the weight and size of the stroller to ensure it is manageable and fits well in different environments . Ease of cleaning Ensuring the stroller is easy to clean and maintain , with removable and washable fabric components . Adjustability The stroller should have adjustable handlebars and footrests to accommodate different caregivers and growing babies . Canopy A large and adjustable canopy to protect the baby from the sun and other elements . Reversible seat Having the option to face the baby towards the parent or away from the parent . Brake system Having a reliable brake system that is easy to engage and disengage . Car seats compatibility Offering the ability to attach a car seat to the stroller for convenient travel . Adjustable height Allowing for adjustable handlebars to accommodate different heights of caregivers . Easy assembly Providing clear instructions and easy assembly process for the stroller . Design and aesthetics Considering the overall design and aesthetics of the stroller to match personal preferences . Weight capacity Specifying the maximum weight limit the stroller can safely carry . Warranty Checking for a warranty or guarantee that covers any potential defects or issues with the stroller . Brand reputation Considering the reputation and reliability of the brand manufacturing the stroller . Accessories Offering additional accessories such as rain covers , mosquito nets , or parent organizers for added convenience . Table 1 : Commonly considered criteria that Selenite retrieves for the topic of “best baby strollers . ” [ 1 , 101 , 108 ] , and , in our particular case , a suitable “oracle” for pro - viding a set of commonly considered criteria given a particular topic . In Selenite , we use GPT - 4 as a knowledge retriever — for any given topic , we prompt it to produce a list of around 20 commonly con - sidered criteria ( Figure 1c ) , complete with their respective names ( Figure 1c1 ) and descriptions ( Figure 1c2 ) for each topic . To min - imize potential anchoring biases , we strive to achieve a balance between relevance and diversity in our prompting strategy . First , we specifically requested an initial set of criteria that are deemed as “most relevant to the topic , ” “frequently considered , ” and can “cover a broad range of perspectives . ” Then , we iteratively prompted GPT - 4 while applying the Self - Refine technique [ 73 ] , where in each iteration , we requested the generation of five additional criteria that were “different , more diverse , and more important” than the previous ones . 8 We also relied on GPT - 4 for ranking the criteria based on their importance . Still , users have the freedom to request additional criteria ( without repetition ) if they believe the existing list is not comprehensive enough ( Figure 1c3 ) , or manually add criteria ( Figure 1c4 ) . We present in Table 1 the list of criteria that Selenite retrieves for the topic of “best baby strollers , ” offering an intuition of their quality and coverage . We further validate our ap - proach through a performance evaluation in section 5 that provides initial evidence that our approach is sufficient for our prototyping purposes . We leave for future work to experiment with advanced 8 Please refer to section D . 3 for the detailed prompt design . approaches , such as retrieval - augmented generation ( RAG ) [ 65 ] , that would potentially provide increased perceived external validity . Automatically recognizing encountered options . Instead of relying on GPT - 4 to access its internal knowledge and retrieve a set of commonly considered options , we instead leverage its zero - shot information extraction capability and expansive context window size [ 80 ] to directly extract options from the entire text content of a web page . This approach ensures that the options presented in the sidebar align with a user’s sensemaking process , i . e . , they are indeed what users have encountered as opposed to something that users would potentially never run into . It also circumvents the potential concern where the world knowledge of an LLM is out - of - date , for example , at the time of writing , GPT - 4 only “knows” information up to September 2021 [ 80 ] . 9 In addition , it surpasses the limited heuristics employed in previous approaches such as Crystalline [ 69 ] , which rely on page titles and HTML < h > - tags as sources for options . This is crucial , because studies have consis - tently demonstrated that web pages frequently disregard semantic web standards and best practices [ 47 , 74 ] . 10 9 While the direct retrieval of criteria from LLMs may also face this potential issue , in practice , we operate under the assumption that criteria are unlikely to suddenly emerge or become outdated . 10 For instance , it is common to find pages where every piece of content is enclosed in < div > tags regardless of their semantic roles . 7 Liu and Wu et al . 4 . 2 . 2 [ D2 ] ProvidingLocalGroundingusingPage & Paragraph - level Summary and Annotation . To address the challenges iden - tified in the formative study , where complex paragraph and page structures frequently led to overlooked information and hindered user comprehension , Selenite introduces the following features : In - situ summaries and annotations of paragraphs . With access to the initial set of common criteria as well as the options extracted from each page , Selenite performs content analysis on each para - graph within a given page to identify the specific criteria being discussed and presents them as in - situ annotations above the re - spective paragraph ( Figure 1e ) . This feature enables users to swiftly scan through a page , understand the key points of each paragraph , and selectively concentrate on the paragraphs that are valuable and engaging for gathering information . Such content analysis is enabled by recent advances in large pre - trained transformer models [ 29 , 64 , 100 ] fine - tuned to perform zero - shot text classification tasks following a natural language in - ference ( NLI ) paradigm [ 110 ] . Specifically , for example , to assess if a given text ( e . g . , “ Angular is very hard to pick up ” ) covers the criterion of “ learning curve , ” we can input the text as the premise and a hypothesis of “ This content discusses { learning curve } . ” into the NLI model . The entailment and contradiction prob - abilities are then converted into label probabilities , indicating the likelihood that the content pertains to the specified criterion . We used the bart - large - mnli model 11 for this purpose and consid - ered options and criteria with a score above 0 . 96 as true positives , displaying them in descending order of scores . We determined this threshold empirically , prioritizing recall over precision , as discussed further in section 5 . In scenarios where users still struggle to comprehend content despite the presence of in - situ annotations , Selenite can perform a deeper analysis on - demand by leveraging the advanced reasoning capabilities of GPT - 4 . Specifically , through parallel and carefully orchestrated prompts , Selenite produces a more comprehensive description that clarifies which sentences or phrases pertain to spe - cific criteria and sentiments ( positive , neutral , or negative ) ( Figure 4 ) . Although a formal evaluation of this method is beyond the scope of this work , recent research suggests that content analysis con - ducted by the latest generation of LLMs achieves state - of - the - art performance in terms of quality , accuracy , and granularity [ 11 , 80 ] , making it suitable for our purposes . Page - level overview of options and criteria . As evidenced by our formative study , providing a page - level overview of the information space to users can greatly assist them in reading and sensemaking tasks . To facilitate this , Selenite consolidates paragraph - level meta - data into the sidebar’s options and criteria entries , with the entries that are present on the page highlighted ( Figure 1c & d ) . This offers two key benefits . First , it provides a comprehensive summary of all the available options and criteria specific to the current page , which allows users to quickly understand the focus of the page as well as judge its value against their personal interests and needs . Second , it enables structured and efficient navigation . By utilizing the “previous / next” button for a given criterion ( Figure 3a ) , users 11 The model can be accessed on - demand through a remote API service that we implemented . can swiftly navigate between distinct parts of the page related to identified criteria ( Figure 3 ) . This feature saves users time and effort , as it eliminates the need for manual searching and filtering , which our formative study found to be the common practice . It is worth noting that the combination of page and paragraph - level annotations effectively addresses a significant prior limitation highlighted by Crystalline [ 69 ] and further revealed in our for - mative study : the inability to manually recognize “latent / implicit criteria , ” where the same criterion can be expressed in various forms without being explicitly mentioned ( for instance , it can identify the criterion of “price” from a statement like “I bought this mp3 player for almost nothing” [ 87 ] ) . 4 . 2 . 3 [ D3 ] Dynamically Suggesting Next Steps in Sensemak - ing . As users navigate through a page and consume the content , Selenite implicitly keeps track of the criteria and aspects that they paid attention to on the page based on dwell time , that is , the amount of time they roughly spent reading a block of content , as specified in [ 21 , 69 ] . Selenite then uses this information to summarize the user’s research progress in the aforementioned summary block ( Figure 1f ) , which consists of three sections : ( 1 ) criteria that users cared about and have seen evidence for based on implicit tracking ( emphasizing their focus and priorities ) , ( 2 ) the remaining ones that are discussed on the page but users ignored or skipped 12 ( reminding users of potentially overlooked criteria ) , and ( 3 ) a set of recommended criteria from the global overview that haven’t been discussed in any of the past articles but could still be worth exploring ( encouraging users to find additional information about these unseen criteria , for example , through conducting additional searches on them , thereby broadening their perspectives and maximizing their information gain ) . Specifically , to achieve this third objective , we leverage users’ the subset of criteria that users cared about and the subset they have intentionally skipped to recommend additional relevant and diverse criteria to search for and read about from the remaining global list . This requirement for the suggested criteria to be both relevant and diverse is similar to the exploration - exploitation trade - off in information retrieval and recommender system literature [ 4 ] , helping maintain user engagement and interest while avoiding over - fit or filter bubbles [ 61 , 96 ] . To operationalize this idea , we consider the problem a graph problem ( following the approach by [ 107 ] ) : by constructing a fully connected graph using the global list of criteria as vertices , we assign edge weights as distances between respective criteria in a semantic embedding space and vertex weights as the criterion’s relevance to the subset of criteria that the user cared about . Our objective then is to recommend a diverse subset of criteria ( vertices ) that have large distances between each other while still being relevant to what users cared about . That is , we need to find a sub - graph 𝐺 ′ of size 𝑛 , which maximizes a weighted ( 𝛽 > 0 ) sum of vertex weights 𝑤 𝑉 ( relevance ) and edge weights 𝑤 𝐸 ( diversity ) : arg max 𝐺 ′ ⊂ 𝐺 , | 𝐺 ′ | = 𝑛 𝛽 · 𝑤 𝑉 ( 𝐺 ′ ) + 𝑤 𝐸 ( 𝐺 ′ ) 12 The threshold of determining if the user indeed paid attention to a paragraph is set to2secondsbasedonourempiricaltesting . Futureworkcaninvestigatemoreadaptive methods , such as taking into account the length of a paragraph , the amount of new information contained in a paragraph compared to users’ existing knowledge , or if users appear to be idling and performing irrelevant activities . 8 Selenite : Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models To build the graph , we measure relevance with the perplex - ity score of the sentence “ { global _ criterion } tend to be considered together ( or is a trade - off ) with { cared _ abou t _ criterion } ” using GPT - 2 [ 89 ] and characterize diversity with the cosine distance between the SentenceBERT [ 90 ] embeddings of the two vertices ( criteria ) . Here , we follow the classic greedy peel - ing algorithm [ 106 ] by dropping vertices with the lowest weights ( the sum of vertex and every edge weight ) one at a time in a greedy fashion until the graph size reaches 𝑛 ( empirically determined as 2 ) . We then present these additional criteria as the ones that users “might be interested in further searching for” ( Figure 1f ) in the summary block . 4 . 3 Implementation Notes The Selenite browser extension is implemented in HTML , Type - Script , and CSS and uses the React JavaScript library [ 34 ] for build - ing UI components . It uses Google Firebase for backend functions , database , and user authentication . As explained previously , we implemented Selenite using state - of - the - art NLP models : off - the - shelf GPT - 4 [ 80 ] and NLI models finetuned on BART [ 64 ] . These models were chosen for their strong performance and efficiency that would satisfy our prototyping needs as well as their generalizability across different application domains ( c . f . Section 5 ) . However , it is important to note that our contributions lie more in the concept of grounded reading , in - terface design , and underlying NLP task abstractions , which are independent of specific model usage . We anticipate that these designs will remain valid as AI techniques continue to ad - vance [ 71 ] . 13 5 STUDY 1 : ACCURACY AND COVERAGE EVALUATION While Selenite can help ground users in what to read , its impact may backfire if the list of options and criteria is not accurate or compre - hensive — anchoring bias [ 99 ] 14 may cause readers to more easily miss information that is indeed included in the page but not reflected in the Selenite - generated options and criteria list . Here we evaluate on a diverse set of topics whether Selenite can : 1 ) accurately report options that are present on a web page ; 2 ) comprehensively report critical criteria people commonly consider . 5 . 1 Methodology 5 . 1 . 1 Topic Sampling . We collected ten topics that exhibit a mix - ture of practicality and diversity ( Table 2 ) : ( 1 ) we randomly sampled 5 topics ( out of the 28 topics reported ) reported by participants in the formative study , and ( 2 ) we collected 5 more from Wirecutter , a popular review site — the three most popular product guides listed in their 2021 year - in - review ( at the time of writing , the 2022 year - in - review has not been published ) as well as their two most recently updated guides for June 2023 . 13 Additional implementation details can be found in section E of the supplemental materials . 14 Anchoring bias refers to people’s inclination towards relying too excessively on the initial set of information they were exposed to on a topic . Regardless of the accuracy or quality of that information , people use it as a reference point , or an “anchor , ” to make subsequent judgments or decisions [ 113 ] . 5 . 1 . 2 Groundtruth Dataset Creation for Options and Criteria . To collect groundtruth criteria that the general audience would care about for each topic , we mimic a typical information collection workflow , where people rely on top sources from popular search engines for their authenticity and credibility . Specifically , we first gathered the top five Google search results using the query template “ best [ product or category ] ” ( excluding promotions or ads ) . We show a partial snapshot of a representative web page in Figure 5 in section B . 1 of the supplementary material . Then , for each web page , two authors independently first read through and annotated the options and criteria mentioned in every paragraph , and then merged all the annotations , excluding duplicate ones . Note that since many criteria are mentioned in a descriptive manner ( e . g . , the phrase “It is available in a black finish” implicitly refers to “aes - thetics” ) , the two authors had some variance in how they named essentially the same criteria . Therefore , the two authors iteratively discussed and resolved their conflicts , merging criteria that they believed were semantically equivalent , producing the groundtruth criteria list ( see Table 6 in section B . 2 of the supplymental mate - rial for the groudtruth criteria list ) . Finally , for the topics that we sampled from the formative studies where participants explicitly collected options and criteria using Unakite , we double - checked and were able to verify that all the criteria that they identified were indeed included in our groundtruth dataset , providing preliminary evidence to the soundness of our groundtruth dataset . 5 . 1 . 3 Evaluation Metrics . Option Extraction . Since Selenite directly extracts options from web pages , we evaluated this capability using the accuracy , that is , the percentage of options extracted by Selenite out of all the options available on a page . Criteria Retrieval . We also evaluated Selenite’s ability to retrieve the right set of criteria on two levels . First , to answer whether Se - lenite helps find useful criteria for each topic , we compute topic - level precision ( “the fraction of criteria retrieved by Selenite that coin - cided with the groundtruth” ) and recall ( “the fraction of groundtruth criteria that are were also retrieved by Selenite” ) . Second , tomeasurewhetherSelenite provideshigh - qualityground - ings per paragraph , we additionally randomly sampled 20 para - graphs per topic , and computed paragraph - level precision ( “the frac - tion of criteria recognized by Selenite that were indeed mentioned in the paragraph” ) and recall ( “the fraction of criteria mentioned in the paragraph that were indeed reported by Selenite” ) . 5 . 2 Results 5 . 2 . 1 Option Extraction . Selenite achieved 100 % accuracy on ex - tracting options from web pages , i . e . , as long as there was an option explicitly mentioned on a web page , Selenite was able to correctly extract it . This directly speaks to the strong reasoning and infor - mation extraction capabilities of GPT - 4 as described in OpenAI’s technical report [ 80 ] . 5 . 2 . 2 Criteria Retrieval . We present the result of criteria retrieval evaluation metrics in Table 2 , which provides initial evidence to Selenite’s strong capability in presenting to the user a comprehen - sive set of criteria that people commonly consider . Notice that for 9 Liu and Wu et al . Topic Topic - level Paragraph - level # GT ( total ) # Selenite ( total ) Precision Recall F1 # GT ( avg ) # Selenite ( avg ) Precision Recall F1 Best washing machines 19 24 0 . 88 1 . 0 0 . 93 3 . 05 3 . 30 0 . 91 1 . 0 0 . 95 Birthday gift ideas 11 21 0 . 57 0 . 91 0 . 70 1 . 40 3 . 45 0 . 57 0 . 96 0 . 72 Best hybrid app frameworks 15 21 0 . 86 0 . 93 0 . 89 2 . 55 3 . 00 0 . 83 1 . 0 0 . 91 Best time tracking tools 20 21 0 . 81 0 . 95 0 . 87 2 . 95 3 . 20 0 . 88 0 . 98 0 . 93 Deep learning frameworks 25 20 0 . 80 0 . 84 0 . 82 3 . 15 3 . 05 0 . 87 0 . 95 0 . 91 Best sleeping bags 19 21 0 . 81 0 . 89 0 . 85 2 . 85 3 . 15 0 . 95 1 . 0 0 . 97 Best air purifiers 20 24 0 . 83 1 . 0 0 . 91 3 . 05 3 . 75 0 . 83 0 . 98 0 . 90 Best robot vacuums 23 28 0 . 82 1 . 0 0 . 90 3 . 10 4 . 05 0 . 95 1 . 0 0 . 97 Best baby strollers 22 24 0 . 92 1 . 0 0 . 96 3 . 45 3 . 65 0 . 81 1 . 0 0 . 90 Best tropical vacation spots 15 19 0 . 74 0 . 93 0 . 82 2 . 55 3 . 10 0 . 92 1 . 0 0 . 96 Mean 19 . 0 22 . 3 0 . 80 0 . 95 0 . 87 2 . 81 3 . 37 0 . 85 0 . 98 0 . 91 Table 2 : Statistics of the accuracy and coverage evaluation on Selenite’s capability to retrieve a high - quality set of commonly considered criteria by topic . For topic - level , we report the number of groundtruth criteria , i . e . , “ # GT ( total ) ” , the number of criteria retrieved by Selenite , i . e . , “ # Selenite ( total ) ” , as well as the precision , recall and F1 - score . For paragraph - level ( recall that we randomly sampled 20 paragraphs per topic ) , we report the average number of groundtruth criteria mentioned per paragraph , i . e . , “ # GT ( avg ) ” , the average number of criteria reported by Selenite , i . e . , “ # Selenite ( avg ) ” , as well as the precision , recall and F1 - score . most of the topics , Selenite retrieved more criteria compared to the groundtruth set . This is not surprising , partly due to the fact that GPT - 4 has likely synthesized information from significantly more sources than what was considered during the construction of the groundtruth dataset ( five web pages for each topic ) . Theoretically , there is also a possibility that GPT - 4 hallucinated some criteria that are largely irrelevant to a given topic , however , upon further man - ual inspection , we did not see evidence of hallucination , at least for the 10 topics considered in this evaluation ( for example , Table 1 shows a list of commonly considered criteria that Selenite retrieves for the topic of “best baby strollers” ) . Topic - level recommendations . Selenite achieved both high recall and high precision on multiple topics ( e . g . , best washing machines , best baby strollers , and best robot vacuums ) , and usually achieves higher recall than precision , suggesting that Selenite has the ten - dency of finding supersets of what users would generally be able to identify from reading , i . e . , criteria in the groundtruth set . We qualitatively analyzed the topics with a lower - than - average topic - level criteria recall , and found two contributing reasons : ( 1 ) Some web pages cover factual information that is not necessarily relevant . Multiple pages describing Best Hybrid App Frameworks mentioned First Release Date , which arguably is not a criterion neces - sary for selection . ( 2 ) Some criteria are inter - correlated . For example , in the case of deep learning framework , whereas it did not explic - itly mention “ growth speed , ” Selenite did suggest “ innovation , ” whose description is “ the ability of the framework to stay up - to - date with the latest research and developments in deep learning , and to incorporate new techniques and architectures as they emerge . ” While we did not count these two as equivalent in the evaluation , in practice , these two have a high correlation , and we believe having one included might be sufficient . Still , this potential mismatch reflects the necessity of allowing users to edit the criteria and descriptions . Meanwhile , upon initial observation , Selenite’s lower precision on certain topics may suggest its inclination towards retrieving unnecessary criteria . However , a closer examination revealed an interesting insight : for instance , when it comes to topics like birthday gift ideas , popular web pages often present a list of 10 + diverse options that lack strict comparability and are all described using generic terms such as “fun” or “sweet . ” This lack of specificity makes it challenging to de - termine a comprehensive set of groundtruth criteria . In contrast , Se - lenite offers comprehensive overviews that encompass factors like personalization , uniqueness , practicality , sentimentality , and presentation ( wrapping ) , among others . Paragraph - levelGrounding . Selenitealsoachievedhighper - parag - raph performances , again with a bias towards higher recalls . This is intentional — we tuned the parameters of the NLI - based method such that it is more likely for Selenite to claim non - existing criteria than overlooking actual existing ones . This approach prioritizes avoiding information loss , which , suggested by prior work [ 71 ] , is a more expensive mistake compared to user verification . We order the criteria based on their probability score from the NLI model and will , in future iterations , fade the ones with a lower score . We did notice that in some rare circumstances , the NLI perfor - mance can be influenced by a criterion’s description , e . g . , changing “appropriate for age” to “appropriate for kids , adults , or elderly” can reduce Selenite’s error on recognizing arbitrary numbers as ages . Therefore , in future iterations of Selenite , we will provide a hint to users , prompting them to try tweaking the description when they attempt to delete a criterion due to its seemingly low grounding efficacy . 6 STUDY 2 : USABILITY EVALUATION We also conducted an initial usability study to evaluate if the fea - tures provided by Selenite are usable and if the approach of provid - ing global as well as contextual grounding can allow users to read , navigate , and comprehend information more efficiently . Specifically , we were interested in the following quantitative research questions : • [ RQ1 ] Does using Selenite speed up people’s process of reading and understanding information ? • [ RQ2 ] Does using Selenite help people achieve a more compre - hensive understanding of an information space ? • [ RQ3 ] Can Selenite help people obtain new information in ad - dition to their existing knowledge ? 10 Selenite : Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models Mental demand Physical demand Temporal demand Performance Effort Frustration Selenite 3 . 0 ( 3 . 03 ± 1 . 76 ) * 1 . 0 ( 0 . 51 ± 1 . 74 ) 2 . 5 ( 2 . 26 ± 1 . 68 ) * 8 . 5 ( 8 . 47 ± 1 . 32 ) * 3 . 5 ( 4 . 08 ± 1 . 88 ) * 0 . 5 ( 0 . 33 ± 1 . 51 ) Baseline 6 . 5 ( 6 . 43 ± 2 . 07 ) * 1 . 0 ( 0 . 79 ± 1 . 98 ) 4 . 0 ( 4 . 29 ± 2 . 08 ) * 6 . 5 ( 6 . 54 ± 1 . 73 ) * 6 . 0 ( 5 . 98 ± 2 . 23 ) * 1 . 0 ( 0 . 89 ± 1 . 91 ) Table 3 : Study 2 participants’ responses to NASA TLX questions ( on a scale from 0 to 10 ) in study 2 . Format : median ( mean ± standard deviation ) . Statistically significant differences ( p < 0 . 05 ) through t - tests are marked with an * . Question category Statement Response Comprehensibility I would consider my interactions with the tool to be understandable and clear . 6 ( 6 . 33 ± 1 . 10 ) Learnability I would consider it easy for me to learn how to use this tool . 7 ( 6 . 71 ± 1 . 04 ) Enjoyability I enjoyed the features provided by the tool . 6 ( 6 . 13 ± 1 . 72 ) Applicability Using this tool would make solving sensemaking problems more efficient and effective . 6 ( 6 . 28 ± 1 . 39 ) Recommendability If possible , I would recommend the tool to my friends and colleagues . 6 ( 6 . 23 ± 0 . 94 ) Table 4 : Study 2 participants’ responses to System Usability Scale questions ( on a scale of 1 to 7 , where 1 represents “strongly disagree” and 7 represents “strongly agree” ) in study 2 regarding their Selenite experience . Format : median ( mean ± standard deviation ) 6 . 1 Methodology We recruited 12 participants ( 5 female , 7 male ) aged 21 - 40 ( 𝜇 = 28 . 9 , 𝜎 = 5 . 2 ) through social media . Participants were required to be 18 or older and fluent in English . All participants reported that they regularly engage in the process of seeking and sifting through large volumes of online information , whether for professional or personal purposes , on a weekly basis . The study was a within - subjects design , where participants were presented with two tasks and were asked to complete each one under a different condition , counterbalanced for order . For each task , participants were given a topic that they needed to investigate and two web pages relevant to the topic that they were required to read and process . The two topics were “best baby strollers” and “best robot vacuums . ” 15 The provided two web pages for each topic were all product comparison pages used in the previous study ( see section 5 . 1 ) . For each task , participants were asked to read through the two required pages , either by themselves without any aid ( a control condition simulating how people normally read ) or with Se - lenite ( experimental condition ) . While reading , they were instructed to write down as many criteria as they learned and thought were important for the topic as well as the reason why they were impor - tant as if they needed to thoroughly explain the topic to a friend later . Then , participants were instructed to optionally search ( using Google ) and gather additional information that they still wanted to learn about but weren’t able to from reading the two required pages . We imposed a 25 - minute limit per task to keep participants from getting caught up in one of the tasks . However , they were instructed to inform the researcher that they felt like they could make no further progress , i . e . , having learned as much as they could about the given topic . Each study session started by obtaining consent and having par - ticipants fill out a demographic survey . Participants were then given 15 To ensure realism and participant engagement , the tasks were selected based on actual topics that the formative study participants reported investigating . Rather than letting participants search for their own pages to read from the get - go , we provided them with a predefined set of pages to enable a fair comparison of the results ( e . g . , speed , etc . ) . Requiring participants to use predefined pages ( each contains , on average , 15 screenfuls of content ) for the first portion of the study also helps ensure that the two tasks are of roughly equal difficulty in terms of reading and cognitive processing effort . As described in the results , there was no significant difference by task . a 5 - minute tutorial showcasing the various features of Selenite and a 5 - minute practice session before starting . At the end of the study , the researcher conducted a NASA TLX survey and a questionnaire , eliciting feedback on their experience in both conditions . Each study was conducted via Zoom for about 60 minutes . Each participant was compensated with $ 15 USD . The study was approved by our institution’s IRB . 6 . 2 Results All participants were able to complete all tasks in both condi - tions , and nobody went over the pre - imposed time limit . Below , we present primarily quantitative evidence to evaluate the usability of Selenite with respect to our research questions . First , we were interested in understanding if Selenite can help participants read and process information faster compared to the baseline condition ( RQ1 ) . To examine this , we measured the time it took for them to finish reading all the materials in each task . A two - way repeated measures ANOVA was conducted to examine the within - subject effects of the condition ( baseline vs . Selenite ) and task on completion time . There was a statistically significant effect of condition ( F ( 1 , 20 ) = 102 . 5 , p < 0 . 01 ) such that participants completed tasks significantly faster ( 36 . 3 % ) with Selenite ( Mean = 840 . 3 seconds , SD = 102 . 7 seconds ) than in the baseline condition ( Mean = 1319 . 3 seconds , SD = 120 . 0 seconds ) . There was no sig - nificant effect of task ( F ( 1 , 20 ) = 0 . 40 , p = 0 . 53 ) , indicating the two tasks were indeed of roughly equal difficulty . These results suggest that Selenite helped participants read and comprehend information more efficiently . We discuss additional qualitative insights into why Selenite was more efficient in the following open - ended case study ( section 7 . 2 ) . In addition , we were interested in understanding if Selenite can help participants achieve a more comprehensive understanding of a topic ( RQ2 ) . To measure this , we first compared the quantity of criteria that participants externalized under each condition . As a pre - filtering step , two researchers rated all the criteria that participants externalized as either valid or invalid blind to the conditions . Valid criteria are considered as ones that are relevant to the topic and backed by specific evidence that can be traced back to the content , 11 Liu and Wu et al . consistent with those standards used by prior work in judging the quality of subjective evidence [ 15 ] . After resolving conflicts ( which were minimal ) between the two researchers and filtering out the criteria that were invalid , we found that the average total number of valid criteria increased by 90 . 4 % when using Selenite ( Mean = 12 . 93 , SD = 3 . 90 ) compared to the baseline condition ( Mean = 6 . 79 , SD = 4 . 07 ) , which is statistically significant ( p < 0 . 01 ) under a t - test . Thus , using Selenite appeared to enable participants to identify and learn significantly more criteria about a topic compared to people’s current way of reading information . In addition to quantity , we also examined the quality of the cri - teria by comparing the ones that participants externalized with the groundtruth criteria curated in the previous accuracy and cov - erage evaluation — we can calculate the precision ( calculated as 𝑛 Hit / 𝑛 Total ) and recall ( calculated as 𝑛 Hit / 𝑛 Groundtruth ) of partic - ipants’ criteria that hit the groundtruth ( where 𝑛 Total is the total number of valid criteria participants externalized , and 𝑛 Groundtruth is the number of groundtruth criteria for each task ) . On average , participants in the Selenite condition achieved significantly higher precision ( 98 . 8 % vs . 78 . 4 % , p < 0 . 05 ) and recall ( 73 . 0 % vs . 30 . 4 % , p < 0 . 05 ) in both tasks . Thus , using Selenite appeared to have enabled participants to improve the quality of their understanding of an information space in terms of its criteria . Furthermore , to understand if Selenite can help participants ob - tain new information in addition to what they have already learned from reading the two required pages ( RQ3 ) , we examined : 1 ) the number of additional searches that they performed in the Selen - ite condition ( Mean = 2 . 01 , SD = 1 . 39 ) , which turned out to be significantly more ( p < 0 . 05 ) than the baseline condition ( Mean = 0 . 33 , SD = 0 . 62 ) ; 2 ) the number of additional pages visited in the Selenite condition ( Mean = 2 . 76 , SD = 2 . 32 ) , which turned out to be significantly more ( p < 0 . 05 ) than the baseline condition ( Mean = 0 . 42 , SD = 0 . 74 ) ; and 3 ) the number of additional criteria that participants externalized in the Selenite condition ( Mean = 1 . 58 , SD = 0 . 91 ) , which turned out to be significantly more ( p < 0 . 05 ) than the baseline condition ( Mean = 0 . 33 , SD = 0 . 22 ) . These results suggest that Selenite did encourage and help participants to seek additional information beyond their existing perspective . Last but not least , participants filled out a NASA TLX [ 43 ] cog - nitive load scale and a System Usability Scale ( SUS ) [ 63 ] question - naire for each condition . SUS Likert items were integer - coded on a scale from 1 ( strongly disagree ) to 7 ( strongly agree ) . The median response values are presented in Tables 3 and 4 . Notably , partic - ipants perceived Selenite to have significantly lowered workload across mental , temporal , and effort demands as well as significantly increased perceived performance based on paired t - tests ) . This suggests that using Selenite can reduce the cognitive load and in - teraction costs when reading and understanding information , even when users had to learn and get used to a new user interface . 7 STUDY 3 : OPEN - ENDED CASE STUDY Encouraged by the promising performance outcome of the previous two studies , we conducted a third open - ended case study to un - derstand the usefulness and effectiveness of the Selenite prototype from a qualitative perspective . 7 . 1 Methodology We recruited 8 participants ( 3 male , 5 female ; 3 students , 2 soft - ware engineers , 1 dermatologist , 1 accountant , and 1 researcher ) aged 24 - 55 years old ( Mean = 33 . 6 , SD = 8 . 1 ) through emails and social media . The same recruitment requirements were applied , but individuals who participated in the previous usability study were excluded from this study . Each participant first completed two pre - defined tasks , where they used Selenite to help them read information about an un - familiar topic . From the topics that participants reported having explored in the formative study , we randomly selected two that the participant was unfamiliar with ( indicated in their screening survey ) . For each task , participants were presented with a set of three web pages that covered the topic that the formative study par - ticipants had gone through . The provided web pages were primarily review articles comparing several options together or product detail pages . We imposed a 20 - minute limit per task to keep participants from getting caught up in one of the tasks . To further explore Se - lenite’s potential , all participants then used Selenite to make sense of a third topic that they intend to explore in real - life . Here , we purposefully did not limit the topic to be “unfamiliar , ” allowing participants to revisit previous topics of interest and potentially uncover fresh perspectives . Each study session began by obtain - ing consent and demographic information . Participants were then given a 5 - minute tutorial showcasing the various features of Se - lenite and a 5 - minute practice session before starting . At the end of the study , the researcher elicited feedback on using Selenite through a semi - structured interview , which was recorded and later transcribed for coding and thematic analysis [ 19 ] . Each study was conducted via Zoom for up to 1 hour . Each participant was re - warded with $ 15 USD . The study was approved by our institution’s IRB . 7 . 2 Results Below , we present the major qualitative findings from the obser - vation of participants’ behaviors using Selenite as well as their feedback from the post - study interviews . 16 Time and effort savings . All of the participants mentioned that using Selenite would save them a lot of time and effort compared to using their typical reading and information collection workflow , echoing the quantitative results reported in the usability evaluation ( see section 6 ) . First of all , having access to the global overview felt like “ a game - changer ” ( P8 ) that offers a “ bird’s - eye view ” ( P4 ) or access to “ on - demand expert opinion ” ( P1 ) that “ took away the anxiety and guesswork of wondering what other folks would actually care about ” ( P5 ) . P7 suggested that “ this is something that I always wished for when reading about stuff that I’m not an expert in . It se - riously saves me a ton of time that I’d otherwise spend trying to wrap my head around it little by little , ” while P6 , who couldn’t “ stand the huge deal of work of figuring out stuff that I’m not used to ” said “ now I really feel like I’m chilling in the passenger seat and not having to do all the heavy - lifting personally . ” 16 To see all the topics that participants explored in this study , please refer to Table 7 in the supplemental material . 12 Selenite : Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models Second , participants seemed to appreciate the in - context anno - tations and summaries of each paragraph provided by Selenite . They thought that this feature “ made things incredibly easy ” ( P3 ) by “ helping me grasp the key points without wasting time reading a paragraph through ” ( P1 ) , and “ felt like back in the day when my classmate would mark all the important stuff in the textbook after a class when I couldn’t make it . ” However , some did report that the in - context annotations can occasionally be “ a little bit distract - ing ” , especially for paragraphs that are “ apparently unrelated to the main content ” ( P2 ) , such as those that talk about related articles or terms of services , suggesting that future versions of Selenite should consider more robust content filtering techniques . Last but not least , participants also appreciated that Selenite can help them brainstorm search queries that would enable them to find new information more efficiently that was “ almost always one step ahead ” ( P4 ) , especially in the third task . For example , after reading two review articles about e - readers , Selenite suggested that P5 could do some additional investigations about “supported file formats” and “syncing across devices . ” P5 admitted that “ I’d totally miss those if I’m by myself , and even if I’m trying to be super careful , it would take me forever to figure out that I need to check out those as - pects . ” Additionally , we observed that when integrating the Selenite suggested criteria into subsequent search queries , the search engine did return result pages that turned out to be noticeably different yet sufficiently high - quality for users to explore . Impact on reading patterns and habits . Participants all mentioned that they immediately checked out the commonly considered crite - ria from the sidebar before diving into reading the first web page . They claimed that compared to what they normally do , which is “ just have to hunker down and read ” , reading the overview first helped them “ cut to the chase and get a feel of what’s out there ” ( P7 ) and remind them of criteria that would otherwise “ slip my [ their ] mind ” ( P3 ) . On a per - paragraph level , we noticed an initial hesitation among some participants ( 3 out of 8 ) towards relying solely on the pro - vided criteria labels . As a safety precaution , they personally read through a handful of paragraphs to confirm the labels’ accuracy and reliability . We further corroborated this observation with their reflections , such as “ I’ve never seen anything like this before , so hon - estly , I was a bit skeptical at first . But hey , everything looked legit ! ” ( P5 ) After this initial hurdle , participants tend to “ rely on the labels to tell me the gist of a paragraph ” ( P4 ) and only read paragraphs that discuss criteria that they truly cared about . For the content that participants did end up reading , they think the corresponding criteria “ definitely helped me [ them ] process and digest it better ” ( P6 ) , and even “ saved me [ them ] from otherwise misunderstanding things ” ( P7 ) . For example , while exploring healthy diet plans , P7 reflected that he would have initially thought a paragraph detailing the caloric allocation for each meal was seemingly discussing “calorie intake , ” however , Selenite preemptively clarified that the focus was on “portion control , ” i . e . , “providing guidelines on portion sizes . ” Participants also enjoyed the easy navigation feature that Selen - ite offers , and used it to frequently jump between different criteria mentionings for easy comparison and digestion ( 7 / 8 ) . They claimed that finding specific criteria about different options in a long article used to be “ link finding a needle in a haystack ” ( P8 ) that they were hesitant to do , but with Selenite , “ it’s more like following a well - lit path ” ( P5 ) . For example , P2 reflected on her experience exploring VPN solutions , and claimed that “ now I get it , McAfee Safe Connect seems to be keeping track of all sorts of my information while Surf - Shark doesn’t do any of that . If I can’t quickly switch between these two points on the page , by the time I reach SurfShark’s no - logging policy , I would have totally forgotten about what McAfee does , or that I should even be concerned about logging at all . ” In addition , participants liked the fact that they can more effectively break out from the original structure and narrative of an article ; for instance , P1 recounted that “ you don’t gotta stick to what the authors say anymore , ya know ? Because , let’s face it , their storylines can get all tangled and complicated sometimes . ” Last but not least , we did not observe much usage of the “zoom in” feature , where Selenite can leverage GPT - 4 to provide a thor - ough analysis of a piece of content — only 3 participants tried it for a total of 8 times . We hypothesize that 1 ) the web pages utilized in the study were all professionally crafted , resulting in content that was relatively easy to comprehend ; 2 ) the criteria labels gen - erated by our NLI pipeline proved to be adequate in addressing the participants’ information needs ; 3 ) the time required for the “zoom in” feature to provide a useful analysis , typically ranging from 5 to 10 seconds , still exceeded the participants’ patience and attention span . Future work could explore solutions to address this limited adoption from these perspectives , for example , with models that boast significantly increased inference speeds . Additional findings . One interesting theme that emerged was that some participants ( 4 / 8 ) opted to use Selenite in the third task to revisit topics that they had previously explored and wanted to be able to “ double - check ” ( P3 ) whether their prior understanding of the topic was truly comprehensive . Consistently , each participant uncovered something new that they hadn’t considered before . For example , P3 , who had recently been making plans to move in with his partner , revisited the topic of “choosing the right mattress , ” and realized that he had never taken into consideration criteria such as “motion transfer” ( i . e . , the extent to which movement on one side of the mattress affects the other side ) or “noise reduction” ( i . e . , the ability of the mattress to minimize noise from springs , coils , or other components ) , which prompted him to reassess his original mattress purchase . As another example , P4 , a professional software engineer , revisited the topic of “choosing a hybrid app framework” and discovered that he had neglected to consider the “licensing and legal considerations” ( i . e . , compliance with licensing requirements and legal considerations ) as suggested by Selenite . Consequently , P4 was able to find additional evidence to confirm the validity of their original framework choice made back in 2017 . In the post - study interview , many participants ( 6 / 8 ) felt that now they “ can’t imagine reading without a tool like this ( Selenite ) ” ( P3 ) . Half even inquired about the possibility of installing Selenite on their personal computers for post - study usage , and we gladly fulfilled their requests . Despite encountering a few bugs in our research prototype during the study and having no obligation or incentives for continued usage after the study , the fact that they were willing to do so suggests that our grounded reading approach indeed holds value for our participants . 13 Liu and Wu et al . 8 DISCUSSION Some of the participants ( 3 / 8 ) from the case study expressed concern about the coverage of Selenite’s overview criteria and the criteria labels for each paragraph at the beginning . They wondered if Selen - ite might overlook important criteria that they should also consider . This concern was valid , given that we presented the tool as an AI - powered oracle that could potentially be fallible or overlook certain factors and encouraged users to conduct their own explorations in addition to relying on Selenite’s insights . However , our accuracy and coverage evaluation described in section 5 provides an initial validation that the criteria and options provided by Selenite are indeed comprehensive , relevant , and accurate . In addition , after the study , participants also acknowledged that the current set of criteria offered by Selenite already “ far exceeds what I [ they ] could identify and keep track of on my [ their ] own ” ( P4 ) ; therefore , they “ wouldn’t mind at all if the algorithm misses any minor ones ” ( P1 ) . Indeed , despite the participants’ awareness of the opportunity to request additional criteria from Selenite in the case of insufficient coverage ( as confirmed in the post - study interviews ) , we did not observe any instances of such usage . Nevertheless , further research is necessary to investigate : 1 ) ways that would further improve the coverage and accuracy of Selenite , such as leveraging retrieval - augmented models [ 65 ] ; 2 ) mechanisms and interventions designed to reduce over - dependence on Selenite as well as encourage critical thinking and user - led explorations . Though primarily designed as a tool for grounded reading , Se - lenite might also have the potential to address some of the issues identified by prior work regarding structuring information during sensemaking — prior research suggested that asking users to struc - ture information too early might lead to a more poorly structured information space [ 55 ] . In addition , the knowledge structures that people created often become obsolete , and new structures often emerge as their mental representations evolve over the course of their investigation [ 36 , 46 , 55 ] , resulting in having to spend sig - nificant effort in refactoring the structures every once in a while . Here , Selenite provides users with a well - structured framework from the outset , including a set of commonly acknowledged crite - ria . This readily usable scaffold serves as a starting point , aiming to encompass the majority’s perspective and thereby minimizing the necessity of refactoring or restructuring . Hopefully , it simplifies the iterative and cognitive - demanding process of building a mental model , transforming it into a possibly more manageable task of refining and pruning [ 69 ] . There wasalso a concern that GPT - 4 could potentially hallucinate or generate irrelevant or even false criteria and thus mislead users in their subsequent exploration . However , it is important to note that in our case study , as well as in study 2 , we did not observe any such episodes or evidence of this occurring . This could be attributed to the fact that the topics explored in the study were all common subjects with abundant source materials available online , which were likely encountered by GPT - 4 during its training process . We would like to further conjecture that even if hallucination occurs , users can readily identify irrelevant or false criteria by carefully reading their descriptions and comparing them with common sense or their intuitive knowledge about the topic , mitigating the actual impact of hallucination . 9 LIMITATIONS & FUTURE WORK Connections among criteria . In Selenite , we made the implicit as - sumption that criteria are completely independent . However , in real - ity , there could be connections among criteria — for instance , when evaluating the “Best Baby Stroller , ” the specific criterion of “suspen - sion system” falls under the broader category of “safety” ( hierarchy ) , while on the other hand , aspects like “price” and “versatility” are typ - ically trade - offs that are impractical to optimize for simultaneously ( correlation ) . Currently , Selenite takes into account one form of cri - teria connections , i . e . , relevance between criteria , when suggesting the next steps . This proved promising in the study , which gave us reasons to believe that further exploiting these connections between criteria can better support users’ reading . For example , instead of presenting the criteria in a simple list , one can imagine creating a behind - the - scenes knowledge graph where criteria are connected using edges of relations ( TypeOf , CompetesWith , etc . ) . By initially displaying a portion of this graph and allowing users to “zoom in” on the specific criteria they are interested in ( e . g . , a subset of “safety” - related features ) , we can help users intuitively reason through an initially overwhelming list . In addition , one can again imagine “overview first , details later” - style UIs [ 93 ] that accommodate cri - teria hierarchies , e . g . , multi - level tables or lists , granting users the flexibility to combine or decompose criteria at decision time . Availability of domain knowledge in LLMs . LLMs , such as GPT - 4 , possess an extensive range of encoded knowledge , yet they might lack domain - specific information for specialized or emerging topics , as well as for topics involving confidential or sensitive information . Our technical implementation in Selenite is primarily based on extracting knowledge ( e . g . , commonly considered criteria ) from commercially available LLMs , and its effectiveness is highly depen - dent on the LLM’s capability to capture and synthesize relevant domain knowledge from its training data . Without such knowledge , the guidance provided may be subpar . In addition , LLMs themselves can sometimes be biased , and the response they generate might be incorrect or harmful [ 60 , 77 ] . However , our approach to ground the reading process with domain knowledge would also work with other sources of knowledge bases as well , for example , Unakite + Strata tables [ 67 , 68 ] , or crowdsourced [ 40 , 75 ] , or a combination of them . Furthermore , we should also urge users to thoroughly ex - amine the Selenite overview when dealing with critical situations . Generalizing beyond comparison tasks . In this work , we focus on helping people with sensemaking tasks that often need users to systematically compare different options with respect to various criteria . As evidenced by our formative as well as case studies , it is beneficial for people to be aware of the criteria that other people commonly consider upfront to help with their subsequent sense - making journey . However , there are other types of sensemaking tasks , such as those that are purely exploratory or investigatory ( e . g . , debugging , learning a new skill ) , that do not entail well - established options and criteria , and as such , they are not ideally compatible for Selenite to assist with . Nevertheless , we postulate that the no - tion of procuring comprehensive expert perspectives upfront may still apply in these non - comparison tasks — for example , one can imagine asking an LLM for advice on a range of typical strategies to try when debugging or a list of common steps to take to master 14 Selenite : Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models a new skill . Future research can work on enabling users to obtain these categories of overviews by adapting and customizing the LLM prompts used in Selenite ( that were originally used to obtain criteria ) and continue to receive similar in - context annotations and reading guidance grounded on those overviews . Impact on learning . The current design of Selenite functions as an “index” to direct users to relevant parts of a web page for reading and processing . However , we need to be cautious about a potential risk associated with this approach — some users might believe they have gained sufficient knowledge about a topic by merely reading the overview and may , therefore , skip engaging with the actual web content . This behavior could lead to incomplete , biased , or even inaccurate understandings of the subject . It is akin to only reading the table of contents or indices of a book without delving into the actual passages . Nevertheless , our studies conducted under controlled settings have shown that participants did , in fact , engage with the actual web content after going through the overviews . To build on this promising evidence , future research should addi - tionally investigate interface and interaction designs that motivate users to explore and read the actual web content with the assistance of Selenite - style guidance . One potential approach could be progres - sively revealing criteria information to users based on their reading behavior , encouraging deeper exploration and understanding . Field Study . In the future , once all the bugs and usability issues have been thoroughly addressed , we aim to conduct an extensive , long - term field study on a larger scale , where people will have both sufficient motivations to investigate topics relevant to their own personal context and familiarity with Selenite through repeated usage . This could potentially shed light on situations where Selen - ite performs reasonably well , as well as situations where it may fall short . Additionally , we are also interested in understanding Selenite’s long - term impact on individuals’ analytical skills and problem - solving abilities . 10 CONCLUSION Sensemaking in unfamiliar domains can often be challenging , with users having to sift through large volumes of information and compare different options with respect to various criteria . Previous sensemaking research as well as our new formative study has shown that people would benefit from seeing an overview of the informa - tion space , such as the criteria that others have previously found useful . However , existing systems have been limited by the “cold start” issue — they require substantial effort from previous users to gather and structure information to produce such an overview , and , even if it has been produced , there are no straightforward methods of sharing that overview with future users and making it so that fu - ture users would find it to be comprehensive , unbiased , and useful . In this work , by leveraging recent advances in large language models and natural language processing , we introduce a novel sys - tem named Selenite that automates finding an initial set of options , criteria , and evidence , and provides a comprehensive overview to users at the start of their sensemaking process . In addition , it also adapts as people use it , helping users find , read , and navigate unfamiliar information in a systematic yet personalized manner . As such , it provides a valuable proof of concept of how a future LLM - powered sensemaking tool that provides users with compre - hensive overviews and in - context reading guidance can scaffold their sensemaking and learning of an unfamiliar space . REFERENCES [ 1 ] Badr AlKhamissi , Millicent Li , Asli Celikyilmaz , Mona Diab , and Marjan Ghazvininejad . 2022 . A Review on Language Models as Knowledge Bases . https : / / doi . org / 10 . 48550 / arXiv . 2204 . 06031 arXiv : 2204 . 06031 [ cs ] . [ 2 ] Stefanos Angelidis and Mirella Lapata . 2018 . Summarizing Opinions : Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised . In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , Ellen Riloff , David Chiang , Julia Hockenmaier , and Jun’ichi Tsujii ( Eds . ) . Association for Computational Linguistics , Brussels , Belgium , 3675 – 3686 . https : / / doi . org / 10 . 18653 / v1 / D18 - 1403 [ 3 ] Anthropic . 2023 . Claude . https : / / claude . ai / chats [ 4 ] Kumaripaba Athukorala , Alan Medlar , Antti Oulasvirta , Giulio Jacucci , and Dorota Glowacka . 2016 . Beyond Relevance : Adapting Exploration / Exploitation in Information Retrieval . In Proceedings of the 21st International Conference on Intelligent User Interfaces ( IUI ’16 ) . Association for Computing Machinery , New York , NY , USA , 359 – 369 . https : / / doi . org / 10 . 1145 / 2856767 . 2856786 [ 5 ] TalAugust , LucyLuWang , JonathanBragg , MartiA . Hearst , AndrewHead , and Kyle Lo . 2023 . Paper Plain : Making Medical Research Papers Approachable to Healthcare Consumers with Natural Language Processing . ACM Transactions on Computer - Human Interaction ( April 2023 ) . https : / / doi . org / 10 . 1145 / 3589955 Just Accepted . [ 6 ] Michelle Q . Wang Baldonado and Terry Winograd . 1997 . SenseMaker : An Information - exploration Interface Supporting the Contextual Evolution of a User’s Interests . In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems ( CHI ’97 ) . ACM , New York , NY , USA , 11 – 18 . https : / / doi . org / 10 . 1145 / 258549 . 258563 [ 7 ] Yejin Bang , Samuel Cahyawijaya , Nayeon Lee , Wenliang Dai , Dan Su , Bryan Wilie , Holy Lovenia , Ziwei Ji , Tiezheng Yu , Willy Chung , Quyet V . Do , Yan Xu , and Pascale Fung . 2023 . A Multitask , Multilingual , Multimodal Evaluation of ChatGPT on Reasoning , Hallucination , and Interactivity . https : / / doi . org / 10 . 48550 / arXiv . 2302 . 04023 arXiv : 2302 . 04023 [ cs ] . [ 8 ] Krishna Bharat . 2000 . SearchPad : explicit capture of search context to support Web search . Computer Networks 33 , 1 ( June 2000 ) , 493 – 501 . https : / / doi . org / 10 . 1016 / S1389 - 1286 ( 00 ) 00047 - 5 [ 9 ] BlockTechnology . 2023 . AskYourPDF : The Best PDF AI Chat App . https : / / askyourpdf . com [ 10 ] Samuel Brody and Noemie Elhadad . 2010 . An Unsupervised Aspect - Sentiment Model for Online Reviews . In Human Language Technologies : The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics , Ron Kaplan , Jill Burstein , Mary Harper , and Gerald Penn ( Eds . ) . Association for Computational Linguistics , Los Angeles , California , 804 – 812 . https : / / aclanthology . org / N10 - 1122 [ 11 ] Sébastien Bubeck , Varun Chandrasekaran , Ronen Eldan , Johannes Gehrke , Eric Horvitz , Ece Kamar , Peter Lee , Yin Tat Lee , Yuanzhi Li , Scott Lundberg , Harsha Nori , Hamid Palangi , Marco Tulio Ribeiro , and Yi Zhang . 2023 . Sparks of Artificial General Intelligence : Early experiments with GPT - 4 . https : / / doi . org / 10 . 48550 / arXiv . 2303 . 12712 arXiv : 2303 . 12712 [ cs ] . [ 12 ] Jürgen Buder , Christina Schwind , Anja Rudat , and Daniel Bodemer . 2015 . Se - lective reading of large online forum discussions : The impact of rating visual - izations on navigation and learning . Computers in Human Behavior 44 ( March 2015 ) , 191 – 201 . https : / / doi . org / 10 . 1016 / j . chb . 2014 . 11 . 043 [ 13 ] Robert Capra and Jaime Arguello . 2023 . How does AI chat change search behaviors ? https : / / doi . org / 10 . 48550 / arXiv . 2307 . 03826 arXiv : 2307 . 03826 [ cs ] . [ 14 ] Joseph Chee Chang , Nathan Hahn , and Aniket Kittur . 2016 . Supporting Mobile Sensemaking Through Intentionally Uncertain Highlighting . In Proceedings of the 29th Annual Symposium on User Interface Software and Technology ( UIST ’16 ) . ACM , New York , NY , USA , 61 – 68 . https : / / doi . org / 10 . 1145 / 2984511 . 2984538 [ 15 ] Joseph Chee Chang , Nathan Hahn , and Aniket Kittur . 2020 . Mesh : Scaffolding Comparison Tables for Online Decision Making . In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology ( UIST ’20 ) . Association for Computing Machinery , New York , NY , USA , 391 – 405 . https : / / doi . org / 10 . 1145 / 3379337 . 3415865 [ 16 ] JosephCheeChang , NathanHahn , AdamPerer , andAniketKittur . 2019 . Search - Lens : composing and capturing complex user interests for exploratory search . In Proceedings of the 24th International Conference on Intelligent User Interfaces ( IUI ’19 ) . Association for Computing Machinery , Marina del Ray , California , 498 – 509 . https : / / doi . org / 10 . 1145 / 3301275 . 3302321 [ 17 ] Joseph Chee Chang , Yongsung Kim , Victor Miller , Michael Xieyang Liu , Brad A Myers , andAniketKittur . 2021 . Tabs . do : Task - CentricBrowserTabManagement . In The 34th Annual ACM Symposium on User Interface Software and Technology . Association for Computing Machinery , New York , NY , USA , 663 – 676 . https : / / doi . org / 10 . 1145 / 3472749 . 3474777 15 Liu and Wu et al . [ 18 ] Joseph Chee Chang , Aniket Kittur , and Nathan Hahn . 2016 . Alloy : Clustering with Crowds and Computation . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems ( CHI ’16 ) . ACM , New York , NY , USA , 3180 – 3191 . https : / / doi . org / 10 . 1145 / 2858036 . 2858411 [ 19 ] Kathy Charmaz . 2006 . Constructing Grounded Theory : A Practical Guide through Qualitative Analysis . SAGE . Google - Books - ID : 2ThdBAAAQBAJ . [ 20 ] TianyingChen , MichaelXieyangLiu , EmilyDing , EmmaO’Neil , MansiAgarwal , Robert E Kraut , and Laura Dabbish . 2023 . Facilitating Counselor Reflective Learning with a Real - time Annotation tool . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems ( CHI ’23 ) . Association for Computing Machinery , New York , NY , USA , 1 – 17 . https : / / doi . org / 10 . 1145 / 3544548 . 3581551 [ 21 ] Xiang’Anthony’ Chen , Chien - Sheng Wu , Tong Niu , Wenhao Liu , and Caiming Xiong . 2022 . Marvista : A Human - AI Collaborative Reading Tool . arXiv preprint arXiv : 2207 . 08401 ( 2022 ) . [ 22 ] Lydia B . Chilton , Greg Little , Darren Edge , Daniel S . Weld , and James A . Landay . 2013 . Cascade : crowdsourcing taxonomy creation . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’13 ) . Association for Computing Machinery , New York , NY , USA , 1999 – 2008 . https : / / doi . org / 10 . 1145 / 2470654 . 2466265 [ 23 ] Aakanksha Chowdhery , Sharan Narang , Jacob Devlin , Maarten Bosma , Gaurav Mishra , Adam Roberts , Paul Barham , Hyung Won Chung , Charles Sutton , Se - bastian Gehrmann , Parker Schuh , Kensen Shi , Sasha Tsvyashchenko , Joshua Maynez , Abhishek Rao , Parker Barnes , Yi Tay , Noam Shazeer , Vinodkumar Prabhakaran , Emily Reif , Nan Du , Ben Hutchinson , Reiner Pope , James Brad - bury , Jacob Austin , Michael Isard , Guy Gur - Ari , Pengcheng Yin , Toju Duke , Anselm Levskaya , Sanjay Ghemawat , Sunipa Dev , Henryk Michalewski , Xavier Garcia , Vedant Misra , Kevin Robinson , Liam Fedus , Denny Zhou , Daphne Ip - polito , David Luan , Hyeontaek Lim , Barret Zoph , Alexander Spiridonov , Ryan Sepassi , David Dohan , Shivani Agrawal , Mark Omernick , Andrew M . Dai , Thanumalayan Sankaranarayana Pillai , Marie Pellat , Aitor Lewkowycz , Er - ica Moreira , Rewon Child , Oleksandr Polozov , Katherine Lee , Zongwei Zhou , Xuezhi Wang , Brennan Saeta , Mark Diaz , Orhan Firat , Michele Catasta , Jason Wei , Kathy Meier - Hellstern , Douglas Eck , Jeff Dean , Slav Petrov , and Noah Fiedel . 2022 . PaLM : Scaling Language Modeling with Pathways . https : / / doi . org / 10 . 48550 / arXiv . 2204 . 02311 arXiv : 2204 . 02311 [ cs ] . [ 24 ] Jason Chuang , Daniel Ramage , Christopher Manning , and Jeffrey Heer . 2012 . Interpretation and trust : designing model - driven visualizations for text analysis . In ProceedingsoftheSIGCHIConferenceonHumanFactorsinComputingSystems ( CHI ’12 ) . Association for Computing Machinery , New York , NY , USA , 443 – 452 . https : / / doi . org / 10 . 1145 / 2207676 . 2207738 [ 25 ] Roi Cohen , Mor Geva , Jonathan Berant , and Amir Globerson . 2023 . Crawling the Internal Knowledge - Base of Language Models . https : / / doi . org / 10 . 48550 / arXiv . 2301 . 12810 arXiv : 2301 . 12810 [ cs ] . [ 26 ] Dick Cunninghamand Scott L . Shablak . 1975 . Selective ReadingGuide - O - Rama : The Content Teacher’s Best Friend . The Journal of Reading ( 1975 ) . [ 27 ] Subha Jyoti Das , Riki Murakami , and Basabi Chakraborty . 2021 . Development of a two - step LDA based aspect extraction technique for review summarization . International Journal of Applied Science and Engineering 18 , 1 ( 2021 ) , 1 – 18 . https : / / doi . org / 10 . 6703 / IJASE . 202103 _ 18 ( 1 ) . 003 Publisher : Chaoyang University of Technology . [ 28 ] BrendaDervin . 1983 . Anoverviewofsense - makingresearchconcepts , methods , and results to date . ( 1983 ) . http : / / www . worldcat . org / title / overview - of - sense - making - research - concepts - methods - and - results - to - date / oclc / 733067203 [ 29 ] Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 . BERT : Pre - training of Deep Bidirectional Transformers for Language Under - standing . arXiv : 1810 . 04805 [ cs ] ( May 2019 ) . http : / / arxiv . org / abs / 1810 . 04805 arXiv : 1810 . 04805 . [ 30 ] G . A . Di Lucca , M . Di Penta , and A . R . Fasolino . 2002 . An approach to identify duplicated web pages . In Proceedings 26th Annual International Computer Soft - ware and Applications . 481 – 486 . https : / / doi . org / 10 . 1109 / CMPSAC . 2002 . 1045051 ISSN : 0730 - 3157 . [ 31 ] Mira Dontcheva , Steven M . Drucker , Geraldine Wade , David Salesin , and Michael F . Cohen . 2006 . Summarizing Personal Web Browsing Sessions . In Proceedings of the 19th Annual ACM Symposium on User Interface Soft - ware and Technology ( UIST ’06 ) . ACM , New York , NY , USA , 115 – 124 . https : / / doi . org / 10 . 1145 / 1166253 . 1166273 [ 32 ] Paul Dourish and Victoria Bellotti . 1992 . Awareness and coordination in shared workspaces . In Proceedings of the 1992 ACM conference on Computer - supported cooperative work ( CSCW ’92 ) . Association for Computing Machinery , Toronto , Ontario , Canada , 107 – 114 . https : / / doi . org / 10 . 1145 / 143457 . 143468 [ 33 ] Evernote . [ n . d . ] . Best Note Taking App - Organize Your Notes with Evernote . https : / / evernote . com [ 34 ] Facebook . 2018 . React - A JavaScript library for building user interfaces . https : / / reactjs . org / [ 35 ] David K . Farkas and Christopher Raleigh . 2013 . Designing Documents for Selective Reading . Information Design Journal 20 , 1 ( Jan . 2013 ) , 2 – 15 . https : / / doi . org / 10 . 1075 / idj . 20 . 1 . 01far Publisher : John Benjamins . [ 36 ] Kristie Fisher , Scott Counts , and Aniket Kittur . 2012 . Distributed Sensemaking : ImprovingSensemakingbyLeveragingtheEffortsofPreviousUsers . In Proceed - ings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’12 ) . ACM , New York , NY , USA , 247 – 256 . https : / / doi . org / 10 . 1145 / 2207676 . 2207711 [ 37 ] Andrew J . Flanagin and Miriam J . Metzger . 2000 . Perceptions of Internet Infor - mation Credibility . Journalism & Mass Communication Quarterly 77 , 3 ( Sept . 2000 ) , 515 – 540 . https : / / doi . org / 10 . 1177 / 107769900007700304 Publisher : SAGE Publications Inc . [ 38 ] RaymondFok , HitaKambhamettu , LucaSoldaini , JonathanBragg , KyleLo , Marti Hearst , Andrew Head , and Daniel S Weld . 2023 . Scim : Intelligent Skimming Support for Scientific Papers . In Proceedings of the 28th International Conference on Intelligent User Interfaces ( IUI ’23 ) . Association for Computing Machinery , New York , NY , USA , 476 – 490 . https : / / doi . org / 10 . 1145 / 3581641 . 3584034 [ 39 ] Saskia Gilmer , Avinash Bhat , Shuvam Shah , Kevin Cherry , Jinghui Cheng , and Jin L . C . Guo . 2023 . SUMMIT : Scaffolding OSS Issue Discussion Through Summarization . Proceedings of the ACM on Human - Computer Interaction 7 , CSCW2 ( Sept . 2023 ) , 1 – 27 . https : / / doi . org / 10 . 1145 / 3610088 arXiv : 2308 . 02780 [ cs ] . [ 40 ] Nathan Hahn , Joseph Chang , Ji Eun Kim , and Aniket Kittur . 2016 . The Knowl - edge Accelerator : Big Picture Thinking in Small Pieces . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems ( CHI ’16 ) . ACM , New York , NY , USA , 2258 – 2270 . https : / / doi . org / 10 . 1145 / 2858036 . 2858364 [ 41 ] Nathan Hahn , Joseph Chee Chang , and Aniket Kittur . 2018 . Bento Browser : ComplexMobileSearchWithoutTabs . In Proceedingsofthe2018CHIConference on Human Factors in Computing Systems ( CHI ’18 ) . ACM , Montreal QC , Canada , 251 : 1 – 251 : 12 . https : / / doi . org / 10 . 1145 / 3173574 . 3173825 [ 42 ] Dianne J . Hall and Robert A . Davis . 2007 . Engaging multiple perspectives : A value - based decision - making model . Decision Support Systems 43 , 4 ( Aug . 2007 ) , 1588 – 1604 . https : / / doi . org / 10 . 1016 / j . dss . 2006 . 03 . 004 [ 43 ] Sandra G . Hart and Lowell E . Staveland . 1988 . Development of NASA - TLX ( Task Load Index ) : Results of Empirical and Theoretical Research . In Advances in Psychology , Peter A . Hancock and Najmedin Meshkati ( Eds . ) . Human Mental Workload , Vol . 52 . North - Holland , 139 – 183 . https : / / doi . org / 10 . 1016 / S0166 - 4115 ( 08 ) 62386 - 9 [ 44 ] Andrew Head , Amber Xie , and Marti A . Hearst . 2022 . Math Augmentation : How Authors Enhance the Readability of Formulas using Novel Visual Design Practices . In Proceedings of the 2022 CHI Conference on Human Factors in Com - puting Systems ( CHI ’22 ) . Association for Computing Machinery , New York , NY , USA , 1 – 18 . https : / / doi . org / 10 . 1145 / 3491102 . 3501932 [ 45 ] Marti A . Hearst . 2006 . Clustering versus faceted categories for information exploration . Commun . ACM 49 , 4 ( April 2006 ) , 59 – 61 . https : / / doi . org / 10 . 1145 / 1121949 . 1121983 [ 46 ] Marti A . Hearst . 2014 . What’s Missing from Collaborative Search ? Computer 47 , 3 ( March 2014 ) , 58 – 61 . https : / / doi . org / 10 . 1109 / MC . 2014 . 77 [ 47 ] Lawrence J . Henschen and Julia C . Lee . 2009 . Using Semantic - Level Tags in HTML / XML Documents . In Universal Access in Human - Computer Interac - tion . Applications and Services ( Lecture Notes in Computer Science ) , Constantine Stephanidis ( Ed . ) . Springer , Berlin , Heidelberg , 683 – 692 . https : / / doi . org / 10 . 1007 / 978 - 3 - 642 - 02713 - 0 _ 72 [ 48 ] Andrew Hogue and David Karger . 2005 . Thresher : automating the unwrapping ofsemanticcontentfromtheWorldWideWeb . In Proceedingsofthe14thinterna - tionalconferenceonWorldWideWeb ( WWW’05 ) . AssociationforComputingMa - chinery , New York , NY , USA , 86 – 95 . https : / / doi . org / 10 . 1145 / 1060745 . 1060762 [ 49 ] Johan F . Hoorn and Teunis D . van Wijngaarden . 2010 . Web Intelligence for the Assessment of Information Quality : Credibility , Correctness , and Readability . Web Intelligence and Intelligent Agents ( March 2010 ) . https : / / doi . org / 10 . 5772 / 8372 Publisher : IntechOpen . [ 50 ] Amber Horvath , Michael Xieyang Liu , River Hendriksen , Connor Shannon , Emma Paterson , Kazi Jawad , Andrew Macvean , and Brad A Myers . 2022 . Un - derstanding How Programmers Can Use Annotations on Documentation . In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems ( CHI ’22 ) . Association for Computing Machinery , New York , NY , USA , 1 – 16 . https : / / doi . org / 10 . 1145 / 3491102 . 3502095 [ 51 ] Jane Hsieh , Michael Xieyang Liu , Brad A . Myers , and Aniket Kittur . 2018 . An Exploratory Study of Web Foraging to Understand and Support Programming Decisions . In 2018 IEEE Symposium on Visual Languages and Human - Centric Computing ( VL / HCC ) . 305 – 306 . https : / / doi . org / 10 . 1109 / VLHCC . 2018 . 8506517 ISSN : 1943 - 6092 . [ 52 ] Wei Jin , Hung Hay Ho , and Rohini K . Srihari . 2009 . OpinionMiner : a novel machine learning system for web opinion mining and extraction . In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining ( KDD ’09 ) . Association for Computing Machinery , New York , NY , USA , 1195 – 1204 . https : / / doi . org / 10 . 1145 / 1557019 . 1557148 [ 53 ] Hyeonsu B . Kang , Joseph Chee Chang , Yongsung Kim , and Aniket Kittur . 2022 . Threddy : An Interactive System for Personalized Thread - based Exploration and Organization of Scientific Literature . https : / / doi . org / 10 . 1145 / 3526113 . 3545660 arXiv : 2208 . 03455 [ cs ] . 16 Selenite : Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models [ 54 ] Aniket Kittur , Andrew M . Peters , Abdigani Diriye , and Michael Bove . 2014 . Standing on the Schemas of Giants : Socially Augmented Information Foraging . In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing ( CSCW ’14 ) . ACM , New York , NY , USA , 999 – 1010 . https : / / doi . org / 10 . 1145 / 2531602 . 2531644 [ 55 ] AniketKittur , AndrewM . Peters , AbdiganiDiriye , TruptiTelang , andMichaelR . Bove . 2013 . Costs and Benefits of Structured Information Foraging . In Proceed - ings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’13 ) . ACM , NewYork , NY , USA , 2989 – 2998 . https : / / doi . org / 10 . 1145 / 2470654 . 2481415 [ 56 ] Aniket Kittur , Bongwon Suh , and Ed H . Chi . 2008 . Can you ever trust a wiki ? impacting perceived trustworthiness in wikipedia . In Proceedings of the 2008 ACM conference on Computer supported cooperative work ( CSCW ’08 ) . Association for Computing Machinery , San Diego , CA , USA , 477 – 480 . https : / / doi . org / 10 . 1145 / 1460563 . 1460639 [ 57 ] G . Klein , B . Moon , and R . R . Hoffman . 2006 . Making Sense of Sensemaking 1 : Alternative Perspectives . IEEE Intelligent Systems 21 , 4 ( July 2006 ) , 70 – 73 . https : / / doi . org / 10 . 1109 / MIS . 2006 . 75 [ 58 ] Torkel Klingberg . 2009 . The Overflowing Brain : Information Overload and the Limits of Working Memory . Oxford University Press , USA . Google - Books - ID : IxMSDAAAQBAJ . [ 59 ] Weize Kong and James Allan . 2014 . Extending Faceted Search to the General Web . In Proceedings of the 23rd ACM International Conference on Conference on InformationandKnowledgeManagement ( CIKM’14 ) . AssociationforComputing Machinery , New York , NY , USA , 839 – 848 . https : / / doi . org / 10 . 1145 / 2661829 . 2661964 [ 60 ] Sachin Kumar , Vidhisha Balachandran , Lucille Njoo , Antonios Anastasopoulos , and Yulia Tsvetkov . 2023 . Language Generation Models Can Cause Harm : So What Can We Do About It ? An Actionable Survey . In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics . Association for Computational Linguistics , Dubrovnik , Croatia , 3299 – 3321 . https : / / aclanthology . org / 2023 . eacl - main . 241 [ 61 ] Matevž Kunaver and Tomaž Požrl . 2017 . Diversity in recommender systems – A survey . Knowledge - Based Systems 123 ( May 2017 ) , 154 – 162 . https : / / doi . org / 10 . 1016 / j . knosys . 2017 . 02 . 009 [ 62 ] Andrew Kuznetsov , Joseph Chee Chang , Nathan Hahn , Napol Rachatasumrit , Bradley Breneisen , Julina Coupland , and Aniket Kittur . 2022 . Fuse : In - Situ Sensemaking Support in the Browser . https : / / doi . org / 10 . 1145 / 3526113 . 3545693 arXiv : 2208 . 14861 [ cs ] . [ 63 ] James R . Lewis . 2018 . The System Usability Scale : Past , Present , and Future . International Journal of Human – Computer Interaction 34 , 7 ( July 2018 ) , 577 – 590 . https : / / doi . org / 10 . 1080 / 10447318 . 2018 . 1455307 Publisher : Taylor & Francis _ eprint : https : / / doi . org / 10 . 1080 / 10447318 . 2018 . 1455307 . [ 64 ] Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer . 2019 . BART : DenoisingSequence - to - SequencePre - trainingforNaturalLanguageGeneration , Translation , and Comprehension . CoRR abs / 1910 . 13461 ( 2019 ) . http : / / arxiv . org / abs / 1910 . 13461 arXiv : 1910 . 13461 . [ 65 ] Patrick Lewis , Ethan Perez , Aleksandra Piktus , Fabio Petroni , Vladimir Karpukhin , Naman Goyal , Heinrich Küttler , Mike Lewis , Wen - tau Yih , Tim Rocktäschel , Sebastian Riedel , and Douwe Kiela . 2020 . Retrieval - Augmented Generation for Knowledge - Intensive NLP Tasks . In Ad - vances in Neural Information Processing Systems , Vol . 33 . Curran Asso - ciates , Inc . , 9459 – 9474 . https : / / proceedings . neurips . cc / paper / 2020 / hash / 6b493230205f780e1bc26945df7481e5 - Abstract . html [ 66 ] Michael Xieyang Liu , Nathan Hahn , Angelina Zhou , Shaun Burley , Emily Deng , Aniket Kittur , and Brad A . Myers . 2018 . UNAKITE : Support Developers for Capturing and Persisting Design Rationales When Solving Problems Using Web Resources . Workshop on Designing Technologies to Support Human Problem Solving at the IEEE Sympo - sium on Visual Languages and Human - Centric Computing ( Oct . 2018 ) . https : / / par . nsf . gov / biblio / 10152060 - unakite - support - developers - capturing - persisting - design - rationales - when - solving - problems - using - web - resources [ 67 ] Michael Xieyang Liu , Jane Hsieh , Nathan Hahn , Angelina Zhou , Emily Deng , Shaun Burley , Cynthia Taylor , Aniket Kittur , and Brad A . Myers . 2019 . Unakite : Scaffolding Developers’ Decision - Making Using the Web . In Proceedings of the 32Nd Annual ACM Symposium on User Interface Software and Technology ( UIST ’19 ) . ACM , New Orleans , LA , USA , 67 – 80 . https : / / doi . org / 10 . 1145 / 3332165 . 3347908 event - place : New Orleans , LA , USA . [ 68 ] Michael Xieyang Liu , Aniket Kittur , and Brad A . Myers . 2021 . To Reuse or Not To Reuse ? A Framework and System for Evaluating Summarized Knowledge . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW1 ( April 2021 ) , 166 : 1 – 166 : 35 . https : / / doi . org / 10 . 1145 / 3449240 [ 69 ] Michael Xieyang Liu , Aniket Kittur , and Brad A . Myers . 2022 . Crystalline : Lowering the Cost for Developers to Collect and Organize Information for Decision Making . In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems ( CHI ’22 ) . Association for Computing Machinery , New York , NY , USA . https : / / doi . org / 10 . 1145 / 3491102 . 3501968 event - place : New Orleans , LA , USA . [ 70 ] Michael Xieyang Liu , Andrew Kuznetsov , Yongsung Kim , Joseph Chee Chang , Aniket Kittur , and Brad A . Myers . 2022 . Wigglite : Low - cost Information Collec - tion and Triage . In The 35th Annual ACM Symposium on User Interface Software and Technology ( UIST ’22 ) . Association for Computing Machinery , New York , NY , USA . https : / / doi . org / 10 . 1145 / 3526113 . 3545661 [ 71 ] Michael Xieyang Liu , Advait Sarkar , Carina Negreanu , Benjamin Zorn , Jack Williams , Neil Toronto , and Andrew D . Gordon . 2023 . “What It Wants Me To Say” : Bridging the Abstraction Gap Between End - User Programmers and Code - Generating Large Language Models . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems ( CHI ’23 ) . Association for Computing Machinery , New York , NY , USA , 1 – 31 . https : / / doi . org / 10 . 1145 / 3544548 . 3580817 [ 72 ] Nelson F . Liu , Tianyi Zhang , and Percy Liang . 2023 . Evaluating Verifiabil - ity in Generative Search Engines . https : / / doi . org / 10 . 48550 / arXiv . 2304 . 09848 arXiv : 2304 . 09848 [ cs ] . [ 73 ] AmanMadaan , NiketTandon , PrakharGupta , SkylerHallinan , LuyuGao , Sarah Wiegreffe , UriAlon , NouhaDziri , ShrimaiPrabhumoye , YimingYang , Shashank Gupta , BodhisattwaPrasadMajumder , KatherineHermann , SeanWelleck , Amir Yazdanbakhsh , and Peter Clark . 2023 . Self - Refine : Iterative Refinement with Self - Feedback . https : / / doi . org / 10 . 48550 / arXiv . 2303 . 17651 arXiv : 2303 . 17651 [ cs ] . [ 74 ] Joaquim Mendes , Nuno Laranjeiro , and Marco Vieira . 2018 . Toward characterizing HTML defects on the Web . Software : Practice and Expe - rience 48 , 3 ( 2018 ) , 750 – 757 . https : / / doi . org / 10 . 1002 / spe . 2545 _ eprint : https : / / onlinelibrary . wiley . com / doi / pdf / 10 . 1002 / spe . 2545 . [ 75 ] Danaë Metaxa , Joon Sung Park , Ronald E . Robertson , Karrie Karahalios , Christo Wilson , Jeff Hancock , and Christian Sandvig . 2021 . Auditing Algorithms : Understanding Algorithmic Systems from the Outside In . Foundations and Trends® in Human – Computer Interaction 14 , 4 ( Nov . 2021 ) , 272 – 344 . https : / / doi . org / 10 . 1561 / 1100000083 Publisher : Now Publishers , Inc . . [ 76 ] MeredithRingelMorrisandEricHorvitz . 2007 . SearchTogether : AnInterfacefor Collaborative Web Search . In Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology ( UIST ’07 ) . ACM , New York , NY , USA , 3 – 12 . https : / / doi . org / 10 . 1145 / 1294211 . 1294215 [ 77 ] Moin Nadeem , Anna Bethke , and Siva Reddy . 2021 . StereoSet : Measuring stereotypical bias in pretrained language models . In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) . Association for Computational Linguistics , Online , 5356 – 5371 . https : / / doi . org / 10 . 18653 / v1 / 2021 . acl - long . 416 [ 78 ] An T . Nguyen , Aditya Kharosekar , Saumyaa Krishnan , Siddhesh Krishnan , Elizabeth Tate , Byron C . Wallace , and Matthew Lease . 2018 . Believe it or not : Designing a Human - AI Partnership for Mixed - Initiative Fact - Checking . In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology ( UIST ’18 ) . Association for Computing Machinery , New York , NY , USA , 189 – 199 . https : / / doi . org / 10 . 1145 / 3242587 . 3242666 [ 79 ] OpenAI . 2023 . ChatGPT . https : / / chat . openai . com [ 80 ] OpenAI . 2023 . GPT - 4 Technical Report . https : / / doi . org / 10 . 48550 / arXiv . 2303 . 08774 arXiv : 2303 . 08774 [ cs ] . [ 81 ] Sharoda A . Paul and Meredith Ringel Morris . 2009 . CoSense : Enhancing Sense - making for Collaborative Web Search . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’09 ) . ACM , New York , NY , USA , 1771 – 1780 . https : / / doi . org / 10 . 1145 / 1518701 . 1518974 [ 82 ] Alvaro Pereira , Ricardo Baeza - Yates , and Nivio Ziviani . 2006 . Where and How Duplicates Occur in the Web . In 2006 Fourth Latin American Web Congress . 127 – 134 . https : / / doi . org / 10 . 1109 / LA - WEB . 2006 . 39 [ 83 ] Peter Pirolli and Stuart Card . 2005 . The Sensemaking Process and Lever - age Points for Analyst Technology as Identified Through Cognitive Task Analysis . In Proceedings of International Conference on Intelligence Analy - sis . http : / / www . phibetaiota . net / wp - content / uploads / 2014 / 12 / Sensemaking - Process - Pirolli - and - Card . pdf [ 84 ] Yannis Plegas and Sofia Stamou . 2013 . Reducing information redundancy in search results . In Proceedings of the 28th Annual ACM Symposium on Applied Computing ( SAC ’13 ) . Association for Computing Machinery , New York , NY , USA , 886 – 893 . https : / / doi . org / 10 . 1145 / 2480362 . 2480533 [ 85 ] Marlene A . Plumlee . 2003 . The Effect of Information Complexity on Analysts’ Use of That Information . The Accounting Review 78 , 1 ( Jan . 2003 ) , 275 – 296 . https : / / doi . org / 10 . 2308 / accr . 2003 . 78 . 1 . 275 [ 86 ] G . Poonkuzhali , R . Kishore Kumar , R . Kripa Keshav , P . Sudhakar , and K . Sarukesi . 2011 . Correlation Based Method to Detect and Remove Redun - dant Web Document . Advanced Materials Research 171 - 172 ( 2011 ) , 543 – 546 . https : / / doi . org / 10 . 4028 / www . scientific . net / AMR . 171 - 172 . 543 Publisher : Trans Tech Publications Ltd . [ 87 ] Soujanya Poria , Erik Cambria , Lun - Wei Ku , Chen Gui , and Alexander Gelbukh . 2014 . A rule - based approach to aspect extraction from product reviews . In Proceedings of the second workshop on natural language processing for social media ( SocialNLP ) . 28 – 37 . [ 88 ] Napol Rachatasumrit , Jonathan Bragg , Amy X . Zhang , and Daniel S Weld . 2022 . CiteRead : Integrating Localized Citation Contexts into Scientific Paper 17 Liu and Wu et al . Reading . In 27th International Conference on Intelligent User Interfaces ( IUI ’22 ) . Association for Computing Machinery , New York , NY , USA , 707 – 719 . https : / / doi . org / 10 . 1145 / 3490099 . 3511162 [ 89 ] Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , Ilya Sutskever , and others . 2019 . Language models are unsupervised multitask learners . OpenAI blog 1 , 8 ( 2019 ) , 9 . [ 90 ] Nils Reimers and Iryna Gurevych . 2019 . Sentence - BERT : Sentence Embeddings using Siamese BERT - Networks . https : / / doi . org / 10 . 48550 / arXiv . 1908 . 10084 arXiv : 1908 . 10084 [ cs ] . [ 91 ] Daniel M . Russell , Mark J . Stefik , Peter Pirolli , and Stuart K . Card . 1993 . The Cost Structure of Sensemaking . In Proceedings of the INTERACT ’93 and CHI ’93 Conference on Human Factors in Computing Systems ( CHI ’93 ) . ACM , New York , NY , USA , 269 – 276 . https : / / doi . org / 10 . 1145 / 169059 . 169209 [ 92 ] M . C . schraefel , Yuxiang Zhu , David Modjeska , Daniel Wigdor , and Shengdong Zhao . 2002 . Hunter Gatherer : Interaction Support for the Creation and Manage - ment of Within - web - page Collections . In Proceedings of the 11th International Conference on World Wide Web ( WWW ’02 ) . ACM , New York , NY , USA , 172 – 181 . https : / / doi . org / 10 . 1145 / 511446 . 511469 [ 93 ] Ben Shneiderman . 2000 . Designing trust into online experiences . Commun . ACM 43 , 12 ( Dec . 2000 ) , 57 – 59 . https : / / doi . org / 10 . 1145 / 355112 . 355124 [ 94 ] Alexandra N Spichtig , Elfrieda H Hiebert , Christian Vorstius , Jeffrey P Pascoe , P David Pearson , and Ralph Radach . 2016 . The decline of comprehension - based silent reading efficiency in the United States : A comparison of current data with performance in 1960 . Reading Research Quarterly 51 , 2 ( 2016 ) , 239 – 259 . Publisher : Wiley Online Library . [ 95 ] Bongwon Suh , Ed H . Chi , Aniket Kittur , and Bryan A . Pendleton . 2008 . Lifting the veil : improving accountability and social transparency in Wikipedia with wikidashboard . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’08 ) . Association for Computing Machinery , Florence , Italy , 1037 – 1040 . https : / / doi . org / 10 . 1145 / 1357054 . 1357214 [ 96 ] João Sá , Vanessa Queiroz Marinho , Ana Rita Magalhães , Tiago Lacerda , and Diogo Goncalves . 2022 . Diversity Vs Relevance : A Practical Multi - objective Study in Luxury Fashion Recommendations . In Proceedings of the 45th Inter - national ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR ’22 ) . Association for Computing Machinery , New York , NY , USA , 2405 – 2409 . https : / / doi . org / 10 . 1145 / 3477495 . 3531866 [ 97 ] H . HoldenThorp . 2023 . ChatGPTisfun , butnotanauthor . Science 379 , 6630 ( Jan . 2023 ) , 313 – 313 . https : / / doi . org / 10 . 1126 / science . adg7879 Publisher : American Association for the Advancement of Science . [ 98 ] Hugo Touvron , Louis Martin , Kevin Stone , Peter Albert , Amjad Almahairi , Yasmine Babaei , Nikolay Bashlykov , Soumya Batra , Prajjwal Bhargava , Shruti Bhosale , andothers . 2023 . Llama2 : Openfoundationandfine - tunedchatmodels . arXiv preprint arXiv : 2307 . 09288 ( 2023 ) . [ 99 ] Amos Tversky and Daniel Kahneman . 1974 . Judgment under Uncertainty : Heuristics and Biases : Biases in judgments reveal some heuristics of thinking under uncertainty . science 185 , 4157 ( 1974 ) , 1124 – 1131 . Publisher : American association for the advancement of science . [ 100 ] Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N . Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 . Attention Is All You Need . https : / / doi . org / 10 . 48550 / arXiv . 1706 . 03762 arXiv : 1706 . 03762 [ cs ] . [ 101 ] Chenguang Wang , Xiao Liu , and Dawn Song . 2020 . Language Models are Open KnowledgeGraphs . https : / / doi . org / 10 . 48550 / arXiv . 2010 . 11967 arXiv : 2010 . 11967 [ cs ] . [ 102 ] Jason Wei , Xuezhi Wang , Dale Schuurmans , Maarten Bosma , Brian Ichter , Fei Xia , Ed Chi , Quoc Le , and Denny Zhou . 2023 . Chain - of - Thought Prompting Elicits Reasoning in Large Language Models . https : / / doi . org / 10 . 48550 / arXiv . 2201 . 11903 arXiv : 2201 . 11903 [ cs ] . [ 103 ] Dale M . Willows . 1974 . Reading between the Lines : Selective Attention in Good and Poor Readers . Child Development 45 , 2 ( 1974 ) , 408 – 415 . https : / / doi . org / 10 . 2307 / 1127962 Publisher : [ Wiley , Society for Research in Child Development ] . [ 104 ] Dale M . Willows and G . E . MacKinnon . 1973 . Selective reading : Attention to the " unattended " lines . Canadian Journal of Psychology / Revue canadienne de psychologie 27 , 3 ( 1973 ) , 292 – 304 . https : / / doi . org / 10 . 1037 / h0082480 Place : Canada Publisher : University of Toronto Press . [ 105 ] Wondershare . 2023 . Chat with PDF * Powered by ChatGPT . https : / / www . hipdf . com / chat - with - pdf [ 106 ] Nicholas C . Wormald . 1995 . Differential Equations for Random Processes and Random Graphs . The Annals of Applied Probability 5 , 4 ( 1995 ) , 1217 – 1235 . https : / / doi . org / 10 . 1214 / aoap / 1177004612 Publisher : Institute of Mathematical Statistics . [ 107 ] Chenyang Yang , Rishabh Rustogi , Rachel Brower - Sinning , Grace A . Lewis , Christian Kästner , and Tongshuang Wu . 2023 . Beyond Testers’ Biases : Guiding Model Testing with Knowledge Bases using LLMs . https : / / doi . org / 10 . 48550 / arXiv . 2310 . 09668 arXiv : 2310 . 09668 [ cs ] . [ 108 ] ZhengyuanYang , ZheGan , JianfengWang , XiaoweiHu , YumaoLu , ZichengLiu , and Lijuan Wang . 2022 . An Empirical Study of GPT - 3 for Few - Shot Knowledge - Based VQA . Proceedings of the AAAI Conference on Artificial Intelligence 36 , 3 ( June 2022 ) , 3081 – 3089 . https : / / doi . org / 10 . 1609 / aaai . v36i3 . 20215 Number : 3 . [ 109 ] Ilan Yaniv and Shoham Choshen - Hillel . 2012 . When guessing what another person would say is better than giving your own opinion : Using perspective - taking to improve advice - taking . Journal of Experimental Social Psychology 48 , 5 ( Sept . 2012 ) , 1022 – 1028 . https : / / doi . org / 10 . 1016 / j . jesp . 2012 . 03 . 016 [ 110 ] Wenpeng Yin , Jamaal Hay , and Dan Roth . 2019 . Benchmarking Zero - shot Text Classification : Datasets , Evaluation and Entailment Approach . https : / / doi . org / 10 . 48550 / arXiv . 1909 . 00161 arXiv : 1909 . 00161 [ cs ] . [ 111 ] Amy X . Zhang and Justin Cranshaw . 2018 . Making Sense of Group Chat Through Collaborative Tagging and Summarization . Proc . ACM Hum . - Comput . Interact . 2 , CSCW ( Nov . 2018 ) , 196 : 1 – 196 : 27 . https : / / doi . org / 10 . 1145 / 3274465 [ 112 ] AmyX . Zhang , LeaVerou , andDavidKarger . 2017 . Wikum : BridgingDiscussion Forums and Wikis Using Recursive Summarization . In Proceedings of the 2017 ACMConferenceonComputerSupportedCooperativeWorkandSocialComputing ( CSCW ’17 ) . ACM , New York , NY , USA , 2082 – 2096 . https : / / doi . org / 10 . 1145 / 2998181 . 2998235 [ 113 ] Yi Zong and Xiaojie Guo . 2022 . An Experimental Study on Anchoring Effect of Consumers’ Price Judgment Based on Consumers’ Experiencing Scenes . Frontiers in Psychology 13 ( 2022 ) . https : / / www . frontiersin . org / articles / 10 . 3389 / fpsyg . 2022 . 794135 18 Selenite : Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models A UNFAMILIAR TOPICS PARTICIPANTS EXPLORED IN THE FORMATIVE STUDY Index Theme Topic Participants 1 Software & online services Choosing a hybrid app framework P2 , P5 2 Selecting a secure password manager P3 , P7 3 Choosing a suitable ERP ( Enterprise Resource Planning ) solution P1 4 Choosing a reliable VPN ( Virtual Private Network ) provider P1 , P5 5 Picking a deep learning framework P8 6 Deciding on the best data visualization tool P6 7 Choosing the best time tracking tool P4 8 Consumer Electronics & Technology Choosing a high - quality digital camera P2 , P5 9 Choosing the best action camera P8 10 Selecting a VR headset P7 11 Picking a drone P3 , P5 12 Picking a smart home ecosystem P1 , P6 , P8 13 Home Appliances & Furniture Picking the best robot vacuum P2 , P5 14 Choosing the best air purifier P4 15 Selecting the best washing machine P3 16 Picking the right refrigerator P3 17 Selecting the best mattress P3 , P6 18 Outdoor & Adventure Choosing the best city bike P7 19 Choosing the best barbecue grill P3 20 Choosing the best tropical vacation location P4 21 Health & fitness Choosing an effective diet plan P1 , P7 22 Picking a reliable treadmill P3 23 Picking the best running shoes P8 24 Gifts & special events Choosing a birthday gift P5 , P6 25 Picking the right wedding venue P2 26 Picking an engagement ring P2 27 Parenting Choosing the best baby stroller P8 28 Pets Choosing a breed of dog to adopt P4 Table 5 : Unfamiliar topics ( organized by themes ) that participants in the formative study reported encountering and exploring . Some topics were explored by multiple participants , such as “Picking a smart home ecosystem” and “Choosing a reliable VPN provider . ” 19 Liu and Wu et al . B STUDY 1 DETAILS B . 1 Example web pages Below in Figure 5 , we show a partial snapshot of a representative comparison article on the topic of “best baby strollers . ” Figure 5 : Example comparison article on the topic of “best baby strollers . ” Note that it only contains approximately 1 / 4 of the article , which originally includes content about 10 baby strollers , along with other long - form commentaries . The article can be found online at https : / / www . babygearlab . com / topics / getting - around / best - stroller . 20 Selenite : Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models B . 2 Dataset of Groundtruth Criteria vs Selenite - retrieved Criteria Topic Groundtruth Criteria Selenite - retrieved Criteria Best washing machines Cleaning performance * , Fabric care * , Water usage * , Smart features * , Durability * , Water Temperature Options * , Noise level * , Load type * , Odor control * , Ease - of - use * , Price * , Aesthetics * , Power source * , Speed & Cycle time * , Features * , Capacity * , Warranty & Customer support * , Size * , Brand reputation * ( n = 19 ) Capacityˆ , Energy efficiency , Water efficiencyˆ , Noise levelˆ , Durabilityˆ , Cleaning perfor - manceˆ , Cycle optionsˆ , Spin speedˆ , Priceˆ , Sizeˆ , Brand reputationˆ , User - friendliness / ease - of - useˆ , Warrantyˆ , Featuresˆ , Aestheticsˆ , Cycle timeˆ , Load typeˆ , Maintenance , Smart connectivityˆ , Fabric careˆ , Water Temperature Optionsˆ , Power sourceˆ , Child Safety Lock , Odor Controlˆ ( n = 24 ) Birthday gift ideas Practicality * , Aesthetics * , Experience * , Price * , Age appropriateness * , Prerequisite , Uniqueness * , Sentimentalvalue * , Personalization * , Gen - der appropriateness * , Shipping * ( n = 11 ) Personalizationˆ , Uniquenessˆ , Practicalityˆ , Sentimentalvalueˆ , Presentation ( wrapping ) , Priceˆ , Creativityˆ , Durability , Functionalityˆ , Brand , Age appropriatenessˆ , Gender ap - propriatenessˆ , Timeliness , Accessibility ( can get it in time ) ˆ , Social norms , Environment impact , Size , Experienceˆ , Aestheticˆ , Relevance , Surprise factor ( n = 21 ) Best hybrid app frame - works Programming Language , Code reusability * , Cross - platform com - patibility * , Development time * , Ease of learning * , Third - Party In - tegration * , Popularity * , Features * , Testing and debugging * , Plugin availability * , Native features access * , Performance * , User interface * , Framework size * , Community support * ( n = 15 ) Cross - platform compatibilityˆ , Performanceˆ , User interfaceˆ , Community supportˆ , Plu - gin availabilityˆ , Development timeˆ , Maintenance , Costˆ , Third - Party Integrationˆ , Secu - rityˆ , Scalabilityˆ , PlatformSupportˆ , Easeoflearningˆ , Codereusabilityˆ , Customizationˆ , Testing and debuggingˆ , App store compliance , Innovation , Popularity and Adoptionˆ , Framework sizeˆ , Native features accessˆ ( n = 21 ) Best time tracking tools Real - time tracking * , Editing ability * , Export and Invoice * , Accessi - bility * , Price * , OS platform * , Features * , Easy - of - use * , Tracking accu - racy * , Simplicity * , Extensibility * , Automation * , Privacy * , Learning curve * , Intrusiveness , Customization * , Interface * , Integration * , Focus work feature * , Project management * ( n = 20 ) User interface ( ease - of - use , navigation ) ˆ , Featuresˆ , Compatibilityˆ , Reportingˆ , Mobile accessˆ , Pricingˆ , Customer support , Customizationˆ , Security & privacyˆ , Time tracking accuracyˆ , Time tracking methods ( manual automatic timer ) ˆ , Project managementˆ , Team managementˆ , Invoicingˆ , Time off managementˆ , Analyticsˆ , User permissions , Offline tracking , Ease of setupˆ , Work typesˆ , Team sizes ( n = 21 ) Deep learning frame - works Programming language * , Data flow / graph structure * , Ease - of - use * , Support for product * , Support for research * , Visualization * , Release date , OSplatform * , Licensing * , Communitysupport * , Growingspeed ( community ) , Used - by , Documentation * , Extensibility * , Developed - by , Flexibility * , Scalability * , Speed / efficiency * , Performance * , Popu - larity * , Hardware support * , Supported network architecture * , Dis - tributed compute * , Dependency * , CUDA usage * ( n = 25 ) Ease of useˆ , Flexibilityˆ , Performanceˆ , Scalabilityˆ , Community supportˆ , Compatibiilty ( programminglamguage , hardware , OS , libraries ) ˆ , Modelaccuracy ( availablepre - trained models ) ˆ , Hardware supportˆ , Model interpretability ( suppport for model understand - ing ) , Model size , Development speed , Security , Licensingˆ , Data preprocessingˆ , Model architectureˆ , Documentationˆ , Innovation ( research ) ˆ , Deploymentˆ , Debugging and profilingˆ , Interoperability ( integrate the model with other systems and applications ) ˆ ( n = 20 ) Best sleeping bags Temperature rating * , Comfort * , Weight * , Packed size * , Filling ma - terial * , Outside material * , Durability * , Price * , Easy to clean , Space inside * , Shape * , Color * , Hooded or not * , Waterproofness * , Quality of zipper * , For extreme weather * , Easy setup , Length * , Warranty * ( n = 19 ) Temperature ratingˆ , Weightˆ , Insulation typeˆ , Comfortˆ , Ventilation , Size and fitˆ , Packabilityˆ , Durabilityˆ , Waterresistanceˆ , Zipperlocation , Zipperqualityˆ , Priceˆ , Hood and collar design and availbilityˆ , Versatility ( different environments and conditions ) ˆ , Brand reputation , Shape ( mummy , rectangular , or semi - rectangular ) ˆ , Lengthˆ , Shell materialˆ , Lining material , Warrantyˆ , Color optionsˆ ( n = 21 ) Best air purifiers Ease of use * , Coverage * , Noise * , Price * , Air quality indicator * , Speed settings * , Portability * , Design * , Smart features * , Warranty * , Cus - tomer reviews * , Customer support * , Filter type * , Sleep mode * , Re - movesmell * , Powerconsumption * , Performance * , Ozonesafe * , Main - tenance requirements * , Filter replacement indicator * ( n = 20 ) Filter typeˆ , Coverage areaˆ , CADR ( Clean Air Delivery Rate ) , Fan speed settingsˆ , Noise levelˆ , Energy efficiencyˆ , Additional features ( such as ionizer , air quality sensors , auto mode , or remote control ) ˆ , Designˆ , Size , Priceˆ , Brand reputation and customer reviewsˆ , effectiveness ( dust , pollen , smoke , pet dander , and mold spores ) ˆ , Maintenance and filter replacementˆ , Odor eliminationˆ , Smart featuresˆ , Filter indicatorˆ , Warrantyˆ , User - friendly controlsˆ , Portabilityˆ , Ozone emissions ( high levels can be harmful to health ) ˆ , Customer supportˆ , Sleep modeˆ , Child lock , Certifications ( n = 24 ) Best robot vacuums Performance * , Pet hair handling * , Battery life * , Reliability * , Ease of use * , Price * , Design * , Functionality * , Dust bin size * , Customer support * , Suction power * , Ability to handle different floor types * , Noise level * , Compactness * , Navigation capabilities * , Maintenance requirements * , Auto - recharge * , Smart home compatibility * , Sched - uling capabilities * , Boundary control features * , Voice control * , Effec - tiveness in corners and edges * , Automatic dirt disposal * ( n = 23 ) Cleaning performanceˆ , Navigation and mappingˆ , Battery life and chargingˆ , Noise levelˆ , Dustbin capacityˆ , Smart features and connectivityˆ , Edge cleaning and corner reachˆ , Maintenance and filter replacementˆ , Price and value for moneyˆ , Customer reviews and ratings , Runtime and automatic dockingˆ , Scheduling and automationˆ , Filtration system ( the efficiency of the vacuum’s filtration system in capturing small particles and allergens ) , Suction powerˆ , Size and designˆ , Multi - floor cleaningˆ , Virtual walls and boundary settingˆ , Pet hair performanceˆ , Durability and reliabilityˆ , Brand reputation and customer supportˆ , Accessories and additional featuresˆ , User - friendly interface & controlˆ , Warranty and after - sales service , Mopping capabilities , Remote control options ( e . g . , voice ) ˆ , Energy efficiency , Auto - emptying capabilitiesˆ , Integration with smart home systemsˆ ( n = 28 ) Best baby strollers Safety features * , Comfort for baby * , Durability * , Foldability * , Steer and maneuver * , Storage space * , Color options * , For twins * , Com - patibility with car seats * , Customer support * , Reviews * , Ease of cleaning * , Expandability for growing child * , Suspension system * , Terrain adaptability * , Included accessories ( like cup holders , trays , etc . ) * , Canopy and weather protection * , Height and angle of han - dlebars * , Reversible seat * , Price * , Weight and size * , Lockable swivel wheels * ( n = 22 ) Safety ( secure harness , sturdy construction , and reliable brakes ) ˆ , Comfortˆ , Maneu - verabilityˆ , Durabilityˆ , Storageˆ , Folding mechanism and abilityˆ , Weight and sizeˆ , Versatility ( terrain ) ˆ , Priceˆ , Ease of cleaning & maintenanceˆ , Design and Styleˆ , Suspen - sionˆ , Canopy & UV and weather protectionˆ , Reversible seatˆ , Adjustable handlebar ( to accommodate parents of different heights and provide comfortable pushing ) ˆ , Brake & locking system ( ensure the stroller stays in place when needed ) ˆ , Travel system compat - ibilityˆ , Accessoriesˆ , Customer reviews and ratingsˆ , Adjustablility ( to accommodate babygrowth ) ˆ , Adjustableseatheight ( toaccommodatedifferenttableheights ) , Warranty & customer supportˆ , Easy assembly , Accommodation for twinsˆ ( n = 24 ) Best tropical vacation spots Weather * , Cost * , Beach quality * , Cultrual experiences * , Outdoor ac - tivities * , Ease of travel * , Hotel * , Scenery * , Child - friendly * , Shopping opportunities * , Undertanding of language * , Food * , LGBTQ + friendly , Bars and clubs * , Safety * ( n = 15 ) Weatherˆ , Beachesˆ , Activities & Adventureopportunitiesˆ , Naturalbeautyˆ , Accommoda - tionˆ , Safetyˆ , Transportation & Accessibility ( Ease of travel and transportation options ) ˆ , Culture & historyˆ , Dining optionsˆ , Cost & Value for moneyˆ , Nightlife ( bars , clubs , or live music venues ) ˆ , Reviews and recommendations , Family - friendlyˆ , Wildlife , Local hospitality , Shoppingˆ , Sustainability ( Commitment to eco - tourism practices , conserva - tion efforts , and protection of the environment ) , Communication & Language barrierˆ , Accessibility for people with disabilities ( n = 19 ) Table 6 : Dataset of Groundtruth Criteria vs Selenite - retrieved Criteria . Groundtruth criteria that are retrieved by Selenite are marked with a “ * ” , while Selenite - retrieved criteria that appear in the Groundtruth set are marked with a “ˆ” . 21 Liu and Wu et al . C TOPICS EXPLORED IN STUDY 3 Participant Topics Participant Topics P1 Choosing a high - quality digital camera P5 Picking the best robot vacuum Choosing the best air purifier Picking a drone Picking a suitable hand truck Choosing the best e - reader P2 Selecting the best washing machine P6 Picking the right refrigerator Choosing a reliable VPN provider Choosing the best barbecue grill Deciding on unique thank - you gifts Choosing the best tropical vacation location P3 Selecting the best mattress P7 Choosing an effective diet plan Choosing the best city bike Choosing a breed of dog to adopt Choosing a birthday gift Selecting a suitable SUV P4 Selecting a secure password manager P8 Picking a reliable treadmill Choosing the best tropical vacation location Picking the right wedding venue Choosing a hybrid app framework Choosing the best skiing venue Table 7 : Topics that participants in study 3 explored . D GPT - 4 PROMPTS USED IN SELENITE Here , we outline the techniques we employed to guide the GPT - 4 model , developed by OpenAI [ 80 ] , within the context of Selenite . If not explicitly stated , the temperature of the model is set to 0 . 3 for a balance between consistency and creativity . If not explicitly stated , the initial [ System Message ] 17 was set as the following : [ System Message ] You are a helpful assistant that performs content analysis according to user requests . Follow the user’s requirements carefully and to the letter . D . 1 Obtaining topic from a web page The prompt that we used to obtain a concise topic given a web page ( with [ title ] and [ content of the first few paragraphs ] ) is a two - step prompt : [ User Message ] Step 1 Given the following information of an article : Title : [ title ] First few paragraphs : [ content of the first few paragraphs ] What is this article about ? [ Assistant Message ] This article is about . . . [ User Message ] Step 2 I want to find articles similar to this one in terms of the general topic . What should I search for ? Output one search phrase ( in double quotes ) . Additionally , we set the n parameter to 10 , thereby instructing GPT - 4 to produce 10 simultaneous responses . Subsequently , we determined the most commonly occurring one among these 10 as the topic for the article . While it could be assumed that a higher value for n would result in a lengthier response time from the model , our observations indicate that such delay is practically insignificant . D . 2 Obtaining options from a web page The prompt that we used to obtain extract options from a given web page ( with [ title ] and [ content of the web page ] ) is a two - step prompt : 17 Thesystemmessagehelpssetthebehaviorofthemodelresponse , i . e . , the [ Assistant Message ] . However , asstatedbyOpenAI : “ . . . notethatthesystemmessageisoptionalandthe model’sbehaviorwithoutasystemmessageislikelytobesimilartousingagenericmessagesuchas‘Youareahelpfulassistant’ . . . ” ( https : / / platform . openai . com / docs / guides / gpt / chat - completions - api ) 22 Selenite : Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models [ User Message ] Step 1 Given the following information of an article : Title : [ title ] Is the article likely to be discussing one or more aspects of " one specific option " ( e . g . , a single javascript framework , for example , React , or a single baby stroller option , or a specific Airbnb listing ) or " multiple options / topics " ? Output in the following format : Reasoning : your reasoning process . Verdict : " one specific option / multiple options " [ Assistant Message ] Reasoning : model’s reasoning process . . . Verdict : " one specific option / multiple options " [ User Message ] Step 2 Now , given the content of the article below , what is / are the options ? Content : [ content of the web page ] Output should be in the following format : [ " option _ 1 " , " option _ 2 " , . . . ] D . 3 Obtaining commonly - considered criteria from a web page The prompt that we used to obtain a set of commonly considered criteria resembles something like the following ( given a [ topic ] ) : [ User Message ] Step 1 : Ask for an initial set of criteria What are some common aspects , criteria , or dimensions that people consider on the topic of [ topic ] ? Note that the criteria should be * * most relevant to the topic * * , * * frequently considered * * , and can * * cover a broad range of perspectives * * . Output should be a single bulleted list in the format of : - Criterion : short description . Do not output anything else . [ Assistant Message ] - [ Criterion 1 ] : [ Short description ] - [ Criterion 2 ] : [ Short description ] - [ Criterion 3 ] : [ Short description ] . . . [ User Message ] Step 2 + : Ask for additional criteria until we get around 20 . Give me five more that are different from , more diverse than , and possibly as important as the ones listed above . Output in the same format . D . 4 Obtaining detailed analysis of text content The prompts that we used to obtain a detailed analysis of text content given the [ text content ] , the list of NLI criteria , and the list of [ options ] on the corresponding web page is two - fold : First , we ask GPT - 4 to extract phrases from the content that describes a given criterion as well as determine each extracted phrase’s sentiment with respect to the criterion : [ User Message ] 23 Liu and Wu et al . Given the following * * content * * and list of * * criteria * * : * * Content * * : [ content ] * * Criteria ( with definitions ) * * : - [ NLI Criterion 1 ] : [ description ] - [ NLI Criterion 2 ] : [ description ] . . . For each criterion : 1 ) extract * * every possible * * utterance that * * mentions * * or * * explicitly describes * * that criterion from the content 2 ) perform sentiment analysis to determine if the utterance is " positive " , " neutral " , or " negative " with respect to that criterion . Remember to use the * * exact same words * * from the content . Do not paraphrase ! Output must follow the format below : # # criterion _ 1 _ name - " extracted _ sentence _ or _ phrase _ 1 " - > positive , - " extracted _ sentence _ or _ phrase _ 2 " - > neutral , # # criterion _ 2 _ name NONE FOUND # # criterion _ 3 _ name - " extracted _ sentence _ or _ phrase _ 1 " - > neutral , - " extracted _ sentence _ or _ phrase _ 2 " - > negative , - " extracted _ sentence _ or _ phrase _ 3 " - > positive , Second , we ask GPT - 4 to label each extracted phrase with a possible [ option ] on the web page ( we framed options as “subjects” of a phrase to achieve a better empirical performance ) : [ User Message ] Given the following * * content * * and the * * phrases * * extracted from the content below : * * Content * * : [ content ] * * Extracted phrases * * : - " extracted _ phrase _ 1 " - " extracted _ phrase _ 2 " - " extracted _ phrase _ 3 " . . . For each phrase , determine the * * subject * * of the phrase based on the * * content * * . Possible subjects are : [ option _ 1 , option _ 2 , option _ 3 , . . . ] Say " N / A " if you cannot determine the subject . Output should be in the following format : " extracted phrase 1 " - > " subject " or " N / A " " extracted phrase 2 " - > " subject " or " N / A " . . . 24 Selenite : Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models E OTHER IMPLEMENTATION DETAILS We leverage GPT - 4 for several use cases in Selenite , and encountered several challenges 18 : First , due to the limited context window size of GPT - 4 ( 8192 tokens or approximately 6100 English words ) , we occasionally need to divide the entire text content of a web page into smaller chunks and run parallel queries to extract options . As of July 2023 , we don’t have access to the version of GPT - 4 with a 32k context window , which would significantly reduce the need for chunking and parallel queries . Second , unfortunately , there are occasions when the GPT - 4 model becomes overloaded with requests or takes an exceptionally long time to respond . To mitigate these problems and provide uninterrupted user experience to Selenite users , we have employed the following two approaches : 1 ) Dual API requests : We send two identical requests using separate API keys simultaneously . We prioritize the response that returns first with valid information , indicating that it is not an error and contains the requested information from the prompt ; 2 ) Graceful error handling & retry : In the event of an error , we introduce a random delay ( ranging from 1 to 5 seconds ) before retrying the request . We repeat this retry process for up to 5 attempts , allowing sufficient opportunity for a successful response . Note that these issues are attributable , in part , to the current limited beta status of GPT - 4 . Consequently , it is uncertain whether these issues will persist in the future . Nevertheless , we delve into them here to provide a comprehensive and accurate accounting of our experience interacting with the API . To efficiently perform natural language inference ( NLI ) during the analysis of article content to produce per - paragraph summaries and annotations of options and criteria , we experimented with both the roberta - large - mnli and bart - large - mnli models that are fine - tuned for multi - genre natural language inference ( MNLI ) tasks 19 , and ended up using the latter for its better performance in our informal testing . In addition , we implemented a REST API service that the Chrome extension can query on demand . To decrease model inference time and ensure a smooth user experience , we ran the service on multiple Google Cloud virtual machines with NVIDIA L4 GPUs . 18 We have documented the specific prompt designs for those tasks in section D , and only discuss a series of challenges we experienced while interacting with the GPT - 4 API here in this section . 19 These two models are considered to be able to achieve state - of - the - art performance as of June 2023 . 25