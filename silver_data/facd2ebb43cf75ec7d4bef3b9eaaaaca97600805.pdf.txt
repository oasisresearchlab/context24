Summary : SLINK : An optimally eﬃcient algorithm for the single - link cluster method , by R Sibson ( King’s College Research Centre , King’s College , Cam - bridge and Cambridge University Statistical Laboratory when paper was writ - ten in 1972 ) Main point Sibson gives an O ( n 2 ) algorithm for single - linkage clustering , and proves that this algorithm achieves the theoretically optimal lower time bound for obtaining a single - linkage dendrogram . This improves upon the naive O ( n 3 ) implementation of single linkage clustering . A single linkage dendrogram is a tree , where each level of the tree corresponds to a diﬀerent threshold dissimilarity measure h . The nodes of a dataset are grouped into “equivalence classes” c ( h ) at each level of the dendrogram , where two classes C i and C j are merged if there is a pair of “OTU’s” ( vertices ) v i ∈ C i and v j ∈ C j such that the dissimilarity measure between v i and v j is less than h , or D ( v i , v j ) < h . For example , consider a set of 10 vertices v 1 , . . . , v 10 for which the dissimilarity matrix D is given below , with D ij equal to the dissimilarity between v i and v j . D =   0 1 . 2 5 5 4 . 2 7 9 7 . 6 11 4 . 3 1 . 2 0 3 . 4 4 . 1 5 6 4 . 1 6 . 4 5 . 3 4 . 5 5 3 . 4 0 2 . 1 6 6 . 2 4 . 6 9 11 . 3 22 5 4 . 1 2 . 1 0 11 5 13 4 . 1 4 . 3 5 . 5 4 . 2 5 6 11 0 1 . 9 7 9 5 . 5 4 . 3 7 6 6 . 2 5 1 . 9 0 7 . 5 5 . 6 6 . 3 4 . 5 9 4 . 1 4 . 6 13 7 7 . 5 0 3 . 6 8 10 7 . 6 6 . 4 9 4 . 1 9 5 . 6 3 . 6 0 4 . 9 2 . 9 11 5 . 3 11 . 3 4 . 3 5 . 5 6 . 3 8 4 . 9 0 1 . 4 4 . 3 4 . 5 22 5 . 5 4 . 3 4 . 5 10 2 . 9 1 . 4 0   Suppose we take four cut - oﬀ dissimilarity measures h 1 , h 2 , h 3 , h 4 and produce the dendrogram according to these thresholds . An example illustrating how the 10 vertices are grouped into equivalence classes at each level is shown in Figure 1 . Since no dissimilarity is at or below 1 , each vertex or “OTU” is its own equiv - alence class at the level corresponding to h 1 = 1 . At the next level , however , we see that some classes have been merged together because several dissimilarity measures are below h 2 = 2 . We can see that c ( h 2 ) consists of 6 equivalence classes , c ( h 3 ) has 3 equivalence classes , and c ( h 4 = 4 ) aggregates all the vertices into one equivalence class . In single linkage clustering , the number of levels in the tree is determined by the nearest - neighbor criterion – at each level , at least one new merge is made between two clusters , and the merge is made for clusters C i and C j if the minimal distance between vertices v i ∈ C i and v j ∈ C j is the smallest such distance across all the clusters . In other words , the nearest neigh - bors between clusters C j and C i are found , and if these neighbors are closer than all the other nearest - neighbor pairs , then C i and C j are merged . A single 1 𝑂𝑇𝑈 ′ 𝑠 = operational taxonomic unit , i . e . individual objects ( here there are 𝑃 = 10 of them ) 𝑐 ℎ 1 = 1 = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 } 𝑐 ℎ 2 = 2 = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 } 𝑐 ℎ 3 = 3 = { 1 , 2 , ( 3 , 4 , 5 , 6 , 7 ) , ( 8 , 9 , 10 ) } 𝑐 ℎ 4 = 4 = { ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) } ℎ 1 < ℎ 2 < ℎ 3 < ℎ 4 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 . 2 1 . 9 1 . 5 1 . 4 1 . 2 1 . 9 1 . 5 1 . 4 2 . 1 2 . 6 2 . 9 1 . 2 1 . 9 1 . 5 1 . 4 2 . 1 2 . 6 2 . 9 3 . 4 3 . 6 Figure 1 : An example of the equivalence classes c ( h ) formed at each level of the dendrogram with cutoﬀ thresholds h . linkage dendrogram for the dissimilarity matrix in the example is shown in Fig - ure 2 . A naive algorithm constructing this dendrogram would proceed as follows : 1 ) Construct the dissimilarity matrix D ( O ( n 2 ) ) , and initialize each cluster to contain a single vertex , set the level in the dendrogram to i = 0 2 ) Increment i , ﬁnd the smallest dissimilarities between two clusters O ( n 2 ) , and merge the clusters together . Update D to contain one less row and column than before , corresponding to this merge . In the example above , after merging vertices 1 and 2 we would have : D =   0 3 . 4 4 . 1 4 . 2 6 4 . 1 6 . 4 5 . 3 4 . 3 3 . 4 0 2 . 1 6 6 . 2 4 . 6 9 11 . 3 22 4 . 1 2 . 1 0 11 5 13 4 . 1 4 . 3 5 . 5 4 . 2 6 11 0 1 . 9 7 9 5 . 5 4 . 3 6 6 . 2 5 1 . 9 0 7 . 5 5 . 6 6 . 3 4 . 5 4 . 1 4 . 6 13 7 7 . 5 0 3 . 6 8 10 6 . 4 9 4 . 1 9 5 . 6 3 . 6 0 4 . 9 2 . 9 5 . 3 11 . 3 4 . 3 5 . 5 6 . 3 8 4 . 9 0 1 . 4 4 . 3 22 5 . 5 4 . 3 4 . 5 10 2 . 9 1 . 4 0   Notice that D is now a 9 × 9 matrix , and the distance from cluster 2 1 2 3 4 5 6 7 8 9 10 ℎ 1 = 1 . 2 ℎ 2 = 1 . 4 ℎ 3 = 1 . 5 ℎ 4 = 1 . 9 ℎ 5 = 2 . 1 ℎ 6 = 2 . 6 ℎ 7 = 2 . 9 ℎ 8 = 3 . 4 ℎ 9 = 3 . 6 Figure 2 : An example of a single linkage dendrogram , where each level cor - responds to a threshold h that is the minimum nearest - neighbor link between clusters at the previous level . C 1 = { v 1 , v 2 } to the other vertices is given in the ﬁrst row and col - umn of the new D . These distances are simply the minima of the dis - tances of v 1 and v 2 to the other vertices . For instance , d ( { v 1 , v 2 } , v 10 ) = min ( d ( v 2 , v 10 ) , d ( v 1 , v 10 ) ) = 4 . 3 . Level i of the dendrogram contains the newly merged cluster and everything below it as equivalence classes . In the example , level 1 contains the 9 clusters or equivalence classes : { 1 , 2 } , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 . The threshold h i at this level is the smallest distance just found , so in the example h 1 = 1 . 2 . 3 ) delete the row and column corresponding to each of these clusters , and add a new row and column corresponding to the new , merged cluster ; 4 ) repeat steps ( 2 ) and ( 3 ) until all the vertices are merged into one cluster As written , the naive single linkage algorithm takes O ( n 3 ) time , because at each of O ( n ) iterations we take O ( n 2 ) time to ﬁnd the smallest nearest - neighbor link . Sibson realized that we don’t need to do this at each stage , because all O ( n 2 ) dissimilarities are already computed when we computed the dissimilarity matrix – therefore , at each step of SLINK , we simply maintain an array of size O ( n ) that tells us the nearest neighbor of each vertex and the distance of that neighbor . While computing the dissimilarity matrix , we can also compute the 3 nearest neighbor of each vertex and store it in an array A V and its corresponding distance in an array A D . After merging two clusters C i and C j , where i < j and C i is indexed by i , into C ij , we update the dissimilarity matrix the same way as in the naive algorithm . We then update A D ( i ) to be the new smallest distance to C ij , which is found by taking the minimum D ik and D jk for all k (cid:54) = i , j , taking O ( n ) time , and set A V ( i ) to the cluster index corresponding to this distance . Set A D ( j ) to be ∞ . For all other clusters C k , if A V ( C k ) = i or A V ( C k ) = j , update this entry to A V ( C k ) = i , otherwise leave A V ( C k ) as it was . Thus , instead of searching for the smallest dissimilarity in each step in the updated dissimilarity matrix , we instead take O ( n ) time to ﬁnd it in A D and merge the corresponding clusters that are stored in A V . For the example above , we would start with : A D = (cid:2) 1 . 2 1 . 2 2 . 1 1 . 9 1 . 5 1 . 5 2 . 9 1 . 4 1 . 4 (cid:3) A V = (cid:2) 2 1 4 3 6 7 6 10 10 9 (cid:3) In our example , after merging vertices v 1 and v 2 , we would get the following : A D = (cid:2) 3 . 4 ∞ 2 . 1 1 . 9 1 . 5 1 . 5 2 . 9 1 . 4 1 . 4 (cid:3) A V = (cid:2) 3 1 4 3 6 7 6 10 10 9 (cid:3) We proceed in this way , at each step saving the equivalence classes that make up the dendrogram . Thus , each iteration takes O ( n ) time as opposed to O ( n 2 ) , giving a time complexity of O ( n 2 ) for SLINK . 4