Crowdsourcing Multi - Label Classiﬁcation for Taxonomy Creation Jonathan Bragg Mausam Daniel S . Weld Department of Computer Science and Engineering University of Washington Seattle , WA 98195 { jbragg , mausam , weld } @ cs . washington . edu Abstract Recent work has introduced C ASCADE , an algorithm for cre - ating a globally - consistent taxonomy by crowdsourcing mi - crowork from many individuals , each of whom may see only a tiny fraction of the data ( Chilton et al . 2013 ) . While C ASCADE needs only unskilled labor and produces tax - onomies whose quality approaches that of human experts , it uses signiﬁcantly more labor than experts . This paper presents D ELUGE , an improved workﬂow that produces tax - onomies with comparable quality but demands signiﬁcantly less crowd labor . Our proposed method for solving the novel problem of crowdsourcing multi - label classiﬁcation optimizes C ASCADE ’s categorization step—its most costly step—using less than 10 % of the labor required by the orig - inal approach . D ELUGE ’s savings come from the use of de - cision theory , machine learning , and probabilistic inference , which allow it to pose microtasks that aim to maximize infor - mation gain . Introduction The presence of large amounts of data is both a boon and bane of modern times . Large datasets enable many novel applications , for example , supervised machine learning and data mining . On the other hand , organizing these datasets for easy access and better understanding requires signiﬁcant human effort . One such organization practice involves con - structing a taxonomy , a hierarchy of categories where each edge denotes an isA relationship ( e . g . , president isA per - son ) . Instances of each category ( items ) are associated with the appropriate node in the taxonomy , for example , the en - tity Barack Obama is an instance of the president category . Wordnet ( Miller 1991 ) and the Linnaean taxonomy are in - ﬂuential examples . Taxonomizing large datasets and maintaining a taxonomy over time raise signiﬁcant challenges , since they are a drain on the ontologist ( s ) responsible for these tasks . A promis - ing answer to this challenge was recently proposed : a dis - tributed crowdsourcing workﬂow , called C ASCADE ( Chilton et al . 2013 ) . C ASCADE provides a sequence of steps for generating a taxonomy from scratch and for taxonomizing a new item by asking multiple choice questions from un - skilled workers on a labor market , such as Amazon Mechan - Copyright c (cid:13) 2013 , Association for the Advancement of Artiﬁcial Intelligence ( www . aaai . org ) . All rights reserved . ical Turk . Unfortunately , the C ASCADE workﬂow was not optimized and is proﬂigate in its use of workers . While the overall cost of a C ASCADE - produced taxonomy is compara - ble to one produced by an expert , C ASCADE requires about six times as much labor . This suggests that one might be able to reﬁne the workﬂow , making taxonomy creation both inexpensive and low latency . Toward this end , we propose D ELUGE , a decision - theoretic reﬁnement of C ASCADE . D ELUGE adopts the high - level skeleton of C ASCADE ’s workﬂow , but optimizes its most expensive steps : elucidation of category labels and assignment of those labels to data items . The latter step is an example of a multi - label classiﬁcation problem , an im - portant class of problems which has not been previously op - timized in a crowdsourcing setting . Where C ASCADE gen - erates a large number of human tasks for each item - label pair , D ELUGE saves by ordering the tasks intelligently using the evolving structure of the taxonomy , as well as a learned model of label probabilities and co - occurrence probabilities . In summary , our paper presents the following primary contributions : • We describe a decision - theoretic alternative for C ASCADE ’s category label elucidation phase , which allows D ELUGE to estimate the marginal value of asking workers to propose new category labels in terms of the expected number of new labels that would be generated . • We present an efﬁcient solution to the novel problem of crowdsourcing multi - label classiﬁcation . We describe several alternative methods , culminating in a decision - theoretic approach with two components : ( 1 ) a noisy worker model that allows a worker to be probabilistically accurate and estimates the true value of item - label rela - tionships based on probabilistic inference , and ( 2 ) a con - troller that chooses , for each item , which questions pro - vide the maximum value of information toward a joint categorization . • We provide theoretical guarantees for the optimality of our control strategy , as well as an efﬁcient method for se - lecting batches of labels that makes our approach imme - diately usable in an online labor market environment . • We conduct live experiments on Mechanical Turk show - ing that our best combination of policies requires less than 10 % of the labor used by C ASCADE for the categorization step . Beyond reducing the cost of crowdsourced taxonomies , our work on D ELUGE shows that artiﬁcial intelligence and decision - theoretic techniques can be applied to much more complex workﬂows than the simple iterative improve - ment ( Dai , Mausam , and Weld 2010 ; 2011 ) and consen - sus task ( Wauthier and Jordan 2011 ; Kamar , Hacker , and Horvitz 2012 ) workﬂows previously tackled . Basic Taxonomy Algorithm Both C ASCADE and our reﬁnement , D ELUGE , take as input a set of items to be categorized , such as photographs or text snippets . Their output is a tree whose interior nodes are each labeled with a text string label ( category ) ; see Figure 4 for an example . Our taxonomy creation algorithms use a mixture of algo - rithmic steps and three task schemata , which are submitted to human workers in the labor market . From a functional perspective , these tasks may be deﬁned as follows : • Generate ( t items ) → t labels : Displays t items and asks a worker to suggest a label for each item . • SelectBest ( 1 item , c labels ) → 1 label : Presents a worker with a single item and c different labels and asks her to pick the best one . • Categorize ( 1 item , s labels ) → bit vector of size s : Shows a worker a single item and s labels and asks him to indicate which labels apply to the item . ( See Figure 2 for our user interface corresponding to an instance of this task . ) Since humans are the bottleneck in this approach to tax - onomy creation , we seek to minimize the number of tasks requested from the labor market . At the highest level , both C ASCADE and our derivatives start by using Generate tasks to brainstorm a set of candidate category labels . They then use SelectBest tasks to ﬁlter out poor labels . After - wards , Categorize tasks identify appropriate labels for all items . A ﬁnal , purely algorithmic step , called global structure inference , builds a hierarchy from this data by inducing a parent - child relationship between two labels when most of the items in one label are also in the other . Labels with too few items are eliminated , and labels with too great an overlap are merged . In this paper , we make no changes to C ASCADE ’s approach to this ﬁnal step ; see ( Chilton et al . 2013 ) for details . Figure 1 summarizes this high - level algorithm , but does not specify exactly how the set of category labels should be elucidated , nor does it state how to efﬁciently catego - rize each item using a ﬁxed set of category labels . We dis - cuss these issues in the next two subsections . As we shall see , C ASCADE takes a relatively simple approach to these questions , but more sophisticated techniques can greatly decrease the amount of human labor required . Likewise , C ASCADE terminates when the number of uncategorized items is below a ﬁxed threshold , while D ELUGE uses a decision - theoretic criterion . Elucidating Category Labels Noting that there would likely be wasteful duplication if one asked humans ( via a Generate task ) to brainstorm candidate category labels for every one of the Items , C ASCADE ’s im - plementation of ElucidateLabels starts by considering only the ﬁrst few ( m = 32 ) items , termed the initial item set . C ASCADE partitions this initial item set into groups of t = 8 and creates a Generate task for each , which is sent k = 5 workers . After all (cid:100) km / t (cid:101) tasks are completed , C ASCADE is left with km candidate category labels , not necessarily distinct . C ASCADE ’s next step is to prune the candidate labels . At this point , each of the m initial items will have up to k distinct suggested category labels , so for each item C ASCADE submits k SelectBest tasks requesting a hu - man to choose which of the labels seems most appropriate . Any labels with two or more votes are retained , so after this step p ≤ 2 m distinct labels remain ( assuming k = 5 ) . In the next section we use a combinatorial balls - and - urns model to describe an alternative , decision - theoretic method for controlling category elucidation . Categorizing Items Once Labels are Known Once the category labels have been elucidated , C ASCADE enters its most costly phase , which incurs O ( np ) worker tasks , where n = | Items | and p = | Labels | . Intuitively , the idea is to iterate through the items and labels asking k different workers whether a label applies to an item . Chilton et al . observed that workers sometimes lack the context to make these decisions so they proposed categorizing in two sequential phases , which they term adaptive context ﬁlter - ing . The ﬁrst phase iterates through items and labels as de - scribed above ; every label which receives at least two ( out of ﬁve ) votes progresses to the next phase . In the second phase , workers are only shown labels which made the ﬁrst round cut and a label is considered to ﬁt an item if at least four of the ﬁve workers deem it so . Thus , both phases to - gether use between (cid:100) knp / s (cid:101) and 2 (cid:100) knp / s (cid:101) worker tasks . In two sections , we present several improved algorithms for performing this categorization process . The ﬁrst gener - ates precisely the same labeling per phase with strictly fewer worker tasks . The second uses substantially fewer workers , but produces a slightly different taxonomy . The ﬁnal ap - proaches incrementally build probabilistic models of cate - gory occurrence and co - occurrence , which they use to opti - mize the order in which they pose questions to workers . P´olya’s Urn for Label Elucidation C ASCADE ’s category label elucidation step asks workers to brainstorm relevant categories to be added to the taxonomy . C ASCADE performs this step on a set of m items , where m (cid:28) n , the total number of items to be categorized . The key insight behind elucidating labels for a small number of items is that labels generated for a random subset of items can be globally relevant , and that workers are likely to repeat labels Procedure BuildTaxonomy ( Items ) : ItemLabelMatrix : = [ ] While TaxonomyNeedsImproving ? ( ItemLabelMatrix ) Do Labels : = ElucidateLabels ( the subset of Items with no label ) For each Item in Items Do ItemLabelMatrix : = Categorize ( Item , Labels , ItemLabelMatrix ) Taxonomy = GlobalStructureInference ( ItemLabelMatrix ) Return Taxonomy Figure 1 : The general taxonomy creation algorithm . C ASCADE and D ELUGE differ in their termination conditions and their implementations of ElucidateLabels and Categorize . across items . An important control question for optimizing this step involves the choice of m . C ASCADE sets m = 32 in an ad hoc manner , but ideally we would like to estimate the quality of a set of category labels as it grows in order to determine when elucidating more labels would likely be wasteful . D ELUGE proposes modeling the brainstorming process using a P ´ olya urn model ( Johnson and Kotz 1977 ) . A very general framework , P ´ olya urn models are particularly suited for modeling discrete , multi - label distributions where the number of labels is unknown a priori , as is the case for our category labels . The metaphor for this generative model is that of an urn containing colored balls , where colors corre - spond to labels . In each iteration , a ball is drawn uniformly from the urn and then placed back in the urn along with a new ball . If the drawn ball is black ( a specially - designated color ) , the new ball is a previously unseen color ; otherwise , the new ball is the same color as the drawn ball . As balls are drawn from the urn , the number of colors in the urn increases but the probability of obtaining a new color decreases . Moreover , colors that are drawn frequently have a higher probability of being drawn than other colors . This be - havior can be seen from the probabilities that govern draws from the urn . Suppose that there are n c balls of color c , N non - black balls , and α black balls . Then , the probability of drawing a ball of color c is n c / ( N + α ) and the probability of drawing a previously unseen color is α / ( N + α ) . A P ´ olya urn model is parameterized by α , which denotes the number of black balls in the urn . Larger values of α imply higher probability of brainstorming new category la - bels . We note that our urn formulation is equivalent to the Chinese Restaurant Process , which computes similar proba - bilities with a different metaphor . A useful quantity to estimate for determining a stopping condition is the expected number of new labels that would be generated by a ﬁxed number of future worker tasks . Theorem 1 Let our P´olya urn contain N colored balls and α black balls . Let the random variable X d be the number of new colors present in the urn after d future draws . Then , E [ X d ] = d − 1 (cid:88) i = 0 α N + α + i . Recall that C ASCADE asks k workers to brainstorm cate - gories for each item . Thus , if we have generated labels for m items , with n − m = r items remaining , we have N = km and d = kr . Terminating the label elucidation phase at this point will result in an expected (cid:80) kr − 1 i = 0 α km + α + i missed la - bels . The expected fractional increase in the total number of labels is this quantity divided by the number of distinct labels seen after the ﬁrst m items . Our model provides a principled stopping condition for this phase : terminate when the expected fractional increase in the number of labels is below a desired threshold . In or - der to operationalize this policy , we compute the maximum - likelihood estimate of α using gradient ascent on the log - likelihood of generating the observed data . A reader may observe that this model assumes that all cat - egories are independent and that workers are equally likely to generate new labels for any particular item . These as - sumptions are inaccurate due to the underlying label co - occurrence probabilities , as well as potential differences in the number of accessible labels for each item . However , these approximations are reasonable for the brainstorming phase , since we will likely not have enough data to learn parameters for category co - occurrences or individual items , even if we modeled them . Our model lets us estimate the ap - proximate impact of stopping , which can be used to identify an appropriate termination point for this phase . Improved Categorization Control Algorithms C ASCADE , like many crowdsourcing workﬂows , imple - ments voting on binary outcomes by requesting a ﬁxed num - ber of votes k and setting a threshold number of votes T ( majority voting is the special case where T = k / 2 ) . Once the requested number of votes is returned , this procedure returns a positive outcome if and only if the number of posi - tive votes is at least T . The amount of work required by this procedure can be quite large , especially when attempting to scale workﬂows . In the Adaptive Context Filtering step of its workﬂow , C ASCADE asks k workers to vote on each item and label combination . Supposing there are n items and p labels , this step requires asking for O ( knp ) votes . A Lossless Improvement to Threshold Voting The ﬁrst observation we make is that given a threshold num - ber of votes T , asking for all k votes is unnecessary . Once one has received T positive votes , or k − T + 1 negative votes , one need not ask for further votes since the answer using k total votes is fully determined to be positive in the former case and negative in the latter case . We call this stopping condition lossless stopping ; it can be seen as a generaliza - tion of the “Ask two people to vote and only ask a third if the ﬁrst two disagree” policy in TurKit ( Little et al . 2009 ) . One - Away Approximation for Threshold Voting One can further reduce the number of votes required with a simple heuristic method , which we call the one - away heuristic , that we hypothesized would result in only a small amount of error compared to the original threshold voting . ( Note that lossless stopping results in no error , compared to the original . ) The one - away heuristic returns true early if we observe max { T − 1 , 0 } positive votes and no negative votes , or returns false early if we observe max { k − T , 1 } negative votes and no positive votes . The intuition behind this heuristic is that although the lossless stopping condition may not have been met , we have observed strong evidence for returning an outcome and no evidence in support of the alternative outcome . A Simple Probabilistic Model Another way to approach the problem is the Bayesian per - spective . Suppose we have already labeled a large num - ber of items , I ∈ I , and hence know for each I if la - bel L holds , denoted ⊕ ( I , L ) = 1 , or does not , denoted ⊕ ( I , L ) = 0 . Now , when given a new item I (cid:48) we know nothing about , we can use the previously observed data to calculate the maximum likelihood prior probability of any label P ( ⊕ ( I (cid:48) , L ) ) = (cid:80) I ∈I ⊕ ( I , L ) / | I | . In order to update our posterior for ⊕ ( I (cid:48) , L ) after ob - serving a worker’s vote , we must consider a noisy worker model . Prior work 1 has used a single - parameter accuracy model , but we use a two - parameter model that distinguishes between accuracy of detecting true positives and true nega - tives , which we term a worker’s sensitivity and speciﬁcity , respectively . We observe that worker speciﬁcity is much higher than worker sensitivity ( our data contains mostly true negatives ) , and that incorporating an additional parameter in the worker model greatly improves the discriminative ability of our model . If we know , then , that a worker’s sensitivity and speciﬁcity are p tp and p tn , respectively , and they an - swer that a label holds , we can update our pos - terior by simply multiplying by the likelihood ratio ( p tp + ( 1 − p tn ) ) / ( ( 1 − p tp ) + p tn ) . In this model , the agent always knows the most probable value for ⊕ ( I , L ) , and if a utility model associates different costs for false cat - egorization positives than false negatives , it can easily trade off between these errors . One subtlety concerns the treatment of past data . Since the agent has no access to gold data , it does not know the true labels , ⊕ ( I , L ) , even when it has seen the assessments of many workers . We use expectation - maximization ( EM ) to jointly estimate the values of these latent labels , along with the parameters of our noisy worker model . We perform a Bayesian estimate of our parameters by using weak sym - metric Beta priors and computing a maximum a posteriori 1 BUG : Cite ? ( MAP ) estimate to avoid problems when categorization is just starting . We term this model the independent model since it naively assumes that labels are independent . Formally , the marginal label probabilities are P ( L | v ) ∝ P ( L ) P ( v L | L ) , where L is a random boolean variable corresponding to an outcome ⊕ ( I , L ) for an item and v L is the vector of ob - served votes associated with a particular variable L . Modeling Label Co - Occurrence The assumption of label independence made by the previous model is obviously quite false . For example , an item which has been categorized as “football player” is more likely to be a member of the “person” category than to be a member of the “newspaper” category . This observation is especially pertinent of taxonomies with deep hierarchical structure , but is true for any set with overlapping labels . It is natural , therefore , to learn a joint model of label prob - abilities . In this model , when a worker responds that an item is in a given category , the posterior for all other cate - gories can be updated . This update will also affect the choice of which label has the highest value of information in the greedy search . There are many ways to represent a complex joint distri - bution . As a baseline approach , we explore a simple model which we term the multi - label naive Bayes ( MLNB ) model . Using notation from the previous section , if L is the set of all labels , then the marginal label probabilities are P ( L | v ) ∝ P ( L ) P ( v L | L ) (cid:89) L (cid:48) ∈L \ { L } (cid:88) L (cid:48) P ( L (cid:48) | L ) P ( v L (cid:48) | L (cid:48) ) . This approximation models pairwise co - occurrences be - tween categories directly but ignores the higher order inter - actions . We can incorporate observed votes into the model locally as in the independent model . In order to estimate these additional co - occurrence pa - rameters in an efﬁcient manner , we ﬁrst run EM as in the independent model and then estimate P ( L j | L k ) as (cid:80) I ∈I ( P ( L j ) P ( L k ) ) / (cid:80) I ∈I P ( L k ) . Additionally , given the sparsity of labels in our datasets , we introduce a bias against category co - occurrence by placing a weak Beta prior skewed toward 0 on the co - occurrence parameters and com - puting the MAP estimate . We have also explored a more sophisticated partially - directed graphical model that combines our generative in - dependent model with a pairwise Markov network over the label variables for an item . We found that this model , which requires approximate inference methods such as loopy belief propagation , is too slow to be useful in a live crowdsourc - ing setting and does not produce signiﬁcant gains over the MLNB model ; we do not describe it further in this paper . Control of Probabilistic Models One may also consider different control strategies for choos - ing the next question to dispatch to a worker . One may use Figure 2 : Sample interface for the Categorize worker task primitive used in our Mechanical Turk experiment . a simple round - robin strategy , as used in C ASCADE , or as we advocate , a greedy search that asks about the category where a worker’s vote would provide the greatest value of information . In order to perform a nonmyopic greedy search , we max - imize the expected value of information for the set of la - bels in the probabilistic models we have described . Let the information gain for labels L of selecting a set of worker votes A ⊂ V , where V is the ( inﬁnite ) set of possible fu - ture votes , be I ( L ; A ) = H ( L ) − H ( L | A ) . Then since we assume votes in V are independent given L , informa - tion gain is submodular ( Krause and Guestrin 2005 ) , and we can use a greedy variable selection algorithm to select a set A that is at most a factor of ( 1 − 1 / e ) worse than op - timal ( Nemhauser , Wolsey , and Fisher 1978 ) . Furthermore , Krause and Guestrin show that this factor is the best we can do unless P = NP . An additional assumption by our models enables us to re - ﬁne the variable selection algorithm . Theorem 2 Let each vote V ∈ V be independent of all other votes given the label L V ∈ L corresponding to that vote . Then the greedy heuristic for information gain must select the vote V ∗ ∈ V such that V ∗ ∈ argmax V ∈V H ( V | A ) − H ( V | L V ) . In the case where we are interested in which single vote to ask next , H ( V | A ) is simply H ( V ) . Experiments In our experiments , our goal is to compare the various con - trol strategies from the categorization control section . We ﬁrst compare the simple improvements to threshold voting by analyzing the cost savings for each strategy ( lossless , one - away ) and threshold settings ( T = 2 , 3 , 4 ) along with the quality of the taxonomy produced . Next , we evaluate the probabilistic models on their predictive performance and compare that against the original strategy from C ASCADE . Dataset In order to better analyze the effect of different categoriza - tion algorithms , we controlled for variation in label elucida - tion policies and adopted a ﬁxed set of candidate category la - bels from the literature . Speciﬁcally , we took a subset of the Method T F - score Votes % savings Lossless 2 0 . 83 130 21 One - away 2 0 . 82 96 42 Lossless 3 0 . 84 102 38 One - away 3 0 . 83 69 58 Lossless 4 0 . 75 72 56 One - away 4 0 . 70 38 77 Table 1 : Comparison of threshold voting methods . Mean F - score and number of votes per item . ﬁne - grained entity tags described in ( Ling and Weld 2012 ) by eliminating low probability tags and all those for organi - zations , events , and facilities ; this process yielded a manage - able set of 33 category labels . We then over - generated items for each of these labels , and constructed a random subset of 100 items . On Mechanical Turk , we simulated a run of the Categorize procedure from C ASCADE , called on these 100 items and 33 categories . We had k = 5 workers vote on batches of seven labels per Human Intelligence Task ( HIT ) , as was done in C ASCADE . The interface shown in Figure 2 uses validation to ensure that a worker either selects at least one label , or deliberately indicates that none of the displayed labels apply to the item in question . Each HIT cost $ 0 . 04 and the total amount paid to workers was $ 100 . We later gath - ered an additional k = 10 worker votes in the same manner at an additional cost of $ 200 upon realizing that our proba - bilistic control strategies tend to select more than ﬁve votes for some labels . The purpose of gathering this data was to allow us to compare different control strategies , controlling for worker error , since each control strategy would be seeing the same worker responses . Threshold Approaches In the ﬁrst experiment , we compared the threshold voting modiﬁcations to the naive version of threshold voting imple - mented in C ASCADE . Since C ASCADE uses various thresh - old settings in the adaptive context ﬁltering , we tested with thresholds of T = 2 , 3 , 4 out of 5 total votes . Table 1 shows the number of votes per item used by lossless stopping and the one - away heuristic , and the fraction of votes saved com - pared to the original approach in C ASCADE , which used 165 votes per item . Note that lossless stopping , which returns ex - actly the same predictions as the threshold voting procedure from C ASCADE , is able to save up to 56 % of the votes when T = 4 . In order to better understand the impact of the one - away heuristic on classiﬁcation performance , in Figure 3 we plot F - score performance vs . the number of votes used for loss - less stopping and the one - away heuristic . For threshold values T = 2 and T = 3 , the one - away strategy signiﬁ - cantly lowers the already reduced cost associated with loss - less stopping without introducing a statistically signiﬁcant decrease in predictive performance ( F - score ) . The decrease in F - score for T = 4 is statistically signiﬁcant ( p < 0 . 01 us - ing a two - tailed paired t - test ) , but can be attributed to poor recall . The one - away heuristic at this threshold setting re - Figure 3 : F - score vs . cost for threshold voting improve - ments turns false if the ﬁrst vote is negative , which is suboptimal since worker sensitivity is signiﬁcantly lower than worker speciﬁcity . In addition to classiﬁcation performance , we are also in - terested in how our improvement methods impact the quality of the ﬁnal output taxonomy . Visual inspection for errors in the output taxonomies did not reveal a decrease in quality when using the one - away heuristic . Figure 4 shows a high - quality taxonomy produced by the one - away heuristic with threshold T = 2 . We were surprised that a threshold of 3 produced higher F - score than that of 4 , but the difference may be attributed to increased recall at the lower threshold . Inference - Based Approaches We hypothesized that scaling taxonomy creation to a large number of items requires a probabilistic approach . To em - pirically determine the effectiveness of our approaches , we compared the performance of various inference and control strategies using the votes gathered from Mechanical Turk . In our experiments , we tested three inference methods ( MLNB , Independent , and Majority ) and two control strate - gies ( greedy and round - robin ) . The MLNB and Indepen - dent inference methods were described in the previous sec - tion , and Majority performs simple majority vote estimation that defaults to a negative answer and breaks ties in favor of a positive answer ( we found that these simple modiﬁca - tions improved results for our dataset ) . The greedy control strategy uses the submodularity result to select labels that maximize information gain , while the round - robin control strategy ﬁlls in votes layer by layer ( e . g . , it asks once about each label before asking twice about any label ) . Majority vote with round - robin is our reconstruction of the original C ASCADE approach . In order to test how our models will perform when scal - ing in the number of items , we evaluate the performance of our models using leave - one - out cross - validation for the 100 items . Model parameters are estimated using ﬁve worker (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:2)(cid:9)(cid:10)(cid:11)(cid:12) (cid:1)(cid:2)(cid:3)(cid:4)(cid:3)(cid:5)(cid:6)(cid:7)(cid:2)(cid:8)(cid:8)(cid:9)(cid:6)(cid:10)(cid:5)(cid:6)(cid:11)(cid:12)(cid:2)(cid:6)(cid:13)(cid:14)(cid:15)(cid:2)(cid:5)(cid:6)(cid:16)(cid:2)(cid:17)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:13)(cid:14)(cid:15)(cid:7)(cid:5)(cid:2)(cid:9)(cid:16)(cid:12) (cid:16)(cid:19)(cid:20)(cid:14)(cid:8)(cid:8)(cid:19)(cid:5)(cid:6)(cid:16)(cid:2)(cid:21)(cid:6)(cid:22)(cid:9)(cid:19)(cid:8)(cid:5)(cid:6)(cid:23)(cid:24)(cid:25)(cid:8)(cid:2)(cid:24)(cid:5)(cid:6)(cid:26)(cid:19)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:17)(cid:18)(cid:5)(cid:4)(cid:14)(cid:15)(cid:7)(cid:5)(cid:2)(cid:9)(cid:19)(cid:12) (cid:16)(cid:2)(cid:17)(cid:6)(cid:10)(cid:25)(cid:27)(cid:3)(cid:14)(cid:8)(cid:5)(cid:6)(cid:28)(cid:29)(cid:19)(cid:20)(cid:6)(cid:13)(cid:25)(cid:30)(cid:30) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:5)(cid:4)(cid:20)(cid:18)(cid:21)(cid:18)(cid:7)(cid:22)(cid:6)(cid:2)(cid:20)(cid:4)(cid:13)(cid:17)(cid:4)(cid:5)(cid:2)(cid:9)(cid:23)(cid:12) (cid:11)(cid:12)(cid:2)(cid:6)(cid:13)(cid:14)(cid:15)(cid:2)(cid:5)(cid:6)(cid:29)(cid:19)(cid:27)(cid:27)(cid:25)(cid:5)(cid:6)(cid:13)(cid:14)(cid:15)(cid:2)(cid:6)(cid:1)(cid:14)(cid:12)(cid:8)(cid:6)(cid:13)(cid:19)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:3)(cid:7)(cid:20)(cid:18)(cid:15)(cid:18)(cid:14)(cid:18)(cid:13)(cid:8)(cid:2)(cid:9)(cid:23)(cid:12) (cid:31)(cid:4)(cid:2)(cid:2)(cid:8)(cid:6)(cid:23)(cid:17)(cid:25)(cid:32)(cid:19)(cid:27)(cid:2)(cid:30)(cid:12)(cid:5)(cid:6)(cid:26)(cid:25)(cid:17)(cid:17)(cid:19)(cid:29)(cid:9)(cid:6)(cid:33)(cid:17)(cid:25)(cid:8)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:13)(cid:5)(cid:15)(cid:18)(cid:6)(cid:15)(cid:2)(cid:9)(cid:23)(cid:12) (cid:23)(cid:24)(cid:25)(cid:8)(cid:2)(cid:24)(cid:5)(cid:6)(cid:16)(cid:19)(cid:20)(cid:14)(cid:8)(cid:8)(cid:19)(cid:5)(cid:6)(cid:16)(cid:14)(cid:8)(cid:2)(cid:30)(cid:5)(cid:6)(cid:10)(cid:2)(cid:14)(cid:29)(cid:21)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:24)(cid:22)(cid:6)(cid:18)(cid:14)(cid:18)(cid:13)(cid:8)(cid:2)(cid:9)(cid:25)(cid:12) (cid:23)(cid:24)(cid:25)(cid:8)(cid:2)(cid:24)(cid:5)(cid:6)(cid:7)(cid:2)(cid:8)(cid:8)(cid:9)(cid:6)(cid:10)(cid:5)(cid:6)(cid:10)(cid:2)(cid:14)(cid:29)(cid:21)(cid:2)(cid:6)(cid:26)(cid:19)(cid:29)(cid:29)(cid:25)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:26)(cid:7)(cid:7)(cid:15)(cid:27)(cid:13)(cid:20)(cid:20)(cid:2)(cid:3)(cid:20)(cid:13)(cid:28)(cid:4)(cid:5)(cid:2)(cid:9)(cid:29)(cid:12) (cid:13)(cid:2)(cid:9)(cid:30)(cid:14)(cid:8)(cid:6)(cid:16)(cid:19)(cid:8)(cid:8)(cid:25)(cid:8)(cid:21)(cid:5)(cid:6)(cid:1)(cid:14)(cid:2)(cid:6)(cid:16)(cid:14)(cid:8)(cid:30)(cid:19)(cid:8)(cid:19)(cid:5)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:13)(cid:22)(cid:15)(cid:30)(cid:7)(cid:5)(cid:2)(cid:9)(cid:10)(cid:12) (cid:16)(cid:19)(cid:29)(cid:30)(cid:12)(cid:19)(cid:6)(cid:34)(cid:30)(cid:2)(cid:35)(cid:19)(cid:29)(cid:30)(cid:5)(cid:6)(cid:1)(cid:14)(cid:12)(cid:8)(cid:6)(cid:36)(cid:14)(cid:37)(cid:38)(cid:2)(cid:5)(cid:6)(cid:39)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:7)(cid:5)(cid:21)(cid:13)(cid:8)(cid:18)(cid:31)(cid:13)(cid:15)(cid:18)(cid:7)(cid:8)(cid:2)(cid:26)(cid:7)(cid:22)(cid:8)(cid:17)(cid:4)(cid:5)(cid:2)(cid:9)(cid:32)(cid:12) (cid:16)(cid:19)(cid:29)(cid:30)(cid:12)(cid:19)(cid:6)(cid:34)(cid:30)(cid:2)(cid:35)(cid:19)(cid:29)(cid:30)(cid:5)(cid:6)(cid:34)(cid:30)(cid:2)(cid:40)(cid:2)(cid:6)(cid:1)(cid:14)(cid:27)(cid:3)(cid:5)(cid:6)(cid:41)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:24)(cid:18)(cid:20)(cid:18)(cid:15)(cid:13)(cid:5)(cid:28)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:2)(cid:9)(cid:32)(cid:12) (cid:34)(cid:19)(cid:20)(cid:20)(cid:19)(cid:24)(cid:6)(cid:26)(cid:4)(cid:3)(cid:3)(cid:2)(cid:25)(cid:8)(cid:5)(cid:6)(cid:42)(cid:29)(cid:2)(cid:20)(cid:2)(cid:29)(cid:25)(cid:37)(cid:38)(cid:6)(cid:30)(cid:12)(cid:2)(cid:6)(cid:18)(cid:18)(cid:18) (cid:1)(cid:2)(cid:20)(cid:7)(cid:14)(cid:13)(cid:15)(cid:18)(cid:7)(cid:8)(cid:2)(cid:9)(cid:11)(cid:25)(cid:12) (cid:42)(cid:25)(cid:43)(cid:25)(cid:5)(cid:6)(cid:39)(cid:8)(cid:20)(cid:14)(cid:8)(cid:2)(cid:3)(cid:25)(cid:19)(cid:5)(cid:6)(cid:34)(cid:12)(cid:19)(cid:8)(cid:21)(cid:12)(cid:19)(cid:25)(cid:5)(cid:6)(cid:44)(cid:19)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:14)(cid:7)(cid:22)(cid:8)(cid:15)(cid:5)(cid:28)(cid:2)(cid:9)(cid:33)(cid:12) (cid:42)(cid:25)(cid:43)(cid:25)(cid:5)(cid:6)(cid:10)(cid:29)(cid:2)(cid:2)(cid:8)(cid:17)(cid:19)(cid:8)(cid:20)(cid:5)(cid:6)(cid:39)(cid:8)(cid:20)(cid:14)(cid:8)(cid:2)(cid:3)(cid:25)(cid:19)(cid:5)(cid:6)(cid:42)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:18)(cid:6)(cid:20)(cid:13)(cid:8)(cid:17)(cid:2)(cid:9)(cid:29)(cid:12) (cid:42)(cid:25)(cid:43)(cid:25)(cid:5)(cid:6)(cid:44)(cid:12)(cid:25)(cid:20)(cid:27)(cid:2)(cid:9)(cid:6)(cid:39)(cid:3)(cid:17)(cid:19)(cid:8)(cid:20)(cid:5)(cid:6)(cid:10)(cid:29)(cid:2)(cid:2)(cid:8)(cid:17)(cid:19)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:14)(cid:18)(cid:15)(cid:28)(cid:2)(cid:9)(cid:10)(cid:12) (cid:34)(cid:12)(cid:19)(cid:8)(cid:21)(cid:12)(cid:19)(cid:25)(cid:5)(cid:6)(cid:44)(cid:19)(cid:3)(cid:12)(cid:25)(cid:8)(cid:21)(cid:30)(cid:14)(cid:8)(cid:5)(cid:6)(cid:42)(cid:17)(cid:14)(cid:29)(cid:25)(cid:20)(cid:19)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:5)(cid:18)(cid:34)(cid:4)(cid:5)(cid:2)(cid:9)(cid:32)(cid:12) (cid:11)(cid:12)(cid:2)(cid:6)(cid:33)(cid:12)(cid:19)(cid:29)(cid:17)(cid:2)(cid:3)(cid:6)(cid:22)(cid:25)(cid:40)(cid:2)(cid:29)(cid:5)(cid:6)(cid:11)(cid:12)(cid:2)(cid:6)(cid:13)(cid:14)(cid:30)(cid:14)(cid:24)(cid:19)(cid:6)(cid:18)(cid:18)(cid:18) (cid:1)(cid:2)(cid:34)(cid:4)(cid:30)(cid:18)(cid:14)(cid:20)(cid:4)(cid:2)(cid:9)(cid:11)(cid:19)(cid:12) (cid:26)(cid:14)(cid:8)(cid:20)(cid:19)(cid:6)(cid:45)(cid:37)(cid:37)(cid:14)(cid:29)(cid:20)(cid:5)(cid:6)(cid:26)(cid:4)(cid:24)(cid:24)(cid:2)(cid:29)(cid:5)(cid:6)(cid:16)(cid:25)(cid:10)(cid:46)(cid:47)(cid:48)(cid:5)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:14)(cid:13)(cid:5)(cid:2)(cid:9)(cid:29)(cid:12) (cid:33)(cid:12)(cid:2)(cid:40)(cid:9)(cid:6)(cid:11)(cid:19)(cid:12)(cid:14)(cid:2)(cid:5)(cid:6)(cid:42)(cid:14)(cid:29)(cid:20)(cid:6)(cid:11)(cid:19)(cid:4)(cid:29)(cid:4)(cid:3)(cid:5)(cid:6)(cid:26)(cid:4)(cid:24)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:15)(cid:5)(cid:22)(cid:14)(cid:35)(cid:2)(cid:9)(cid:32)(cid:12) (cid:33)(cid:12)(cid:2)(cid:40)(cid:9)(cid:6)(cid:34)(cid:25)(cid:17)(cid:40)(cid:2)(cid:29)(cid:19)(cid:20)(cid:14)(cid:5)(cid:6)(cid:30)(cid:29)(cid:19)(cid:37)(cid:30)(cid:14)(cid:29)(cid:6)(cid:30)(cid:29)(cid:19)(cid:25)(cid:6)(cid:18)(cid:18)(cid:18) (cid:1)(cid:2)(cid:3)(cid:5)(cid:7)(cid:2)(cid:13)(cid:15)(cid:30)(cid:20)(cid:4)(cid:15)(cid:4)(cid:2)(cid:9)(cid:11)(cid:11)(cid:12) (cid:13)(cid:2)(cid:9)(cid:30)(cid:14)(cid:8)(cid:6)(cid:16)(cid:19)(cid:8)(cid:8)(cid:25)(cid:8)(cid:21)(cid:5)(cid:6)(cid:1)(cid:14)(cid:2)(cid:6)(cid:49)(cid:19)(cid:24)(cid:19)(cid:30)(cid:12)(cid:5)(cid:6)(cid:1)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:27)(cid:13)(cid:6)(cid:35)(cid:4)(cid:15)(cid:27)(cid:13)(cid:20)(cid:20)(cid:2)(cid:3)(cid:20)(cid:13)(cid:28)(cid:4)(cid:5)(cid:2)(cid:9)(cid:10)(cid:12) (cid:36)(cid:2)(cid:27)(cid:29)(cid:14)(cid:8)(cid:6)(cid:1)(cid:19)(cid:24)(cid:2)(cid:3)(cid:5)(cid:6)(cid:36)(cid:19)(cid:29)(cid:29)(cid:9)(cid:6)(cid:28)(cid:25)(cid:29)(cid:20)(cid:5)(cid:6)(cid:34)(cid:12)(cid:19)(cid:6)(cid:18)(cid:18)(cid:18) (cid:1)(cid:2)(cid:14)(cid:5)(cid:4)(cid:13)(cid:15)(cid:18)(cid:34)(cid:4)(cid:2)(cid:36)(cid:7)(cid:5)(cid:35)(cid:2)(cid:9)(cid:11)(cid:37)(cid:12) (cid:33)(cid:19)(cid:30)(cid:37)(cid:12)(cid:2)(cid:29)(cid:6)(cid:25)(cid:8)(cid:6)(cid:30)(cid:12)(cid:2)(cid:6)(cid:22)(cid:9)(cid:2)(cid:5)(cid:6)(cid:10)(cid:2)(cid:14)(cid:29)(cid:21)(cid:2)(cid:6)(cid:26)(cid:19)(cid:6)(cid:18)(cid:18)(cid:18) (cid:1)(cid:2)(cid:15)(cid:34)(cid:2)(cid:3)(cid:5)(cid:7)(cid:21)(cid:5)(cid:13)(cid:24)(cid:2)(cid:9)(cid:29)(cid:12) (cid:34)(cid:2)(cid:3)(cid:19)(cid:24)(cid:2)(cid:6)(cid:34)(cid:30)(cid:29)(cid:2)(cid:2)(cid:30)(cid:5)(cid:6)(cid:42)(cid:29)(cid:25)(cid:2)(cid:8)(cid:20)(cid:3)(cid:5)(cid:6)(cid:11)(cid:12)(cid:2)(cid:6)(cid:28)(cid:6)(cid:18)(cid:18)(cid:18) (cid:1)(cid:2)(cid:8)(cid:4)(cid:36)(cid:6)(cid:3)(cid:13)(cid:3)(cid:4)(cid:5)(cid:2)(cid:9)(cid:25)(cid:12) (cid:49)(cid:2)(cid:35)(cid:3)(cid:35)(cid:2)(cid:2)(cid:38)(cid:5)(cid:6)(cid:39)(cid:8)(cid:20)(cid:25)(cid:19)(cid:6)(cid:11)(cid:14)(cid:20)(cid:19)(cid:9)(cid:5)(cid:6)(cid:11)(cid:12)(cid:2)(cid:6)(cid:28)(cid:14)(cid:6)(cid:18)(cid:18)(cid:18) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:1)(cid:2)(cid:24)(cid:13)(cid:21)(cid:13)(cid:31)(cid:18)(cid:8)(cid:4)(cid:2)(cid:9)(cid:19)(cid:12) (cid:39)(cid:8)(cid:20)(cid:25)(cid:19)(cid:6)(cid:11)(cid:14)(cid:20)(cid:19)(cid:9)(cid:5)(cid:6)(cid:49)(cid:2)(cid:35)(cid:3)(cid:35)(cid:2)(cid:2)(cid:38) (cid:1)(cid:2)(cid:5)(cid:4)(cid:20)(cid:18)(cid:21)(cid:18)(cid:7)(cid:8)(cid:2)(cid:9)(cid:10)(cid:12) (cid:13)(cid:14)(cid:15)(cid:2)(cid:6)(cid:42)(cid:29)(cid:19)(cid:8)(cid:37)(cid:25)(cid:3)(cid:5)(cid:6)(cid:26)(cid:25)(cid:8)(cid:20)(cid:4)(cid:25)(cid:3)(cid:24)(cid:5)(cid:6)(cid:50)(cid:19)(cid:14)(cid:25)(cid:3)(cid:6)(cid:18)(cid:18)(cid:18) (cid:1)(cid:2)(cid:27)(cid:7)(cid:7)(cid:35)(cid:2)(cid:9)(cid:10)(cid:12) (cid:33)(cid:19)(cid:30)(cid:37)(cid:12)(cid:2)(cid:29)(cid:6)(cid:25)(cid:8)(cid:6)(cid:30)(cid:12)(cid:2)(cid:6)(cid:22)(cid:9)(cid:2)(cid:5)(cid:6)(cid:11)(cid:12)(cid:2)(cid:6)(cid:51)(cid:52)(cid:53)(cid:14)(cid:29)(cid:6)(cid:18)(cid:18)(cid:18) (cid:1)(cid:2)(cid:26)(cid:18)(cid:20)(cid:24)(cid:2)(cid:9)(cid:10)(cid:12) (cid:11)(cid:12)(cid:2)(cid:6)(cid:10)(cid:29)(cid:2)(cid:19)(cid:30)(cid:6)(cid:10)(cid:19)(cid:30)(cid:3)(cid:27)(cid:9)(cid:5)(cid:6)(cid:34)(cid:30)(cid:19)(cid:29)(cid:6)(cid:44)(cid:19)(cid:29)(cid:3)(cid:6)(cid:46)(cid:6)(cid:18)(cid:18)(cid:18) (cid:1)(cid:2)(cid:13)(cid:18)(cid:5)(cid:14)(cid:5)(cid:13)(cid:26)(cid:15)(cid:2)(cid:9)(cid:10)(cid:12) (cid:28)(cid:46)(cid:47)(cid:6)(cid:27)(cid:14)(cid:24)(cid:27)(cid:2)(cid:29)(cid:5)(cid:6)(cid:45)(cid:25)(cid:29)(cid:27)(cid:4)(cid:3)(cid:6)(cid:45)(cid:48)(cid:54)(cid:55)(cid:5)(cid:6)(cid:28)(cid:14)(cid:2)(cid:25)(cid:6)(cid:18)(cid:18)(cid:18) (cid:1)(cid:2)(cid:27)(cid:7)(cid:13)(cid:15)(cid:2)(cid:9)(cid:19)(cid:12) (cid:19)(cid:25)(cid:29)(cid:37)(cid:29)(cid:19)(cid:53)(cid:30)(cid:6)(cid:37)(cid:19)(cid:29)(cid:29)(cid:25)(cid:2)(cid:29)(cid:5)(cid:6)(cid:11)(cid:12)(cid:2)(cid:6)(cid:11)(cid:25)(cid:30)(cid:19)(cid:8)(cid:25)(cid:37) (cid:1)(cid:2)(cid:7)(cid:15)(cid:30)(cid:4)(cid:5)(cid:2)(cid:9)(cid:19)(cid:12) (cid:34)(cid:19)(cid:30)(cid:4)(cid:29)(cid:8)(cid:5)(cid:6)(cid:30)(cid:9)(cid:15)(cid:12)(cid:14)(cid:25)(cid:20) Figure 4 : The one - away policy with threshold 3 used only 42 % of the labor required by C ASCADE , yet produced an excellent taxonomy ( excerpt shown ) . votes for each item - label pair in the training set containing 99 items . Figure 5 shows the results of this experiment using worker votes from Mechanical Turk . MLNB and Independent show a clear improvement over the simple round - robin method , and MLNB in particular reaches high levels of performance very quickly . The improvement of MLNB over Indepen - dent is highly statistically signiﬁcant at the 0 . 05 signiﬁcance level ( using a two - tailed paired t - test ) for the ﬁrst 47 votes , lending credence to our hypothesis that co - occurrence infor - mation is important to predictive performance . Furthermore , points where Independent crosses slightly above MLNB are not statistically signiﬁcant . It should be noted that the prob - abilistic models achieve signiﬁcant gains despite limitations in the number of collected worker votes . While the total k = 15 votes per item - label pair was more than enough for selecting 5 × 33 = 165 votes with a round - robin con - trol strategy , the probabilistic control strategies frequently requested votes for item - label pairs in excess of 15 votes . These limitations occurred for MLNB on 53 out of 100 items ( ﬁrst happening after 87 votes on average ) and for Indepen - dent on 31 items ( happening after 130 votes on average ) . In spite of this handicap , our probabilistic methods exhibit statistically signiﬁcant gains . So , which control strategy is best ? On this dataset , C ASCADE ’s voting policy ( accept a label if four out of ﬁve workers think it applies ) required 165 worker tasks per item and yielded an F - score of 75 % when compared to gold - truth data . In contrast , our one - away strategy with threshold 3 , had an F - score of 83 % and only used 42 % as much labor . Our probabilistic approaches are anytime and can be stopped after any number of worker tasks . MLNB with a greedy control strategy produced an F - score around 76 % after only 16 tasks per item , which is less than 10 % as much labor as Figure 5 : Performance vs . number of votes ( votes from Me - chanical Turk ) C ASCADE required to achieve similar performance . Batching Tasks In order to be practically useful in a crowdsourcing setting , our control strategies need to be able to group tasks together so that a worker can answer multiple questions about an item at once . A simple control strategy for selecting batches of la - bels , which we term k - best , simply selects the top k labels ranked by our control strategies for selecting single labels . More complex control strategies for batching include looka - head search or near - optimal selection using submodularity of information gain ( Krause and Guestrin 2005 ) . We tested various batch control strategies and found that k - best offers the best tradeoff between performance and computational complexity . Figure 6 shows that for k = 7 ( the same number used by C ASCADE and our own live ex - periment ) , MLNB with k - best control results in a small de - crease in performance compared to MLNB with single label selection . This difference is statistically signiﬁcant ( at the 0 . 05 signiﬁcance level using a two - tailed paired t - test ) only until about 35 votes per item , and the batched version of MLNB still outperforms Independent with single label se - lection . The more complex batching strategies require additional computation and failed to produce signiﬁcant performance gains . For instance , making use of our submodularity re - sult for k > 1 requires computing nontrivial conditional en - tropies for the MLNB model . One possible reason the more intensive approaches fail to improve performance is that we require labels within a batch to be distinct ( it is not bene - ﬁcial to ask the same worker the same question more than once ) . Given this restriction , k - best is an effective heuristic that incurs no additional cost compared to selection of single labels . Figure 6 : Performance vs . number of votes for selecting batches of k = 7 ( MLNB with single label selection shown for comparison ) . Simulation Study An intelligent control procedure must be robust to noise due to worker quality . In order to assess the behavior of our techniques on more complex taxonomization problems where the workers may be more error - prone , we simulated worker with 60 % sensitivity and 80 % speciﬁcity . We per - form this experiment using the ground truth taxonomy and item - label answers in a purely simulation setting , which has the additional beneﬁt of not being limited by the number of votes per item - label pair . The overall higher performance of our results in Figure 7 despite less accurate workers ( aver - age sensitivity and speciﬁcity for workers in our dataset was 76 % and 98 % 2 , respectively ) can be explained discrepan - cies between the ground truth labeled by the authors and the collective ground truth decided by workers on Mechanical Turk . Figure 7 shows the same statistically signiﬁcant or - dering of the models as we saw with real worker votes , and highlights the value of using joint inference for more com - plex taxonomy generation tasks under cost constraints . Related Work Our research ﬁts into the broad theme of using AI technol - ogy for optimization of crowdsourced tasks ( Weld , Mausam , and Dai 2011 ) . A large body of work has optimized sim - ple tasks such as classiﬁcation with noisy workers ( Sheng , Provost , and Ipeirotis 2008 ; Whitehill et al . 2009 ; Raykar et al . 2010 ; Welinder et al . 2010 ; Wauthier and Jordan 2011 ; Kamar , Hacker , and Horvitz 2012 ) . Relatively less research has gone into optimizing more complex workﬂows such as C ASCADE ’s that have a much richer space of possible out - comes . A notable exception is the optimization of itera - tive improvement workﬂows using decision - theoretic con - trol ( Dai , Mausam , and Weld 2010 ; 2011 ) . 2 BUG : Double - check this Figure 7 : Performance vs . number of votes for a more difﬁ - cult simulated task ( sensitivity = 0 . 6 , speciﬁcity = 0 . 8 ) Closely related work on optimizing the categorization of items within a taxonomy takes a graph - theoretic ap - proach ( Parameswaran et al . 2011 ) , but does not consider a probabilistic framework for modeling noisy workers , a crit - ical component of crowdsourcing systems . Moreover , that approach assumes labels are organized in a taxonomy that is known a priori , and does not model label co - occurrence , which our experiments show dramatically improves labeling efﬁciency and accuracy . Other related research investigates selecting the next best question from a set of known questions . Often , the goal is to use active learning to improve the accuracy of a classiﬁer , by selecting questions based on label uncertainty or model uncertainty ( Sheng , Provost , and Ipeirotis 2008 ; Wauthier and Jordan 2011 ) . Our approach to multi - label classiﬁcation seeks to ask questions that optimize the value of information within a graphical model ( Krause and Guestrin 2005 ) , rather than to optimize performance on an external task . Our approach to label elucidation is related to work on collaborative and social tagging . ( Golder and Huberman 2006 ) uses a P ´ olya urn model to explain why relative tag proportions tend to stabilize over time for bookmarks on Delicious , a collaborative tagging system for web book - marks . ( Chi and Mytkowicz 2008 ) investigates tagging on Delicious as well , using information - theoretic measures to model the developing vocabulary of tags and the effective - ness of the set of tags for document retrieval . In a crowd la - bor setting , ( Lin , Mausam , and Weld 2012b ) uses a Chinese Restaurant Process model to optimize free response question answering . 3 Other instances of use of AI within crowdsourcing in - clude assignment of tasks to workers ( Donmez , Carbonell , and Schneider 2010 ; Tran - Thanh et al . 2012 ) , collaboration 3 BUG : Fourth reviewer also wanted us to cite the following pa - per but seems less relevant : Furnas et al . 1987 . The vocabulary problem in human - system communication . Commun . ACM of crowdsourcing and a machine learning model for effec - tive labeling ( Kamar , Hacker , and Horvitz 2012 ) , selecting workers based on their known skills ( Shahaf and Horvitz 2010 ) , and choosing between multiple workﬂows for the same task ( Lin , Mausam , and Weld 2012a ) . Conclusions & Future Work Machine learning and decision theoretic techniques offer the potential for dramatically reducing the amount of human labor required in crowdsourced applications . However , to date , most work has focused on optimizing relatively simple workﬂows , such as consensus tasks and iterative improve - ment workﬂows . Taxonomy generation is an important task , which requires a complex workﬂow to create a globally con - sistent interpretation of a large dataset from workers who typically have only a narrow view of a small data subset . Since previous work on crowdsourcing taxonomy creation , C ASCADE , was both promising yet labor intensive , it is a natural target for decision - theoretic optimization . This paper presents D ELUGE , a reﬁnement of the C ASCADE algorithm with novel approaches to the subprob - lems of category elucidation and categorization . For the for - mer , we introduce a combinatorial P ´ olya urn model that al - lows us to calculate the relative cost of stopping the label generation phase early . For the problem of categorization given a ﬁxed set of labels , we present four models : lossless , one - away , a simple probabilistic model and a naive Bayes model of category co - occurrence . The latter two models support a greedy control strategy that chooses the most in - formative label to ask a human to evaluate , within a constant factor of the optimal next label . We also provide a batching strategy , making our approach to multi - label classiﬁcation both highly general and practically useful . Using a new dataset of Freebase entities we performed live experiments on Mechanical Turk to evaluate the rela - tive effectiveness of the categorization approaches . Using C ASCADE ’s voting policy ( accept a category if four out of ﬁve workers think it applies ) required 165 worker tasks per item and yielded an F - score of 75 % when compared to gold - truth data . In contrast , our one - away strategy with threshold 3 , had an F - score of 83 % and only used 42 % as much labor . Our probabilistic approaches are anytime and can be stopped after any number of worker tasks . The MLNB greedy search method produced an F - score of 76 % after only 16 tasks per item . To put this in context , since there are 33 categories , C ASCADE would post 165 worker tasks . Hence D ELUGE achieves comparable accuracy but uses just under 10 % as much labor . 4 It should be noted that in our experiments we do not distinguish between individual workers , since we focus on comparing different representations of the underlying distri - bution on labels . However , learning individual noisy worker models would likely improve results for the probabilistic models we have presented . Despite these promising results , much remains to be 4 BUG : We are repeating almost verbatim from the Threshold Approaches section earlier done . 5 Acknowledgements We thank the anonymous reviewers for useful feedback . References Chi , E . H . , and Mytkowicz , T . 2008 . Understanding the efﬁciency of social tagging systems using information the - ory . In Proceedings of the Nineteenth ACM Conference on Hypertext and Hypermedia ( HT ’08 ) , 81 – 88 . Chilton , L . B . ; Little , G . ; Edge , D . ; Weld , D . S . ; and Landay , J . A . 2013 . Cascade : Crowdsourcing taxonomy creation . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’13 ) . Dai , P . ; Mausam ; and Weld , D . S . 2010 . Decision - theoretic control of crowd - sourced workﬂows . In Proceedings of the Twenty - Fourth AAAI Conference on Artiﬁcial Intelligence ( AAAI ’10 ) . Dai , P . ; Mausam ; and Weld , D . S . 2011 . Artiﬁcial intelli - gence for artiﬁcial , artiﬁcial intelligence . In Proceedings of the Twenty - Fifth AAAI Conference on Artiﬁcial Intelligence ( AAAI ’11 ) . Donmez , P . ; Carbonell , J . G . ; and Schneider , J . 2010 . A probabilistic framework to learn from multiple annotators with time - varying accuracy . In Proceedings of the SIAM International Conference on Data Mining ( SDM ’10 ) , 826 – 837 . Golder , S . A . , and Huberman , B . A . 2006 . Usage patterns of collaborative tagging systems . Journal of Information Sci - ence 32 ( 2 ) : 198 – 208 . Johnson , N . L . , and Kotz , S . 1977 . Urn models and their ap - plication : an approach to modern discrete probability the - ory . Wiley New York . Kamar , E . ; Hacker , S . ; and Horvitz , E . 2012 . Combining hu - man and machine intelligence in large - scale crowdsourcing . In Proceedings of the Eleventh International Conference on Autonomous Agents and Multiagent Systems ( AAMAS ’12 ) . Krause , A . , and Guestrin , C . 2005 . Near - optimal nonmyopic value of information in graphical models . In Proceedings of the Twenty - First Conference on Uncertainty in Artiﬁcial Intelligence ( UAI ’05 ) . Lin , C . ; Mausam ; and Weld , D . S . 2012a . Dynamically switching between synergistic workﬂows for crowdsourc - ing . In Proceedings of the Twenty - Sixth AAAI Conference on Artiﬁcial Intelligence ( AAAI ’12 ) . Lin , C . H . ; Mausam ; and Weld , D . S . 2012b . Crowdsourcing control : Moving beyond multiple choice . In Proceedings of the Twenty - Eighth Conference on Uncertainty in Artiﬁcial Intelligence ( UAI ’12 ) . Ling , X . , and Weld , D . S . 2012 . Fine - grained entity resolu - tion . In Proceedings of the Twenty - Sixth AAAI Conference on Artiﬁcial Intelligence ( AAAI ’12 ) . 5 BUG : Evaluating task performance using created taxonomies ? Dynamic interfaces ? Little , G . ; Chilton , L . B . ; Goldman , M . ; and Miller , R . C . 2009 . TurKit : Tools for iterative tasks on mechanical turk . In Human Computation Workshop ( HCOMP ’09 ) . Miller , G . 1991 . WordNet : An on - line lexical database . International Journal of Lexicography 3 ( 4 ) : 235 – 312 . Nemhauser , G . L . ; Wolsey , L . A . ; and Fisher , M . L . 1978 . An analysis of approximations for maximizing submodular set functions . Mathematical Programming 14 ( 1 ) : 265 – 294 . Parameswaran , A . G . ; Sarma , A . D . ; Garcia - Molina , H . ; Polyzotis , N . ; and Widom , J . 2011 . Human - assisted graph search : It’s okay to ask questions . Proceedings of the VLDB Endowment 4 ( 5 ) : 267 – 278 . Raykar , V . C . ; Yu , S . ; Zhao , L . H . ; and Valadez , G . 2010 . Learning from crowds . Journal of Machine Learning Re - search 11 : 1297 – 1322 . Shahaf , D . , and Horvitz , E . 2010 . Generalized task mar - kets for human and machine computation . In Proceedings of the Twenty - Fourth AAAI Conference on Artiﬁcial Intelli - gence ( AAAI ’10 ) . Sheng , V . S . ; Provost , F . ; and Ipeirotis , P . G . 2008 . Get another label ? Improving data quality and data mining us - ing multiple , noisy labelers . In Proceedings of the Four - teenth ACM SIGKDD International Conference on Knowl - edge Discovery and Data Mining ( KDD ’08 ) . Tran - Thanh , L . ; Stein , S . ; Rogers , A . ; and Jennings , N . R . 2012 . Efﬁcient crowdsourcing of unknown experts using multi - armed bandits . In Proceedings of the Twentieth Euro - pean Conference on Artiﬁcial Intelligence ( ECAI ’12 ) , 768 – 773 . Wauthier , F . , and Jordan , M . 2011 . Bayesian bias mitiga - tion for crowdsourcing . In Advances in Neural Information Processing Systems 24 ( NIPS ’11 ) . 1800 – 1808 . Weld , D . S . ; Mausam ; and Dai , P . 2011 . Human Intelli - gence Needs Artiﬁcial Intelligence . In Human Computation Workshop ( HCOMP ’11 ) . Welinder , P . ; Branson , S . ; Belongie , S . ; and Perona , P . 2010 . The multidimensional wisdom of crowds . In Ad - vances in Neural Information Processing Systems 23 ( NIPS ’10 ) . 2424 – 2432 . Whitehill , J . ; Ruvolo , P . ; Wu , T . ; Bergsma , J . ; and Movellan , J . 2009 . Whose vote should count more : Optimal integration of labels from labelers of unknown expertise . In Advances in Neural Information Processing Systems 22 ( NIPS ’09 ) . 2035 – 2043 .