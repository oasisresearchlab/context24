Sparse Additive Generative Models of Text Jacob Eisenstein jacobeis @ cs . cmu . edu Amr Ahmed amahmed @ cs . cmu . edu Eric P . Xing epxing @ cs . cmu . edu School of Computer Science , Carnegie Mellon University , 5000 Forbes Ave . , Pittsburgh , PA 15203 USA Abstract Generative models of text typically associate a multinomial with every class label or topic . Even in simple models this requires the esti - mation of thousands of parameters ; in multi - faceted latent variable models , standard ap - proaches require additional latent “switch - ing” variables for every token , complicating inference . In this paper , we propose an alter - native generative model for text . The cen - tral idea is that each class label or latent topic is endowed with a model of the devi - ation in log - frequency from a constant back - ground distribution . This approach has two key advantages : we can enforce sparsity to prevent overﬁtting , and we can combine gen - erative facets through simple addition in log space , avoiding the need for latent switching variables . We demonstrate the applicability of this idea to a range of scenarios : classi - ﬁcation , topic modeling , and more complex multifaceted generative models . 1 . Introduction Generative models of text overwhelmingly rely on the Dirichlet - multinomial conjugate pair . The primary ad - vantage is that estimation is straightforward and ef - ﬁcient , with the Dirichlet prior contributing pseudo - counts to the observed counts generated by the multi - nomial . However , the ease of parameter estimation comes at a cost : unnecessarily complicated latent vari - able structures and lack of robustness to limited train - ing data . More concretely , we see three main problems with Dirichlet - multinomial generative models : Appearing in Proceedings of the 28 th International Con - ference on Machine Learning , Bellevue , WA , USA , 2011 . Copyright 2011 by the author ( s ) / owner ( s ) . • Inference cost There is increasing interest in modeling text with multiple generative facets , such as syntax ( Griﬃths et al . , 2005 ) , senti - ment ( Mei et al . , 2007 ) , and ideological and cul - tural perspective ( Ahmed & Xing , 2010 ; Paul & Girju , 2010 ) . In most cases , the incorporation of multiple facets requires an additional latent vari - able per token , to act as a “switch” controlling which facet is currently active . This huge num - ber of additional latent variables makes inference more expensive . • Overparametrization Standard Dirichlet - multinomial generative models learn a unique probability distribution over the entire vocabu - lary . General lexical patterns — for example , the high frequency of function words “the” and “of” — must be re - learned for every topic , wasting training data . In practice function words are typically removed using heuristics , or must be handled explicitly through additional latent variables ( Chemudugunta et al . , 2006 ) . • Lack of sparsity The Dirichlet - multinomial is incapable of using sparsity to limit model com - plexity . While the Dirichlet prior can induce zeros in the multinomials it generates , such sparsity is counterproductive to robustness : for example , su - pervised models that assign zero generative like - lihood for some terms will be extremely brittle , because the label assignment of an entire docu - ment can be vetoed by a single word . This paper proposes an alternative to the Dirichlet - multinomial for generative models of text : the Sparse Additive Generative model ( SAGE ) . The problems with the Dirichlet - multinomial stem from a root cause : directly modeling the lexical probabilities associated with each document class or latent factor . In con - trast , SAGE models the diﬀerence in log - frequencies from a background lexical distribution ( see Figure 1 ) . This has two key advantages : ﬁrst , we can apply a Sparse Additive Generative Models of Text background switch topic gate word distribution perspective background topic + word distribution perspective Figure 1 . A schematic comparison between a standard multinomial switching model ( left ) and SAGE ( right ) . Rather than choosing among probability distributions for each facet , SAGE additively combines sparse zero - mean variations . sparsity - inducing prior to limit the number of terms whose probability diverges from the background lexi - cal frequencies . This increases predictive accuracy and robustness to limited training data . Second , we can construct multi - faceted latent variable models by sim - ply adding together SAGE component vectors . For example , if a blog post on the topic of climate change is written from a right - wing perspective , we can model the text by summing the SAGE components associated with the topic , perspective , and topic - perspective in - teraction . No latent “switching” variables are required to decide which of these components is active for a given token . SAGE is intended as a drop - in replacement for the Dirichlet - multinomial , and can be applied in a broad range of generative models . We demonstrate SAGE’s advantages in a number of diﬀerent settings . First , we substitute SAGE for the Dirichlet - multinomial in a na¨ıve Bayes text classiﬁer , obtaining higher overall ac - curacy , especially in the face of limited training data . Second , we use SAGE in a topic model , obtaining bet - ter predictive likelihood on held - out text by learning simpler topics with less variation on rare words . Third , we apply SAGE in generative models which combine topics with additional facets : ideology and geographi - cal variation . 2 . Additive generative models of text The core idea of our generative model is that of a background lexical distribution , which is modiﬁed by adding additional vectors in log - space . In the simplest case , we have a background distribution m ∈ R V , and a set of components { η k ∈ R V } , where V is the size of the vocabulary . Each component η k corresponds to a document label y ∈ 1 . . . Y max . Then the generative distribution for each word in a document d is , P ( w | y d , m , η ) = exp (cid:0) m + η y d (cid:1) (cid:80) i exp ( m i + η y d , i ) ( 1 ) In this formulation each document has a single compo - nent index y d . If this index is observed then this model corresponds to a na¨ıve Bayes model of text , where we have substituted the vector of log frequency deviations η for the standard multinomial ; if y d is not observed then the model is a mixture of unigrams . We will ex - tend this framework to include per - word latent indices , drawn from a document - speciﬁc topic distribution — this corresponds to a variant of latent Dirichlet allo - cation ( Blei et al . , 2003 ) , in which each topic is repre - sented by log frequency deviations η rather than word probabilities β . Thus , we can replicate the most com - mon existing generative models of text using SAGE , taking advantage of sparsity - inducing priors on η to obtain additional robustness . However , SAGE has another advantage : by modeling log term frequencies rather than raw probabilities , it is easy to combine multiple facets simply by adding them . We can replicate existing models by adding a sparse deviation vector η to the background log - frequencies m ; by adding additional facets , we obtain richer and more complex models . For example , in a topic - perspective model , we associate a single perspec - tive y d with the entire document ; for each token w n we have a unique topic z n . Using Dirichlet - multinomials would require an additional switching variable to de - termine whether the token w n is drawn from the topic β ( t ) z n or the perspective β ( p ) y d ( Ahmed & Xing , 2010 ) . But with SAGE , we draw the token w n from a distri - bution proportional to exp (cid:16) m + η ( p ) y d + η ( t ) z n (cid:17) , without need for latent switching variables . We ignore covariance between terms and treat each element η ki independently , where k indexes the com - Sparse Additive Generative Models of Text ponent vector η k and i indexes into the vocabu - lary . A zero - mean Laplace prior has the same ef - fect as placing an L 1 regularizer on η ki , inducing sparsity while at the same time permitting more ex - treme deviations from the mean . The Laplace distri - bution L ( η ; m , σ ) is equivalent to a compound model , (cid:82) N ( η ; m , τ ) E ( τ ; σ ) dτ , where E ( τ ; σ ) indicates the Ex - ponential distribution ( Lange & Sinsheimer , 1993 ; Figueiredo , 2003 ) . This identity is the cornerstone of our inference , which treats the variance τ as a latent variable . We now present a generative story for the incorporation of SAGE in a na¨ıve Bayes classiﬁer : • Draw the background m from an uninformative prior • For each class k – For each term i ∗ Draw τ k , i ∼ E ( γ ) ∗ Draw η k , i ∼ N ( 0 , τ k , i ) – Set β k ∝ exp ( η k + m ) • For each document d – Draw a class y d from a uniform distribution – For each word n , draw w ( d ) n ∼ β y d In general we work in a Bayesian setting , but for the components η we take maximum a posteriori point estimates . Bayesian uncertainty is problematic due to the logistic transformation : even if the expectation (cid:104) η ki (cid:105) = 0 , any posterior variance over η ki would make (cid:104) exp ( η ki + m i ) (cid:105) > (cid:104) exp m i (cid:105) . We resort to a combi - nation of MAP estimation over η and Bayesian infer - ence over all other latent variables — this is similar to the treatment of the topics β in the original formu - lation of latent Dirichlet allocation ( Blei et al . , 2003 ) . The background word distribution m is assumed to be ﬁxed , and we ﬁt a variational distribution over the remaining latent variables , optimizing the bound , (cid:96) = (cid:88) d N d (cid:88) n log P ( w ( d ) n | m , η y d ) + (cid:88) k (cid:104) log P ( η k | 0 , τ k ) (cid:105) + (cid:88) k (cid:104) log P ( τ k | γ ) (cid:105) − (cid:88) k (cid:104) log Q ( τ k ) (cid:105) , ( 2 ) where N d is the number of tokens in document d . 3 . Estimation We now describe how SAGE components can be eﬃ - ciently estimated using a Newton optimization . 3 . 1 . Component means First we address learning the component vectors η . Letting c d represent the vector of term counts for doc - ument d , and C d = (cid:80) i c di , we compute the relevant parts of the bound , (cid:96) ( η k ) = (cid:88) d : c d = k c T d η k − C d log (cid:88) i exp ( η ki + m i ) − η T k diag (cid:0)(cid:10) τ − 1 k (cid:11)(cid:1) η k / 2 ( 3 ) ∂(cid:96) ∂ η k = c k − C k exp ( η k + m ) (cid:80) i exp ( η ki + m i ) − diag (cid:0)(cid:10) τ − 1 k (cid:11)(cid:1) η k = c k − C k β k − diag (cid:0)(cid:10) τ − 1 k (cid:11)(cid:1) η k , ( 4 ) abusing notation so that c k = (cid:80) d : c d = k c d and C k = (cid:80) i c ki . Note that the fraction exp ( η k + m ) (cid:80) i exp ( η ki + m i ) is equal to the term frequency vector β k . The gradient has an intuitive interpretation as the diﬀerence of the true counts c k from their expectation C k β k , minus the di - vergence of η from its prior mean 0 , scaled by the ex - pected inverse - variance . We will use Newton’s method to optimize η , so we ﬁrst derive the Hessian , d 2 (cid:96) dη 2 ki = C k β ki ( β ki − 1 ) − (cid:10) τ − 1 ki (cid:11) , d 2 (cid:96) dη ki dη ki (cid:48) = C k β ki β ki (cid:48) H ( η k ) = C k β k β T k − diag (cid:0) C k β k + (cid:10) τ − 1 k (cid:11)(cid:1) . ( 5 ) The Hessian matrix H is rank - one plus diagonal , so it can be eﬃciently inverted using the Sherman - Morrison formula . For notational simplicity , we elide the class index k , and deﬁne the convenience variable A = diag (cid:0) − ( C β + (cid:10) τ − 1 (cid:11) ) − 1 (cid:1) . We can now derive a Newton optimization step for η , using the gradient g ( η ) = ∂(cid:96)∂ η from Equation 4 : H − 1 ( η ) = A − A C ββ T A 1 + C β T A β − ∆ η = H − 1 ( η ) g ( η ) = A g ( η ) − C A β 1 + C β T A β (cid:104) β T ( A g ( η ) ) (cid:105) , ( 6 ) where the parenthesization deﬁnes an order of opera - tions that avoids forming any non - diagonal matrices . Thus , the complexity of each Newton step is linear in the size of the vocabulary . 3 . 2 . Variances Next we consider the variance ; recall that we have a random vector τ k for every component k . Unlike the components η , we are Bayesian with respect to τ , and construct a fully - factored variational distribution Q τ k ( τ k ) = (cid:81) i Q τ ki ( τ ki ) . We set the form Q τ ki to be a Gamma distribution with parameters (cid:104) a , b (cid:105) : Q ( τ ) = G ( τ ; a , b ) = τ a − 1 exp ( − τ / b ) Γ ( a ) b a , so that (cid:104) τ (cid:105) = ab , (cid:10) τ − 1 (cid:11) = ( ( a − 1 ) b ) − 1 , and (cid:104) log τ (cid:105) = ψ ( a ) + log ( b ) . The prior on τ is an Exponential Sparse Additive Generative Models of Text distribution with parameter γ , so that P ( τ | γ ) = γ exp ( − γτ ) . We can now deﬁne the contribution to the variational bound from Q ( τ ) , (cid:96) ( τ ) = (cid:104) log P ( η | τ ) (cid:105) + (cid:104) log P ( τ | γ ) (cid:105) − (cid:104) log Q ( τ ) (cid:105) ∝ − 1 2 (cid:104) log τ (cid:105) − 1 2 η 2 (cid:10) τ − 1 (cid:11) − γ (cid:104) τ (cid:105) − ( a − 1 ) (cid:104) log τ (cid:105) + (cid:104) τ (cid:105) / b + log Γ ( a ) + a log b ( 7 ) − ∆ a = ( 1 / 2 − a ) ψ 1 ( a ) + 12 η 2 b − 1 ( a − 1 ) − 2 − γb + 1 ( 1 / 2 − a ) ψ 2 ( a ) − ψ 1 ( a ) − η 2 b − 1 ( a − 1 ) − 3 b = 1 + (cid:113) 1 + 8 γη 2 aa − 1 4 γa , obtaining a Newton optimization for a and a closed - form update for b . We use ψ 1 ( a ) and ψ 2 ( a ) indicate the trigamma and quad - gamma functions respectively . For a parameter - free model , we can replace the Ex - ponential prior on τ with an improper Jeﬀrey’s prior , P ( τ ) ∝ 1 / τ . The combination of the Jeﬀrey’s prior P ( τ ) with the Gaussian P ( η | 0 , τ ) no longer yields a Laplace distribution . However , the Normal - Jeﬀrey’s compound distribution also induces sparsity , and re - lieves us from having to choose a value for γ ; moreover , Guan & Dy ( 2009 ) ﬁnd that it yields better results for sparse probabilistic PCA than the Laplace distribu - tion . To derive the variational parameters of Q ( τ ) , we need only replace the term − γ (cid:104) τ (cid:105) in Equation 7 with − (cid:104) log τ (cid:105) . The resulting updates are , − ∆ a = ( 1 / 2 + a ) ψ 1 ( a ) − 12 η 2 b − 1 ( a − 1 ) − 2 − 1 ( 1 / 2 + a ) ψ 2 ( a ) + η 2 b − 1 ( a − 1 ) − 3 b = η 2 a − 1 . These variational parameters are necessary only to compute the bound ; we can directly compute the ex - pectation (cid:10) τ − 1 ki (cid:11) = η 2 ki . Application 1 : Document classiﬁcation Our ﬁrst evaluation is on document classiﬁcation : we test SAGE as a drop - in replacement for the multinomial - Dirichlet that is traditionally used in na¨ıve Bayes text classiﬁers . Both generative mod - els are parameter - free : for SAGE , we use the non - parametric Jeﬀrey’s prior on the variance τ ; for the multinomial - Dirichlet , we perform a coordinate ascent in which a Newton optimization ( Minka , 2003 ) is used to update the precision of the Dirichlet prior . 1 Dis - criminative methods may yield better performance on the document classiﬁcation task , but our goal here is 1 Wallach et al ( 2009a ) ﬁnd that asymmetric Dirichlet priors for term distributions provide no advantage over symmetric priors . 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 proportion of training set a cc u r a cy SAGEDirichlet Figure 2 . Accuracy on 20 Newsgroup text classiﬁcation , as the amount of training data is varied to compare generative models which are amenable to incorporation in the more complex latent variable set - tings described in the remainder of the paper . We evaluate on the classic benchmark “Twenty Newsgroups” data , in which the task is to clas - sify unlabeled newsgroup postings into twenty dif - ferent newsgroups . Using the training / test split from the website http : / / people . csail . mit . edu / jrennie / 20Newsgroups / , there are 11 , 269 training documents ; we randomly subsampled a range of train - ing sets including as few as 5 % of the original doc - uments . We did not perform stopword ﬁltering , and used a vocabulary of 50 , 000 terms . Results are shown in Figure 2 : the amount of training data varied on the x - axis ; each point corresponds to a diﬀerent ran - dom subsample . SAGE substantially outperforms the Dirichlet - multinomial in every experiment . Its advan - tage is particularly robust in the limited - data settings , where the raw improvement in accuracy is more than 10 % . The Jeﬀrey’s prior on τ adaptively controls the sparsity , which increases monotonically from 90 % in the full - data setting to more than 98 % in the mini - mal data setting . Performance gains were smaller in a pilot experiment with a vocabulary of 10 , 000 , suggest - ing that SAGE’s strength is its ability to exploit rare words without overﬁtting . 4 . Latent variable models Next , we consider how to incorporate SAGE in a la - tent variable model of text . We focus on topic models , which contain one latent discrete variable per token , and a latent vector of topic proportions per document . The generative story is similar to the document classi - ﬁcation model the previous section , with the following additions : each document is endowed with a vector of topic proportions θ d ∼ Dirichlet ( α ) ; each token has an associated latent topic z ( d ) n ; and the probability distri - Sparse Additive Generative Models of Text bution for a given token is P ( w ( d ) n | z ( d ) n , η , m ) ∝ exp (cid:16) η z ( d ) n + m (cid:17) . We can combine the mean ﬁeld variational inference for latent Dirichlet allocation ( LDA ) with the varia - tional treatment of τ , optimizing the bound , (cid:96) = (cid:88) d (cid:104) log P ( θ d | α ) (cid:105) + N d (cid:88) n (cid:68) log P ( w ( d ) n | m , η z ( d ) n ) (cid:69) + (cid:68) log P ( z ( d ) n | θ d ) (cid:69) + (cid:88) k (cid:104) log P ( η k | 0 , τ k ) (cid:105) + (cid:88) k (cid:104) log P ( τ k | γ ) (cid:105) − (cid:104) log Q ( τ , z , θ ) (cid:105) . ( 8 ) The updates for Q ( z ) and Q ( θ ) are identical to stan - dard LDA ; the updates for Q ( τ ) remain as Sec - tion 3 . 2 . However , the presence of latent variables slightly changes the MAP estimation for η : (cid:96) ( η k ) = (cid:88) d N d (cid:88) n Q z ( d ) n ( k ) (cid:32) η k − log (cid:88) i exp ( η ki + m i ) (cid:33) − η T k diag (cid:0)(cid:10) τ − 1 k (cid:11)(cid:1) η k / 2 ∂(cid:96) ∂ η k = (cid:104) c k (cid:105) − (cid:104) C k (cid:105) exp ( η k + m ) (cid:80) i exp ( η ki + m i ) − diag (cid:0)(cid:10) τ − 1 k (cid:11)(cid:1) η k , where (cid:104) c ki (cid:105) = (cid:80) d (cid:80) n Q z ( d ) n ( k ) δ ( w ( d ) n = i ) , and (cid:104) C k (cid:105) = (cid:80) i (cid:104) c ki (cid:105) . Thus , the exact counts c k are replaced with their expectations under Q ( z ) . We deﬁne an EM procedure in which the M - step con - sists in iteratively ﬁtting the parameters η and Q ( τ ) . It is tempting to perform a “warm start” by intializing with the values from a previous iteration of the outer EM loop . However , these parameters are tightly cou - pled : as the component mean η ki goes to zero , the expected variance (cid:104) τ ki (cid:105) is also driven to zero ; once (cid:104) τ ki (cid:105) is very small , η ki cannot move away from zero regardless of the expected counts c k . This means that a warm start risks locking in a sparsity pattern dur - ing the early stages of EM which may be far from the global optimum . There are two solutions : either we abandon the warm start ( thus expending more compu - tation ) , or we do not iterate to convergence in each M - step ( thus obtaining noisier and less sparse solutions , initially ) . Fortunately , pilot experiments showed that good results can be obtained by performing just one iteration in each M - step , while using the warm start technique . Application 2 : Sparse topic models Our second evaluation applies the SAGE Topic Model to the benchmark NIPS dataset . 2 Following the eval - uation of Wang and Blei ( 2009 ) , we subsample to 10 % 2 http : / / www . cs . nyu . edu / ∼ roweis / data . html number of topics pe r p l e x i t y 2100 2150 2200 2250 2300 2350 l l l 10 25 50 system Dirichlet−Multinomial SAGE Figure 3 . Perplexity results for SAGE and latent Dirichlet allocation on the NIPS dataset . 2 4 6 8 10 0 . 05 0 . 1 0 . 15 0 . 2 0 . 25 0 . 3 term frequency quantile p r opo r t i on o f t o t a l v a r i a t i on SAGEDirichlet−multinomial Figure 4 . Proportion of total variation committed to words at each frequency decile . Dirichlet - multinomial LDA makes large distinctions in the topic - term frequencies of very rare words , while SAGE only distinguishes the topic - term frequencies of words with robust counts . of the tokens in each document , and hold out 20 % of the documents as a “test set” on which to evalu - ate predictive likelihood . We limit the vocabulary to the 10 , 000 terms that appear in the greatest number of documents ; no stopword pruning is applied . Over - all this leaves 1986 training documents with 237 , 691 tokens , and a test set of 497 documents and 57 , 427 tokens . We evaluate perplexity using the Chib - style estimation procedure of Wallach et al . ( 2009b ) . For comparison , we implement variational latent Dirichlet allocation , making maximum - likelihood updates to a symmetric Dirichlet prior on the topic - term distribu - tions . Results are shown in Figure 3 , using box plots over ﬁve paired random initializations for each method . SAGE outperforms standard latent Dirichlet alloca - tion as the number of topics increases ; with both 25 and 50 topics , every SAGE run outperformed its coun - terpart Dirichlet - multinomial . As in the classiﬁcation task , SAGE controls sparsity adaptively : as the num - ber of topics increases from 10 to 50 , the proportion of non - zero weights decreased ﬁve - fold ( from 5 % to 1 % ) , holding the total model complexity constant . Sparse Additive Generative Models of Text Figure 4 compares the overall term frequency ( mea - sured in deciles and shown on the x - axis ) with the amount of topic - term variation that each model ac - cords , for a single run with 50 topics . In SAGE , the to - tal variation for a term i is (cid:80) k | η ki | , while in Dirichlet - multinomial LDA , we measure the total variation from the mean log frequency , (cid:80) k | β ki − β i | . The ﬁgure shows that SAGE admits very little topical variation for low frequency words . In contrast , the Dirichlet - multinomial displays little sensitivity to the overall term frequency , and actually assigns more topical vari - ation to the lowest frequency terms . Note that our im - plementation of Dirichlet - multinomial LDA incorpo - rates a symmetric Dirichlet prior which acts to smooth the topic - term frequencies of rare words — even so , it overﬁts the training data and learns widely divergent probabilities for these words . We believe that this phe - nomenon explains the better predictive performance obtained by the SAGE topic model — by focusing on high - frequency terms with accurate counts , it learns more robust topics . A related point is that the topics induced by standard LDA may be more diﬃcult to in - terpret , because the rare words may cause documents to be assigned to topics in a way that is not predictable from simply examining the most salient terms in each topic . 5 . Multifaceted generative models Finally , we consider how SAGE can be used to combine multiple generative facets . We focus on models that combine per - word latent topics and document labels , thus oﬀering a structured view of labels and topics — for example , revealing the words and documents that reﬂect a left - wing perspective on education policy . Ex - isting multifaceted generative models ( Mei et al . , 2007 ; Paul & Girju , 2010 ; Ahmed & Xing , 2010 ) incorporate latent “switch” variables that determine whether each word token is generated from a topic or from a distri - bution associated with the document label ( as in the left panel of Figure 1 ) . If the token is to be drawn from a topic , then an additional latent variable determines which topic will be active . Topic - label interactions can also be included , capturing the distributions of words at the intersection of , say , topic and ideology . The number of parameters thus becomes very large , grow - ing to the product of the vocabulary size , the number of topics , and the number of labels — plus the addi - tional switching variable per token . Collapsed Gibbs sampling can analytically marginalize the topic and label word distributions , but it still may suﬀer from high variance if the number of parameters is too large compared to the training data . θ α z η ( T ) ki τ ( T ) ki w η ( I ) jki τ ( I ) jki y η ( A ) ji τ ( A ) ji m i i ∈ { 1 , . . . , W } Figure 1 : A plate diagram for the SAGE topic - aspect model . 1 Figure 5 . Plate diagram for a multifaceted topic model us - ing SAGE . SAGE enables multifaceted topic models that encour - age sparse variation from the background term distri - bution , while eliminating the need for switching vari - ables . 3 A plate diagram is shown in Figure 5 . On the left , we have the standard document plate from latent Dirichlet allocation , augmented with the observed la - bel y . On the right , we have an outer plate that makes explicit the repetition across all W words in the vocab - ulary . Within this plate , we have the observed back - ground term frequency m i , as well as sparse deviations for : each topic η ( T ) ki , k < K ; each label distribution η ( A ) ji , j < A ; and each topic - label interaction , η ( I ) jki . The variance parameters from the compound Normal - Jeﬀrey’s distribution are shown as τ ( T ) ki , etc . The gen - erative probability for a single token is obtained by adding the SAGE components to the prior term fre - quencies : P ( w ( d ) n | z ( d ) n , η , m , y d ) ∝ exp (cid:18) η ( T ) z ( d ) n + η ( A ) y d + η ( I ) y d , z ( d ) n + m (cid:19) . Estimation is very similar to the models encountered earlier in the paper . For the topic components η ( T ) , we obtain the gradient , ∂(cid:96) ∂ η ( T ) k = (cid:68) c ( T ) k (cid:69) − (cid:88) j (cid:104) C jk (cid:105) β jk − diag ( (cid:104) ( τ ( T ) k ) − 1 (cid:105) ) η ( T ) k , where β jk ∝ exp (cid:16) η ( T ) k + η ( A ) j + η ( I ) jk + m (cid:17) and (cid:104) C jk (cid:105) gives the expected counts for each topic - label combi - nation . Using this gradient , we can apply the Newton optimization as before , substituting (cid:80) j C jk β jk into the Hessian in place of C k β k ( see Equations 5 and 6 ) . The updates for η ( A ) are almost identical , but we have exact counts C ( A ) j , as the labels are observed . The in - 3 Zhu et al . ( 2006 ) also augment LDA topics using addi - tion of log term frequencies ( for per - token labels ) , but they did not employ sparsity . Sparse Additive Generative Models of Text 0 10 20 30 40 50 0 . 64 0 . 66 0 . 68 0 . 7 0 . 72 number of topics a cc u r a cy Figure 6 . SAGE’s accuracy on the ideological perspective task ; the state - of - the - art is 69 . 1 % ( Ahmed & Xing , 2010 ) . teraction components η ( I ) jk depend on exactly one la - bel distribution η ( A ) j and one topic η ( T ) k , so we can use C jk β jk directly without computing any sums . Application 3 : Topic and ideology We ﬁrst evaluate on a publicly - available dataset of po - litical blogs describing the 2008 U . S . presidential elec - tion ( Eisenstein & Xing , 2010 ) . There are a total of six blogs — three from the right and three from left — comprising 20 , 827 documents , 5 . 1 million word tokens , and a vocabulary of 8284 items . The task is to predict the ideological perspective of two unlabeled blogs , us - ing the remaining four as a training set . We strictly follow the experimental procedure of Ahmed & Xing ( 2010 ) , allowing us to compare with their reported re - sults directly . 4 Ahmed and Xing considered three ideology - prediction tasks , and found that the six - blog task was the most diﬃcult : their Multiview LDA model achieves accu - racy between 65 % and 69 . 1 % depending on the num - ber of topics . They ﬁnd a comparable result of 69 % using support vector machines ; alternative latent vari - able models Discriminative LDA ( Lacoste - Julien et al . , 2008 ) and Supervised LDA ( Blei & McAuliﬀe , 2007 ) do worse . Our results are shown in Figure 6 , reporting the median across ﬁve random initialization at each setting for K . Our best median result is 69 . 6 % , at K = 30 , equalling the state - of - the - art ; our best individual run achieves 71 . 9 % . Our model obtains sparsity of 93 % for topics , 82 % for labels , and 99 . 3 % for topic - label interactions ; nonetheless , pilot experiments show that the absence of topic - label interactions reduces perfor - mance substantially . Application 4 : Geolocation from text We now consider the setting in which the label is it - self a latent variable , generating both the text ( as de - scribed above ) and some additional metadata . This is the setting for the Geographical Topic Model , in which a latent “region” helps to select the distributions that 4 The sole exception is that we learn the prior α from data , while Ahmed & Xing set it manually . Table 1 . Prediction error for Twitter geolocation . error in kilometers : median mean ( Eisenstein et al . , 2010 ) 494 900 ( Wing & Baldridge , 2011 ) 479 967 SAGE 501 845 generate both text and observed GPS locations ( Eisen - stein et al . , 2010 ) . By training on labeled examples in which both text and geolocation are observed , the model is able to make predictions about the GPS lo - cation of unlabeled authors . The Geographic Topic Model induces region - speciﬁc versions of each topic by chaining together log - Normal distributions . This is equivalent to an additive model in which both the topic and the topic - region inter - action exert a zero - mean Gaussian deviation from a background language model . SAGE diﬀers by allow - ing eﬀects that are region - speciﬁc but topic - neutral , and by inducing sparsity . We follow the tuning proce - dures from Eisenstein et al . ( 2010 ) exactly : the number of latent regions is determined by running a Dirichlet process mixture model on the location data alone , and the number of topics is tuned against a development set . We also present more recent results from Wing & Baldridge ( 2011 ) , who use a nearly identical dataset , but include a larger vocabulary . As shown in Table 1 , SAGE achieves the best mean error of any system on this task , though Wing & Baldridge ( 2011 ) have the best median error . 6 . Related work Sparse learning ( Tibshirani , 1996 ; Tipping , 2001 ) typ - ically focuses on supervised settings , learning a sparse set of weights that minimize a loss on the training la - bels . Two recent papers apply sparsity to topic mod - els . Williamson et al . ( 2010 ) induce sparsity in the topic proportions by using the Indian Buﬀet Process to represent the presence or absence of topics in a docu - ment . More closely related is the SparseTM of Wang & Blei ( 2009 ) , which induces sparsity in the topics them - selves using a spike - and - slab distribution . However , the notion of sparsity is diﬀerent : in the SparseTM , the topic - term probability distributions can contain zeros , while in SAGE , each topic is a set of sparse deviations from a background distribution . Inference in the SparseTM requires computing a combinatorial sum over all sparsity patterns , while in our case a rel - atively simple coordinate ascent is possible . Sparse dictionary learning provides an alternative ap - proach to modeling document content with sparse bases ( Jenatton et al . , 2010 ) . In general , such ap - Sparse Additive Generative Models of Text proaches emphasize sparsity in the number of dictio - nary components that are active for a given docu - ment . However , the application of a sparsity - inducing prior to the dictionary components would be similar to SAGE . The fundamental diﬀerence is that SAGE is a generative model that deﬁnes the probability of each token ; as such , it can easily be embedded in larger latent variable structures . 7 . Conclusion We have presented SAGE , a new generative model for discrete data . Each token is generated by adding back - ground log - probabilities to a set of sparse variation vectors associated with each generative factor . Ap - plying SAGE to na¨ıve Bayes classiﬁcation and topic modeling , we ﬁnd that it learns simpler models with better predictive performance . We feel that the most promising feature of SAGE is its facilitation of the con - struction of multifaceted generative models . We plan to explore the application of SAGE to even richer mul - tifaceted generative models , such as hierarchical topic - aspect models and mixed - eﬀects models that account for author - speciﬁc linguistic patterns . Acknowledgments We thank Arthur Gretton , Noah A . Smith and Jun Zhu for helpful discussions . Our im - plementation relied heavily on Tom Minka’s Lightspeed li - brary , and we used evaluation code from Wallach et al . ( 2009b ) and Brendan O’Connor . This research was sup - ported by AFOSR FA95501010247 , ONR N000140910758 , NSF IIS - 0713379 , NSF DBI - 0546594 , and an Alfred P . Sloan Fellowship . References Ahmed , Amr and Xing , Eric P . Staying informed : super - vised and semi - supervised multi - view topical analysis of ideological perspective . In Proceedings of EMNLP , pp . 1140 – 1150 , 2010 . Blei , D . M . and McAuliﬀe , J . D . Supervised topic models . In NIPS , 2007 . Blei , D . M . , Ng , A . Y . , and Jordan , M . I . Latent Dirichlet allocation . JMLR , 3 : 993 – 1022 , 2003 . Chemudugunta , Chaitanya , Smyth , Padhraic , and Steyvers , Mark . Modeling General and Speciﬁc Aspects of Documents with a Probabilistic Topic Model . In Ad - vances in Neural Information Processing Systems , 2006 . Eisenstein , Jacob and Xing , Eric . The CMU 2008 political blog corpus . Technical report , Carnegie Mellon Univer - sity , 2010 . Eisenstein , Jacob , O’Connor , Brendan , Smith , Noah A . , and Xing , Eric P . A latent variable model of geographic lexical variation . In Proceedings of EMNLP , 2010 . Figueiredo , M´ario A . T . Adaptive sparseness for supervised learning . Pattern Analysis and Machine Learning , 2003 . Griﬃths , Thomas L . , Steyvers , Mark , Blei , David M . , and Tenenbaum , Joshua B . Integrating topics and syntax . In Neural Information Processing Systems , pp . 537 – 544 , 2005 . Guan , Yue and Dy , Jennifer G . Sparse probabilistic prin - cipal component analysis . In Proceedings of AISTATS , 2009 . Jenatton , R . , Mairal , J . , Obozinski , G . , and Bach , F . Prox - imal methods for sparse hierarchical dictionary learning . In Proceedings of ICML , 2010 . Lacoste - Julien , S . , Sha , F . , and Jordan , M . I . DiscLDA : Discriminative learning for dimensionality reduction and classiﬁcation . In Neural Information Processing Sys - tems , 2008 . Lange , Kenneth and Sinsheimer , Janet S . Nor - mal / Independent Distributions and Their Applications in Robust Regression . Journal of Computational and Graphical Statistics , 2 ( 2 ) , 1993 . Mei , Q . , Ling , X . , Wondra , M . , Su , H . , and Zhai , C . X . Topic sentiment mixture : modeling facets and opinions in weblogs . In Proceedings of WWW , 2007 . Minka , T . P . Estimating a Dirichlet distribution . Technical report , Massachusetts Institute of Technology , 2003 . Paul , M . and Girju , R . A two - dimensional topic - aspect model for discovering multi - faceted topics . In Proceed - ings of AAAI , 2010 . Tibshirani , Robert . Regression Shrinkage and Selection via the Lasso . Journal of the Royal Statistical Society . Series B ( Methodological ) , 58 ( 1 ) : 267 – 288 , 1996 . Tipping , Michael E . Sparse Bayesian Learning and the Relevance Vector Machine . Journal of Machine Learning Research , 1 : 211 – 244 , 2001 . Wallach , Hanna M . , Mimno , David , and McCallum , An - drew . Rethinking LDA : Why Priors Matter . In Neural Information Processing Systems , 2009a . Wallach , Hanna M . , Murray , Iain , Salakhutdinov , Ruslan , and Mimno , David . Evaluation methods for topic mod - els . In Proceedings of ICML , pp . 1105 – 1112 , 2009b . Wang , Chong and Blei , David M . Decoupling sparsity and smoothness in the discrete hierarchical dirichlet process . In Neural Information Processing Systems , 2009 . Williamson , S . , Wang , C . , Heller , K . , and Blei , D . The IBP compound dirichlet process and its application to focused topic modeling . In Proceedings of ICML , 2010 . Wing , Benjamin and Baldridge , Jason . Simple supervised document geolocation with geodesic grids . In Proceed - ings of ACL , 2011 . Zhu , Xiaojin , Blei , David M . , and Laﬀerty , John . Taglda : Bringing document structure knowledge into topic mod - els . Technical Report TR - 1553 , University of Wisconsin , 2006 .