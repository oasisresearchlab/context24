Maximizing Submodular Functions for Recommendation in the Presence of Biases Anay Mehrotra Nisheeth K . Vishnoi Yale University Yale University USA USA ABSTRACT Subset selection tasks , arise in recommendation systems and search engines and ask to select a subset of items that maximize the value for the user . The values of subsets often display diminishing returns , and hence , submodular functions have been used to model them . If the inputs defning the submodular function are known , then existing algorithms can be used . In many applications , however , inputs have been observed to have social biases that reduce the util - ity of the output subset . Hence , interventions to improve the utility are desired . Prior works focus on maximizing linear functions— a special case of submodular functions—and show that fairness constraint - based interventions can not only ensure proportional representation but also achieve near - optimal utility in the presence of biases . We study the maximization of a family of submodular functions that capture functions arising in the aforementioned ap - plications . Our frst result is that , unlike linear functions , constraint - based interventions cannot guarantee any constant fraction of the optimal utility for this family of submodular functions . Our second result is an algorithm for submodular maximization . The algorithm provably outputs subsets that have near - optimal utility for this family under mild assumptions and that proportionally represent items from each group . In empirical evaluation , with both synthetic and real - world data , we observe that this algorithm improves the utility of the output subset for this family of submodular functions over baselines . KEYWORDS submodular maximization , recommendation systems , subset selec - tion , algorithmic fairness ACM Reference Format : Anay Mehrotra and Nisheeth K . Vishnoi . 2023 . Maximizing Submodular Functions for Recommendation in the Presence of Biases . In Proceedings of the ACM Web Conference 2023 ( WWW ’23 ) , April 30 – May 04 , 2023 , Austin , TX , USA . ACM , New York , NY , USA , 12 pages . https : / / doi . org / 10 . 1145 / 3543507 . 3583195 1 INTRODUCTION Subset selection arises in many web - based applications including diferent types of content recommendation systems [ 13 , 22 , 56 , 65 ] and search engines [ 2 ] . Generally speaking , in all of these applica - tions , given a set of  items ( e . g . , posts , products , videos , or websites ) , This work is licensed under a Creative Commons Attribution - NonCommercial International 4 . 0 License . WWW ’23 , April 30 – May 04 , 2023 , Austin , TX , USA © 2023 Copyright held by the owner / author ( s ) . ACM ISBN 978 - 1 - 4503 - 9416 - 1 / 23 / 04 . https : / / doi . org / 10 . 1145 / 3543507 . 3583195 the task is to select a subset  of size  that is the most valuable for the user . Submodular functions are often used to capture the utility of subsets of items because of the diminishing returns property that arises in the above applications [ 2 , 24 , 42 ] . Diminishing returns arise because of the fact that an item’s value to a user depends on the other items shown to the user [ 2 , 12 , 22 , 45 , 65 ] . For instance , in a recommendation system , where items belong to diferent cat - egories ( such as genres or product types ) , each additional item from the same category provides a diminishing value to the user [ 22 , 56 , 65 ] . Similarly , in web search , results belong to diferent cate - gories ( based on , e . g . , relevance to technology , news , locations , etc . ) and each additional result from the same category adds diminishing value [ 2 ] . In settings with multiple stakeholders ( e . g . , multiple users , content creators , and platform ) , adding item  to a subset containing another item , relevant to the same stakeholder as  , has a lower increment in utility than adding  to a subset not containing such an item [ 24 ] . Submodular functions model such diminishing returns . Formally , a set function  is said to be submodular if , for any two subsets  ⊆  and item  ∉  , the increase in value on adding  to  is at least as large as the increase in value on adding  to  , i . e . ,  (  ∪ {  } ) −  (  ) ≥  (  ∪ {  } ) −  (  ) . In the above applications , given  items and an upper - bound 1 ≤  ≤  on the maximum number of selected items , at a high level , the goal is to solve the following maximization program for a suitable submodular function  : 2 [  ] → R max  ⊆ [  ] : |  | ≤   (  ) . ( 1 ) A family of submodular functions . There is a vast literature on maximizing submodular functions [ 25 , 26 , 30 , 42 , 53 , 57 ] . This literature studies various types of submodular functions . We focus on a family of submodular functions that captures many functions used in recommendation and web search . In these applications , each item has  attributes and an item  ∈ { 1 , 2 , . . . ,  } generates a value or utility    ≥ 0 for a user ( or stakeholder ) who is look - ing for items with the  - th attribute . For instance , on Amazon Music , [ 56 ] choose    to encode the utility of song  for users interested in songs from genre  . They use the submodular func - tion  (  ) = Í   = 1 log ( 1 + Í  ∈     ) to capture the value of a set  of song recommendations . Another example is [ 13 ] who , roughly speaking , set    to encode the utility of song  for stakeholder  . Í Í  √Í They use  (  ) =  ∈    1 +  ∈     to capture the value  = 2 of a playlist  on Spotify . Generalizing these examples , we consider the following family of submodular functions : Given  increasing concave functions  1 ,  2 , . . . ,   : R ≥ 0 → R ≥ 0 , corresponding to each attribute , and utilities  , the submodular function  is ∀  ⊆ [  ] ,  (  ) B Í   = 1   ( Í  ∈     ) . ( 2 ) 3625 WWW ’23 , April 30 – May 04 , 2023 , Austin , TX , USA Anay Mehrotra and Nisheeth K . Vishnoi Here ,  captures an item’s utility in “isolation” and the concavity of  1 ,  2 , . . . ,   ensures that the utilities of sets of items display diminishing returns . 1 We denote the above family of functions by F . F captures the function in [ 56 ] when   (  ) = log ( 1 +  ) for all  and  . It also captures the function considered by [ 13 ] when √  1 (  ) =  and  2 (  ) = · · · =   (  ) =  for all  . Moreover , in Appendix A , we show that F captures functions used by [ 2 , 65 ] . If the utilities  are accurately known , then one can use stan - dard algorithms to approximately solve Program ( 1 ) ( see Section 2 ) . However , in the above applications , the utilities are often derived from users ; either directly from users’ feedback or indirectly from predictions of learning algorithms trained on user data . Hence , soci - etal biases can creep into such observed utilities . Consequently , the subset maximizing the objective defned by these observed utilities can have a sub - optimal value with respect to the objective  defned by the true or latent utilities . Bias in inputs . There are various mechanisms through which biases can arise in observed utilities . If certain social groups are overrepresented in the data compared to their proportion in the user base , then utilities derived from this data will be skewed toward the opinions of these groups . For instance , the IMDB rating of the 2016 remake of Ghostbusters was “sabotaged by a faction of fans who appeared to be upset by its all - female cast” [ 6 , 50 ] . Apart from such explicit biases , humans also have unconscious implicit biases [ 7 , 11 , 28 , 44 , 68 ] . Humans’ implicit biases can manifest in data and , in turn , introduce skews in the utilities [ 11 , 21 ] . Bias can also arise due to diferences in user characteristics across socially - salient groups . For instance , [ 39 ] observe that SOTA text summarization algorithms output summaries that under - represent minority dialects of English . [ 39 ] postulate that this is due to “structural diferences” across dialects ( e . g . , diferences in lengths of sentences or of Tweets ) . When algorithms are used to estimate utilities , such algorithmic biases can introduce skews in estimated utilities . A model of bias . To capture such skews , we consider a model that extends the model in [ 41 ] . In this model , items belong to one of  disjoint groups  1 ,  2 , . . . ,   . Each group  ℓ has an unknown and increasing bias function  ℓ : R ≥ 0 → R ≥ 0 . The observed utilities of items  in group  ℓ are defned as follows ( ) ∀ 1 ≤  ≤  ,  b   B  ℓ    . ( 3 ) In the above examples , the groups  1 ,  2 , . . . ,   can correspond to the set of movies whose protagonists are male ( respectively non - male ) and the set of tweets written in Standard English ( respectively African American English ) . To gain some intuition , consider the special case studied in [ 41 ] : for each ℓ and  ,  ℓ (  ) =  ℓ ·  for some parameter 0 <  ℓ < 1 . In this case , the observed utilities of items in group  ℓ are  ℓ times smaller than their latent utilities . To see a concrete connection to the above examples , consider user - given ratings used for recommendation : suppose  fraction of the users are explicitly biased and give female - led movies a score of 1 ( lowest ) . This reduces the score of a female - led movie 1 To see this , imagine we have multiple items  √ ∈ { 1 , 2 , . . . } and the utility of each item  on the frst attribute is   1 = 1 . Let  (  ) be  for each  , which is increasing and concave . Selecting the frst item increases the utility of the selection  ( 1 ) −  ( 0 ) = 1 . Further selecting the second item increases the utility by a smaller amount  ( 2 ) −  ( 1 ) ≈ 0 . 414 , and further yet , selecting the third item increases the utility by an even smaller amount  ( 3 ) −  ( 2 ) ≈ 0 . 318 . from its true value of , say  , to  ( 1 −  ) +  . One can model this skew using by setting  ℓ to be the set of female - led movies and  ℓ (  ) =  · ( 1 −  ) +  for each  . The above setup can also be used to model algorithmic biases . For instance , consider a ( hypothetical ) algorithm that scores Tweets proportional to the TF - IDF score of the text in the Tweet . Since the TF - IDF score is known to be lower for text with more “common words” [ 46 ] , Tweets in dialects  that use a higher fraction , say   , of common words , receive lower scores . This can be modeled with  ℓ (  ) =  · ℎ (   ) for some decreasing function ℎ , where group  ℓ consists of all Tweets in dialect  . Since a platform that does not observe the latent utilities  , it naturally outputs the subset  b that maximizes the function  b defned by observed utilities  b : b b  B argmax  ⊆ [  ] : |  | ≤   (  ) , Í where for all  ⊆ [  ] ,  b (  ) B Í  = 1   (  b   ) .   ∈  Since  b optimizes a diferent objective than  , the latent utility of  ,  (  b ) , can be smaller than the optimal latent utility , OPT B max  ⊆ [  ] : |  | ≤   (  ) . Given  b , can we fnd a set  such that  (  ) is close to OPT ? Related work . Recent works [ 17 , 41 , 47 ] have studied the above model in the special case where the objective  is a linear function – a specifc type of functions in F – and the bias functions , for each ℓ , are  ℓ (  ) =  ℓ ·  for some parameters 0 <  1 ,  2 , . . . ,   < 1 . These works explore if requiring the output  to satisfy fairness constraints can improve its latent utility  (  ) . Various fairness constraints have been considered in practice [ 5 , 66 ] : Equal representation requires  to have at most  /  items from each group  ℓ and proportional representation requires  to have at most  · |  ℓ | /  items from each group  ℓ . Generalizations of these constraints have also been considered , given values  ℓ , generalizations require |  ∩  ℓ | ≤  ℓ ·  respectively for each ℓ . There are many reasons to use fairness constraints , including , ethical and legal ones [ 5 , 60 , 61 , 73 ] . [ 17 , 41 , 47 ] demonstrate that another beneft of fairness constraints is that they can improve the latent utility of the output . Given  = (  1 ,  2 , . . . ,   ) , let   be the subset maximizing the observed utility  b (   ) subject to satisfying the constraint specifed by  . In Í the special case , where  is linear ( e . g . ,  (  ) B  ∈    1 ) and latent utilities  are drawn i . i . d . from some distribution , [ 17 , 47 ] show that if  captures proportional representation , then  (   ) ≥ ( 1 −   ( 1 ) ) · OPT . Thus , a natural question is if there is a  such that  (   ) is close to OPT for a submodular function  in the family F . Stochasticity in groups . [ 17 , 41 , 47 ] assume that entries of  are drawn i . i . d . from some distribution . We weaken this assumption : we let  be arbitrary and assume that the groups  1 ,  2 , . . . ,   are generated stochastically – independent of  . This is done by uni - formly sampling |  1 | distinct items and assigning them to  1 , then sampling |  2 | distinct items ( from those remaining ) and assigning them to  2 , and so on . This captures the belief that there are no systematic diferences in the latent utilities of items in diferent groups . If , in addition , entries of  are also sampled i . i . d . , then this is equivalent to the model of [ 17 , 41 , 47 ] . Further discussion of the model and results of [ 17 , 41 , 47 ] appears in Appendix D . 3626 Maximizing Submodular Functions for Recommendation in the Presence of Biases WWW ’23 , April 30 – May 04 , 2023 , Austin , TX , USA Our contributions . We begin by studying the efectiveness of fairness constraints to achieve a high latent utility . Our frst result shows that for any  > 0 and upper bound parameters  , there is a submodular function  in F , latent utilities  , and functions  1 ,  2 , . . . ,   , such that with high probability , the latent utility of   is at most  · OPT ( Theorem 4 . 1 ) . Thus , no choice of  can ensure that  (   ) is close to OPT . This result holds with the “multiplicative” bias functions  1 ,  2 , . . . ,   studied in [ 17 , 41 , 47 ] . Hence , it con - trasts the result of [ 17 , 41 , 47 ] that for any linear  , there is always a  specifying fairness constraints such that  (   ) ≥ ( 1 −   ( 1 ) ) · OPT . This shows a diference in the efectiveness of fairness constraints with linear objective functions compared to submodular functions . On the positive side , we give an algorithm for submodular maxi - mization in the presence of biases ( Algorithm 1 ) . Algorithm 1 can be used with any submodular function  in the family F . If each item  has a non - zero utility for at most one attribute ( Assumption 1 ) , then the algorithm provably outputs a subset with near - optimal la - tent utility ( Theorem 4 . 3 ) . Assumption 1 is natural in some settings : for instance , [ 2 ] used the assumption in the context of web search . Moreover , the assumption is also satisfed by the construction in our frst result . Concretely , we show that under Assumption 1 , given observed utilities  b , Algorithm 1 outputs a subset  whose latent utility is at least ( 1 −  (  − 1  2  − 1 / 4 ) ) · OPT , where  > 0 is the minimum value such that all non - zero entries of  are between  and  − 1 ( Theorem 4 . 3 ) . Algorithm 1 difers from the standard greedy algorithms for con - strained monotone submodular maximization [ 26 , 54 , 57 ] : Roughly speaking , given constraints , greedy algorithms iteratively select the item with the highest marginal utility ( or an approximation of marginal utility ) . Algorithm 1 , on the other hand , frst computes the “right” constraints for the given data and , then , performs sub - modular maximization subject to the computed constraints . Empirically , we evaluate the performance of Algorithm 1 when Assumption 1 does not hold . We run simulations on the Movie - Lens 20M [ 32 ] and two synthetic datasets and compare against the baselines Uncons and ProportionalRepr , which output the subset maximizing the observed utility and the subset maximizing the observed utility subject to satisfying proportional representation respectively . We fx  1 (  ) =  and  ℓ (  ) =  ·  for each ℓ ≠ 1 . In simulations with synthetic datasets , we observe that Algorithm 1 achieves a latent utility higher than 0 . 95 times the latent utility achieved when  = 1 ( i . e . , there is no bias ) , even for small values of  (  ≤ 0 . 01 ) . Whereas , Uncons outputs subsets whose latent utility decreases with  and for  < 0 . 1 is up to 12 % smaller than OPT . On MovieLens 20M [ 32 ] , we observe that the predicted rel - evance scores in the data are disproportionately higher ( by up to 3 times ) for movies led by male actors compared to movies led by non - male actors in genres stereotypically associated with men . In contrast , user ratings for these sets of movies are within 6 % of each other in all genres . We use these ( biased ) relevance scores to recommend movies from sets of men - stereotypical genres and eval - uate the performance of recommended movies with user ratings . Algorithm 1 outperforms Uncons and ProportionalRepr by 3 % or more on 14 / 31 genre sets and has a similar as ( within 1 % ) or better performance than Uncons and ProportionalRepr on more than 87 % sets of men - stereotypical genres . 2 OTHER RELATED WORK Fairness in information retrieval and recommendation . Infor - mation retrieval and recommendation systems ( such as personalized feed generators , news recommenders , and search engines ) have a signifcant societal infuence [ 58 ] . They are one of the primary sources of information for individuals [ 20 , 45 , 58 , 61 ] , who , in turn , impart signifcant trust to these systems – tending to agree with their outputs [ 19 ] and to follow their suggestions [ 29 ] . Without fairness considerations , the outputs of existing systems have been observed to encode various societal biases [ 59 ] – leading to underrepresenta - tion of some social groups [ 37 ] , the polarization of user opinions [ 51 ] , and denial of economic opportunities available to individuals [ 31 ] . Consequently , a growing body of works design interventions to mitigate the adverse efects of biases [ 20 , 45 , 61 ] . These works can be broadly divided into those mitigating adverse efects on the users [ 1 , 36 , 71 ] , those mitigating adverse efects on the items ( denoting providers such as journalists in news recommendation , artists in song recommendation , and individuals on online hiring platforms ) [ 8 , 27 , 63 , 69 , 70 , 72 ] , and those considering both [ 48 , 49 ] . Works in each of these categories take diverse approaches : from modifying the relevance estimation pipeline to satisfy fairness criteria [ 70 – 72 ] , to requiring the output to satisfy fairness constraints [ 8 , 27 , 63 , 69 ] , to modifying the objective of system to capture fairness metrics [ 1 , 36 , 48 , 49 ] . We focus on harm for the items or providers due to biases in the input and examine the efcacy of fairness constraints to mitigate these harms when the output’s utility is captured by a submodular function . Unlike this work , most prior works assume that the input data is accurate and is absent of biases . Submodular maximization . There is a vast literature on maximiz - ing submodular functions subject to diferent types of constraints [ 25 , 26 , 30 , 42 , 53 , 57 ] . Among these , cardinality constraints are of specifc interest . These are constraints of the form |  | ≤  for some fxed  . Given a cardinality constraint and an evaluation oracle for  , the standard greedy algorithm of [ 57 ] selects a sub - set  of size  such that  (  ) ≥ ( 1 −  − 1 ) · OPT while making at most   evaluations of  and doing at most  (   ) additional arithmetic operations [ 57 ] , where OPT = max |  | ≤   (  ) . Several variants of this algorithm have also been designed . These variants extend the constant - factor approximation guarantee to other types of constraints ( including upper bounds stated in Section 1 ) , improve its running time , and design distributed variants of the algorithm [ 14 , 52 , 54 , 55 ] . Finally , motivated by context - specifc fairness re - quirements , a number of recent works [ 3 , 4 , 9 , 10 , 16 , 67 ] also study submodular maximization in the presence of constraints beyond the family of constraints introduced in Section 1 . However , unlike the present work , these works , assume that one can evaluate the true function  which may not be possible in the presence of biases . 3 MODEL Let there be  items , indexed by the set of values [  ] B { 1 , 2 , . . . ,  } . A set function  : 2 [  ] → R is said to be submodular if for each pair of subsets  ⊆  ⊆ [  ] and item  ∈ [  ] ,  (  ∪ {  } ) −  (  ) ≥  (  ∪ {  } ) −  (  ) . We consider the following family of submodular functions that are studied in the context of content recommendation [ 13 , 22 , 56 , 65 ] and web search [ 2 ] . Definition 3 . 1 ( A family of submodular functions ) . F is the family of all submodular functions  that are parameterized by a 3627 | | | | | | WWW ’23 , April 30 – May 04 , 2023 , Austin , TX , USA Anay Mehrotra and Nisheeth K . Vishnoi number  , increasing concave functions  1 ,  2 , . . . ,   : R ≥ 0 → R ≥ 0 , and matrix  ∈ R  ×  as follows :  (  ) B Í  = 1   ( Í  ∈     ) . ≥ 0  Setting   (  ) = log ( 1 +  ) for all  and  in the above defnition , we get the submodular function tested by [ 56 ] on Amazon Music . √ With  1 (  ) =  and   (  ) =  for all  and  ≠ 1 , we get the submodular function used by [ 13 ] to measure the quality of song playlists . Further examples appear in Appendix A . Given a suitable  ∈ F and a number  , the goal in our motivat - ing applications is to solve the following program : max  ⊆ [  ] : |  | ≤   (  ) . If  and , hence ,  is known , then one can hope to fnd a subset  such that  (  ) is close to the optimal value , OPT , of this program . However , as discussed , in many contexts , the utilities observed by a platform  b can encode societal biases , and hence , be diferent from the true or latent utilities  . Here , we consider a model of bias that builds on [ 17 , 41 ] . In this model , items belong to one of  disjoint groups  1 ,  2 , . . . ,   . We weaken the assumption in [ 17 , 41 ] ( and related models in [ 47 ] ) by allowing  to be arbitrary and requiring  1 ,  2 , . . . ,   to be generated stochastically . In particular , given sizes |  1 | , |  2 | , . . . ,   ,  1 is constructed by selecting |  1 | items | | uniformly without replacement ,  2 is constructed by selecting |  2 | items uniformly from those remaining , and so on . We defne  > 0 to be a constant such that for each 1 ≤ ℓ ≤  |  ℓ | ≥   . The model of bias is as follows . Definition 3 . 2 ( Model of bias ) . For each ℓ , there is an un - known and increasing bias function  ℓ : R ≥ 0 → R ≥ 0 such that the ob - ( ) served utility of item  ∈  ℓ for the  - th attribute is  b   B  ℓ    . Appendix B extends this to overlapping groups . The specifc groups vary with application and could , for instance , be defned by socially salient attributes ( e . g . , gender , race , or age ) associated with each item ( see examples in Section 1 ) . For any  ∈ F , parameterized by  , let  b be the corresponding function parameterized by  b . In this work , we study the following problem . Problem 1 . Given  functions  1 ,  2 , . . . ,   and an  ×  ma - trix  b , parameterizing a monotone submodular function  b , without knowledge of the specifc bias functions  1 ,  2 , . . . ,   , fnd a subset  of size at most  such that  (  ) ≈ OPT . Note that for linear functions , which also belong to the family F , if the bias functions are “multiplicative” then [ 17 , 41 ] already show that the set   that maximizes observed utility subject to satisfying the proportional representation constraints satisfes  (   ) ≈ OPT . Since   can be efciently found when  is linear , this answers Problem 1 for linear  and multiplicative bias functions . This work studies generalizations to other bias functions and submodular  . 4 THEORETICAL RESULTS 4 . 1 Fairness constraints do not guarantee high latent utility In this section , we consider a family of fairness constraints and show that no fairness constraints in this family can guarantee a constant fraction of the optimal latent utility . For each 1 ≤ ℓ ≤  , let  ℓ be the fraction of all items that are in  ℓ , i . e . ,  ℓ B |  ℓ | . For any vectors  and  , the constraint specifed  by  and  requires the output subset  to satisfy 1 ≤ ℓ ≤  , |  ∩  ℓ | ≤ (  ℓ +  ℓ  ℓ ) ·  . This family of fairness constraints generalizes the family introduced in Section 1 ( which corresponds to the subset of the above family where  = 0 ) . In particular , the above family captures the equal representation when  ℓ = 1 and  ℓ = 0 for each ℓ and proportional  representation when  ℓ = 0 and  ℓ = 1 for each ℓ . Given (  ,  ) , let    be the subset that maximizes the observed utility  b subject to sat - isfying the constraints specifed by  and  . Our frst result studies the utility of    under multiplicative bias , as studied by [ 17 , 41 ] . Theorem 4 . 1 ( Fairness constraints do not guarantee any fraction of OPT ) . Defne  1 (  ) =  1 ·  and  2 (  ) =  2 ·  for all  . For any 0 <  < 1 ,  = (  1 ,  2 ) , and  = (  1 ,  2 ) there exists • a submodular function  ∈ F , • numbers 0 ≤  1 ,  2 ≤ 1 specifying group sizes , • numbers 0 <  1 ,  2 ≤ 1 specifying  1 and  2 respectively , and • family of  ×  matrices W parameterized by  , such that , for any  ≥ poly (  − 1 ) ,  ≥  · poly (  − 1 ) , and  ∈ W (  ) , Pr [  (    ) ≤  · OPT ] ≥ 1 −  , where the probability is over the randomness in  1 and  2 . Thus , for any fairness constraint in the above family and any  > 0 , there is a submodular function in the family F and family of utili - ties , such that the subset maximizing the observed utility subject to satisfying the fairness constraint has a utility of at most  times the optimal ( with high probability ) . Theorem 4 . 1 straightforwardly generalizes to  > 2 groups by adding empty groups . It also generalizes to  > 3 attributes by fxing utilities so that    = 0 for each item  and  ∈ [  ] \ [ 3 ] . Note that in the above result the bias functions ,  1 and  2 , are multiplicative with positive multiplicative constants  1 ,  2 > 0 . This ensures that the order of utilities of items is preserved within groups ( irrespective of how close  1 and  2 are to 0 ) . Under this property , [ 17 ] shows that the proportional representation con - straint recovers near - optimal latent utility with a linear function  . In contrast , Theorem 4 . 1 shows that this is not true when  is sub - modular ( even when the bias functions satisfy the same multiplicative property ) . In fact , Theorem 4 . 1 shows that if  is submodular then the subset maximizing the observed utility sub - ject to satisfying proportional representation ( or any other fair - ness constraint in the above family for that matter ) can have a latent utility signifcantly smaller than the optimal . The proof of Theorem 4 . 1 appears in Supplementary Material F . 1 . 4 . 2 Algorithmic result In this section , we present our main algorithm , Algorithm 1 , and the theoretical guarantee of its performance ( Theorem 4 . 3 ) . Algo - rithm 1 can be used for any function in the family F . It outputs a subset that proportionally represents items from each group and has near - optimal latent utility for functions in F that satisfy an algorithmically verifable “disjointedness” assumption ( Assumption 1 ) . Disjointedness assumption . For each  , let   be the set of items  which have positive utility for the  - th attribute , i . e . , { }   B  ∈ [  ] :    > 0 . Intuitively ,   is the set of items that are relevant to the “  - th attribute . ” More concretely , in many of our motivation contexts , attributes correspond to diferent categories of items such as genres , 3628 Maximizing Submodular Functions for Recommendation in the Presence of Biases WWW ’23 , April 30 – May 04 , 2023 , Austin , TX , USA topics , or retail types [ 2 , 40 , 56 ] . Here ,   is the set of items in the  - th category . Assumption 1 ( Disjoint categories ) . The matrix  ∈ R  ×  is such that  1 , . . . ,   are disjoint . Assumption 1 holds in any context where the relevant categories of items are disjoint . In the context , of web search , the above assump - tion is identifed and studied by [ 2 ] . Deviations from Assumption 1 can deteriorate the performance of Algorithm 1 . In Section 5 , we evaluate the performance of Algorithm 1 on MovieLens 20M data [ 32 ] that does not satisfy Assumption 1 ( Figure 2 ) . 2 We also evalu - ate Algorithm 1 on synthetic data that violates Assumption 1 and is inspired by the deployment of submodular - maximization based algorithms by [ 13 ] ( Figure 1 ( b ) ) . Our algorithm . Algorithm 1 outputs a subset that satisfes the pro - portional representation constraint . However , crucially , its output may not maximize the observed utility subject to satisfying the pro - portional representation constraint . Instead , its output satisfes data - dependent fairness constraints ( which , in particular , guarantee that the output proportionally represents each group  1 ,  2 , . . . ,   ) . In particular , the constraints of Algorithm 1 are specifed by  values  1 ,  2 , . . . ,   such that  1 +  2 + · · · +   =  : for each  , the output subset has   items from the  - th category   that proportionally represent the protected groups  1 ,  2 , . . . ,   . Note that this , in particular , guarantees that for any  1 ,  2 , . . . ,   , the output subset has a proportional number of items from each group  1 ,  2 , . . . ,   . The algorithm is divided into two parts . The frst part computes the “right” constraints from given data . This fxes the parameters  1 ,  2 , . . . ,   which are used in the second part . The second part performs submodular maximization subject to the computed con - straints : for each  , it selects a subset   of size   that maximizes the ( Í ) observed utility    ∈     subject to selecting a proportional number of items from each group  1 ,  2 , . . . ,   . Algorithm 1 out - puts the subset  1 ∪  2 ∪ · · · ∪   . Main theoretical result . Our theoretical result on the performance of Algorithm 1 assumes that there is one group , say  1 , such that the utilities of items in  1 face no “bias” , i . e . , the bias function  1 corresponding to  1 is the identity function . When utilities capture the relative “quality” or “relevance” of items then this assumption says that the observed utilities of items in  1 act as a “reference” with respect to which the bias functions for all other groups are defned . For instance , consider the multiplicative bias functions where  ℓ (  ) =  ℓ ·  for each  and 1 ≤ ℓ ≤  . The assumption that  1 is the identity function corresponds to re - scaling the other bias  ℓ functions by  1 , i . e . , choosing  ℓ (  ) = ·  for each  and ℓ .  1 The performance of Algorithm 1 depends on the range of the non - zero entries of  . We defne a parameter  to capture this range . Definition 4 . 2 . Let  > 0 be the smallest constant such that for each item  ∈ [  ] and attribute  ∈ [  ] , either    = 0 or  <    < 1 /  . The performance of Algorithm 1 depends on  due to a certain Í concentration inequality , involving sums of the form  ∈  ∩  ℓ    , which depends on  ( Lemma F . 4 in Supplementary Material F . 2 ) . 2 In the MovieLens 20M data , the sets  1 , . . . ,   denote the genders of movies and can be non - disjoint when a movie has more than one genre . Theorem 4 . 3 ( Latent utility guarantee of Algorithm 1 ) . Suppose  1 is the identity function . There is an algorithm ( Algorithm 1 ) that , given observed utilities  b ∈ R  ×  and evaluation oracles for  1 ,  2 , . . . ,   : R ≥ 0 → R ≥ 0 , outputs a set  of size  with the fol - lowing property ( under Assumption 1 ) : For any  ,  ,  > 0 and any increasing functions  2 ,  3 , . . . ,   : R ≥ 0 → R ≥ 0 , there is a large enough  0 ( dependent on  ,  ,  , and  ) such that for any  ≥  0 , with probability at least 1 −   (  ) ≥ OPT · ( 1 −  ) . Where the probability is over the randomness in the choice of  1 , . . .   . The algorithm makes  (   ) evaluations of each  1 ,  2 , . . . ,   and does  (   log  ) additional arithmetic operations . If the latent utilities are known , then under the disjointedness as - sumption ( Assumption 1 ) one can efciently fnd a set  that with the optimal latent utility using standard submodular maximiza - tion algorithms [ 2 ] . The challenge is to output a high latent utility subset when the latent utilities are unknown , and where existing submodular - maximization algorithms ( either with or without fair - ness constraints ) can have a low latent utility ( Theorem 4 . 1 ) . Theo - rem 4 . 3 shows that , for  is “large , ” Algorithm 1 outputs a set with near - optimal latent utility without knowing the latent utilities . Theorem 4 . 3 can be extended ( with the same proof ) to a general - ization of the bias model where the bias parameter corresponding to    not only depends on the protected group ( s ) which  is in but also on the index  : For each 1 ≤ ℓ ≤  and  - th attribute there is an increasing function  ℓ  : R → R such that the observed utilities of an item  ∈   are (  b  1 ,  b  2 , . . . ,  b   ) = (   1 (   1 ) ,   2 (   2 ) , . . . ,    (    ) ) . ( 4 ) This reduces to the model of bias in Defnition 3 . 2 when   ℓ =  ℓ for each 1 ≤  ≤  . The parameter  0 in Theorem 4 . 3 depends ( ) on  ,  ,  , and  as  e  − 4  − 4  − 10  8 . For a general submodular function , it is NP - hard to output a set  with  (  ) ≥ ( 1 −  − 1 ) · OPT [ 25 ] . Algorithm 1 gives a stronger guarantee ( for large  ) because this hardness result does not hold under Assumption 1 : if  1 ,  2 , . . . ,   are known and Assumption 1 holds , then there is an efcient algorithm to fnd a subset  with optimal latent utility [ 2 ] . Finally , we remark that in the statements of our results , we have tried to capture the dependence on each parameter as cleanly as possible and not tried to optimize the constants . The proof of Theorem 4 . 3 appears in Supplementary Material F . 2 . 4 . 3 Proof overviews of theoretical results 4 . 3 . 1 Technical challenges in extending approaches for linear  . Consider the linear function  (  ) = Í  ∈    1 and the subset   that maximizes the observed utility for  subject to satisfying pro - portional representation constraints . [ 17 , 47 ] show that the latent utility of   ,  (   ) , is at least ( 1 −   ( 1 ) ) · OPT . This result relies on two properties of any linear function  : ( 1 ) For any subset  ,  (  ) = Í ℓ ∈ [  ]  (  ∩  ℓ ) ( 2 ) For any increasing functions  1 ,  2 , . . . ,   , ℓ ∈ [  ] , and  ⊆ [  ] , argmax  ⊆  ∩  ℓ : |  | ≤   (  ) = argmax  ⊆  ∩  ℓ : |  | ≤   (  ) . b Suppose  OPT satisfes proportional representation . Using property Ð  Ð  1 , one can show that   =  b ℓ and  OPT = ℓ = 1  ℓ , where ℓ = 1 b  b ℓ B argmax  (  ) and  ℓ B argmax  (  ) .  ⊆  ℓ , |  | ≤  |  ℓ | /   ⊆  ℓ , |  | ≤  |  ℓ | /  3629 . . . . . . . . . . . . . . . . . . | | | | | | | | | | | | | | | | | | | | | | | | WWW ’23 , April 30 – May 04 , 2023 , Austin , TX , USA Anay Mehrotra and Nisheeth K . Vishnoi Algorithm 1 b R  ×  Input : A matrix  ∈ , a number  ∈ N , groups  1 ,  2 , . . . ,   , and sets  1 , . . . ,   Output : A subset  of size  ⊲ Part 1 : Compute data - dependent constraints 1 : Initialize  e = ∅ and defne following function ∀  ⊆ [  ] ,  e (  ) B Í   = 1   (  · Í  ∈  ∩  1  b   ) . ( 5 ) |  1 | 2 : for 1 ≤  ≤  do 3 : Sort items  in   ∩  1 in decreasing order of  e (  ) −  e ( ∅ ) , √ add the frst min {  ,   ∩  1 } items in  e | | 4 : end for |  1 | 5 : while |  e | <  · do  6 : Set  e B  e ∪ {  } where  it the item in  1 that maximizes the marginal increase in observed utility :  e (  e ∪ {  } ) −  e (  e ) 7 : end while 8 : Let   B |  e ∩   | for each 1 ≤  ≤  ⊲ Part 2 : For each  , select a set   ⊆   of size   that maximizes the observed utility while satisfying proportional representation 9 : Initialize   B ∅ for each 1 ≤  ≤  10 : for 1 ≤  ≤  and 1 ≤  ≤   do 11 : Defne C as the set of items in   that can be added to   without violating proportional representation constraints 12 : Set   B   ∪ {  } , where  is the item in C with the highest ( ) ( ) Í Í marginal utility :      +  ∈      −    ∈      . 13 : end for 14 : return  =  1 ∪  2 ∪ · · · ∪   Further , because of property 2 ( with  = [  ] ) , it follows that for any ℓ ∈ [  ] ,  ℓ =  ℓ and , hence ,   =  OPT . This relies on the b assumption that  OPT satisfes proportional representation . This may not be always true , but one can show that , with high proba - bility ,  OPT nearly - satisfes proportional representation . Using this and accounting for approximations in the above argument one can show that  (   ) ≈  (  OPT ) . However , unfortunately , straightfor - ward examples show that neither of the above properties holds for submodular functions in F . 4 . 3 . 2 Proof overview of Theorem 4 . 3 . Under Assumption 1 , we prove the following variants of properties 1 and 2 stated above : • For any subset  ,  (  ) = Í  ∈ [  ] Í ℓ ∈ [  ]  (  ∩  ℓ ∩   ) • For any increasing functions  1 ,  2 , . . . ,   , ℓ ∈ [  ] , and  ∈ [  ] b argmax  ⊆   ∩  ℓ : |  | ≤   (  ) = argmax  ⊆   ∩  ℓ : |  | ≤   (  ) . Observation . Let  OPT be any subset with the optimal latent utility . Suppose we know   B  OPT ∩   for all 1 ≤  ≤  . If for all  , | | √   ≥  , then using the above properties ( and adapting the analysis of [ 47 ] ) , we can show that , with high probability ,  B Ð  = 1   (   )  has latent utility at least ( 1 −   ( 1 ) ) ·  (  OPT ) . Where , for each 1 ≤  ≤  ,   (   ) ⊆   is the subset that maximizes the observed utility subject to selecting at most   items and satisfying proportional representation constraint . Part 1 of Algorithm 1 computes estimates ,  ˜ 1 ,  ˜ 2 , . . . ,  ˜  , of  1 ,  2 , . . . ,   . For this , it relies on the fact that  ℓ is the identity func - tion for some ( known ) ℓ and that the groups  1 ,  2 , . . . ,   are constructed stochastically . The estimated values  ˜ 1 , . . . ,  ˜  specify the constraints for Part 2 . Part 2 of Algorithm 1 outputs the set √  B Ð   = 1   (  ˜  ) . It is possible that for some  ,  OPT ∩   <  | | and , hence , the above argument does not apply . √ Algorithm 1 avoids this by overestimating   to ensure that  ˜  ≥  for all 1 ≤  ≤  . At a high level , this guarantees that the output set  contains any item in  OPT ∩   with a “high - utility” with high probability . | | 5 EMPIRICAL RESULTS We evaluate Algorithm 1’s performance on both synthetic and real - world data . 3 Baselines and setup . We compare Algorithm 1’s performance against two baselines : Uncons and ProportionalRepr . Uncons , given  and observed utilities  b , runs the standard greedy algorithm of [ 57 ] to fnd a subset of size  that approximately maximizes the ob - served utility ( Algorithm 2 in Appendix E ) . ProportionalRepr , given  and observed utilities  b , uses a variant of the greedy algorithm that satisfes proportional representation constraints ( Algorithm 3 in Appendix E ) . ProportionalRepr outputs the subset of size  that approximately maximizes the observed utility subject to selecting a proportional number of items from each group . In all simulations , we generate latent and observed utilities  and  b ( as explained in subsequent sections ) and run algorithms with the following inputs : Algorithm 1 are given  b ,  , and the protected groups and Uncons is given  b and  . Simulations with synthetic datasets . In this simulation , we show that Algorithm 1 can achieve high latent utility even in some cases where Assumption 1 does not hold . We consider two synthetic datasets inspired by the recommendation algorithms used on Spo - tify [ 13 ] and tested on Amazon music [ 56 ] . Among these , the frst dataset and the corresponding function do not satisfy Assumption 1 . Setup . In both simulations , the task is to recommend a set of  B 50 songs to the current user . We set  B 250 ,  B 3 , and consider two disjoint groups . 4 In the frst simulation , we fx the objective as  (  ) B Í  ∈    1 + Í 3 √Í 1  with  = 5 Here ,   1 ≥ 0 ,   2 ∈ { 0 , 1 } ,  = 2  ∈     20 . and   3 ∈ { 0 , 1 } denote some measure of the song’s popularity  , whether  is from an “emerging artist , ” and whether the song has not been heard by the current user respectively . Intuitively , among songs  with similar popularity ( i . e . , similar   1 ) , songs from emerging artists and those not heard by the current user have a higher marginal utility . We draw the entries of  i . i . d . from natural distributions that can arise in these contexts ( see Appendix C for 3 The code for our simulations is available at https : / / github . com / AnayMehrotra / Submodular - Maximization - in - the - Presence - of - Biases 4 Our parameter choices draw from [ 13 ] ’s context ( recommendation on Spotify ) : Spo - tify’s algorithmically - curated playlists have 50 songs , hence ,  = 50 .  is small and unspecifed in [ 13 ] . We set  = 3 with synthetic data to keep the corresponding scenario simple . With real - world data , we try all values of  ( for chosen genres ) . [ 13 ] do not specify  . We varied  over { 100 , 250 , 500 } , observed similar results , and chose  = 250 arbitrarily . 5 This is the same as the objective function used by the recommendation algorithm , Mostra , on Spotify [ 13 ] . Except that Mostra considers more than  = 3 attributes and instead   1 encoding the number of song - plays , it encodes a “relevance score” predicted by a learning algorithm . 3630 Maximizing Submodular Functions for Recommendation in the Presence of Biases WWW ’23 , April 30 – May 04 , 2023 , Austin , TX , USA details ) . We divide the items into two groups  1 and  2 ; and vary { }   3  the size of  1 among 4 , 2 , . Here ,  2 can denote songs from 4 artists that users want to hear , but which are nevertheless under - recommended due to biases in the recommendation pipeline , e . g . , as have been recently observed for regional music on Spotify in India [ 62 ] . Given  1 ,  2 and a parameter  ∈ [ 0 , 1 ] , we generate observed utilities  b as follows :   1 , if  ∈  1 b b b   1 = { ,   2 =   2 , and   3 =   3 .  ·   1 if  ∈  2 . For the frst attribute , this corresponds to using  1 (  ) =  and  2 (  ) =  ·  for all  . For the last two attributes , we do not apply bias because they encode values that the platform may know . This violates the model in Defnition 3 . 2 where the same bias function acts on all attributes , hence , is a hard case for Algorithm 1 . ( It falls into the extension of this model discussed in Equation 4 . ) Í 3 In the second simulation , we fx the objective :  (  ) B = 1 log ( 1 + Í   ∈     ) . 6 Here , attributes denote genre , and the sets  1 ,  2 ,  3 are disjoint . For any item  in genre ℎ (  ) ,   ℎ (  ) is number of times users played song  and    = 0 for  ≠ ℎ (  ) . At a high level , this pro - motes the content to be diverse across genres as between two items  and  with a similar number of user plays ,  has a higher marginal utility if Í  ∈    ℎ (  ) < Í  ∈    ℎ (  ) . Like the previous simulation , we draw entries of  i . i . d . from natural distributions that can arise in these contexts . We divide items into two protected groups  1 { }   3  and  2 ; and vary the size of  1 among . The complete im - 4 , 2 , 4 plementation details of these simulations appear in Appendix C . { }   3  Results and discussion . We vary  ∈ [ 0 , 1 ] and |  1 | ∈ , 4 , 2 , 4 and report the normalized latent utilities of diferent algorithms . Figure 1 presents the results for |  1 | = 0 . 5 . The results with  { }  3  |  1 | ∈ 4 , appear in Figures 3 and 4 in Supplementary Ma - 4 terial G . Across all fgures and both synthetic datasets , Algorithm 1 outputs subsets with NLU > 0 . 95 even for values of  close to 0 .  In contrast , when  approaches 0 and |  1 | = 2 , Uncons achieves NLU ≤ 0 . 85 . As  approaches 1 ( i . e . , as the bias decreases ) , the util - ity achieved by Uncons increases and approaches NLU ≈ 1 ( which is the optimal value and matches the performance of Algorithm 1 . Thus , we observe that Algorithm 1 can achieve high latent utility . This observation also holds on the frst synthetic dataset , where Assumption 1 does not hold . Hence , Algorithm 1 can also achieve high latent utility in some cases where Assumption 1 is violated . Results on real - world data . In this simulation , we evaluate Al - gorithm 1 on real - world data with pre - existing bias ( as discussed below ) and show that it can outperform ProportionalRepr and Un - cons , even when the data does not satisfy Assumption 1 . Data . MovieLens 20M [ 32 ] contains 20 million user ratings ( on a scale of 0 to 5 ) for 27 , 000 movies submitted by 138 , 000 users of the movielens . org . For each movie  , apart from user - rating , the data has a set of genres of  and relevance scores    ∈ [ 0 , 1 ] of each genre  encoding “how strongly [ movie  ] exhibits particular properties represented by [ genre  ] . ” In addition , we gathered information about the lead actor ( the frst - listed cast member ) of each movie from movielens . org . Preprocessing . For each movie  , we predict the ( probable ) gender of its lead actor using the Genderize API ( gender - api . com ) and 6 This is the same as the function used by [ 56 ] except that they allow  ≥ 3 . 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 75 0 . 80 0 . 85 0 . 90 0 . 95 1 . 00 N o r m a li ze d L a t e n t U tilit y Simulation with parameters : n = 250 , k = 50 , frac . - of - non - men = 0 . 5 , SAME _ G = True ( log ( 1 + x ) ) m = 3 , power - law - dist ( delta = 2 ) , ITER = 20 , weight _ F = [ 1 1 1 ] Uncons Algorithm 1 ( less bias ) ( more bias ) |  1 | ( a )  1 (  ) =  ,  2 (  ) =  ·  , = 0 . 5 , and  = 2 . 0  0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 75 0 . 80 0 . 85 0 . 90 0 . 95 1 . 00 N o r m a li z e d L a t e n t U tilit y Simulation with parameters : NON - DISJn = 250 , k = 50 , frac . - of - non - men = 0 . 5 , SAME _ G = True ( log ( 1 + x ) ) m = 2 , power - law - dist ( delta = 2 ) , ITER = 10 , weight _ F = [ 1 . 0 . 05 0 . 05 ] Uncons Algorithm 1 ( less bias ) ( more bias ) |  1 | ( b )  1 (  ) =  ,  2 (  ) =  ·  , = 0 . 5 , and  = 2 . 0  Figure 1 : Simulation on synthetic data : We run Algorithm 1 and Uncons ( Figures 1 ( b ) and 1 ( a ) ) on synthetic data and report their normalized latent utility ( error bars denote stan - dard error of the mean ) . The results show that Uncons can lose a signifcant fraction of the optimal latent utility in the presence of bias ( up to 15 % for  < 0 . 1 ) while Algorithm 1 have a normalized latent utility higher than 0 . 99 . remove all movies where this prediction has confdence less than 0 . 9 ; this leaves 6612 and 1990 movies led by male and non - male actors respectively . 7 Among the remaining movies , movies led by male actors have disproportionately higher relevance scores on genres that are stereotypically associated with men ( e . g . , “Action” or “War” ) compared to movies led by non - male actors ( difering by up to 300 % ; see Table 1 in Supplementary Material G ) . In contrast to relevance scores , user ratings are relatively balanced across movies led by male and non - male actors across all genres ( the diference is at most 6 % across all genres , see Table 2 in Supplementary Material G ) . ( This observation also holds if we consider all 27 , 000 movies ; see Table 3 in Supplementary Material G . ) Hence , compared to the user ratings , the relevance scores are systematically lower for movies led by an actor of a non - stereotypical gender in many genres . Setup . Given a subset  of genres , the task is to recommend  movies from genres  to the users . For each genre  , let   be the ratio of the average relevance score of movies in this genre led by male actors and non - male actors . We select all genres  where   ≥ 2 , these are  = { action , adventure , crime , western , and war } . Given a set  of movies and a subset of selected genres  ⊆  , we recommend a subset of movies  that maximizes the following “observed utility : ” Í √ Í  b (  ) B  ∈     . This function captures the beneft of  ∈  7 We choose a high threshold of 0 . 9 to ensure that the gender predictions have a low error rate among the remaining movies . We repeated the simulation with thresholds 0 . 7 and 0 . 8 and observed similar results . 3631 WWW ’23 , April 30 – May 04 , 2023 , Austin , TX , USA Anay Mehrotra and Nisheeth K . Vishnoi recommending movies that are relevant to the selected genres  and the √· captures the diminishing return of recommending multiple movies from the same genre  ∈  . We use the user ratings to evaluate the quality of the recommended movies ( or their “latent utility” ) : Given a set of movies  , we say its latent utility is  (  ) B Í 1  ∈  rat  , where rat  is the average user rating of movie  . |  | In the simulation , we vary  ∈ { 50 , 100 , 150 , 200 } and select diferent nonempty subsets  ⊆  ( 31 such subsets ) . For each  and  , we repeat the simulation 100 times . In each iteration , we draw a user  uniformly at random from the set of users who have rated at least 200 movies ( 19 % of the total users ) . We set  to be the set of movies rated by user  ; intuitively , this ensures that  contains movies that are watched by users on the platform . Assumption 1 requires that each movie has a unique genre . This does not hold , but we still use Algorithm 1 ( without modifcations ) . Results . We report the normalized latent utilities of Uncons , Pro - portionalRepr , and Algorithm 1 in Figure 2 ( for all subsets  of size 4 ) . The results for the remaining choices of  appear in Figures 5 to 9 in Supplementary Material G . Across all choices of  , we ob - serve that Algorithm 1 achieves 3 % higher quality than Uncons and ProportionalRepr for 14 / 31 subsets of stereotypical genres ( > 45 % ) , has similar quality ( within 1 % ) as Uncons or ProportionalRepr in 13 / 31 subsets of stereotypical genres ( 42 % ) , and has up to 2 % lower quality than Uncons in 4 / 31 choices of genres ( 13 % ) . 6 LIMITATIONS AND CONCLUSION This work studies the maximization of submodular functions which have been used to capture the utility of subsets of items in recom - mendation systems and web search . It studies this in the setting where the inputs defning the submodular function have social biases – modeled by an extension of [ 41 ] ’s bias model – and these biases lead to a reduction in the latent utility of the output subset . Our frst result shows that maximizing the observed utility subject to fairness constraints is not sufcient to recover any fraction of the optimal latent utility ( Theorem 4 . 1 ) . On the positive side , we give an algorithm ( Algorithm 1 ) for submodular maximization that works for a general family of submodular functions capturing sub - modular functions used in recommendation and web search . Under mild assumptions , the algorithm provably outputs a subset with near - optimal latent utility ( Theorem 4 . 3 ) . Empirically , the subsets output by this algorithm have higher latent utility than baselines even when the assumptions required by the theoretical results do not hold ( Figures 1 and 2 ) . Our work raises several questions for future work . Our algorithm achieves near - optimal latent utility when the number of items se - lected  is large compared to the number of attributes  . Designing algorithms that achieve a high latent utility for a larger range of  is an interesting direction . Further , our model ( like several others [ 17 , 41 ] ) has the limitation that it does not model intersectionality , while this can be partially addressed by defning each intersection as a separate group , this may be unfeasible when the sizes of the intersections are too small [ 38 ] . Empirical results on real - world data showed that our algorithm can outperform baselines , even in some cases where data does not follow the theoretical model considered . However , a careful assessment of our algorithms’ per - formance on application - specifc data , both pre - deployment and post - deployment , would be important to avoid any unintended Figure 2 : Simulation on MovieLens 20M data : The relevance scores in the data are disproportionately higher ( up to 3 times ) for movies led by male actors compared to movies led by non - male actors , in genres stereotypically associated with men . In contrast , user ratings for these sets of movies are within 6 % for each genre . We use relevance scores to rec - ommend  ∈ { 50 , 100 , 150 , 200 } movies from diferent subsets of men - stereotypical genres and use user ratings to estimate the latent utility of the recommended movies . Figures 2 ( a ) and 2 ( b ) present results for two subsets of size 4 – we observe that Algorithm 1 achieves 1 . 40 % and 0 . 28 % higher normal - ized latent utility than Uncons and ProportionalRepr for all  . ( Results for other genre subsets of other sizes appear in Figures 5 to 9 in Supplementary Material G . ) harm . Moreover , in certain contexts , afrmative action policies have been shown to have positive long - term efects on the bias in the “system” [ 15 , 33 ] . Our algorithm can be seen as an afrmative action policy with data - dependent constraints , and studying its long - term efects is an interesting direction . Furthermore , submod - ular maximization is one part of the larger information retrieval or recommendation system ; examining the efect of biases in the input on other parts of the system and evaluating the efcacy of our algorithm in conjunction with the broader system are interesting directions . ACKNOWLEDGMENTS This project is supported in part by NSF Awards ( CCF - 2112665 and IIS - 2045951 ) , and an AWS MLRA Award . 3632 Maximizing Submodular Functions for Recommendation in the Presence of Biases WWW ’23 , April 30 – May 04 , 2023 , Austin , TX , USA REFERENCES [ 1 ] Himan Abdollahpouri . Popularity bias in recommendation : a multi - stakeholder perspective . PhD thesis , University of Colorado at Boulder , 2020 . [ 2 ] Rakesh Agrawal , Sreenivas Gollapudi , Alan Halverson , and Samuel Ieong . Diver - sifying Search Results . In WSDM , pages 5 – 14 . ACM , 2009 . [ 3 ] Abolfazl Asudeh , Tanya Berger - Wolf , Bhaskar DasGupta , and Anastasios Sidiropoulos . Maximizing coverage while ensuring fairness : A tale of conficting objectives . Algorithmica , 2022 . [ 4 ] Sayan Bandyapadhyay , Aritra Banik , and Sujoy Bhore . On fair covering and hitting problems . In Łukasz Kowalik , Michał Pilipczuk , and Paweł Rzążewski , editors , Graph - Theoretic Concepts in Computer Science , pages 39 – 51 , Cham , 2021 . Springer International Publishing . [ 5 ] Solon Barocas , Moritz Hardt , and Arvind Narayanan . Fairness and Machine Learning . fairmlbook . org , 2019 . http : / / www . fairmlbook . org . [ 6 ] Alyssa Bereznak . The Problem With IMDb’s Rating System , January 2019 . https : / / www . theringer . com / tv / 2019 / 6 / 12 / 18661850 / imdb - rating - system - problems - chernobyl . [ 7 ] Marianne Bertrand and Sendhil Mullainathan . Are Emily and Greg More Employ - able than Lakisha and Jamal ? A Field Experiment on Labor Market Discrimination . American economic review , 94 ( 4 ) : 991 – 1013 , 2004 . [ 8 ] Asia J . Biega , Krishna P . Gummadi , and Gerhard Weikum . Equity of attention : Amortizing individual fairness in rankings . In SIGIR , pages 405 – 414 . ACM , 2018 . [ 9 ] Víctor Blanco and Ricardo Gázquez . Fairness in Maximal Covering Location Problems , 2022 . [ 10 ] Niclas Boehmer and Tomohiro Koana . The Complexity of Finding Fair Many - to - One Matchings , 2022 . [ 11 ] Iris Bohnet . What Works : Gender Equality by Design . Harvard University Press , 2016 . [ 12 ] Bert Boyce . Beyond topicality : A two stage view of relevance and the retrieval process . Information Processing & Management , 18 ( 3 ) : 105 – 109 , 1982 . [ 13 ] Emanuele Bugliarello , Rishabh Mehrotra , James Kirk , and Mounia Lalmas . Mostra : A Flexible Balancing Framework to Trade - of User , Artist and Platform Objectives for Music Sequencing . In WWW , pages 2936 – 2945 . ACM , 2022 . [ 14 ] Gruia Calinescu , Chandra Chekuri , Martin Pál , and Jan Vondrák . Maximizing a Monotone Submodular Function Subject to a Matroid Constraint . SIAM Journal on Computing , 40 ( 6 ) : 1740 – 1766 , 2011 . [ 15 ] L . Elisa Celis , Chris Hays , Anay Mehrotra , and Nisheeth K . Vishnoi . The Efect of the Rooney Rule on Implicit Bias in the Long Term . In Proceedings of the 2021 ACM Conference on Fairness , Accountability , and Transparency , FAccT ’21 , page 678 – 689 , New York , NY , USA , 2021 . Association for Computing Machinery . [ 16 ] L . Elisa Celis , Lingxiao Huang , and Nisheeth K . Vishnoi . Multiwinner voting with fairness constraints . In IJCAI , pages 144 – 151 . ijcai . org , 2018 . [ 17 ] L . Elisa Celis , Anay Mehrotra , and Nisheeth K . Vishnoi . Interventions for Ranking in the Presence of Implicit Bias . In FAT * , pages 369 – 380 . ACM , 2020 . [ 18 ] Aaron Clauset , Cosma Rohilla Shalizi , and Mark EJ Newman . Power - Law Distri - butions in Empirical Data . SIAM review , 51 ( 4 ) : 661 – 703 , 2009 . [ 19 ] Dan Cosley , Shyong K . Lam , Istvan Albert , Joseph A . Konstan , and John Riedl . Is seeing believing ? how recommender system interfaces afect users’ opinions . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems , CHI ’03 , page 585 – 592 , New York , NY , USA , 2003 . Association for Computing Machinery . [ 20 ] Michael D Ekstrand , Anubrata Das , Robin Burke , Fernando Diaz , et al . Fairness in information access systems . Foundations and Trends® in Information Retrieval , 16 ( 1 - 2 ) : 1 – 177 , 2022 . [ 21 ] Michael D . Ekstrand and Daniel Kluver . Exploring author gender in book rating and recommendation . User Modeling and User - Adapted Interaction , 31 ( 3 ) : 377 – 420 , 2021 . [ 22 ] Khalid El - Arini , Gaurav Veda , Dafna Shahaf , and Carlos Guestrin . Turning down the Noise in the Blogosphere . In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’09 , page 289 – 298 , New York , NY , USA , 2009 . Association for Computing Machinery . [ 23 ] Vitalii Emelianov , Nicolas Gast , Krishna P . Gummadi , and Patrick Loiseau . On Fair Selection in the Presence of Implicit and Diferential Variance . Artifcial Intelligence , 302 : 103609 , 2022 . [ 24 ] Spotify Engineering . Reach for the Top : How Spotify Built Shortcuts in Just Six Months , April 2020 . https : / / engineering . atspotify . com / reach - for - the - top - how - spotify - built - shortcuts - in - just - six - months / . [ 25 ] Uriel Feige . A Threshold of ln  for Approximating Set Cover . J . ACM , 45 ( 4 ) : 634 – 652 , jul 1998 . [ 26 ] Marshall L Fisher , George L Nemhauser , and Laurence A Wolsey . An Analysis of Approximations for Maximizing Submodular Set Functions —II . In Polyhedral combinatorics , pages 73 – 87 . Springer , 1978 . [ 27 ] Sahin Cem Geyik , Stuart Ambler , and Krishnaram Kenthapadi . Fairness - aware ranking in search & recommendation systems with application to linkedin talent search . In KDD , pages 2221 – 2231 . ACM , 2019 . [ 28 ] Anthony G Greenwald and Linda Hamilton Krieger . Implicit Bias : Scientifc Foundations . California Law Review , 94 ( 4 ) : 945 – 967 , 2006 . [ 29 ] Ulrike Gretzel and Daniel R . Fesenmaier . Persuasion in recommender systems . International Journal of Electronic Commerce , 11 ( 2 ) : 81 – 100 , 2006 . [ 30 ] Anupam Gupta , Aaron Roth , Grant Schoenebeck , and Kunal Talwar . Constrained Non - monotone Submodular Maximization : Ofine and Secretary Algorithms . In Amin Saberi , editor , Internet and Network Economics , pages 246 – 257 , Berlin , Heidelberg , 2010 . Springer Berlin Heidelberg . [ 31 ] Anikó Hannák , Claudia Wagner , David Garcia , Alan Mislove , Markus Strohmaier , and Christo Wilson . Bias in online freelance marketplaces : Evidence from taskrab - bit and fverr . In Proceedings of the 2017 ACM conference on computer supported cooperative work and social computing , pages 1914 – 1933 , 2017 . [ 32 ] F . Maxwell Harper and Joseph A . Konstan . The MovieLens Datasets : History and Context . ACM Trans . Interact . Intell . Syst . , 5 ( 4 ) : 19 : 1 – 19 : 19 , December 2015 . [ 33 ] Hoda Heidari and Jon Kleinberg . Allocating Opportunities in a Dynamic Model of Intergenerational Mobility . In Proceedings of the 2021 ACM Conference on Fairness , Accountability , and Transparency , FAccT ’21 , page 15 – 25 , New York , NY , USA , 2021 . Association for Computing Machinery . [ 34 ] Lu Hong and Scott E Page . Groups of Diverse Problem Solvers Can Outperform Groups of High - Ability Problem Solvers . Proceedings of the National Academy of Sciences , 101 ( 46 ) : 16385 – 16389 , 2004 . [ 35 ] Lars Bo Jeppesen and Karim R Lakhani . Marginality and Problem - Solving Efec - tiveness in Broadcast Search . Organization science , 21 ( 5 ) : 1016 – 1033 , 2010 . [ 36 ] Toshihiro Kamishima and Shotaro Akaho . Considerations on recommendation independence for a fnd - good - items task . 2017 . [ 37 ] Matthew Kay , Cynthia Matuszek , and Sean A . Munson . Unequal Representation and Gender Stereotypes in Image Search Results for Occupations . In Bo Begole , Jinwoo Kim , Kori Inkpen , and Woontack Woo , editors , Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems , CHI 2015 , Seoul , Republic of Korea , April 18 - 23 , 2015 , pages 3819 – 3828 , Seoul , Republic of Korea , 2015 . ACM . [ 38 ] Michael Kearns , Seth Neel , Aaron Roth , and Zhiwei Steven Wu . Preventing fairness gerrymandering : Auditing and learning for subgroup fairness . In Jennifer Dy and Andreas Krause , editors , Proceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pages 2564 – 2572 . PMLR , 10 – 15 Jul 2018 . [ 39 ] Vijay Keswani and L . Elisa Celis . Dialect Diversity in Text Summarization on Twitter . In WWW , pages 3802 – 3814 . ACM / IW3C2 , 2021 . [ 40 ] Jon Kleinberg and Maithra Raghu . Team Performance with Test Scores . ACM Trans . Econ . Comput . , 6 ( 3 – 4 ) , oct 2018 . [ 41 ] Jon M . Kleinberg and Manish Raghavan . Selection Problems in the Presence of Implicit Bias . In ITCS , volume 94 of LIPIcs , pages 33 : 1 – 33 : 17 . Schloss Dagstuhl - Leibniz - Zentrum für Informatik , 2018 . [ 42 ] Andreas Krause and Daniel Golovin . Submodular Function Maximization . Tractability , 3 : 71 – 104 , 2014 . [ 43 ] Hui Lin , Jef Bilmes , and Shasha Xie . Graph - Based Submodular Selection for Ex - tractive Summarization . In 2009 IEEE Workshop on Automatic Speech Recognition & Understanding , pages 381 – 386 . IEEE , 2009 . [ 44 ] Karen S Lyness and Madeline E Heilman . When Fit Is Fundamental : Performance Evaluations and Promotions of Upper - Level Female and Male Managers . Journal of Applied Psychology , 91 ( 4 ) : 777 , 2006 . [ 45 ] Christopher Manning , Prabhakar Raghavan , and Hinrich Schütze . Introduction to information retrieval . Natural Language Engineering , 16 ( 1 ) : 100 – 103 , 2010 . [ 46 ] Christopher Manning , Prabhakar Raghavan , and Hinrich Schütze . Introduction to Information Retrieval . Natural Language Engineering , 16 ( 1 ) : 100 – 103 , 2010 . [ 47 ] Anay Mehrotra , Bary S . R . Pradelski , and Nisheeth K . Vishnoi . Selection in the Presence of Implicit Bias : The Advantage of Intersectional Constraints . In FAccT , page To appear . ACM , 2022 . [ 48 ] Rishabh Mehrotra , Ashton Anderson , Fernando Diaz , Amit Sharma , Hanna Wal - lach , and Emine Yilmaz . Auditing search engines for diferential satisfaction across demographics . In Proceedings of the 26th international conference on World Wide Web companion , pages 626 – 633 , 2017 . [ 49 ] Rishabh Mehrotra , James McInerney , Hugues Bouchard , Mounia Lalmas , and Fernando Diaz . Towards a fair marketplace : Counterfactual evaluation of the trade - of between relevance , fairness & satisfaction in recommendation systems . In Proceedings of the 27th ACM International Conference on Information and Knowledge Management , CIKM ’18 , page 2243 – 2251 , New York , NY , USA , 2018 . Association for Computing Machinery . [ 50 ] Scott Mendelson . ’Ocean’s 8’ Can Aford To Ignore The Trolls Who Sabotaged ’Ghostbusters’ , May 2018 . https : / / www . forbes . com / sites / scottmendelson / 2018 / 05 / 17 / box - ofce - oceans - 8 - sandra - bullock - cate - blanchett - anne - hathaway - ghostbusters - rihanna / ? sh = 76cfd561596d . [ 51 ] Christopher Mims . Why social media is so good at polarizing us , October 2020 . https : / / www . wsj . com / articles / why - social - media - is - so - good - at - polarizing - us - 11603105204 . [ 52 ] Michel Minoux . Accelerated Greedy Algorithms for Maximizing Submodular Set Functions . In J . Stoer , editor , Optimization Techniques , pages 234 – 243 , Berlin , Heidelberg , 1978 . Springer Berlin Heidelberg . [ 53 ] Baharan Mirzasoleiman , Ashwinkumar Badanidiyuru , and Amin Karbasi . Fast Constrained Submodular Maximization : Personalized Data Summarization . In Maria Florina Balcan and Kilian Q . Weinberger , editors , Proceedings of The 33rd 3633 WWW ’23 , April 30 – May 04 , 2023 , Austin , TX , USA Anay Mehrotra and Nisheeth K . Vishnoi International Conference on Machine Learning , volume 48 of Proceedings of Ma - chine Learning Research , pages 1358 – 1367 , New York , New York , USA , 20 – 22 Jun 2016 . PMLR . [ 54 ] Baharan Mirzasoleiman , Ashwinkumar Badanidiyuru , Amin Karbasi , Jan Von - drak , and Andreas Krause . Lazier Than Lazy Greedy . Proceedings of the AAAI Conference on Artifcial Intelligence , 29 ( 1 ) , Feb . 2015 . [ 55 ] Baharan Mirzasoleiman , Amin Karbasi , Rik Sarkar , and Andreas Krause . Dis - tributed Submodular Maximization . The Journal of Machine Learning Research , 17 ( 1 ) : 8330 – 8373 , 2016 . [ 56 ] Houssam Nassif , Kemal Oral Cansizlar , Mitchell Goodman , and S . V . N . Vish - wanathan . Diversifying Music Recommendations . CoRR , abs / 1810 . 01482 , 2018 . [ 57 ] George L Nemhauser , Laurence A Wolsey , and Marshall L Fisher . An Analysis of Approximations for Maximizing Submodular Set Functions - I . Mathematical Programming , 14 ( 1 ) : 265 – 294 , 1978 . [ 58 ] Safya Umoja Noble . Algorithms of Oppression : How Search Engines Reinforce Racism . NYU Press , 2018 . [ 59 ] Alexandra Olteanu , Carlos Castillo , Fernando Diaz , and Emre Kiciman . Social data : Biases , methodological pitfalls , and ethical boundaries . Frontiers Big Data , 2 : 13 , 2019 . [ 60 ] Gourab K . Patro , Lorenzo Porcaro , Laura Mitchell , Qiuyue Zhang , Meike Zehlike , and Nikhil Garg . Fair Ranking : A Critical Review , Challenges , and Future Direc - tions . In FAccT , page To appear . ACM , 2022 . [ 61 ] Evaggelia Pitoura , Kostas Stefanidis , and Georgia Koutrika . Fairness in Rankings and Recommendations : An Overview . The VLDB Journal , 2021 . [ 62 ] Soumyajit Saha . Spotify Adopts Indian Habits to Avoid the ‘Netfix Problem’ , May 2022 . https : / / the - ken . com / story / spotify - adopts - indian - habits - to - avoid - the - netfix - problem / . [ 63 ] Ashudeep Singh and Thorsten Joachims . Fairness of Exposure in Rankings . In KDD , pages 2219 – 2228 . ACM , 2018 . [ 64 ] Michael Tauberg . Power Law in Popular Media , June 2018 . https : / / michaeltauberg . medium . com / power - law - in - popular - media - 7d7efef3fb7c . [ 65 ] Choon Hui Teo , Houssam Nassif , Daniel Hill , Sriram Srinivasan , Mitchell Good - man , Vijai Mohan , and S . V . N . Vishwanathan . Adaptive , Personalized Diversity for Visual Discovery . In Proceedings of the 10th ACM Conference on Recommender Systems , RecSys ’16 , page 35 – 38 , New York , NY , USA , 2016 . Association for Computing Machinery . [ 66 ] Sahil Verma and Julia Rubin . Fairness Defnitions Explained . In Yuriy Brun , Brittany Johnson , and Alexandra Meliou , editors , Proceedings of the International Workshop on Software Fairness , FairWare @ ICSE 2018 , Gothenburg , Sweden , May 29 , 2018 , pages 1 – 7 , Gothenburg , Sweden , 2018 . ACM . [ 67 ] Yanhao Wang , Yuchen Li , Francesco Bonchi , and Ying Wang . Balancing Utility and Fairness in Submodular Maximization ( Technical Report ) , 2022 . [ 68 ] Christine Wennerås and Agnes Wold . Nepotism and Sexism in Peer - Review . Nature , 387 ( 6631 ) : 341 – 343 , May 1997 . [ 69 ] Ke Yang , Vasilis Gkatzelis , and Julia Stoyanovich . Balanced ranking with diversity constraints . In IJCAI , pages 6035 – 6042 . ijcai . org , 2019 . [ 70 ] Ke Yang , Joshua R . Loftus , and Julia Stoyanovich . Causal intersectionality and fair ranking . In FORC , volume 192 of LIPIcs , pages 7 : 1 – 7 : 20 . Schloss Dagstuhl - Leibniz - Zentrum für Informatik , 2021 . [ 71 ] Sirui Yao and Bert Huang . Beyond parity : Fairness objectives for collaborative fltering . Advances in neural information processing systems , 30 , 2017 . [ 72 ] Meike Zehlike and Carlos Castillo . Reducing disparate exposure in ranking : A learning to rank approach . In WWW , pages 2849 – 2855 . ACM / IW3C2 , 2020 . [ 73 ] Meike Zehlike , Ke Yang , and Julia Stoyanovich . Fairness in Ranking , Part I : Score - Based Ranking . ACM Comput . Surv . , apr 2022 . Just Accepted . 3634 Maximizing Submodular Functions for Recommendation in the Presence of Biases WWW ’23 , April 30 – May 04 , 2023 , Austin , TX , USA A ADDITIONAL EXAMPLES OF SUBMODULAR FUNCTIONS USED BY PRIOR WORKS The diminishing returns property , of submodular functions , arises in many applications , including content recommendation [ 13 , 56 ] , web search [ 2 ] , text summarization [ 43 ] , and team selection [ 34 , 35 , 40 ] . This property is one of the key reasons for the use of submod - ular functions in the above applications [ 42 ] . For instance , submodular functions are used when a recommen - dation system has diferent objectives and each additional item satisfying the same objective has a diminishing return . Concretely , [ 13 ] explain that “a fundamental requirement of [ Spotify’s ] music recommender system is its ability to accommodate considerations from the users ( e . g . short - term satisfaction objectives ) , artists ( e . g . exposure of emerging artists ) and platform ( e . g . facilitating discov - ery and boosting strategic content ) when surfacing music content to users” [ 13 ] design the recommendation system for Spotify that uses a submodular objective function . For each item  ( e . g . , song or podcast ) , let   1 ≥ 0 denote its relevance to a user ( predicted by a learning algorithm ) and   2 , . . . ,    ∈ { 0 , 1 } indicate artist and platform - specifc metrics ( e . g . , if  is created by an emerging artist or if the platform wants to promote  ) . [ 13 ] use the following objective to capture the utility of a playlist  for the user ∑ √∑ √∑  (  ) B   1 +   2 + · · · +    . ( 6 )  ∈   ∈   ∈  Submodular functions also arise in web search , where each query  can have multiple interpretations : For instance , the query “fash” can refer to the Adobe Flash player , the superhero “The Flash” , or the village Flash with the highest elevation in Great Britain . Irrespective of the intended interpretation , each additional result related to the same interpretation ofers a smaller marginal utility to the user [ 2 ] . Suppose a query  ( e . g . , “Flash” ) belongs to category  ∈ [  ] ( e . g . , technology , movies , or location ) with probability Pr [  |  ] and , conditioned on the event that  belongs to category  , a result  satisfes the user with probability Pr [  |  ,  ] ( independent of other items ) . [ 2 ] observe that the set of search results ,  , that maximizes the following submodular objective has a higher quality than search results of a commercial search engine . ∑  Ö  (  ) B Pr [  |  ] ( 1 − ( 1 − Pr [  |  ,  ] ) ) . ( 7 )  = 1  ∈  This is in the family F . To see this set ,    = log 1 and 1 − Pr [  |  ,  ]   (  ) = − Pr [  |  ] ·  −  for all  and 1 ≤  ≤  , and   + 1 (  ) = 1 for all  . Given weights  1 , . . . ,   − 1 and scores  1 , . . . ,   , [ 65 ] consider the following submodular function  ∈ F defned by ( 1 )   (  ) =   · log ( 1 +  ) for all 1 ≤  ≤  − 1 and   (  ) =  , ( 2 ) for each 1 ≤  ≤  − 1 ,    = 1 if  has the  - th attribute and    = 0 otherwise , and ( 3 )    =   . B OVERLAPPING GROUPS AND NEED FOR STOCHASTICITY IN GROUPS Consider two overlapping groups  1 ,  2 , these divide the items into four disjoint “intersections” :   B  1 ∩  2 ,   B  1 \  2 ,   B  2 \  1 , and   B [  ] \ (  1 ∪  2 ) . Similarly ,  overlapping groups  1 ,  2 , . . . ,   divide the items into up to 2  intersections . [ 17 ] extend the bias model of [ 41 ] to overlapping groups . In their model , each item  in a diferent intersection faces a diferent amount Î of bias : The observed utility of  is ℓ :  ℓ ∋   ℓ times smaller than its latent utility . Remark B . 1 ( Overlapping groups ) . Instead of  overlapping groups  1 ,  2 , . . . ,   , one can consider the groups as the intersections formed by  1 ,  2 , . . . ,   For example , with  = 2 , one can consider   ,   ,   , and   as the groups with bias functions   =  1 ◦  2 ,   =  1 ,   =  2 , and   (  ) =  respectively . Since there are at most min (  , 2  ) non - empty intersections , this does not increase the running time of the algorithm proposed in the paper ( Theorem 4 . 3 ) . For simplicity , in this paper , we assume that the groups are disjoint . The need for stochasticity in groups . In the model , we assume that the latent utilities are deterministically chosen and the protected groups are stochastic . This generalizes the model of [ 17 , 41 , 47 ] which is equivalent to the model that draws latent utilities i . i . d . from some distribution and also constructs protected groups stochasti - cally . A further generalization could consider the case where both the latent utilities and protected groups are stochastic . However , in this model , it is information - theoretically impossible to output a subset whose latent utility is to guaranteed to be at least a positive fraction of OPT . Formally , we can show the following result if both latent utilities and protected groups can be arbitrary then no algorithm can recover any constant factor of approximation of the optimal utility . Let   be the subset output by algorithm  when given  b and  1 ,  2 , . . . ,   as input . For any  > 0 and any algorithm  , there are two disjoint protected groups  1 and  2 and bias functions  1 ,  2 : R → R such that  (   ) ≤  . C IMPLEMENTATION DETAILS OF SIMULATIONS Code . The code for our simulations is available at https : / / github . com / AnayMehrotra / Submodular - Maximization - in - the - Presence - of - Biases Synthetic dataset 1 . Figure 1 ( a ) presents results with this data . This data has  B 3 attributes and  B 250 songs , and uses the Í Í 3 √Í 1 objective  (  ) B  ∈    1 +   = 2  ∈     ( with  = 20 ) . • First , we select a subset   of songs with size |   | = 0 . 8  and label all songs in   to be from an emerging artist and all other songs as songs from non - emerging artists . This implies that   2 = 1 if  ∈   and   2 = 0 otherwise . • Next , for each song  ∈ [  ] , with probability  NH B 0 . 9 , we label it as “not heard by the current user” and set   3 = 1 and , otherwise , we label it as “heard by the current user” and set   3 = 0 . • Finally , for each song  ∉   , we independently draw a value  from the power - law distribution with exponent  and set   1 =  · 1000 . 8 For each  ∈   , we independently draw a value  from the power - law distribution with exponent  conditioned on  ≤ 2 and set   1 =  · 1000 . The conditioning encodes the fact that emerging artists do not have any “popular” songs yet . 1 We fx  B 20 , |   | = 0 . 8  , and  NH = 0 . 9 and vary  ∈ [ 0 , 1 ] ,  ∈ { 1 , 1 . 5 , 2 , 2 . 5 , 3 } , and |  1 | ∈ { 0 . 25 , 0 . 5 , 0 . 75 } .  3635 | | | | | | 1 WWW ’23 , April 30 – May 04 , 2023 , Austin , TX , USA Remark C . 1 . We also repeated the simulation with |   | ∈ { 0 . 4  , 0 . 9  } , { } 1 1  NH ∈ { 0 . 4 , 1 . 0 } , and  ∈ , and observed similar results as 10 , 5 Figures 1 , 3 , and 4 . Synthetic dataset 2 . The second synthetic dataset corresponds Figure 1 ( b ) , has  B 3 attributes and  B 250 songs , and uses the Í 3 Í objective function  (  ) B  = 1 log ( 1 +  ∈     ) . • We generate sets  1 ,  2 ,  3 uniformly at random : For each item  , with probability 13 we assign it to  1 , otherwise with probability 3 we assign it to  2 , and otherwise we assign it to  3 . • For each ℎ ∈ [ 3 ] and  ∈  ℎ , we independently draw a value  from the power - law distribution with exponent  and set   1 =  · 1000 . 8 Like the frst synthetic dataset , we generate groups  1 and  2 by assigning |  1 | items chosen uniformly at random without replace - ment to  1 and the remaining items to  2 Given  ∈ [ 0 , 1 ] , we generate observed utilities  b as in Defnition 3 . 2 with  1 (  ) =  and  2 (  ) =   . The simulation on this data fxed varies  ∈ [ 0 , 1 ] ,  ∈ { 1 , 2 , 3 } , and |  1 | ∈ { 0 . 25 , 0 . 5 , 0 . 75 } .  D FURTHER DISCUSSION OF RELATED WORKS Recent works [ 17 , 23 , 41 , 47 ] have demonstrated the beneft of im - posing fairness constraints on the output of subset selection on the latent utility of the output when the objective is additive or linear . [ 41 ] introduce the mathematical model of bias mentioned in Sec - tion 1 . They consider two groups , with  2 being the disadvantaged group , and study conditions on  , group sizes ,  , and the distri - bution of  , where requiring the output  to satisfy |  ∩  2 | ≥ 1 increases the latent utility of the output . [ 17 ] study a generaliza - tion of subset selection , ranking , where the selected individuals also need to be ordered . Specializing their work to subset selection : They consider two groups  1 and  2 and show that if  encodes proportional representation and entries of  are drawn i . i . d . from 9 the uniform distribution on [ 0 , 1 ] , then  (   ) ≥ ( 1 −   ( 1 ) ) · OPT . [ 23 ] study selection under a diferent model of bias , where the observed utility has higher than average noise for individuals in one group . They give a family of fairness constraints ( including proportional representation ) which increases the output’s latent utility . [ 47 ] study subset selection under the same model as [ 17 ] , but with multiple and overlapping groups . They compare the ef - cacy of the fairness constraints applied on groups  1 ,  2 , . . . ,   to fairness constraints applied on the disjoint intersections formed by these groups . Most relevant to this work , they extend the result of Anay Mehrotra and Nisheeth K . Vishnoi [ 17 ] for proportional representation constraints to multiple disjoint groups  1 ,  2 , . . . ,   . Unlike all of these works , we study the ef - cacy of fairness constraints when the objective of subset selection is submodular . 8 This is natural as power - law distributions have been observed to arise in the per - formance of musicians [ 64 ] and in the performance of other creative professionals [ 18 ] . 9 To be precise they required the output to have at least  ℓ items from group  ℓ . For two groups , this is equivalent to requiring the output subset to have at most  ℓ =  −  ¬ ℓ items from  ℓ . E STANDARD ALGORITHMS FOR SUBMODULAR MAXIMIZATION In this section , we present a standard greedy algorithm by [ 57 ] for maximizing monotone submodular functions with , e . g . , cardinality constraints . Algorithm 2 The standard greedy algorithm ( [ 57 ] ) Input : An evaluation oracle for  , a number  , and a set of feasible sets  ⊆ 2 [  ] Output : A subset  ∈  with |  | ≤  1 : Initialize  = ∅ 2 : while |  | <  do { } 3 : Let  ★ be the item in [  ] that maximizes  (  ∪  ★ ) −  (  ) { } subject to  ∪  ★ ∈  { } 4 : Set  =  ∪  ★ 5 : end while 6 : return  Algorithm 3 A variant of the standard greedy algorithm that satis - fes given upper bound constraints Input : An evaluation oracle for  , a numbers  ,  1 , . . . ,   , and groups  1 ,  2 , . . . ,   Output : A subset  with |  | ≤  and |  ∩  ℓ | ≤  ℓ 1 : Initialize  = ∅ 2 : while |  | <  do 3 : Let  ★ be the item in [  ] that maximizes  (  ∪ {  ★ } ) −  (  ) subject to (  ∪ {  ★ } ) ∩  ℓ ≤  ℓ ( for each ℓ ) | { } | 4 : Set  =  ∪  ★ 5 : end while 6 : return  3636