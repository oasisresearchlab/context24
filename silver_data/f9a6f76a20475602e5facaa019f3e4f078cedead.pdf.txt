Zero - Shot Opinion Summarization with GPT - 3 Adithya Bhaskar IIT Bombay adithyabhaskar @ cse . iitb . ac . in Alexander R . Fabbri Salesforce AI Research afabbri @ salesforce . com Greg Durrett UT Austin gdurrett @ cs . utexas . edu Abstract Very large language models such as GPT - 3 have shown impressive performance across a wide variety of tasks , including text summa - rization . In this paper , we show that this strong performance extends to opinion summariza - tion . We explore several pipeline methods for applying GPT - 3 to summarize a large collec - tion of user reviews in a zero - shot fashion , no - tably approaches based on recursive summa - rization and selecting salient content to sum - marize through supervised clustering or ex - traction . On two datasets , an aspect - oriented summarization dataset of hotel reviews and a generic summarization dataset of Amazon and Yelp reviews , we show that the GPT - 3 mod - els achieve very strong performance in human evaluation . We argue that standard evalua - tion metrics do not reﬂect this , and evaluate against several new measures targeting faith - fulness , factuality , and genericity to contrast these different methods . 1 Introduction The past decade has seen several shifts in summa - rization research , from primarily extractive models ( Erkan and Radev , 2004 ; Gu et al . , 2022 ; Kwon et al . , 2021 ; Jia et al . , 2020 ; Zhong et al . , 2020 ) to abstractive models with copy mechanisms ( See et al . , 2017 ; Song et al . , 2018 ; Xu et al . , 2020 ; Gehrmann et al . , 2018 ) to pre - trained models ( De - vlin et al . , 2019 ; Isonuma et al . , 2021 ; Lewis et al . , 2020 ; Zhang et al . , 2020a ; He et al . , 2020 ) . GPT - 3 ( Brown et al . , 2020 ; Wu et al . , 2021a ; Saunders et al . , 2022 ; Goyal et al . , 2022 ) promises another shift : it gives excellent zero and few - shot perfor - mance across a variety of tasks , including text gen - eration . However , its capabilities have not been extensively benchmarked for opinion summariza - tion . Unlike news , where extractive lead baselines are often highly effective , opinion summarization requires balancing contradictory opinions and a higher degree of abstraction to convey all of the viewpoints faithfully . In this paper , we apply GPT - 3 , speciﬁcally the text - davinci - 002 model , to the task of opinion summarization , focusing on reviews of products , hotels , and businesses . Applying GPT - 3 in this set - ting is not straightforward , as the combined length of the reviews or posts may exceed the maximum input length of GPT - 3 . Furthermore , we ﬁnd that certain styles of inputs can lead to GPT - 3 simply echoing back an extract of the inputs despite them being well within the character limits of GPT - 3 . To mitigate these issues , we explore a family of pipelined approaches to reduce the size of the in - put to GPT - 3 . Speciﬁcally , we explore ( 1 ) ﬁltering a subset of sentences with an extractive summa - rization model , ( 2 ) chunking with repeated summa - rization using GPT - 3 , and ( 3 ) review - score based stratiﬁcation . In the context of aspect - oriented summarization , we also explore the inclusion of a sentence - wise topic prediction and clustering step . We show that our approaches yield high - quality summaries according to human evaluation . We ﬁnd that the errors of the systems consist of subtle issues of balancing contradictory viewpoints and erroneous generalization of speciﬁc claims , which are not captured by standard n - gram - based eval - uation using ROUGE ( Lin , 2004 ) or BERTScore ( Zhang et al . , 2020b ) . This result corroborates work calling for a re - examination of current metrics ( Fab - bri et al . , 2021 ) and the need for ﬁne - grained eval - uation ( Gehrmann et al . , 2022 ) . We therefore intro - duce an array of metrics using entailment as a proxy for support , to measure the factuality , faithfulness and genericity of produced summaries for a more comprehensive understanding of system outputs . These measure the extent of over - generalization of claims and misrepresentation of viewpoints while checking for generic summaries . Our results show that GPT - 3 produces reason - ably faithful and factual summaries when the input a r X i v : 2211 . 15914v1 [ c s . C L ] 29 N ov 2022 Reviews ( T ) opic classiﬁca1on of sentences ( C ) hunk summariza1on ( G ) enera1on of ﬁnal summary The rooms were so clean ! Stained carpets and untidy beds…ew . The housekeeping staff did a great job keeping the rooms clean Great food ! [ … ] All staff with the exception of the front desk were so polite and friendly . The housekeeping staff did a great job keeping the rooms clean . The manager would not register our complaint . } < latexit sha1 _ base64 = " RixS9AL70apkyjMvFolO8petKH0 = " > AAACW3icbZFNS8MwGMfTTt2sU6fiyUtwDDyM0XaCHodePI2J7gW2UtIs3cLSlyWpMEq / pCc9 + FXEdNthLz4k8M / veR7y5B8vZlRI0 / zW9MLB4VGxdGyclE / PzisXlz0RJRyTLo5YxAceEoTRkHQllYwMYk5Q4DHS92bPeb7 / QbigUfguFzFxAjQJqU8xkgq5FT7KjNooQHLq + ekic1NUpxsEZ65d3zg0jdrcbas8G0dSGDWsVr2dQwsuy3iQDt + cDM5dexusOhRvu5Wq2TCXAfeFtRZVsI6OW / kcjSOcBCSUmCEhhpYZSydFXFLMSGaMEkFihGdoQoZKhiggwkmX3mSwpsgY + hFXO5RwSTc7UhQIsQg8VZmPK3ZzOfwvN0yk / + ikNIwTSUK8ushPGJQRzI2GY8oJlmyhBMKcqlkhniKOsFTfYSgTrN0n74ue3bCaDfv1vtp6WttRAjfgFtwBCzyAFngBHdAFGHyBX62olbQfvaAbenlVqmvrniuwFfr1H6fNsms = < / latexit > The staff was found to be polite and friendly , with special praise given to the housekeeping staff . } < latexit sha1 _ base64 = " RixS9AL70apkyjMvFolO8petKH0 = " > AAACW3icbZFNS8MwGMfTTt2sU6fiyUtwDDyM0XaCHodePI2J7gW2UtIs3cLSlyWpMEq / pCc9 + FXEdNthLz4k8M / veR7y5B8vZlRI0 / zW9MLB4VGxdGyclE / PzisXlz0RJRyTLo5YxAceEoTRkHQllYwMYk5Q4DHS92bPeb7 / QbigUfguFzFxAjQJqU8xkgq5FT7KjNooQHLq + ekic1NUpxsEZ65d3zg0jdrcbas8G0dSGDWsVr2dQwsuy3iQDt + cDM5dexusOhRvu5Wq2TCXAfeFtRZVsI6OW / kcjSOcBCSUmCEhhpYZSydFXFLMSGaMEkFihGdoQoZKhiggwkmX3mSwpsgY + hFXO5RwSTc7UhQIsQg8VZmPK3ZzOfwvN0yk / + ikNIwTSUK8ushPGJQRzI2GY8oJlmyhBMKcqlkhniKOsFTfYSgTrN0n74ue3bCaDfv1vtp6WttRAjfgFtwBCzyAFngBHdAFGHyBX62olbQfvaAbenlVqmvrniuwFfr1H6fNsms = < / latexit > Most reviewers enjoyed their experience . However , one reviewer specifically complained about the manager… The reviews were generally positive about the service , with praise for the housekeeping staff and chefs . Some reviewers did find their room damp and dark , but were happy to be upgraded to a better suite . [ … ] } < latexit sha1 _ base64 = " RixS9AL70apkyjMvFolO8petKH0 = " > AAACW3icbZFNS8MwGMfTTt2sU6fiyUtwDDyM0XaCHodePI2J7gW2UtIs3cLSlyWpMEq / pCc9 + FXEdNthLz4k8M / veR7y5B8vZlRI0 / zW9MLB4VGxdGyclE / PzisXlz0RJRyTLo5YxAceEoTRkHQllYwMYk5Q4DHS92bPeb7 / QbigUfguFzFxAjQJqU8xkgq5FT7KjNooQHLq + ekic1NUpxsEZ65d3zg0jdrcbas8G0dSGDWsVr2dQwsuy3iQDt + cDM5dexusOhRvu5Wq2TCXAfeFtRZVsI6OW / kcjSOcBCSUmCEhhpYZSydFXFLMSGaMEkFihGdoQoZKhiggwkmX3mSwpsgY + hFXO5RwSTc7UhQIsQg8VZmPK3ZzOfwvN0yk / + ikNIwTSUK8ushPGJQRzI2GY8oJlmyhBMKcqlkhniKOsFTfYSgTrN0n74ue3bCaDfv1vtp6WttRAjfgFtwBCzyAFngBHdAFGHyBX62olbQfvaAbenlVqmvrniuwFfr1H6fNsms = < / latexit > Chunkwise summary per aspect Final summary per aspect Great staff… Loved it… The bed… Stayed for… Terrible… Arrived… Review sentences grouped by topic S 1 = S 1 ( C 0 | service ) < latexit sha1 _ base64 = " oOsa6WafmQw0ELVxN / j0Zhbc0qI = " > AAACnXicbVFta9swEJa9buu8t3T9tn6YWAh0EIKdFtovg7AyaMsILV2aQmKEfJZbUfmlklwIRv + qv6Tf + m8mO95I2x06eO557qTTXVQIrrTvPzjui7WXr16vv / Hevnv / 4WNn49O5yksJbAK5yOVFRBUTPGMTzbVgF4VkNI0Em0bXB7U + vWVS8Tz7rRcFC1N6mfGEA9WWIp27eUr1VZRUZ4YE + DtuQqCiibf / igeG + Fbi8VKXaaWYvOXAzDevNzfW28SFIRXt8xUGDBn2V4Idr3dDxlYXca6V1wN7 + uOaDP5dPjsLDb4hw8fEssLyY9Lp + gO / MfwcBC3ootZOSOd + HudQpizTIKhSs8AvdFhRqTkIZrx5qVhB4ZpespmFGU2ZCqtmugb3LBPjJJfWM40bdrWioqlSizSymXW76qlWk / / TZqVO9sOKZ0WpWQbLh5JSYJ3jelU45pKBFgsLKEhue8VwRSUFbRfq2SEET7 / 8HJwPB8HOYHi62x39aMexjrbQV7SNArSHRugQnaAJAuezM3KOnGP3i / vT / eWOl6mu09ZsokfmTv8ABlXJ9Q = = < / latexit > S 2 = S 2 ( C 1 | service ) < latexit sha1 _ base64 = " 9SlRn7I2 / zc0YSHZ9Wp3o1 / y1dE = " > AAAC33icdVLLatwwFJXdV + o + Mm2W3YgOhhSGwZ4U2k0hNJushpR0ksDYmGv5OhGRH5HkwGC86SaLlNJtf6u7 / kjXlT3T4EzaiwTnnqN7dPWIS8GV9rxfln3v / oOHjzYeO0 + ePnu + OXjx8kgVlWQ4Y4Uo5EkMCgXPcaa5FnhSSoQsFngcn + + 1 + vElSsWL / LNelBhmcJrzlDPQhooGv90gA30Wp / VhE / n0A + 1SBqLLt / + Ke03kGYknS11mtUJ5yRk2b5yewWTNYNI38P9n4AaNc9PGoolqGPEew4zPqJfsOO5FNDW6SAqtHJeZMZq2pH9jPj8MG3ph + rlFLCsMP40GQ2 / sdUHvAn8FhmQVB9HgZ5AUrMow10yAUnPfK3VYg9ScCWycoFJYAjuHU5wbmEOGKqy792moa5iEpoU0M9e0Y / sVNWRKLbLYrGzbVetaS / 5Lm1c6fR / WPC8rjTlbbpRWguqCto9NEy6RabEwAJjkplfKzkAC0 + ZLOOYS / PUj3wVHk7G / M558ejvc / bi6jg3yirwm28Qn78gu2ScHZEaYFVhfrGvrqw32lf3N / r5calurmi1yK + wffwAuTeR / < / latexit > Figure 1 : Illustration of the TCG pipeline . Sentences are clustered based on the aspects closest to their topic ( T step ) - examples shown for rooms , food and service . The relevant cluster is then repeatedly chunked + summarized until the combined length falls below 35 sentences ( C step ) . Then a ﬁnal round of GPT - 3 summarization is done ( G step ) . reviews are short ( fewer than 1000 words ) , and augmented approaches do not show much improve - ment . However , as the input size grows larger , re - peated hierarchical summarization leads the model to produce generalities ( and in turn loss of factu - ality ) as well as unfaithful selection of viewpoints , relative to the ﬁrst round . We demonstrate that using QFSumm ( Ahuja et al . , 2022 ) , an extrac - tive summarization model , to ﬁlter out sentences prior to GPT - 3 ( instead of multi - level summariza - tion ) can help with factuality and faithfulness . The resulting summaries also present a more speciﬁc selection of viewpoints , but are generally smaller and use a higher proportion of common words . A topicwise clustering and ﬁltration step pre - pended to the pipeline alleviates both of these issues signif - icantly , while relinquishing a portion of the gains on factuality and faithfulness . Our main contributions are : ( 1 ) We introduce two approaches to long - form opinion summariza - tion with GPT - 3 , namely , hierarchical GPT - 3 sum - marization with chunking , and pre - extraction with an extractive summarization model . ( 2 ) We es - tablish the strength of these approaches with a human study , and demonstrate the need for ob - jective and automatic means of evaluation . ( 3 ) We develop entailment - based metrics for factuality , faithfulness , and genericity that are better suited to evaluate extremely ﬂuent summaries as com - pared to metrics based on n - gram matching . Code will soon be made publicly available at https : / / github . com / testzer0 / ZS - Summ - GPT3 . 2 Background Review summarization involves the summarization of the text of multiple reviews of a given product or service into a coherent synopsis . More formally , given a set of reviews R = { R i } ni = 1 with the re - view R i consisting of l i sentences { r ij } l i j = 1 , we deﬁne a summarization system S to be a function that takes as input the combined reviews C and then produces k output sentences S = { s i } ki = 1 : S = S ( C ) where C ≡ combine ( R ) is typically obtained by concatenating the review sentences . In the ex - amples we consider , segmentation is done at the sentence level . We therefore reuse the notation combine to refer to combination of both sentences and reviews . We further deﬁne a summarization pipeline to be a series of summarization systems S 1 , · · · , S m where each system takes as input the condensed results of the previous system . Speciﬁ - cally , S 0 = R C i = combine ( S i − 1 ) S i = S i ( C i ) We can also instantiate this pipeline for aspect - oriented review summarization , which involves the summarization of multiple reviews conditioned on an aspect a ( such as ‘cleanliness’ ) . In particular , the summarization at the i th step is given by S i = S i ( C i | a ) . We consider aspect - agnostic review summarization as a special case of aspect - oriented review summarization with the aspect ‘none’ for Pipeline Constituents SPACE Q QFSumm A AceSum TCG Topicwise - Clustering + Recursive GPT - 3 - Chunking First - TCG ∗ TCG - Output of first GPT - 3 - Chunking Layer QG QFSumm - long + GPT - 3 TQG Topicwise - Clustering + QFSumm - long + GPT - 3 RG Review - Stratification + Recursive GPT - 3 - Chunking First - RG ∗ RG - Output of first GPT - 3 - Chunking Layer FewSum Q QFSumm G GPT - 3 QG QFSumm - long + GPT - 3 CG GPT - 3 - Chunking + Recursive GPT - 3 - Chunking First - CG ∗ CG - Output of first GPT - 3 - Chunking Layer 1 Table 1 : The pipelines compared for SPACE and Few - Sum , and their constituents . Entries marked with ∗ are not actual pipelines but are included for comparison . notational simplicity . We showcase an example pipeline in Figure 1 , with one stage extracting the relevant sentences from the reviews and the next summarizing the extracted sentences . Related work Historically , most work tackling opinion summarization was extractive in nature ( Ku et al . , 2006 ; Paul et al . , 2010 ; Carenini et al . , 2006 ; Angelidis and Lapata , 2018 ) , relying on se - lecting a small subset of the input sentences that best capture the essence of the reviews . Extrac - tive abstraction continues to be a popular approach in the era of pre - trained language models ( Zhong et al . , 2020 ; Jia et al . , 2020 ; Kwon et al . , 2021 ; Gu et al . , 2022 ; Ahuja et al . , 2022 ) . On the other hand , abstractive approaches ( Carenini et al . , 2006 ; Ganesan et al . , 2010 ; Di Fabbrizio et al . , 2014 ; Bražinskas et al . , 2020b ; Isonuma et al . , 2021 ) to summarizing reviews have traditionally been rarer , though they have become more popular ( Liu and Lapata , 2019a ; Bražinskas et al . , 2020b ; Amplayo et al . , 2021b ; Isonuma et al . , 2021 ) in recent years . In terms of system design , most frequently , an end - to - end model is used to generate summaries ( Liu and Lapata , 2019b ; Du et al . , 2022 ; Ahuja et al . , 2022 ) . Multi - stage approaches ( Chen and Bansal , 2018 ; Li et al . , 2021 ; Zhang et al . , 2022 ) , on the other hand , have recently shown great promise . For instance , Li et al . ( 2021 ) extracts relevant ev - idence spans and then summarizes them to tackle long documents . Recursive summarization has been explored in ( Wu et al . , 2021b ) for book sum - marization , but involved ﬁne - tuning of GPT - 3 to the task . Other approaches can also generally be represented as a summarization pipeline . For in - stance , a mixture - of - experts re - ranking approach as in Ravaut et al . ( 2022 ) can be considered as a two - step approach where the combine function ranks and the ﬁlters the outputs of ﬁrst stage . 3 GPT - 3 Summarization Pipelines 3 . 1 Extractors First , extractors select relevant parts of a set of reviews , optionally conditioned on an aspect . Our extractors include : GPT - 3 Topic Clustering We pass each sentence to GPT - 3 and prompt it to produce a single word that denotes the topic of the given sentence . Then , we map each of the produced words to the aspect that is closest to the produced topic using the L2 distance of GloVe ( Pennington et al . , 2014 ) word vectors . This deﬁnes a selection of sentences to be used for summarization . This step is only used for pipelines on SPACE , as FewSum is aspect - agnostic . QFSumm - long We use the aspect - speciﬁc ex - tractive summarization model introduced in ( Ahuja et al . , 2022 ) to extract up to n most relevant sen - tences from the input text . Since FewSum does not provide aspects , we prompt GPT - 3 for keywords based on a random selection of ﬁve reviews . These are then passed to QFSumm during summarization . The value of n is set to 3 by default , but we use n = 35 for the “long” variant . QFSumm was de - signed to allow extremely long inputs , and thus no truncation is required at this stage . Review Stratiﬁcation This involves clustering reviews by the reviewers’ scores ( available in the dataset ) and summarizing each cluster with GPT - 3 . If a cluster’s length exceeds GPT - 3’s upper limit at this stage , it is truncated to the maximum number of sentences that ﬁt . Instead of or in addition to extractors , we also utilize GPT - 3 - chunking in some of our pipelines . This refers to the chunking of the sentences from the prior step into non - overlapping chunks , which are then summarized individually by GPT - 3 . The results are then concatenated for the next step . We strive for the length of the chunks ( in sentences ) to be both as close to each other and to 30 as possible ; thus , when there are l sentences total to be chunked , we take c = (cid:100) l 30 (cid:101) to be the number of chunks , and SPACE FewSum Average # reviews per entity 100 . 00 22 . 41 Average # sentences per review 9 . 16 3 . 37 Average # words per sentence 17 . 56 12 . 12 Table 2 : SPACE and FewSum dataset statistics . allocate (cid:98) lc (cid:99) sentences to each chunk ( except the last one , which may have fewer ) . 3 . 2 Summarizers Our summarizers summarize the text at this stage one ﬁnal time , to produce the output summary . All of our pipelines use GPT - 3 as the summarizer . However , we also compare to the QFSumm ( Ahuja et al . , 2022 ) and AceSum ( Amplayo et al . , 2021a ) systems here . These are speciﬁed further below . GPT - 3 This refers to feeding the penultimate set of sentences to GPT - 3 with the prompt “Summa - rize what the X said of the Y : ” , where X is either “ reviewers ” or “ accounts ” ( based on whether GPT - 3 - chunking was used so far or not ) and Y is the aspect being summarized ( SPACE ) or just “ Prod - uct ” ( FewSum ) . The preamble is either “Here’s what some reviewers said about a hotel : ” or “Here are some accounts of what some reviewers said about the hotel” in the case of SPACE . The word “ hotel ” is replaced by “ product ” for FewSum . QFSumm We use QFSumm ( Ahuja et al . , 2022 ) with the default setting of n = 3 sentences . AceSum The model from ( Amplayo et al . , 2021a ) . Since it has no means of controlling the length of the output , we only use AceSum as a standalone model and not as a part of any pipeline . These building blocks ( extractors and summariz - ers ) are composed to build various summarization pipelines . We list the compared pipelines in Table 1 . An illustration of one pipeline ( TCG ) is shown in Figure 1 . Since topic - wise clustering is unneces - sary for FewSum ( due to lack of aspects ) we only compare G ( GPT - 3 ) , CG ( Chunking + GPT - 3 ) , QG ( QFSumm - long + GPT - 3 ) and Q ( QFSumm ) for it . We include vanilla GPT - 3 for comparison in FewSum , as most products reviews’ ﬁt into its max - imum window size ; the longer ones are truncated to the largest admissible number of sentences . The table also lists some entries that are the ﬁrst stages of pipelines that begin with GPT - 3 - chunking which we also compare against in the automatic metrics section . Pipeline ROUGE - 1 ROUGE - L BERTScore SPACE Q 19 . 2 16 . 7 85 . 4 A 32 . 4 30 . 2 89 . 8 TCG 23 . 5 20 . 6 88 . 7 QG 25 . 1 22 . 1 89 . 1 TQG 25 . 2 22 . 3 89 . 0 RG 23 . 0 20 . 5 88 . 5 FewSum - Amazon Q 27 . 0 24 . 3 86 . 2 G 27 . 0 23 . 9 88 . 7 QG 26 . 2 23 . 7 88 . 4 CG 25 . 7 22 . 9 88 . 6 FewSum - Yelp Q 23 . 8 20 . 6 84 . 3 G 26 . 1 21 . 4 88 . 4 QG 27 . 1 22 . 1 88 . 5 CG 26 . 5 21 . 5 88 . 2 1 Table 3 : ROUGE - 1 , ROUGE - L , and BERTScore ( F1 ) for the compared models . 4 Evaluation 4 . 1 Datasets SPACE ( Amplayo et al . , 2021a ) involves the summarization of reviews of hotels along the as - pects { general , rooms , building , cleanliness , lo - cation , service , food } and provides three human - written summaries for each ( hotel , aspect ) pair . Ta - ble 2 shows that the reviews of SPACE are too long to feasibly summarize with a non - pipelined system , despite text - davinci - 002 ’s large context window size . We exclude the general aspect from our experiments . FewSum ( Bražinskas et al . , 2020a ) contains product reviews from Amazon and Yelp . As op - posed to SPACE , FewSum is not aspect - oriented , and the reviews are typically much shorter . For many of the products , the combined length of the reviews falls bellow 900 words , enabling summa - rization with plain GPT - 3 . FewSum provides three golden summaries for only a small portion of the products . Across these two splits , FewSum pro - vides golden summaries for 32 and 70 products in the Amazon and Yelp categories respectively . We list SPACE and FewSum statistics in Table 2 . 4 . 2 ROUGE and BERTScore We compute ROUGE ( Lin , 2004 ) and BERTScore ( Zhang et al . , 2020b ) and show results in Table 3 . Golden summary 1 The room itself was very nice and quite large by normal hotel standards . The small kitchenette in the room included a coffee maker , microwave oven , and a refrigerator . The beds were comfortable and the sheets good quality , but the furniture is pretty dated , and the bathroom very tired looking and small . Golden summary 2 The rooms were large but the bathroom ( while comfortable ) was small . The beds were very comfortable , and the amenities ( a fridge , microwave , and coffee maker ) were a plus . Golden summary 3 The room itself was very updated , nice , and quite large . There was a very comfortable bed , a small kitchenette , coffee maker , microwave oven , and a refrigerator . The bathroom was very tired looking . QFSumm ( Extractive ) [ Q ] The Primrose is a good hotel for people who plan on staying just a few dates in Toronto and plan on only sleeping there . The hotel parking was a little expensive ( CAN $ 15 ) and the garage is compact so I would be careful if you drive a big car or SUV . The bathroom needed a little work but it was good enough for my needs . AceSum [ A ] The room was spacious and comfortable . The bathroomwas a bit big , but the bathroomhad a king size bed and a sofa . Topicwise Clustering + GPT - 3 - Chunking + GPT - 3 [ TCG ] Overall , reviewers thought the rooms were spacious , clean , comfortable , and a good value for the price . Some reviewers noted that the hotel seemed to be aging , with noise from the air conditioningunit and slow drainage in the shower , but these were not major concerns . Aspect : Rooms ( Hotel ID 182002 ) Figure 2 : Example summaries from TCG , Q and A along with the Golden Summaries from the SPACE dataset . Pipeline Intrinsic Factuality Extrinsic Factuality Faithful - ness Relev - ance TCG 2 . 85 2 . 99 4 . 86 4 . 60 TQG 2 . 86 2 . 95 4 . 83 4 . 32 QG 2 . 88 2 . 97 4 . 79 3 . 93 A 3 . 00 2 . 96 4 . 91 3 . 62 Q 3 . 00 3 . 00 4 . 88 2 . 30 Maximum 3 3 5 5 Fleiss - Kappa 0 . 64 0 . 49 0 . 49 0 . 64 Table 4 : Average ratings and agreement scores along the axes of Intrinsic / Extrinsic Factuality , Faithfulness and Relevance on the SPACE dataset . Colors indicate moderate ( light green ) and substantial ( darker green ) agreement , respectively . The BERTScores for AceSum as well as all GPT - 3 - related models are in in the range of 88 − 90 . QF - Summ lags behind the other pipelines on SPACE based on BERTScore , but otherwise differences in performance are unclear . AceSum achieves the highest ROUGE - 1 as well as ROUGE - L scores by far , and is followed by TQG and QG . QFSumm does particularly poorly based on the ROUGE scores . The scores are all in the same ballpark on FewSum , with it being difﬁcult to draw any conclu - sions . The GPT - 3 systems perform slightly better than QFSumm on the Yelp split ( in contrast to the Amazon split ) which we attribute to the smaller combined per - product review lengths of Yelp . We argue that these scores are not informative , and that they are at times unreliable when compar - ing the quality of two summaries . Figure 2 demon - strates this by way of an example . ROUGE and BERTScore have been critiqued in prior work as Pipeline Intrinsic Factuality Extrinsic Factuality Faithful - ness Relev - ance G 2 . 63 2 . 89 4 . 68 4 . 98 CG 2 . 72 2 . 95 4 . 73 4 . 98 QG 2 . 68 2 . 90 4 . 63 4 . 98 Q 2 . 96 2 . 98 4 . 52 4 . 92 Maximum 3 3 5 5 Fleiss - Kappa 0 . 38 0 . 34 0 . 08 0 . 03 Table 5 : Average ratings and agreement scores along the axes of Intrinsic / Extrinsic Factuality , Faithfulness and Relevance on the FewSum dataset . Colors indicate fair ( yellow ) and slight ( red ) agreement respectively . inaccurate indicators of summary quality ( Fabbri et al . , 2021 ; Liu and Liu , 2008 ; Cohan and Gohar - ian , 2016 ) , particularly as the ﬂuency and coher - ence of the outputs increases to near - human levels . For one , n - gram methods penalize GPT - 3 for gen - erating summaries in a slightly different style : “ The reviewers found the rooms to be clean ” instead of “ The rooms were clean . ” The actual mistakes in our systems generally include over - generalization and misrepresentation of viewpoints of popularities thereof , which are not well - represented by match - ing n - grams . Figure 3 shows an example of such errors . We therefore conclude that metrics bench - marking the summaries on different dimensions are necessary . 4 . 3 Human Evaluation To better target these factors , we manually evalu - ated the summaries of the pipelines TCG , TQG , AceSum ( A ) and QFSumm ( Q ) for 50 randomly Here’s what some reviewers said about a hotel : The location couldn’t be better and there are lots of restaurants around the hotel . We left early to get to the parks and arrived back late after shopping at the outlets , so it did us for a week . [ … ] Walking distance to many restaurants and shopping . The hotel is about 10 minutes from Disney and 20 from Universal or Sea World . [ … ] Has to be the word place I ever stayed . It’s not the best location as you do need to drive everywhere but that was all part of the fun for us ! ! Summarize what reviewers thought of the location : GPT3 ( G stage ) The vast majority of reviewers thought that the location was great . They especially appreciated how close it was to the park and to restaurants . A few reviewers thought that the location was not the best because it was not centrally located and you had to drive to get to most places . Figure 3 : An example of the errors made by GPT - 3 . The input is truncated for brevity . Most of the sum - mary is correct , but the viewpoint of a single reviewer is wrongly expressed as that of a “few reviewers” and generalized to the hotel not being centrally located , al - though the other reviews contradict this ( blue ) . chosen ( hotel , aspect ) pairs from the SPACE dataset , and QFSumm ( Q ) , GPT - 3 ( G ) , CG and QG for 50 ranomly chosen products ( 25 each from the Amazon and Yelp splits ) from the FewSum dataset . The axes of evaluation were : • Intrinsic Factuality : Does the summary accu - rately represent the popularity of opinions ? ( E . g . , a mistake would be “Some people said X , while others said Y” , where X is a com - monly raised argument while Y is very rare ) . • Extrinsic Factuality : Are all statements in the summary supported somewhere in the input ? ( E . g . , “The parking is expensive” , when park - ing is in fact free ) . • Faithfulness : Are the viewpoints selected the consensus ones ? Note that Faithfulness is dis - tinct from Intrinsic Factuality in that the for - mer is concerned with the selection of appro - priate viewpoints , while the latter ensures that the popularity of the viewpoint is not over - or under - stated in the summary . • Relevance : Are the points raised in the sum - mary are relevant to the aspect being summa - rized ? The three authors of this paper independently rated the summaries along the above axes on Likert scales of 1 - 3 for both variations of factuality , and 1 - 5 for faithfulness and relevance . The average human scores , along with the Krippendorff’s Al - pha and Fleiss Kappa scores ( measuring consensus among the raters ) are presented in Table 4 . We observe that among the compared pipelines , TCG improves upon TQG and QG substantially in terms of relevance . All three have a very high score under extrinsic factuality , underscoring our initial statement that models of the scale of GPT - 3 seldom make blatantly counterfactual statements . View - points selected by QFSumm are generally faithful , and factual due to their extractive nature , but may include statements not relevant to the aspect being summarized . Overall , AceSum and TCG perform better than the other pipelines . However , the differ - ences in Factuality and Faithfulness are small . We list the corresponding metrics for FewSum in Table 5 . CG tends to perform well , but the consen - sus is tenuous for Faithfulness and Relevance . The lack of aspects may contribute to the low agreement and small differences among scores on FewSum ; dimensions such as Relevance may be considered underconstrained , and thus more difﬁcult to agree upon , in this setting ( Kryscinski et al . , 2019 ) . 5 New Tools for Evaluation and Analysis Our human evaluation was both time - consuming and , in some cases , high variance . When a large number of reviews are presented to a system , it may be nearly impossible even for a dedicated evaluator to sift through all of them to evaluate a summary . We therefore investigate the question of how we can automate this evaluation using existing tools ; contrary to what’s typical , this may be more reli - able and unbiased than human evaluation . One of the chief areas where automatic evalua - tion may help is faithfulness . Since faithfulness represents the degree to which a system is accurate in representing general consensus , it requires mea - suring the proportion of reviews supporting each claim of a summary . A viewpoint with a larger support is more popular and , consequently , more faithful . Our key idea is to use entailment as a proxy for support . Past work ( Goyal and Durrett , 2021 ; Laban et al . , 2022 ) has used Natural Lan - guage Inference ( NLI ) models to assess summary factuality by computing entailment scores between pairs of sentences . The reviews of the hotel were generally positive , with most people finding the rooms clean and the staff apologetic . However , some found the carpets to be stained , and one reviewer reported dust balls in their room . Split and rephrase ( GPT - 3 ) Split and rephrase the following sentences into simple propositions : Sentence : The reviewers were mixed , with some praising the central location of the hotel , and some finding the surrounding area to be polluted . Output : The hotel is centrally located . The surrounding area of the hotel is polluted [ …more in - context examples ] The room was spotless . And… Will never come back here… The shuttle service is convenient… The rooms were clean . The rooms had dust balls . The staff was apologetic . The carpets had stains on them . Entailment 0 . 96 0 . 01 0 . 02 Figure 4 : Per - sentence entailment scores are calculated by taking the maximum among the various candidates . However , the summaries produced by GPT - 3 and related pipelines often consist of compound sentences that contrast two viewpoints . In addi - tion , GPT - 3 prefers to say “ The reviewers said . . . ” instead of directly stating a particular viewpoint . We found these artifacts to impact the entailment model . We use a split - and - rephrase step to split these sentences into atomic value judgments . This is achieved by prompting GPT - 3 as shown in Fig - ure 4 . We then use the zero - shot entailment model from SummaC ( Laban et al . , 2022 ) to compute the entailment scores for these atomic value judge - ments . Similar to approach in the SummaC paper , we observe that a summary statement is factual when strongly entailed by at least one sentence and thus select the top entailment score of each sum - mary sentence as its factuality score , and aggregate this score to produce per - system numbers . The choice of the model as well as that of using GPT - 3 for the split - and - rephrase step are explained further in Appendix A . However , a system could potentially game this metric by producing relatively “safe” statements ( like most reviewers found the rooms clean ) . We therefore also want to evaluate genericity . For the purposes of automatic evaluation , we distinguish between two types of genericity to help deﬁne each more precisely . First , semantic genericity refers to making common , non - speciﬁc value judgements of a product or service . On the other hand , lexi - cal genericity consists of using similar and often over - used words in the produced summaries . We tackle semantic genericity by framing it as the ag - gregate entailment score across sentences of differ - ent reviews , while lexical genericity is measured by using a TF - IDF - like metric . Pipeline Percentage of split - and - rephrased sentences with n supports SPACE n = 0 n = 1 n = 2 − 4 n = 5 + Q 8 . 1 29 . 0 21 . 2 41 . 8 A 7 . 7 8 . 6 12 . 7 71 . 0 First - TCG 18 . 7 16 . 8 18 . 1 46 . 2 TCG 22 . 8 16 . 9 19 . 4 41 . 0 QG 14 . 9 16 . 6 16 . 3 52 . 2 TQG 18 . 6 19 . 2 17 . 8 44 . 4 First - RG 23 . 7 22 . 0 19 . 9 34 . 4 RG 27 . 4 22 . 1 20 . 8 29 . 6 FewSum ( Amazon ) n = 0 n = 1 n = 2 − 4 n = 5 + Q 9 . 5 51 . 6 26 . 1 12 . 7 G 28 . 0 32 . 4 27 . 7 12 . 0 QG 27 . 6 34 . 7 23 . 6 14 . 2 First - CG 27 . 8 26 . 6 25 . 0 20 . 5 CG 31 . 9 32 . 2 22 . 6 13 . 3 ( Yelp ) n = 0 n = 1 n = 2 − 4 n = 5 + Q 8 . 3 46 . 2 31 . 3 14 . 2 G 27 . 2 24 . 3 29 . 3 19 . 3 QG 30 . 6 30 . 3 27 . 4 11 . 6 First - CG 24 . 4 25 . 6 26 . 8 23 . 3 CG 26 . 3 28 . 3 26 . 2 19 . 2 1 Table 6 : Percentages of split - and - rephrased sentences binned according to support sizes , for all compared pipelines . The threshold used is τ = 0 . 75 . 5 . 1 Terminology We establish further terminology here for a consis - tent discussion of the metrics presented later . The set of sentences in the summary of the reviews of a hotel h ∈ H w . r . t aspect a ∈ A are called S h , a . Passing these to the split - and - rephrase step gives us a set of split sentences Z h , a . For any two sentences s 1 , s 2 we denote the entailment score of s 2 with respect to s 1 according to the SummaC - ZS ( Laban et al . , 2022 ) model by e ( s 1 , s 2 ) ∈ [ − 1 . 0 , 1 . 0 ] . A score of 1 . 0 indicates perfect entailment while that of − 1 . 0 denotes complete contradiction . Finally , we denote by N k ( s ) the ( multi - ) set of k - grams ( with multiplicity ) of the sentence s . In particu - lar , N 1 ( s ) is the set of words in the sentence s . 5 . 2 Evaluation of Entailment We ﬁrst evaluate whether entailment is effective at identifying the support of the mentioned view - points by human evaluation . The three authors of this paper marked 100 random pairs ( 50 each from SPACE and FewSum ) of sentences and assertions entailed with a score above 0 . 5 on the scale of 0 − 2 . Here , 2 indicates that the assertion is completely supported , and 1 that the assertion’s general hy - Pipeline AverageTopScore Pipeline Average Top Score SPACE FewSum Q 91 . 59 ( Amazon ) ( Yelp ) A 92 . 49 Q 85 . 29 86 . 62 First - TCG 84 . 96 G 65 . 81 68 . 59 TCG 82 . 06 QG 67 . 63 65 . 04 QG 87 . 50 First - CG 68 . 34 69 . 86 TQG 84 . 68 CG 66 . 43 68 . 58 First - RG 81 . 54 RG 79 . 85 Table 7 : The Average Top Score for each pipeline on the SPACE and FewSum datasets . Pipeline AverageIDF Pipeline Average IDF SPACE FewSum Q 12 . 00 ( Amazon ) ( Yelp ) A 5 . 77 Q 4 . 38 4 . 33 TCG 8 . 40 G 3 . 02 2 . 93 QG 6 . 93 QG 3 . 10 2 . 93 TQG 7 . 82 CG 3 . 00 2 . 86 RG 8 . 87 Table 8 : Measurement of lexical genericity . Average IDF ( larger is better ) for the compared pipelines . The FewSum pipelines report lower ranges for average IDF due to fewer total number of documents . pothesis is supported , but some speciﬁcs are left out . The average score of the selection across the three raters was 1 . 88 with a Fleiss - Kappa consen - sus score of 0 . 56 ( moderate agreement ) . Many of the lower - rated entailed sentences also had lower entailment scores ( closer to 0 . 5 ) . The score illus - trates that the precision of the entailment approach is high . 5 . 3 Faithfulness : Support Set Sizes We propose an entailment metric for determining how the viewpoints in the summary reﬂect the con - sensus of the input . We ﬁrst compute per - sentence entailment scores as shown in Figure 4 . For each sentence of the split - and - rephrased summary , we measure the number of review sentences that entail it with a score greater than a threshold τ = 0 . 75 ( also called the “support” of the sentence below ) . This threshold was determined based on manual inspection . We bin these counts into 0 , 1 , 2 , 3 , 4 and 5 + . These numbers tell us the popularity of a presented viewpoint . The frequencies of the bins are converted to percentages and listed in Table 6 . On the Amazon split of FewSum , QG comes out on top with repeated summarization slightly Pipeline Genericity Percentage of scores greater than τ SPACE Q 0 . 640 64 . 6 A 0 . 828 82 . 8 TCG 0 . 781 80 . 1 QG 0 . 759 76 . 5 TQG 0 . 738 73 . 7 RG 0 . 788 80 . 0 FewSum ( Amazon ) ( Yelp ) ( Amazon ) ( Yelp ) Q 0 . 339 0 . 406 32 . 6 37 . 8 G 0 . 582 0 . 654 56 . 9 65 . 2 QG 0 . 565 0 . 653 53 . 9 64 . 7 First - CG 0 . 604 0 . 732 63 . 4 69 . 1 CG 0 . 554 0 . 682 56 . 7 68 . 1 1 Table 9 : Semantic genericity based on entailment , along with the raw percentage of scores above the threshold . The threshold used is τ = 0 . 5 . hurting the CG method . However , on the Yelp split , G and CG outperform other methods . This is likely tied to the fact that the Yelp split has fewer reviews per - product than Amazon , implying that it is much likelier for the combined reviews of a product to ﬁt in a manageable number of words . As a general takeaway , the “pure” GPT - 3 systems perform pretty well on the relatively short review sets of FewSum . As we move to the long combined lengths of the reviews on SPACE , however , the pure GPT - 3 pipelines start to fall behind others in terms of faithfulness . In particular , we observe that repeated summarization causes a major dip from First - TCG to TCG , indicating that this is not an attractive documentation for long - form inputs . QG outper - forms other GPT - 3 - related pipelines by a large mar - gin . As we saw in human evaluation , however , QG may include some irrelevant viewpoints in this process . Abating this behaviour by performing a topic - clustering step ﬁrst brings its numbers down to a level comparable with First - TCG , which is still more faithful than the TCG pipeline . AceSum has the largest number of statements with 5 + supports on the SPACE ; however , as we will see later , many of its summaries are very generic and support for them can be easily found among the large num - ber of reviews . Q has the smallest percentage of statements with no support because it is extractive . 5 . 4 Factuality : Top Score As depicted in Figure 4 , averaging the per - sentence entailment scores ( ﬁrst per - summary , then per - system ) gives us the Top Score metric . The average top score is a proxy for factuality since true state - ments will typically be strongly entailed by at least one sentence of the reviews . We list the computed average top scores in Table 7 . The scores are similar on FewSum , with QG and CG performing bets on the Amazon and Yelp splits . However , on the longer inputs of SPACE , the differences in factulity become more apparent . In particular , TCG and RG rank the lowest . In particular , to reconcile similar but distinct view - points , repeated summarization leads to a type of generalizing which can hurt factuality . Among the GPT - 3 pipelines , QG performs the best , followed by TQG . TQG yet again delivers performance com - parable to First - TCG and therefore presents a rea - sonable trade - off with some gains on factuality and increased relevance . 5 . 5 Genericity As mentioned before , we want to ensure that state - ments like the service was helpful , which are likely to be faithful and factual , are evaluated so that they could be penalized . ( AceSum in particular generates many such statements . ) We deﬁne gener - icity around two related but distinct concepts . We call the over - use of certain words , which makes a summary read as generic , lexical genericity . On the other hand , semantic genericity refers to statements that would even otherwise be widely and generally applicable to other products or services in the same class . To measure lexical genericity , we use the sen - tences from all summaries on the corresponding dataset as the set of documents to calculate an averaged Inverse Document Frequency ( IDF ) of the summaries , with stopwords removed and stem - ming applied . Since generic words are likely to occur more frequently and therefore have a low IDF , a smaller score indicates higher genericity . The scores calculated this way are listed in Table 8 . Our approach to measuring semantic generic - ity employs the observation that generic sentences from a summary are often widely applicable and thus likely to be strongly entailed statements from other summaries . We calculate the similarity sim ( S , S (cid:48) ) of two sets of sentences using the av - eraged top score , as Figure 4 shows . Similarly , we also measure the fraction frac ( S , S (cid:48) , τ ) of sen - tences whose top score exceeds a threshold τ . As semantically generic statements are broadly appli - cable to a number of entities and therefore likely to other products / hotels , Equation 2 computes the average similarity score between sentences that belong to two reviews by the same system but dif - ferent ( hotel , aspect ) pairs . Equation 3 computes the corresponding metric based on frac . We report these two metrics in Table 9 . N = # ordered pairs of different ( h , a ) ( 1 ) G = 1 N (cid:88) ( h , a ) (cid:54) = ( h (cid:48) , a (cid:48) ) sim ( Z h , a , Z h (cid:48) , a (cid:48) ) ( 2 ) F τ = 1 N (cid:88) ( h , a ) (cid:54) = ( h (cid:48) , a (cid:48) ) frac ( Z h , a , Z h (cid:48) , a (cid:48) , τ ) ( 3 ) On the short inputs of FewSum , all GPT - 3 pipelines have their ﬁgures in the same ballpark , with QG having marginally lower semantic gener - icity as compared to the other GPT - 3 pipelines . Unsurprisingly , QFSumm produces outputs that are highly speciﬁc , owing to its extractive nature . Moving to SPACE , however , the range of scores becomes much wider . Forced to reconcile multi - ple disparate opinions during repeated summariza - tion , TCG and RG produces summaries that are semantically much more generic than their coun - terparts . On the other hand , using QFSumm as an extractor gives semantically speciﬁc summaries , but also ones that are generally shorter and use common words much more , leading to their low lexical speciﬁcity ( high lexical genericity ) . A topic - clustering and selection step prior to extraction with QFSumm helps improve both measures of generic - ity . We also note that AceSum is the most generic on both fronts , and by a very large margin in the case of lexical genericity . 5 . 6 Comparison with ROUGE Evaluation Axis Entailment - Based Metric ROUGE Factuality 0 . 36 0 . 05 Faithfulness 0 . 29 - 0 . 03 Table 10 : Spearman Correlation Coefﬁcients of the entailment - based metrics and ROUGE with human judgements . Our entailment - based approaches set out to mea - sure Factuality and Faithfulness ; how well do these correlate with our human evaluation ? We compute Spearman’s Rank Correlation Coefﬁcient on the human - annotated SPACE examples with the av - eraged annotator scores , as the consensus among rater scores was high on that dataset . In particular , we use the average of the Intrinsic and Extrinsic Factuality scores among the three raters as the net human score on Factuality on any example , and the mean score on Faithfulness as that for Faithfulness . Correspondingly , we consider the Top Score metric as the automatic measurement of Factuality and the percentage of statements with 3 or more supports as Faithfulness . We list the obtained Spearman Correlation Coefﬁcients in Table 10 . While there is still room for stronger metrics , we see encour - aging signs of progress , as the introduced metrics correlate much better than ROUGE with human judgment on both axes . 6 Discussion and Conclusion With the advent of large language models like GPT - 3 , text summarization has reached near - human lev - els of ﬂuency and coherence . Opinion summariza - tion is no exception . At the same time , the qual - ity of opinion summaries produced by such sys - tems still lags behind human summaries in terms of faithfulness , and they tend to over - generalize viewpoints at the potential expense of factuality . Progress toward improving their quality along these axes calls for metrics that can measure factuality and faithfulness . Motivated by the increasingly many observations by recent work regarding the in - adequacy of n - gram based metrics to capture these subtleties , we venture a ﬁrst step in this direction . We propose the use of entailment as a proxy for support . This leads us to develop metrics that mea - sure the factuality , faithfulness and genericity of the produced summaries . GPT - 3’s window size is a severe limitation to summarization of long documents . Using the met - rics we deﬁne , we explore the impact of two sepa - rate approaches to control the size of the input via pre - summarization on two opinion summarization datasets . The experiments on FewSum demonstrate that with reasonably sized inputs , GPT - 3 and CG produce faithful as well as non - generic outputs . However , as we move to long - form review summa - rization , their factuality and faithfulness drop . Us - ing QFSumm , an extractive summarization model to select sentences from the input before summa - rization with GPT - 3 helps to alleviate this issue signiﬁcantly , while also astutely picking highly spe - ciﬁc viewpoints . The outputs produced this way , however , may include some irrelevant viewpoints and be generally shorter while also using compari - tively more general / common words . Pre - pending a topic - clustering step to the QFSumm + GPT - 3 pipeline allows us to trade away a portion of the gains on factuality and faithfulness while making the summaries appreciably less generic and more relevant . We hope that our efforts inspire future work to explore more robust metrics for opinion sum - mary evaluation , an essential step towards further progress in this area . Limitations Recent work has noted that language models may be susceptible to learning biases from training data ( Sheng et al . , 2019 ; Wallace et al . , 2019 ; Shwartz et al . , 2020 ) , and this phenomenon has also been ob - served for GPT - 3 ( Lucy and Bamman , 2021 ) . We did not stress test the models studied for biases and furthermore only experiment on English - language data ; we request that the users be aware of potential issues in applying the models presented . When properly used , the summarization models described in this paper can be time - saving . How - ever , as noted above , summary outputs may be factually inconsistent with the input documents or not fully representative of the input , and in such a case could contribute to misinformation . This issue is present among all current abstractive models and is an area of active research . References Ojas Ahuja , Jiacheng Xu , Akshay Gupta , Kevin Horecka , and Greg Durrett . 2022 . ASPECTNEWS : Aspect - oriented summarization of news documents . In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 6494 – 6506 , Dublin , Ireland . Association for Computational Linguistics . Reinald Kim Amplayo , Stefanos Angelidis , and Mirella Lapata . 2021a . Aspect - controllable opinion summarization . In Proceedings of the 2021 Con - ference on Empirical Methods in Natural Language Processing , pages 6578 – 6593 , Online and Punta Cana , Dominican Republic . Association for Compu - tational Linguistics . Reinald Kim Amplayo , Stefanos Angelidis , and Mirella Lapata . 2021b . Unsupervised opinion sum - marization with content planning . Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 35 ( 14 ) : 12489 – 12497 . Stefanos Angelidis and Mirella Lapata . 2018 . Sum - marizing opinions : Aspect extraction meets senti - ment prediction and they are both weakly super - vised . In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3675 – 3686 , Brussels , Belgium . Association for Computational Linguistics . Arthur Bražinskas , Mirella Lapata , and Ivan Titov . 2020a . Few - shot learning for opinion summariza - tion . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process - ing ( EMNLP ) , pages 4119 – 4135 , Online . Associa - tion for Computational Linguistics . Arthur Bražinskas , Mirella Lapata , and Ivan Titov . 2020b . Unsupervised opinion summarization as copycat - review generation . In Proceedings of the 58th Annual Meeting of the Association for Compu - tational Linguistics , pages 5151 – 5169 , Online . As - sociation for Computational Linguistics . Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert - Voss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel Ziegler , Jeffrey Wu , Clemens Winter , Chris Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , and Dario Amodei . 2020 . Language models are few - shot learners . In Advances in Neural Information Processing Systems , volume 33 , pages 1877 – 1901 . Curran Associates , Inc . Giuseppe Carenini , Raymond Ng , and Adam Pauls . 2006 . Multi - document summarization of evaluative text . In 11th Conference of the European Chap - ter of the Association for Computational Linguistics , pages 305 – 312 , Trento , Italy . Association for Com - putational Linguistics . Yen - Chun Chen and Mohit Bansal . 2018 . Fast abstrac - tive summarization with reinforce - selected sentence rewriting . In Proceedings of the 56th Annual Meet - ing of the Association for Computational Linguis - tics ( Volume 1 : Long Papers ) , pages 675 – 686 , Mel - bourne , Australia . Association for Computational Linguistics . Arman Cohan and Nazli Goharian . 2016 . Revisiting summarization evaluation for scientiﬁc articles . In Proceedings of the Tenth International Conference on Language Resources and Evaluation ( LREC’16 ) , pages 806 – 813 , Portorož , Slovenia . European Lan - guage Resources Association ( ELRA ) . Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 . BERT : Pre - training of deep bidirectional transformers for language under - standing . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171 – 4186 , Minneapolis , Minnesota . Associ - ation for Computational Linguistics . Giuseppe Di Fabbrizio , Amanda Stent , and Robert Gaizauskas . 2014 . A hybrid approach to multi - document summarization of opinions in reviews . In Proceedings of the 8th International Natural Lan - guage Generation Conference ( INLG ) , pages 54 – 63 , Philadelphia , Pennsylvania , U . S . A . Association for Computational Linguistics . Zhengxiao Du , Yujie Qian , Xiao Liu , Ming Ding , Jiezhong Qiu , Zhilin Yang , and Jie Tang . 2022 . GLM : General language model pretraining with au - toregressive blank inﬁlling . In Proceedings of the 60th Annual Meeting of the Association for Compu - tational Linguistics ( Volume 1 : Long Papers ) , pages 320 – 335 , Dublin , Ireland . Association for Computa - tional Linguistics . Günes Erkan and Dragomir R . Radev . 2004 . Lexrank : Graph - based lexical centrality as salience in text summarization . J . Artif . Int . Res . , 22 ( 1 ) : 457 – 479 . Alexander R . Fabbri , Wojciech Kry ´ sci ´ nski , Bryan McCann , Caiming Xiong , Richard Socher , and Dragomir Radev . 2021 . SummEval : Re - evaluating Summarization Evaluation . Transactions of the As - sociation for Computational Linguistics , 9 : 391 – 409 . Kavita Ganesan , ChengXiang Zhai , and Jiawei Han . 2010 . Opinosis : A graph based approach to abstrac - tive summarization of highly redundant opinions . In Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010 ) , pages 340 – 348 , Beijing , China . Coling 2010 Organizing Committee . Yanjun Gao , Ting - Hao Huang , and Rebecca J . Passon - neau . 2021 . ABCD : A graph framework to convert complex sentences to a covering set of simple sen - tences . In Proceedings of the 59th Annual Meet - ing of the Association for Computational Linguistics and the 11th International Joint Conference on Nat - ural Language Processing ( Volume 1 : Long Papers ) , pages 3919 – 3931 , Online . Association for Computa - tional Linguistics . Sebastian Gehrmann , Elizabeth Clark , and Thibault Sellam . 2022 . Repairing the cracked foundation : A survey of obstacles in evaluation practices for gener - ated text . arXiv preprint arXiv : 2202 . 06935 . Sebastian Gehrmann , Yuntian Deng , and Alexander Rush . 2018 . Bottom - up abstractive summarization . In Proceedings of the 2018 Conference on Em - pirical Methods in Natural Language Processing , pages 4098 – 4109 , Brussels , Belgium . Association for Computational Linguistics . Tanya Goyal and Greg Durrett . 2021 . Annotating and modeling ﬁne - grained factuality in summarization . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa - tional Linguistics : Human Language Technologies , pages 1449 – 1462 , Online . Association for Compu - tational Linguistics . Tanya Goyal , Junyi Jessy Li , and Greg Durrett . 2022 . News Summarization and Evaluation in the Era of GPT - 3 . arXiv . Nianlong Gu , Elliott Ash , and Richard Hahnloser . 2022 . MemSum : Extractive summarization of long doc - uments using multi - step episodic Markov decision processes . In Proceedings of the 60th Annual Meet - ing of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 6507 – 6522 , Dublin , Ireland . Association for Computational Linguistics . Junxian He , Wojciech Kry´sci´nski , Bryan McCann , Nazneen Rajani , and Caiming Xiong . 2020 . CTRL - sum : Towards Generic Controllable Text Summa - rization . arXiv . Masaru Isonuma , Junichiro Mori , Danushka Bollegala , and Ichiro Sakata . 2021 . Unsupervised abstractive opinion summarization by generating sentences with tree - structured topic guidance . Transactions of the Association for Computational Linguistics , 9 : 945 – 961 . Ruipeng Jia , Yanan Cao , Hengzhu Tang , Fang Fang , Cong Cao , and Shi Wang . 2020 . Neural extractive summarization with hierarchical attentive heteroge - neous graph network . In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan - guage Processing ( EMNLP ) , pages 3622 – 3631 , On - line . Association for Computational Linguistics . Joongwon Kim , Mounica Maddela , Reno Kriz , Wei Xu , and Chris Callison - Burch . 2021 . BiSECT : Learning to split and rephrase sentences with bitexts . In Pro - ceedings of the 2021 Conference on Empirical Meth - ods in Natural Language Processing , pages 6193 – 6209 , Online and Punta Cana , Dominican Republic . Association for Computational Linguistics . Wojciech Kryscinski , Nitish Shirish Keskar , Bryan Mc - Cann , Caiming Xiong , and Richard Socher . 2019 . Neural text summarization : A critical evaluation . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan - guage Processing ( EMNLP - IJCNLP ) , pages 540 – 551 , Hong Kong , China . Association for Computa - tional Linguistics . Lun - Wei Ku , Yu - Ting Liang , and Hsin - Hsi Chen . 2006 . Opinion extraction , summarization and tracking in news and blog corpora . In AAAI Spring Symposium : Computational Approaches to Analyzing Weblogs . Jingun Kwon , Naoki Kobayashi , Hidetaka Kamigaito , and Manabu Okumura . 2021 . Considering nested tree structure in sentence extractive summarization with pre - trained transformer . In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 4039 – 4044 , Online and Punta Cana , Dominican Republic . Association for Computational Linguistics . Philippe Laban , Tobias Schnabel , Paul N . Bennett , and Marti A . Hearst . 2022 . SummaC : Re - visiting NLI - based models for inconsistency detection in summa - rization . Transactions of the Association for Compu - tational Linguistics , 10 : 163 – 177 . Faisal Ladhak , Esin Durmus , He He , Claire Cardie , and Kathleen McKeown . 2022 . Faithful or extractive ? on mitigating the faithfulness - abstractiveness trade - off in abstractive summarization . In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1410 – 1421 , Dublin , Ireland . Association for Computational Linguistics . Mike Lewis , Yinhan Liu , Naman Goyal , Mar - jan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer . 2020 . BART : Denoising sequence - to - sequence pre - training for natural language generation , translation , and comprehension . In Proceedings of the 58th An - nual Meeting of the Association for Computational Linguistics , pages 7871 – 7880 , Online . Association for Computational Linguistics . Haoran Li , Arash Einolghozati , Srinivasan Iyer , Bhar - gavi Paranjape , Yashar Mehdad , Sonal Gupta , and Marjan Ghazvininejad . 2021 . EASE : Extractive - abstractive summarization end - to - end using the in - formation bottleneck principle . In Proceedings of the Third Workshop on New Frontiers in Summariza - tion , pages 85 – 95 , Online and in Dominican Repub - lic . Association for Computational Linguistics . Chin - Yew Lin . 2004 . ROUGE : A package for auto - matic evaluation of summaries . In Text Summariza - tion Branches Out , pages 74 – 81 , Barcelona , Spain . Association for Computational Linguistics . Feifan Liu and Yang Liu . 2008 . Correlation between ROUGE and human evaluation of extractive meeting summaries . In Proceedings of ACL - 08 : HLT , Short Papers , pages 201 – 204 , Columbus , Ohio . Associa - tion for Computational Linguistics . Yang Liu and Mirella Lapata . 2019a . Text summariza - tion with pretrained encoders . In Proceedings of the 2019 Conference on Empirical Methods in Nat - ural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3730 – 3740 , Hong Kong , China . Association for Computational Linguistics . Yang Liu and Mirella Lapata . 2019b . Text summariza - tion with pretrained encoders . In Proceedings of the 2019 Conference on Empirical Methods in Nat - ural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3730 – 3740 , Hong Kong , China . Association for Computational Linguistics . Li Lucy and David Bamman . 2021 . Gender and repre - sentation bias in GPT - 3 generated stories . In Pro - ceedings of the Third Workshop on Narrative Un - derstanding , pages 48 – 55 , Virtual . Association for Computational Linguistics . Michael Paul , ChengXiang Zhai , and Roxana Girju . 2010 . Summarizing contrastive viewpoints in opin - ionated text . In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Process - ing , pages 66 – 76 , Cambridge , MA . Association for Computational Linguistics . Jeffrey Pennington , Richard Socher , and Christopher Manning . 2014 . GloVe : Global vectors for word representation . In Proceedings of the 2014 Confer - ence on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1532 – 1543 , Doha , Qatar . Association for Computational Linguistics . Mathieu Ravaut , Shaﬁq Joty , and Nancy Chen . 2022 . SummaReranker : A multi - task mixture - of - experts re - ranking framework for abstractive summarization . In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 4504 – 4524 , Dublin , Ireland . Association for Computational Linguistics . William Saunders , Catherine Yeh , Jeff Wu , Steven Bills , Long Ouyang , Jonathan Ward , and Jan Leike . 2022 . Self - critiquing models for assisting human evaluators . arXiv . Abigail See , Peter J . Liu , and Christopher D . Manning . 2017 . Get to the point : Summarization with pointer - generator networks . In Proceedings of the 55th An - nual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1073 – 1083 , Vancouver , Canada . Association for Computa - tional Linguistics . Emily Sheng , Kai - Wei Chang , Premkumar Natarajan , and Nanyun Peng . 2019 . The woman worked as a babysitter : On biases in language generation . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan - guage Processing ( EMNLP - IJCNLP ) , pages 3407 – 3412 , Hong Kong , China . Association for Computa - tional Linguistics . Vered Shwartz , Rachel Rudinger , and Oyvind Tafjord . 2020 . “you are grounded ! ” : Latent name artifacts in pre - trained language models . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 6850 – 6861 , Online . Association for Computational Linguistics . Kaiqiang Song , Lin Zhao , and Fei Liu . 2018 . Structure - infused copy mechanisms for abstractive summariza - tion . In Proceedings of the 27th International Con - ference on Computational Linguistics , pages 1717 – 1729 , Santa Fe , New Mexico , USA . Association for Computational Linguistics . Eric Wallace , Shi Feng , Nikhil Kandpal , Matt Gardner , and Sameer Singh . 2019 . Universal adversarial trig - gers for attacking and analyzing NLP . In Proceed - ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter - national Joint Conference on Natural Language Pro - cessing ( EMNLP - IJCNLP ) , pages 2153 – 2162 , Hong Kong , China . Association for Computational Lin - guistics . Jeff Wu , Long Ouyang , Daniel M . Ziegler , Nisan Sti - ennon , Ryan Lowe , Jan Leike , and Paul Christiano . 2021a . Recursively Summarizing Books with Hu - man Feedback . arXiv . Jeff Wu , Long Ouyang , Daniel M . Ziegler , Nisan Sti - ennon , Ryan Lowe , Jan Leike , and Paul Christiano . 2021b . Recursively summarizing books with human feedback . Song Xu , Haoran Li , Peng Yuan , Youzheng Wu , Xi - aodong He , and Bowen Zhou . 2020 . Self - attention guided copy mechanism for abstractive summariza - tion . In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 1355 – 1362 , Online . Association for Compu - tational Linguistics . Jingqing Zhang , Yao Zhao , Mohammad Saleh , and Pe - ter Liu . 2020a . PEGASUS : Pre - training with ex - tracted gap - sentences for abstractive summarization . In Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 11328 – 11339 . PMLR . Tianyi Zhang , Varsha Kishore , Felix Wu , Kilian Q . Weinberger , and Yoav Artzi . 2020b . Bertscore : Evaluating text generation with BERT . In 8th Inter - national Conference on Learning Representations , ICLR 2020 , Addis Ababa , Ethiopia , April 26 - 30 , 2020 . OpenReview . net . Yusen Zhang , Ansong Ni , Ziming Mao , Chen Henry Wu , Chenguang Zhu , Budhaditya Deb , Ahmed Awadallah , Dragomir Radev , and Rui Zhang . 2022 . Summ n : A multi - stage summarization framework for long input dialogues and documents . In Proceed - ings of the 60th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Pa - pers ) , pages 1592 – 1604 , Dublin , Ireland . Associa - tion for Computational Linguistics . Ming Zhong , Pengfei Liu , Yiran Chen , Danqing Wang , Xipeng Qiu , and Xuanjing Huang . 2020 . Extrac - tive summarization as text matching . In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 6197 – 6208 , On - line . Association for Computational Linguistics . A The approach chosen for Entailment In line with our motivation , we would like to be able to utilize an NLI ( Natural Language Infer - ence ) model to retrieve entailment scores of the produced summaries with respect to the input re - views . We tested a number of approaches includ - ing BERTScore , due to it being trained on entail - ment / contradiction pairs , but ﬁnally settled on us - ing the zero - shot model from SummaC ( Laban et al . , 2022 ) to produce the entailment scores . Sum - maC is already becoming a standard evaluation tool and is easy to set up . We chose to forego the trained “Conv” SummaC model as we found that it did not generalize well to the kind of data we were working with . Speciﬁcally , two common is - sues were that ( 1 ) the range of scores assigned to the sentences from the reviews was very small , and ( 2 ) sometimes ( especially for the most weakening statements ) the scores assigned to the sentences seemed arbitrary and did not make a lot of sense . In comparison , the zero shot model had neither of these issues . This issue is highlighted in Figure 6 in the Appendix . Further , typically a proposition X is not consid - ered to entail statements of the form “ The reviewers said X ” , or “ X and Y ” , where Y is another propo - sition . Accordingly , the entailment scores are not very high for these two cases . We highlight this in Figure 7 . Thus , we decide to split and rephrase all sentences of the produced summary to simple value propositions for all entailment - related met - rics . Note that here rephrasing also includes remov - ing any attribution such as “ The guests said . . . ” . We considered a number of models to this end , includ - ing BiSECT ( Kim et al . , 2021 ) and ABCD ( Gao et al . , 2021 ) , but found two common issues with all of them : • The split sentences maintained the words from the original sentences , so a sentence such as “ The food was received well but it was served late ” would have one output part as “ It was served late ” , which requires a round of en - tity disambiguation to follow the split - and - rephrase step . • These models do not remove attribution of viewpoints as we would like . • A statement such as “ I liked the setting of the movie but not its cast ” produces one of the outputs as “ Not its cast ” , which does not make any sense by itself . Pipeline Complexity ( % ) Pipeline Complexity ( % ) SPACE FewSum Q 16 . 8 ( Amazon ) ( Yelp ) A 5 . 1 Q 14 . 7 7 . 8 First - TCG 28 . 6 G 36 . 1 31 . 9 TCG 30 . 7 QG 34 . 6 32 . 8 QG 27 . 0 First - CG 28 . 8 22 . 0 TQG 27 . 3 CG 27 . 5 19 . 6 First - RG 24 . 0 RG 30 . 7 1 Table 11 : Complexity as measured by the percentage of contrasting sentences . Thus , we utilize GPT - 3 to perform the split - and - rephrase task , with few shot prompting used to illustrate removal of attribution and other desired characteristics . We also experimented with hav - ing separate steps for split and rephrase , and found no signiﬁcant difference in the outputs or quality thereof . We utilize the split - and - rephrased sen - tences for all of the automatic metrics that involve entailment of any sort . B Complexity One of the challenges of opinion summarization is that sentences may contrast opinions : “ Most re - viewers liked the service , but there were a few complaints about sluggish response times . ” We quantify the percentage of simple and contrasting statements in the model outputs , since it is subtly related to the extent of expression of opposing view - points . We use the original ( non - split ) sentences for this purpose , and classify a sentence as contrasting if it contains one or more words from the set K = { ’while’ , ’but’ , ’though’ , ’although’ , ’other’ , ’others’ , ’however’ } , as Equation 4 depicts . We present these percentages in Table 11 . C = (cid:80) h ∈H , a ∈A (cid:80) s ∈ S h , a 1 ( N 1 ( s ) ∩ K (cid:54) = ∅ ) (cid:80) h ∈H , a ∈A | S h , a | ( 4 ) We note that AceSum produces the smallest per - centage of contrasting statements . We see that topic - wise clustering pushes up the number of con - trasting statements for QG . We hypothesize that this is because when bringing together statements with the same topics in a cluster two opposing state - ments are likelier to fall into the same chunk . In cases where two opposing statements fall into dif - ferent chunks , say X and Y , the chunks are likely to each contain statements similar to others in the Hereiswhatsomereviewersthoughtofahotel : StayedhereduringSakura - Conwithagroupofteens . MyroomwaswarmandIdiscoveredthefanwason , buttheA / Cwasnot . ThethermostatwaswornsoyoucouldnottellwhichsettingwasA / Candwhichwasheat . Theyuseenergysavingcompactfluorescentbulbssotheroomwasnotverybrightforreading . Greatpricingcomparedtootherhotelsnearby . NottomentionwhenIcheckedintheyyelledmyroomnumberouttome , notagoodideaasafemaletravelingalone . . . Prettybasicinformationforaconciergetoscrewupon ! WhenIhadgotbacktomyroomthefirstdayIwastherehousekeepinghadstillnotbeentomyroom . Ihadleftat9amanditwasthen3 : 30 . IcalleddowntothefrontdesktofindoutwhattimehousekeepingwouldbecomingbyasIwantedtohaveanapbeforeIwenttothegame . Hasfitness ( Ididnotuseit ) , freeWi - Fiandadeskwhereyoucanprinte . g . Don ' t know the concierges name , it would be the olderman in the morning but he had great recommendations forrestaurants , each one excellent andwas so knowledgeable aboutthecityandwhenwasthebesttimestogotoevents . The reviewers said that thehotelwasgoodvalue , but that the room was not very well - lit and that housekeeping was slow . They also said that the conciergewashelpful . Summarize what the reviewerssaid : The guests said that the room was warm and the A / C was not working . The thermostat was worn and it was difficult to tell which setting was for the A / C . The light bulbs in the room were not very bright . The guests said that the hotel was a great value compared to other hotels in the area . The guests said that when they checked in , the staffmemberyelled out theirroom number , which was not a good ideafora female guesttraveling alone . Theguestssaid thatwhentheygotback to theirroom , housekeepinghadstillnotbeenthere . Theycalledthefrontdesktofind out when housekeeping would be coming and were told that they would be there soon . The guests saidthat the hotelhas fitness facilities andfree Wi - Fi . The guests said that the concierge was very knowledgeable about the city and made great recommendationsforrestaurants . - The hotel was overpriced compared toothersinthearea - The room was not very bright , and thethermostatwasdifficulttouse - The housekeeping staff was slow andunresponsive - The concierge was helpful and knowledgeable Insummary , theguestssaid : Figure 5 : Aspects of summarization such as verbosity or the format of output are affected by the speciﬁc wording of the prompt . “The hotel is situated close to restaurants and shops . ” The location is a little out the way for Universal , Prime Outlets ( formerlyBeltz ) , IDriveetcbutit ' sonly5minutesfromDisney , you can see the top of Expedition Everest from the car park at the back ! ! Weateatasteakhouseclosebycalledthecoloradohouseofbeef , it will ringyou up about 70 $ for two , and you eat well , with good service . The parking lot was clean , the location of the hotel was GREAT ( right next to many restaurants , souvenir shops , and within 5 minutestotheDisneyWorldParks ) . Fantastic location wit restaurants , gift shops etc all within walking distanceofthehotel ( IdidthewholethingonfootasIdon ' tdrive ) . ThelocationoftheHotelisnottoofarfromthemeparks . 0 . 55 0 . 55 0 . 55 0 . 55 0 . 55 Supporting There ' saWalmartanda Targetstore about 20 minutesWestfrom the hotel , some shopping arease ( groceries , pharmacy etc ) are closerby . The location is really good , it is across the street from a supermarketrestaurantsandotherusefulthings . VeryclosetoSuperTarget , OliveGarden , Publixyounameit ! Don ' tknowwheretherentalcarswerelocated ? ! ? ! ? TheonethingIwasabit ' meh ' about ; youhavetopayforice . 0 . 52 0 . 52 0 . 53 0 . 53 0 . 53 Weakening Figure 6 : The top 5 supporting and weakening sentences from the reviews for the statement “ The hotel is situated close to restaurants and shops ” as found by the Conv SummaC model . The corresponding entailment scores are included in parentheses . We see that the scores are very close to each other , and that the “weakening” statements do not weaken the statement at all . These issues led us to use the zero shot model instead . same chunk . Thus , the summaries of those chunks are likely to be highly contrasting and thus increase the above measure even more for the ﬁnal stage , as is observed above for TCG . C Abstractiveness We further investigate how the choice of the pipeline affects abstractiveness . To measure this , we calculate the percentage of k - grams in the sum - maries that do not appear in the input reviews for k ∈ { 3 , 4 , 5 } . For this we use the original ( non - split ) sentences from the output summaries . The results are tabulated in Table 8 . Since QFSumm is a purely extractive model , it is no surprise that Q has low abstractiveness . The numbers are non - zero due to some quirks of QF - Summ about splitting into sentences - this leads to some partial sentences ending up next to each other . The next stand - out is that A has very low abstractiveness . This is in line with our observa - tion that even though AceSum is abstractive , it tends to highly generic observations such as “ The rooms were clean ” , which very likely appear almost verbatim in some user reviews . We also observe that QG has a relatively low abstractiveness and that topic clustering drives up abstractiveness . We suspect that the above is a result of GPT - 3 sim - ply mashing together some sentences when pre - sented with chunks containing highly disparate sen - tences ( since it is hard to ﬁnd a common thread among them ) , which promotes extraction over ab - straction . Another observation is that multi - GPT - “The room was warm . ” Theroomwasverycold . - 1 . 00 Theheaterwouldnotturnon . - 0 . 85 Theheaterwasbroken . - 0 . 63 I can ' t believe they are still using heaters from a decadeago ! Insummerstheroomcangetverywarm . Wefoundtheroomwarmandcozy . They give you fur lined blankets . . . majestic and fitsthecold . . . brrr ! I don ' t see the use of fur lined blankets in this scorchingsummer . The heater was broken , but thankfully we didn ' t needtouseit . Theheatersavedallofusfromfreezingtodeath . Thesummersherecangetveryhot - ourroomfelt likeanoven . HeatersbutnoA / Cinthisheat . . . uff . 0 . 11 0 . 89 0 . 98 - 0 . 99 - 0 . 22 - 0 . 19 0 . 57 0 . 97 0 . 10 “The reviews said that the room was warm . ” Theroomwasverycold . - 0 . 96 Theheaterwouldnotturnon . - 0 . 53 Theheaterwasbroken . - 0 . 50 I can ' t believe they are still using heaters from a decadeago ! Insummerstheroomcangetverywarm . Wefoundtheroomwarmandcozy . They give you fur lined blankets . . . majestic and fitsthecold . . . brrr ! I don ' t see the use of fur lined blankets in this scorchingsummer . The heater was broken , but thankfully we didn ' t needtouseit . Theheatersavedallofusfromfreezingtodeath . Thesummersherecangetveryhot - ourroomfelt likeanoven . HeatersbutnoA / Cinthisheat . . . uff . - 0 . 02 0 . 00 0 . 94 - 0 . 96 - 0 . 42 - 0 . 08 - 0 . 10 0 . 01 - 0 . 24 “The room was warm and the rugs were clean . ” Theroomwasverycold . - 1 . 00 Theheaterwouldnotturnon . - 0 . 63 Theheaterwasbroken . - 0 . 82 I can ' t believe they are still using heaters from a decadeago ! Insummerstheroomcangetverywarm . Wefoundtheroomwarmandcozy . They give you fur lined blankets . . . majestic and fitsthecold . . . brrr ! I don ' t see the use of fur lined blankets in this scorchingsummer . The heater was broken , but thankfully we didn ' t needtouseit . Theheatersavedallofusfromfreezingtodeath . Thesummersherecangetveryhot - ourroomfelt likeanoven . HeatersbutnoA / Cinthisheat . . . uff . - 0 . 03 0 . 00 0 . 00 - 0 . 99 - 0 . 71 - 0 . 03 0 . 00 - 0 . 01 - 0 . 74 Figure 7 : The scores of three statements with respect to a set of sentences , highlighting the issues with directly using the model output to compute entailment scores . Scores rounded to three decimal places are included in parantheses before the corresponding sentences , with important lines highlighted in color . We note that quoting a proposition as said by someone else or having multiple propsitions in the same sentence serve to cloud entailment scores . Pipeline Percentage of novel n - grams n = 3 n = 4 n = 5 Q 4 . 3 5 . 3 6 . 3 A 30 . 1 61 . 7 79 . 1 First - TCG 71 . 9 87 . 4 92 . 8 TCG 78 . 3 93 . 1 97 . 5 QG 62 . 1 81 . 0 88 . 2 TQG 70 . 4 86 . 4 92 . 6 First - RG 71 . 6 87 . 2 92 . 6 RG 79 . 1 93 . 0 97 . 1 SPACE Pipeline Percentage of novel n - grams Amazon Yelp n = 3 n = 4 n = 5 n = 3 n = 4 n = 5 Q 4 . 5 5 . 7 7 . 0 4 . 2 5 . 4 6 . 6 G 93 . 1 97 . 5 98 . 8 94 . 4 97 . 9 99 . 4 QG 91 . 0 95 . 5 97 . 7 94 . 2 97 . 9 99 . 0 First - CG 91 . 8 96 . 3 98 . 1 92 . 9 96 . 6 97 . 9 CG 91 . 8 96 . 2 97 . 9 93 . 3 97 . 0 98 . 0 FewSum 1 Figure 8 : Abstractiveness as measured by the percentage of novel n - grams when compared with the source reviews 3 - gram Abstractiveness P e r c en t age o f s en t en c e s w i t h 5 + s uppo r t s 0 25 50 75 0 25 50 75 100 Figure 9 : Average Top Score v / s Abstractiveness on the SPACE dataset . 3 pipelines ( TCG and RG ) are more abstractive than single - GPT - 3 ones , since there are two rounds of abstraction as opposed to one . All the GPT - 3 - derived pipelines are highly abstractive in the case of FewSum . This is unsurprising , since the com - bined length of the reviews in the case of FewSum is much smaller when compared to SPACE , and therefore there are relatively fewer propositions to compress into general statements . Motivated by ( Ladhak et al . , 2022 ) , we display the line graph of the average Top Score v / s 3 - gram Abstractive - ness for the SPACE dataset in Figure 9 . The trio of QG , TQG and TCG deﬁne the best frontier on the Factuality - Abstractiveness tradeoff , followed by RG , then A and Q .