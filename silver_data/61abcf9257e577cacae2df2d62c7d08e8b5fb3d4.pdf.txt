1 Iceberg Sensemaking : A Process Model for Critical Data Analysis and Visualization Charles Berret Linköping University , Sweden Tamara Munzner University of British Columbia , Canada Abstract We offer a new model of the sensemaking process for data science and visualization . Whereas past sensemaking models have been grounded in positivist assumptions about the nature of knowledge , we reframe data sensemaking in critical , humanistic terms by approaching it through an interpretivist lens . Our three - phase process model uses the analogy of an iceberg , where data is the visible tip of the schema underneath it . In the Add phase , the analyst acquires data , incorporates explicit schemas from the data , and absorbs the tacit schemas of both data and people . In the Check phase , the analyst interprets the data with respect to the current schemas and evaluates whether the schemas match the data . In the Refine phase , the analyst considers the role of power , articulates what was tacit into explicitly stated schemas , updates data , and formulates findings . Our model has four important distinguishing features : Tacit and Explicit Schemas , Schemas First and Always , Data as a Schematic Artifact , and Schematic Multiplicity . We compare the roles of schemas in past sensemaking models and draw conceptual distinctions based on a historical review of schemas in different philosophical traditions . We validate the descriptive and prescriptive power of our model through four analysis scenarios : noticing uncollected data , learning to wrangle data , downplaying 2 inconvenient data , and measuring with sensors . We conclude by discussing the value of interpretivism and the virtue of epistemic humility . Keywords : sensemaking , schemas , process models , data science , epistemic humility 1 . Introduction Data science has a problem : the messy politics of knowledge . Critical studies of data , algorithms , and visualization have grown into a vital interdisciplinary field that challenges the basic assumptions of data science and demands greater accountability for the social impact of data - driven systems ( Barocas 2017 , boyd 2011 , boyd 2012 , Correll 2019 , Dörk 2013 , O’Neil 2016 , D’Ignazio 2016 , Crawford 2021 ) . Cases of algorithmic injustice have been tied to targeted advertising ( Cadwalladr 2018 ) , search engines ( Noble 2018 ) , predictive policing ( Shapiro 2017 , Brayne 2017 ) , parole decisions ( Angwin et al . 2016 ) , loan approval ( Eubanks 2018 ) , and the general reinforcement of prejudice and other forms of injustice ( Benjamin 2019 , Broussard 2018 ) . Recent critiques of data science locate the source of these concerns not only in the outcomes of data science , but also in its philosophical underpinnings ( D’Ignazio & Klein 2020 , Jones 2021 ) . Like many other quantitative fields , data science is typically grounded in positivism : the philosophical stance that objective truth is accessible through experimental methods based in empirical measurement , and that proper application of these methods will yield results that are independent from the analyst’s own perspective and position in the world . Although the basic tenets of positivism are rarely stated aloud , a positivist stance toward knowledge and methodology predominates across data science , visualization , and many other scientific fields ( Drucker 2019 , Meyer & Dykes 2019 ) . Many data 3 analysts are at least implicitly positivist in treating large - scale analysis techniques as though they can minimize the subjective dimensions of human judgment . Objections to positivism are often grounded in interpretivism , the philosophical stance that knowledge is created through acts of interpretation , that many different interpretations of a given subject are possible , and the viability of different interpretations can be judged through critical examination of their assumptions , cogency , ethical implications , and explanatory value . For an interpretivist , you cannot separate data from human values , contexts , and practices ; doing so is always an erasure . To resurface these erasures , interpretivist critiques often examine structural power , ideology , and other matters taken for granted as the natural order of things . When aimed at data science , the ultimate thrust of interpretivist critique is that any effort to separate data from the messy politics of human knowledge is a fundamental misunderstanding of data itself . Our work is directed at models of the sensemaking process , in which we search for new understanding of a subject by gathering data and analyzing it to answer task - specific questions . Several influential sensemaking models in data science and visual analytics have been proposed in fields such as HCI , statistics , and visualization ( Russell et al . 1993 , Grolemund & Wickham 2014 , Sacha et al . 2014 ) . All of these models have their theoretical foundations in positivism and , as we will argue , are susceptible to the same failings as other positivist approaches to data science . Our core contribution is a new model for sensemaking that is built on interpretivist principles . The Iceberg Sensemaking model starts with the idea that data itself is not the fundamental basis of knowledge , but rather the tip of a larger iceberg constituting frameworks of knowledge , often called ‘schemas’ in the sensemaking literature . 4 We identify and discuss in depth four important features of our model that differentiate it from previous sensemaking models . Following a distinction made by sociologists and historians of science ( Polanyi 1958 , Collins 2012 , Jones 2015 ) , we argue that the schema , a longstanding feature of sensemaking models , needs to be considered at two different levels : Tacit and Explicit Schemas . As interpretivists , this distinction points to a difference between the explicit schema provided as documentation or annotation of a dataset , and the tacit schema encompassing disparate , unstated features relating to the creation , context , interpretation , or implications of a dataset . Distinguishing between the tacit and explicit schema allows for both descriptive analysis of and prescriptive guidelines for the interplay between these two parts of a schema : the part we actively work with ( explicit ) and the part that remains unexamined until it is identified and articulated ( tacit ) . A second feature of our model is to place Schemas First and Always in sensemaking . Everyone carries tacit schemas into the sensemaking process through past experience , social conditioning , and other factors that guide their interpretation of a given problem . We thus argue that schemas come into play before even a scrap of data is considered , and these schemas exist throughout the sensemaking process as vital components at every stage . This stance is a key point of distinction with current sensemaking models , where schemas are often treated like they enter the process only after some sensemaking work has already taken place . The third feature of our model is to treat Data as a Schematic Artifact : every dataset rests on top of a schema that serves as the foundation for the seemingly raw facts and figures it contains . To deploy a familiar analogy , we treat data as the tip of the iceberg in sensemaking . A dataset ' s explicit schema 5 is visible just under the waterline , while the tacit schema that guides its original creation and later interpretation is a larger , submerged mass concealed in the deep waters of structural , hegemonic , disciplinary , and interpersonal power ( Collins 1990 ) — until and unless we go to the trouble of exploring it . Treating a dataset as ground truth rather than a designed artifact constitutes a failure to map out the tacit schemas and thus ignore the schematic structure that has already given shape to it . When schemas are explicitly acknowledged and well - mapped , many pitfalls of data analysis can be avoided . Any time an analyst goes to the trouble of scrutinizing the assumptions underlying a particular dataset , they are already treating data as a schematic artifact . The fourth feature of our model is our insistence upon Schematic Multiplicity , the active consideration of multiple schemas throughout the sensemaking process . We assert that responsible data analysis requires acknowledgment of alternative perspectives . This guiding principle serves as a check on the tendency to carry a single schema all the way to the conclusion without testing it against other possibilities . In the following sections , we discuss related work on process models and sensemaking , followed by an in - depth discussion of schemas in different philosophical traditions as a secondary contribution . We then present our main contribution , the Iceberg Sensemaking process model . We then validate the utility and transferability of this model by using it to analyze four data analysis scenarios : noticing uncollected data , teaching data wrangling , downplaying inconvenient data , and measuring with sensors . We conclude by discussing the value of interpretivism and the virtue of epistemic humility — the recognition that knowledge itself is as multitudinous , complicated , and potentially flawed as the human beings who construct that knowledge . We emphasize that epistemic humility does not mean 6 giving up on the pursuit of scientific knowledge , but rather attuning ourselves to its limits and dealing with those limits more frankly by following an interpretivist model of the sensemaking process . 2 . Related Work A process model classifies actions according to a set of stages , offering practical guidance on the order and transition criteria for progressing through those stages ( Meyer 2015 ) . A sensemaking model is a type of process model that centers the human analyst in the act of gathering and interpreting information , capturing the “how” of a data analysis workflow , breaking it down into stages , and outlining the transitions that lead to a conclusion . We now discuss three major sensemaking models proposed in previous work . These summaries focus on the role of schemas and related concepts in each model . 7 2 . 1 . Sensemaking in HCI and Intelligence Analysis Fig . 1 : Pirolli and Card ( 2005 ) model of sensemaking . The figure has been redesigned to highlight the placement of schemas and schematization . Sensemaking was first described by Xerox PARC researchers Russell , Pirolli , and Card ( 1993 ) as the process of “finding a representation that organizes information to reduce the cost of an operation in an information task” ( p . 271 ) . Pirolli & Card’s ( 2005 ) subsequent model combines various sensemaking tasks from their earlier work to offer a single , unified articulation of the sensemaking process for intelligence analysis . This model has proved highly influential in a broad set of use cases , particularly in visual analytics . In the Pirolli & Card model ( Fig . 1 ) , the sensemaking process begins by gathering external data sources , then moving elements from these sources into a shoebox and evidence file before finally formulating a schema , leading to a hypothesis and ultimately a presentation of findings . The process moves forward and cycles back in an iterative refinement sequence , with loops at multiple levels of granularity . Pirolli & Card treat the construction of schemas 8 as the result of an active , rational , deliberate process in which the analyst fits the information to an apt and useful structure : “Schemas are the re - representation or organized marshaling of the information so that it can be used more easily to draw conclusions” ( p . 2 ) . As we argue below , this treatment of schemas only addresses the explicit schema constructed around a dataset , not the tacit schemas brought to the process by the analyst and implicit in the data itself . 2 . 2 . Sensemaking in Data Science and Statistics Fig . 2 : Grolemund and Wickham ( 2014 ) model of sensemaking . The figure has been redesigned to highlight the placement of schemas and process of searching for a relevant schema . Another highly influential model of the sensemaking process covering data science is given by the statisticians Grolemund & Wickham ( 2014 ) . Their model ( Fig . 2 ) describes the sensemaking process through a parsimonious set of loops in which schemas , insights , and information ( data ) are the central elements . Grolemund & Wickham break down the process of data analysis into an exploratory stage and a confirmatory stage . Exploratory data analysis begins with information , then the analyst searches for a relevant schema to explain it . Confirmatory data analysis begins with a schema , followed by a search for data to confirm it . This distinction draws on the statistician John Tukey ' s ( 1962 , 1972 ) seminal models of the data analysis process , which remain central to many modern 9 frameworks for interactive data analysis ( Hullman and Gelman , 2021 ) . Grolemund & Wickham define a schema as “a mental model that contains a breadth of information about a specific type of object or concept” ( p . 5 ) . As we illustrate below , the key distinction between this model and ours is that Grolemund & Wickham treat schemas merely as mental models and not an active component of the data they work with , postponing the consideration of schemas to the point when the analyst constructs one explicitly . 2 . 3 . Sensemaking in Visual Analytics Fig . 3 : Sacha et al . ( 2014 ) model of sensemaking . The figure has been redesigned to highlight the placement of knowledge , the closest approximation of a schema in this model . Sacha et al . ( 2014 ) take a visual analytics approach to sensemaking ( Fig . 3 ) that builds upon ideas from Pirolli & Card to place the interplay between visualization , models , and data as interrelated means of constructing knowledge for the human analyst . Their model also draws upon Norman ' s 10 ( 1988 ) gulfs of evaluation and execution , defining stages of interactions in which goals lead the analyst from hypotheses to actions ( execution ) and from findings to insights ( evaluation ) . Their use of the word ' knowledge ' corresponds to the role of ' schema ' in the other two sensemaking models , and they define it simply as ' justified belief ' . By treating knowledge ( schema ) as both the beginning and end point of the sensemaking process , this model is distinct from the previous two because it underlines that human understanding guides every stage of sensemaking . As we later discuss in greater detail , the major limitation of the Sacha model is its under - theorization of knowledge . 3 . Background : Three Ways of Understanding Schemas Although the term schema is a standard feature of many sensemaking models , it is rarely given close attention as a word with a complicated history and several distinct meanings . We present a brief keyword study ( Williams 1975 ) of schemas and related concepts to delineate different stances in philosophy , psychology , and critical theory , calling particular attention to the intersection of these theories with HCI and visualization design . We also briefly describe the use of the word ‘schema’ in the computer science database literature , mainly to distinguish the conventional sense of a database schema from the mental and social schemas that are more fundamental to the sensemaking process . This review spans a substantial historical and disciplinary range , answering a recent provocation from Meyer and Dykes ( 2019 ) for visualization researchers to give greater attention to the philosophical groundwork underlying the field . We group traditional theories of knowledge pertaining to schemas into three philosophical categories : transcendental , cognitivist , and interpretivist . The principal differences between these perspectives can be summarized as follows . In the traditional definition used by the philosopher Immanuel Kant , schemas exist in every human mind as our means of perceiving such basic phenomena as the 11 passage of time . For cognitivists , schemas are discrete , transportable structures of understanding that we acquire in the course of learning and finding order in the world . As we will show , the cognitivist approach to schemas in the sensemaking literature adopts a positivist sense without acknowledging its paradigmatic baggage . The alternative we propose is interpretivism , which entails viewing schemas as something we construct and acquire through social practices , systems , and institutions that offer a shared means of framing things in the world , albeit one that is highly contingent and contestable . While it is beyond the scope of this article to survey every major touchstone in these disparate theories of knowledge , our focus is to delineate a divergence in the treatment of schemas and articulate an alternative path not yet traveled in data science and related fields . In contrast to these three ways of thinking about schemas as patterns in the mind , the word ‘schema’ has a much narrower technical definition in the computer science database literature . A database schema is a model describing relationships between entities within a particular dataset or database ( Garcia - Molina 2022 ) . In data analysis , this kind of schema can be explicitly communicated through a data dictionary packaged with a dataset , or even through meaningful column headers in tabular spreadsheets . Although this definition is fairly distant from the philosophical concerns outlined below , it carries a historical and procedural connection to these other uses : a database schema is a technical instantiation of a mental schema , much like a schematic drawing in architecture or engineering . 3 . 1 . Transcendental Schemas 12 The sense of the word schema as a mental construct was coined by the philosopher Immanuel Kant , who took it from the Greek σχῆμα meaning shape or figure . In his Critique of Pure Reason ( 1998 [ 1781 ] ) , Kant described schemas as innate forms that shape the way perceptible phenomena appear to us . Schemas serve a crucial role in Kant’s elaborate theory of knowledge , mediating between our empirical sensation of the world and the transcendental concepts that exist only in our minds . For instance , Kant treats time as a schema because it is neither an inherent property of the world nor an ideal concept like a triangle , but rather a mental construct that fundamentally orders our understanding of things . While this origin story may seem distant from modern concerns , it usefully illustrates how the concept of the schema was introduced into both philosophy and modern science : schemas are ordering principles that easily go unnoticed because they serve as the starting point for knowing anything at all . The next two sections outline different ways of thinking about mental schemas today , marking a key distinction in modern approaches to the sensemaking process . 3 . 2 . Cognitivist Schemas and Positivism The term schema was revived in the early twentieth century by the psychologist Jean Piaget , whose cognitivist theory of mind treats schemas as basic units of understanding that we acquire through inherent cognitive faculties . For Piaget ( 1955 ) , schemas enter human cognition at the developmental stage when a child first grasps the permanence of objects . Piaget reasoned that something abstract ( a mental schema ) must be enabling them to think about this object in its absence . From this point forward , according to Piaget , we organize knowledge about the world through the acquisition of various schemas . This theory forms the initial basis of cognitive constructivism in psychology : the schema still carries the same basic importance to human understanding that Kant ascribed to this term , but with the caveat that we acquire schemas over time through basic cognitive faculties and 13 developmental processes . The fundamental power of this theory is its capacity to explain how a child apparently begins as a blank slate and develops into a wielder of various symbols , concepts , models , and categories . This cognitive approach to schemas influenced later models of the human mind derived from cybernetics ( Wiener 1948 , McCulloch 1965 ) , logical positivism ( Hempel 1952 , Ayer 1959 , Carnap 1958 ) , formal linguistics ( Chomsky 1956 ) , and analytic philosophy of mind ( Fodor 1968 , Putnam 1975 ) to form the interdisciplinary field of cognitive science , where schemas are treated as a priori , stable , objectively comprehensible mechanisms of understanding . For a cognitivist , once someone learns the schema for a histogram , truth table , or road map , these are schemas one shares with everyone else who knows them . From the 1950s through the 1980s , psychologists and computer scientists enjoyed a productive exchange of ideas that wedded this cognitive model of schemas to computational problems such as artificial intelligence ( AI ) . The early AI research of Simon ( 1969 ) and Newell ( 1959 ) , in particular , worked in pursuit of a general problem solver following the models developed by Turing ( 1950 ) , Shannon ( 1950 ) , and others who viewed computers and human minds as essentially the same thing : information processors capable of solving discrete problems through symbolic manipulation . Simon treated schemas as computational means of categorizing these problems , first by finding a discrete structure to represent each problem , then searching for available strategies that reliably lead to satisfactory solutions . This early AI work was overtly positivist in the assumption that human cognition reduces to the input of objective data , ordered within discrete schemas , and subjected to calculation . Minsky ( 1974 ) as well as Goldstein and Papert ( 1977 ) recast these schemas as ‘frames’ that order information in a given situation . 14 While these early cognitive approaches to AI eventually ran into limitations ( Lighthill 1973 ) , the application of cognitive science to human - computer interaction ( HCI ) and visualization led to many notable successes and technical breakthroughs even in these early years ( Sutherland 1962 , Engelbart 1962 , Kay 1972 , Papert 1980 ) . From 1970 forward , some of the most productive interdisciplinary exchanges between psychologists and computer scientists took place at Xerox PARC , where cognitivism informed fundamental R & D work on computer interfaces ( Card , Newell , and Moran 1983 ) . HCI researchers including Shneiderman ( 1982 ) and Hutchins , Hollan , and Norman ( 1985 ) recognized the benefits of direct manipulation of computers and often described their findings using a theoretical lens that included cognitive schemas . The foundational work in visualization from PARC closely adhered to a cognitivist framework , as laid out in the Cognitive Coprocessor Architecture ( Robertson , Card , and Mackinlay 1989 ) . This approach was followed by the vast majority of early information visualization researchers ( Card , Mackinlay , and Shneiderman 1999 ) . Another stream of psychology research involving schemas originates with Gibson ( 1966 ) , who departed from the strictly computational view of cognition with his theory of affordances , drawing attention toward the ways a subject may perceive or fail to perceive different uses of things in their environment . Norman ( 1988 ) made fundamental contributions to HCI by adapting Gibson ' s theory of affordances to design practices , describing the gulf of evaluation and gulf of execution as the key problems to be addressed by designers . Still , Gibson and Norman present a fairly positivist view of affordances as fundamental properties of objects rather than active constructions of their users . Although visualization as a field has matured and broadened over time , the cognitivist mentality has become the default for development and evaluation of new visual encoding and interaction techniques . With it has come the historical baggage of positivism , whether or not these researchers 15 actively identify as positivists . A recent survey of effective methods in visual data communication ( Franconeri et al . 2021 ) offers a useful index of progress in this field while also displaying the tendency to view schemas through a purely cognitive lens . Upon presenting a graphic that’s just an ambiguous assortment of circles , the authors write : “If you are having trouble extracting the data from this visualization , it is not your fault — you do not have the needed schema” ( 2021 , p . 132 ) . This explanation succinctly illustrates the role schemas serve in many cognitive accounts of data visualization today : schemas are treated as discrete , transportable skills that you either have at your disposal , or don’t . Uses of a particular schema by different individuals are treated as equivalent , which is undeniably practical in many research scenarios , but comes at the expense of examining the complex social , cultural , and political factors underlying the acquisition , use , significance , and limitations of these schemas . 3 . 3 . Interpretivist Schemas and Frames While cognitivism and positivism have yielded valuable research in visualization and HCI , these stances carry unexamined privilege as the assumed basis of the sensemaking process . There is nothing mysterious about the source of this privilege , given the longstanding affinity between cognitivism and positivism , but one consequence of building sensemaking models on these principles has been a failure to account for context , power , and other entanglements in the construction of knowledge . As interpretivists , we depart from cognitivists and positivists primarily in our emphasis on the multiplicity , variability , and contingency of the schemas that shape our understanding of things and people in the world . In this domain , schemas may also be called frames ( Goffman 1986 ) , sharing with Minsky ( 1974 ) as well as Goldstein and Papert ( 1977 ) an interest in delineating which things fall 16 inside and outside a given frame . Crucial to the interpretivist approach to frames is the recognition that any framework may be reframed — challenging its basic assumptions when we find the implications false , unjust , unsound , unfit for the intended purpose , or otherwise lacking . The interpretivist sees empirical data as inseparable from acts of interpretation that occur throughout the process of gathering and analyzing data . As many skeptics of data science are quick to remind us , raw data is something of an oxymoron concealing the interpretive work that already underlies any dataset from the moment we encounter it ( Bowker 2005 , Gitelman 2013 ) . While most early HCI research at Xerox PARC adopted a cognitivist and often positivist lens , as outlined above , the anthropologist Lucy Suchman ( 1987 , 2007 ) departed toward a more interpretivist approach in her research at PARC on the notoriously tricky case of photocopier interfaces . Suchman examined users’ interpretive tactics , and especially the breakdown of user understanding , as a result of highly contingent forms of reasoning on the fly while trying to follow instructions for navigating interfaces and other technical tasks . For Suchman , “crucial processes are interactional and circumstantial , located in the relationships among actors and between actors and their embedding situations” ( 2007 , p . 30 ) . Here , Suchman departed from the positivist tradition exemplified by Simon , arguing that schemas guiding understanding and action are highly contextual and symbolic in nature ( Suchman 1993 ) . Suchman ' s work has become something of a classic in Science and Technology Studies ( STS ) , an interdisciplinary field in which historians , sociologists , anthropologists , and philosophers examine the construction of facts and artifacts as consequences of human institutions , values , culture , and politics ( Winner 1980 , Pinch & Bijker 1984 , MacKenzie & Wajcman 1999 ) . Suchman ' s ethnographic work dovetails with the feminist STS theorist Donna Harraway ' s ( 1988 ) influential account of situated 17 knowledge , which treats any claim to knowledge as a positional standpoint of the individual and their context . The STS literature marks a critical departure from the positivist assumption that knowledge can be treated as objective and human - independent , instead emphasizing interpretive complexities , contextual factors , and systems of domination that shape scientific knowledge and technological systems . Another humanistic model of interpretation that dovetails with the STS literature was presented by Drucker ( 2019 ) in a recent IEEE VIS capstone as particularly suitable for critical approaches to visualization : hermeneutics . The basic idea behind hermeneutics is that we come to understand the whole of something by examining its parts , and the parts by examining the whole , tracing back and forth in this manner to continuously revise our understanding . This sequential , iterative process of interpretation is called the hermeneutic circle ( Dilthey 1990 [ 1900 ] , Heidegger 1996 [ 1927 ] , Gadamer 1977 ) . The hermeneutic circle is used across the humanities as a model of understanding centered on individual observers who may construct different interpretations of the same information due to positional differences . While deconstructionists ( Derrida 1976 ) emphasize the ultimate instability of meaning , today this stance is largely outweighed by milder , though still critical , forms of hermeneutics focused on the nature and consequences of interpretation . Drawing on these productive features of hermeneutic analysis , our model depicts sensemaking as an iterative sequence of interpretations tracing back and forth between schemas and data . Whereas hermeneutics tends to focus on sensemaking at the individual level , many interpretivists are social constructionists , emphasizing the role of culture , institutions , and power in the creation of meaning ( Berger & Luckmann 1967 ) . For a social constructionist , even qualities as basic as color , time , and temperature rest on contingent , subjective schemas for collectively making sense of the 18 world around us . To be a social constructionist does not require denying that these qualities ultimately emerge from physical reality , but rather insists that our subjective understanding precedes and makes possible any pursuit of knowledge . Even data that are gathered , ordered , calculated , and otherwise manipulated by the machine are also socially constructed because the schemas used by a machine emerged from social processes and institutional structures with specific histories . This point is the crux of many social constructionist critiques of science and technology , especially in the realm of data . Against the claim that facts can speak for themselves , social constructionists often reply that facts speak for the powerful , whose interests are served by the dominant interpretation ( Haraway 1988 , D’Ignazio & Klein 2020 ) . Critical theorists exemplified by Michel Foucault ( 1969 , 1972 ) developed this deeper constructionist concern with power by describing the specific ways that modern systems of authority assert control by gathering and manipulating information under the guise of objective knowledge . The power to label , sort , and categorize people and things into schemas becomes calcified in systems that reproduce themselves through disciplinary institutions such as schools , workplaces , prisons , and even mundane systems of classification ( Foucault 1979 , 1994 ; Bowker & Star 2000 ) . Foucault’s account of prisons and the construction of criminality has been particularly influential in critiques of data science , where algorithms for predictive policing and parole recommendations make use of schemas that reproduce the systemic racism of the criminal justice system ( Shapiro 2017 ) . When a social construction like ‘criminality’ is treated as a basic fact about people , even a perfectly sound sensemaking process will go awry by overlooking its own political entanglements . The algorithmic recommendations given to judges on a parole board may be presented as the calculated likelihood of a specific person committing another crime based on their demographics and past offenses , yet these algorithms are notably skewed in terms of race because 19 racialized populations are often systematically over - policed while white - collar crime is rarely even tabulated ( O’Neil 2016 , Karakatsanis 2019 ) . In addition to philosophical disagreements about the relative soundness of cognitivism versus critical approaches like interpretivism , we join other data critics in arguing that data - driven fields require theoretical foundations capable of identifying and mitigating injustice . Many interpretivist critiques of injustice in data science focus on algorithms and predictive systems like machine learning ( ML ) and AI rather than human sensemaking per se ( Ananny & Crawford 2016 , Noble 2018 , Eubanks 2018 ) . Yet schemas initially developed during early sensemaking analysis are often directly carried over into later automated systems , amounting to a form of tacit knowledge present in every algorithmic system ( Jones 2013 ) . We thus advocate for critical , interpretivist , and social constructionist approaches to data sensemaking as a direct response to these critiques , drawing attention to unjust or otherwise misleading schemas at the earliest stages of data science and visualization . This critique of data sensemaking thus carries through to ML and AI systems that attempt to automate and extend human sensemaking . 4 . Model Here we describe our process of devising this sensemaking model , present an overview of how it works , and discuss its four key features . 4 . 1 . Process This model was constructed through an iterative process of reflective synthesis that included several rounds of literature review . In our initial review of the sensemaking literature , we identified the 20 schema as a concept often under - theorized and inconsistently applied , suggesting that a careful examination of schemas could yield insights and new approaches to sensemaking . We then reviewed the literature on schemas , frames , and related concepts to conduct a keyword study of these terms and philosophical distinctions in their application and implications . Reflecting on these distinctions revealed that our basic assumptions as interpretivists often clashed with the broadly positivist assumptions guiding previous sensemaking models , leading us to develop an alternative sensemaking model from an interpretivist viewpoint with schemas as its core . Early in the model development process , we identified four key principles that constitute fundamental differences in the treatment of schemas within our model versus previous sensemaking models . We developed and fine - tuned the model itself through multiple rounds of iteration . In each round , we generated sketches of potential structures , then evaluated their descriptive and prescriptive power through detailed walkthroughs of how these models would apply to several driving scenarios for sensemaking . We developed these scenarios to illustrate the practical application of the model to concrete use cases , and to test the clarity and cogency of the developing model . In some cases we drew directly from the literature , where we found salient critiques of past sensemaking approaches ; in others , we devised an emblematic situation to investigate important implications . For each scenario , we describe both a positive and a negative variant . The positive case illustrates functional data analysis , adhering to the principles of the model ; the negative case illustrates dysfunctional sensemaking that this model serves to diagnose . When we noted shortcomings in the evolving model ' s ability to specify important aspects of these scenario contexts or provide guidance to analysts , we honed the model to sharpen its ability to describe and diagnose the sensemaking process in these practical scenarios . In turn , we also refined and augmented the scenarios as the model developed . In Scenarios ( Sec . 5 ) , we present these walkthroughs in detail . 21 4 . 2 . Overview Fig 4 : The Iceberg Sensemaking Model . The Iceberg Sensemaking model ( Fig . 4 ) is built around the central metaphor that data is the visible tip of an underlying schematic iceberg . In other words , data always grows out of schemas , not vice versa . We further distinguish between explicit and tacit schemas . The explicit schemas are recorded in some human - or machine - readable way that accompanies what is considered to be the data itself . These explicit schemas can range from annotations , to basic metadata such as column headers , to a complete data dictionary , to a detailed provenance record of how the dataset came to be . Tacit schemas are not explicitly recorded — some are matters of context , some constitute fundamental 22 assumptions in the dataset , and in many cases they are not immediately apparent to the analyst . Tacit schemas include undocumented aspects of the creation and transformation of a dataset , domain knowledge considered common sense by the dataset creators , and unconsidered assumptions that were not surfaced in any previous analysis . If you have ever found the need to stop and examine a previously ignored feature of a dataset that gives it context , you have articulated part of the tacit schema and moved it into the explicit schema . Findings are conclusions that have been evaluated , confirmed , and communicated as the result of the sensemaking process . A single iteration of the main sensemaking loop may include any or all of the three phases in sequence : Add , Check , and Refine . Each phase encompasses multiple actions . In the Add phase , the analyst acquires data , incorporates explicit schemas from the data , and absorbs the tacit schemas of both data and people . At the start of the sensemaking process , an initial dataset is acquired . Acquiring data necessarily results in the incorporation of tacit and explicit schemas underlying it . The absorbed tacit schema of the analyst is their own understanding of the subject matter , which guides them from the beginning of the sensemaking process . The analyst may also absorb the tacit schemas of outside parties , such as managers , colleagues , or other stakeholders . These new schemas may also be specific hypotheses or alternative viewpoints drawn from outside consultation and other information sources such as news coverage of the subject . In the Check phase , the analyst interprets the data with respect to their schemas , and evaluates whether the schemas match the data . Interpretation means using schemas , both tacit and explicit , to learn about what your dataset contains and draw basic inferences about features in the data , such as trends and outliers . Evaluation means looking at the data to determine whether your schemas are 23 accurate and sufficient to the task at hand . Work conducted in this phase may verify alignment or uncover mismatches between the data and schemas , particularly for newly acquired data or newly incorporated schemas . In the Refine phase , the analyst should consider the role of power in their work . This consideration may include questions about who made the dataset , people reflected in the data , potential absences in the data , people affected by the results of their analysis , assumptions made at the outset of the project , or even managerial pressure to produce certain findings . The analyst then articulates tacit schemas that were previously unstated . These tacit schemas may arise from any of the three phases : absorbing during the Add phase , evaluating during the Check phase , or considering power in the Refine phase . The articulation process moves previously unstated material into active consideration within the explicit schemas . Articulation of tacit schemas is a choice , and the choices an analyst makes about what to make explicit are fundamentally at the core of sensemaking outcomes . Although a complete articulation of tacit schemas is basically impossible because the full context of both the dataset and the analyst’s mindset may never be known , the more thoroughly we articulate schemas in conversation with other possible schemas , the greater our certainty that we have not taken for granted false , misleading , or even harmful considerations in the background of the sensemaking process . With this newly articulated material , the analyst may use the explicit schema to update the dataset by cleaning , sorting , filtering , recoding , restructuring , and otherwise adapting their datasets . Next , the analyst uses their data to formulate findings . These may be preliminary findings that lead the analyst to loop back and perform another add , check , or refine phase . Eventually the process ends , perhaps due to deadline pressure or the analyst’s subjective sense of satisfaction with the thoroughness of their results . In either case , the final formulation of findings marks the end of the sensemaking process . 24 A data analysis scenario could , in theory , involve only a single pass through the main Add - Check - Refine sequence , but like earlier models , the Iceberg Model encourages the looping nature of the sensemaking process . Analysts typically iterate with many passes through this loop , repeating the cycle of acquiring and incorporating , interpreting and evaluating , considering and articulating and updating , before formulating their final findings . Inner loops may occur at any phase ; for example , multiple rounds of interpretation and evaluation could happen before any refinement takes place . 4 . 3 . Key Principles We now expand on the four guiding principles of Iceberg Sensemaking , while highlighting contrasts with previous models . Tacit and Explicit Schemas Our model reflects the complex dual nature of schemas as both implicit mental models ( tacit schemas ) and articulated technical frameworks ( explicit schemas ) that are recorded and accounted for in data analysis . Delineating tacit and explicit schemas allows us to describe and prescribe a process of moving from the former to the latter . Previous models do not capture this aspect of sensemaking . When sensemaking models use the term schema to mean only the explicitly constructed framework of understanding , they fail to guide analysts to articulate and account for tacit background influences on the sensemaking process . When a sensemaking model deploys the term schema to mean only the explicit schema , it fails to account for knowledge , beliefs , interests , and biases implicit for both the 25 dataset and the analyst , thus neglecting to address and ameliorate possible sources of bias , power , systemic injustice , and even the basic blindspots of different professions and field of study . Pirolli & Card ' s ( 2005 ) treatment of schemas reflects only the creation of the explicit schema , eliding the tacit schemas at play from the beginning of the sensemaking process . The schema plays a larger role in the Grolemund & Wickham model ( 2014 ) , seemingly encompassing both tacit and explicit dimensions in the search for a relevant schema during the exploratory phase and incorporation of new insights during confirmation phase . Nevertheless , because Grolemund & Wickham elide this distinction , the schema is mainly treated as an explicitly understood model to refine , not an unknown factor to investigate . The Sacha et al . model ( 2014 ) reflects an overly optimistic treatment of the analyst ' s starting point as a solid block of reliable knowledge : we prefer to treat the assumptions , models , values , and motivations brought to the sensemaking process as schemas in order to underline that these are constructs in need of evaluation and assessment with respect to alternatives . Schemas First and Always Our interpretivist account of sensemaking always begins with schemas . Any account of the sensemaking process that begins without a schema , or some equivalent acknowledgement of the views and perspectives already in play , suggests that the analyst is able to search for data and make sense of a dataset in the absence of inherited values , interests , and frameworks of understanding . Likewise , every dataset carries existing schemas , both tacit and explicit , by virtue of the decisions surrounding the gathering , cleaning , and presentation that preceded the data’s arrival on the analyst’s desk . Crucially , these initial schemas are just the point of departure : schemas change , develop , and multiply throughout the sensemaking process . 26 Although the Pirolli & Card model ( 2005 ) does discuss schema formation as a framing process , it does not adhere to our Schemas First and Always principle . To treat schemas as devices that enter the sensemaking process midway is to foreground the explicit articulation of a schema , eliding the tacit schemas that shaped the analyst ' s thinking . This treatment of schemas places crucial information beyond consideration . Grolemund & Wickham ' s ( 2014 ) cognitive model also depicts schemas entering into sensemaking after the process is already underway . Their depiction of confirmatory analysis could be construed as a schema - first process in which one ' s initially held schema directs the search for relevant data . Yet in the earlier phase of exploratory analysis , the search for a relevant schema is treated as though it occurs without mental schemas already in play . For a cognitivist who treats schemas as discrete tools that we acquire and place in a kit for later use , it’s easy to imagine starting the sensemaking process with some schemas tucked away , others yet to be acquired , and a basically undecided stance in the absence of a working schema . We contend that schemas are the prime mover of the sensemaking process rather than a gear set in motion at a later stage . Because Sacha et al . ( 2014 ) begin and end with knowledge , the element closest to a schema in their model , it might seem to meet our Schemas First and Always principle . The trouble lies in their treatment of knowledge as a simple input defined as “justified belief . ” This definition once held some favor during the heyday of Logical Positivism ( Carnap 1967 [ 1928 ] ) but was decisively refuted a half - century ago ( Gettier 1963 , Quine 1969 ) by illustrating that justification is neither a necessary nor sufficient condition for knowledge . Furthermore , from an interpretivist perspective , justification is no simple property to locate in one belief versus another , but rather a social construction deeply entangled with structural power from the beginning of the sensemaking process . Crucially , what 27 counts as justification may lead to different sensemaking outcomes , so the criteria for justified belief should be treated as a variegated feature of different schemas introduced throughout the sensemaking process . Data as a Schematic Artifact Data is a designed artifact that arises from choices and acts of interpretation made by those who gather it . We assert that you cannot simply lay schemas on top of data ; rather , you need schemas to create data in the first place . Every dataset presupposes a schema that gives it structure and meaning . This principle guides our depiction of data as the tip of the schematic iceberg : data is the visible outgrowth of a schematic mass that is largely concealed , below the waterline . To overlook the primacy of schemas leads to the illusion that data lives a life apart from the messiness of human culture , society , and politics . None of the three major sensemaking models we analyze adhere to this principle . Every previous sensemaking model takes as a starting point the idea that data is a foundational input , fundamentally unschematized in its raw form . They treat schemas as the narrative and explanatory frame that we superimpose upon neutral facts in order to bring sense to them . The mindset of treating data as property , where the sensemaking process is intended to make something useful from it , is particularly entrenched in the logic of governments and corporations producing and using datasets . We note that the first sensemaking models emerged from the corporate context of a research lab ( Russell et al . 1993 ) , and were later articulated in the governmental context of intelligence analysis ( Pirolli and Card , 2005 ) . 28 The mindset of treating data as truth — as raw measurement of objective reality — is very common in the sciences , so a model that sensemaking " uses data to draw conclusions about the world " ( Sacha et al . 2014 ) feels very natural . The decidedly cognitivist approach of Grolemund & Wickham ( 2014 ) treats objectivity as the goal , and frames the subjectivity of sensemaking as a flaw to be ameliorated through the data analysis process ( 2014 , p . 16 ) . In contrast , our declaration that data is a schematic artifact frames data gathering as an intrinsically subjective process . Our model guides analysts on how to interrogate the repercussions of their own subjectivity , and that of the data gatherers , rather than ( fruitlessly ) attempting to eliminate it . We do recognize the utility of the positivist mindset in specific contexts , and do not seek to change it entirely : the tip of the iceberg is the place of congruence between our model and previous ones . The difference with the Iceberg model is an expansion of scope that also attends to what happened during the process of gathering the data , before moving on to drawing conclusions . Our model insists on the articulation of data ' s tacit schematization , even if an otherwise positivist stance is adopted in the sensemaking process . Schematic Multiplicity A sensemaking model that actively depicts the interaction of multiple schemas better matches the reality of data analysis than a single - schema model . Single - schema models depict the sensemaking process through the interplay of an analyst and their data in a conceptual vacuum , without consulting others or considering alternative perspectives and hypotheses . We specifically call for a sensemaking process that incorporates multiple schemas . At minimum there are two schemas in play , one accompanying a dataset and one in the mind of the analyst . Beyond this bare minimum , we advocate 29 for the explicit consideration of multiple schemas , especially in terms of multiple hypotheses that can be tested through data analysis . Among the process models discussed above , only the Pirolli & Card model ( 2005 ) supports the Schematic Multiplicity principle , and it does so only partially . They do allude to the importance of generating multiple hypotheses through annotation of the main model , followed with a brief discussion of this issue , but there is no guidance on how and when to pursue such multiplicity directly within the model itself . The Sacha et al . ( 2014 ) model does not explicitly support this principle , nor does Grolemund & Wickham ( 2014 ) . While Grolemund & Wickham ' s ( 2014 ) approach to data analysis is neatly functional and intuitively powerful , they specifically acknowledge that it suffers from a problem : the potential for an analyst to retain a single false schema throughout the sensemaking process . Our model combats this tendency with the prescriptive guidance to explicitly consider more than one schema . A single - schema approach to sensemaking lies at the root of this problem and others in conventional models of data analysis . We argue that these problems can be ameliorated by attending to one ' s tacitly held schemas up front , explicitly pursuing schematic multiplicity by considering alternative schemas , and interrogating these schemas throughout the sensemaking process . 5 . Scenarios We present four scenarios to validate the Iceberg Model’s descriptive and prescriptive power ( Beaudouin - Lafon 2004 , Shneiderman 2016 ) . 30 5 . 1 . Noticing Uncollected Data The use of data in law enforcement furnishes especially salient examples of the problems that arise when datasets about people and society are framed by the lens of state power . Consider the case of data - driven policing , in which the messy and biased data produced by law enforcement and the courts often reinforce the policing of race and poverty ( O’Neil 2016 ) . A skeptic such as Alec Karakatsanis ( 2019 ) , who wants to change the public discourse about how we define criminality , makes data - driven arguments to shine light on contradictions and omissions in how the people involved in law enforcement label and prosecute crimes at different levels of society . One of his core arguments is that only some laws , for some people , are enforced . 1A . Emphasizing Lower - Class Crime The legal scholar Alec Karakatsanis ( 2019 ) has illustrated fundamental bias in the US justice system’s pursuit , enforcement , and sentencing of crimes typically committed by different socioeconomic classes . Law enforcement data systematically inflates overpoliced , lower - class crimes such as shoplifting , while also deflating numbers for upper - class crimes such as wage theft that police are less likely to pursue in their daily work . Using such flawed datasets without attending to this political dimension amounts to a failure of the data sensemaking process . Consider a scenario in which a civic official uses raw police data to make a straightforward assessment of crime trends . In the Add phase , they acquire a dataset directly from the local police department showing crime trends over the last six months . The official incorporates the explicit 31 schema of this dataset while also absorbing tacit schemas from the dataset itself as well as from their own experience . In the Check phase , the official interprets the data using both the explicit schema that accompanies the dataset , as well as the tacit schemas that frame both the dataset and their own working practices . Likewise , they evaluate these tacit and explicit schemas against the dataset , albeit in a fairly limited manner because they view the dataset as basically reliable . They see nothing especially surprising in these numbers with the same general shape they are used to seeing : high rates of lower - class crimes like shoplifting in certain neighborhoods , and low rates of crime in upper - class neighborhoods . In the Refine phase , this official does not follow the guidance of the Iceberg Sensemaking model to consider the role of power in the construction of their dataset . As such , they pass over the articulate phase without surfacing elements of the tacit schema that might otherwise refine the naive explicit schema they are actively working with . The update phase may include some simple data wrangling , but otherwise leaves the original dataset mostly untouched . Finally , the official formulates findings based on a dataset , and these findings continue to reflect the general bias of criminal justice data rooted in the over - policing of racialized and lower - class crimes . 1B . Revealing Upper - Class Crime Now consider a sensemaking process where police data is subjected to critical scrutiny . In the Add phase , an activist acquires a dataset providing counts of criminal offenses and incorporates the dataset’s explicit schema for crimes such as shoplifting and petty theft as defined by the criminal justice system . They also incorporate the tacit schema that gives this dataset its as - yet - unarticulated context as a sociopolitical issue broader than its explicit schema . The activist also absorbs tacit 32 schemas from their own experience , as well as schemas drawn from other people such as journalists and researchers whose work informs their general understanding of this subject . In the Check phase , the activist begins by interpreting the data using both their tacit schemas and the explicit schema of the dataset . This initial exploration of the dataset shows that neighborhoods with high rates of poverty are high - crime areas , but in evaluating this trend he notes that even though it matches the explicit schema , the phenomenon is more complicated in light of tacit schemas not yet articulated in the data analysis process . In the Refine phase , they consider the role of hegemonic power in the dataset and recognize both omissions and suspect categorizations that skew the dataset toward the initial trends found in the check phase . For example , white - collar crimes such as embezzlement and wage theft are not even tracked , nor are thefts by the police themselves through civil asset forfeiture . By articulating these realizations and including them in the explicit schema , the activist either updates the dataset to reflect these realizations , or else loops back to the add phase and acquires more data about those crimes guided by this new explicit schema . When the activist is satisfied with their dataset , which now includes a more complete accounting of criminal behavior by the rich and powerful as well as the poor , they proceed to formulate findings . Their new conclusion is that the financial impact of the white - collar crime dwarfs the measures of theft initially reflected in police data , meaning that rich neighborhoods are the real crime hotspots , not the poor neighborhoods that are over - policed and over - represented in the standard law enforcement datasets . The tacit schema of the original dataset was a mirror reflecting the structure and activity of the criminal justice system , whose over - policing of racial minorities in impoverished locales is mistakenly treated as an objective proxy for the actual totality of criminal activity . 33 This scenario demonstrates the descriptive and prescriptive power of the Iceberg Sensemaking model . It describes the shortcomings of data used for predictive policing , explains how this system leads to algorithmic injustice through automated systems for predictive policing , and also prescribes improvements through the introduction of alternative schemas reflecting the disproportionate and underrecognized scale of white - collar crime . In this way , sensemaking to uncover data injustice can inform the responsible construction of more just algorithms . It can also be used to call out algorithmic injustice in action . This scenario reflects efforts to rebuke algorithmic injustice by inverting the typical , unjust model of predictive policing and redirecting it toward those who commit crime from a position of privilege ( Lavigne et al . 2017 ) . We encourage further critical sensemaking work in other domains in which algorithmic injustice has already been revealed , such as banking ( Eubanks 2018 ) and targeted advertising ( Cadwalladr 2018 ) . 5 . 2 . Learning to Wrangle Data In addition to the broad implications of the Iceberg Model for how we educate data scientists , it would be immediately effective for recasting lessons on data wrangling ( Kandel 2011 , Kasica et al . 2021 ) as exercises in scrutinizing the construction and limitations of datasets . This scenario follows an illustrative anecdote recounted by boyd ( 2021 ) in which she encourages data science students to critically explore a dataset before pulling answers from it . Here , the teacher distributes a dataset of police encounters in New York , asking students to find the average age of people apprehended under the city’s controversial and highly racialized Stop and Frisk policy . In the course of the lesson , the students first treat the data as ground truth and report a figure skewed by the messiness of the dataset , then update the data after realizing its messy nature . They finally consider aspects of the data’s construction that might lead to recognizing deeper limitations . 34 2A . Getting a Quick Answer In the Add phase , students first acquire the dataset from their teacher and import it into their analysis software . Because the dataset was gathered and published by the New York Police Department ( NYPD ) , the students have incorporated the explicit schema of concepts , categories , and demographics used by the NYPD while building this dataset . In addition , they have incorporated a range of tacit schemas , including some of the omissions and biases of official data discussed in earlier scenarios . Each student also has absorbed tacit schemas based on their implicit assumptions and past experiences . In the Check phase , the students begin by interpreting the dataset using only its explicit schema , treating it as an objective basis to calculate the average age of people stopped and frisked by the NYPD . The answer yielded by the dataset is 37 . The students quickly evaluate this figure against their tacit schemas and find the number reasonable . In the Refine phase , the students initially skip past consideration of power and do not articulate tacit schemas that may call it into question . They do not update the dataset before formulating findings based on the raw data , so they reach a straightford answer to the teacher’s question : the students say the average age for offenders recorded in this dataset is 37 . The teacher acknowledges that this answer follows directly from the data , but cautions against stopping analysis too soon and encourages the class to loop back and take a closer look . 35 2B . Looking Closer at the Data Returning to the Add phase to start a second loop of analysis , the teacher presents a new tacit schema for students to absorb , pointing out that extensive media coverage of the stop - and - frisk policy found that teenagers were usually targeted , making 37 a rather high figure for an average . She also presents an explicit schema to incorporate into their analysis plan , suggesting that the students should look at the distribution of ages instead of just the mean . The students then jump directly to the Refine phase to articulate this new idea in terms of an explicit schema about the expected age ranges for the targets of this police action . In a third analysis loop , the students return to the Check phase : the class re - interprets the dataset and re - evaluates its contents against the new explicit schema . The students notice that many entries record an age of “99” and evaluate that this number seems implausible , due to a mismatch of the explicit schema on the expected age range and their own implicit schemas on the demographics of the urban citizenry . Proceeding once more to the Refine phase , the students articulate the insight that “99” value is likely a placeholder value indicating unavailable data , so it should be excluded from calculations , and update their dataset to filter them out . They then re - formulate findings that reflect a more accurate average age . Although a basic data wrangling lesson might stop there , the instructor also encourages the students to undertake a fourth analysis cycle , with a self - loop back to Refine , to consider power from a critical perspective in the form of incentives for the people gathering the data . Why were so many ages recorded with the placeholder value ? The underlying motivation of police in conducting stop and frisk 36 is to exercise disciplinary power in this domain of surveillance and punishment , so the accuracy of the demographic data may be a lesser concern for them . The police may not choose to devote resources to providing clean data if there is no accountability for ensuring its accuracy , such as a structural oversight process that ties senior management bonuses to dataset quality metrics . They may even benefit from obfuscating their activities to reduce public scrutiny . The teacher then invites students to investigate the plausibility of other distributions in the dataset , to potentially surface more limitations before formulating their ultimate findings . Previous sensemaking models would not capture the interplay between data , power , and the various schemas at play in this scenario . Teaching data scientists to thoughtfully engage with politically charged and otherwise fraught subjects such as police data is a challenging remit , but fully tractable beginning with basic lessons like this one and scaling up to more advanced scrutiny of the concepts and categories that fundamentally frame the construction of a dataset . 5 . 3 . Downplaying Inconvenient Data The Iceberg Sensemaking model is also useful in focusing attention toward the ways that authority figures have interests that may direct the sensemaking process toward their own preferred conclusions . In some data analysis scenarios , the chain of command may introduce pressure to produce certain data analysis results . Consider the case of an analyst at an investment bank who is asked to evaluate a risk model for the bank ' s investment strategy . An aggressive strategy that minimizes cash reserves would result in very high yields in the short run for the bank , which would result in a multi - million dollar bonus for their manager . However , it presents substantial risks for the bank ' s liquidity and stability in the long run . The analyst in the following scenario is under considerable pressure from their manager to conclude that the high - yield path is the best approach . 37 3A . Acquiescing to Pressure In the Add phase , the analyst acquires datasets based on the bank ' s standard risk model . They incorporate both the explicit schemas for these datasets and the tacit schemas that give these datasets context . They also absorb the tacit schema of their manager , who is highly motivated to approve of the strategy that leads to personal enrichment . In the Check phase , analysis of the dataset shows that the high - risk strategy is unwise . However , the analyst interprets the warning as erroneous , because it does not align with their incentives to choose that strategy . They choose not to critically evaluate the validity of their schemas , even in the face of this warning . In the Refine phase , they choose to change a key assumption in their model , and re - generate a new dataset . In their second analysis loop , returning to the Check phase , the warning disappears . They proceed to the Refine phase and formulate findings that the high - risk strategy is sound . The bank profits immensely , and the manager reaps their millions . However , one year later , the bank fails , because the initial warning was in fact correct . This story of motivated reasoning essentially describes what happened with the Silicon Valley bank failure of 2023 ( Gilbert et al . 2023 ) . 3B . Questioning Authority We now consider how adhering to the Iceberg model could have changed the sensemaking outcome in this scenario . The Add phase proceeds as above . However , in the Check phase , the analyst takes the warning arising from the dataset seriously and evaluates whether their schemas harbor faulty assumptions . In the Refine phase , they consider the role of interpersonal power in their assumptions , 38 and realize that the pressure from their manager to green - light the pursuit of short - term profits should not lead them to dismiss the warning too hastily . Faced with a mismatch between dataset and explicit schema , they could either interpret the dataset as incorrect , or evaluate whether the explicit schema may be incorrect . Informed by their consideration of power , they articulate an explicit schema recognizing that very real risks may threaten the long - term viability of the bank itself . These two scenario variants also illustrate the benefits of schematic multiplicity as a means of ameliorating confirmation bias . If multiple perspectives and alternative framings are brought into consideration , it lowers the likelihood of carrying a false schema from the beginning of the sensemaking process all the way to its conclusion . A skeptic might say “this is just the result of bad analysis . ” In fact , that’s the point : other sensemaking models would fail to account for the role of power in the workplace and its effect on sensemaking . The absence of political considerations in other sensemaking models means these models are unable to explain cases where power knocks data analysis off the rails . 5 . 4 . Measuring With Sensors We finally consider the question of sensemaking with datasets that arise from measurements taken with sensors . At first glance , it may be tempting to assume that all such contexts are prime territory for a positivist framing , where interpretivism is an unnecessary diversion of attention . However , the Iceberg Model has explanatory value in showing how the sensemaking process can fail when bad data is certified as official and reliable while alternative data sources are ignored . Our model also has prescriptive value in showing how to avoid these failures . 39 4A : Ignoring Unsafe Conditions The Flint , Michigan water crisis in 2014 rested on a failure to properly test the city’s water for the presence of toxins like lead . Official samples were gathered from limited locations in affluent areas , even as utterly miasmic , poisonous brown water poured from faucets throughout the city’s poorer areas ( Goodnough et al . 2016 ) . Even when a dataset is gathered by law to monitor health conditions , flaws in the data can be routinely overlooked . The city of Flint had been regularly testing water , like every municipality in the United States , but flaws in the data were overlooked and dangerous conditions persisted for years because these flaws were not recognized . To understand how this failure of sensemaking could have occurred , we analyze the original state of Flint’s water monitoring through the lens of the Iceberg model . In the Add phase , the city acquired data about water quality that incorporated an explicit schema about where and how to conduct the tests . In the Check phase , the measurements obtained were interpreted as indicating acceptable quality levels . In the Refine phase , the city formulated findings about water safety . Notably , many of the actions suggested by the Iceberg model were not carried out . 4B : Recognizing a Water Crisis Imagine a new analyst who takes over water quality measurement in a city like Flint , shortly after governmental officials admit to problems in the water testing regime . In the Add phase , the analyst acquires data by taking samples of the city’s drinking water using the procedure established by their predecessor . They incorporate existing explicit schemas , including directives on where and how often to gather and test the water samples . At the same time , the limitations of this dataset are also incorporated as an implicit schema because this is essential context that has yet to be articulated . From our privileged perch as post - hoc observers of a water crisis , we recognize that the full story 40 behind the data has yet to be revealed because it lies implicit in a tacit schema . The tacit schema also includes the analyst’s personal knowledge , such as news reports of city residents forced to rely entirely on bottled water because their tap water is undrinkable . This knowledge is absorbed into the analysis process as a tacit schema . In the Check phase , the analyst interprets the given data and sees similar figures to those reported by their predecessor . The fact that the data show perfectly safe toxin levels leads them to evaluate their tacit schema and wonder whether reports of unsafe water are exaggerated , or if the dataset itself is flawed . Here , the value of schematic multiplicity is evident in the serious consideration of different hypotheses that may require new data and alternative forms of analysis . Proceeding to the Refine phase , the analyst considers the influence of structural power : they note that the limited water samples are taken at just a few houses with relatively new plumbing in a middle - class area , whereas their city also has many impoverished areas linked to a different segment of the water system . They articulate an explicit schema stating that a larger set of samples better representing the city as a whole may yield more accurate figures for water quality . They use this new explicit schema to update their dataset to reflect its current omissions . Now they loop back to the Add phase , acquire additional data from a more representative sample of neighborhoods , and proceed through the sensemaking process again to interpret that data and evaluate their schemas . In the final Refine phase , they formulate findings that reflect previously unreported hazards in the city’s drinking water . 4C . Quantifying Rainfall 41 Finally , consider a fairly straightforward data analysis scenario : rainfall sensors used by a government scientist to assess drought trends in their state . The sensors are placed at convenient locations to facilitate periodic calibration , and the data is gathered remotely , stored in a spreadsheet , and later analyzed to look for short - term changes and long - term trends . The process begins with the Add phase , when data is acquired from sensors . Acquiring data means incorporating explicit schemas , such as measurement parameters and actively documented contextual details like geolocation and calibration data . This phase of the process also involves absorbing tacit schemas inherent in the dataset and brought by the analyst , such as their background understanding of the landscape , local ecology , and their instruments . The analyst is familiar with the region , makes sound choices about where to place sensors , and uses their knowledge of local topography to distribute sensors for a representative sample . In the Check phase , the scientist interprets sensor data using an explicit schema specifying standard ranges given historical patterns , as well as tacit schemas involving a range of knowledge , skills , and context . For instance , their interpretation may be guided by tacit schemas in the form of hunches ( Lin et al . 2023 ) about anomalies in the data . In this case , the scientist notices that several sensors could be miscalibrated and flags them for examination . The analyst also uses the dataset to evaluate whether trends or anomalies may challenge their explicit or tacit schemas . For instance , if the data suggest a sharp decline in rainfall ( even without faulty sensors ) , this data may challenge the scientist’s everyday , on - the - ground intuition that precipitation had been fairly consistent with past trends . 42 In the Refine phase , the scientist considers the four possible varieties of power from the matrix of domination ( Collins 1990 , D’Ignazio & Klein 2020 ) in their data analysis task . For example , the definition of a drought in their state may be a contested political issue worth taking into consideration . If so , they should articulate this matter ; however , in this case , structural power in the form of local policies and their implications do not raise any red flags for their sensemaking process . Similarly , they conclude that the other three varieties of power and domination are not immediately relevant to their situation . They update the data by removing a few incorrect measurements from their dataset because some of the sensors were miscalibrated . Using this updated dataset , they formulate findings . In this scenario , it’s fine to work in the basic mode of positivism , treating measurements as a straightforward proxy for the phenomenon under study , while focusing attention mainly on sensor calibration , the soundness of your model , and accurate presentation of statistical findings . In cases like this , positivism is expedient and legitimate , granting speed and simplicity to the sensemaking process . Our argument is that data analysts must perform some due diligence in the examination of power before embracing a positivist stance , rather than simply assuming that a straightforward positivist approach to their subject is warranted . The Iceberg Sensemaking model thus accommodates positivism , treating it as a special case within a broader interpretivist framework . Previous sensemaking models could produce adequate descriptive and explanatory accounts of this scenario because the role of tacit schemas and power is largely uncomplicated . Yet such uncomplicated circumstances are not ideal for testing the value of a sensemaking model , which should also explain how sense emerges from problematic datasets . While a positivist approach to sensemaking may have descriptive power in well understood cases , it has poor explanatory value in 43 situations where sensemaking requires context , skepticism , and critique — qualities that characterize the best scientific work , yet often remain tacit and unarticulated . The Iceberg Model would diagnose the original shortcoming of the Flint water quality analysis as the failure to articulate tacit schemas or consider the influence of political and socio - economic factors in the sensemaking process . The placement of sensors could be affected by careless municipal policy , or even the misconduct of public officials . The more political a subject , the more likely that gaps and biases will be present in a dataset . In these cases , the typical positivist approach to data gathering and analysis may treat datasets as reliable and objective even though they are skewed in ways difficult to notice because hegemonic ways of thinking will suggest that the data is reliable and everything is fine . The Iceberg Model offers a means of describing both the unproblematic case of rainwater monitoring above , where the role of political influence is minimal , as well as politically problematic cases like water quality testing , in which sensemaking should include close scrutiny of power . 6 . Discussion We first discuss the value of interpretivism as our basic epistemic stance , then consider the general benefits of epistemic humility in data science . 6 . 1 . The Value of an Interpretivist Model In building this model , we have made an active effort to describe how positivist approaches to data analysis work when they work , while also explaining how they go wrong when they go wrong . A sensemaking model should be just as good at diagnosing how things go wrong as describing how 44 things work out well . The positivism of past sensemaking models is most evident in the absence of consideration for faulty and misleading data . Correcting this limitation requires more than just the inclusion of a cleaning phase , because the deepest problems with many datasets only emerge through scrutiny of the conceptual frameworks they take for granted . An interpretivist stance toward data requires the acknowledgment that data is ultimately rooted in human ideas , practices , values , and institutions . The positivist tendency to isolate data from these matters of human context ( tacit schemas ) can be expedient , but hazardous . The basic function of our interpretivist model of data science is to clearly present how existing uses of data are strengthened , not undermined , by starting from interpretivist principles and attending to interpretivist concerns at crucial stages of the sensemaking process . Our model is built on interpretivist principles because the most striking challenges to data science are based in interpretivist critiques . These critiques of data science have already informed works of investigative journalism ( Angwin 2015 , Propublica 2016 ) , the emerging industry of algorithm auditing ( O’Neil 2016 ) , and official inquiries into the social and political impact of tech platforms ( FTC 2016 ) . Some critics are data scientists , mathematicians , and computer scientists dissenting from within ( boyd 2012 , boyd 2011 , boyd 2021 , Correll 2019a , Correll 2019b , O’Neil 2013 , Broussard 2018 ) ; others come from sociology and the digital humanities , where explorations of data - driven methods within critical frameworks have already demonstrated the value of this interdisciplinary synthesis ( Drucker 2014 , 2019 ; D’Ignazio & Klein 2020 ) . As a rule of thumb , the more political a subject , the more fraught the implications of tacit schemas in data analysis , and the greater the need for interpretivist approaches to sensemaking . For example , 45 court records may seem like straightforward data , but they carry a tacit schema reflecting the entire structure of the criminal justice system . Likewise , calibrating a sensor to measure water quality may seem straightforward , but making use of the readings to assess public health concerns involves acts of interpretation with serious political implications . While data entry and automated data gathering could be seen as passive tasks that simply require application of the intended cognitive schema , sensemaking with these datasets requires active attention to the construction of interpretive schemas and articulation of the tacit schemas already in play . Although the causes of data bias are typically discussed in the context of ethics , they are just as much an epistemic matter in many cases where unfairness is tantamount to inaccuracy . In the specific case of visualization , the nested model of visualization design ( Munzner 2009 ) can be a useful guide for when to take an interpretivist or cognitivist stance . In the nested model , the abstraction layer is where the designer must make judgements about the tasks and data of target users , and this is prime territory for an interpretivist approach . Problem - driven design and requirements elicitation are processes that necessarily involve interpretation , judgment , and subjectivity . In contrast , the lower two levels of the nested model are more amenable to a cognitivist approach . At the idiom layer , which deals with visual encoding and interaction design choices , success is often gauged using methods such as controlled laboratory experiments where human performance is measured in terms of time and error . At the algorithm layer , where the goal is to develop automatic methods to instantiate particular idioms , success is often judged through computational benchmarks of system time and storage . Oftentimes these unproblematic evaluation criteria can be safely treated in the objective manner typical of positivism . However , one must be alert to the emergence of cultural , political , and ethical factors that call for full adoption of an interpretivist stance . 46 When confronted with a highly political subject , such as the tendency for police data to perpetuate systemic racial violence and inequality , cognitivism tends to be a fairly toothless theoretical stance . Cases of social injustice make an especially pointed case for the limitations of cognitivism when it comes to the messy politics of human knowledge . We argue that a purely cognitive approach to schemas is insufficient for a nuanced model of sensemaking , despite the past success of this theoretical stance in domains of HCI where cognitivism has been an apt , effective , and relatively unproblematic stance , such as scientific visualization and interface design . In cases where this approach does prove problematic , an interpretivist stance offers tools for moral and critical intervention in sensemaking processes . In calling for attention to moral dimensions of data science , the Iceberg Model is also aligned with D ' Ignazio and Klein ' s ( 2020 ) salutary call for data feminism , an approach to data that confronts power , embraces pluralism of perspectives , and surfaces the context in which data is collected and used . Rooting data science in an interpretivist theory of knowledge yields a more humane , ethical , and defensible stance toward sensemaking with data . We argue that a key dimension of the changes called for by D’Ignazio & Klein will be to account for the centrality , ubiquity , and variety of schemas at play in any sensemaking process . 6 . 2 . The Virtue of Epistemic Humility Recognizing these entanglements entails adopting a fundamental stance of humility with respect to data . Facts are never neutral , nobody is truly impartial , and matters of knowledge are inevitably shaped by concepts , measurements , models , and tools of our own creation . But this observation does not mean the world is unknowable — far from it . An interpretivist viewpoint calls attention to the 47 fact that knowledge originates in real human labor , ideas , and institutions . This framing means knowledge must be situated in human context ( Haraway 2018 ) , treating a multiplicity of perspectives and interpretations as elements of a more comprehensive picture than any one viewpoint . In order for knowledge to be constructed soundly and communicated clearly , it must always be framed . And while every framework leaves something outside its edges , there is value in making the framing process explicit because this underlines the purposeful decisions that lead to one conclusion rather than another . Some biases neatly isolate what matters to us , just as other biases lead us astray . Epistemology is the study of knowledge itself , and epistemic humility acknowledges how easily human beings fail in the pursuit of knowledge . Our interpretivist process model promotes the virtue of epistemic humility by foregrounding the messy complexity of knowledge as an omnipresent factor in sensemaking . This stance is especially needed in data - driven fields , where data ' s apparent ' givenness ' is implied in the etymology of the very word ' data ' , Latin for what is given . Opposition to this misnomer led Drucker to suggest the neologism capta to emphasize that this information is never just given but rather captured , gathered , ordered , and given meaning through people ' s actions ( Drucker 2011 ) . Whether we adopt the word capta or merely reform our assumptions about the nature of data , it is incumbent on data analysts to heed this distinction . While cognitivism has proved both popular and useful at the intersection of psychology and computer science , as a broadly positivist stance it risks drifting into a reductive view of data as objective facts that come with no perspective of their own . The philosopher Thomas Nagel ( 1989 ) has critiqued this naive realist stance as the fallacy of a view from nowhere , arguing that “the subjectivity of consciousness is an irreducible feature of reality — without which we couldn ' t do physics or anything else . ” Haraway ( 1988 ) labels the same problem as the god trick , in which the visual presentation of 48 scientific data gives the impression of an all - encompassing understanding like that of an omniscient deity . When data science goes astray , the cause is often this pursuit of an objective view from nowhere without attending to subjective and socially embedded dimensions of data like its context , construction , and ultimate limitations . We challenge practitioners to see data as an artifact that ultimately emerges from human interpretation and is often enmeshed in society and politics . The ideal outcome for data science is to continue developing its powerful affordances while adopting a more supple understanding of data as contingent , constructed , and contestable . 7 . Conclusion Data science has reached a remarkable moment as a relatively young field , but the political complexities of human knowledge present ongoing problems . For all the utility and appeal of data science , the basic conception of data as given raises many objections about the social impact of data - driven reasoning . Addressing this impact starts with human sensemaking even when its ultimate effects often emerge from automated systems using algorithms , ML , and AI . Matters of race , gender , policing , and economic inequality pose especially difficult challenges , but there is real promise in acknowledging the fundamental role of interpretation and the complicating effects of power in the sensemaking process . We thus offer an interpretivist model of sensemaking where datasets grow out of the schemas that give them structure , meaning and context , not vice versa . We distinguish between explicit and tacit schemas , rejecting the treatment of schemas as monolithic structures that previous models place late in the sensemaking process . We also emphasize the importance of examining multiple schemas , to avoid the limitations of a single perspective . We showcase the Iceberg Model ' s descriptive and prescriptive capacity through four scenarios that map the successes 49 and failures of the sensemaking process to the three phases and nine actions of the model . The result is a framework that confronts the limitations of positivism to address urgent criticism of data science . Disclosure Statement The authors have no conflicts of interest related to the work presented in this article . Acknowledgments The authors wish to thank Miriah Meyer , Ericka Johnson , Ben Shneiderman , Mark Hansen , Nick Diakopoulos , the UBC InfoVis group , and the Vis Collective at Linköping University for helpful feedback . Steve Kasica contributed to an early iteration of this model that focused on the work of data journalists . Rosalie Yu provided additional design work for Figure 4 . This work was supported in part by a grant from the Wallenberg Center for AI and Autonomous Systems ( WASP ) and by NSERC DG RGPIN - 2014 - 06309 . Works Cited M . Ananny and K . Crawford ( 2018 ) . Seeing without knowing : Limitations of the transparency ideal and its application to algorithmic accountability . New Media & Society ( 20 : 3 ) . J . Angwin , J . Larson , S . Mattu , and L . Kirchner ( 2016 ) . “Machine Bias , ” ProPublica , May 2016 , https : / / www . propublica . org / article / machine - bias - risk - assessments - in - criminal - sentencing A . J . Ayer ( 1959 ) . Logical positivism . Free Press . 50 S . Barocas and d . boyd ( 2017 ) . Engaging the ethics of data science in practice . Communications of the ACM , 60 ( 11 ) : 23 – 25 . S . Barocas , A . Rosenblat , d . boyd , S . P . Gangadharan , and C . Yu ( 2014 ) . Data & Civil Rights : Technology Primer . Data & Civil Rights Conference . M . Beaudouin - Lafon ( 2004 ) . Designing Interaction , not Interfaces . In Proc . Working conference on Advanced Visual Interfaces ( AVI ) , page 15 , ACM Press . P . L . Berger and T . Luckmann ( 1967 ) . The social construction of reality : a treatise in the sociology of knowledge . Doubleday . G . Bowker ( 2005 ) . Memory Practices in the Sciences . Cambridge , Mass : MIT Press . d . boyd and K . Crawford ( 2011 ) . Six provocations for big data . Keynote Address : A Decade in Internet Time . Symposium on the Dynamics of the Internet and Society . d . boyd and K . Crawford ( 2012 ) . Critical questions for big data . Information , Communication & Society , 15 ( 5 ) : 662 – 679 . d . boyd ( 2021 ) . In the pursuit of knowledge , there be dragons . IEEE VIS Keynote . d . boyd ( 2021 ) . Statistical imaginaries : An ode to responsible data science . Microsoft Research Summit Talk . S . Brayne ( 2017 ) . Big data surveillance : The case of policing . American Sociological Review , 82 ( 5 ) : 977 – 1008 . C . Cadwalladr and E . Graham - Harrison ( 2018 ) . Revealed : 50 million facebook profiles harvested for cambridge analytica in major data breach . The Guardian . https : / / www . theguardian . com / news / 2018 / mar / 17 / cambridge - analytica - facebook - influence - us - election S . K . Card , J . D . Mackinlay , and B . Shneiderman , editors ( 1999 ) . Readings in Information Visualization : Using Vision to Think . Morgan Kaufmann . S . K . Card , A . Newell , and T . P . Moran ( 1983 ) . The psychology of human - computer interaction . L . Erlbaum Associates . R . Carnap ( 1967 [ 1928 ] ) . The Logical Structure of the World . Rolf A . George ( trans . ) . Berkeley , CA : University of California Press . 51 R . Carnap ( 1958 ) . Meaning and necessity : a study in semantics and modal logic . University of Chicago Press . N . Chomsky ( 1956 ) . Three models for the description of language . IRE Transactions on Information Theory , 2 ( 3 ) : 113 – 124 . B . Clifton , S . Lavigne , and F . Tseng ( 2017 ) . Predicting Financial Crime : Augmenting the Predictive Policing Arsenal . https : / / whitecollar . thenewinquiry . com / static / whitepaper . pdf arXiv 1704 . 07826 . H . Collins ( 2012 ) . Tacit and Explicit Knowledge . Chicago : University of Chicago Press . P . H . Collins ( 1990 ) . Black Feminist Thought : Knowledge , Consciousness , and the Politics of Empowerment . Boston : Unwin Hyman . M . Correll ( 2019a ) . Counting , collaborating , and coexisting : Visualization and the digital humanities . IEEE VIS Workshop on Visualization for the Digital Humanities ( Vis4DH ) Capstone . M . Correll ( 2019b ) . Ethical dimensions of visualization research . In Proc . ACM Conf . Human Factors in Computing Systems ( CHI ) , pages 1 – 13 . J . Derrida ( 1976 [ 1967 ] ) . Of grammatology . Johns Hopkins University Press . C . D’Ignazio and L . F . Klein ( 2016 ) . Feminist data visualization . In IEEE VIS Workshop on Visualization for the Digital Humanities ( Vis4DH ) . C . D’Ignazio and L . F . Klein ( 2020 ) . Data Feminism . MIT Press . W . Dilthey ( 1990 [ 1900 ] ) . Gesammelte Schriften , Volume 1 . Albany : State University of New York Press . M . Do ̈ rk , P . Feng , C . Collins , and S . Carpendale ( 2013 ) . Critical InfoVis : exploring the politics of visualization . In Proc . ACM CHI Extended Abstracts on Human Factors in Computing Systems , page 2189 – 2198 . J . Drucker ( 2011 ) . Humanities approaches to graphical display . Digital Humanities Quarterly ( DHQ ) , 5 ( 1 ) . J . Drucker ( 2014 ) . Graphesis : Visual Forms of Knowledge Production . Harvard University Press . J . Drucker ( 2019 ) . Visualizing temporality and chronologies for the humanities . IEEE VIS Capstone . 52 D . Engelbart ( 1962 ) . Augmenting the Human Intellect : A Conceptual Framework . Washington , DC : Office of Scientific Research . V . Eubanks ( 2018 ) . Automating Inequality : How High - Tech Tools Profile , Police , and Punish the Poor . St . Martin’s Press . J . A . Fodor ( 1968 ) . Psychological explanation : an introduction to the philosophy of psychology . Random House . M . Foucault ( 1972 [ 1969 ] ) . The archaeology of knowledge . Tavistock Publications . M . Foucault ( 1979 [ 1975 ] ) . Discipline and punish : the birth of the prison . Penguin . M . Foucault ( 1994 [ 1963 ] ) . The birth of the clinic : an archaeology of medical perception . Vintage Books . S . L . Franconeri , L . M . Padilla , P . Shah , J . M . Zacks , and J . Hullman ( 2021 ) . The science of visual data communication : What works . Psychological Science in the Public Interest , 22 ( 3 ) , 110 – 161 . H . G . Gadamer ( 1977 ) . Philosophical hermeneutics . University of California Press . H . Garcia - Molina , J . D . Ullman , and J . Widom ( 2002 ) . Database Systems : The Complete Book . Prentice Hall . E . L . Gettier ( 1963 ) . Is Justified True Belief Knowledge ? . Analysis , 23 ( 6 ) : 121 – 123 . J . J . Gibson ( 1966 ) . The senses considered as perceptual systems . Houghton Mifflin . D . Gilbert , T . C . Frankel , and J . Menn ( 2023 ) . Silicon Valley Bank’s risk model flashed red . So its executives changed it . Washington Post . https : / / www . washingtonpost . com / business / 2023 / 04 / 02 / svb - collapse - risk - model / L . Gitelman , ed . ( 2013 ) . " Raw data " is an oxymoron . Infrastructures series . MIT Press . E . Goffman ( 1986 ) . Frame analysis : an essay on the organization of experience . Northeastern University Press . A . Goodnough , M . Davey , and Mitch Smith ( 2016 ) . When the Water Turned Brown . New York Times . https : / / www . nytimes . com / 2016 / 01 / 24 / us / when - the - water - turned - brown . html 53 G . Grolemund and H . Wickham ( 2014 ) . A cognitive interpretation of data analysis . International Statistical Review , 82 ( 2 ) : 184 – 204 . D . Haraway ( 1988 ) . Situated knowledges : The science question in feminism and the privilege of partial perspective . Feminist Studies , 14 ( 3 ) : 575 – 599 . M . Heidegger ( 1996 ) . Being and time : a translation of Sein und Zeit . Translated by J . Stambaugh . Albany : State University of New York Press . C . G . Hempel ( 1952 ) . Fundamentals of concept formation in empirical science . Chicago : University of Chicago Press . J . Hullman and A . Gelman ( 2021 ) . Designing for Interactive Exploratory Data Analysis Requires Theories of Graphical Inference . Harvard Data Science Review , 3 ( 3 ) . E . L . Hutchins , J . D . Hollan , and D . A . Norman ( 1985 ) . Direct manipulation interfaces . Human - Computer Interaction , 1 ( 4 ) : 311 – 338 . M . L . Jones ( 2013 ) . Data Hubris . In Schutt and O’Neil , eds . Doing Data Science . Sebastopol : O’Reilly Media . M . L . Jones ( 2021 ) . How We Became Instrumentalists ( Again ) : Data Positivism since World War II . Historical Studies in the Natural Sciences 48 ( 5 ) : 673 - 684 . S . Kandel , J . Heer , C . Plaisant , and J . Kennedy ( 2011 ) . Research directions in data wrangling : Visualizations and transformations for usable and credible data . Information visualization , 10 ( 4 ) : 271 . I . Kant ( 1998 [ 1781 ] ) . Critique of Pure Reason . Edited by Paul Guyer and Allan W . Wood . Cambridge University Press . A . Karakatsanis ( 2019 ) . The punishment bureaucracy : How to think about criminal justice reform . Yale Law Journal , 128 . S . Kasica , C . Berret , and T . Munzner ( 2021 ) . TableScraps : An actionable framework for multi - table data wrangling from an artifact study of computational journalism . IEEE Trans . Visualization and Computer Graphics , 27 ( 2 ) : 957 – 966 . A . Kay ( 1972 ) . A personal computer for children of all ages [ Dynabook ] . In Proc . ACM National Conference . 54 S . Lavigne , B . Clifton , and F . Tseng ( 2017 ) . “Predicting Financial Crime : Augmenting the Predictive Policing Arsenal . ” ArXiv abs / 1704 . 07826 . J . Lighthill ( 1973 ) . Artificial Intelligence : A General Survey . Artificial Intelligence : a paper symposium . Science Research Council . H . Lin , D . Akbaba , M . Meyer and A . Lex ( 2023 ) . Data Hunches : Incorporating Personal Knowledge into Visualizations . IEEE Trans . Visualization and Computer Graphics , 29 ( 1 ) : 504 - 514 . D . MacKenzie & J . Wajcman , eds . The Social Shaping of Technology . Milton Keynes : Open University Press . W . S . McCulloch ( 1965 ) . Embodiments of mind . MIT Press . M . Meyer and J . Dykes ( 2019 ) . Criteria for rigor in visualization design study . IEEE Transactions on Visualization and Computer Graphics , 26 ( 1 ) : 87 - 97 . M . Meyer , M . Sedlmair , P . S . Quinan , and T . Munzner ( 2015 ) . The nested blocks and guidelines model . Information Visualization , 14 ( 3 ) : 234 – 249 . M . Minsky ( 1974 ) . A Framework for Representing Knowledge . MIT AI Lab Memo No . 306 . T . Munzner ( 2009 ) . A nested model for visualization design and validation . IEEE Transactions on Visualization and Computer Graphics , 15 ( 6 ) : 921 – 928 . T . Nagel ( 1989 ) . The View From Nowhere . Oxford University Press . A . Newell , J . C . Shaw , and H . A . Simon ( 1959 ) . Report on a general problem - solving program . In Proc . International Conference on Information Processing ( IFIP ) , pages 256 – 264 . S . Noble ( 2018 ) . Algorithms of Oppression . NYU Press . D . Norman ( 1988 ) . The Psychology of Everyday Things . Basic Books . C . O’Neil ( 2013 ) . On Being a Data Skeptic . Sebastopol : O’Reilly . C . O’Neil ( 2016 ) . Weapons of Math Destruction . Sebastopol : O’Reilly . I . Goldstein and S . Papert ( 1977 ) . Artificial Intelligence , Language , and the Study of Knowledge . Cognitive Science 1 ( 1 ) : 84 - 123 . 55 S . Papert ( 1980 ) . Mindstorms : Children , Computers , and Powerful ideas . New York : Basic Books . J . Piaget ( 1955 ) . The language and thought of the child . Meridian Books . T . J . Pinch and W . E . Bijker ( 1984 ) . The social construction of facts and artefacts : Or how the sociology of science and the sociology of technology might benefit each other . Social studies of science , 14 ( 3 ) : 399 – 441 . P . Pirolli and S . Card ( 2005 ) . The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis . In Proc . International Conference on Intelligence Analysis . M . Polanyi ( 1958 [ 1998 ] ) . Personal Knowledge : Towards a Post Critical Philosophy . London : Routledge . H . Putnam ( 1975 ) . Mind , language , and reality . Cambridge University Press , 2nd edition . Quine , W . V . ( 1969 ) . " Epistemology Naturalized , " in Ontological Relativity and Other Essays . New York : Columbia University Press . G . Robertson , S . K . Card , and J . D . Mackinlay ( 1989 ) . The cognitive coprocessor architecture for interactive user interfaces . In Proc . ACM SIGGRAPH Symposium User Interface Software and Technology ( UIST ) , pages 10 – 18 . D . M . Russell , M . J . Stefik , P . Pirolli , and S . K . Card ( 1993 ) . The cost structure of sensemaking . In Proc Conf . Human Factors in Computing Systems ( CHI ) , pages 269 – 276 . D . Sacha , A . Stoffel , F . Stoffel , B . C . Kwon , G . Ellis , and D . A . Keim ( 2014 ) . Knowledge generation model for visual analytics . IEEE Transactions on Visualization and Computer Graphics , 20 ( 12 ) : 1604 – 1613 . C . E . Shannon ( 1950 ) . A chess - playing machine . Scientific American , 182 ( 2 ) : 48 – 51 . A . Shapiro ( 2017 ) . Reform predictive policing . Nature , 541 ( 7638 ) : 458 – 460 . B . Shneiderman ( 1982 ) . The future of interactive systems and the emergence of direct manipulation . Behaviour & Information Technology , 1 ( 3 ) : 237 – 256 . B . Shneiderman , C . Plaisant , M . Cohen , S . Jacobs , N . Elmqvist , and N . Diakopoulos ( 2016 ) . Designing the User Interface : Strategies for Effective Human - Computer Interaction . Pearson . 56 H . A . Simon ( 1969 ) . The sciences of the artificial . MIT Press L . A . Suchman ( 1987 ) . Plans and situated actions : The problem of human - machine communication . Cambridge University Press . L . Suchman ( 2007 ) . Human - Machine Reconfigurations : Plans and Situated Actions , 2nd ed . New York : Cambridge University Press . L . Suchman ( 1993 ) . Response to Vera and Simon’s Situated Action : A Symbolic Interpretation . Cognitive Science , 17 : 71 - 75 . J . W . Tukey ( 1962 ) . The future of data analysis . The Annals of Mathematical Statistics , 33 ( 1 ) : 1 – 67 . J . W . Tukey ( 1972 ) . Data analysis , computation , and mathematics . Quarterly of applied mathematics , 30 ( 1 ) : 51 – 65 . A . M . Turing ( 1950 ) . Computing Machinery and Intelligence . Mind , LIX ( 236 ) : 433 – 460 . N . Wiener ( 1948 ) . Cybernetics . J . Wiley . R . Williams ( 1975 ) . Keywords : A Vocabulary of Culture and Society . London : Fontana . L . Winner ( 1980 ) . Do Artifacts Have Politics ? Daedalus ( 109 : 1 ) .