Differences in the Use of Search Assistance for Tasks of Varying Complexity Robert Capra , Jaime Arguello , Anita Crescenzi , Emily Vardell School of Information & Library Science University of North Carolina at Chapel Hill Chapel Hill , NC , USA [ rcapra , jarguell , amcc , evardell ] @ email . unc . edu ABSTRACT In this paper , we study how users interact with a search assistance tool while completing tasks of varying complex - ity . We designed a novel tool referred to as the search guide ( SG ) that displays the search trails ( queries issued , results clicked , pages bookmarked ) from three previous users who completed the task . We report on a laboratory study with 48 participants that investigates diﬀerent factors that may inﬂuence user interaction with the SG and the eﬀects of the SG on diﬀerent outcome measures . Participants were asked to ﬁnd and bookmark pages for four tasks of varying complexity and the SG was made available to half the par - ticipants . We collected log data and conducted retrospec - tive stimulated recall interviews to learn about participants’ use of the SG . Our results suggest the following trends . First , interaction with the SG was greater for more complex tasks . Second , the a priori determinability of the task ( i . e . , whether the task was perceived to be well - deﬁned ) helped predict whether participants gained a bookmark from the SG . Third , participants who interacted with the SG , but did not gain a bookmark , felt less system support than those who gained a bookmark and those who did not interact . Fi - nally , a qualitative analysis of our interviews suggests diﬀer - ences in motivation and beneﬁts from SG use for diﬀerent levels of task complexity . Our ﬁndings extend prior research on search assistance tools and provide insights for the design of systems to help users with complex search tasks . Categories and Subject Descriptors H . 3 [ Information Storage and Retrieval ] : Information Storage and Retrieval Keywords Search assistance , search trails , search behavior Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , topostonserversortoredistributetolists , requirespriorspeciﬁcpermission and / or a fee . Request permissions from Permissions @ acm . org . SIGIR’15 , August 09 - 13 , 2015 , Santiago , Chile . Copyright is held by the owner / author ( s ) . Publication rights licensed to ACM . ACM 978 - 1 - 4503 - 3621 - 5 / 15 / 08 . . . $ 15 . 00 . DOI : http : / / dx . doi . org / 10 . 1145 / 2766462 . 2767741 . 1 . INTRODUCTION Current search engines are eﬀective in helping users com - plete simple search tasks such as homepage - ﬁnding and fact - ﬁnding . However , they provide less support in helping users with complex tasks that may involve exploration , analysis , comparison , and evaluation . Prior work has sought to ad - dress this limitation by exploring diﬀerent types of interac - tive tools to support and assist search engine users . These include tools to help users formulate better queries [ 9 , 15 , 18 ] , to communicate system features [ 17 ] , to assist with note - taking [ 8 ] , and tools that display the “search trails” followed by other users who completed a related task [ 20 , 23 , 26 ] . Our focus in this work is on search trails . The idea un - derlying search trails is simple and intuitive—search engine users may beneﬁt from seeing how someone else approached the same or a similar task . To support this , search trails provide an interactive display with information about how another person searched , and may include the queries issued , results clicked , pages viewed , pages bookmarked , and anno - tations made by the original searcher . Trails can be created manually , or algorithmically from search log or toolbar data . Prior research on search trails has focused on measuring the information content of search trails [ 23 ] , understand - ing the diﬀerences between trails generated by domain ex - perts versus novices [ 26 ] , developing algorithms for predict - ing search trails for a given search session [ 19 ] , and eval - uating the usefulness of trail - end point pages for diﬀerent types of search tasks [ 21 ] . While this prior work suggests the usefulness of search trails and their feasibility as a form of search assistance , there have been few controlled labora - tory studies to directly evaluate their beneﬁts and use . This is the main focus of our work . We investigate how users en - gage with an interactive search trail , when they use it ( i . e . , for which types of tasks ) , and what beneﬁts they report . We study user interaction with a search assistance tool we refer to as the search guide ( SG ) . Our search guide tool displays the search trails from three users who completed the same task . Each trail shows the sequence of queries that were issued , the results that were clicked , and the pages that were bookmarked . We report on a user study with 48 participants . Participants were given search tasks and asked to use a search engine to ﬁnd and bookmark pages that would help in constructing a response for the task . Access to the SG was a between subjects variable—24 participants had access to the SG and 24 participants used a control system without the SG . To gain insight into factors aﬀecting user interaction with the SG , the study used a concurrent think - aloud protocol followed by a stimulated recall interview . In this paper , we investigate ﬁve main research questions . In our ﬁrst research question ( RQ1 ) , we study the eﬀects of task complexity on user interaction with the SG . Par - ticipants completed four tasks of varying levels of cognitive complexity , which refers to the amount of learning and cog - nitive eﬀort required to complete the task [ 1 ] . Prior studies found that more cognitively complex tasks are perceived to be more diﬃcult and require more search eﬀort [ 2 , 3 , 12 , 24 ] . We suspected that these characteristics may lead people to seek search assistance more frequently for complex tasks . Our second research question ( RQ2 ) investigates combined factors of the user and the task that may inﬂuence user inter - action with the SG . For this question , we consider pre - task factors such as the participant’s level of interest in the task , prior knowledge and search experience in the task domain , and expectations about the task diﬃculty . In our third research question ( RQ3 ) , we study whether having access to the SG inﬂuences outcome measures that reﬂect the user’s experience during the search task . We con - sider post - task measures such as the level of enjoyment , ac - quired interest and knowledge , satisfaction with the solution and search strategy , experienced diﬃculty , and perceived level of system support . Seeking search assistance incurs a cost to the user in terms of time and eﬀort . In this respect , outcome measures such as satisfaction and perceived level of system support are likely to depend on whether the user was successful in seeking search assistance . In our fourth research question ( RQ4 ) , we investigate whether post - task outcome measures diﬀered among searches where partici - pants : ( 1 ) used the SG and gained from it , ( 2 ) used the SG but did not gain from it , and ( 3 ) did not use the SG at all . In our ﬁnal research question ( RQ5 ) , we examine the eﬀects of task complexity on why and how users interact ( or choose not to interact ) with the SG . This question diﬀers from RQ1 in that qualitative data from our stimulated re - call interviews was used to characterize SG use and non - use along three dimensions : ( 1 ) motivations for use , ( 2 ) beneﬁts gained from use , and ( 3 ) reasons for non - use . 2 . RELATED WORK This work is informed by three branches of prior research : ( 1 ) studies of task complexity and its eﬀects on users’ percep - tions , behaviors , and outcomes ; ( 2 ) studies of help - seeking in information retrieval ; and ( 3 ) studies of search assistance . Task Complexity . A large body of prior work has fo - cused on characterizing tasks along diﬀerent dimensions ( see Li and Belkin [ 16 ] ) . One such dimension is task complexity , which is an inherent property of the task , and is indepen - dent of the task doer [ 16 ] . Diﬀerent characterizations of task complexity have been proposed . Early work by Camp - bell [ 7 ] characterized task complexity in terms of the num - ber of required outcomes , the number of alternative paths to the outcomes , the level of uncertainty regarding the paths , and the degree of interdependence between paths . Bystr¨om and J¨arvelin [ 6 ] deﬁned task complexity based on the a pri - ori determinability of the task , a measure of the extent to which a searcher can read the task description and deduce the required outcomes , the information needed to produce the outcomes , and the processes associated with ﬁnding the required information . Bell and Ruthven [ 4 ] deﬁned task complexity in terms of the a priori determinability of the in - formation required to complete the task , the search strategy , and the ability to recognize relevant content . Finally , Jansen et al . [ 12 ] ( and later Arguello et al . [ 3 , 2 ] and Wu et al . [ 24 ] ) characterized task complexity in terms of the amount of learning and cognitive eﬀort required to complete the task . To this end , they adopted a taxonomy of learning outcomes proposed by Anderson and Krathwohl [ 1 ] for designing edu - cational materials . In this work , we use this cognitive view of task complexity . Prior studies have shown that more cogni - tively complex tasks are associated with higher levels of ex - pected ( pre - task ) diﬃculty [ 24 ] , higher levels of experienced ( post - task ) diﬃculty [ 2 , 24 ] , and higher levels of search ac - tivity as indicated by measures derived from queries , clicks , bookmarks , and task completion time [ 2 , 3 , 12 , 24 ] . We con - tribute to this body of literature by investigating whether and how task complexity aﬀects use of search assistance . Help - Seeking in Information Retrieval . Searchers encounter diﬃculty in diﬀerent ways and for diﬀerent rea - sons . In a large - scale user study , Xie and Cool [ 25 ] found that searchers encounter diﬃculty with seven general pro - cesses : ( 1 ) getting started , ( 2 ) identifying relevant resources , ( 3 ) navigating a resource , ( 4 ) constructing queries , ( 5 ) con - straining the search results , ( 6 ) recognizing useful informa - tion , and ( 7 ) monitoring the task process . They also identi - ﬁed factors that give rise to help - seeking situations includ - ing : the user’s domain knowledge and search experience , properties of the task ( e . g . , its complexity ) , and character - istics of the interface and the quality of the search results . Search assistance tools provide an opportunity to support users who encounter diﬃculty . However , there are challenges in creating successful assistance tools . Prior work points to several reasons for why users do not use help systems , including the cost of cognitively disengaging from the main task , the fear of unproductive help - seeking , and the refusal to admit defeat [ 10 ] . Prior studies also found that users may not notice the help when they are cognitively engaged in the main task [ 10 , 13 ] or may prefer to attempt the task on their own before seeking assistance [ 13 ] . Search Assistance Tools . Search assistance tools are aimed to help users with diﬀerent aspects of the search pro - cess . In this review , we focus on prior work on search trails . White et al . [ 21 ] experimented with a search assistant tool called popular destinations . Given a query , the search sys - tem presented a set of trail - endpoint webpages for trails orig - inating from similar queries . Results from a user study found that popular destinations were better suited for exploratory tasks , while query suggestions were better suited for known - item tasks . For exploratory tasks , popular destinations were associated with improved perceptions about the search ex - perience , the quality of the information found , and the level of system support . In a follow - up study , White and Chan - drasekar [ 22 ] proposed a modiﬁcation to the popular des - tinations tool to help users with diﬃcult known - item tasks . The proposed tool surfaces ‘labels’ associated with the prob - able target page . Labels were generated from anchor text , queries with clicks on the target , and social bookmarks . A query - log analysis suggests that surfacing labels might help users ﬁnd target webpages faster . White and Huang [ 23 ] analyzed search trails captured us - ing a browser toolbar and compared the usefulness of the ﬁrst trail page , the last page , and the full trail , which in - cludes visited pages in between . Using diﬀerent heuristics , full trails were associated with more coverage , diversity , nov - elty , and utility , suggesting that users have something to gain from seeing the full trail versus only the endpoints . Yuan and White [ 26 ] compared the quality of search trails produced by experts and novices in the medical domain . Study participants were explicitly asked to produce search trails to be used by others . Experts produced trails with more relevant pages , more objective information , and a more logical transition from general to speciﬁc information . An important step in using trails to support searchers is to predict which trail to display for a particular search ses - sion . Singla et al . [ 19 ] formulated the task as predicting the best trail in response to an input query - click pair . They de - veloped diﬀerent trail - ﬁnding algorithms and evaluated their performance retrospectively using toolbar data . Results sug - gest that diﬀerent algorithms perform well for diﬀerent met - rics based on coverage , diversity , utility , and relevance , and that based on a particular metric , the best - found trail often outperformed the one followed by the actual user . Finally , Fisher et al . [ 11 ] evaluated the usefulness of knowl - edge maps constructed by a single user or diﬀerent users over several iterations . Knowledge maps are diﬀerent from trails and consist of bookmarked pages that are organized and annotated to convey the schema of the solution . Interest - ingly , knowledge maps that were iterated upon were found to be more useful precisely because the schema of the so - lution was easier to understand and intrinsically valuable . This suggests that search trails may become more valuable if they can be extended and curated by users over time . 3 . METHODS AND MATERIALS 3 . 1 User Study A laboratory study with 48 participants was conducted to investigate our ﬁve main research questions ( RQ1 - RQ5 ) . Participants were undergraduate university students ( 75 % female ) . The study used a concurrent think - aloud proto - col with a retrospective stimulated recall interview . Each participant completed four search tasks of varying levels of cognitive complexity ( Section 3 . 3 ) . Participants were asked to use a live search system to ﬁnd and bookmark webpages that would be useful in constructing a response for the task . The system used the Bing Web Search API to retrieve re - sults from the open web and allowed participants to issue queries , click and view results , navigate away from a land - ing page , and bookmark pages . Participants were asked to provide a brief justiﬁcation when bookmarking each page . Access to the SG was a between - subjects variable—24 par - ticipants were given access to the SG and 24 participants were not . Task complexity was a within - subjects variable— participants completed four tasks associated with four dif - ferent levels of cognitive complexity ( remember , understand , analyze , and evaluate ) and all four tasks were from the same domain ( Section 3 . 3 ) . Cognitive complexity was rotated across participants using a Latin square . The study protocol proceeded as follows . First , partic - ipants were asked to complete a consent form and a de - mographic questionnaire . Next , participants were shown a video describing the bookmarking features of the system . For the 24 participants who were given access to the SG , the video contained an additional section describing its basic functionality . Participants were told that the SG conveyed information about how three other searchers completed a similar task . The video described how each of the three search trails or “paths” showed the queries that were issued and the results that were clicked and bookmarked . Our study protocol involved having participants think - aloud while they searched . Participants were instructed to narrate their searches by describing their thought processes and actions . To familiarize participants with the system and to help them become comfortable with thinking aloud , we asked them to spend a few minutes exploring the system and trying an example task before starting the main tasks . All four tasks followed the same procedure . First , par - ticipants were asked to read the task carefully and then complete a pre - task questionnaire ( Section 3 . 4 ) . Next , par - ticipants were directed to the search interface . During the search , participants were gently prompted to continue think - ing aloud if they fell silent . Participants were given 12 minutes to complete each task . A pop - up message noti - ﬁed participants when they had three minutes remaining . After completing the task , participants were directed to a post - task questionnaire ( Section 3 . 4 ) . All four search ses - sions and think - aloud comments were recorded using Morae screen recording software . After completing all four tasks , participants started the retrospective portion of the study . During the search sessions where participants had access to the SG , the study moderator used Morae Observer to view the participant’s search and mark the points where the participant moused or clicked in the SG . These points were later used in the stimulated recall interview ( Section 3 . 5 ) . 3 . 2 Search Interface and Search Guide Search Interface . The search interface used in the study is shown in Figure 1 . The interface in the experimental and control conditions looked identical except that the SG was only present in the experimental condition . The system al - lowed participants to issue queries , click results , bookmark pages , delete bookmarks , and ( in the experimental condi - tion ) to interact with the SG . The search task description ( A ) was always displayed directly above the query input box ( B ) . Results were returned using the Bing Web Search API , which produces 50 results per query . The top 10 results were displayed directly below the query input box ( C ) and pagi - nation controls were shown below the results . Participants used a Chrome web browser with four buttons integrated into the browser bookmark bar ( D ) . These buttons allowed participants to : ( 1 ) return to the search page , ( 2 ) book - mark the current page , ( 3 ) show the current set of book - marks , and ( 4 ) terminate the task . Clicking the “bookmark this page” button displayed a pop - up window ( not shown ) that prompted participants to : “Brieﬂy describe why you are bookmarking this page . ” Participants could bookmark any page including pages linked directly and indirectly from the SERP or the SG . Clicking the “show bookmarks” but - ton displayed a pop - up window ( not shown ) that listed the current set of bookmarks , with justiﬁcations included . From the bookmark view page , participants could delete a book - mark if desired . In the experimental condition , the search guide was displayed to the right of the search results ( E ) . We used Javascript and AJAX to log all user interactions on the SERP including scroll and mouse - enter events . Search Guide . As shown in Figure 1 ( E ) , the SG dis - played three“paths”taken by three diﬀerent users who com - pleted the same search task . Participants could explore the paths using tabs ( Path 1 - 3 ) . Each path included the list of queries that were issued by another user and , for each query , the sequence of search results that were clicked and bookmarked . Participants could use an accordion control AB C D E Figure 1 : Search Interface and Search Guide to expand a query to see the sequence of results that were clicked and bookmarked for that query . Clicked and book - marked pages were displayed using the page title , URL , and summary snippet , and bookmarked pages were distinguished using a thumbs - up icon displayed to the left of the result ti - tle . Participants could hover their mouse over a bookmarked page to trigger a tooltip that displayed the justiﬁcation pro - vided when the page was bookmarked . Clicking on an SG result took the participant to the landing page . Finally , clicking on the magnifying glass icon to the right of an SG query re - issued the query and displayed the results in the main SERP region . Again , we used Javascript and AJAX to record all user interactions with the search guide , including mouse - enter events , clicks on an SG result or query , clicks to expand the accordion control , and tooltip display events . Two decisions regarding the search guide had to be made— When to display the SG and which paths to display ? In regard to the ﬁrst question , the SG was displayed to par - ticipants after issuing the ﬁrst query and was present on all SERPs for the rest of the search session . In practice , a system might need to predict when to display the SG . We decided against displaying the SG dynamically in order to control how participants experienced the SG and to learn about SG use at all points in the search process , including early in the search session . In regard to the second ques - tion , as explained in more detail below , we decided to show paths for the same search task . In practice , a system might need to predict which paths to show . We were interested in exploring the best - case scenario where the system ﬁnds paths that match the user’s current search task . However , to avoid biasing participants to use the SG , they were told that the paths corresponded to searches for a similar task . Search Paths . We used a total of 12 search tasks in our study ( Section 3 . 3 ) . Each search task was associated with its own unique set of SG paths . For a given task , participants in the SG condition saw the same SG paths . Paths were selected from a previous user study that included the same search tasks [ 2 ] . For each task , we selected three paths that had at least three queries , at least one click per query , and a total of at least three bookmarks . 3 . 3 Search Tasks Participants completed four tasks of varying levels of cog - nitive complexity . Cognitive complexity refers to the amount of learning and cognitive eﬀort required to complete the task . We used a subset of 12 tasks from the original 20 tasks de - veloped by Wu et al . [ 24 ] . The tasks varied across three domains ( commerce , health , science ) and across four lev - els of cognitive complexity from Anderson and Krathwohl’s Taxonomy of Learning [ 1 ] : ( 1 ) remember : recalling relevant knowledge from long - term memory , ( 2 ) understand : con - structing meaning through summarizing and explaining , ( 3 ) analyze : breaking material into constituent parts and deter - mining how the parts relate to each other , and ( 4 ) evaluate : making judgements through checking and critiquing . The tasks were situated in scenarios geared towards our partici - pant population ( undergraduate students ) [ 5 ] . Table 1 shows the four tasks associated with the health domain . Higher - complexity tasks required more informa - tion and more mental processing : remember tasks required ﬁnding a fact ; understand tasks required compiling a list of items ; analyze tasks required compiling a list of items and understanding their diﬀerences ; and evaluate tasks required compiling a list of items , understanding their diﬀerences , and making a recommendation . Remember —You recently watched a documentary about peo - ple living with HIV in the United States . You thought the disease was nearly eradicated , and are now curious to know more about the prevalence of the disease . Speciﬁcally , how many people in the US are currently living with HIV ? Understand — Your nephew is considering trying out for a football team . Most of your relatives are supportive of the idea , but you think the sport is dangerous and are worried about the potential health risks . Speciﬁcally , what are some long - term health risks faced by football players ? Analyze —Having heard some of the recent reports on risks of natural tanning , it seems like a better idea to sport an artiﬁcial tan this summer . What are some of the diﬀerent types of artiﬁcial tanning methods ? What are the health risks associated with each method ? Evaluate —One of your siblings got a spur of the moment tat - too , and now regrets it . What are the current available methods for tattoo removal , and how eﬀective are they ? Which method do you think is best ? Why ? Table 1 : Example Search Tasks from Health Domain 3 . 4 Pre - and Post - Task Questionnaires The pre - task questionnaire asked about ﬁve measures : ( 1 ) level of interest , ( 2 ) prior knowledge , ( 3 ) prior search expe - rience , ( 4 ) a priori determinability , and ( 5 ) expected dif - ﬁculty . Questions were asked using ﬁve - point scales with labeled endpoints , except level of interest , which used a 7 - point scale , and prior search experience , which had 4 choices . We asked participants one question each about their level of interest in the task , prior knowledge about the task , and prior search experience in the task domain . We included three questions about a priori determinability . Participants were asked how deﬁned the task was in terms of the ( i ) expected solution , ( ii ) the information needed to solve the task , and ( iii ) the steps required to ﬁnd the necessary infor - mation . These three questions were combined into a single a priori determinability scale ( Cronbach’s α = . 777 ) . We included ﬁve questions about expected diﬃculty . Partici - pants were asked about their expected level of diﬃculty in ( i ) constructing queries for the task , ( ii ) understanding the search results , ( iii ) determining the usefulness of the results , ( iv ) deciding when to stop gathering information , as well as their ( v ) expected level of overall diﬃculty . These ﬁve ques - tions were combined into a single expected diﬃculty scale ( Cronbach’s α = . 848 ) . The post - task questionnaire asked about nine measures : ( 1 ) level of enjoyment , ( 2 ) engagement , ( 3 ) concentration , ( 4 ) acquired interest , ( 5 ) acquired knowledge , ( 6 ) experi - enced diﬃculty , ( 7 ) satisfaction , ( 8 ) time pressure , and ( 9 ) system support . All questions were asked using ﬁve - point scales with labeled endpoints . We asked participants one question each about their experienced level of enjoyment , engagement , concentration , and time pressure during the task . Similarly , we asked one question each about their level of acquired interest and knowledge . Consistent with the pre - task questionnaire , we included ﬁve questions about experienced diﬃculty that were combined into a single ex - perienced diﬃculty scale ( Cronbach’s α = . 853 ) . We asked two questions about satisfaction : ( i ) satisfaction with the information found and ( ii ) the satisfaction with the chosen search strategy . These two questions were combined into a single satisfaction scale ( Cronbach’s α = . 792 ) . Finally , we in - cluded three questions about system support . Participants were asked whether the system ( i ) helped them get started , ( ii ) helped them ﬁnd resources with useful information , and ( iii ) provided overall support in completing the task . Again , these three questions were combined into a single system support scale ( Cronbach’s α = . 808 ) . 3 . 5 Stimulated Recall Interview After completing the post - task questionnaire for the ﬁ - nal search task , a stimulated recall interview was conducted with the 24 participants in the SG condition . For each search task , the study moderator used the Morae markers to iden - tify the ﬁrst and last use of the SG ( if any ) . For each of these SG uses , the moderator played back a portion of the recording around the point of use and asked a series of structured questions . In order to stimulate the participant’s memory of the context , the playback included their think - aloud comments , and started about 10 seconds before the SG use started and continued until the SG use ended . After each playback , the moderator asked questions to elicit : ( 1 ) motivations for using the SG and ( 2 ) beneﬁts gained from using the SG . At the end of each task , we asked about ( 3 ) the times and reasons when the participant purposely avoided using the SG . Participants gave verbal free - form responses to all the questions , which were recorded for later analysis . Interview Analysis . We used qualitative techniques to analyze participants’ responses from the interviews . This analysis involved three rounds of qualitative coding . In the ﬁrst round , two of the researchers independently coded in - terviews from four participants ( 16 interviews ) using open coding and then resolved their codes to form an initial set of closed codes for each interview question . In a second round , two researchers used the closed codes on interviews from four additional participants and made reﬁnements to the coding scheme . Then , using the ﬁnal coding scheme , two researchers each coded half of all interviews and reviewed the codes for the other researcher’s half . Any points of dis - agreement were discussed and resolved by both researchers . 4 . RESULTS In this section we present results from our study . First , we present results of a manipulation check to see whether more cognitively complex tasks were found to be more diﬃcult and required more eﬀort ( Section 4 . 1 ) . Then we present results for each of our main research questions ( RQ1 - RQ5 ) in Sections 4 . 2 - 4 . 6 , respectively . 4 . 1 Task Complexity Check Participants completed four tasks of varying levels of cog - nitive complexity . As a manipulation check , we ﬁrst examine whether more complex tasks were found to be more diﬃ - cult by participants . We focus on four aspects of diﬃculty : ( 1 ) expected diﬃculty , ( 2 ) a priori determinability , ( 3 ) level of search activity , and ( 4 ) experienced diﬃculty . Expected diﬃculty and a priori determinability were measured using responses to the pre - task questionnaire ; level of search ac - tivity was measured using behavioral signals captured by the system ; and experienced diﬃculty was measured using responses to the post - task questionnaire . In terms of search activity , we derived behavioral signals from queries , clicks , bookmarks , mouse - overs , scrolls , and elapsed time . We used ANOVAs to measure the eﬀects of task com - plexity on all measures . Results are presented in Table 2 . 1 Overall , more complex tasks were found to be more diﬃ - cult in terms of the four aspects of diﬃculty considered . More complex tasks were associated with higher levels of expected and experienced diﬃculty , and lower levels of a priori determinability . That is , more complex tasks were perceived to be less well - deﬁned in terms of the expected solution , required information , and steps to follow . Finally , more complex tasks were associated with more search activ - ity : more queries , clicks , and bookmarks ; lower - ranked clicks and bookmarks ; more queries without a click or bookmark ; more mouse - enter and scroll events ; and required more time to complete . Post - hoc tests found that , in most cases , re - member tasks were signiﬁcantly diﬀerent from understand , analyze , and evaluate tasks . However , understand , analyze , and evaluate tasks were often indistinguishable . This dis - tinction will be important as we discuss the main results . 4 . 2 Effect of Task Complexity on SG Use Our ﬁrst research question ( RQ1 ) investigates whether task complexity aﬀects user interaction with the search guide . To explore this question , for each of the 96 task sessions where participants had access to the SG ( 24 participants x 4 tasks ) , we computed three binary measures that indicate diﬀerent levels of interaction with the SG : SGclicked : Represents if the participant clicked some - where in the SG during the task ( 1 ) , or not ( 0 ) . This con - sidered all clicks in the SG , including clicks on the accordion and tab controls to explore the SG queries and paths . This measure is an indication of whether the participant was re - ceptive to search assistance for the task . SGclickedRQ : Represents whether the participant clicked on at least one SG result or query during the task ( 1 ) , or not ( 0 ) . This measure indicates not only the desire for as - sistance , but whether the participant found something of interest in the SG . SGbookmarked : Represents whether the participant book - marked a page that was discovered by clicking on an SG result ( 1 ) , or not ( 0 ) . 2 This measure is an indication of whether the participant gained a direct beneﬁt from inter - acting with the SG . Figure 2 shows the number of participants that reached each level of SG interaction , organized by task complexity level ( max of 24 ) . 1 TotalScrollDistance was measured in units equal to the height of the SERP . 2 Includes bookmarks made directly on an SG result as well as bookmarks found by navigating links from an SG result . Table 2 : Eﬀects of task complexity on expected and experienced diﬃculty , a priori determinability , and search activity . Remember Understand Analyze Evaluate F ( 3 , 188 ) ; p - value post - hoc A Priori Det . 4 . 59 ( 0 . 55 ) 3 . 90 ( 0 . 86 ) 3 . 62 ( 0 . 82 ) 3 . 87 ( 0 . 79 ) 14 . 14 ; p = . 000 R < U , A , E Expected Diﬀ . 1 . 65 ( 0 . 64 ) 2 . 25 ( 0 . 66 ) 2 . 50 ( 0 . 83 ) 2 . 31 ( 0 . 73 ) 12 . 40 ; p = . 000 R < U , A , E NumQueries 1 . 92 ( 1 . 44 ) 3 . 42 ( 1 . 80 ) 4 . 81 ( 3 . 09 ) 5 . 15 ( 3 . 09 ) 17 . 10 ; p = . 000 R < U < A , E ; NumClicks 3 . 90 ( 2 . 32 ) 5 . 71 ( 2 . 91 ) 6 . 98 ( 3 . 66 ) 7 . 27 ( 3 . 07 ) 12 . 36 ; p = . 000 R < U , A , E ClicksPerQuery 2 . 43 ( 1 . 27 ) 2 . 08 ( 1 . 69 ) 1 . 79 ( 1 . 03 ) 1 . 78 ( 0 . 92 ) 2 . 79 ; p = . 042 R < E AvgClickRank 2 . 91 ( 1 . 36 ) 4 . 19 ( 2 . 21 ) 3 . 62 ( 1 . 98 ) 3 . 81 ( 2 . 18 ) 3 . 59 ; p = . 015 R < U AvgTimeToFirstClick 23 . 59 ( 9 . 10 ) 26 . 08 ( 13 . 29 ) 31 . 34 ( 20 . 94 ) 31 . 60 ( 19 . 45 ) 2 . 80 ; p = . 041 - NumAbandonedQueries 0 . 38 ( 0 . 82 ) 0 . 77 ( 1 . 02 ) 1 . 37 ( 1 . 93 ) 1 . 25 ( 1 . 45 ) 5 . 38 ; p = . 001 R < A , E PctAbandonedQueries 0 . 11 ( 0 . 19 ) 0 . 17 ( 0 . 19 ) 0 . 21 ( 0 . 22 ) 0 . 20 ( 0 . 18 ) 2 . 60 ; p = . 054 - NumBooks 2 . 02 ( 1 . 02 ) 3 . 46 ( 1 . 24 ) 4 . 29 ( 1 . 99 ) 4 . 67 ( 2 . 38 ) 21 . 66 ; p = . 000 R < U , A , E ; U < E NumBooksPerQuery 1 . 37 ( 0 . 67 ) 1 . 35 ( 0 . 98 ) 1 . 20 ( 0 . 88 ) 1 . 10 ( 0 . 58 ) 1 . 23 ; p = . 301 - AvgBookRank 2 . 73 ( 1 . 80 ) 4 . 03 ( 2 . 49 ) 3 . 60 ( 2 . 33 ) 3 . 35 ( 2 . 58 ) 2 . 62 ; p = . 052 R < U QueriesWOBooks 0 . 60 ( 1 . 18 ) 1 . 23 ( 1 . 36 ) 1 . 96 ( 2 . 26 ) 1 . 88 ( 1 . 86 ) 6 . 50 ; p = . 000 R < A , E PctQueriesWOBooks 0 . 17 ( 0 . 27 ) 0 . 28 ( 0 . 25 ) 0 . 30 ( 0 . 26 ) 0 . 31 ( 0 . 21 ) 3 . 08 ; p = . 029 R < E NumMouseovers 43 . 90 ( 43 . 25 ) 79 . 71 ( 65 . 72 ) 80 . 42 ( 64 . 46 ) 91 . 58 ( 66 . 13 ) 5 . 61 ; p = . 001 R < U , A , E TotalScrollDistance 1 . 77 ( 2 . 09 ) 4 . 81 ( 5 . 51 ) 4 . 26 ( 5 . 27 ) 5 . 01 ( 4 . 69 ) 5 . 07 ; p = . 002 R < U , A , E TimeToComplete 287 . 39 ( 368 . 46 ) 461 . 26 ( 207 . 37 ) 507 . 50 ( 194 . 85 ) 540 . 72 ( 169 . 04 ) 9 . 94 ; p = . 000 R < U , A , E Experienced Diﬀ . 1 . 73 ( 0 . 90 ) 2 . 22 ( 0 . 92 ) 2 . 24 ( 0 . 79 ) 2 . 23 ( 0 . 80 ) 4 . 14 ; p = . 007 R < A , E remember understand analyze evaluate clicked 10 19 14 14 clickedRQ 6 18 11 12 bookmarked 3 12 9 11 0 4 8 12 16 20 24 p a r ti c i p a n t s Figure 2 : Number of participants ( out of 24 ) who reached each level of SG interaction for diﬀerent task complexity levels . As Figure 2 shows , SG interaction was diﬀerent based on task complexity level . For SGclicked , there was a signiﬁcant eﬀect of task complexity ( Cochran’s Q test , Q ( 3 ) = 11 . 372 , p = . 01 ) . Post - hoc McNemar tests 3 showed that fewer par - ticipants interacted with the SG during the remember - level tasks as compared to the understand tasks . For SGclickedRQ , task complexity also had a signiﬁcant eﬀect ( Q ( 3 ) = 15 . 316 , p = . 002 ) . Post - hoc tests showed that more participants clicked on SG results and queries during the understand - level tasks as compared to the remember and analyze ones ( evaluate was marginally signiﬁcant ) . There was also a signiﬁcant eﬀect of task complexity on SGbookmarked ( Q ( 3 ) = 11 . 038 , p = . 012 ) . Post - hoc tests showed that fewer participants gained a bookmark from in - teracting with the SG during the remember tasks as com - pared to the understand and evaluate ones . These results show that task complexity had an eﬀect on SG use . As indicated by SGclicked , there was an interest in using the SG across all task complexity levels . The eﬀect of task complexity was stronger for measures of interaction that indicate more beneﬁt from the SG use ( SGclicked and SGbookmarked ) . Finally , the diﬀerences in SG interaction were the most pronounced between the remember and un - derstand tasks , with remember tasks having less SG inter - 3 Throughout our analysis , for non - parametric post - hoc tests we use the modiﬁed Bonferroni correction outlined by Kep - pel [ 14 ] . For ANOVAs , we use the Tukey correction . action . In Section 4 . 6 , we examine diﬀerences in the moti - vations and beneﬁts that participants described when using the SG during tasks of diﬀerent complexity levels . 4 . 3 Effect of Pre - task Factors on SG Use Our second research question ( RQ2 ) investigates whether the factors measured in our pre - task questionnaire ( interest , prior knowledge , search experience , a priori determinability , and expected diﬃculty ) inﬂuenced interaction with the SG . To investigate this question , we ran three logistic regressions to predict the binary measures of SG interaction deﬁned in Section 4 . 2 . Again , this analysis was conducted on the 96 task sessions where participants had access to the SG ( 24 participants x 4 tasks ) . Since task complexity was a known source of variance ( from RQ1 ) , we also included it as a factor in our model using three indicator variables to distinguish understand ( U ) , analyze ( A ) , and evaluate ( E ) tasks from remember tasks ( treated as the baseline ) . Figures 3 ( a ) - 3 ( c ) show the mean values of each pre - task factor for each SG interaction measure . For SGclicked , the regression model was not statistically signiﬁcant ( χ 2 ( 8 ) = 10 . 865 , p = . 209 ) , meaning that the pre - task factors did not signiﬁcantly predict whether or not a participant clicked on the SG during a task . For SGclickedRQ , the regression model was statistically signiﬁcant ( χ 2 ( 8 ) = 19 . 907 , p = . 011 ) . The model explained 25 . 0 % of the variance ( Nagelkerke R 2 ) and correctly clas - siﬁed 66 . 7 % of the cases . Only task complexity was a sig - niﬁcant predictor ( p = . 01 ) . A priori determinability was marginally signiﬁcant ( p = . 058 ) . For SGbookmarked , the regression model was statistically signiﬁcant , ( χ 2 ( 8 ) = 17 . 895 , p = . 022 ) . Table 3 shows the results . The model explained 23 . 3 % of the variance ( Nagelk - erke R 2 ) and correctly classiﬁed 70 . 8 % of the cases ( an in - crease of 11 % from the 63 . 5 % baseline of always predicting SGbookmarked to be zero ) . Using the Wald criteria , two variables were signiﬁcant : task complexity ( p = . 01 ) and a priori determinability ( p = . 035 ) . Based on the odds ratio ( Exp ( B ) ) , participants were 2 . 581 times more likely to gain a bookmark from the SG for every unit increase in the a priori determinability of the task . Overall , results for RQ2 show that none of the pre - task factors were signiﬁcant predictors of clicked - based interac - tion with the SG . However , the a priori determinability of the task ( along with task complexity ) was a signiﬁcant pre - dictor of whether a participant gained a bookmark from the interest search exp . prior know . a priori det . expect . diff . no click 4 . 30 1 . 33 2 . 05 4 . 01 2 . 27 click 4 . 45 1 . 28 1 . 94 4 . 09 2 . 23 0 . 00 1 . 00 2 . 00 3 . 00 4 . 00 5 . 00 6 . 00 ( a ) SGclicked interest search exp . prior know . a priori det . expect . diff . no clickRQ 4 . 14 1 . 29 1 . 92 3 . 97 2 . 30 clickRQ 4 . 64 1 . 32 2 . 06 4 . 13 2 . 20 0 . 00 1 . 00 2 . 00 3 . 00 4 . 00 5 . 00 6 . 00 ( b ) SGclickedRQ interest search exp . prior know . a priori det . expect . diff . no bookmark 4 . 26 1 . 31 1 . 98 3 . 99 2 . 27 bookmark 4 . 60 1 . 29 2 . 00 4 . 16 2 . 21 0 . 00 1 . 00 2 . 00 3 . 00 4 . 00 5 . 00 6 . 00 ( c ) SGbookmarked Figure 3 : Mean pre - task factor rating from participants who achieved each level of SG interaction . SG . In other words , participants were more likely to gain a bookmark from the SG when they perceived the task to be well - deﬁned in terms of the expected solution , required information , and associated steps . Table 3 : Logistic Regression for SGbookmark , χ 2 ( 8 ) = 17 . 895 , p = . 022 B S . E . df p - value Exp ( B ) interest 0 . 14 0 . 14 1 0 . 309 1 . 152 search exp . - 0 . 31 0 . 48 1 0 . 516 0 . 732 prior know . - 0 . 12 0 . 30 1 0 . 679 0 . 885 a priori det . 0 . 95 0 . 45 1 0 . 035 2 . 581 expect . diﬀ . 0 . 05 0 . 44 1 0 . 914 1 . 048 complexity 3 0 . 010 complexity ( U ) 2 . 65 0 . 83 1 0 . 001 14 . 166 complexity ( A ) 2 . 31 0 . 87 1 0 . 008 10 . 116 complexity ( E ) 2 . 63 0 . 86 1 0 . 002 13 . 869 constant - 6 . 52 2 . 76 1 0 . 018 0 . 001 4 . 4 Effect of SG Access on Post - task Factors Our third research question ( RQ3 ) investigates whether access to the search guide had an eﬀect on the factors mea - sured in our post - task questionnaire . To address this , we compare post - task measures of the 24 participants who had access to the SG to the 24 in the control condition . Figure 4 shows the means and 95 % conﬁdence intervals for each post - task measure . We conducted ANOVAs to see if ac - cess to the SG inﬂuenced the post - task scores on enjoyment , engagement , concentration , interest and knowledge increase , task diﬃculty , time pressure , satisfaction , and the perceived level of system support . Of these , none were signiﬁcant ex - cept for system support ( F ( 1 , 189 ) = 10 . 587 , p < . 001 ) . In - terestingly , participants who had access to the SG reported lower levels of system support ( M = 3 . 80 , SD = . 89 ) than participants who did not ( M = 4 . 27 , SD = . 78 ) . This result surprised us . Analysis of the eﬀects of SG use presented in the next section helps shed light on this result . enjoy . engage . conc . inc . interest inc . know . exp . diff . time press . satis - faction system support no SG 3 . 22 3 . 55 1 . 59 3 . 13 3 . 54 2 . 05 2 . 01 3 . 88 4 . 27 SG 3 . 25 3 . 60 1 . 79 3 . 17 3 . 60 2 . 16 1 . 91 3 . 89 3 . 80 0 . 00 1 . 00 2 . 00 3 . 00 4 . 00 5 . 00 Figure 4 : Mean post - task factor ratings the control ( no SG ) and experimental ( SG ) group . 4 . 5 Effect of SG Use on Post - task Factors Seeking assistance from the SG required time and cogni - tive eﬀort from users . For this investment , users may expect to beneﬁt . For RQ4 , we investigate whether participants’ post - task outcome ratings diﬀered among searches where they used the SG and gained a clear beneﬁt , those where they used the SG but did not gain , and those where they did not use the SG at all . To examine this , we categorized each of the 96 SG task sessions into one of three categories : ( 1 ) the participant did not click on the SG ( n = 38 ) , ( 2 ) the participant clicked on the SG , but did not gain a bookmark from using it ( n = 22 ) , and ( 3 ) the participant clicked on the SG and gained a bookmark ( n = 35 ) . Figure 5 shows the means for each post - task factor , group - ed by category . Results of ANOVAs found a signiﬁcant eﬀect of category on level of engagement ( F ( 2 , 92 ) = 3 . 93 , p < . 023 ) and level of experienced diﬃculty ( F ( 2 , 92 ) = 3 . 18 , p < . 046 ) , and a marginally signiﬁcant eﬀect on level of system support ( F ( 2 , 92 ) = 2 . 76 , p < . 07 ) . No other post - task factors were signiﬁcant . Post - hoc tests showed the following diﬀerences . With regard to engagement , when participants gained a book - mark from using the SG , they reported signicantly higher levels of engagement ( M = 3 . 97 , SD = . 82 , p = . 02 ) than when they did not click on the SG at all ( M = 3 . 29 , SD = 1 . 21 ) . For experienced diﬃculty , when participants clicked but did not gain , they reported higher levels of ex - perienced diﬃculty ( M = 2 . 54 , SD = 1 . 12 , p = . 036 ) than when they did not click ( M = 1 . 92 , SD = . 92 ) . In terms of system support , when participants gained a bookmark , they reported higher levels of system support ( M = 4 . 04 , SD = . 80 , p = . 058 ) than when they clicked but did not gain ( M = 3 . 48 , SD = . 94 ) . Interestingly , when participants clicked but did not gain , they reported less system support than when they did not click at all ( M = 3 . 76 , SD = . 91 ) , but this diﬀerence was not signiﬁcant ( p = . 487 ) . Together , the above results suggest that outcome mea - sures that relate to the user experience ( e . g . , engagement , experienced diﬃculty , and perceptions of system support ) may depend not only on the use of search assistance , but on whether the use is productive and results in a tangible beneﬁt ( e . g . , a bookmark ) . These results underscore the importance of providing relevant , high - quality trails . Finally , these results also provide insight into the surpris - ing result from Section 4 . 4 . Comparing the results for sys - tem support in Figures 4 and 5 , we see that even partici - pants who gained a bookmark from the SG reported lower levels of system support ( M = 4 . 04 , SD = . 80 ) than the participants in the control condition , who did not have ac - enjoy . engage . conc . inc . interest inc . know . exp . diff . time press . satis - faction system support no click 3 . 03 3 . 29 1 . 74 2 . 95 3 . 42 1 . 92 1 . 82 3 . 88 3 . 75 click / no - book 3 . 36 3 . 55 1 . 73 3 . 00 3 . 59 2 . 54 1 . 95 3 . 77 3 . 48 bookmark 3 . 43 3 . 97 1 . 89 3 . 51 3 . 80 2 . 19 1 . 97 3 . 97 4 . 04 0 . 00 1 . 00 2 . 00 3 . 00 4 . 00 5 . 00 Figure 5 : Mean post - task factor rating from partic - ipants in diﬀerent SG use groups : ( 1 ) did not click , ( 2 ) clicked , but did not gain a bookmark , ( 3 ) clicked and gained a bookmark . cess to the SG ( M = 4 . 27 , SD = . 78 ) . This suggests that participants in the experimental and control groups had dif - ferent expectations ( or used diﬀerent grounds for compari - son ) when responding to our questions about system sup - port . This highlights an important risk in providing search assistance—users’ expectations may also increase . 4 . 6 Differences in SG Use by Task Complexity Our ﬁnal research question ( RQ5 ) investigates how SG use and non - use varies for diﬀerent task complexity levels by evaluating participants’ responses during the retrospec - tive stimulated recall interviews . Data from the interviews was used to characterize SG use along three dimensions : ( 1 ) motivations for use , ( 2 ) beneﬁts from use , and ( 3 ) reasons for non - use . We report on participants’ free - form responses to our questions using the coding scheme developed . 4 . 6 . 1 Motivations for Use We identiﬁed three main categories of motivations for us - ing the SG : ( 1 ) to ﬁnd new information , ( 2 ) to conﬁrm previ - ously found information or to conﬁrm the search approach , and ( 3 ) to change the search approach . Figure 6 ( a ) shows the number of participants who described each motivation category at least once during the interview ( max of 24 for each bar ) . We elaborate on each category below . Find new information . Participants described wanting to ﬁnd new or better information than they had already found , wanting to get closer to an “answer” , and wanting to explore what others had found . As Figure 6 ( a ) shows , “ﬁnd new information” was cited by more participants for the higher complexity level tasks ( U = 11 , A = 10 , E = 12 ) than for the remember tasks ( R = 5 ) . Conﬁrm information already found . As opposed to ﬁnding new information , participants also used the SG to conﬁrm information they had already found . This cate - gory included motivations to conﬁrm a speciﬁc fact , to con - ﬁrm their search approach , and to conﬁrm the completeness of their ﬁndings . This type of motivation was described by more participants for the lower complexity tasks ( R = 9 , U = 10 ) than for the higher complexity tasks ( A = 4 , E = 4 ) . This illustrates a theme that we will see again later in this section—for the lower complexity tasks , the SG was used more to conﬁrm information , but for the higher complexity tasks it was used to ﬁnd new information . Change approach . Another motivation for using the SG was to help change a participant’s search approach . This included using the SG to help get started with the search , to get new ideas for query terms , and to look for divergent or contradictory information . Again this motivation was cited by more participants for the higher complexity levels ( U = 5 , A = 5 , E = 8 ) than for the remember tasks ( R = 1 ) . 4 . 6 . 2 Beneﬁts Gained from SG Use We identiﬁed four main categories of beneﬁts : ( 1 ) gained speciﬁc information , ( 2 ) gained a new search strategy , ( 3 ) reassurance , and ( 4 ) no gain . Figure 6 ( b ) shows the number of participants who described each gain category at least once during the interview . Gained speciﬁc information . Participants described a variety of ways that they gained speciﬁc information from using the search guide . Responses in this category included : ﬁnding relevant web pages in the SG results , directly ﬁnding an answer as part of an SG result snippet , ﬁnding informa - tion that contributed to their knowledge of the task domain , and identifying new dimensions of an answer that they had not considered . Following the same trend as the “ﬁnd new info”motivation , this beneﬁt was cited by more participants for the higher complexity tasks ( U = 13 , A = 9 , E = 9 ) than for remember tasks ( R = 2 ) . Interestingly , the highest number of participants cited this for the understand - level ( U = 13 ) tasks , suggesting that there may be characteristics of these tasks that make them well - suited to SG use . Gained a new search strategy . Participants also de - scribed gaining new search strategies through their use of the SG . This category included gaining new query terms to use and getting ideas for search strategies from the paths in the SG . This category followed a similar trend to the“gained speciﬁc information” beneﬁt : more participants cited it for the higher complexity tasks ( U = 7 , A = 10 , E = 7 ) than for the remember tasks ( R = 1 ) . Reassurance . Participants described gaining reassur - ance about a speciﬁc source of data , about their search approach , about a speciﬁc answer , and reassurance that they had found enough information and not missed any - thing . Reassurance followed a pattern somewhat opposite to “gained speciﬁc info” and “gained a new strategy” ; it was mentioned more for the lower - complexity tasks ( R = 8 , U = 10 ) than for the higher ones ( A = 6 , E = 3 ) . These re - sults are consistent with the overall trends for the motiva - tion categories—when participants used the SG for lower - complexity tasks , they mainly did so to conﬁrm information they had already found , and the beneﬁts they gained were reassurance that the information they found was good . No gain . In some instances where the participants inter - acted with the SG , they reported no gain or beneﬁt . These cases are important to consider because they represent situ - ations where the participant sought help , but the SG failed to provide it . Reports of “no gain” were most prevalent for the lowest ( R = 5 ) and highest levels of complexity ( E = 7 ) , suggesting that the SG was more helpful for tasks at the middle levels of complexity ( U = 4 , A = 2 ) . 4 . 6 . 3 Non - Use We identiﬁed three main reasons for non - use of the SG : ( 1 ) the task was straightforward , ( 2 ) the participant preferred to search on their own ( at least in the beginning ) , and ( 3 ) reasons related to the novelty and unfamiliarity of the SG . Figure 6 ( c ) shows the number of participants who described each non - use reason at least once during the interview . Straightforward . One of the most frequent reasons for not using the SG was that the task was straightforward and the participant did not think they needed help for the task . This reason was cited by more participants for the remember tasks ( R = 12 ) than for the higher complexity tasks ( U = 4 , A = 4 , E = 4 ) . Wanted to start the search on their own . In many cases , participants mentioned that they did not use the SG at ﬁrst , but intended to use it later to verify the quality or completeness of information they found . Participants said that they preferred to start searching on their own more frequently for the higher complexity tasks ( U = 6 , A = 8 , E = 7 ) than for the remember tasks ( R = 3 ) . Novelty / Unfamiliarity . Another reason participants reported for not using the SG was the novelty of the tool or the participant’s unfamiliarity with it . We did not notice any trend for this reason across levels of task complexity . 5 . DISCUSSION Our ﬁndings provide insights about when , why , and how searchers engaged with search assistance , and about the ef - fects of task complexity on use and beneﬁts of the search guide . Next , we discuss our main ﬁndings and implications . Task complexity inﬂuenced help - seeking ( RQ1 ) . We found a signiﬁcant eﬀect of task complexity on user interaction with the SG . Users were more likely to interact with the SG ( and to gain a bookmark ) for the more complex tasks ( understand , analyze , evaluate ) than for the least complex ( remember ) . Our manipulation check ( Section 4 . 1 ) found a similar grouping of task complexity levels with respect to diﬃculty and search eﬀort . Post - hoc tests showed that re - member tasks were consistently diﬀerent than understand , analyze , and / or evaluate tasks . Together , these ﬁndings sug - gest that more complex tasks were more diﬃcult and led users to seek and beneﬁt more from search assistance . Pre - viously , Xie and Cool [ 25 ] suggested that task complexity might inﬂuence help - seeking and our ﬁndings support this hypothesis . In addition , our results extend work by White et al . [ 21 ] who found diﬀerences in help - seeking behaviors for fact - ﬁnding versus exploratory tasks . Well - deﬁned tasks were more likely to lead to SG interac - tion and gains ( RQ2 ) . In addition to task complexity , the a priori determinability of the task also inﬂuenced SG interac - tion and gain . This result suggests that users are more likely to navigate and gain information from someone else’s search trail when they perceive the task to be well - deﬁned in terms of the expected solution and the steps required . One pos - sible explanation is that when the task is well - deﬁned , the searcher is better able to understand how another person’s search trail may be accessible and beneﬁcial . An interesting area for future work is to explore ways to improve the accessibility and utility of search trails , es - pecially for less well - deﬁned tasks . In the context of dis - tributed sensemaking applications , Fisher et al . [ 11 ] found that knowledge maps created by one person were not as easy to understand and as helpful as ones that had been it - eratively reﬁned by a sequence of users . Search trails may beneﬁt from a similar approach . Rather than showing the exact trail from a single individual , trails could be iteratively reﬁned and organized . Through this process , the accessibil - ity and usefulness of search trails for more open - ended tasks could be improved by moving the trail toward the most com - mon interpretations and approaches to the task . Interest , prior knowledge , and search experience did not inﬂuence SG use ( RQ2 ) . Our results did not ﬁnd pre - task factors such as level of interest , prior knowledge , and search experience to be signiﬁcant predictors of SG use . Prior work by Jansen and McNeese [ 13 ] considered whether users’ self - rated problem solving abilities inﬂuenced whether they sought search assistance , but also found no eﬀect . These results suggest that other properties of the system and task ( such as task complexity ) play a larger role in determining whether assistance is sought . Presenting search assistance can lower impressions of sup - port ( RQ3 ) . Participants with access to the SG reported lower levels of system support than participants in the con - trol group . A possible explanation for this is that by adding the search guide , participants’ expectations were raised but not fully met , resulting in lower support scores . Another factor may be that having the SG available throughout the task ( even at times it was not needed or desired ) may have created negative perceptions . This result suggests the im - portance of showing search assistance dynamically when it is needed . Future work should explore this diﬀerence and investigate methods to conﬁdently predict points during the search when assistance is likely to be beneﬁcial . Users’ experience is worse when assistance fails to deliver ( RQ4 ) . When our participants interacted with the SG , but did not gain a bookmark from it , they reported experiencing the lowest levels of system support and the highest levels of experienced diﬃculty . In contrast , when users gained a bookmark from interacting with the SG , they reported the highest levels of system support and engagement within the SG group . These results show that users’ perceptions of the search experience can depend on whether or not the use of search assistance was productive . These ﬁndings illustrate the importance of predicting the best search trail to display and ensuring that the trail quality is high . Work by Singla et al . [ 19 ] has reported on techniques to predict the best trails . Verify and conﬁrm for simple tasks ; provide new ideas for complex tasks ; allow users to start searches on their own ( RQ5 ) . Analysis from our retrospective interviews shows that for the least complex ( remember ) tasks , when partici - pants used the SG , it was mainly to conﬁrm and verify infor - mation . In contrast , for the more complex tasks they used it to ﬁnd new sources of information and new search strategies . In general , participants did not use the SG to get started , instead preferring to attempt the search on their own before seeking assistance . These results have implications for the design and implementation of search assistance tools . First , since users did not use the SG to get started , search trails do not need to be shown immediately . This represents an op - portunity for the system to accumulate evidence about the current task before predicting which search trail ( s ) to dis - play . Second , the trail selection criteria should consider task type . For simple tasks , the trails could be geared towards veriﬁcation and conﬁrmation , for example , by showing only the trail endpoints . For complex tasks , the trails could con - vey more information about the search process or the system could select trails with more divergent information . 6 . CONCLUSION We reported on a user study that investigated ﬁve research questions about user interaction with our search guide ( SG ) tool . Our ﬁndings show that users engaged with the SG and beneﬁted more for complex tasks compared to simpler ﬁnd new info conﬁrm info change approach R 5 9 1 U 11 10 5 A 10 4 5 E 12 4 8 0 3 6 9 12 15 18 21 24 p a r ti c i p a n t s ( a ) Motivation for SG use speciﬁc info new strategy reassurance no gain R 2 1 8 5 U 13 7 10 4 A 9 10 6 2 E 9 7 3 7 0 3 6 9 12 15 18 21 24 p a r ti c i p a n t s ( b ) Beneﬁts from SG use straight - forward start self novelty R 12 3 4 U 4 6 2 A 4 8 5 E 4 7 3 0 3 6 9 12 15 18 21 24 p a r ti c i p a n t s ( c ) Reasons for non - use Figure 6 : Diﬀerences in SG use for diﬀerent levels of task complexity . ones ( RQ1 ) . Tasks that were perceived as well - deﬁned were more likely to lead to beneﬁts from using the SG , but other pre - task factors such as level of interest and prior knowledge did not inﬂuence SG use or gain ( RQ2 ) . Having access to the SG was not found to impact outcome measures such as enjoyment , engagement , and satisfaction ( RQ3 ) . However , system support ratings were lower for participants who had access to the SG , suggesting that expectations diﬀered when the SG was shown . When participants interacted with the SG but did not gain a bookmark , they reported higher levels of diﬃculty and lower levels of system support compared to searches where they did gain or did not use the SG ( RQ4 ) . Analysis of our qualitative results shows that task com - plexity had an eﬀect on participants’ motivations for SG use , beneﬁts from SG use , and reasons for non - use . For the least complex tasks , participants mostly relied on the SG for conﬁrmation and reassurance , and when they did not use it , it was because the task was straightforward . For the more complex tasks , participants relied on the SG to ﬁnd new in - formation or search strategies , and when they did not use it , it was because they preferred to start on their own . Our ﬁndings point to several directions for future work . Behavioral measures that vary with task complexity may be useful features for predicting when to oﬀer search assis - tance . Another challenge is how to make search trails more accessible for tasks that are not well - deﬁned and for which users are likely to diverge widely in their approaches . Fi - nally , depending on the task complexity , users are likely to have diﬀerent motivations for interacting with search trails . Future work might consider customizing the trail display or the trail - ﬁnding algorithm to ﬁt diﬀerent goals ( e . g . , conﬁr - mation vs . ﬁnding new information ) . 7 . REFERENCES [ 1 ] L . W . Anderson and D . R . Krathwohl . A taxonomy for learning , teaching , and assessing : A revision of Bloom’s taxonomy of educational objectives . New York : Longman , 2001 . [ 2 ] J . Arguello . Predicting search task diﬃculty . In ECIR . Springer - Verlag , 2014 . [ 3 ] J . Arguello , W . - C . Wu , D . Kelly , and A . Edwards . Task complexity , vertical display and user interaction in aggregated search . In SIGIR , pages 435 – 444 . ACM , 2012 . [ 4 ] D . J . Bell and I . Ruthven . Searchers’ assessments of task complexity for web searching . In ECIR , pages 57 – 71 . Springer - Verlag , 2004 . [ 5 ] P . Borlund . Experimental components for the evaluation of interactive information retrieval systems . Journal of Documentation , 56 ( 1 ) : 71 – 90 , 2000 . [ 6 ] K . Bystr¨om and K . J¨arvelin . Task complexity aﬀects information seeking and use . Inf . Process . Manage . , 31 ( 2 ) : 191 – 213 , 1995 . [ 7 ] D . J . Campbell . Task complexity : A review and analysis . The Academy of Management Review , 13 ( 1 ) : 40 – 52 , 1988 . [ 8 ] D . Donato , F . Bonchi , T . Chi , and Y . Maarek . Do you want to take notes ? : Identifying research missions in yahoo ! search pad . In WWW , pages 321 – 330 . ACM , 2010 . [ 9 ] H . Duan , Y . Li , C . Zhai , and D . Roth . A discriminative model for query spelling correction with latent structural svm . In EMNLP - CoNLL , pages 1511 – 1521 . Association for Computational Linguistics , 2012 . [ 10 ] G . Dworman and S . Rosenbaum . Helping users to use help : Improving interaction with help systems . In CHI , pages 1717 – 1718 . ACM , 2004 . [ 11 ] K . Fisher , S . Counts , and A . Kittur . Distributed sensemaking : Improving sensemaking by leveraging the eﬀorts of previous users . In CHI , pages 247 – 256 . ACM , 2012 . [ 12 ] B . J . Jansen , D . Booth , and B . Smith . Using the taxonomy of cognitive learning to model online searching . Inf . Process . Manage . , 45 ( 6 ) : 643 – 663 , Nov . 2009 . [ 13 ] B . J . Jansen and M . D . Mcneese . Evaluating the eﬀectiveness of and patterns of interactions with automated searching assistance . JASIST , 56 : 1480 – 1503 , 2005 . [ 14 ] G . Keppel and T . D . Wickens . Design and Analysis : A Researcher’s Handbook . Prentice Hall , 3rd edition , 1991 . [ 15 ] Y . Kim and W . B . Croft . Diversifying query suggestions based on query documents . In SIGIR , pages 891 – 894 . ACM , 2014 . [ 16 ] Y . Li and N . J . Belkin . A faceted approach to conceptualizing tasks in information seeking . Information Processing and Management , 44 ( 6 ) : 1822 – 1837 , 2008 . [ 17 ] N . Moraveji , D . Russell , J . Bien , and D . Mease . Measuring improvement in user search performance resulting from optimal search tips . In SIGIR , pages 355 – 364 . ACM , 2011 . [ 18 ] M . Shokouhi . Learning to personalize query auto - completion . In SIGIR , pages 103 – 112 . ACM , 2013 . [ 19 ] A . Singla , R . White , and J . Huang . Studying trailﬁnding algorithms for enhanced web search . In SIGIR , pages 443 – 450 . ACM , 2010 . [ 20 ] A . Wexelblat and P . Maes . Footprints : History - rich tools for information foraging . In CHI , pages 270 – 277 . ACM , 1999 . [ 21 ] R . W . White , M . Bilenko , and S . Cucerzan . Studying the use of popular destinations to enhance web search interaction . In SIGIR , pages 159 – 166 . ACM , 2007 . [ 22 ] R . W . White and R . Chandrasekar . Exploring the use of labels to shortcut search trails . In SIGIR , pages 811 – 812 . ACM , 2010 . [ 23 ] R . W . White and J . Huang . Assessing the scenic route : Measuring the value of search trails in web logs . In SIGIR , pages 587 – 594 . ACM , 2010 . [ 24 ] W . - C . Wu , D . Kelly , A . Edwards , and J . Arguello . Grannies , tanning beds , tattoos and nascar : evaluation of search tasks with varying levels of cognitive complexity . In IIIX , pages 254 – 257 . ACM , 2012 . [ 25 ] I . Xie and C . Cool . Understanding help seeking within the context of searching digital libraries . JASIST , 60 ( 3 ) : 477 – 494 , 2009 . [ 26 ] X . Yuan and R . White . Building the trail best traveled : Eﬀects of domain knowledge on web search trailblazing . In CHI , pages 1795 – 1804 . ACM , 2012 .