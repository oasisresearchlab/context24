Code Duplication and Reuse in Jupyter Notebooks Andreas P . Koenzen Department of Computer Science University of Victoria Victoria , Canada akoenzen @ uvic . ca Neil A . Ernst Department of Computer Science University of Victoria Victoria , Canada nernst @ uvic . ca Margaret - Anne D . Storey Department of Computer Science University of Victoria Victoria , Canada mstorey @ uvic . ca Abstract —Duplicating one’s own code makes it faster to write software . This expediency is particularly valuable for users of computational notebooks . Duplication allows notebook users to quickly test hypotheses and iterate over data . In this paper , we explore how much , how and from where code duplication occurs in computational notebooks , and identify potential barriers to code reuse . Previous work in the area of computational notebooks describes developers’ motivations for reuse and duplication but does not show how much reuse occurs or which barriers they face when reusing code . To address this gap , we ﬁrst analyzed GitHub repositories for code duplicates contained in a repository’s Jupyter notebooks , and then conducted an observational user study of code reuse , where participants solved speciﬁc tasks using notebooks . Our ﬁndings reveal that repositories in our sample have a mean self - duplication rate of 7 . 6 % . However , in our user study , few participants duplicated their own code , preferring to reuse code from online sources . Index Terms —Jupyter , computational notebooks , code dupli - cation , code clones , code reuse , data analysis , data exploration , exploratory programming . I . I NTRODUCTION Computational notebooks have become the preferred tool for users exploring and analyzing data . Their power , versatility and ease of use have made this new medium of computation the de facto standard for data exploration [ 1 ] . During intensive data exploration sessions , users tend to generate great numbers of artifacts [ 2 ] . By reusing these artifacts—in the form of Jupyter code cells—users can expedite experimentation and test hypotheses faster [ 3 ] , [ 4 ] . Despite the fact that software engineering best practices include avoiding code duplication whenever possible [ 5 ] , [ 6 ] , it is common behaviour with Jupyter notebooks as it is especially easy to duplicate cells , make minor modiﬁcations , and then execute them [ 7 ] , [ 8 ] . This form of code reuse expedites data exploration but creates notebooks that are hard to read , maintain and debug . The recommended way to reuse code is to create modules , which are standalone code ﬁles ( e . g . , written in Python ) that can be imported locally into a notebook [ 8 ] . Unfortunately , it is reported that only about 10 % of notebooks contain such local imports ( those imported from the repository directory ) [ 9 ] . Hence , there is a great amount of code in notebooks for which there is no provenance , and understanding where code in notebooks originates and how it is reused is important if we want to create new tools for this environment . Previous work in the area of computational notebooks describes developers’ motivations for reuse and duplication but does not show how much reuse occurs or which barriers they face when reusing code . To address this gap , we ﬁrst analyzed GitHub repositories for code duplicates contained in Jupyter notebooks , and then conducted an observational user study where participants solved speciﬁc tasks using notebooks . In our ﬁrst study , we focused explicitly on code duplicates . Our deﬁnition of code duplicates is that of Roy and Cordy : “ snippets of code copied and pasted with or without modiﬁca - tions , intentionally reused in order save time and effort ” [ 5 ] , although there is still some debate as to what exactly a clone is [ 10 ] . Given the often transient nature of notebooks , combined with the fast - paced nature of data exploration , we hypothesized that code duplication happens often in Jupyter notebooks and that it might even be useful for reducing time between ideas and results while exploring data . We know from software engineering research that “ Cloning can be a good strategy if you have the right tools in place . Let programmers copy and adjust , and then let tools factor out the differences with appropriate mechanisms . ” [ 10 ] We argue that code duplication can be beneﬁcial for Jupyter notebooks with the support of the “right tools” . Code duplicates—also known as code clones—have been studied extensively in software engineering , and research shows that a signiﬁcant number of software systems contain code clones 1 [ 5 ] , [ 11 ] . No such study exists for computational notebooks . We differentiate between code duplication ( artifact ) and code reuse ( behaviour ) . We analyzed code duplication inside repositories and not across them . Hence , in this paper we use the term code duplicate to signal code that is contained and replicated in a single repository . Although notebooks support cells of multiple types ( including code and markdown text ) , we focused our study on code cells . Our study focused on three research questions : RQ1 : How much cell code duplication occurs in Jupyter notebooks ? 1 In this paper , we use the terms clone and duplicate interchangeably . 978 - 1 - 7281 - 6901 - 9 / 20 / $ 31 . 00 ©2020 IEEE a r X i v : 2005 . 13709v1 [ c s . S E ] 27 M a y 2020 RQ2 : How does cell code reuse happen in Jupyter note - books ? RQ3 : What are the preferred sources for code reuse in Jupyter notebooks ? We conducted two studies to answer these questions . In the ﬁrst study , we mined GitHub repositories containing Jupyter notebooks , looking for code duplicates and near - duplicates . We focused on a random sample of 1 , 000 GitHub repositories each containing at least one Jupyter notebook . For the second study , we designed an observational lab study ( n = 8 ) where we observed participants while they solved a particular set of tasks . Our results show that approximately 1 in 13 code cells in Jupyter notebooks are duplicates , and the nature of these dupli - cated snippets varies between 4 main categories : visualization ( 21 % ) , machine learning ( 15 % ) , the deﬁnition of functions ( 12 % ) and data science ( 9 % ) . Our second study shows that the preferred method of reuse is through web browsing , mostly through tutorial sites ( 35 % ) , API documentation ( 32 % ) and Stack Overﬂow ( 14 % ) . II . B ACKGROUND AND R ELATED W ORK Computational notebooks are a relatively new interactive computational paradigm that allows users to interleave code and text via a web interface . Programming code is introduced and segmented into code cells that are executed in a kernel ( Python , R , Julia , C + + , other ) with computation output / results returned to the web interface for display . This new way of computation makes sharing and coding easy for programming newcomers as users do not need to compile code or deal with low - level conﬁgurations . Several services currently offer com - putational notebooks : Google Colab [ 12 ] & Cloud AI Platform [ 13 ] , Azure Notebooks [ 14 ] , Databricks [ 15 ] , nteract [ 16 ] , Apache Zeppelin [ 17 ] , to name a few . These services provide even more abstraction by taking care of kernel conﬁgurations and just providing one for the user to select and use . In this section , we will discuss code reuse within this medium of computation . A . Code Duplication and Reuse Code cloning or duplication is considered a bad practice or bad smell in software engineering as described by Fowler [ 6 ] as it is believed to cause maintainability issues [ 5 ] , [ 18 ] . However , other studies that analyzed the impact and damage of code clones have provided evidence that the problem might be less severe than what was originally estimated [ 19 ] . It is always preferable , and in fact it is highly recommended as good practice , to create modules with functions that can be ac - cessed through interface implementations . However , resorting to duplicates can sometimes simplify the development effort , especially if the goal of the code is to be used as a playground or for testing , as is the case with Jupyter notebooks [ 7 ] . Previous studies in computational notebooks have analyzed how people use them , and reports show that when it comes to modularity , only about 10 % of Jupyter notebooks contain imports from local libraries [ 9 ] . This ﬂexibility in the design of Jupyter notebooks might be due to the fact that their users are not concerned with coding best practices [ 20 ] but with ease of use . Or maybe it may be due to the fact that users of Jupyter notebooks prioritize ﬁnding a solution over writing high quality code , as reported in a study by Kery et al . [ 21 ] . Although coding best practices are not paramount for users of Jupyter notebooks , there are projects that try to shift that attitude into one more oriented towards reusability and modularity . One of these projects is Papermill [ 22 ] , an nteract [ 16 ] library for passing parameters to Jupyter Notebooks . It lets users reuse a notebook by passing speciﬁc parameters at run - time , allowing one to try multiple approaches without needing to create extra cells . This form of reuse is particularly necessary for computational notebooks since they allow users to execute notebooks from the command line just like a reg - ular script , and to collect computation results using different mediums ( local ﬁles , S3 , and others ) . Another form of reuse that is widely used is the practice of adding snippets of code to notebooks with the click of a mouse . This form of reuse entails a local library of snippets , from which the user can read and write snippets . Google Colab [ 12 ] offers a function for users to specify a notebook where reusable snippets of code reside and from where users can reuse with a simple click . This form of quick duplication and reuse has been deﬁned by users of notebooks on Stack Over - ﬂow and other internet forums as a “ super needed feature ” and as a “ useful way to insert small , reusable code chunks into a notebook with a single click ” . Other forms of reuse have been studied before . Kery and Myers [ 3 ] reported that developers relied extensively on copy - ing versions of their ﬁles to support their data exploration sessions . Others have suggested new tools that expedite ex - ploration by enabling better access to previous artifacts and exploration history [ 21 ] , [ 23 ] . These tools have focused on internal in - notebook code duplication and reuse , using past cells and a notebook’s history as a source of reuse . Chattopadhyay et al . [ 24 ] surveyed Microsoft data scientists about notebook pain points . One of the reported pain points is the difﬁculty of exploring and analyzing code , which results in continual copy & paste cycles . Their participants also ranked activities based on importance , and Reuse Existing code was labeled as at least important 94 % of the time . It is also worth mentioning that reuse is not limited to any speciﬁc source . It can come from either web pages , other notebooks , or from version control system ( VCS ) repositories ( e . g . git , SVN and others ) . Other studies have investigated version control systems supporting analysts’ exploration of data , like studies conducted by Kery [ 25 ] , where participants reported not relying on VCS for their exploration sessions despite using them often for other tasks . As it is with VCSes , reusing code from other Jupyter notebooks presents some issues as well . Studies have reported difﬁculties choosing easily identiﬁable names for ﬁles and folders [ 21 ] , which generate confusion when trying to ﬁnd relevant snippets of code . Imagine a data analyst creating a different notebook for each analysis path they decide to take , e . g . , they may well end up with many different notebooks named hypothesis 1 . ipynb , hypothesis 2 . ipynb and so on [ 24 ] , [ 26 ] . This type of versioning presents many problems when it comes to ﬁnding useful snippets of code , including how to distinguish one exploration path from the other , and how to quickly know which one contains the snippet we are looking for . One way to solve these problems would be to provide for longer names that could describe more in depth what a notebook contains or is about , but that introduces new problems in itself , namely , longer and more convoluted names . Another solution would be to allow users to traverse previous notebooks more easily . In light of these issues with reuse and duplication , we set out to understand how , how much and from where reuse and duplication occurs within Jupyter projects and repositories . III . S TUDY 1 : C OUNTING J UPYTER C ODE C ELL D UPLICATES ON G IT H UB In order to better understand code duplication in compu - tational notebooks , we decided to mine GitHub repositories using the data set created by Rule in [ 26 ] . Rule’s study retrieved 1 . 25 million notebooks from GitHub , which they estimate as 95 % of the notebooks available in 2017 . We used a random sample of 1 , 000 repositories provided with this data set . Jupyter notebooks are just JSON text ﬁles segmented into cells and associated metadata . Cells can be markdown ( documentation and text ) , source code ( using the kernel ) , images ( e . g . , charts ) , or raw data . This study focuses on source code cells—the ones with snippets of programming code . In our ﬁrst study , we set out to answer RQ1 : How much cell code duplication occurs in Jupyter notebooks ? A . Code Duplicates According to Roy and Cordy [ 5 ] , code duplicates can be introduced in a software system by a ) copy and paste , b ) by forking , and c ) by design , functionality and logic reuse . They categorized duplicates into four types : Type - 1 : An exact copy of a code snippet except for white spaces and comments in the source code . Type - 2 : A syntactically identical copy where only user - deﬁned identiﬁers such as variable names are changed . Type - 3 : A modiﬁed copy of a Type - 2 clone where state - ments are added , removed or modiﬁed . Also , a Type - 3 clone can be viewed as a Type - 2 clone with gaps in - between . Type - 4 : Two or more code snippets that perform the same computation but are implemented through different syntactic variants . In this study , we focused on the ﬁrst three types of dupli - cates . Detecting Type - 4 duplicates is complex and we believed the ﬁrst three types are sufﬁcient to answer RQ1 . B . Method We started with a random sample of 1 , 000 GitHub reposi - tories containing approximately 6 , 000 notebooks from Rule et al . [ 27 ] 2 . We cloned each repository and looked at the 2 All artifacts used for this study are provided at https : / / doi . org / 10 . 5281 / zenodo . 3836691 latest commit available . We then extracted all code cells from each notebook in that repository . Once we extracted all code cells from a repository , we ran our own function ( Function 1 ) on every code cell , comparing that cell against all other cells in the repository , looking for Type - 1 , Type - 2 and Type - 3 duplicates . Based on the duplicate counts , we calculated a Repository Duplicates Ratio , which is the ratio of duplicated code cells to total number of code cells ( references to ‘cell’ should be taken as referring to ‘code cells’ for the remainder of the paper ) . DR ( C 1 , C 2 ) = LD ( C 1 , C 2 ) ( log avglen ( C 1 , C 2 ) ) λ 1 + ( log avgloc ( C 1 , C 2 ) ) λ 2 ( 1 ) Function 1 returns the duplicate ratio ( DR ) between two cells . It returns a real number in the interval [ 0 , + ∞ ) . LD ( C 1 , C 2 ) corresponds to the Levenshtein distance [ 28 ] between cells C 1 and C 2 . avglen ( C 1 , C 2 ) corresponds to the average number of characters in cell C 1 and C 2 , and avgloc ( C 1 , C 2 ) is the average number of lines of code in cells C 1 and C 2 . Parameters λ 1 and λ 2 are constants which act as weights . λ 1 weights the number of characters , and λ 2 weights cell lines of code . Setting these parameters allows us to de - emphasize short , quick print statements ( few lines of code ) or long blocks of text with few lines of code . We experimented with λ settings , heuristically determining the optimal setting to be λ 1 = 6 , λ 2 = 8 , such that lines of code carry more weight than the number of characters . Duplicates with a DR of 0 are identical ( Type - 1 duplicates ) , and the bigger the DR value is , the less similar the two blocks are . We only considered code to be duplicated if it had a DR of 0 . 3 or lower . We came up with that cut - off value by heuristics , experimenting with a smaller random sample , empirically assessing snippets detected as duplicates . We detected duplicates with different thresholds for the cut - off value , e . g . , 0 . 0 − 0 . 1 , 0 . 1 − 0 . 2 , . . . , 0 . 9 − 1 . 0 , and we were able to verify that at threshold 0 . 8 − 0 . 9 , the quality of duplicates began to decrease drastically . We opted for a text - based / string - based method of detecting clones because it has been used effectively in other studies [ 29 ] . We also required cross - language support because Jupyter notebooks support multiple programming languages and ker - nels . The Levenshtein distance is the minimum number of operations ( insertions , deletions or substitutions ) required for a string to be equal to another one . This method for detecting code duplicates proved to be effective for detecting Type - 1 , Type - 2 and Type - 3 duplicates ( see below ) , but with a highly inefﬁcient running time of O ( ( n ∗ m ) ! ) , where n and m are the lengths of C 1 and C 2 in characters . For this study , we implemented our own function ( Function 1 ) in order to have more control in the detection of snippets . We also removed comments and leading / trailing white space from lines of code . Finally , once we found duplicates , we randomly sampled 500 duplicates and thematically coded the duplicated code’s purpose in order to get a better understanding of the nature of duplicates . When coding the snippets , we tried to answer questions like what is its goal and what is it trying to compute . C . Results We searched for duplicates using Function 1 on 897 reposi - tories , consisting of 6 , 386 notebooks containing 78 , 020 code cells . 103 repositories were no longer available . Only 429 contained more than 28 code cells in total ( across all notebooks in that repository ) . Since 28 was the median , and the number of code cells is exponentially distributed , we discarded reposito - ries with fewer than 28 code cells to a ) reduce the running time and b ) ensure trivial repositories were not counted . From that analysis , we detected 5 , 872 Type - 1 , Type - 2 and Type - 3 code duplicates in total . Our mining results show that 74 % ( 4355 out of 5872 ) of the clones were Type - 2 and Type - 3 , and the rest were Type - 1 . The number of code duplicates in a repository varies mostly between 0 and 100 , with some outliers . We now discuss our ﬁndings for the distance between duplicates ( their duplicate type ) , the distribution of duplicate ratio ( DR ) , and duplicate purpose . 1 ) Duplicate Type : The Levenshtein distance ( LD ) between code cells follows an exponential distribution , with a median of 21 , mean of 41 . 08 , standard deviation of 59 . 66 , minimum value of 0 and maximum value of 535 . Most duplicates detected by our algorithm were Type - 1 and Type - 2 ( closer to zero ) , with a long fat - tail where some Type - 3 ( further away from zero ) duplicates were detected . 2 ) Repository Duplicates Ratio : Duplicate ratio measures the number of duplicate code cells over the total number of code cells in a repository . It also follows an exponential distribution , with a median ratio of duplicates per repository of about 5 . 0 % with a mean and standard deviation of µ = 7 . 6 % ( one in thirteen ) , σ = 8 . 3 % . The minimum ratio was 0 % , i . e . , a repository with no duplicates , and the maximum ratio was 47 . 5 % , i . e . , a repository where nearly half the code cells were duplicates . 3 ) Coding of Duplicates : Figure 1 shows the result of our inductive coding . Snippets of code that get duplicated the most within Jupyter notebooks are the ones whose main activity concerns visualization ( 21 . 35 % ) , followed by machine learning ( 15 . 45 % ) , deﬁnition of functions ( 12 . 85 % ) and data science ( 9 . 03 % ) . We use the term “main activity” because some categories overlap in their activities to some degree , e . g . , we merged mathematics and statistics together . Fig . 1 : Inductive coding of cell code snippets marked as duplicates by our algorithm . IV . S TUDY 2 : O BSERVING C ODE R EUSE B EHAVIOUR In order to better understand how users of computational notebooks reuse code , we conducted an observational study in our lab . We observed Jupyter users reusing code using Jupyter notebooks , git and web browsing . We used this observational study to answer RQ2 : How does cell code reuse happen in Jupyter notebooks ? and RQ3 : What are the preferred sources for code reuse in Jupyter notebooks ? A . Method We observed the behaviour of eight participants ( six M . Sc . and two Ph . D . ) . All were University students from Computer Science ( 6 ) and Chemistry ( 2 ) ; two were female ; three last used notebooks over one month ago , ﬁve within the past day . Four reported intermediate programming skill , two advanced , and two beginner . We explicitly expressed to our participants that we were not measuring programming abilities . We re - cruited participants with experience with Jupyter notebooks , irrespective of their level of proﬁciency with programming languages . We drew this convenience sample through personal contacts and email . Each participant was asked to solve three different tasks on our lab computers . These tasks were distributed as Jupyter notebooks according to the level of proﬁciency each partici - pant reported having ( Levels A , B , C , below ) . Each set of tasks were of varying difﬁculty . Each task was designed to take around 20 minutes to complete , but participants were given more time if needed . Full instructions for how to complete each task was given in full detail on each Jupyter notebook . In total , each participant received three Jupyter notebooks with instructions for each of the three tasks 3 . Two small data sets ( 10 elements ) were also provided within each notebook with instructions . The tasks given to participants were ( in the order that they were presented to the participant ) : Proﬁciency Level A 1 ) Calculate the mean of a data set . 2 ) Calculate the sum of all elements of a data set . 3 ) Calculate the mean of data set # 1 and calculate the sum of all elements of data set # 2 . Proﬁciency Level B 1 ) Calculate the standard deviation of a data set . 2 ) Create and plot a histogram with all elements of a data set . 3 ) Calculate the standard deviation of data set # 1 and create and plot a histogram with all elements of a data set . Proﬁciency Level C 1 ) Create a function that calculates the mean of a data set . 2 ) Create a function that calculates the standard deviation of a data set . 3 ) Calculate the mean of a data set using a function and write the function in the notebook . Calculate the standard 3 All artifacts generated for this study are provided at https : / / doi . org / 10 . 5281 / zenodo . 3836691 TASK # 1 P5 20 31 1 31 2 32 14 31 2 32 3 30 3 30 3 20 36 20 36 4 3 30 3 30 5 Actions 1 Create new notebook 11 Reorder cells 21 Browse Git repo 31 Switch to instructions tab 2 Copy dataset 12 Open ﬁle explorer 22 Git checkout or revert 32 Paste dataset 3 Type code 13 New cell 23 Download repo 33 Switch to another notebook 4 Browse online 14 Delete cell 24 Download notebook 34 Type markdown 5 Complete task 15 Copy ( Cut ) & paste cell 25 Duplicate notebook 35 Paste code 6 Save notebook 16 Type comments 26 Download from internet 36 Switch to working notebook 7 Rename notebook 17 Git push 27 Open notebook 37 Git di ﬀ 8 Git add 18 Edit previous task 28 Open command line 9 Git commit 19 Retype code ( Correction ) 29 Incomplete task 10 Copy code 20 Browse GitHub repo 30 Execute cell Reuse Actions Copy & Paste Code ( Own or Third - Party ) Duplicate Notebook Revert from Git Copy & Paste Cell Observe Examples : Only take into consideration examples where the user has typed code ( 3 & 19 ) right after browsing online ( 4 ) or right after a ( 33 & 36 ) actions . This latter example presents a problem , where the user might have seen the example and decided to do another action before reusing the code . Notes * = Task performed on another notebook . ^ = Potential problem performing the task . Internal Reuse . TASK # 2 P5 1 7 31 2 32 30 3 30 20 28 20 4 3 4 10 35 3 4 3 30 19 30 5 TASK # 3 P5 6 * 1 2 32 3 30 33 10 35 33 10 35 33 10 35 3 30 30 5 20 31 1 Fig . 2 : Example coding of steps one of our participants ( P5 ) made during the observational study , based on video and audio recordings . deviation of a data set using a function and write the function in the notebook . Tasks 1 and 2 were designed to be completely independent of each other , while task 3 was an intersection of the previous two ( e . g . , participants could have re - used the solutions to tasks 1 and 2 ) . Tasks that were based on data exploration remained fairly generic and did not rely much on external libraries . The restrictions imposed on the participants on how to accomplish these tasks were minimal . We told them each task should be completed in a notebook different from the one given with the instructions . This allowed us to observe if users created new notebooks or reused old ones . Supported languages were Python , R and JavaScript , which the partic - ipant could choose at any time , or change in the middle of the task if needed . Participants were told they could use any resource they found online . We also linked the instructions to a GitHub repository that contained the solution to each of the three tasks in its commit history . One researcher observed each participant during a time frame of roughly 60 minutes . The researcher took detailed notes of the behaviours each participant displayed . Audio and screen video was also recorded . The observational study was complemented with a questionnaire and a follow - up unstructured interview . Audio , video and notes were coded for qualitative and quantitative conclusions by one researcher . Video coding was important to quantify the steps each par - ticipant took to complete each task , to understand order and to sequence and ﬁnd patterns in participant behaviours . The video coding resulted in a detailed workﬂow analysis with tasks coded as shown in Figure 2 . Audio was transcribed and coded to derive the qualitative aspects of the answers each participant gave . B . Results Our participants duplicated code in a variety of ways . After analyzing the video artifacts , we created the following coding scheme to describe how they reused their code : C & P : Copying & pasting lines of code . CELL : Copying & pasting code of an entire cell ( reuse from notebooks ) . TYPE : Typing code written in another notebook of theirs , instead of C & P it . DUPE : Duplicating a notebook of their own . GIT : Reusing from git . TYPE ON : A special case where participants would browse online and then type the code they extracted from the source instead of C & P it . NONE : No reuse ; directly enter solution from memory . All participants reused code quite extensively , and only one participant typed from memory ( NONE ) . Most participants reused code from online sources . Foraging for code online is a popular form of code reuse among programmers and analysts , as was pointed out by Brandt et al . in [ 30 ] . We also observed that participants reused code from internal sources , like other notebooks . This was expected given they had easy access to previous tasks . None of the participants decided to reuse from git ( GIT ) , although full solutions to each task were readily available in the local git repository and on GitHub . 1 ) Code Reuse from Other Notebooks : Four out of eight participants decided to reuse what they did in task 1 and 2 for task 3 . The other four decided to perform task 3 from scratch , even though they had the necessary code for task 3 already implemented for task 1 and 2 ( recall that task 3 is deliberately structured as the union of 1 and 2 ) . When asked why , one participant ( P1 ) said : “ Muscle memory . As a means of preserving knowledge . ” P2 had trouble copying and pasting the code inside a cell . One explanation for this is that in Jupyter , when doing a right - click of the mouse , only copy or cut at the cell level is available . P4 and P6 both stated that they enjoyed typing . 2 ) Code Reuse from External Sources : All participants used the web extensively to assist themselves in the completion of each task . The time spent browsing online accounted on average for 18 % of the total time spent working on tasks ( Browsing time : µ = 233 ± 180 seconds ) . We also observed that they relied on online resources like API documenta - tion and tutorials for repetitive tasks , e . g . , some participants browsed online more than once for examples on how to import the same library , even though they performed the same import just minutes before ( Browsing count : µ = 9 . 6 ± 3 . 8 times ) . We noted that some participants used the web as a memory delegate [ 30 ] . Participant Time Percentage Count P1 176 sec 30 % 11 times P2 170 sec 21 % 3 times P3 155 sec 13 % 7 times P4 678 sec 31 % 14 times P5 95 sec 8 % 6 times P6 178 sec 9 % 13 times P7 317 sec 18 % 14 times P8 95 sec 9 % 9 times TABLE I : Portion of study spent browsing online per participant . Browsing and reusing common libraries in Python was also a popular resource among our participants , as all of them relied heavily on the numpy library for solving the tasks . We noticed this reliance on external sources and libraries saved time and effort for the participants , since calculating the mean of a data set took them at most two lines of code to accomplish using the numpy library ( including the import statement ) instead of using for loops . Figure 3 shows our coded responses for which sites were visited according to primary role . The two most visited sites among participants were tutorials ( e . g . , tutorialspoint ) and API documentation , followed by Stack Overﬂow . Participants queries were usually something short and precise , like “ mean numpy ” or “ histogram numpy ” , but in some cases we observed longer , more natural language oriented queries , like “ what is git repo ? and how to use it ? ” or “ how to create Jupyter project ” . We observed two distinct browsing habits : half the par - ticipants visited web sites nine times or fewer ( Table I ) , spending overall 129 seconds on average . For example , P2 spent 21 % of their time browsing online , but only did this three times . They took their time skimming the web page for code reuse . The other half had 11 or more visits , taking 337 seconds on average . This group used web resources like Fig . 3 : Inductive coding of sites participants visited while solving tasks . Note : Google implies information taken directly from Google’s results page . an external memory aid , going back and forth between the working notebook and the online resource multiple times . 3 ) Code Reuse from VCS : We observed that although some participants went to browse for the provided solutions on the study’s GitHub repository , they either did not restore them from the git history , or lost interest after a few tries . The so - lutions were not readily available at the HEAD of the commit tree , so knowledge on how to traverse the commit tree and on how to move the HEAD of the tree to a particular commit or how to checkout a particular commit was necessary in order to access the solutions . That is , above average knowledge of git was necessary for reusing code from the local repository . We received various answers from our participants when asked why they did not restore the provided solutions from git . Two out of eight participants stated “Sufﬁcient Knowledge” as their answer . It was implied by these participants that the tasks were not sufﬁciently difﬁcult to merit restoring them from git . And they perceived restoring from git as far more time consuming than actually coding the task . There was also a perception of complexity in restoring from git as noted by one of our participants ( P2 ) . When asked why they did not restore from git : “ . . . if I got really stuck , then just look things up , because probably it would have taken me more time to ﬁnd it in git , than actually do it myself . ” V . D ISCUSSION We used two research strategies to triangulate our under - standing of code duplication and reuse . In the ﬁrst study , we conducted a computational data analysis of existing GitHub notebooks . In the second study , we focused on how that duplication occurs with human observation in a lab setting . In both studies , we observed that duplicated code was important . We also identiﬁed some curious properties of version control , and ﬁnally , we revealed some patterns in external source reuse . We look at each of these observations in turn , and conclude with how we think they inﬂuence research and practice for future work in computational notebooks . A . Code Duplicates Figure 4 shows an example of a Type - 2 snippet of code detected by our algorithm . We can observe in the ﬁgure that both blocks of code are pretty much similar , with the exception of the variables passed as arguments to the plotting function . Before conducting our studies , we hypothesized that this type of duplicate—ones created to quickly perform some function on data—would be common in computational notebooks due to the scratch pad nature of the medium and the attitude of its users . Kery and Myers showed that users expect the notebook to be preliminary work and short - lived [ 7 ] . Our computational study ( Study 1 ) revealed that these duplicates had a median rate of 5 % per repository . This is an under - count , since it only looks at self - duplication , not duplication of code snippets from API usage guides or other resources . Study 2 showed that these were popular sources for reuse . Fig . 4 : Example of a Type - 2 duplicate detected by our algorithm with Levenshtein distance of 42 and Duplicate Ratio of 0 . 27 . Coded as Visualization . Practice implications : After conducting our study , we argue in favor of tools that support this type of reuse , whether they are offered through functions like Google Colab’s Code Snippets or similar ones designed for other types of notebooks . Research implications : We observed that while this type of duplication was present in our sample , our users struggled to easily make use of previous cells , even when those cells solved exactly the same problem . This suggests that merely providing a mechanism to duplicate code has to overcome barriers of ease of use . It seems to be simpler , for some cases , to copy and paste from online sources ( like Stack Overﬂow ) than to do the same thing from one’s own work ( reinventing the wheel attitude ) . B . Use of Version Control Version control for notebooks has been the focus of several notebook plugins , many blog posts and feature requests ( e . g . , jupyterlab - git [ 31 ] , Verdant [ 32 ] , nbdime [ 33 ] ) , and several research studies [ 21 ] , [ 24 ] , [ 34 ] . Some notebook services like NextJournal [ 35 ] value the importance of preserving history in computational notebooks so much , that they offer automatic versioning of the notebook and related artifacts . Our ﬁrst study did not examine the role of version control as a source for duplication , in part because identifying duplication from version control is tricky ( since ﬁles can be renamed , sections moved , etc . ) . However , part of our lab study was intended to explore how version control systems ( VCS ) , speciﬁcally git , were used in code reuse . We saw that users struggled with the interaction model of git and were unable to use it for duplication purposes . Practice implications : Based on our limited study , version control of notebooks is less important for future code reuse than for archival purposes or collaboration . For single - user notebooks , in particular , complex tools for version control and diff might be replaced with simpler save and restore functionality like in backup interfaces ( such as Apple’s Time Machine [ 36 ] model ) . Research implications : Evidence from other notebook studies [ 3 ] , [ 27 ] , [ 37 ] , and our observations in this paper show that code duplication and reuse using version control history is a challenge . However , git was designed for software development on the Linux operating system , and evidence suggests the code reuse scenario is low on the list of reasons developers use version control . As Codoban et al . reported , software developers instead use software history for debug - ging , program understanding , and collaboration [ 38 ] . This use case is different than searching for previous solutions and may explain why version control tools like git are a bad ﬁt for exploratory programming . C . How External Sources Were Used We found the use of external sources to be very common . In Figure 3 , we reported on the most frequent types of sources used . Participants frequently skimmed these sites and used them as external aids , and accessed them for short periods of time ( browsing statistics per participant can be found on Table I ) . We also looked at what types of duplicates were most common in Figure 1 . Given the results , it seems like a possible correlation exists between task familiarity and importance . Visualization snippets , for example , are frequently duplicated , because they are vital to the analysis process , but often unfa - miliar to the analyst , who may not have extensive visualization training and instead relies on support from libraries such as Altair or GGPlot . Duplication initially seems like a major time saver—since the chart , for instance , can be quickly reproduced—but even - tually adds to technical debt and maintainability issues [ 19 ] . At that point , the duplication can be refactored into a common module , e . g . , the corporate visualization module that deﬁnes fonts , themes , label sizes , etc . , or common functions and classes used across notebooks . Commercial data science teams at places like Netﬂix have begun to support this process with extensive scaffolding around the basic notebook metaphor , for example , with Netﬂix’s Metaﬂow [ 39 ] or AWS Step Functions [ 40 ] . Even further back , scientiﬁc workﬂow software [ 41 ] has been managing data processing models for many years . Practice implications : Study 1 shows that it is difﬁcult to understand when a code cell should be a module instead of duplicated code in a notebook . Software analytic techniques , applied in more conventional software programs , may help identify when duplicate code is becoming harmful [ 42 ] . Research implications : Social media and external ( web ) sources are widely recognized as a vital part of modern programming [ 43 ] . Programming support in notebooks should recognize this and support it , possibly by automating prove - nance and import , so that the original source can be referenced . In some samples , we saw this done manually with a comment referring to the Stack Overﬂow or API URL the solution was taken from . VI . L IMITATIONS Construct Validity . The main constructs we discuss are code duplicate and code reuse . We used the Levenshtein distance between code cells to detect duplicates , similar to Duala - Ekoko and Robillard in [ 29 ] , which is different than using an Abstract Syntax Tree ( AST ) , which is more common in other code cloning research . This was due to the nature of Jupyter notebooks , which can support multiple kernels and program - ming languages , so we required a cross - language solution . We also set thresholds for duplicates , which are determined empirically based on soundness of the duplicates . However , for a different sample , these thresholds would provide sub - optimal results . In the future , an improvement would be to use a systematic analysis or grid search to ﬁnd the parameters for duplicate detection and compare the detection results with an oracled data set [ 44 ] ; this is especially true for λ 1 and λ 2 , which control weights for the length and lines of code . However , we were not claiming general results for code duplication in all notebooks , and our ﬁndings should be seen as restricted to this sample . Also , given the emphasis we assigned on larger snippets and quantity of lines of code , there could be some under - reporting of duplicates , especially of shorter , more concise snippets of code , like short print statements , and other debugging techniques . As we observed users to detect code reuse , it is possible that our small tasks did not truly test all forms of reuse . For example , our protocol did not allow for reuse from other par - ticipants . Similarly , our users were constrained to use our lab equipment . Using their own devices may have shown different reuse techniques ( e . g . , local copies of documentation ) . Internal Validity . We used 897 randomly selected repositories , consisting of 6 , 386 notebooks and eight convenience sampled students . Our random sample of notebooks relied on the data set created by Rule in [ 27 ] . Because we sampled 897 repositories and only considered inter - repository reuse , we may have missed reuse derived from external sources , such as popular repositories , tutorials , and training material . This almost certainly led to an under - reporting of code reuse . Convenience sampling does not support statistical general - ization , but since this was an exploratory study , generalization of the observed phenomena was not one of our objectives . While it is possible the behaviours we report on were unique to this sample , the participants all engaged in data analysis for several years in undergraduate and graduate courses at the university . We asked for self - assessed proﬁciency with notebooks and version control . This has a potential observer - expectancy bias because we were known to the participants , who may have wanted to inﬂate their self - assessment . As an exploratory study , this is acceptable , but to test a speciﬁc hypothesis , our measures should be less prone to bias . The tasks were scaled for the estimated skill of our partic - ipants . For example , np . mean ( lst ) is sufﬁcient to solve task 1 part 1 , and for an experienced data scientist , could be retrieved from working memory . However , the tasks were designed for students , and if we were to use professionals , the tasks would have been more challenging . These tasks were designed to stimulate external / distributed cognition . We used three levels of complexity for our study , so that experienced analysts were given tasks commensurate with their skill . However , this is an imperfect matching process . External Validity . We sampled from notebooks on GitHub , which may differ from notebooks used in corporate settings . Similarly , the use of students as subjects makes it difﬁcult to draw generalizations about industry practitioners [ 45 ] . That being said , the students in our sample reported using note - books ( and the other tools ) frequently , and information on how professionals use notebooks is still limited . VII . C ONCLUSION We examined how code duplication and reuse happens in Jupyter notebooks . Our ﬁrst study looked at how much self - duplication ( i . e . , within the repository ) exists . We discovered that on average 7 . 6 % of code in repositories is self - duplicated . However , this did not explain how or from where code was duplicated to begin with . We conducted a lab study with eight participants and deliberately crafted tasks designed to encourage reuse be - haviour . We observed how participants reused code to solve data science tasks and how they leveraged version control , online sources and other notebooks . Reusing code from online sources proved to be the preferred method of reuse for our participants , with 18 % of their time spent browsing for code examples online , and version control systems proved to be the least effective method of reuse . Snippets of code that visualize data are the ones that are duplicated the most . We conclude this paper by discussing observations and implications from our studies . First , while code duplication is clearly common in notebooks , the source of that duplication is important . Second , although much attention focuses on version control , for code reuse , other sources , such as API examples , are more important . Finally , these external sources are used for various tasks . Notebook interfaces should support modularization and reuse to improve cognitive support for data scientists . VIII . A CKNOWLEDGEMENTS Many thanks to Cassandra Petrachenko and Soroush Youseﬁ for their help with paper editing and data analysis , respectively . We also acknowledge the support of the Natural Sciences and Engineering Research Council of Canada ( NSERC ) . R EFERENCES [ 1 ] J . M . Perkel , “Why jupyter is data scientists’ computational notebook of choice , ” Nature , vol . 563 , no . 7729 , pp . 145 – 146 , Oct 2018 . [ Online ] . Available : http : / / dx . doi . org / 10 . 1038 / d41586 - 018 - 07196 - 1 [ 2 ] S . Kandel , A . Paepcke , J . M . Hellerstein , and J . Heer , “Enterprise data analysis and visualization : An interview study , ” IEEE Transactions on Visualization and Computer Graphics , vol . 18 , no . 12 , pp . 2917 – 2926 , Dec 2012 . [ Online ] . Available : http : / / dx . doi . org / 10 . 1109 / TVCG . 2012 . 219 [ 3 ] M . B . Kery and B . A . Myers , “Exploring exploratory programming , ” in 2017 IEEE Symposium on Visual Languages and Human - Centric Computing ( VL / HCC ) , Oct 2017 , pp . 25 – 29 . [ 4 ] J . Brandt , P . J . Guo , J . Lewenstein , and S . R . Klemmer , “Opportunistic programming , ” Proceedings of the 4th international workshop on End - user software engineering - WEUSE ’08 , 2008 . [ Online ] . Available : http : / / dx . doi . org / 10 . 1145 / 1370847 . 1370848 [ 5 ] C . K . Roy and J . R . Cordy , “A survey on software clone detection research , ” School of Computing TR 2007 - 541 , Queen’s University , vol . 115 , 2007 . [ 6 ] M . Fowler , Refactoring : improving the design of existing code . Addison - Wesley Professional , 2018 . [ 7 ] M . B . Kery , M . Radensky , M . Arya , B . E . John , and B . A . Myers , “The story in the notebook : Exploratory data science using a literate programming tool , ” in Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , ser . CHI ’18 . New York , NY , USA : ACM , 2018 , pp . 174 : 1 – 174 : 11 . [ Online ] . Available : http : / / doi . acm . org / 10 . 1145 / 3173574 . 3173748 [ 8 ] A . Rule , A . Birmingham , C . Zuniga , I . Altintas , S . - C . Huang , R . Knight , N . Moshiri , M . H . Nguyen , S . B . Rosenthal , F . P´erez , and et al . , “Ten simple rules for writing and sharing computational analyses in jupyter notebooks , ” PLOS Computational Biology , vol . 15 , no . 7 , p . e1007007 , Jul 2019 . [ Online ] . Available : http : / / dx . doi . org / 10 . 1371 / journal . pcbi . 1007007 [ 9 ] J . F . Pimentel , L . Murta , V . Braganholo , and J . Freire , “A large - scale study about quality and reproducibility of jupyter notebooks , ” 2019 IEEE / ACM 16th International Conference on Mining Software Repositories ( MSR ) , May 2019 . [ Online ] . Available : http : / / dx . doi . org / 10 . 1109 / MSR . 2019 . 00077 [ 10 ] R . Koschke , “Survey of research on software clones , ” in Duplication , Redundancy , and Similarity in Software , ser . Dagstuhl Seminar Proceedings , R . Koschke , E . Merlo , and A . Walenstein , Eds . , no . 06301 . Dagstuhl , Germany : Internationales Begegnungs - und Forschungszentrum f¨ur Informatik ( IBFI ) , Schloss Dagstuhl , Germany , 2007 . [ Online ] . Available : http : / / drops . dagstuhl . de / opus / volltexte / 2007 / 962 [ 11 ] S . Uchida , A . Monden , N . Ohsugi , T . Kamiya , K . - i . Matsumoto , and H . Kudo , “Software analysis by code clones in open source software , ” Journal of Computer Information Systems , vol . XLV , pp . 1 – 11 , 04 2005 . [ 12 ] “Google colab . ” [ Online ] . Available : https : / / colab . research . google . com [ 13 ] “Google cloud ai platform . ” [ Online ] . Available : https : / / cloud . google . com [ 14 ] “Microsoft azure notebooks . ” [ Online ] . Available : https : / / notebooks . azure . com [ 15 ] “Databricks . ” [ Online ] . Available : https : / / databricks . com [ 16 ] “nteract . ” [ Online ] . Available : https : / / nteract . io [ 17 ] “Apache zeppelin . ” [ Online ] . Available : https : / / zeppelin . apache . org [ 18 ] E . Juergens , F . Deissenboeck , B . Hummel , and S . Wagner , “Do code clones matter ? ” 2009 IEEE 31st International Conference on Software Engineering , 2009 . [ Online ] . Available : http : / / dx . doi . org / 10 . 1109 / ICSE . 2009 . 5070547 [ 19 ] S . Thummalapenta , L . Cerulo , L . Aversano , and M . Di Penta , “An empirical study on the maintenance of source code clones , ” Empirical Software Engineering , vol . 15 , no . 1 , pp . 1 – 34 , Mar 2009 . [ Online ] . Available : http : / / dx . doi . org / 10 . 1007 / s10664 - 009 - 9108 - x [ 20 ] J . Wang , L . Li , and A . Zeller , “Better code , better sharing : on the need of analyzing jupyter notebooks , ” 2019 . [ 21 ] M . B . Kery , A . Horvath , and B . Myers , “Variolite : Supporting exploratory programming by data scientists , ” in Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems , ser . CHI ’17 . New York , NY , USA : ACM , 2017 , pp . 1265 – 1276 . [ Online ] . Available : http : / / doi . acm . org / 10 . 1145 / 3025453 . 3025626 [ 22 ] “Papermill . ” [ Online ] . Available : https : / / github . com / nteract / papermill [ 23 ] M . B . Kery , “Towards scaffolding complex exploratory data science programming practices , ” in 2018 IEEE Symposium on Visual Languages and Human - Centric Computing ( VL / HCC ) , Oct 2018 , pp . 273 – 274 . [ 24 ] S . Chattopadhyay , I . Prasad , A . Z . Henley , A . Sarma , and T . Barik , “What’s wrong with computational notebooks ? pain points , needs , and design opportunities , ” in CHI , 2020 . [ 25 ] M . B . Kery , “Tools to support exploratory programming with data , ” in 2017 IEEE Symposium on Visual Languages and Human - Centric Computing ( VL / HCC ) , Oct 2017 , pp . 321 – 322 . [ 26 ] A . Rule , “Design and use of computational notebooks , ” Ph . D . disserta - tion , UC San Diego , 2018 . [ 27 ] A . Rule , A . Tabard , and J . D . Hollan , “Exploration and explanation in computational notebooks , ” in Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , ser . CHI ’18 . New York , NY , USA : ACM , 2018 , pp . 32 : 1 – 32 : 12 . [ Online ] . Available : http : / / doi . acm . org / 10 . 1145 / 3173574 . 3173606 [ 28 ] V . I . Levenshtein , “Binary codes capable of correcting deletions , inser - tions , and reversals , ” in Soviet physics doklady , vol . 10 , no . 8 , 1966 , pp . 707 – 710 . [ 29 ] E . Duala - Ekoko and M . P . Robillard , “Tracking code clones in evolving software , ” 29th International Conference on Software Engineering ( ICSE’07 ) , May 2007 . [ Online ] . Available : http : / / dx . doi . org / 10 . 1109 / ICSE . 2007 . 90 [ 30 ] J . Brandt , P . J . Guo , J . Lewenstein , M . Dontcheva , and S . R . Klemmer , “Two studies of opportunistic programming , ” Proceedings of the 27th international conference on Human factors in computing systems - CHI 09 , 2009 . [ Online ] . Available : http : / / dx . doi . org / 10 . 1145 / 1518701 . 1518944 [ 31 ] “jupyterlab - git extension . ” [ Online ] . Available : https : / / github . com / jupyterlab / jupyterlab - git [ 32 ] “Verdant . ” [ Online ] . Available : https : / / github . com / mkery / Verdant [ 33 ] “Jupyter notebook diff and merge tools . ” [ Online ] . Available : https : / / nbdime . readthedocs . io / en / latest / [ 34 ] M . B . Kery and B . A . Myers , “Interactions for untangling messy history in a computational notebook , ” in 2018 IEEE Symposium on Visual Languages and Human - Centric Computing ( VL / HCC ) , Oct 2018 , pp . 147 – 155 . [ 35 ] “Nextjournal . ” [ Online ] . Available : https : / / nextjournal . com [ 36 ] “Apple timemachine . ” [ Online ] . Available : https : / / en . wikipedia . org / wiki / Time Machine ( macOS ) [ 37 ] A . Head , F . Hohman , T . Barik , S . Drucker , and R . DeLine , “Managing messes in computational notebooks , ” in Managing Messes in Computational Notebooks . ACM , May 2019 . [ On - line ] . Available : https : / / www . microsoft . com / en - us / research / publication / managing - messes - in - computational - notebooks / [ 38 ] M . Codoban , S . S . Ragavan , D . Dig , and B . Bailey , “Software history under the lens : A study on why and how developers examine it , ” in IEEE International Conference on Software Maintenance and Evolution ( ICSME ) , Sep . 2015 . [ 39 ] “Netﬂix’s metaﬂow . ” [ Online ] . Available : https : / / netﬂixtechblog . com [ 40 ] “Aws step functions . ” [ Online ] . Available : https : / / aws . amazon . com [ 41 ] Y . Gil , E . Deelman , M . Ellisman , T . Fahringer , G . Fox , D . Gannon , C . Goble , M . Livny , L . Moreau , and J . Myers , “Examining the challenges of scientiﬁc workﬂows , ” IEEE Computer , vol . 40 , no . 12 , pp . 24 – 32 , Dec . 2007 . [ 42 ] C . J . Kapser and M . W . Godfrey , “”cloning considered harmful” con - sidered harmful : patterns of cloning in software , ” Empirical Software Engineering , vol . 13 , no . 6 , pp . 645 – 692 , Jul . 2008 . [ 43 ] M . - A . Storey , L . Singer , B . Cleary , F . F . Filho , and A . Zagalsky , “The ( r ) evolution of social media in software engineering , ” in Proceedings of the on Future of Software Engineering - FOSE 2014 , 2014 . [ 44 ] S . Bellon , R . Koschke , G . Antoniol , J . Krinke , and E . Merlo , “Com - parison and evaluation of clone detection tools , ” IEEE Transactions on Software Engineering , vol . 33 , no . 9 , pp . 577 – 591 , 2007 . [ 45 ] R . Feldt , T . Zimmermann , G . R . Bergersen , D . Falessi , A . Jedlitschka , N . Juristo , J . M¨unch , M . Oivo , P . Runeson , M . Shepperd , D . I . K . Sjøberg , and B . Turhan , “Four commentaries on the use of students and professionals in empirical software engineering experiments , ” Empirical Software Engineering , vol . 23 , no . 6 , pp . 3801 – 3820 , Nov . 2018 . [ Online ] . Available : https : / / doi . org / 10 . 1007 / s10664 - 018 - 9655 - 0