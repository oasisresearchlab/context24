A Metadata Generation System with Semantic Understanding for Video Retrieval in Film Production Feilin Han Department of Film & TV Technology Beijing Film Academy Beijing , China hanfeilin @ bfa . edu . cn Zhaoxu Meng Department of Film & TV Technology Beijing Film Academy Beijing , China jackie . mzx @ gmail . com A BSTRACT In ﬁlm production , metadata plays an important role in original raw video indexing and classiﬁcation within the industrial post - production software . Inspired by deep visual - semantic methods , we propose an automated image information extraction process to ex - tend the diversity of metadata entities for massive large - scale raw video searching and retrieval . In this paper , we introduce the pro - posed system architecture and modules , integrating semantic anno - tation models and user - demand - oriented information fusion . We conducted experiments to validate the effectiveness of our system on Film Raw Video Semantic Annotation Dataset ( Film - RVSAD ) and Slate Board Template Dataset ( SBTD ) , two benchmark datasets built for cinematography - related semantic annotation and slate de - tection . Experimental results show that the proposed system pro - vides an effective strategy to improve the efﬁciency of metadata generation and transformation , which is necessary and convenient for collaborative work in the ﬁlmmaking process . Index Terms : Information systems—Data management systems— Extraction , transformation and loading ; Applied computing—Arts and humanities—Media arts ; 1 I NTRODUCTION While the industrialization process of ﬁlm production , how to im - prove the efﬁciency of existing ﬁlmmaking workﬂow has attracted much attention thanks to the development of digital media technol - ogy . Nowadays , almost all ﬁlms are shot by digital cameras , which means original shot videos , named raw video , are digitalized . These raw videos are extraordinarily large because they are recorded in RAW format with lossless compression . For a regular commercial ﬁlm , the originally recorded videos exceed 100TB in general , which is a time - consuming issue for post - production artists to browse all the videos . To solve this problem , there are several works assigned to im - prove efﬁciency . Firstly , when shooting the video , the digital ﬁlm camera will record basic information of this take and storied them in the metadata , a speciﬁc data structure that is designed for video management and digital image engineering . The detailed data struc - ture of metadata depends on the camera manufacturer , however , usually includes typical parameters of cameras , e . g . frame rate , shutter speed , aperture , focus , ISO , and timecode . Another im - portant work in the shooting period is slating . When the director says , ’Action ! ’ , the slate board will appear in front of the cam - era and close with a big sound . It is used to mark the name of this shooting , director , date , as well as the number of the scene , shot , and take , which plays a vital role in post - production editing . Figure 1 : An instance metadata example after being imported into the pose - production software . Meanwhile , a script supervisor is employed on the ﬁlm set , who is responsible for recording the cinematography - related information and scene details , such as lens number , camera movement , shot scale , time length , actors , dialogue , simple description of the per - formance , props , etc . , in a detailed and accurate continuity report . Metadata could be extracted while videos imported into the post - production software , such as DaVinci Resolve , AVID , and Adobe Premiere , shown as Figure 1 , working as a ﬁle classiﬁcation ref - erence . The Slate board could be previewed in the spotlight of this take . The script of the continuity report could be noted by the Digital Imaging Technician ( DIT ) , who is in charge of the manage - ment and veriﬁcation of raw videos . Otherwise , the editing assistant will import this information into the metadata via the metadata edit - ing workspace in post - production software manually . The works mentioned above are important in the ﬁlmmaking process but still need to be optimized due to their inefﬁciency , time - consuming , non - standardization , and experience - dependent . Considering manually annotated information is highly seman - tic . Inspired by computer - vision - based classiﬁcation and recogni - tion tasks , which have been proved to be practical and effective , we would like to embed semantic understanding methods with au - tomatic metadata generation in order to improve the efﬁciency of the ﬁlm raw video retrieval . Semantic label extraction and video caption generation could be used to construct the highly - integrated metadata , which could meet the requirements of post - production artists and users . Taking the use of intelligent and automatic meth - ods , the sematic label and value could be extracted and stored into the metadata structure , and re - construct the comprehensive meta - data ﬁle for video search and retrieval inside the post - production software . This approach can be based embedded into the existing ﬁlm production process . Figure 2 : Concept Figure . Our system integrates basic information , recorded by a camera , and image information , extracted as semantic annotations , into metadata ﬁle for raw video search and retrieval in the ﬁlm post - production process . In this work , we propose a metadata extraction method with se - mantic understanding , shown as Figure 2 , including semantic label annotation and user - demand - oriented information fusion , for video retrieval in ﬁlm production . We also collected a ﬁlm raw video re - trieval benchmark dataset to analyse the efﬁciency of the proposed system . The system could extract , transform , and import differ - ent metadata structures , adapting to the ﬁlm - production industrial software , so that post - production artists could choose the format they use . The proposed method improves the efﬁciency of retrieval , which could be essential and convenient for collaborative work . 2 R ELATED W ORK Video retrieval is widely used in ﬁlm recommendation and search for stream media platforms . As usual , when we search for a ﬁlm , we type in keywords which is related to the ﬁlm information , such as names of ﬁlm , directors , and actors . This kind of information , which we named instance information , comes from the crew , script , content , and abstract of the ﬁlm , given by ﬁlm producers and distrib - utors . However , the ﬁlms raw video search and retrieval task in ﬁlm production is different from the normal video retrieval to some ex - tent . In our work , we focus on the video retrieval in the ﬁlmmaking process , which pays more attention to image information , includ - ing the shot , visual , and content information , such as scene number , shot number , take number , camera movement , shot scale , actor , ac - tion , places , etc . These image information could be extracted by semantic understanding methods to generate comprehensive meta - data for search and retrieval . Inspired by deep visual - semantic em - bedding and image captioning , we would like to employ methods in semantic understanding , video retrieval , and ﬁlm asset management for reference to address this practical problem . Semantic Understanding for content - based visual information extraction has become an important research topic to discover high - level semantic descriptions within an image . Based on semantic un - derstanding , relevant keywords of a video clip could be extracted through text obtained from video metadata , speech - to - text , and clip summarizies [ 10 ] . To align and leverage visual data , a deep visual - semantic learning model could generate representations of image - region - level descriptions [ 8 ] . Deep - learning - based methods , such as semantic understanding of visual scenes [ 20 ] , face localisa - tion [ 1 ] , textual description [ 16 ] , and segmentation of images [ 4 ] , also play a signiﬁcant role in semantic extraction of visual content and show an exploring strategy for aggregating the context of image information . Video Retrieval is a critical challenging issue to search for a designative video with natural language queries , condensing multi - modal and high - dimension information into a single and compact representation [ 9 ] . In the literature , two strategies could be em - ployed for this task , annotation - based and visual - feature - based ap - proaches [ 14 ] . The former indexing system takes use of textual descriptions , attribute information , and keywords to represent the video content [ 17 ] , whereas the latter is focusing on aggregating the frame visual features into a global descriptor , such as deep - features [ 7 ] and sentence - level - features [ 11 ] , which could generate rich dynamic content of video for searching , retrieving , or brow - ing [ 18 ] . Video retrieval methods mentioned above mainly distin - guish videos with regional contextual relationships between entities , which is usually used for video recommendation and ranking [ 6 ] . However , in view of the fact that raw videos may be shot in the same scene from different angles of the camera , with a strong intra - instance difference and inter - instance similarity , which makes the application of video retrieval methods more difﬁcult to meet the requirement in ﬁlm production . Asset Management works as a discipline that provides meth - ods and tools for higher efﬁciency of production and operation . For traditional ﬁlm asset management in practical , Digital Imaging Technicians lead the effort to build collaborating ﬁle management pipeline , including standardized rules of ﬁle naming , authorization , and replica , with the help of daily shooting reports , Filemaker soft - ware , and a storage server . Nowadays , most state - of - research as - set management tools focus on tracking , exploring , and retrieving data to conduct structural knowledge and organized information , of - fering collaboration and workﬂow - execution - related operations [ 5 ] , which provides a new idea for ﬁlm raw video management . With the development of advanced digital ﬁlm technologies , such as vir - tual production , digital double actors , and VFX composition , the ﬁlm assets grow up in diversity and amount . Human - in - the - loop systems could accelerate repeated workﬂows by intelligently track - ing changes and intermediate results over time [ 15 ] , which offers the opportunity to realize the user - demand - oriented information fu - sion . Moreover , industrialization gives more challenges to address the data veriﬁcation and handover process in an efﬁcient way and support various ﬁlm industrial post - production software . 3 S YSTEM A RCHITECTURE In this work , we propose a metadata extraction system , which is embedded with semantic understanding and video content analy - sis , aiming for user - demand - oriented search and raw video retrieve . This system could format the metadata according to the user de - mand and specialized post - production software , and attach it to the basic information recorded by the camera . In this section , the sys - tem architecture and the main methodology modules are presented with the user interface and function introduction as well . 3 . 1 Pre - process The original raw videos are in RAW formats , such as ARRIRAW , which is developed by ARRI digital ﬁlm camera manufacturer . RAW format video is recorded and created by a digital camera , without any processing of light information transmitted to the sen - sor to preserve the highest quality with the minimum compression . These original videos are usually managed , transferred , stored , and copied by DIT from the camera to the workstation directly in the ﬁlm production process . In order to use the original video in RAW , named raw video , as our input , there are two main pre - processing steps in our system , de - Bayer and image transformation . De - Bayer . Raw video can be converted into RGB video only after de - Bayer processing , which is the process of transforming the original single - channel image sequence , which represents the origi - nal Bayer array recorded by the camera sensor , into the RGB three - channel image sequence . Meanwhile , the metadata is generated together in the RAW format . Image Transformation . When shooting a ﬁlm , raw videos are usually created in LOG , which is a video recording mode that ap - plies the logarithmic function to the exposure curve , resulting in preserving the details of highlights and shadows . Different camera manufacturers developed different LOG modes , such as C - log , N - log , and S - log , from Canon , Nikon , and Sony respectively . Raw videos in LOG visually look grayer than normal color . As we all know , the computer - vision - based semantic understanding models have a certain color sensitivity , so color gamut conversion is needed before the semantic extraction . By applying look - up - table ( LUT ) to the raw video , the color of the image sequence could be converted into RGB colorspace , which is called primary color matching in ﬁlm post - production . Because raw videos are in extremely high resolution ( 4K or 8K ) and frame rate ( 60fps , or even 120fps ) , it is also necessary to reduce the resolution , down - sample frames , en - code , and compress in pre - processing . 3 . 2 Semantic Annotation After interviewing post - production directors , DITs , script supervi - sors , and editing assistants , we summarize the limitations of tradi - tional video management methods in ﬁlm production . Considering a combination of basic , instance , and image information , we select several semantic information for experimental implementation . It should be mentioned that the labels introduced in this system could be extended and amended in terms of user demands . To validate the availability of this system , we build the semantic understanding module with 4 main submodules , named Slate Detection , Camer - aMove Recognition , Actor Detection , and Scene and Object Recog - nition , to extract 10 classes of annotations , shown in Table 1 . Table 1 : The metadata labels and modules in our system . MetadataClass Values Module Slate SceneNum Int SlateDetection ShotNum Int TakeNum Int Camera CameraMove pedestal , dolly , truck , CameraMoveRecognition tilt , pan , zoom , etc . Shot ShotType full , medium - full , medium , ActorDetection close , close - up , etc . Actor ActorPID ActorName Scene Time Day , Night SceneType Inside , Outside SceneandObject Places CategoryName Recognition Object ObjectType CategoryName Figure 3 : Diagram of slate detection , camera movement recogni - tion , and shot scale recognition in our framework . 3 . 2 . 1 Slate Detection When we shoot a ﬁlm , each take of the raw video contains a slat - ing action in front of the video , which could offer us helpful infor - mation , such as scene number , shot number , take number , director name , cameraman namer , etc . In our work , the slate board detec - tion module , including slate board localization , region segmenta - tion , and OCR , is shown in Figure 3 ( a ) . In this system , we retrained the YOLOv5 model [ 12 ] , extracting feature points with the help of ORB method [ 13 ] to align the template image , following with OCR to recognize handwritten digits and words . This module is used to extract the recorded contents and transform them into the data of the corresponding metadata label . 3 . 2 . 2 CameraMove Recognition The main idea of camera movement recognition is to generate the sparse optical ﬂow trajectory of the frame sequence and compare it with patterns of typical camera movement . We extract features in each frame and predict the upcoming points in the next frame to obtain the feature point trajectory . the shown as Figure 3 ( b ) . To explain it more clearly , we choose several representative camera movements as references . When the camera pans or tilts , the im - age transforms in Euclidean transformation . We could reduce noise corner points with RANSAC [ 3 ] and add up the trajectory to de - termine whether its pattern matches the Euclidean transformation matrix . When the camera dollys , the corner points conform per - spectively . So that calculating angles between trajectories could be utilized to represent its trajectory pattern . When the cameraman shot handheld , the trajectory shows high - frequency irregular jitter , and the direction of the motion vector changes rapidly . The patterns of trajectory will be used to classify the cameramove types . 3 . 2 . 3 Actor Detection This system module includes actor recognition and shot scale recog - nition . The detection and recognition of actors can use the widely used location and recognition method to obtain character informa - tion . In this system , we employ the pre - trained RetinaFace [ 1 ] and ArcFace [ 2 ] models for face detection and feature extraction . Ac - cording to the principles in ﬁlmmaking visual language , shot scale is related to the proportion of the subject to the frame , for example , a medium shot usually shows half of the person from head to waist . So that we could calculate the proportion of the height of face to the height of frame to estimate the shot scale . When the person is facing away from the camera , we utilize the 3d pose estimation method to predict the height of the actor , which could be used to estimate the shot scale by calculating its proportion to the height of frame , shown as Figure 3 ( c ) Figure 4 : Diagram of our system user interface . 3 . 2 . 4 Scene and Object Recognition For the object and scene recognition module , we need to extract four kinds of labels , including time , scene type , place name , and object category . The normal recognition model can be used for ob - ject category recognition , and the scene type and place category can be predicted by the pre - trained places365 scene recognition model [ 19 ] . To classify the time , we select day and night binary classiﬁcation , which is a piece of important information marked for each scene in ﬁlm scripts . From the perspective of visual color , the night scene seems dark and blue , so that the value of HSV and RGB can be used to classify day and night . Meanwhile , the scene type , inside or outside , should be considered as prior knowledge . 3 . 3 Metadata Transformation Metadata ﬁle format transformation and conversion are integrated into this module . The camera manufacturers have independently - developed metadata ﬁle formats as varied as their brand . Mean - while , post - production softwares also present metadata information in different way . For instance , the metadata ﬁle formats of two mainly used post - production software , Avid Media Composer and Davinci Resolve , are in ale and CSV respectively . According to the camera manufacturer and software offered metadata deconstruction APIs , our system could read and extract metadata information , after importing the raw videos into the software media workspace . Our system fuses user - demand - oriented information into the ﬁ - nal metadata information , where allows users to choose the labels they need and the ﬁle format that their post - production software could read . The user interface is shown as Figure 4 . After pars - ing the basic information from the original metadata , we will add up the extracted metadata labels or re - construct a new metadata ﬁle to extend the diversity of semantic descriptions . All the metadata information can be automatically loaded for classify and index cor - responding videos in video management and retrieval . 4 I MPLEMENTATION For validating the proposed strategy , we conducted experiments to evaluate the efﬁciency of our system and the performance of generated metadata . The experimental conﬁguration is an integra - tion of Slate Detection , CameraMove Recognition , Actor Detec - Table 2 : Evaluation results of semantic annotation . SceneNum ShotNum TakeNum Time Accuracy 0 . 531 0 . 695 0 . 711 0 . 863 PID ShotScale CameraMove SceneType Accuracy 0 . 819 0 . 752 0 . 836 0 . 906 tion , and Scene and Object Recognition modules , with a graphical user interface . To quantitatively evaluate our system , we built two benchmark datasets , named Film Raw Video Semantic Annotation Dataset ( Film - RVSAD ) and Slate Board Template Dataset ( SBTD ) for cinematography - related information extraction with metadata semantic labels . 4 . 1 Evaluation Film - RVSAD is used to validate the effectiveness of our system . We collected the real ﬁlm original raw videos and annotated them with semantic information , shown as the upper line in Figure 5 . Considering the copyright of raw videos , we were not able to collect a large - scale dataset to retrain models for semantic understanding . In this paper , we build a speciﬁc dataset to showcase the effective - ness of the proposed architecture . The performance of our system is shown in Table 2 , which is the experimental results by employing pre - trained models and tools introduced above . In SBTD , we collected six kinds of slate board images that ap - peared in Film - RVSAD . As shown in Figure 6 ( a ) , all the slate board images were clear and the semantic regions in each image are annotated with the position coordinates of their segments . When training the slate board detection model , we applied image enhance - ment as well . The detection accuracy are shown in Figure 6 ( b ) . 4 . 2 Results We present the ability of our proposed system to generate the spe - ciﬁc semantic annotations in metadata , shown in Figure 5 . Com - pared with the ground truth , given in the upper line of Figure 5 , the proposed system could efﬁciently extract the required seman - tic information . As the output of our system , the metadata ﬁle can be loaded into post - production software and edited in the metadata workspace as well . The work we presented in this paper is still in the early stage . The system we designed offers an intelligent data management method for ﬁlm production , which can effectively improve the efﬁciency of large - scale raw video search and retrieval in the ﬁlmmaking pro - cess . However , it is not comprehensive enough . It is necessary to extend and improve the diversity of semantic information and sys - tem modules . The accuracy in quantitative evaluation depends on the performance of the selected pre - trained model . However , the benchmark dataset we collected is not rich enough to retrain all the models to improve the accuracy . With the development of virtual production and advanced VFX technology , multi - stand shooting , multi - scene transition , and col - laborative ﬁlmmaking workﬂow present more challenges . Metadata needs to be as detailed and accurate as possible to improve the ef - ﬁciency of storage , retrieval , browsing , and delivery . In the future , we will continue to conquer and ﬁnd out solutions for more complex and diverse data retrieval . This paper aims to provide a new appli - cation scenario , using the applicable semantic analysis model to ef - fectively browse raw videos , so that we could help post - production artists and producers improve their efﬁciency . Figure 5 : Region predictions and semantic annotations on Film - RVSAD . We use our method to generate metadata for video search and retrieval . Examples in the upper line are ground - truth in Film - RVSAD , and examples in the bottom line are results from our proposed system . Figure 6 : The Slate Board Template Dataset samples and the per - formance of slate board detection module . 5 C ONCLUSION We introduced an application scenario of browsing the massive large - scale raw videos with the help of metadata . In this paper , we propose an automated method with semantic understanding for raw video search and retrieval . Our system utilizes deep visual - semantic models to extract cinematography - related image informa - tion , which can be integrated into metadata labels . We conducted experiments to validate the effectiveness of our system . We showed that the proposed system provides an applicable metadata extraction strategy , generating semantic annotations in terms of user demand , to improve the efﬁciency of video retrieval in ﬁlm production . 6 A CKNOWLEDGEMENT This work was supported by Beijing Natural Science Foundation ( No . 4214073 ) and the National Social Science Fundation Art Project ( No . 20BC040 ) . Special thanks to all the cast and crew from Nian Nian , The Deep Blue , Spring in the Hood , MiLaZhiGe , YueShi , TianMingYiFuChouJi for generously offering original raw videos to support our research . R EFERENCES [ 1 ] J . Deng , J . Guo , E . Ververas , I . Kotsia , and S . Zafeiriou . Retinaface : Single - shot multi - level face localisation in the wild . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition , pages 5203 – 5212 , 2020 . [ 2 ] J . Deng , J . Guo , N . Xue , and S . Zafeiriou . Arcface : Additive an - gular margin loss for deep face recognition . In Proceedings of the IEEE / CVF conference on computer vision and pattern recognition , pages 4690 – 4699 , 2019 . [ 3 ] M . A . Fischler and R . C . Bolles . Random sample consensus : a paradigm for model ﬁtting with applications to image analysis and automated cartography . Communications of the ACM , 24 ( 6 ) : 381 – 395 , 1981 . [ 4 ] S . Hao , Y . Zhou , and Y . Guo . A Brief Survey on Semantic Segmenta - tion with Deep Learning . Neurocomputing , 406 : 302 – 321 , Sept . 2020 . [ 5 ] S . Idowu , D . Strüber , and T . Berger . Asset Management in Machine Learning : State - of - research and State - of - practice . ACM Computing Surveys , page 3543847 , June 2022 . [ 6 ] P . Jafarzadeh , Z . Amirmahani , and F . Ensan . Learning to Rank Knowl - edge Subgraph Nodes for Entity Retrieval . In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 2519 – 2523 , Madrid Spain , July 2022 . ACM . [ 7 ] Q . - Y . Jiang , Y . He , G . Li , J . Lin , L . Li , and W . - J . Li . SVD : A Large - Scale Short Video Dataset for Near - Duplicate Video Retrieval . In 2019 IEEE / CVF International Conference on Computer Vision ( ICCV ) , pages 5280 – 5288 , Seoul , Korea ( South ) , Oct . 2019 . IEEE . [ 8 ] A . Karpathy and L . Fei - Fei . Deep visual - semantic alignments for gen - erating image descriptions . IEEE Transactions on Pattern Analysis and Machine Intelligence , 39 ( 4 ) : 664 – 676 , 2017 . [ 9 ] Liu Yang , Samuel Albanie , Arsha Nagrani , and Andrew Zisserman . Use What You Have : Video Retrieval Using Representations From Collaborative Experts . page 14 , Cardiff , UK , Sept . 2019 . [ 10 ] R . Madhok , S . Mujumdar , N . Gupta , and S . Mehta . Semantic Un - derstanding for Contextual In - Video Advertising . Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 32 ( 1 ) , Apr . 2018 . [ 11 ] S . Maeoki , K . Uehara , and T . Harada . Interactive Video Retrieval with Dialog . In 2020 IEEE / CVF Conference on Computer Vision and Pattern Recognition Workshops ( CVPRW ) , pages 4091 – 4099 , Seattle , WA , USA , June 2020 . IEEE . [ 12 ] J . Redmon , S . Divvala , R . Girshick , and A . Farhadi . You only look once : Uniﬁed , real - time object detection . In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 779 – 788 , 2016 . [ 13 ] E . Rublee , V . Rabaud , K . Konolige , and G . Bradski . Orb : An efﬁ - cient alternative to sift or surf . In 2011 International conference on computer vision , pages 2564 – 2571 . Ieee , 2011 . [ 14 ] E . M . Saoudi and S . Jai - Andaloussi . A distributed Content - Based Video Retrieval system for large datasets . Journal of Big Data , 8 ( 1 ) : 87 , Dec . 2021 . [ 15 ] D . Xin , L . Ma , J . Liu , S . Macke , S . Song , and A . Parameswaran . Ac - celerating human - in - the - loop machine learning : Challenges and op - portunities . In Proceedings of the second workshop on data manage - ment for end - to - end machine learning , pages 1 – 4 , 2018 . [ 16 ] J . Xu , T . Mei , T . Yao , and Y . Rui . Msr - vtt : A large video description dataset for bridging video and language . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , June 2016 . [ 17 ] X . Yang , T . Zhang , and C . Xu . Text2video : An end - to - end learning framework for expressing text with videos . IEEE Transactions on Multimedia , 20 ( 9 ) : 2360 – 2370 , 2018 . [ 18 ] G . Zhao , M . Zhang , Y . Li , J . Liu , B . Zhang , andJ . - R . Wen . Pyramidre - gional graph representation learning for content - based video retrieval . Information Processing & Management , 58 ( 3 ) : 102488 , May 2021 . [ 19 ] B . Zhou , A . Khosla , A . Lapedriza , A . Torralba , and A . Oliva . Places : An image database for deep scene understanding . arXiv preprint arXiv : 1610 . 02055 , 2016 . [ 20 ] B . Zhou , H . Zhao , X . Puig , T . Xiao , S . Fidler , A . Barriuso , and A . Torralba . SemanticUnderstandingofScenesThroughtheADE20K Dataset . International Journal of Computer Vision , 127 ( 3 ) : 302 – 321 , Mar . 2019 .