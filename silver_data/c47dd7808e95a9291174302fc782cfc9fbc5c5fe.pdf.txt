A Reinforcement Learning Approach to Interactive - Predictive Neural Machine Translation Tsz Kin Lam † , ∗ and Julia Kreutzer ∗ and Stefan Riezler † , ∗ ∗ Computational Linguistics & † IWR , Heidelberg University , Germany { lam , kreutzer , riezler } @ cl . uni - heidelberg . de Abstract We present an approach to interactive - predictive neural machine translation that attempts to reduce human effort from three directions : Firstly , instead of requiring hu - mans to select , correct , or delete segments , we employ the idea of learning from hu - man reinforcements in form of judgments on the quality of partial translations . Sec - ondly , human effort is further reduced by using the entropy of word predictions as uncertainty criterion to trigger feedback requests . Lastly , online updates of the model parameters after every interaction allow the model to adapt quickly . We show in simulation experiments that re - ward signals on partial translations sig - niﬁcantly improve character F - score and BLEU compared to feedback on full trans - lations only , while human effort can be re - duced to an average number of 5 feedback requests for every input . 1 Introduction Interactive - predictive machine translation aims at obtaining high - quality machine translation by in - volving humans in a loop of user validations of partial translations suggested by the machine translation system . This interaction protocol can easily be ﬁt to neural machine translation ( NMT ) ( Bahdanau et al . , 2015 ) by conditioning the model’s word predictions on the user - validated preﬁx ( Knowles and Koehn , 2016 ; Wuebker et al . , 2016 ) . User studies conducted by Green et c (cid:13) 2018 The authors . This article is licensed under a Creative Commons 3 . 0 licence , no derivative works , attribution , CC - BY - ND . al . ( 2014 ) for phrase - based machine translation have shown that the interactive - predictive inter - action protocol leads to signiﬁcant reductions in post - editing effort . Other user studies on interac - tive machine translation based on post - editing have shown that human effort can also be reduced by improving the online adaptation capabilities of the learning system , both for statistical phrase - based ( Bentivogli et al . , 2016 ) or NMT systems ( Kari - mova et al . , 2017 ) . The goal of our work is to further reduce human effort in interactive - predictive NMT by combining the advantages of the interactive - predictive pro - tocol with the advantages of learning from weak feedback . For the latter we rely on techniques from reinforcement learning ( Sutton and Barto , 2017 ) , a . k . a . bandit structured prediction ( Sokolov et al . , 2016 ; Kreutzer et al . , 2017 ; Nguyen et al . , 2017 ) in the context of sequence - to - sequence learning . Our approach attacks the problem of reducing human effort from three innovative directions . • Firstly , instead of requiring humans to cor - rect or delete segments proposed by the ma - chine translation system , we employ the re - inforcement learning idea of humans provid - ing reward signals in form of judgments on the quality of the machine translation . Hu - man effort is reduced since each partial trans - lation receives a human reward signal at most once , rendering it a bandit - type feedback sig - nal , and each reward signal itself is easier to obtain than a correction of a translation . • In order to reduce the amount of feedback signals even further , we integrate an uncer - tainty criterion for word predictions to trig - ger requests for human feedback . Using the comparison of the current average entropy to the entropy of word predictions in the history as a measure for uncertainty , we reduce the amount of feedbacks requested from humans to an average number of 5 requests per input . • In contrast to previous approaches to interactive - predictive translation , the param - eters of our translation system are updated online after receiving feedback for partial translations . The update is done according to an actor - critic reinforcement learning proto - col where each update pushes up the score function of the partial translation sampled by the model ( called actor ) proportional to a learned reward function ( called critic ) . Fur - thermore , since the entropy criterion is based on the actor , it is also automatically updated . Frequent updates improve the adaptability of our system , resulting in a further reduction of human effort . The rest of this paper is structured as follows . In Section 2 , we will situate our approach in the con - text of interactive machine translation and analyze our contribution related to reinforcement learn - ing for sequence prediction problems . Details of our algorithm are given in Section 3 . We eval - uate our approach in a simulation study where bandit feedback is computed by evaluating par - tial translations against references under a charac - ter F - score metric ( Popovi´c , 2015 ) without reveal - ing the reference translation to the learning sys - tem ( Section 4 ) . We show that segment - wise re - ward signals improve translation quality over rein - forcement learning with sparse sentence - wise re - wards , measured by character F - score and corpus - based BLEU against references . Furthermore , we show that human effort , measured by the number of feedback requests , can be reduced to an average number of 5 requests per input . These implications of our new paradigm are discussed in Section 5 . 2 Related Work The interactive - predictive translation paradigm reaches back to early approaches for IBM - type ( Foster et al . , 1997 ; Foster et al . , 2002 ) and phrase - based machine translation ( Barrachina et al . , 2008 ; Green et al . , 2014 ) . Knowles and Koehn ( 2016 ) and Wuebker et al . ( 2016 ) presented neural inter - active translation prediction — a translation sce - nario where translators interact with an NMT sys - tem by accepting or correcting subsequent target tokens suggested by the NMT system in an auto - complete style . NMT is naturally suited for this incremental production of outputs , since it mod - els the probability of target tokens given a his - tory of target tokens sequentially from left to right . In standard supervised training with teacher forc - ing , this history comes from the ground truth , while in interactive - predictive translation it is pro - vided by the preﬁx accepted or entered by the user . Both approaches use references to simulate an in - teraction with a translator and compare their ap - proach to phrase - based preﬁx - search . They ﬁnd that NMT is more accurate in word and letter pre - diction and recovers better from failures . Similar to their work , we will experiment in a simulated environment with references mimicking the trans - lator . However , we do not use the reference di - rectly for teacher forcing , but only to derive weak feedback from it . Furthermore , our approach em - ploys techniques to reduce the number of inter - actions , and to update the model more frequently than after each sentence . Our work is also closely related to approaches for interactive pre - post - editing ( Marie and Max , 2015 ; Domingo et al . , 2018 ) . The core idea is to ask the translator to mark good segments and use these for a more informed re - decoding . Both studies could show a reduction in human effort for post - editing in simulation experiments . We share the goal of using human feedback more ef - fectively by targeting it towards essential transla - tion segments , however , our approach does adhere to the left - to - right navigation through translation hypotheses . In difference to these approaches , we try to reduce human effort even further by min - imizing the number of feedback requests and by frequent model updates . Reinforcing / penalizing a targeted set of actions can also be found in recent approaches to rein - forcement learning from human feedback . For ex - ample , Judah et al . ( 2010 ) presented a scenario where users interactively label freely chosen good and bad parts of a policy’s trajectory . The pol - icy is directly trained with this reinforcement sig - nal to play a real - time strategy game . Simulations of NMT systems interacting with human feed - back have been presented ﬁrstly by Kreutzer et al . ( 2017 ) , Nguyen ( 2017 ) , or Bahdanau et al . ( 2017 ) who apply different policy gradient algorithms , William’s REINFORCE ( Williams , 1992 ) or actor - critic methods ( Konda and Tsitsiklis , 2000 ; Sutton et al . , 2000 ; Mnih et al . , 2016 ) , respectively . While Bahdanau et al . ’s ( 2017 ) approach operates in a fully supervised learning scenario , where rewards are simulated in comparison to references with smoothed and length - rescaled BLEU , Kreutzer et al . ( 2017 ) and Nguyen et al . ( 2017 ) limit the setup to sentence - level bandit feedback , i . e . only one feedback is obtained for one completed translation per input . In this paper , we use actor - critic update strategies , but we receive simulated bandit feed - back on the sub - sentence level . We adopt techniques from active learning to re - duce the number of feedbacks requested from a user . Gonz´alez - Rubio et al . ( 2011 ; 2012 ) apply active learning for interactive machine translation , where a user interactively ﬁnishes the translation of an SMT system . The active learning component decides which sentences to sample for translation ( i . e . receive full supervision for ) and the SMT sys - tem is updated online ( Ortiz - Mart´ınez et al . , 2010 ) . In our algorithm the active learning component de - cides which preﬁxes to be rated ( i . e . receive weak feedback for ) based on their average entropy . En - tropy is a popular measure for uncertainty in active learning : the rationale is to feed the learning algo - rithm with labeled instances where it is least con - ﬁdent about its own predictions . This uncertainty sampling algorithm ( Lewis and Gale , 1994 ) is a popular choice for active learning for NLP tasks with expensive gold labeling , such as text classiﬁ - cation ( Lewis and Gale , 1994 ) , word - sense disam - biguation ( Chen et al . , 2006 ) and statistical parsing ( Tang et al . , 2002 ) . Our method falls into the cate - gory of stream - based online active learning ( as op - posed to pool - based active learning , selecting in - stances from a large pool of unlabeled data ) , since the algorithm decides on the ﬂy ( online ) which translation preﬁxes of the stream of source tokens to request feedback for . Instead of receiving gold annotations , as in the studies mentioned above , our algorithm receives weaker , bandit feedback — but the motivation of minimizing human labeling ef - fort is the same . 3 Reinforcement Learning for Interactive - Predictive Translation In the following , we will introduce the key ideas of our approach , formalize them , and present an al - gorithm for reinforcement learning for interactive - predictive NMT . 3 . 1 Actor - Critic Reinforcement Learning for NMT The objective of reinforcement learning methods is to maximize the expected reward obtainable from interactions of an agent ( here : a machine transla - tion system ) with an environment ( here : a human translator ) . In our case , the agent / system performs actions by predicting target words y t according to a stochastic policy p θ parameterized by an RNN encoder - decoder NMT system ( Bahdanau et al . , 2015 ) where p θ ( y | x ) = T y (cid:89) t = 1 p θ ( y t | x , y < t ) . ( 1 ) The environment / human can be formalized as a Markov Decision Process where a state at time t is a tuple s t = (cid:104) x , y < t (cid:105) consisting of the condition - ing context of the input x and the current produced history of target tokens y < t . Note that since states s t + 1 include the current chosen action y t and can contain long histories y < t , the state distribution is sparse and deterministic . The reward distribu - tion of the environment / critic is estimated by func - tion approximation in actor - critic methods . The re - ward estimator ( called critic ) is trained on actual rewards and updated after every interaction , and then used to update the parameters of the policy ( called actor ) in a direction of function improve - ment . We use the advantage actor critic frame - work of Mnih et al . ( 2016 ) which estimates the advantage A φ ( y t | s t ) in reward of choosing action y t in a given state s t over the mean reward value for that state . This framework has been applied to reinforcement learning for NMT by Nguyen et al . ( 2017 ) . The main objective of the actor is then to maximize the expected advantage L θ = E p ( x ) p θ ( y | x )   T y (cid:88) t = 1 A φ ( y t | s t )   . ( 2 ) The stochastic gradient of this objective for a sam - pled target word ˆ y t for an input x can be calculated following the policy gradient theorem ( Sutton et al . , 2000 ; Konda and Tsitsiklis , 2000 ) as ∇ L θ ( ˆ y t ) = T y (cid:88) t = 1 [ ∇ log p θ ( ˆ y t | s t ) A φ ( ˆ y t | s t ) ] . ( 3 ) In standard actor - critic algorithms , the parameters of actor and the critic are updated online at each time step . The actor parameters θ are updated by sampling ˆ y t from p θ and performing a step in the opposite direction of the stochastic gradient of L θ ( ˆ y t ) ; the critic parameters φ are updated by min - imizing L φ ( ˆ y t ) , deﬁned as the mean squared error of the reward estimator for sampled target word ˆ y t with respect to actual rewards ( for more details see Nguyen et al . ( 2017 ) ) . In our experiments , we sim - ulate user rewards by character F - score ( chrF ) val - ues of partial translations . 3 . 2 Triggering Human Feedback Requests by Actor Entropy Besides the idea of replacing human post - edits by human rewards , another key feature of our ap - proach is to minimize the number of requests for human feedback . This is achieved by computing the uncertainty of the policy distribution as the av - erage word - level entropy ¯ H of an n - word partial translation , deﬁned as ¯ H ( ˆ y 1 : n ) = 1 n n (cid:88) t = 1 (cid:34) − (cid:88) v ∈V p θ ( v | s t ) log p θ ( v | s t ) (cid:35) , ( 4 ) where ˆ y 1 : n = { ˆ y 1 , ˆ y 2 , . . . , ˆ y n } is a sequence of n predicted tokens starting at the sentence begin - ning , V is the output vocabulary , and p θ ( v | s t ) is the probability of predicting a word in V at state s t of the RNN decoder . A request for human feedback is triggered when ¯ H ( ˆ y 1 : n ) is higher than a running average γ by a factor of (cid:15) or when < eos > is generated . Upon receiving a reward from the user , both actor and critic are updated . Hence , our algorithm takes the middle ground between updating at each time step t and performing an update only after a reward signal for the completed translation is received . In our simulation experiments , this process is re - peated until the < eos > token is generated , or when a pre - deﬁned maximum length , here T max = 50 , is reached . 3 . 3 Simulating Human Rewards on Translation Quality Previous work on reinforcement learning in ma - chine translation has simulated human bandit feedback by evaluating full - sentence translations against references using per - sentence approxima - tions of BLEU ( Sokolov et al . , 2016 ; Kreutzer et al . , 2017 ; Nguyen et al . , 2017 ) . We found that when working with partial translations , user feedback on translation quality can successfully be simulated by computing the chrF - score ( Popovi´c , 2015 ) of the translation with respect to the refer - ence translation truncated to the same length . If the length of the translation exceeds the length of the reference , no truncation is used . We denote rewards as a function R ( ˆ y 1 : t ) of only the partial translation ˆ y 1 : t , in order to highlight the fact that rewards are in principle independent of reference translations . 3 . 4 Sampling versus Forced Decoding via Preﬁx Buffer Ξ The standard approach to estimate the expected re - ward in policy gradient techniques is to employ Monte - Carlo methods , in speciﬁc , multinomial sampling of actions . This guarantees an unbiased estimator and allows sufﬁcient exploration of the action space in learning . In contrast , interactive - predictive machine translation usually avoids ex - ploration in favor of exploitation by decoding the best partial translation under the current model af - ter every interaction . Since in our framework , learning and decoding are interleaved , we have to ﬁnd the best compromise between exploration and exploitation . The general modus operandi of our framework is simultaneous exploration and exploitation by multinomial sampling actions from the current policy . However , in cases where a partial trans - lation receives a high user reward , we store it in a so - called preﬁx buffer Ξ , and perform forced de - coding by feeding the preﬁx to the decoder for the remaining translation process . 3 . 5 Algorithm for Bandit Interactive - Predictive NMT Algorithm 1 gives pseudo - code for B andit - I nteractive - P redictive N eural M achine T ranslation ( BIP - NMT ) . The algorithm re - ceives an input source sequence x i ( line 4 ) , and incrementally predicts a sequence of output target tokens up to length T max ( line 6 ) . At each step t , a partial translation ˆ y 1 : t is sampled from the policy distribution p θ ( · | x i , y < t , Ξ ) that implements an RNN encoder - decoder with an additional preﬁx buffer Ξ for forced decoding ( line 7 ) . User feedback is requested in case the average entropy ¯ H ( ˆ y 1 : t ) of the policy is larger than or equal to a running average by a factor of (cid:15) or when < eos > is generated ( line 9 ) . If the reward R ( ˆ y 1 : t ) is larger than or equal to a threshold µ , the preﬁx is stored in a buffer for forced decoding ( lines 11 - 12 ) . Next , Algorithm 1 : Bandit Interactive - Predictive 1 : Input : θ 0 , φ 0 , α A , α C , (cid:15) 2 : Output : Estimates θ ∗ , φ ∗ 3 : k ← 1 4 : for i ← 1 , . . . N do 5 : Receive x i , Initialize γ 0 ← 0 , Ξ ← ∅ 6 : for t ← 1 . . . T max do 7 : Sample ˆ y 1 : t ∼ p θ k − 1 ( · | x i , ˆy < t , Ξ ) 8 : Compute ¯ H ( ˆ y 1 : t ) using Eq . ( 4 ) 9 : if ¯ H ( ˆ y 1 : t ) − γ t − 1 ≥ (cid:15) · γ t − 1 or < eos > in ˆ y 1 : t then 10 : Receive feedback R ( ˆ y 1 : t ) 11 : if R ( ˆ y 1 : t ) ≥ µ then 12 : Ξ ← ˆ y 1 : t 13 : end if 14 : θ k ← θ k − 1 − α A ∇ L θ k − 1 ( ˆ y 1 : t ) ( Eq . ( 3 ) ) 15 : φ k ← φ k − 1 − α C ∇ L φ k − 1 ( ˆ y 1 : t ) ( see Eq . ( 7 ) in Nguyen et al . ( 2017 ) ) 16 : k ← k + 1 17 : end if 18 : Update γ t ← γ t − 1 + 1 t (cid:0) ¯ H ( ˆ y 1 : t ) − γ t − 1 (cid:1) 19 : break if < eos > in ˆ y 1 : t 20 : end for 21 : end for updates of the parameters of the policy ( line 14 ) , critic ( line 15 ) , and average entropy ( line 17 ) are performed . Actor and critic each use a separate learning rate schedule ( α A and α C ) . Figure 1 visualizes the interaction of the BIP - NMT system with a human for a single translation : Feedback is requested when the model is uncertain or the translation is completed . It is directly used for a model update and , in case it was good , for ﬁlling the preﬁx buffer , before the model moves to generating the next ( longer ) partial translation . 4 Experiments We simulate a scenario where the learning NMT system requests online bandit feedback for partial translations from a human in the loop . The fol - lowing experiments will give an initial practical as - sessment of our proposed interactive learning algo - rithm . Our analysis of the interactions between ac - tor , critic and simulated human will provide further insights into the learning behavior of BIP - NMT . START Predict partial translation Requestfeedback ? Update parameters Goodpreﬁx ? PreﬁxBuffer No Yes Yes NMT STOP EOS ? Yes No Figure 1 : Interaction of the NMT system with the human during learning for a single translation . Dataset EP ( v . 5 ) ¯ n NC ( WMT07 ) ¯ n Training ( ﬁlt . ) 1 , 346 , 679 23 . 5 9 , 216 21 . 9 Validation 2 , 000 29 . 4 1 , 064 24 . 1 Test - - 2 , 007 24 . 8 Table 1 : Number of parallel sentences and average number of words per sentence in target language ( en ) , denoted by ¯ n , for training ( ﬁltered to a maximum length of 50 ) , validation and test sets for French - to - English translation for Europarl ( EP ) and News Commentary ( NC ) domains . 4 . 1 Setup Data and Preprocessing . We conduct experi - ments on French - to - English translation on Eu - roparl ( EP ) and News Commentary ( NC ) domains . The large EP parallel corpus is used to pre - train the actor in a fully - supervised setting with a standard maximum likelihood estimation objective . The critic network is not pre - trained . For interactive training with bandit feedback , we extract 10k sen - tences from the NC corpus . Validation and test sets are also chosen from the NC domain . Note that in principle more sentences could be used , however , we would like to simulate a realistic sce - nario where human feedback is costly to obtain . Data sets were tokenized and cleaned using Moses tools ( Koehn et al . , 2007 ) . Furthermore , sen - tences longer than 50 tokens were removed from the training data . Each language’s vocabulary con - tains the 50K most frequent tokens extracted from the two training sets . Table 1 summarizes the data statistics . Model Conﬁguration and Training . Following Nguyen et al . ( 2017 ) , we employ an architecture of two independent but similar encoder - decoder frameworks for actor and critic , respectively , each using global - attention ( Luong et al . , 2015 ) and uni - directional single - layer LSTMs 1 . Both the size of word embedding and LSTM’s hidden cells are 500 . We used the Adam Optimizer ( Kingma and Ba , 2015 ) with β 1 = 0 . 9 and β 2 = 0 . 999 . Dur - ing supervised pre - training , we train with mini - batches of size 64 , and set Adam’s α = 10 − 3 . A decay factor of 0 . 5 is applied to α , starting from the ﬁfth pass , when perplexity on the valida - tion set increases . During interactive training with bandit feedback , we perform true online updates ( i . e . mini - batch size is 1 ) with Adam’s α hyper - parameter kept constant at 10 − 5 for both the actor and the critic . In addition , we clip the Euclidean norm of gradients to 5 in all training cases . Baselines and Evaluation . Our supervised out - of - domain baseline consists of the actor NMT sys - tem described as above , pre - trained on Europarl , with optimal hyperparameters chosen according to corpus - level BLEU on the validation set . Starting from this pre - trained EP - domain model , we fur - ther train a bandit learning baseline by employing Nguyen’s ( 2017 ) actor - critic model , trained on one epoch of sentence - level simulated feedback . The choice of comparing models after one epoch of training is a realistic simulation of a human - system interaction on a sequence of data where each input is seen only once . The feedback signal is simulated with chrF , using character - n - grams of length 6 and a value of β = 2 of the importance factor of recall over precision . While during training exploration through sampling is essential , during inference and for ﬁnal model evaluation we use greedy decoding . We evaluate the trained models on our test set from the NC - domain using average sentence - level chrF and standard corpus - level BLEU ( Papineni et al . , 2002 ) to measure how well they got adapted to the new domain . 4 . 2 Results and Analysis Table 2 shows the results of an evaluation of a baseline NMT model pre - trained by maximum likelihood on out - of - domain data . This is com - pared to an actor - critic baseline that trains the model of Nguyen et al . ( 2017 ) on sentence - level in - domain bandit feedback for one epoch . This approach can already improve chrF ( + 0 . 95 ) and BLEU ( + 0 . 55 ) signiﬁcantly by seeing bandit feed - back on in - domain data . BIP - NMT , with opti - mal hyperparameters (cid:15) = 0 . 75 , µ = 0 . 8 chosen 1 Our code can be accessed via the link https : / / github . com / heidelkin / BIPNMT . Figure 2 : Average cumulative entropy during one epoch of BIP - NMT training with µ = 0 . 8 and (cid:15) = { 0 , 0 . 25 , 0 . 5 , 0 . 75 } . on the validation set , is trained in a similar way for one epoch , however , with the difference that even weaker sub - sentence level bandit feedback is provided on average 5 times per input . We see that BIP - NMT signiﬁcantly improves both BLEU ( + 2 . 18 ) and chrF ( + 2 . 04 ) by even larger margins . Table 3 analyzes the impact of the metaparame - ter (cid:15) of the BIP - NMT algorithm . We run each ex - periment three times and report mean results and standard deviation . (cid:15) controls the margin by which the average word - level entropy needs to increase with respect to the running average in order to trigger a feedback request . Increasing this margin from 0 to 0 . 25 , 0 . 5 and 0 . 75 corresponds to de - creasing the number of feedback requests by a fac - tor of 3 from around 16 to around 5 . This reduction corresponds to a small increase in chrF ( + 0 . 29 ) and a small decrease in BLEU ( - 0 . 47 ) . Figure 2 shows another effect of the metaparam - eter (cid:15) : It shows the variation of the average word - level entropy ¯ H over time steps of the algorithm during one epoch of training . This is computed as a cumulative average , i . e . , the value of ¯ H is ac - cumulated and averaged over the number of tar - get tokens produced for all inputs seen so far . We see that average cumulative entropy increases in the beginning of the training , but then decreases rapidly , with faster rates for smaller values of (cid:15) , corresponding to more updates per input . The metaparameter µ controls the threshold of the reward value that triggers a reuse of the pre - ﬁx for forced decoding . In our experiments , we set this parameter to a value of 0 . 8 in order to avoid re - translations of already validated preﬁxes , even if they might sometimes lead to better ﬁnal full trans - System chrF ( std ) BLEU ( std ) ∆ chrF ∆ BLEU Out - of - domain NMT 61 . 30 24 . 77 0 0 Nguyen et al . ( 2017 ) 62 . 25 ( 0 . 08 ) 25 . 32 ( 0 . 02 ) + 0 . 95 + 0 . 55 BIP - NMT ( (cid:15) = 0 . 75 , µ = 0 . 8 ) 63 . 34 ( 0 . 12 ) 26 . 95 ( 0 . 12 ) + 2 . 04 + 2 . 18 Table 2 : Evaluation of pre - trained out - of - domain baseline model , actor - critic learning on one epoch of sentence - level in - domain bandit feedback ( Nguyen et al . , 2017 ) and BIP - NMT with settings (cid:15) = 0 . 75 , µ = 0 . 8 trained on one epoch of sub - sentence level in - domain bandit feedback . Results are given on the NC test set according to average sentence - level chrF and corpus - level BLEU . Result differences between all pairs of systems are statistically signiﬁcant according to multeval ( Clark et al . , 2011 ) . (cid:15) chrF ( std ) BLEU ( std ) Avg # Requests ∆ chrF ∆ BLEU ∆ Avg # Requests 0 61 . 86 ( 0 . 06 ) 25 . 54 ( 0 . 17 ) 15 . 91 ( 0 . 01 ) 0 0 0 0 . 25 62 . 15 ( 0 . 17 ) 25 . 84 ( 0 . 13 ) 11 . 06 ( 0 . 07 ) + 0 . 29 + 0 . 3 - 5 0 . 5 61 . 95 ( 0 . 05 ) 25 . 46 ( 0 . 09 ) 7 . 26 ( 0 . 03 ) + 0 . 09 - 0 . 08 - 9 0 . 75 62 . 15 ( 0 . 04 ) 25 . 07 ( 0 . 12 ) 4 . 94 ( 0 . 02 ) + 0 . 29 - 0 . 47 - 11 Table 3 : Impact of entropy margin (cid:15) on average sentence - level chrF score , corpus BLEU and average number of feedback requests per sentence on the NC validation set . The feedback quality threshold µ is set to 0 . 8 for all models . lations . We found the effect of lowering µ from 1 . 0 to 0 . 8 negligible on the number of feedback re - quests and on translation quality but beneﬁcial for the usability . 4 . 3 Example Protocols Table 4 presents user - interaction protocols for three examples encountered during training of BIP - NMT with (cid:15) = 0 . 75 , µ = 0 . 8 . For illustra - tive purposes , we chose examples that differ with respect to the number of feedback requests , the use of the preﬁx buffer , and the feedback values . Preﬁxes that receive a feedback ≥ µ and are thus stored in the buffer and re - used for later samples are indicated by underlines . Advantage scores < 0 indicate a discouragement of individual tokens and are highlighted in red . In the ﬁrst example , the model makes frequent feedback requests ( in 8 of 17 decoding steps ) and ﬁlls the preﬁx buffer due to the high quality of the samples . The second example can use the preﬁx buffer only for the ﬁrst two tokens since the feed - back varies quite a bit for subsequent partial trans - lations . Note how the token - based critic encour - ages a few phrases of the translations , but discour - ages others . The ﬁnal example shows a translation where the model is very certain and hence requests feedback only after the ﬁrst and last token ( mini - mum number of feedback requests ) . The critic cor - rectly identiﬁes problematic parts of the translation regarding the choice of prepositions . 5 Conclusion We presented a novel algorithm , coined BIP - NMT , for bandit interactive - predictive NMT using re - inforcement learning techniques . Our algorithm builds on advantage actor - critic learning ( Mnih et al . , 2016 ; Nguyen et al . , 2017 ) for an interactive translation process with a human in the loop . The advantage over previously presented algorithms for interactive - predictive NMT is the low human effort for producing feedback ( a translation quality judgment instead of a correction of a translatioin ) , even further reduced by an active learning strategy to request feedback only for situations where the actor is uncertain . We showcased the success of BIP - NMT with simulated feedback , with the aim of moving to real human feedback in future work . Before deploying this algorithm in the wild , suitable interfaces for giving real - valued feedback have to be explored to create a pleasant user experience . Furthermore , in order to increase the level of human control , a combination with the standard paradigm that al - lows user edits might be considered in future work . Finally , our algorithm is in principle not limited to the application of NMT , but can furthermore — thanks to the broad adoption of neural sequence - to - sequence learning in NLP — be extended to other structured prediction or sequence generation tasks . SRC depuis 2003 , la chine est devenue le plus important partenaire commercial du mexique apr ` es les etats - unis . REF since 2003 , china has become mexico ’s most important trading partner after the united states . < / s > Partial sampled translation Feedback since 1 since 2003 , china has 1 since 2003 , china has become 1 since 2003 , china has become mexico 1 since 2003 , china has become mexico ’s 1 since 2003 , china has become mexico ’s most 1 since 2003 , china has become mexico ’s most important 1 since 2003 , china has become mexico ’s most important trading partner after the us . < / s > 0 . 8823 SRC la r´eponse que nous , en tant qu’ individus , acceptons est que nous sommes libres parce que nous nous gouvernons nous - mˆemes en commun plutˆot que d’ ˆetre dirig´es par une organisation qui n’ a nul besoin de tenir compte de notre existence . REF the answer that we as individuals accept is that we are free because we rule ourselves in common , rather than being ruled by some agency that need not take account of us . < / s > Partial sampled translation Feedback the 1 the answer 1 the answer we 0 . 6964 the answer we , 0 . 6246 the answer we as individuals allow to 14 are 0 . 6008 the answer we , as individuals , go down to speak 8 , are being free because we govern ourselves , rather from being based together 0 . 5155 the answer we , as people , accepts is that we principle are free because we govern ourselves , rather than being led by a organisation which has absolutely no need to take our standards . < / s > 0 . 5722 SRC lors d’ un rallye “journ´ee j´erusalem” tenu ` a l’ universit´e de t´eh´eran en d´ecembre 2001 , il a prononc´e l’ une des menaces les plus sinistres du r´egime . REF at a jerusalem day rally at tehran university in december 2001 , he uttered one of the regime ’s most sinister threats . < / s > Partial sampled translation Feedback in 0 in a round of jerusalem called a academic university in teheran in december 2001 , he declared one in the most recent hostility to the regime . < / s > 0 . 5903 Table 4 : Interaction protocol for three translations . These translations were sampled from the model when the algorithm decided to request human feedback ( line 10 in Algorithm 1 ) . Tokens that get an overall negative reward ( in combination with the critic ) , are marked in red , the remaining tokens receive a positive reward . When a preﬁx is good ( i . e . ≥ µ , here µ = 0 . 8 ) it is stored in the buffer and used for forced decoding for later samples ( underlined ) . Acknowledgments . This work was supported in part by DFG Research Grant RI 2221 / 4 - 1 . We would like to thank the members of the Statistical NLP Colloquium Hei - delberg for fruitful discussions and ideas for im - provement of our algorithm . References Bahdanau , Dzmitry , Kyunghyun Cho , and Yoshua Ben - gio . 2015 . Neural machine translation by jointly learning to align and translate . In Proceedings of the International Conference on Learning Represen - tations ( ICLR ) , San Diego , CA . Bahdanau , Dzmitry , Philemon Brakel , Kelvin Xu , Anirudh Goyal , Ryan Lowe , Joelle Pineau , Aaron Courville , and Yoshua Bengio . 2017 . An actor - critic algorithm for sequence prediction . In Proceedings of the 5th International Conference on Learning Repre - sentations ( ICLR ) , Toulon , France . Barrachina , Sergio , Oliver Bender , Francisco Casacu - berta , Jorge Civera , Elsa Cubel , Shahram Khadivi , Antonio Lagarda , Hermann Ney , Jes´us Tom´as , En - rique Vidal , and Juan - Miguel Vilar . 2008 . Statistical approaches to computer - assisted translation . Com - putational Linguistics , 35 ( 1 ) : 3 – 28 . Bentivogli , Luisa , Nicola Bertoldi , Mauro Cettolo , Marcello Federico , Matteo Negri , and Marco Turchi . 2016 . On the evaluation of adaptive machine trans - lation for human post - editing . IEEE Transactions on Audio , Speech and Language Processing ( TASLP ) ) , 24 ( 2 ) : 388 – 399 . Chen , Jinying , Andrew Schein , Lyle Ungar , and Martha Palmer . 2006 . An empirical study of the behavior of active learning for word sense disambiguation . In Human Language Technologies : The 2006 Annual Conference of the North American Chapter of the Association for Computational Linguistics ( NAACL - HLT ) , New York City , NY . Clark , Jonathan , Chris Dyer , Alon Lavie , and Noah Smith . 2011 . Better hypothesis testing for statis - tical machine translation : Controlling for optimizer instability . In Proceedings of the 49th Annual Meet - ing of the Association for Computational Linguistics ( ACL’11 ) , Portland , OR . Domingo , Miguel , ´Alvaro Peris , and Francisco Casacu - berta . 2018 . Segment - based interactive - predictive machine translation . Machine Translation . Foster , George , Pierre Isabelle , and Pierre Plamon - don . 1997 . Target - text mediated interactive machine translation . Machine Translation , 12 ( 1 - 2 ) : 175 – 194 . Foster , George , Philippe Langlais , and Guy Lapalme . 2002 . User - friendly text prediction for transla - tors . In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , Philadelphia , PA . Gonz ´ alez - Rubio , Jes ´ us , Daniel Ortiz - Mart ´ ınez , and Francisco Casacuberta . 2011 . An active learning scenario for interactive machine translation . In Pro - ceedings of the 13th International Conference on Multimodal Interfaces ( ICMI ) , Barcelona , Spain . Gonz ´ alez - Rubio , Jes ´ us , Daniel Ortiz - Mart ´ ınez , and Francisco Casacuberta . 2012 . Active learning for interactive machine translation . In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics ( EACL ) , Avignon , France . Green , Spence , Sida I . Wang , Jason Chuang , Jeffrey Heer , Sebastian Schuster , and Christopher D . Man - ning . 2014 . Human effort and machine learnability in computer aided translation . In Proceedings the Conference on Empirical Methods in Natural Lan - guage Processing ( EMNLP ) , Doha , Qatar . Judah , Kshitij , Saikat Roy , Alan Fern , and Thomas G . Dietterich . 2010 . Reinforcement learning via prac - tice and critique advice . In Proceedings of the 24th AAAI Conference on Artiﬁcial Intelligence , Atlanta , GA . Karimova , Sariya , Patrick Simianer , and Stefan Riezler . 2017 . A user - study on online adaptation of neu - ral machine translation to human post - edits . CoRR , abs / 1712 . 04853 . Kingma , Diederik P . and Jimmy Ba . 2015 . Adam : A method for stochastic optimization . In Proceedings of the International Conference on Learning Repre - sentations ( ICLR ) , San Diego , CA . Knowles , Rebecca and Philipp Koehn . 2016 . Neu - ral interactive translation prediction . In Proceedings of the Conference of the Association for Machine Translation in the Americas ( AMTA ) , Austin , TX . Koehn , Philipp , Hieu Hoang , Alexandra Birch , Chris Callison - Birch , Marcello Federico , Nicola Bertoldi , Brooke Cowan , Wade Shen , Christine Moran , Richard Zens , Chris Dyer , Ondrej Bojar , Alexandra Constantin , and Evan Herbst . 2007 . Moses : Open source toolkit for statistical machine translation . In Proceedings of the ACL 2007 Demo and Poster Ses - sions , Prague , Czech Republic . Konda , Vijay R . and John N . Tsitsiklis . 2000 . Actor - critic algorithms . In Advances in Neural Information Processing Systems ( NIPS ) , Vancouver , Canada . Kreutzer , Julia , Artem Sokolov , and Stefan Riezler . 2017 . Bandit structured prediction for neural sequence - to - sequence learning . In Proceedings of the 55th Annual Meeting of the Association for Com - putational Linguistics ( ACL ) , Vancouver , Canada . Lewis , David D and William A Gale . 1994 . A se - quential algorithm for training text classiﬁers . In Proceedings of the 17th Annual International ACM - SIGIR Conference on Research and Development in Information Retrieval ( SIGIR ) , Dublin , Ireland . Luong , Thang , Hieu Pham , and Christopher D . Man - ning . 2015 . Effective approaches to attention - based neural machine translation . In Proceedings of the Conference on Empirical Methods in Natural Lan - guage Processing ( EMNLP ) , Lisbon , Portugal . Marie , Benjamin and Aur ´ elien Max . 2015 . Touch - based pre - post - editing of machine translation out - put . In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , Lisbon , Portugal . Mnih , Volodymyr , Adri ` a Puigdom ` enech Badia , Mehdi Mirza , Alex Graves , Timothy P . Lillicrap , Tim Harley , David Silver , and Koray Kavukcuoglu . 2016 . Asynchronous methods for deep reinforce - ment learning . In Proceedings of the 33rd Inter - national Conference on Machine Learning ( ICML ) , New York , NY . Nguyen , Khanh , Hal Daum´e , and Jordan Boyd - Graber . 2017 . Reinforcement learning for bandit neural ma - chine translation with simulated feedback . In Pro - ceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , Copen - hagen , Denmark . Ortiz - Mart´ınez , Daniel , Ismael Garc´ıa - Varea , and Fran - cisco Casacuberta . 2010 . Online learning for in - teractive statistical machine translation . In Human Language Technologies : The 2010 Annual Confer - ence of the North American Chapter of the Associ - ation for Computational Linguistics ( NAACL - HLT ) , Los Angeles , CA . Papineni , Kishore , Salim Roukos , Todd Ward , and Wei - Jing Zhu . 2002 . Bleu : a method for automatic eval - uation of machine translation . In Proceedings of the 40th Annual Meeting on Association for Computa - tional Linguistics ( ACL ) , Philadelphia , PA . Popovi´c , Maja . 2015 . chrF : character n - gram f - score for automatic mt evaluation . In Proceedings of the Tenth Workshop on Statistical Machine Translat ion ( WMT ) , Lisbon , Portugal . Sokolov , Artem , Julia Kreutzer , Christopher Lo , and Stefan Riezler . 2016 . Stochastic structured predic - tion under bandit feedback . In Advances in Neural Information Processing Systems ( NIPS ) , Barcelona , Spain . Sutton , Richard S . and Andrew G . Barto . 2017 . Re - inforcement Learning . An Introduction . The MIT Press , second edition . Sutton , Richard S . , David McAllester , Satinder Singh , and Yishay Mansour . 2000 . Policy gradient methods for reinforcement learning with function approxima - tion . In Advances in Neural Information Processings Systems ( NIPS ) , Vancouver , Canada . Tang , Min , Xiaoqiang Luo , and Salim Roukos . 2002 . Active learning for statistical natural language pars - ing . In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics ( ACL ) , Pennsylvania , PA . Williams , Ronald J . 1992 . Simple statistical gradient - following algorithms for connectionist reinforce - ment learning . Machine Learning , 8 : 229 – 256 . Wuebker , Joern , Spence Green , John DeNero , Sasa Hasan , and Minh - Thang Luong . 2016 . Models and inference for preﬁx - constrained machine trans - lation . In Proceedings of the 54th Annual Meet - ing of the Association for Computational Linguistics ( ACL ) , Berlin , Germany .