Dive into Deep Learning Aston Zhang Zachary C . Lipton Mu Li Alexander J . Smola 2019 년 05 월 22 일 This draft is a testing version ( draft date : May 22 , 2019 ) . Visit https : / / d2l . ai to obtain a later or release version . i ii Contents 1 서 문 1 1 . 1 코 드 , 수 학 , HTML 이 모 두 하 나 로 구 성 됨 . . . . . . . . . . . . . . . . . . . . . . . . 2 1 . 2 구 성 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1 . 3 직 접 하 면 서 배 우 기 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1 . 4 감 사 의 글 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2 이 책 의 사 용 방법 5 2 . 1 대 상 독 자 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2 . 2 내 용 및 구 성 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2 . 3 코 드 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2 . 4 토 론 ( Forum ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2 . 5 문 제 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2 . 6 Scan the QR Code to Discuss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3 딥 러 닝 소 개 9 3 . 1 데 이 터 를 활 용 하 는 프 로 그 래 밍 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3 . 2 기 원 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3 . 3 딥 러 닝 으 로 의 길 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3 . 4 성 공 사 례 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3 . 5 주 요 요 소 들 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3 . 6 요 약 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 iii 3 . 7 문 제 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3 . 8 참 고 문 헌 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3 . 9 Scan the QR Code to Discuss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4 딥 러 닝 맛 보 기 23 4 . 1 소 개 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4 . 2 Gluon 시 작 하 기 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 4 . 3 데 이 터 조 작 ( data manipulation ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 4 . 4 선 형 대 수 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 4 . 5 자 동 미 분 ( automatic differentiation ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 4 . 6 확 률 과 통 계 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 4 . 7 나 이 브 베 이 즈 분 류 ( Naive Nayes Classiﬁcation ) . . . . . . . . . . . . . . . . . . . . . 90 4 . 8 샘 플 링 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 4 . 9 문 서 ( documentation ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 5 딥 러 닝 기 초 109 5 . 1 선 형 회 귀 ( Linear Regression ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 5 . 2 선 형 회 귀 를 처 음 부 터 구 현 하 기 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 5 . 3 선 형 회 귀 의 간결 한 구 현 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 5 . 4 Softmax 회 귀 ( regression ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 5 . 5 이 미 지 분 류 데 이 터 ( Fashion - MNIST ) . . . . . . . . . . . . . . . . . . . . . . . . . . 141 5 . 6 Softmax 회 귀 ( regression ) 를 처 음 부 터 구 현 하 기 . . . . . . . . . . . . . . . . . . . . . 146 5 . 7 Softmax 회 귀 ( regression ) 의 간결 한 구 현 . . . . . . . . . . . . . . . . . . . . . . . . . 152 5 . 8 다 층 퍼 셉 트 론 ( Multilayer Perceptron ) . . . . . . . . . . . . . . . . . . . . . . . . . . 155 5 . 9 다 층 퍼 셉 트 론 ( multilayer perceptron ) 을 처 음 부 터 구 현 하 기 . . . . . . . . . . . . . . . 165 5 . 10 다 층 퍼 셉 트 론 ( multilayer perceptron ) 의 간결 한 구 현 . . . . . . . . . . . . . . . . . . 168 5 . 11 모 델 선 택 , 언 더 피 팅 ( underﬁtting ) , 오 버 피 팅 ( overﬁtting ) . . . . . . . . . . . . . . . . . 170 5 . 12 가 중 치 감 쇠 ( weight decay ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 5 . 13 드 롭 아 웃 ( dropout ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 5 . 14 순 전 파 ( forward propagation ) , 역 전 파 ( back propagation ) , 연 산 그 래 프 . . . . . . . . . . 198 5 . 15 수 치 안 정 성 ( numerical stability ) 및 초 기 화 . . . . . . . . . . . . . . . . . . . . . . . . 203 5 . 16 환 경 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 5 . 17 Kaggle 의 주 택 가격 예 측 하 기 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222 6 딥 러 닝 계 산 237 6 . 1 층 ( layer ) 과 블 럭 ( Block ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 6 . 2 파 라 미 터 관 리 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246 iv 6 . 3 초 기 화 지 연 ( deferred Initialization ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257 6 . 4 커 스 텀 층 ( custom layer ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 6 . 5 파 일 입 / 출 력 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265 6 . 6 GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269 7 Appendix 277 7 . 1 이 책 에 기 여 하 는 방법 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277 7 . 2 d2l 패 키 지 색 인 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284 v vi 1 서 문 불 과 몇 년 전 만 해 도 , 주 요 기 업 과 스 타 트 업에 서 지 능 형 제 품 과 서 비 스 를 개 발 하 는 다 수 의 딥 러 닝 과 학 자 들 은 존 재 하 지 않았 습 니다 . 우 리 중 막 내 ( 저 자 ) 가 현 장 에 들 어 갔 을 때 는 머 신 러 닝 은 일 간 지 의 헤 드 라 인을 장 식 하 지 도 않았 습 니다 . 우 리 의 부 모 님 들 은 머 신 러 닝 이 무 엇 인 지 전 혀 몰 랐 습 니다 . 왜 우 리 가 의 학 이 나 법 률 분 야 의 직 업 보 다 그 것 을 선 호 하 는 지 말 할 필 요 도 없었 습 니다 . 머 신 러 닝 은 실 제 응 용 의 좁 은 분 야에 해 당 하 는 미 래 지 향 적 인 분 야 였 습 니다 . 그 리 고 음 성 인 식 및 컴 퓨 터 비 전 과 같 은 응 용 들 은 많 은 도 메 인 지 식 을 필 요 로 했 기 때 문 에 , 머 신 러 닝 은 종종 별 도 로 분 리 된 하 나 의 작은 컴 포 넌 트 로 간 주 되 었 습 니다 . 이 책 에 서 다 루 고 있 는 , 딥 러 닝 의 선 행 모 델 인 뉴 럴 네 트 워 크 는 구 식 도 구 로 간 주 되 었 습 니다 . 지 난 5 년 간 딥 러 닝 은 컴 퓨 터 비 전 , 자 연어 처 리 , 자 동 음 성 인 식 , 강 화 학 습 , 통 계 모 델 링 등 다 양 한 분 야 의 급 속 한 발 전 을 이 룩 하 여 세 상 을 놀 라 게 했 습 니다 . 이 러 한 진 보 를 통 해 우 리 는 이 제 ( 자율 성 이 증 가 하 면 서 ) 스스 로 를 운 전 하 는 자 동 차 , 평 범 한 응 답 을 예 상 하 는 스 마 트 응 답 시스 템 , 사 람 들 이 이 메 일의 산 에 서 발 굴 하 도 록 돕 고 , 바 둑 을 세 계 에 서 제 일 잘 두 는 사 람 보 다 우월 한 소 프 트 웨 어 에 이 전 트 , 수십 년 의 노 력 이 필 요 한 기 술 들 을 만 들 수 있 습 니다 . 이 미 이 러 한 도 구 들 은 영 화 제 작 방 식 , 질 병 진 단 방 식 , 천체 물 리 학 에 서 생 물 학 등 까 지 기 초 과 학 에 서 점점 더 많 은 역 할 을 하 고 있 습 니다 . 이 책 은 딥 러 닝 을 친 숙 하 게 , 개 념 , 문 맥 및 코 드 를 이 해 시 키 려 는 우 리 의 노 력 입 니다 . 1 1 . 1 코 드 , 수 학 , HTML 이 모 두 하 나 로 구 성 됨 컴 퓨 팅 기 술 이 완 전 히 영 향 을 미 치 려 면 , 잘 이 해하 고 , 잘 문 서 화 되 며 , 잘 관 리 되 고 완 성 된 도 구 를 통 해 지 원 되 어야 합 니다 . 핵 심 아 이 디 어 를 명 확 하 게 뽑 아 내 어 , 새 로 운 실 무 자 를 최 신 상 태 로 유 지 해 야 데 필 요 한 온 보 딩 시 간 을 최 소 화 해 야 합 니다 . 성 숙 한 라 이 브 러 리 는 일 반 적 인 작 업 을 자 동 화 해 야 하 며 , 예 제 코 드 는 실 무 자 가 필 요 에 맞 게 공 통 응 용 프 로 그 램 을 쉽 게 수 정 , 적 용 및 확 장 할 수 있 도 록 해 야 합 니다 . 동 적 웹 응 용 프 로 그 램 을 예 로 들 어 보 겠 습 니다 . Amazon 과 같 은 많 은 수 의 회 사 들 이 , 1990 년 대 에 데 이 터 베 이 스 기 반 웹 애 플 리 케 이 션 을 성 공 적 으 로 개 발 했 음 에 도 불 구 하 고 , 창 조적 기 업 가 를 돕 기 위 한 이 러 한 기 술 의 완 전 한 가 능 성 은 강 력 하 고 , 잘 문 서 화 된 프 레 임 워 크 의 개 발 로 겨 우 지 난 10 년 간 현 실 화 되 었 습 니다 . 딥 러 닝 의 실 현 은 , 모 든 어 플 리 케 이 션 은 다 양 한 분 야 의 통 합 을 가 져 왔 기 때 문 에 , 고 유 한 과 제 를 제 시 합 니다 . 딥 러 닝 을 적 용 하 기 위 해 서 는 , ( i ) 특 정 방 식 으 로 문 제제 기 를 위 한 동 기 , ( ii ) 주 어 진 모 델 링 접 근 을 위 한 수 학 , ( iii ) 모 델 을 데 이 터 에 맞 추 기 위 한 최 적 화 알 고 리 즘 ( iv ) 모 델 을 효 율 적 으 로 훈 련 하 는 데 필 요 한 엔 지 니 어 링 들 을 이 해하 고 수 치 연 산 의 위 험 ( Pitfalls ) 을 탐 색 하 고 가 용 한 하 드 웨 어 의 최 대 한 활 용 하 세 요 . 문 제 를 수식 으 로 만 들 기 위 한 비 판 적 사 고 기 술 , 그 것 을 풀 수 있 는 수 학 , 그 리 고 이 러 한 솔 루 션 들 을 모 두 구 현 하 기 위 한 소 프 트 웨 어 도 구 , 이 모 두 를 한 곳 에 서 가 리 치 는 것 은 매 우 어 려 운 과 제 입 니다 . 이 책 에 서 우 리 의 목 표 는 독 자 들 이 최 대 한 빠 르 게 실 무 자 가 될 수 있 도 록 , 통 합 된 자 원 을 제 시 하 는 것 입 니다 . 우 리 는 MXNet 의 새 로 운 Gluon 인 터 페 이 스 를 사 용 자 에 게 설 명 해 야 했 던 , 2017 년 7 월 에 이 책 프 로 젝 트 를 시 작 했 습 니다 . 동 시 에 당 시 에 는 , ( 1 ) 최 신 상 태 이 고 , ( 2 ) 기 술 적 깊 이 와 유 사 한 것 으 로 현 대 의 머 신 러 닝 의 전 체 폭 을 다 루 고 ( 3 ) 실 행 가 능 한 코 드 가 있 는 교 재 부 터 활 발 한 튜 토 리 얼 같 은 것 들 이 없 었 습 니다 . 우 리 는 딥 러 닝 과 관 련 된 프 레 임 워 크 를 사 용 하 는 방법 ( 예 : TensorFlow 에 서 행 렬 을 사 용 하 여 기 본 수 치 계 산 을 수 행하 는 방법 ) 또 는 특 정 기 술 ( 예 : LeNet , AlexNet , ResNet 등 의 코 드 일 부 들 ) 을 구 현 해 내 는 방법 혹 은 예 제 코 드들 을 블 로 그 게 시 물 형 태 또 는 GitHub 에 서 많 이 발 견 했 습 니다 . 그 러 나 이 예 제 는 일 반 적 으 로 주 어 진 접 근 방 식 을 구 현 하 는 방법 에 초 점 을 맞 추 었 지 만 , 왜 그 런 특 정 알 고 리 즘 이 선 택 되 었 는 지 에 대 한 논 의 를 생 략 했 습 니다 . 웹 사 이 트 Distill 또 는 개 인 블 로 그 와 같 은 곳 에 서 산 발 적 인 주제 들 이 논 의 되 는 동 안 , 딥 러 닝 에 서 선 택 한 주제 만 을 다 루 거 나 , 종종 관 련 된 코 드 가 부 족 한 경 우 가 많 았 습 니다 . 다 른 한 편 으 로 는 , 딥 러 닝 의 개 념 에 대 한 훌 륭 한 자 료 를 제 공 하 는 Goodfellow , Bengio and Courville , 2016 에 여 러 교 과 서 가 등 장 하 지 만 이 러 한 자 료 는 설 명 과 코 드 로 그 개 념 을 실 현 하 는 방법 을 잘 녹 아 내 지 못 했 습 니다 . 결 국 독 자 들 은 그 것 을 구 현 하 기 위 한 단 서 들 을 스스 로 찾 도 록 방 치 되 었 습 니다 . 또 한 너 무 많 은 자 료 가 유 료 교 육 과 정 제 공 업 체 에 숨 겨 져 있 습 니다 . 우 리 는 그 래 서 자 료 를 만 들 기 시 작 했 습 니다 . 이 자 료 는 ( 1 ) 모 든 사 람 이 자유 롭 게 이 용 할 수 있 고 , 2 1 . 서 문 ( 2 ) 실 제 로 응 용 머 신 러 닝 과 학 자 가 될 때 필 요 한 , 충 분 한 기 술 적 깊 이 를 제 공 하 고 , ( 3 ) 실 행 가 능 한 코 드 를 포 함 시 키 고 , 실 제 로 문 제 를 해 결 하 는 방법 을 독 자 에 게 보 여 주 며 , ( 4 ) 우 리 뿐 만 아 니 라 많 은 커 뮤 니 티 에 의 해 신 속 하 게 업 데 이 트 할 수 있으 며 , ( 5 ) 기 술 세 부 사 항 에 대 한 상 호 토 론 과 질 문 에 답 하 기 위 해 forum 를 이 용 하 여 보 완 됩 니다 . 이 러 한 목 표 는 종종 충 돌 했 습 니다 . 방 정 식 , 이 론 및 인 용 식 은 LaTeX 에 서 가 장 잘 관 리 되 고 배 치 됩 니다 . 코 드 는 파 이 썬 을 이 용 하 야 가 장 잘 설 명 됩 니다 . 그 리 고 웹 페 이 지 들 은 기 본 적 으 로 HTML 과 Javascript 로 구 성 됩 니다 . 더 나 아 가 , 우 리 는 코 드 가 물 리 적 인 책 , 다 운 로 드 가 능 한 PDF , 인 터 넷 웹 페 이 지 로 모 두 접 근 가 능 하 지 만 동 시 에 실 행 가 능 하 길 바 랍 니다 . 현 재 이 러 한 이 러 한 요 구 에 완 벽 하 게 부 합하 는 도 구 나 워 크 플 로 우 가 존 재 하 지 않 으 므 로 직 접 만 들 어야 했 습 니다 . 우 리 는 우 리 의 접 근 방 식 을 부 록 에 상 세 하 게 기 록 하 였 습 니다 . 우 리 는 이 를 위 해 , 소 스 를 공 유 하 고 편 집 을 허 용 하 기 위 해 Github 를 , 다 양 한 코 드 , 방 적 식 과 문 장 들 을 다 루 기 위 한 Jupyter 노 트 북 , 다 양 한 출 력 을 생산 하 기 위 한 렌 더 링 엔 진 으 로 Sphinx , 포 럼 을 위 해 Discourse 를 각각 설 정 하 였 습 니다 . 아 직 우 리 의 시스 템 이 완 벽 하 지 는 않 지 만 , 이 러 한 선 택 은 경 쟁 적 인 상 황 에 서 좋 은 절 충 안 을 제 공 합 니다 . 우 리 는 이 러 한 통 합 워 크 플 로 우 를 사 용 하 여 출 판 된 첫 번 째 책 이 될 거 라 고 생 각 합 니다 . 1 . 2 구 성 기 본 적 인 수 학 적 배 경 에 대 한 특 별 과 정 을 제 공 하 는 몇 가 지 예 비 노 트 북 을 제 외 하 고 , 각각 의 후 속 노 트 북 은 합 리 적 인 수 의 새 로 운 개 념 을 소 개 하 고 실 제 데 이 터 셋 을 사 용 하 여 하 나 의 독 립 적 인 작 업 예 제 를 제 공 합 니다 . 이 것 은 구 조적 인 과 제 를 제 시 한 다 . 일 부 모 델 은 논 리 적 으 로 단 일 노 트 북 으 로 그 룹 화 될 수 있 습 니다 . 그 리 고 어 떤 아 이 디 어 들 은 여 러 모 델 을 연 속 적 으 로 실 행하 여 가 장 잘 가 르 쳐 질 수 있 습 니다 . 반 면 에 1 작 업 예 제 , 1 노 트 북 의 정 책 을 준 수 하 면 큰 이 점 이 있 습 니다 . 이 렇 게 하 면 우 리 의 코 드 를 활 용 하 여 여 러 분 의 연 구 프 로 젝 트 를 가 능 한 한 쉽 게 시 작 할 수 있 습 니다 . 단 지 단 일 노 트 북 을 복 사 하 고 수 정 하 기 만 하 면 됩 니다 . 우 리 는 필 요 에 따 라 실 행 가 능 한 코 드 를 배 경 자 료 와 함 께 배 치 합 니다 . 일 반 적 으 로 , 우 리 는 종종 일 반 적 으 로 우 리 는 도 구 를 완 전 히 설 명 하 기 전 에 사 용 할 수 있 도 록 만 드 는 측 면 에 서 종종 오 류를 겪 을 것 입 니다 ( 나 중 에 배 경 을 설 명 하 여 후 속 조 치 를 취 합 니다 ) . 예 를 들 어 , 왜 유 용 한 지 또 는 왜 작 동 하 는 지 완 전 히 설 명 하 기 전 에 확 률 적 경 사 하 강 법 ( Stochastic Gradient Descent ) 을 사 용 할 수 있 습 니다 . 이 것 은 실 무 자 에 게 문 제 를 신 속 하 게 해 결 하 는 데 필 요 한 정 보 를 제 공 하 는 데 도 움 이 되 며 , 이 를 위 해 독 자 가 적 어 도 단 기 적 으 로 , 몇 가 지 결 정 에 있 어 서 는 우 리를 신 뢰 해 야 합 니다 . 전 반 적 으 로 MXNet 라 이 브 러 리 로 작 업 하 게 될 것 입 니다 . MXNet 라 이 브 러 리 는 연 구 하 기 에 충 분 히 유 연 하 면 서 도 완 성 품 만 들 기 에 있 어 서 도 충 분 히 빠 르 다는 드 문 특 성 을 가 지 고 있 습 니다 . 이 책 은 딥 1 . 2 . 구 성 3 러 닝 개 념 을 처 음 부 터 가 르 칠 것 입 니다 . 때때 로 , 우 리 는 Gluon 의 고 급 기 능 에 의 해 사 용 자 로 부 터 숨 겨 진 모 델 에 대 한 세 부 사 항 을 탐 구 하 고 자 합 니다 . 이 것 은 특 히 기 본 튜 토 리 얼에 서 나 옵 니다 . 여 기 서 는 주 어 진 층 에 서 일 어 나 는 모 든 것 을 이 해하 기 를 원 합 니다 . 이 경 우 , 우 리 는 일 반 적 으 로 두 가 지 버 전 의 예 제 를 제 시 합 니다 . 하 나 는 처 음 부 터 모 든 것 을 구 현 하 고 NDArray 와 자 동 미 분 ( automatic differentitation ) 에 만 의 존 하 고 다 른 하 나 는 Gluon 을 사 용 하 여 간결 하 게 수 행하 는 방법 을 보 여 줍 니 다 . 한 번 만 층 ( layer ) 이 어 떻 게 동 작 하 는 지 가 르 쳐 주 면 , 후 속 자 습 서 에 서 Gluon 버 전 을 사 용 할 수 있 습 니다 . 1 . 3 직 접 하 면 서 배 우 기 많 은 교 과 서 는 일 련 의 주제 들 을 각각 철 저 히 상 세 하 게 가 르 칩 니다 . 예 를 들 어 , Chris Bishop 의 훌 륭 한 교 과 서 Pattern Recognition and Machine Learning 는 각 주제 를 철 저 히 가 르 쳐 줍 니다 . 선 형 회 귀 분 석 의 장 에 들 어 가 려 면 작 지 않 은 양 의 작 업 이 필 요 합 니다 . 전 문 가 들 은 이 책 을 철 저 하 기 때 문 에 좋 아 하 지 만 , 초 보 자 에 게 는 이 점 이 소 개 텍 스 트 로 써 의 유 용 함 을 감 소 시 킵 니다 . 이 책 에 서 는 대 부분 의 개 념 을 딱 그 시 점 ( Just in time ) 에 가 르 쳐 드 리 겠 습 니다 . 즉 , 실 제 목 적 을 달 성 하 기 위 해 필 요 한 바 로 그 순 간 에 개 념 을 배 우 게 될 것 입 니다 . 선 형 대 수 학 이 나 확 률 과 같 은 기 본 적 인 사 항 을 가 르 치 기 위 해 처 음 에 는 약 간 의 시 간 이 걸 리 지 만 , 좀 더 색 다 른 확 률 분 포 에 대 해 걱 정 하 기 전 에 , 첫 번 째 모 델 을 훈 련 하 는 만 족 감 을 느 끼 기 를 바 랍 니다 . 1 . 4 감 사 의 글 우 리 는 영 어 와 중 국 초 안 모 두 에 대 한 수 백 명 의 기 여 자 에 게 빚 을 지 고 있 습 니다 . 그 들 은 콘 텐 츠 를 개 선 하 는 데 도 움 이 귀 중 한 피 드 백 을 제 공 했 습 니다 . 특 히 , 우 리 는 모 두 를 위 해 더 나 은 버 전 을 만 들 기 위 해 힘 써 준 영 어 초 안 의 모 든 기 여 자 에 게 감 사 드 립 니다 . 그 들 의 Github ID 와 이 름 ( 제 공 된 경 우 ) 은 : bowen0701 , ChaiBapChya ( Chaitanya Prakash Bapat ) , kirk86 , MLWhiz ( Rahul Agarwal ) , mstewart141 , muelleme ( Mike Müller ) , sfermigier , sundeepteki , vishaalkapoor , YaYaB . 더 해 서 , 아 마 존 웹 서 비 시 즈 에 감 사 드 리 며 , 특 히 Swami Sivasubramanian , Raju Gulabani , Charlie Bell , and Andrew Jassy 에 게 이 책 을 쓰 도 록 충 분 히 지 원 해 주 신 대 해 감 사 드 립 니다 . 사 용 가 능 한 시 간 , 자 원 , 동 료 와 의 토 론 , 지 속 적 인 격 려 덕 분 에 책 이 만 들 어 질 수 있 었 습 니다 . 4 1 . 서 문 2 이 책 의 사 용 방법 우 리 는 모 델 만 들 기 부 터 모 델 학 습 , 그 리 고 컴 퓨 터 비 전 및 자 연어 처 리 에 대 한 응 용 까 지 딥 러 닝 의 모 든 것 들 에 대 해 서 전 반 적 인 소 개 를 하 는 것 을 목 표 로 합 니다 . 알 고 리 즘 의 원 리만 을 설 명 하 는 것 이 아 니 라 , Apache MXNet 을 이 용 해 서 직 접 구 현 하 고 운영 하 는 것 까 지 보 여 줄 것 입 니다 . 이 책 의 각 절 은 Jupyter 노 트 북 , 텍 스 트 , 공 식 , 이 미 지 , 코 드 , 그 리 고 수 행 결과 로 구 성 되 어 있 습 니다 . 이 를 통 해 서 이 책 을 직 접 읽 는 것 뿐 만 아 니 라 , 직 접 수 행하 면 서 상 호 적 인 학 습 경 험 을 할 수 있을 것 입 니다 . 하 지 만 , 이 책 을 여 러 분 에 게 딥 러 닝 에 대 한 소 개 를 제 공 하 는 것 이 기 때 문 에 , 더 많 은 것 을 알 고 싶 다 면 , 연 구 커 뮤 니 티 들 에 서 제 공 하 는 툴 박 스 , 컴 파 일 러 , 예 제 들 , 튜 터 리 얼 , 그 리 고 소 스 코 드 를 활 용 하 는 것 을 추 천 합 니다 . 그 럼 여 행 을 시 작 해 봅 시 다 . 2 . 1 대 상 독 자 이 책 은 딥 러 닝 을 배 우 고 자 하 는 대 학 교 학 부 생 , 엔 지 니 어 , 연 구 원 그 리 고 특 히 딥 러 닝 을 실 제 문 제 에 적 용 하 고 자 하 는 사 람 들 을 대 상 으 로 하 고 있 습 니다 . 모 든 개 념 을 기 초 부 터 설 명 하 고 있 기 때 문 에 , 딥 러 닝 이 나 머 신 러 닝 에 대 한 배 경 지 식 이 없어 도 됩 니다 . 딥 러 닝 기 술 이 나 , 응 용 을 설 명 할 때 수 학 이 나 프 로 그 래 밍 을 사 용 하 지 만 , 기 본 적 인 선 형 대 수 , 미 적 분 , 확 률 그 리 고 기 초 Python 프 로 그 래 밍 과 같 은 기 초 적 인 것 들 만 알 고 있으 면 충 분 합 니다 . 부 록 에 서 는 이 책 에 서 다 루 는 대 부분 의 수 학 을 포 함하 고 5 있으 니 필 요 한 경 우 참 고 하 시 기 바 랍 니다 . 이 책 은 소 개 서 이 기 때 문 에 , 수 학 적 으 로 깊 이 들 어 가 는 것 보 다는 직 관 과 아 이 디 어에 더 비 중 을 두 고 있 습 니다 . 흥 미 를 갖 은 독 자 들 은 많 은 훌 륭 한 책 들 을 통 해 서 더 자 세 한 것 들 을 배 울 수 있 습 니다 . 예 를 들 면 , Bela Bollobas 의 Linear Analysis 는 선 형 대 수 와 함 수 분 석 을 아 주 자 세 하 게 다 루 고 있 고 , All of Statistics 은 통 계 에 대 한 아 주 훌 륭 한 가 이 드 입 니다 . 만 약 Python 을 사 용 해 보 지 않았 다 면 , Python tutorial 을 참 고 하 면 좋 습 니다 . 물 론 , 수 학 적 인 내 용 만 관 심 있 다 면 , 프 로 그 래 밍 부분 은 생 략 하 면 서 읽 어 도 됩 니다 . 물 론 반 대 의 경 우 도 그 렇 습 니다 . 2 . 2 내 용 및 구 성 이 책 은 크 게 세 파 트 로 구 성 되 어 있 습 니다 . • 첫 번 째 파 트 는 전제조 건과 기 본 사 항 들 을 다 룹 니다 . 1 장 에 서 는 딥 러 닝 에 대 한 소 개 와 이 책 에 대 한 활 용 방법 을 설 명 합 니다 . 딥 러 닝 맛 보 기 장 에 서 는 이 책 의 코 드 를 어 디 서 받 을 수 있 고 어 떻 게 실 행하 는 지 등 과 같 은 딥 러 닝 핸 즈 온 에 필 요 한 것 들 을 이 야 기 합 니다 . 만 약 시 간 이 없 거 나 딥 러 닝 의 가 장 기 본 적 인 개 념 과 기 법 들 만 을 배 우 고 자 한 다 면 , 이 파 트 만 읽 어 도 충 분 합 니다 . • 두 번 째 파 트 인 다 음 세 장은 최 신 딥 러 닝 기 법 들 을 다 룹 니다 . 딥 러 닝 계 산 은 딥 러 닝 연 산 의 다 양 한 주 요 요 소 들 을 설 명 하 면 서 , 더 복 잡 한 모 델 구 현 을 위 한 기 본 을 다 질 수 있 도 록 합 니다 . 다 음 장은 Convolutional Neural Networks 인 데 , 이 는 최 근 몇 년 동 안 컴 퓨 터 비 전 에 서 성 과 를 거 두 고 있 는 딥 러 닝 기 술 입 니다 . 그 리 고 순 서 가 있 는 데 이 터 를 처 리 하 는 데 일 반 적 으 로 사 용 되 는 Recurrent Neural Networks 를 다 룹 니다 . 여 러 분 은 이 두 번 째 파 트 를 읽으 면 서 최 근 딥 러 닝 기 술 을 이 해할 수 있을 것 입 니다 . • 마 지 막 파 트 는 확 장 성 , 효 율 성 과 응 용 을 다 룹 니다 . 딥 러 닝 모 델 을 학 습시 키 는 데 사 용 되 는 다 양 한 Optimization Algorithms 을 설 명 한 후 , 정 규 화 와 같 이 딥 러 닝 연 산 Performance 에 영 향 을 주 는 중 요 한 요 소 들 에 대 해 서 살 펴 봅 니다 . 9 장 과 10 장은 컴 퓨 터 비 전 과 자 연어 처 리 에 서 사 용 되 는 딥 러 닝 의 응 용 에 대 해 서 알아 봅 니다 . 이 파 트 는 여 러 분 의 관 심 에 따 라 서 선 택 적 으 로 읽 어 도 됩 니다 . 이 책 의 구 성 은 다 음 그 림 과 같 습 니다 . 화 살 표 는 선 행 되 어야 하 는 관 계 를 의 미 합 니다 . 만 약 빠 른 시 간 안 에 딥 러 닝 의 기 본 적 인 개 념 과 기 법 들 을 배 워 야 한 다 면 , 1 장 ~ 3 장 만 읽으 면 됩 니다 . 더 심 도 있 는 내 용 을 원 하 면 , 그 다 음 3 장 ( 4 장 ~ 6 장 ) 을 읽으 세 요 . 마 지 막 4 장은 독 자의 관 심 에 따 라 서 옵 션 입 니다 . 6 2 . 이 책 의 사 용 방법 2 . 3 코 드 이 책 은 모 든 절 에 동 작 하 는 코 드 를 포 함하 고 있 습 니다 . 코 드들 을 수 정 하 고 다 시 수 행해 서 결과 에 어 떤 영 향 을 미 치 는 지 도 확 인 할 수 있 습 니다 . 이 렇 게 한 이유 는 딥 러 닝 에 서 상 호 적 인 학 습 경 험 이 중 요 하 다는 것 을 알아 냈 기 때 문 입 니다 . 아 쉽 게 도 딥 러 닝 은 이 론 적 으 로 잘 이 해 되 지 않 고 있 습 니다 . 그 렇 기 때 문 에 , 많 은 논 의 들 은 코 드 를 수 행해 서 얻 은 경 험 에 많 이 의 존 하 고 있 습 니다 . 글 로 설 명 하 는 것 은 최 선 의 노 력 을 해 도 모 든 자 세 한 것 들 을 다 루 기 에 충 분 하 지 않 을 수 있 습 니다 . 이 론 적 인 진 전 이 더 만 들 어 지 면 그 때 는 이 런 상 황 이 좋 아 질 것 을 기 대 하 지 만 , 지 금 은 독 자 들 이 코 드 를 바 꾸 고 , 결과 를 관 찰 하 고 , 전 반 적 인 과 정 을 요 약 하 는 것 을 통 해 서 이 해 도 를 높 이 고 , 직 관 을 키 우 시 길 바 랍 니다 . 이 책 의 코 드 는 Apache MXNet 을 기 반 으 로 합 니다 . MXNet 은 딥 러 닝 을 위 한 오 픈 소 스 프 레 임 워 크 입 니다 . 이 는 AWS ( Amazon Cloud Services ) 가 선 호 하 는 프 레 임 워 크 이 며 , 많 은 대 학 과 회 사 에 서 사 용 되 고 있 습 니다 . 이 책 의 모 든 코 드 는 MXNet 1 . 2 . 0 을 이 용 해 서 테 스 트 되 었 으 나 , 딥 러 닝 의 빠 른 발 전 으 로 어 떤 코 드 는 , 이 후 MXNet 버 전 에 서 는 이 책 의 인 쇄 버 전 의 코 드 가 잘 작 동 하 지 않 을 수 있 습 니다 . 하 지 만 , 온 라 인 버 전 은 계 속 최 신 을 유 지 할 것 입 니다 . 만 약 그 런 경 우 를 만 난 다 면 , “Installation and Running” 를 참 고 해 서 코 드 와 실 행 환 경 을 업 데 이 트 하 세 요 . 그 리 고 , 불 필 요 한 반복 을 피하 기 위 해 서 , 이 책 에 서 자 주 import 되 거 나 자 주 참 조 되 는 함 수 , 클 래 스 등 은 d2l 패 키 지 에 넣 었 습 니다 . d2l 패 키 지 2 . 3 . 코 드 7 의 버 전 은 1 . 0 . 0 입 니다 . 포 함 된 함 수 와 클 래 스 에 대 한 자 세 한 내 용 은 “d2l package index” 에 있 습 니다 . 이 책 은 MXNet 소 개 서 로 활 용 될 수 도 있 습 니다 . 코 드 를 사 용 한 주 요 목 적 은 텍 스 트 , 이 미 지 , 공 식 과 더 불 어 딥 러 닝 알 고 리 즘 을 배 우 는 또 다 른 방법 을 제 공 하 는 데 있 습 니다 . 이 책 은 각 모 델 과 알 고 리 즘 의 실 제 데 이 터 에 대 한 실 제 영 향 을 이 해하 기 위 해 서 인 터 엑 티 브 한 환 경 을 제 공 합 니다 . 딥 러 닝 알 고 리 즘 의 구 현 에 대 한 자 세 한 내 용 을 설 명 하 기 위 해 서 MXNet 모 듈 의 기 본 적 인 기 능 - ndarray , autograd , gluon 등 - 만 을 사 용 합 니다 . 여 러 분 의 연 구 나 업 무 에 서 다 른 딥 러 닝 프 레 임 워 크 를 사 용 하 는 경 우 에 도 , 이 코 드 는 딥 러 닝 알 고 리 즘 에 대 한 이 해 를 높 이 는 데 도 움 이 될 것 이 라 고 기 대 합 니다 . 2 . 4 토 론 ( Forum ) 이 책 의 내 용 에 대 한 논 의 포 럼 은 discuss . mxnet . io 에 있 습 니다 . 이 책 의 어 느 부분 이 던 지 질 문 이 있 을 때 는 , 각 절 끝 에 있 는 QR 코 드 를 스 캔 해 서 논 의 에 참 여 할 수 있 습 니다 . 이 책 의 저 자 와 MXNet 개 발 자 들 이 포 럼 에 자 주 방 문 하 고 참 여 하 고 있 습 니다 . 2 . 5 문 제 1 . 이 책 의 논 의 포 럼 discuss . mxnet . io 에 계 정 을 생 성 하 세 요 . 2 . 여 러 분 의 컴 퓨 터 에 Python 을 설 치 하 세 요 . 2 . 6 Scan the QR Code to Discuss 8 2 . 이 책 의 사 용 방법 3 딥 러 닝 소 개 2016 년 , 네 임 드 데 이 터 과 학 자인 죠 엘 그 루 스 ( Joel Grus ) 는 한 유 명 인 터 넷 기 업에 서 면 접 을 보 았 습 니다 . 보 통 그 러 하 듯 이 인 터 뷰 담당 자 는 그 의 프 로 그 래 밍 기 술 을 평 가 하 는 문 제 를 냈 습 니다 . 간 단 한 어 린 이 게 임인 FizzzBuzz 를 구 현 하 는 것 이 과 제 였 습 니다 . 그 안에 서 플 레 이 어 는 1 부 터 카 운 트 하 면 서 3 으 로 나 눌 수 있 는 숫 자 는 ‘ﬁzz’ 로 , 5 로 나 눌 수 있 는 숫 자 는 ‘buzz’ 로 바 꿉 니다 . 15 로 나 눌 수 있 는 숫 자 는 ‘FizzBuzz’ 가 됩 니다 . 즉 , 플 레 이 어 는 시 퀀 스 를 생 성 합 니다 . 1 2 fizz 4 buzz fizz 7 8 fizz buzz 11 . . . 전 혀 예 상 하 지 못 한 일이 벌 어 졌 습 니다 . 거 의 몇 줄 의 Python 코 드 로 알 고 리 즘 을 구 현 해 서 문 제 를 해 결 하 는 대 신 , 그 는 데 이 터 를 활 용 한 프 로 그 램 으 로 문 제 를 풀 기 로 했 습 니다 . 그 는 ( 3 , ﬁzz ) , ( 5 , buzz ) , ( 7 , 7 ) , ( 2 , 2 ) , ( 15 , ﬁzzbuzz ) 의 쌍 을 활 용 하 여 분 류 기 를 학 습시 켰 습 니다 . 그 가 작은 뉴 럴 네 트 워 크 ( neural network ) 를 만 들 고 , 그 것 을 이 데 이 터 를 활 용 하 여 학 습시 켰 고 그 결과 꽤 높 은 정 확 도 를 달 성 하 였 습 니 다 ( 면 접 관 이 좋 은 점 수 를 주 지 않아 서 채 용 되 지 는 못 했 습 니다 ) . 이 인 터 뷰 와 같 은 상 황 은 프 로 그 램 설 계가 데 이 터 에 의 한 학 습 으 로 보 완 되 거 나 대 체 되 는 컴 퓨 터 과 학 의 획 기 적 인 순 간 입 니다 . 예 로 든 상 황 은 면 접 이 라 서 가 아 니 라 어 떠 한 목 표 를 손 쉽 게 달 성 할 수 있 게 해 준 다는 점 에 서 중 요 성 을 가 집 니다 . 일 반 적 으 로 는 위 에 서 설 명 한 오 버 스 러 운 방 식 으 로 FizzBuzz 를 해 결 하 지 는 않 겠 지 만 , 얼 굴 을 인 식 하 거 나 , 사 람 의 목 소 리 또 는 텍 스 트 로 감 정 을 분 류 하 거 나 , 음 성 9 을 인 식 할 때 는 완 전 히 다 른 이 야 기 입 니다 . 좋 은 알 고 리 즘 , 많 은 연 산 장 치 및 데 이 터 , 그 리 고 좋 은 소 프 트 웨 어 도 구 들 로 인 해 이 제 는 대 부분 의 소 프 트 웨 어 엔 지 니 어 가 불 과 10 년 전 에 는 최 고 의 과 학 자 들 에 게 도 너 무 도 전적 이 라 고 여 겨 졌 던 문 제 를 해 결 하 는 정 교 한 모 델 을 만 들 수 있 게 되 었 습 니다 . 이 책 은 머 신 러 닝 을 구 현 하 는 여 정 에 들 어 선 엔 지 니 어 를 돕 는 것 을 목 표 로 합 니다 . 우 리 는 수 학 , 코 드 , 예 제 를 쉽 게 사 용 할 수 있 는 패 키 지 로 결 합하 여 머 신 러 닝 을 실 용 적 으 로 만 드 는 것 을 목 표 로 합 니다 . 온 라 인으 로 제 공 되 는 Jupyter 노 트 북 예 제 들 은 노 트 북 이 나 클 라 우 드 서 버 에 서 실 행할 수 있 습 니다 . 우 리 는 이 를 통 해 서 새 로 운 세 대 의 프 로 그 래 머 , 기 업 가 , 통 계 학 자 , 생 물 학 자 및 고 급 머 신 러 닝 알 고 리 즘 을 배 포 하 는 데 관 심 이 있 는 모 든 사 람 들 이 문 제 를 해 결 할 수 있 기 를 바 랍 니다 . 3 . 1 데 이 터 를 활 용 하 는 프 로 그 래 밍 코 드 를 이 용 하 는 프 로 그 래 밍 과 데 이 터 를 활 용 하 는 프 로 그 래 밍 의 차 이 점 을 좀 더 자 세 히 살 펴 보 겠 습 니다 . 이 둘 은 보 이 는 것 보 다 더 심 오 하 기 때 문 입 니다 . 대 부분 의 전 통 적 인 프 로 그 램 은 머 신 러 닝 을 필 요 로 하 지 않 습 니다 . 예 를 들 어 전 자 레 인 지 용 사 용 자 인 터 페 이 스 를 작 성 하 려 는 경 우 약 간 의 노 력 으 로 몇 가 지 버 튼 을 설 계 할 수 있 습 니다 . 다 양 한 조 건 에 서 전 자 레 인 지 의 동 작을 정 확 하 게 설 명 하 는 몇 가 지 논 리 와 규 칙 을 추 가 하 면 완 료 됩 니다 . 마 찬 가 지 로 사 회 보 장 번 호 의 유 효 성 을 확 인 하 는 프 로 그 램 은 여 러 규 칙 이 적 용 되 는 지 여 부 를 테 스 트 하 면 됩 니다 . 예 를 들 어 , 이 러 한 숫 자 는 9 자 리 숫 자 를 포 함해 야 하 며 000 으 로 시 작 하 지 않아야 한 다 와 같 은 규 칙 입 니다 . 위의 두 가 지 예 에 서 프 로 그 램 의 논 리를 이 해하 기 위 해 현 실 세 계 에 서 데 이 터 를 수 집 할 필 요 가 없 으 며 , 그 데 이 터 의 특 징 을 추출 할 필 요 가 없 다는 점 에 주 목 할 가 치 가 있 습 니다 . 많 은 시 간 이 있 다 면 , 우 리 의 상 식 과 알 고 리 즘 기 술 은 우 리 가 작 업 을 완 료 하 기 에 충 분 합 니다 . 우 리 가 전 에 관 찰 한 바 와 같 이 , 심 지 어 최 고 의 프 로 그 래 머 의 능 력 을 넘 는 많 은 예 가 있 지 만 , 많 은 아 이 들 , 심 지 어 많 은 동 물 들 이 쉽 게 그 들 을 해 결 할 수 있 습 니다 . 이 미 지 에 고 양 이 가 포 함 되 어 있 는 지 여 부 를 감 지 하 는 문 제 를 고 려 해 보 겠 습 니다 . 어 디 서 부 터 시 작 해 야 할 까 요 ? 이 문 제 를 더 욱 단 순 화 해 보 겠 습 니다 . 모 든 이 미 지 가 동 일 한 크 기 ( 예 , 400x400 픽 셀 ) 이 라 고 가 정 하 고 , 각 픽 셀 이 빨 강 , 녹 색 및 파 랑 값 으 로 구 성 된 경 우 이 미 지 는 480 , 000 개 의 숫 자 로 표 시 됩 니다 . 우 리 의 고 양 이 탐 지 기 가 관 련 된 정 보 가 어 디 에 있 는 지 결 정 하 는 것 은 불 가 능 합 니다 . 그 것 은 모 든 값 의 평 균 일 까 요 ? 네 모 서 리 의 값 일 까 요 ? 아 니 면 이 미 지 의 특 정 지 점 일 까 요 ? 실 제 로 이 미 지 의 내 용 을 해 석 하 려 면 가 장자 리 , 질 감 , 모 양 , 눈 , 코 와 같 은 수 천 개 의 값 을 결 합 할 때 만 나 타 나 는 특 징 을 찾 아야 합 니다 . 그 래 야 만 이 미 지 에 고 양 이 가 포 함 되 어 있 는 지 여 부 를 판 단 할 수 있 습 니다 . 다 른 전 략 은 최 종 필 요 성 에 기 반 한 솔 루 션 을 찾 는 것 입 니다 . 즉 , 이 미 지 예 제 및 원 하 는 응 답 ( cat , cat 없 음 ) 을 출 발 점 으 로 사 용 하 여 데 이 터 로 프 로 그 래 밍 하 는 것 입 니다 . 우 리 는 고 양 이의 실 제 이 미 지 ( 인 터 넷 에 서 인 기 있 는 주제 ) 들 과 다 른 것 들 을 수 집 할 수 있 습 니다 . 이 제 우 리 의 목 표 는 이 미 지 에 10 3 . 딥 러 닝 소 개 고 양 이 가 포 함 되 어 있 는 지 여 부 를 배 울 수 있 는 함 수 를 찾 는 것 입 니다 . 일 반 적 으 로 함 수 의 형 태 ( 예 , 다 항 식 ) 는 엔 지 니 어에 의 해 선 택 되 며 그 함 수 의 파 라 미 터 들 은 데 이 터 에 서 학 습 됩 니다 . 일 반 적 으 로 머 신 러 닝 은 고 양 이 인 식 과 같 은 문 제 를 해 결 하 는 데 사 용 할 수 있 는 다 양 한 종 류 의 함 수 를 다 룹 니다 . 딥 러 닝 은 특 히 신 경 망 에 서 영 감 을 얻 은 특 정 함 수 의 클 래 스 를 사 용 해 서 , 이 것 을 특 별 한 방법 으 로 학 습 ( 함 수 의 파 라 미 터 를 계 산 하 는 것 ) 시 키 는 방법 입 니다 . 최 근 에 는 빅 데 이 터 와 강 력 한 하 드 웨 어 덕 분 에 이 미 지 , 텍 스 트 , 오 디 오 신 호 등 과 같 은 복 잡 한 고 차 원 데 이 터 를 처 리 하 는 데 있 어 딥 러 닝 이 사 실 상 의 표 준 으 로 ( de facto choice ) 자 리 잡 았 습 니다 . 3 . 2 기 원 딥 러 닝 은 최 근 의 발 명 품 이 지 만 , 인 간 은 데 이 터 를 분 석 하 고 미 래 의 결과 를 예 측 하 려 는 욕 구 를 수 세 기 동 안 가 지 고 있 어 왔 습 니다 . 사 실 , 자 연 과 학 의 대 부분 은 이 것 에 뿌 리를 두 고 있 습 니다 . 예 를 들 어 , 베 르 누 이 분 포 는 야 곱 베 르 누 이 ( 1655 - 1705 ) 의 이 름 을 따 서 명명 되 었 으 며 , 가 우 시 안 분 포 는 칼 프 리 드 리 히 가 우 스 ( 1777 - 1855 ) 에 의 해 발 견 되 었 습 니다 . 예 를 들 어 , 그 는 최 소 평 균 제 곱 알 고 리 즘 을 발 명 했 는 데 , 이 것 은 보 험 계 산 부 터 의 료 진 단 까 지 다 양 한 분 야에 서 오 늘 날 까 지 도 계 속 사 용 되 고 있 습 니다 . 이 러 한 기 술 들 은 자 연 과 학 에 서 실 험 적 인 접 근 법 을 불 러 일으 켰 습 니다 . 예 를 들 어 저 항 기 의 전 류 및 전 압에 관 한 옴 의 법 칙 은 선 형 모 델 로 완 벽 하 게 설 명 됩 니다 . 중 세 시 대 에 도 수 학 자 들 은 예 측 에 대 한 예 리 한 직 관 을 가 지 고 있 었 습 니다 . 예 를 들 어 , 야 곱 쾨 벨 ( 1460 - 1533 ) 의 기 하학 책 에 서 는 발 의 평 균 길 이 를 얻 기 위 해 성 인 남 성 발 16 개 의 평 균 을 사 용 했 습 니다 . 3 . 2 . 기 원 11 그 림 1 . 1 은 이 평 균 이 어 떻 게 얻어 졌 는 지 를 보 여 줍 니다 . 16 명 의 성 인 남 성 은 교 회 를 떠 날 때 한 줄 로 정 렬 하 도 록 요 구 받 았 습 니다 . 그 런 다 음 총 길 이 를 16 으 로 나 누 어 현 재 1 피 트 금 액에 대 한 추 정 치 를 얻 습 니다 . 이 ’ 알 고 리 즘 ’ 은 나 중 에 잘 못 된 모 양 의 발 을 다 루 기 위 해 개 선 되 었 습 니다 - 각각 가 장 짧 고 12 3 . 딥 러 닝 소 개 긴 발 을 가 진 2 명 의 남 성 은 제 외 하 고 나 머 지 발 들 에 대 해 서 만 평 균 값 을 계 산 합 니다 . 이 것 은 절 사 평 균 추 정 치 의 초 기 예 중 하 나 입 니다 . 통 계 는 실 제 로 데 이 터 의 수 집 및 가 용 성 으 로 시 작 되 었 습 니다 . 거 장 중 한 명 인 로 널 드 피 셔 ( 1890 - 1962 ) 는 이 론 과 유 전 학 의 응 용 에 크 게 기 여 했 습 니다 . 그 의 알 고 리 즘 들 ( 예 , 선 형 판 별 분 석 ) 과 수식 들 ( 예 , Fisher 정 보 매 트 릭 스 ) 은 오 늘 날 에 도 여 전 히 자 주 사 용 되 고 있 습 니다 ( 1936 년 에 발 표 한 난 초 ( Iris ) 데 이 터 셋 도 머 신 러 닝 알 고 리 즘 을 설 명 하 는 데 사 용 되 기 도 합 니다 ) . 머 신 러 닝 에 대 한 두 번 째 영 향 은 정 보 이 론 ( 클 로 드 섀 넌 , 1916 - 2001 ) 과 앨 런 튜 링 ( 1912 - 1954 ) 의 계 산 이 론 에 서 나 왔 습 니다 . 튜 링 은 그 의 유 명 한 논 문 , 기 계 및 지 능 컴 퓨 팅 , Computing machinery and intelligence ( Mind , 1950 년 10 월 ) 에 서 , 그 는 “ 기 계가 생 각 할 수 있 습 니 까 ? ” 라 는 질 문 을 제 기 했 습 니다 . 그 의 튜 링 테 스 트 로 설 명 된 것 처 럼 , 인 간 평 가 자 가 텍 스 트 상 호 작 용 을 통 해 기 계 와 인 간 의 응 답 을 구 별 하 기 어 려 운 경 우 ‘’ 기 계 는 지 능 적 이 다 ” 라 고 간 주 될 수 있 습 니다 . 오 늘 날 까 지 지 능 형 기 계 의 개 발 은 신 속 하 고 지 속 적 으 로 변 화 하 고 있 습 니다 . 또 다 른 영 향 은 신 경 과 학 및 심 리 학 에 서 발 견 될 수 있 습 니다 . 결 국 , 인 간 은 분 명 히 지 적 인 행 동 을 합 니다 . 이 러 한 행 동 및 이 에 필 요 한 통 찰 력 을 설 명 하 고 , 아 마 도 리 버 스 엔 지 니 어 링 할 수 있 는 지 여 부 를 묻 는 것 은 합 리 적 입 니다 . 이 를 달 성 하 기 위 한 가 장 오 래 된 알 고 리 즘 중 하 나 는 도 널 드 헤 브 ( 1904 - 1985 ) 에 의 해 공 식 화 되 었 습 니다 . 그 의 획 기 적 인 책 행 동 의 조 직 , The Organization of Behavior ( John Wiley & Sons , 1949 ) 에 서 , 그 는 뉴 런 이 긍 정적 인 강 화 를 통 해 학 습 할 것 이 라 고 가 정 했 습 니다 . 이 것 은 Hebbian 학 습 규 칙 으 로 알 려 지 게 되 었 습 니다 . 그 것 은 Rosenblatt 의 퍼 셉 트 론 학 습 알 고 리 즘 의 원 형 이 며 오 늘 날 딥 러 닝 을 뒷 받 침 하 는 많 은 stochastic gradient descent 알 고 리 즘 의 기 초 를 마 련 했 습 니다 : 뉴 럴 네 트 워 크 의 좋 은 가 중 치 를 얻 기 위 해 바 람 직 한 행 동 은 강 화 하 고 바 람 직 하 지 않 은 행 동 을 감 소 시 킵 니다 . 생 물 학 적 영 감 으 로 부 터 신 경 망 ( Neural Network ) 이 라 는 명 칭 이 탄 생 하 였 습 니다 . 알 렉 산 더 베 인 ( 1873 ) 과 제 임 스 셰 링 턴 ( 1890 ) 이 신 경 망 모 델 을 제 안 한 이 래 한 세 기 이 상 연 구 자 들 은 상 호 작 용 하 는 뉴 런 들 의 네 트 워 크 와 유 사 한 계 산 회 로 를 구 현 하 려 고 시 도 해 왔 습 니다 . 시 간 이 지 남 에 따 라 생 물 학 과 의 연 관 성 은 느 슨 해 졌 지 만 신 경 망 이 라 는 이 름 은 그 대 로 사 용 하 고 있 습 니다 . 오 늘 날 대 부분 의 신 경 망 에 서 찾 을 수 있 는 몇 가 지 주 요 핵 심 원 칙 은 다 음 과 같 습 니다 : • ’ 레 이 어 ’ 라 고 불 리 우 는 선 형 및 비 선 형 처 리 유 닛 들 의 교 차 구 조 • 체 인 규 칙 ( 일 명 역 전 파 ) 을 사 용 하 여 한 번 에 전 체 네 트 워 크 의 매 개 변 수 를 조정 초 기 급 속 한 진 행 이 후 , 뉴 럴 네 트 워 크 의 연 구 는 1995 년 경 부 터 2005 년 까 지 쇠 퇴 했 습 니다 . 여 러 가 지 이유 들 이 있 습 니다 . 우 선 네 트 워 크 학 습 은 매 우 많 은 계 산 량 을 필 요 로 합 니다 . 지 난 세 기 말 경 에 이 르 러 메모 리 는 충 분 해 졌 지 만 계 산 능 력 이 부 족 했 습 니다 . 두 번 째 이유 는 데 이 터 셋 이 상 대 적 으 로 작 았 다는 것 입 니다 . 실 제 로 1932 년 에 나 온 피 셔 ( Fisher ) 의 ’ 난 초 ( Iris ) 데 이 터 셋 ’ 은 각각 50 장의 세 종 류 의 난 초 사 진 으 로 구 성 되 어 있 는 데 알 고 리 즘 의 효 능 을 테 스 트 하 는 데 널 리 사 용 되 는 도 구 였 습 니다 . 지 금 은 3 . 2 . 기 원 13 학 습 예 제 에 흔히 쓰 이 는 60 , 000 개 의 손 으 로 쓴 숫 자 들 로 구 성 된 MNIST 조 차 당 시 에 는 너 무 거 대 한 데 이 터 로 취 급 되 었 습 니다 . 데 이 터 및 계 산 능 력 이 부 족 한 경 우 , 커 널 방법 , 의 사 결 정 트 리 및 그 래 픽 모 델 과 같 은 강 력 한 통 계 도 구 쪽 이 우 수 한 성 능 을 보 여 줍 니다 . 신 경 망 과 는 달 리 이 것 들 은 훈 련 하 는 데 몇 주 씩 걸 리 지 않 으 면 서 도 강 력 한 이 론 적 보 장으 로 예 측 가 능 한 결과 를 제 공 합 니다 . 3 . 3 딥 러 닝 으 로 의 길 시 간 이 흐 르 면 서 월 드 와 이 드 웹 이 등 장 하 고 수 억 명 의 온 라 인 사 용 자 에 게 서 비 스 를 제 공 하 는 회 사 가 출 현 하 였 으 며 저 렴 한 고 품 질 의 센서 , 데 이 터 저 장 비 용 감 소 ( Kryder 의 법 칙 ) , 그 리 고 특 히 원 래 컴 퓨 터 게 임을 위 해 설 계 된 GPU 의 가격 하 락 ( 무 어 의 법 칙 ) 이 진 행 됨 에 따 라 많 은 것 들 이 바 뀌 게 되 었 습 니다 . 갑 자 기 계 산 이 불 가 능 한 것 처 럼 보 이 는 알 고 리 즘 과 모 델 이 의 미 를 지 니 게 된 것 입 니다 ( 반 대 의 경 우 도 마 찬 가 지 입 니다 ) . 이 것 은 아 래 표 에 가 장 잘 설 명 되 어 있 습 니다 . 연 대 데 이 터 셋 메모 리 초 당 부 동 소 수 점 연 산 수 1970 100 ( Iris ) 1 KB 100 KF ( Intel 8080 ) 1980 1 K ( House prices in Boston ) 100 KB 1 MF ( Intel 80186 ) 1990 10 K ( optical character recognition ) 10 MB 10 MF ( Intel 80486 ) 2000 10 M ( web pages ) 100 MB 1 GF ( Intel Core ) 2010 10 G ( advertising ) 1 GB 1 TF ( NVIDIA C2050 ) 2020 1 T ( social network ) 100 GB 1 PF ( NVIDIA DGX - 2 ) RAM 이 데 이 터 의 증 가 와 보 조 를 맞 추 지 않 은 것 은 매 우 분 명 합 니다 . 동 시 에 계 산 능 력 의 향 상 은 사 용 가 능 한 데 이 터 의 증 가 를 앞 서 고 있 습 니다 . 즉 , 통 계 모 델 은 메모 리 효 율 성 이 향 상 되 어야 하 고 ( 일 반 적 으 로 비 선 형 을 추 가 하 여 달 성 됨 ) 컴 퓨 팅 예 산 증 가 로 인 해 이 러 한 매 개 변 수 를 최 적 화 하 는 데 더 많 은 시 간 을 할 애 해햐 야 했 습 니다 . 결 국 머 신 러 닝 및 통 계 에 적절 한 방법 은 선 형 모 델 및 커 널 방 법 에 서 딥 네 트 워 크 로 이 동 했 습 니다 . 이 것 은 다 층 퍼 셉 트 론 ( Multilayer Perceptron ) ( 예 , 맥 컬 록 & 피 츠 , 1943 ) , 컨 볼 루 션 뉴 럴 네 트 워 크 ( Convolutional Neural Network ) ( Le Cun , 1992 ) , Long Short Tem Memory ( Hochreiter & Schmidhuber , 1997 ) , Q - 러 닝 ( 왓 킨 스 , 1989 ) 과 같 은 딥 러 닝 의 많 은 주 류 이 론 들 이 상 당 시 간 동 안 휴 면 기 에 있 다 가 최 근 10 년 간 재 발 견 된 이유 중 에 하 나 입 니다 . 통 계 모 델 , 응 용 프 로 그 램 및 알 고 리 즘 의 최 근 발 전 은 때때 로 캄 브 리 아 폭 발 ( Cambrian Dexplosion ) 에 비 유 되 고 있 습 니다 : 종 의 진 화 가 급 속 히 진 행 되 는 순 간 입 니다 . 실 제 로 , 현 시 점 에 서 가 장 좋 은 성 과 들 ( state of art ) 은 수십 년 동 안 만 들 어 져 온 오 래 된 알 고 리 즘 이 적 용 된 결과가 아 닙니다 . 아 래 목 록 은 연 구 자 들 이 지 난 10 년 간 엄 청 난 진 전 을 달 성 하 는 데 도 움 이 된 아 이 디 어 의 극 히 일 부분 입 니다 . 14 3 . 딥 러 닝 소 개 • 드 롭 아 웃 ( Drop out ) [ 3 ] 과 같 은 새 로 운 용 량 제 어 방법 , 즉 학 습 데 이 터 의 큰 부분 을 암 기 하 는 위 험 없 이 비 교 적 큰 네 트 워 크 의 학 습 이 가 능 합 니다 . 이 것 은 학 습 목 적 을 위 해 무 작위 변 수 로 가 중 치 를 대 체 하 여 네 트 워 크 전 체 에 노 이 즈 주 입 [ 4 ] 을 적 용 하 여 달 성 되 었 습 니다 . • 어 텐 션 메 커 니 즘 ( Attention Mechanism ) 은 1 세 기 이 상 통 계 를 괴 롭 히 던 두 번 째 문 제 를 해 결 했 습 니다 : 수 를 늘 리 지 않 고 시스 템 의 메모 리 와 복 잡 성 을 증 가 시 키 는 방법 , 학 습 가 능 한 매 개 변 수 . [ 5 ] 는 학 습 가 능 한 포 인 터 구 조 로 만 볼 수 있 는 것 을 사 용 하 여 우 아 한 해 결 책 을 찾 았 습 니다 . 즉 , 전 체 문 장을 기 억 할 필 요 없 이 ( 예 : 고 정 차 원 표 현 의 기 계 번 역 의 경 우 ) 저 장 해 야 하 는 모 든 것 은 번 역 프 로 세 스 의 중 간 상 태 에 대 한 포 인 터 였 습 니다 . 이 것 은 문 장을 생 성 하 기 전 에 모 델 이 더 이 상 전 체 문 장을 기 억 할 필 요 가 없 기 때 문 에 긴 문 장의 정 확 도 를 크 게 높 일 수 있 었 습 니다 . • 다단 계 디 자인 ( 예 : Memory Network [ 6 ] 및 Neural programmer - interpreters [ 7 ] ) 를 통 해 통 계 모 델 러 가 추 론 에 대 한 반복 적 인 접 근 법 을 이 용 해 묘 사 할 수 있 게 하 였 습 니다 . 이 러 한 기 술 은 딥 네 트 워 크 의 내 부 상 태 가 반복 적 으 로 변 경가 능 하 도 록 하 였 습 니다 . 그 에 따 라 추 론 체 인의 후 속 단 계 를 진 행하 고 , 이 는 프 로 세서 가 계 산 을 위 해 메모 리를 수 정 할 수 있 는 것과 유 사 합 니다 . • 또 다 른 중 요 한 발 전 은 적 대 적 생 성 신 경 망 ( Generative Adversarial Netoworks ) 의 발 명 입 니다 [ 8 ] . 밀 도 추 정 및 생 성 모 델 에 대 한 전 통 적 인 통 계 방법 은 , 적절 한 확 률 분 포 와 그 들 로 부 터 샘 플 링 에 대 한 ( 종종 근 사 ) 알 고 리 즘 을 찾 는 데 초 점 을 맞 추 었 습 니다 . 결과 적 으 로 , 이 러 한 알 고 리 즘 은 통 계 모 델 에 내 재 된 유 연 성 부 족 으 로 인 해 크 게 제 한 되 었 습 니다 . GAN 의 중 요 한 혁 신 은 샘 플 러 를 다 른 파 라 미 터 들 을 가 진 임의의 알 고 리 즘 으 로 대 체 한 것 입 니다 . 그 런 다 음 Discriminator ( 사 실 상 두 샘 플 테 스 트 ) 에 의 해 가 짜 와 실 제 데 이 터 를 구 분 할 수 없 도 록 조정 됩 니다 . 임의의 알 고 리 즘 을 이 용 하 여 데 이 터 를 생 성 하 는 기 술 을 통 해 , 다 양 한 분 야 의 밀 도 추 정 이 가 능 해 졌 습 니다 . 달 리 는 얼 룩 말 [ 9 ] 과 가 짜 유 명 인 얼 굴 [ 10 ] 의 예 는 이 러 한 발 전 에 대 한 증 명 입 니다 . • 대 부분 의 경 우 단 일 GPU 는 학 습 에 필 요 한 많 은 양 의 데 이 터 를 처 리 하 기 에 는 부 족 합 니다 . 지 난 10 년 간 병 렬 분 산 학 습 알 고 리 즘 을 개 발 하 는 능 력 은 크 게 향 상 되 었 습 니다 . 확 장 가 능 한 알 고 리 즘 을 설 계 할 때 , 가 장 큰 과 제 중 하 나 는 딥 러 닝 최 적 화 , 즉 확 률 적 그 래 디 언 트 디 센 트 의 핵 심 은 처 리 할 데 이 터 에 비 해 상 대 적 으 로 작은 미 니 배 치 ( minibatches ) 에 의 존 한 다는 점 입 니다 . 이 러 한 미 니 배 치 ( minibatch ) 그 래 서 , 하 나 의 작은 batch 때 문 에 GPU 를 최 대 한 활 용 하 지 못 합 니다 . 따 라 서 1024 개 의 GPU 로 , batch 당 32 개 의 이 미 지 를 처 리 하 는 미 니 배 치 학 습 은 , 한 번 의 병 합 된 32K 개 의 이 미 지 처 리 와 같 습 니다 . 최 근 에 는 , 처 음 Li [ 11 ] 에 이 어 You [ 12 ] 와 Jia [ 13 ] 가 최 대 64K 개 의 데 이 터 를 ResNet50 으 로 ImageNet 을 학 습 시 간 을 7 분 미 만 으 로 단 축 시 켰 습 니다 . 초 기 에 이 학 습 시 간 측 정 은 일 단 위 이 었 습 니다 . • 계 산 을 병 렬 화 하 는 능 력 은 , 시 뮬 레 이 션 이 가 능 한 상 황 에 서 강 화 학 습 ( Reinforcement learning ) 분 야에 결 정적 인 기 여 를 하 였 습 니다 . 이 것 은 바 둑 , 아 타 리 게 임 , 스 타크 래 프 트 , 그 리 고 물 리 시 뮬 레 이 션 ( 예 를 들 면 MuJoCo 의 사 용 ) 분 야에 서 컴 퓨 터 가 인 간 능 력 에 다 가 서 거 나 넘 어 설 수 있 도 록 하 는 데 필 요 한 중 요 한 발 전 을 이 끌 어 내 었 습 니다 . 실 예 로 , Silver [ 18 ] 는 AlphaGo 가 어 떻 3 . 3 . 딥 러 닝 으 로 의 길 15 게 이 것 을 달 성 했 는 지 설 명 하 고 있 습 니다 . 요 컨 대 , 많 은 양 의 상 태 , 행 동 , 보 상 의 세 가 지 조 합 데 이 터 들 을 사 용 할 수 있 다 면 , 강 화 학 습 은 각각 의 데 이 터 들 을 어 떻 게 연 관 시 킬 수 있을 지 , 매 우 많 은 양 의 데 이 터 로 시 도 해 볼 수 있 게 합 니다 . 시 뮬 레 이 션 은 그 런 방법 을 제 공 합 니다 . • 딥 러 닝 프 레 임 워 크 는 아 이 디 어 를 널 리 퍼 트 리 는 데 중 요 한 역 할 을 했 습 니다 . 손 쉬 운 모 델 링 을 위 한 프 레 임 워 크 의 첫 번 째 세 대 는 Caffe , Torch , Theano 입 니다 . 많 은 영 향 력 있 는 논 문 들 이 이 도 구 를 이 용 해 작 성 되 었 습 니다 . 지 금 에 이 르 러 이 들 은 TensorFlowTensorFlow 로 대 체 되 었 습 니 다 . 고 수 준 API 인 Keras , CNTK , Caffe 2 및 Apache MxNet 도 이 를 사 용 합 니다 . 3 세 대 툴 , 즉 딥 러 닝 을 위 한 명 령 형 툴 은 틀 림 없 이 , 모 델 을 기 술 하 기 위 해 파 이 썬 NumPy 와 유 사 한 구 문 을 사 용 하 는 Chainer 에 의 해 주 도 될 것 입 니다 . 이 아 이 디 어 는 PyTorch 와 MxNet 의 Gluon API 에 도 채 택 되 었 습 니다 . 이 책 에 서 는 MxNet 의 Gluon API 을 사 용 하 였 습 니다 . 학 습 을 위 해 더 나 은 툴 을 만 드 는 연 구 자 와 더 나 은 신 경 망 을 만 들 기 위 한 통 계 모 델 러 간 , 작 업 시스 템 분 리 는 많 은 것 들 을 단 순 화 하 였 습 니다 . 한 예 로 , 2014 년 카 네 기 멜 론 대 학 의 머 신 러 닝 박 사 과 정 학 생 에 게 선 형 회 귀 분 석 모 델 을 학 습시 키 는 것 은 매 우 중 요 한 과 제 였 습 니다 . 지 금 은 이 작 업 은 10 줄 이 안 되 는 코 드 로 완 료 가 능 하 고 , 프 로 그 래 머 들 이 확 실 히 이 해할 수 있 게 만 들 었 습 니다 . 3 . 4 성 공 사 례 인 공 지 능 은 풀 기 어 려 웠 던 여 러 가 지 문 제 들 을 다 른 방법 으 로 해 결 해 온 오 래 된 역 사 가 있 습 니다 . 하 나 의 예 로 , 편 지 는 광 학 문 자 인 식 기 술 을 이 용 해 정 렬 됩 니다 . 이 시스 템 은 90 년 대 부 터 사 용 되 었 습 니 다 . ( 이 것 이 결 국 , 유 명 한 MINIST 및 USPS 필 기 숫 자 셋 의 출 처 입 니다 . ) 동 시 에 은 행 예 금 의 수 표 책 과 신 청 자의 신 용 점 수 를 읽 는 데 에 도 적 용 됩 니다 . 또 금 융 거 래 에 서 는 자 동 으 로 사 기 여 부 를 체 크 합 니 다 . 페 이 팔 , 스 트 라 이 프 , 알 리 페 이 , 위 챗 , 애 플 , 비 자 , 마 스 터 카 드 등 과 같 은 많 은 e - 커 머 스 지 불 시스 템 의 근 간 을 이 룹 니다 . 체 스 프 로 그 램 은 수십 년 간 경 쟁 력 을 유 지 해 왔 습 니다 . 머 신 러 닝 은 인 터 넷 상 에 서 검 색 , 추 천 , 개 인 화 및 랭 킹 을 제 공 해 왔 습 니다 . 즉 , 인 공 지 능 과 머 신 러 닝 은 비 록 , 종종 시 야에 서 숨 겨 져 있 지 만 , 널 리 퍼 져 있 습 니다 . 최 근 에야 AI 가 각 광 을 받 고 있 는 데 , 이 는 대 부분 이 전 에 다 루 기 어 려 운 문 제 들 을 AI 가 해 결 하 고 있 기 때 문 입 니다 . • 애 플 의 시 리 ( Siri ) , 아 마 존 의 알 렉 사 ( Alexa ) , 구 글 의 조 수 ( assistant ) 와 같 은 지 능 형 조 수 들 은 말 로 전 달 된 질 문 에 대 해 합 당 한 정 도 의 정 확 도 로 대 답 할 수 있 습 니다 . 여 기 에 는 조 명 스 위 치 를 켜 고 이 발 사 와 약 속 을 잡 고 , 대 화형 전 화 지 원 을 제 공 하 는 등 의 일 상 적 인 작 업 이 포 함 됩 니다 . 이 것 은 AI 가 우 리 삶 에 영 향 을 미 치 는 가 장 두드 러 진 사 례 들 일 것 입 니다 . • 디 지 털 조 수 의 핵 심 요 소 는 말 을 정 확 하 게 인 식 할 수 있 는 능 력 입 니다 . 점 차 적 으 로 이 러 한 시스 템 의 정 확 도 는 특 정 응 용 분 야에 서 인 간과 유 사 한 수 준 에 도 달 할 정 도 로 올 라 갔 습 니다 [ 14 ] . 16 3 . 딥 러 닝 소 개 • 객 체 인 식 기 술 도 마 찬 가 지 로 오 랜 시 간 이 걸 렸 습 니다 . 2010 년 에 그 림 에 서 객 체 를 추 정 하 는 것 은 매 우 어 려 운 작 업 이 었 습 니다 . ImageNet 벤 치 마 크 에 서 Lin [ 15 ] 은 top - 5 에 러 율 에 서 28 % 를 달 성 하 였 습 니다 . 2017 년 에 Hu [ 16 ] 은 이 에 러 율을 2 . 25 % 까 지 낮 추 었 습 니다 . 놀 라 운 결과 들 은 새 를 분 석 하 거 나 , 피 부 암 을 진 단 하 기 위 한 분 야에 서 이 루 어 졌 습 니다 . • 게 임은 인 간 지 능 이 우월 한 마 지 막 분 야 같 았 습 니다 . TDGammon [ 23 ] 에 서 시 작 된 , 시 간 차 강 화 학 습 ( Temporal Difference Reinforcement Learning ) 을 사 용 한 주 사 위 놀 이 게 임 처 럼 , 알 고 리 즘 과 컴 퓨 터 의 발 전 은 광 범 위 한 분 야 의 애 플 리 케 이 션 을 위 한 알 고 리 즘 들 을 이 끌 어 냈 습 니다 . 주 사 위 놀 이 와 는 달 리 , 체 스 는 훨 씬 더 복 잡 한 상 태 공간과 행 동 조 합 을 갖고 있 습 니다 . Campbell [ 17 ] 의 DeepBlue 가 게 임 트 리를 이 용 한 효 율 적 인 검 색 과 특 수 한 목 적 의 하 드 웨 어 를 이 용 한 대 량 병 렬 계 산 을 통 해 , Gary Kasparov 를 이 겼 습 니다 . 바 둑 은 그 것 의 커 다 란 상 태 공간 때 문 에 , 여 전 히 체 스 보 다 더 어 렵 습 니다 . 2015 년 에 Silver [ 18 ] 의 Monte Carlo 트 리 샘 플 링 을 조 합한 딥 러 닝 을 사 용 한 , 알 파 고가 인 간 의 수 준 에 다다 랐 습 니다 . 포 커 에 있 어 서 어 려 운 점 은 , 상 태 공간 이 크 고 , 전 체 가 관 찰 되 지 않 는 다는 점 입 니다 . ( 우 리 는 상 대 방 의 카 드 를 알 수 없 습 니다 . ) Brown 과 Sandholm [ 19 ] 의 Libratus 는 효 과 적 으 로 구 조 화 된 전 략 들 을 이 용 해 인 간 의 능 력 을 넘 어 섰 습 니다 . 이 것 은 향 상 된 알 고 리 즘 이 게 임 분 야 의 인 상 적 인 발 전 에 중 요 한 역 할 을 하 고 있음을 나 타 냅 니다 . • AI 발 전 의 또 다 른 지 표 는 자율 주 행 자 동 차 와 트 럭 의 출 현 입 니다 . 아 직 완 전 한 자율 주 행 에 도 달 한 것 은 아 니 지 만 , 모멘 타 , 테 슬 라 , 엔 비 디 아 , 모 바 일 아 이 , Waymo . com 와 같 은 , 적 어 도 부분 적 인 자율 성 을 가 능 하 게 하 는 제 품 을 생산 하 는 회 사 들 이 이 러 한 방 향 으 로 엄 청 난 발 전 을 만 들 어 냈 습 니다 . 완 전 한 자율 주 행 을 만 드 는 것 은 매 우 어 려 운 일입 니다 . 운 전 을 잘 하 기 위 해 서 는 하 나 의 시스 템 에 서 신 호 를 인 식 하 고 , 추 론 하 고 , 조 합할 수 있 는 능 력 을 필 요 로 하 기 때 문 입 니다 . 요 즘 에 는 , 이 러 한 문 제 를 컴 퓨 터 비 전 ( Computer Vision ) 에 딥 러 닝 을 주 로 사 용 합 니다 . 나 머 지 부분 은 엔 지 니 어 들 이 많 은 부분 을 조정 합 니다 . 다 시 말 하 지 만 , 위의 목 록 은 인 공 지 능 으 로 간 주 되 는 것과 머 신 러 닝 이 분 야에 서 일 어 난 놀 라 운 발 견 들 의 극 히 일 부분 에 불 과 합 니다 . 오 늘 날 의 로 봇 공 학 , 물 류 , 전 산생 물 학 , 입자 물 리 학 , 천 문 학 은 크 던 작 던 머 신 러 닝 의 발 전 의 혜 택 을 누 리 고 있 습 니다 . 이 제 머 신 러 닝 은 엔 지 니 어 와 과 학 자 를 위 한 범 용 적 인 도 구 가 되 어 가고 있 는 것 입 니다 . 종종 AI 종 말 론 이 나 인 공 지 능 특 이 성 에 대 한 질 문 들 이 비 기 술 적 인 기 사 에 서 제 기 되 곤 합 니다 . 머 신 러 닝 시스 템 이 지 각 을 갖게 될 것 이 고 , 그 것 을 만 든 프 로 그 래 머 와 는 독 립 적 으 로 인 간 의 생 활 에 직 접적 인 영 향 을 끼 칠 것 들 을 결 정 할 것 이 라 는 것 을 두 려 워 합 니다 . 하 지 만 이 미 AI 는 인 간 의 삶 에 영 향 을 미 치 고 있 습 니다 . 신 용 도 가 자 동 으 로 평 가 되 고 , 오 토 파 일 럿 ( autopilot ) 은 자 동 차 를 안 전 하 게 운 전 할 수 있 게 해 주 며 , 입 력 된 통 계 데 이 터 을 사 용 해 서 보 석 허 용 여 부 를 결 정 하 고 있 습 니다 . 조 금 더 친 근 한 사 례로 우 리 는 Alexa 에 게 커 피 머 신 을 켜 달 라 고 요 청 할 수 있으 며 , Alexa 가 장 치 에 연 결 되 어 있 다 면 요 청 을 수 행할 수 있 습 니다 . 다 행 히 도 우 리 는 인 간 창 조 자 를 노 예 로 만 들 거 나 커 피 를 태 울 준 비 가 된 , 지 각 있 는 AI 시스 템 과 는 3 . 4 . 성 공 사 례 17 거 리 가 멉 니다 . 첫 째 , AI 시스 템 은 특 정 목 표 지 향 적 방 식 으 로 설 계 , 학 습 , 배 포 됩 니다 . 그 들 의 행 동 은 범 용 AI 에 대 한 환 상 을 줄 수 있 지 만 , 어 디 까 지 나 현 재 인 공 지 능 디 자인의 기 초 는 규 칙 , 휴 리 스 틱 , 통 계 모 델 의 조 합 입 니다 . 둘 째 , 아 직 까 지 는 일 반 적 인 일을 수 행하 면 서 스스 로 개 선 하 고 , 스스 로 에 대 해 서 사 고 , 스스 로 의 아 키 텍 처 를 개 선 확 장 하 고 개 선 하 는 일 반 적 인 인 공 지 능 을 위 한 도 구 는 존 재 하 지 않 습 니다 . 훨 씬 더 현 실 적 인 관 심 사 는 AI 가 일 상생 활 에 서 어 떻 게 사 용 되 는 지 입 니다 . 트 럭 운 전 사 및 상 점 보 조 자 가 수 행하 는 사 소 한 일 들 이 자 동 화 될 수 있 고 자 동 화 될 가 능 성 이 있 습 니다 . 농 장 로 봇 은 유 기 농 업 비 용 을 줄 일 수 있있 고 , 또 한 수 확 작 업 을 자 동 화 할 것 입 니다 . 산 업 혁 명 의 이 단 계 는 사 회 의 많 은 이 들 의 삶 에 있 어 서 중 대 한 변 화 를 가 져 올 것 입 니다 . 트 럭 운 전 사 와 매 장 점 원 은 많 은 주 에 서 가 장 일 반 적 인 직 업 중 하 나 입 니다 . 게 다 가 통 계 모 델 이 부 주 의 하 게 적 용 되 면 인 종적 , 성 별 또 는 연 령 편 견 이 발 생 할 수 있 습 니다 . 이 러 한 알 고 리 즘 이 세 심 한 주 의 를 가 지 고 사 용 되 는 지 확 인 하 는 것 이 중 요 합 니다 . 이 것 은 인 류를 멸 망 시 킬 수 있 는 악 의 적 인 초 지 능 의 탄 생 에 대 해 걱 정 하 는 것 보 다 훨 씬 더 현 실 적 이 고 중 요 한 문 제 입 니다 . 3 . 5 주 요 요 소 들 머 신 러 닝 은 데 이 터 를 사 용 하 여 예 제 간 의 변 환 을 학 습 합 니다 . 예 를 들 어 숫 자 이 미 지 는 0 에 서 9 사 이의 정 수 로 변 환 되 고 , 오 디 오 는 텍 스 트 ( 음 성 인 식 ) 로 변 환 되 고 , 텍 스 트 는 다 른 언어 의 텍 스 트 로 변 환 되 거 나 ( 기 계 번 역 ) , 머 그 샷 이 이 름 으 로 변 환 됩 니다 ( 얼 굴 인 식 ) . 그 렇 게 할 때 , 알 고 리 즘 이 데 이 터 를 처 리 하 기 에 적 합한 방 식 으 로 데 이 터 를 표 현 해 야 하 는 경 우 가 종종 있 습 니다 . 이 러 한 특 징 변 환 ( feature transformation ) 의 정 도 는 표 현 학 습 을 위 한 수 단 으 로 , 딥 러 닝 을 언 급 하 는 이유 로 서 종종 사 용 됩 니다 . 사 실 , 국 제 학 습 표 현 회 의 ( the International Conference on Learning Representations ) 의 명 칭 은 이 것 으 로 부 터 유 래 합 니다 . 동 시 에 머 신 러 닝 은 통 계 ( 특 정 알 고 리 즘 이 아 닌 매 우 큰 범 위의 질 문 까 지 ) 와 데 이 터 마 이 닝 ( 확 장 성 처 리 ) 을 똑 같 이 사 용 합 니다 . 현 기 증 나 는 알 고 리 즘 및 응 용 프 로 그 램 집 합 으 로 인 해 딥 러 닝 을 위 한 성 분 이 무 엇 인 지 구 체 적 으 로 평 가 하 기 가 어 렵 습 니다 . 이 것 은 피 자 에 필 요 한 재 료 를 고 정 시 키 는 것 만 큼 어 렵 습 니다 . 거 의 모 든 구 성 요 소 는 대 체 가 능 합 니다 . 예 를 들 어 다 층 퍼 셉 트 론 이 필 수 성 분 이 라 고 가 정 할 수 있 습 니다 . 그 러 나 convolution 만 사 용 하 는 컴 퓨 터 비 전 모 델 이 있 습 니다 . 다 른 것 들 은 시 퀀 스 모 델 만 사 용 하 기 도 합 니다 . 틀 림 없 이 이 러 한 방법 에 서 가 장 중 요 한 공 통 점 은 종 단 간 ( end - to - end ) 학 습 을 사 용 하 는 것 입 니다 . 즉 , 개 별 적 으 로 튜 닝 된 구 성 요 소 를 기 반 으 로 시스 템 을 조 립 하 는 대 신 시스 템 을 구 축 한 다 음 성 능 을 공 동 으 로 튜 닝 합 니다 . 예 를 들 어 , 컴 퓨 터 비 전 과 학 자 들 은 머 신 러 닝 모 델 을 구 축 하 는 과 정 과 특 징 엔 지 니 어 링 프 로 세 스 를 분 리 하 곤 했 습 니다 . Canny 에 지 검 출 기 [ 20 ] 와 Lowe 의 SIFT 특 징 추출 기 [ 21 ] 는 이 미 지 를 형 상 벡 터 에 매 핑하 기 위 한 알 고 리 즘 으 로 10 여 년 간 최 고 였 습 니다 . 불 행 히 도 알 고 리 즘 에 의 해 자 동 으 로 수 행 될 때 수 천 또 는 수 백 만 가 지 선 택 에 대 한 , 일 관 된 평 가 와 관 련 하 여 인 간 이 독 창 성 18 3 . 딥 러 닝 소 개 으 로 성 취 할 수 있 는 많 은 것 들 이 있 습 니다 . 딥 러 닝 이 적 용 되 었 을 때 , 이 러 한 특 징 추출 기 는 자 동 으 로 튜 닝 된 필 터 로 대 체 되 어 뛰 어 난 정 확 도 를 달 성 했 습 니다 . 마 찬 가 지 로 자 연 언어 처 리 에 서 Salton 과 McGill [ 22 ] 의 bag - of - words 모 델 은 오 랫 동 안 기 본 선 택 이 었 습 니다 . 여 기 서 문 장의 단 어 는 벡 터 로 매 핑 되 며 각 좌 표 는 특 정 단 어 가 발 생 하 는 횟 수 에 해 당 합 니다 . 이 것 은 단 어 순 서 ( ‘ 개가 사 람 을 물 었 다 ’ 대 ‘ 사 람 이 개 를 물 었 다 ’ ) 또 는 구 두 점 ( ‘ 먹 자 , 할 머 니 ’ 대 ‘ 할 머 니 를 먹 자 ’ ) 을 완 전 히 무 시 합 니다 . 불 행 히 도 , 더 나 은 특 징 을 수 동 으 로 엔 지 니 어 링 하 는 것 은 다 소 어 렵 습 니다 . 반 대 로 알 고 리 즘 은 가 능 한 특 징 ( feature ) 설 계 의 넓 은 공간 을 자 동 으 로 검 색 할 수 있 습 니다 . 이 것 은 엄 청 난 진 전 을 이 끌 어 왔 습 니다 . 예 를 들 어 의 미 상 관 련 성 이 있 는 단 어 임 베 딩 은 벡 터 공간 에 서 ‘ 베 를린 - 독 일 + 이 탈 리 아 = 로 마 ’ 형 식 의 추 론 을 허 용 합 니다 . 다 시 말 하 지 만 , 이 러 한 결과 는 전 체 시스 템 의 end - to - end 학 습 을 통 해 달 성 됩 니다 . End - to - end 학 습 외 에 도 두 번 째 로 중 요 한 것 은 파 라 미 터 기 반 의 통 계 설 명 에 서 완 전 비 파 라 미 터 기 반 의 모 델 로 의 전 환 을 경 험하 고 있 다는 것 입 니다 . 데 이 터 가 부 족 한 경 우 , 유 용 한 모 델 을 얻 기 위 해 서 는 현 실 에 대 한 가 정 을 단 순 화 하 는 데 의 존 해 야 합 니다 ( 예 , 스 펙 트 럼 방법 을 통 해 ) . 데 이 터 가 풍 부 하 면 현 실 에 더 정 확 하 게 맞 는 비 파 라 미 터 기 반 의 모 형 으 로 대 체 될 수 있 습 니다 . 어 느 정 도 , 이 것 은 컴 퓨 터 의 가 용 성 과 함 께 이 전 세 기 중 반 에 물 리 학 이 경 험한 진 전 과 비 슷 합 니다 . 전 자 가 어 떻 게 동 작 하 는 지 에 대 한 파 라 메 트 릭 근 사 치 를 직 접 해 결 하 는 대 신 , 이 제 연 관 된 부분 미 분 방 정 식 의 수 치 시 뮬 레 이 션 에 의 존 할 수 있 습 니다 . 이 것 은 설 명 가 능 성 을 희 생 시 키 면 서 종종 훨 씬 더 정 확 한 모 델 을 이 끌 어 냈 습 니다 . 예 를 들 어 적 대 적 생 성 신 경 망 ( Generative Aversarial Networks ) 이 있 습 니다 . 그 래 픽 모 델 이 적절 한 확 률 적 공 식 없 이 도 데 이 터 생 성 코 드 로 대 체 됩 니다 . 이 것 은 현혹 적 으 로 현 실 적 으 로 보 일 수 있 는 이 미 지 의 모 델 을 이 끌 어 냈 는 데 , 이 것 은 꽤 오 랜 시 간 동 안 너 무 어 려 웠 던 일입 니다 . 이 전 작 업 의 또 다 른 차 이 점 은 볼 록 하 지 않 은 비 선 형 최 적 화 문 제 ( nonconvex nonlinear optimization problem ) 를 다 루 면 서 차 선 책 의 솔 루 션 을 받 아 들 이 고 , 이 를 증 명 하 기 전 에 시 도 하 려 는 의 지 입 니다 . 통 계 적 문 제 를 다 루 는 새 로 운 경 험 주 의 와 인재의 급 속 한 유입은 실 질 적 인 알 고 리 즘 의 급 속 한 발 전 으 로 이 어 졌 습 니다 ( 많 은 경 우 에 도 불 구 하 고 수십 년 간 존 재 했 던 도 구 를 수 정 하 고 다 시 발 명 하 는 대 신 ) . 마 지 막 으 로 딥 러 닝 커 뮤 니 티 는 학 술 및 기 업 경계 를 넘 어 도 구 를 공 유 하 는 것 을 자 랑 으 로 하 고 있으 며 , 많 은 우 수 한 라 이 브 러 리 , 통 계 모 델 및 학 습 된 네 트 워 크 를 오 픈 소 스 로 공개 합 니다 . 이 러 한 공 유 정 신 에 따 라 , 이 과 정 을 구 성 하 는 노 트 북 은 배 포 및 사 용 이 자유 롭 습 니다 . 우 리 는 모 든 사 람 들 이 딥 러 닝 에 대 해 배 울 수 있 는 진 입장 벽 을 낮 추 기 위 해 열 심 히 노 력 했 으 며 독 자 가 이 것 의 혜 택 을 누 릴 수 있 기 를 바 랍 니다 . 3 . 5 . 주 요 요 소 들 19 3 . 6 요 약 • 머 신 러 닝 은 컴 퓨 터 시스 템 이 어 떻 게 데 이 터 를 사 용 하 여 성 능 을 향 상 시 킬 수 있 는 지 연 구 합 니다 . 통 계 , 데 이 터 마 이 닝 , 인 공 지 능 및 최 적 화 의 아 이 디 어 를 결 합합 니다 . 종종 인위 적 으 로 지 능 형 솔 루 션 을 구 현 하 는 수 단 으 로 사 용 됩 니다 . • 머 신 러 닝 의 클 래 스 로 서 표 현 학 습 은 데 이 터 를 나 타 내 는 적절 한 방법 을 자 동 으 로 찾 는 방법 에 초 점 을 맞 춥 니다 . 이 것 은 종종 학 습 된 변 환 진 행 에 의 해 이 루 어 집 니다 . • 최 근 진 전 의 대 부분 은 값 싼 센서 , 인 터 넷 규 모 의 응 용 프 로 그 램 에 서 발 생 하 는 풍 부 한 데 이 터 , 주 로 GPU 를 통 한 상 당 한 연 산 발 전 에 의 해 촉 발 되 었 습 니다 . • 전 체 시스 템 최 적 화 가 핵 심 입 니다 . 구 성 요 소 를 사 용 하 여 좋 은 성 능 을 얻 을 수 있 습 니다 . 효 율 적 인 딥 러 닝 프 레 임 워 크 의 가 용 성 으 로 인 해 이 프 레 임 워 크 의 설 계 와 구 현 이 훨 씬 쉬 워 졌 습 니다 . 3 . 7 문 제 1 . 현 재 작 성 중 인 코 드 의 어 느 부분 이 ’ 학 습 ’ 될 수 있 습 니 까 ? 즉 , 학 습 에 의 해 서 , 당 신 이 작 성 한 코 드 에 의 해 자 동 으 로 의 사 결 정 을 함 으 로 써 , 향 상 시 킬 수 있 습 니 까 ? 2 . 당 신 의 코 드 는 경 험 적 설 계 선 택 을 포 함합 니 까 ? 어 떠 한 문 제 들 이 , 이 문 제 를 해 결 하 는 방법 에 대 한 많 은 예 가 있 지 만 , 이 를 자 동 화 할 구 체 적 인 방법 은 없 습 니 까 ? 이 들 은 딥 러 닝 을 사 용 할 주 요 후 보 자 일 수 있 습 니다 . 3 . 인 공 지 능 을 하 나 의 새 로 운 산 업 혁 명 으 로 볼 때 , 알 고 리 즘 과 데 이 터 사 이의 관 계 는 무 엇 입 니 까 ? 증 기 엔 진 과 석 탄 의 관 계 와 유 사 합 니 까 ? ( 근 본 적 인 차 이 점 은 무 엇 입 니 까 ? ) 4 . 다 른 어 떠 한 분 야에 end - to - end 학 습 법 을 적 용 할 수 있 습 니 까 ? 물 리 학 ? 공 학 ? 계 량 경 제 학 ? 3 . 8 참 고 문 헌 [ 1 ] Turing , A . M . ( 1950 ) . Computing machinery and intelligence . Mind , 59 ( 236 ) , 433 . [ 2 ] Hebb , D . O . ( 1949 ) . The organization of behavior ; a neuropsychological theory . A Wiley Book in Clinical Psychology . 62 - 78 . [ 3 ] Srivastava , N . , Hinton , G . , Krizhevsky , A . , Sutskever , I . , & Salakhutdinov , R . ( 2014 ) . Dropout : a simple way to prevent neural networks from overﬁtting . The Journal of Machine Learning Research , 20 3 . 딥 러 닝 소 개 15 ( 1 ) , 1929 - 1958 . [ 4 ] Bishop , C . M . ( 1995 ) . Training with noise is equivalent to Tikhonov regularization . Neural computa - tion , 7 ( 1 ) , 108 - 116 . [ 5 ] Bahdanau , D . , Cho , K . , & Bengio , Y . ( 2014 ) . Neural machine translation by jointly learning to align and translate . arXiv preprint arXiv : 1409 . 0473 . [ 6 ] Sukhbaatar , S . , Weston , J . , & Fergus , R . ( 2015 ) . End - to - end memory networks . In Advances in neural information processing systems ( pp . 2440 - 2448 ) . [ 7 ] Reed , S . , & De Freitas , N . ( 2015 ) . Neural programmer - interpreters . arXiv preprint arXiv : 1511 . 06279 . [ 8 ] Goodfellow , I . , Pouget - Abadie , J . , Mirza , M . , Xu , B . , Warde - Farley , D . , Ozair , S . , . . . & Bengio , Y . ( 2014 ) . Generative adversarial nets . In Advances in neural information processing systems ( pp . 2672 - 2680 ) . [ 9 ] Zhu , J . Y . , Park , T . , Isola , P . , & Efros , A . A . ( 2017 ) . Unpaired image - to - image translation using cycle - consistent adversarial networks . arXiv preprint . [ 10 ] Karras , T . , Aila , T . , Laine , S . , & Lehtinen , J . ( 2017 ) . Progressive growing of gans for improved quality , stability , and variation . arXiv preprint arXiv : 1710 . 10196 . [ 11 ] Li , M . ( 2017 ) . Scaling Distributed Machine Learning with System and Algorithm Co - design ( Doc - toral dissertation , PhD thesis , Intel ) . [ 12 ] You , Y . , Gitman , I . , & Ginsburg , B . Large batch training of convolutional networks . ArXiv e - prints . [ 13 ] Jia , X . , Song , S . , He , W . , Wang , Y . , Rong , H . , Zhou , F . , . . . & Chen , T . ( 2018 ) . Highly Scalable Deep Learning Training System with Mixed - Precision : Training ImageNet in Four Minutes . arXiv preprint arXiv : 1807 . 11205 . [ 14 ] Xiong , W . , Droppo , J . , Huang , X . , Seide , F . , Seltzer , M . , Stolcke , A . , . . . & Zweig , G . ( 2017 , March ) . The Microsoft 2016 conversational speech recognition system . In Acoustics , Speech and Signal Process - ing ( ICASSP ) , 2017 IEEE International Conference on ( pp . 5255 - 5259 ) . IEEE . [ 15 ] Lin , Y . , Lv , F . , Zhu , S . , Yang , M . , Cour , T . , Yu , K . , . . . & Huang , T . ( 2010 ) . Imagenet classiﬁcation : fast descriptor coding and large - scale svm training . Large scale visual recognition challenge . [ 16 ] Hu , J . , Shen , L . , & Sun , G . ( 2017 ) . Squeeze - and - excitation networks . arXiv preprint arXiv : 1709 . 01507 , 7 . [ 17 ] Campbell , M . , Hoane Jr , A . J . , & Hsu , F . H . ( 2002 ) . Deep blue . Artiﬁcial intelligence , 134 ( 1 - 2 ) , 57 - 83 . 3 . 8 . 참 고 문 헌 21 [ 18 ] Silver , D . , Huang , A . , Maddison , C . J . , Guez , A . , Sifre , L . , Van Den Driessche , G . , . . . & Dieleman , S . ( 2016 ) . Mastering the game of Go with deep neural networks and tree search . Nature , 529 ( 7587 ) , 484 . [ 19 ] Brown , N . , & Sandholm , T . ( 2017 , August ) . Libratus : The superhuman ai for no - limit poker . In Proceedings of the Twenty - Sixth International Joint Conference on Artiﬁcial Intelligence . [ 20 ] Canny , J . ( 1986 ) . A computational approach to edge detection . IEEE Transactions on pattern analysis and machine intelligence , ( 6 ) , 679 - 698 . [ 21 ] Lowe , D . G . ( 2004 ) . Distinctive image features from scale - invariant keypoints . International journal of computer vision , 60 ( 2 ) , 91 - 110 . [ 22 ] Salton , G . , & McGill , M . J . ( 1986 ) . Introduction to modern information retrieval . [ 23 ] Tesauro , G . ( 1995 ) , Transactions of the ACM , ( 38 ) 3 , 58 - 68 3 . 9 Scan the QR Code to Discuss 22 3 . 딥 러 닝 소 개 4 딥 러 닝 맛 보 기 서 두 르 고 싶 다 면 , 이 장은 딥 러 닝 을 맛 보 기 위 해 필 요 한 모 든 세 부 사 항 들 을 담 고 있 습 니다 . 만 약 제 대 로 살 펴 보 고 싶 은 경 우 에 도 여 전 히 이 장을 읽 어 보 길 권 장 합 니다 , 이 장 에 서 는 어 떻 게 Apaceh MXNet 을 설 치 하 는 지 , 자 동 미 분 ( automatic differentiation ) 사 용 방법 , 데 이 터 와 메모 리 조 작 방법 을 설 명 하 고 있 습 니다 . 우 리 는 또 한 선 형 대 수 와 통 계 학 에 대 해 , 독 자 들 이 기 본 기 위 에 서 속 도 를 낼 수 있 도 록 돕 기 위 해 만 들 어 진 속성 가 이 드 / 기 본 서 를 제 공 합 니다 . 4 . 1 소 개 저 자 들 은 이 책 을 쓰 기 전 에 , 많 은 노 동 력 이 필 요 한 일 처 럼 , 많 은 카 페 인이 필 요 했 습 니다 . . 상상 해 봅 시 다 . 우 리 는 차 에 올 라 타 서 운 전 을 하 기 시 작 했 습 니다 . 아 이 폰 을 사 용 자인 Alex 는 핸 드 폰 의 음 성 인 식 시스 템 을 부 르 기 위 해 서 ’Hey Siri’ 라 고 외 쳤 습 니다 . 그 러 자 Mu 는 ’ 블 루 보 틀 커 피 샵 으 로 가 는 길 을 알 려 줘 ’ 라 고 명 령 했 습 니다 . 핸 드 폰 은 그 의 명 령 을 글 로 바 꿔 서 화 면 에 빠 르 게 보 여 줍 니다 . 우 리 가 길 을 묻 는 것 을 알아 채 고 는 우 리 의 요 청 에 응 하 기 위 해 서 지 도 앱 을 띄 웁 니다 . 지 도 앱 이 실 행 되 자 마 자 여 러 경 로 를 찾 아 냅 니다 . 각 경 로 옆 에 는 예 상 소 요 시 간 이 함 께 표 시 됩 니다 . 설 명 을 위 해 서 지 어 낸 이 야 기 이 긴 하 지 만 , 이 짧 은 시 나 리 오 는 스 마 트 폰 을 통 해 다 양 한 머 신 러 닝 모 델 이 사 용 되 는 것 을 보 여 주 고 있 습 니다 . 23 여 러 분 이 지 금까 지 머 신 러 닝 을 다 뤄 본 적 이 없 다 면 , 무 엇에 대 해 서 이 야 기 를 하 고 있 는 지 모 를 수 도 있 습 니다 . 어 쩌 면 ‘ 그 냥 프 로 그 래 밍 으 로 작 동 하 는 거 아 닌 가 요 ? ’ 라 고 묻 거 나 ‘ 머 신 러 닝 이 무 엇 을 의 미 하 나 요 ? ’ 라 는 질 문 을 던 질 수 도 있 습 니다 . 우 선 확 실 하 게 해 두 기 위 해 서 , 모 든 머 신 러 닝 알 고 리 즘 은 컴 퓨 터 프 로 그 램 을 작 성 해 서 구 현 됩 니다 . 사 실 우 리 는 다 른 컴 퓨 터 과 학 의 분 야 와 동 일 한 언어 와 하 드 웨 어 를 사 용 합 니다 . 하 지 만 , 모 든 컴 퓨 터 프 로 그 램 이 머 신 러 닝 을 포 함하 는 것 은 아 닙니다 . 두 번 째 질 문 에 대 한 답 은 , 머 신 러 닝 은 방 대 한 분 야 이 기 때 문 의 정 의 하 기 어 렵 습 니다 . 이 질 문 은 마 치 ’ 수 학 이 무 엇 인 가 요 ? ’ 라 는 질 문 과 비 슷 합 니다 . 하 지 만 , 여 러 분 이 공 부 를 시 작 할 수 있 도 록 직 관 적 인 설 명 을 충 분 히 해 보 겠 습 니다 . 4 . 1 . 1 동 기 부 여 를 위 한 예 시 우 리 가 매 일 사 용 하 는 컴 퓨 터 프 로 그 램 의 대 부분 은 제 일 원 칙 ( ﬁrst principles ) 을 활 용 해 서 코 드 화 될 수 있 습 니다 . 여 러 분 이 쇼 핑 카 트 에 물 건 을 담 으 면 , 이 커 머 스 어 플 리 케 이 션 은 어 떤 항 목 ( 여 러 분 의 user ID 와 제 품 ID 를 연 관 시 키 는 ) 을 쇼 핑 카 트 데 이 터 베 이 스 태 이 블 에 저 장 합 니다 . 우 리 는 이 런 프 로 그 램 을 제 일 원 칙 에 따 라 서 작 성 하 고 , 실 제 고객 을 본 적 이 없어 도 런 치 할 수 있 습 니다 . 간 단 한 어 플 리 케 이 션 을 만 드 데 에 는 굳 이 머 신 러 닝 을 사 용 하 지 말 아야 합 니다 . ( 머 신 러 닝 개 발 자 커 뮤 니 티 에 게 있 어 서 는 ) 다 행 히 도 , 많 은 문 제 들 에 대 한 해 결 책 이 그 리 쉽 지 만 은 않 습 니다 . 커 피 를 사 러 가 는 이 야 기 로 돌 아 가 서 , ‘Alexa’ , ‘Okay , Google’ , 이 나 ‘Siri’ 같 은 wake word 에 응 답 하 는 프 로 그 램 을 작 성 한 다 고 생 각 해 보 세 요 . 컴 퓨 터 와 코 드 편 집 기 만 사 용 해 서 코 드 를 만 들 어 나 간 다 고 했 을 때 제 일 원 칙 을 이 용 해 서 어 떻 게 그 런 프 로 그 램 을 작 성 할 것 인 가 요 ? 조 금 만 생 각 해 봐 도 이 문 제 가 쉽 지 않 다는 것 을 알 수 있 습 니다 . 매 초 마 다 마 이 크 는 대 략 44 , 000 개 의 샘 플 을 수 집 합 니다 . 소 리 조 각 으 로 부 터 그 소 리 조 각 이 wake word 를 포 함하 는 지 신 뢰 있 게 { yes , no } 로 예 측 하 는 룰 을 만 들 수 있 나 요 ? 어 떻 게 할 지 를 모 른 다 고 해 도 걱 정 하 지 마 세 요 . 우 리 도 그 런 프 로 그 램 을 처 음 부 터 어 떻 게 작 성 해 야 하 는 지 모 릅 니다 . 이 것 이 바 로 우 리 가 머 신 러 닝 을 사 용 하 는 이유입 니다 . 트 릭 을 알 려 드 리 겠 습 니다 . 우 리 는 컴 퓨 터 에 게 명 시 적 으 로 입 력 과 출 력 을 어 떻 게 매 핑해 야 하 는 지 를 알 려 주 는 것 은 모 르 지 만 , 우 리 자 신 은 인 지 적 인 활 동 을 할 수 있 는 능 력 이 있 습 니다 . 즉 , 우 리 는 ’Alexa’ 라 는 단 어 를 인 지 하 도 록 컴 퓨 터 를 프 로 그 램 하 는 방법 은 모 르 지 만 , 여 러 분 은 그 단 어 를 인 지 할 수 있 습 니다 . 이 능 력 을 사 용 해 서 , 우 리 는 오 디 오 샘 플 과 그 오 디 오 샘 플 이 wake word 를 포 함하 는 지 여 부 를 알 려 주 는 레 이 블 을 아 주 많 이 수 집 할 수 있 습 니다 . 머 신 러 닝 접 근 방법 은 우 리 가 wake word 를 바 로 인 식 할 수 있 도 록 명 시 적 인 시스 템 디 자인을 할 수 없 지 만 , 대 신 우 리 는 많 은 수 의 파 라 미 터 들 을 갖 는 유 연 한 프 로 그 램 을 정 의 할 수 있 습 니다 . 이 것 들 은 프 로 그 램 의 행 동 을 바 꾸 기 위 해 서 조 작 하 는 데 사 24 4 . 딥 러 닝 맛 보 기 용 하 는 손 잡이 들 입 니다 . 이 프 로 그 램 을 모 델 이 라 고 부 릅 니다 . 일 반 적 으 로 우 리 의 모 델 은 입 력 을 어 떤 결과 로 변 환 하 는 머 신 일 뿐 입 니다 . 이 경 우 에 는 , 모 델 은 오 디 오 조 각 을 입 력 으 로 받 아 서 , { yes , no } 답 을 출 력 으 로 생 성 하 는 데 , 우 리 는 이 결과가 wake word 의 포 함 여 부 를 담 기 를 원 합 니다 . 만 약 여 러 분 이 좋 은 모 델 을 선 택 했 다 면 , ‘Alexa’ 단 어 를 들 을 때 마 다 yes 를 출 력 하 는 모 델 을 만 드 는 파 라 미 터 세 트 하 나 가 존 재 할 것 입 니다 . 마 찬 가 지 로 ‘Apricot’ 단 어에 대 해 서 yes 를 출 력 하 는 것 이 다 른 조 합 이 있을 수 있 습 니다 . 우 리 는 이 두 가 지 가 비 슷 하 기 때 문 에 , 동 일 한 모 델 이 ‘Alexa’ 인 식 과 ‘Apricot’ 인 식 에 적 용 되 기 를 기 대 합 니다 . 하 지 만 근 본 적 으 로 다 른 입 력 또 는 출 력 을 다 루 기 위 해 서 는 다 른 모 델 이 필 요 할 수 도 있 습 니다 . 예 를 들 어 , 이 미 지 와 캡 션 을 매 핑하 는 머 신 과 영 어 문 장을 중 국 어 문 장으 로 매 핑하 는 모 델 은 서 로 다 른 것 을 사 용 할 것 입 니다 . 이 미 예 상 했 겠 지 만 , 이 손 잡이 를 아 무 렇 게 나 설 명 할 경 우 , 아 마 도 그 모 델 은 ‘Alexa’ , ‘Apricot’ 또 는 어 떤 영 어 단 어 도 인 식 하 지 못 할 것 입 니다 . 일 반 적 으 로 딥 러 닝 에 서 는 학 습 ( learning ) 은 여 러 학 습 기 간 에 걸 쳐 서 모 델 의 행 동 ( 손 잡이 를 돌 리 면 서 ) 을 업 데 이 트 하 는 것 을 말 합 니다 . 학 습 과 정 은 보 통 다 음 과 같 습 니다 . 1 . 임의 로 초 기 화 되 서 아 무 것 도 유 용 한 것 을 못 하 는 모 델 로 시 작 합 니다 . 2 . 레 이 블 을 갖 은 데 이 터 를 수 집 합 니다 . ( 예 , 오 디 오 조 각과 그 에 해 당 하 는 { yes , no } 레 이 블 들 ) 3 . 주 어 진 예 제 들 에 모 델 이 조 금 덜 이 상 하 게 작 동 하 도 록 손 잡이 를 조정 합 니다 . 4 . 모 델 이 좋 아 질 때 까 지 반복 합 니다 . 요 약 하 면 , wake word 를 인 식 하 는 코 드 를 작 성 하 는 것 이 아 니 라 , 아 주 많 은 레 이 블 이 있 는 데 이 터 셋 이 있을 경 우 에 wake word 를 인 식 하 는 것 을 배 우 는 프 로 그 램 을 작 성 하 는 것 입 니다 . 데 이 터 셋 을 제 공 해 서 프 로 그 램 의 행 동 을 결 정 하 는 것 을 programming with data 라 고 생 각 할 수 있 습 니다 . 우 리 는 아 래 이 미 지 들 과 같 은 아 주 많 은 고 양 이 와 개 샘 플 들 을 머 신 러 닝 시스 템 에 제 공 해 서 고 양 이 탐 지 기 프 로 그 램 을 만 들 수 있 습 니다 . 4 . 1 . 소 개 25 cat cat dog dog 26 4 . 딥 러 닝 맛 보 기 이 런 방법 으 로 탐 지 기 는 결 국 에 고 양 이 를 입 력 으 로 받 으 면 아 주 큰 양 수 를 결과 로 나 오 게 , 그 리 고 개 를 입 력 으 로 받 으 면 아 주 큰 음 수 를 결과 로 나 오 게 학 습 될 것 입 니다 . 이 모 델 은 잘 모 르 겠 으 면 0 과 가 까 운 수 를 결과 로 출 력 할 것 입 니다 . 이 예 는 머 신 러 닝 으 로 할 수 있 는 일의 일 부 에 불 과 합 니다 . 4 . 1 . 2 머 신 러 닝 의 현 기 증 이 날 듯 한 다 재 다능 함 이 것 이 머 신 러 닝 의 핵 심 아 이 디 어 입 니다 . 정 해 진 행 동 에 대 한 코 드 를 만 드 는 것 이 아 닌 , 더 많 은 경 험 을 하 면 능 력 이 향 상 되 는 프 로 그 램 을 디 자인 하 는 것 입 니다 . 이 기 본 아 이 디 어 는 여 러 형 태 들 이 될 수 있 습 니다 . 머 신 러 닝 은 여 러 종 류 의 어 플 리 케 이 션 도 메 인 문 제 를 풀 수 있 고 , 모 델 의 다 른 형 태 를 포 함하 고 , 여 러 학 습 알 고 리 즘 에 따 라 업 데 이 트 를 합 니다 . 앞에 서 든 예 의 경 우 자 동 음 성 인 식 문 제 에 적 용 된 지 도 학 습 ( supervised learning ) 의 한 예 입 니다 . 간 단 한 규 칙 기 반 의 시스 템 이 실 패 하 거 나 만 들 기 어 려 운 경 우 에 데 이 터 를 활 용 할 수 있 도 록 하 는 다 양 한 도 구 들 의 집 합 이 머 신 러 닝 입 니다 . 이 다 양 함 때 문 에 머 신 러 닝 을 처 음 접 하 는 경 우 혼 란 스 러 울 수 있 습 니다 . 예 를 들 어 , 머 신 러 닝 기 술 은 검 색 엔 진 , 자율 주 행 차 , 기 계 번 역 , 의 료 진 단 , 스 팸 필 터 링 , 게 임 , 얼 굴 인 식 , 데 이 터 매 칭 , 보 험 프 리 미 엄 계 산 , 사 진 에 필 터 적 용 등 다 양 한 응 용 에 서 이 미 널 리 사 용 되 고 있 습 니다 . 이 문 제 들 은 겉 보 기 에 는 달 라 보 이 지 만 , 많 은 것 들 은 공 통 적 인 구 조 를 가 지 고 있 고 , 딥 러 닝 도 구 를 이 용 해 서 풀 수 있 습 니다 . 이 문 제 들 은 코 드 로 직 접 프 로 그 램 이 어 떻 게 행 동 해 야 하 는 지 를 작 성 하 는 것 이 불 가 능 하 지 만 , 데 이 터 로 프 로 그 램 을 만 들 수 있 다는 것 이 때 문 에 이 들 대 부분 이 비 슷 합 니다 . 대 부분 의 경 우 이 런 종 류 의 프 로 그 램 을 설 명 하 는 가 장 직 접적 인 언어 가 수 학 입 니다 . 이 책 은 다 른 머 신 러 닝 이 나 뉴 럴 네 트 워 크 책 과 는 다 르 게 수 학 표 현 은 최 소 화 하 고 , 실 제 예 제 와 실 제 코 드 를 중 심 으 로 설 명 하 겠 습 니다 . 4 . 1 . 3 머 신 러 닝 기 초 wake word 를 인 식 하 는 문 제 를 이 야 기 할 때 , 음 성 조 각과 레 이 블 로 구 성 된 데 이 터 셋 을 언 급 했 습 니다 . ( 추 상 적 이 긴 하 지 만 ) 음 성 조 각 이 주 어 졌 을 때 레 이 블 을 예 측 하 는 머 신 러 닝 모 델 을 어 떻 게 학 습시 킬 수 있 는 지 설 명 했 습 니다 . 예 제 로 부 터 레 이 블 을 예 측 하 는 설 정 은 ML 의 한 종 류 로 지 도 학 습 ( supervised learning ) 이 라 고 부 릅 니다 . 딥 러 닝 에 서 도 많 은 접 근 법 들 이 있 는 데 , 다 른 절 들 에 서 다 루 겠 습 니다 . 머 신 러 닝 을 진 행하 기 위 해 서 는 다 음 4 가 지 가 필 요 합 니다 . 1 . 데 이 터 2 . 데 이 터 를 어 떻 게 변 환 할 지 에 대 한 모 델 3 . 우 리 가 얼 마 나 잘 하 고 있 는 지 를 측 정 하 는 loss 함 수 4 . 1 . 소 개 27 4 . loss 함 수 를 최 소 화 하 도 록 모 델 파 라 미 터 를 바 꾸 는 알 고 리 즘 데 이 터 일 반 적 으 로 는 데 이 터 가 많 아 질 수 록 일이 더 쉬 워 집 니다 . 더 많 은 데 이 터 가 있 다 면 , 더 강 력 한 모 델 을 학 습시 킬 수 있 습 니다 . 데 이 터 는 딥 러 닝 부 활 의 중 심 이 고 딥 러 닝 에 서 아 주 흥 미 로 운 많 은 모 델 들 은 많 은 데 이 터 가 없 으 면 만 들 어 지지 못 했 습 니다 . 머 신 러 닝 을 수 행하 는 여 러 분 들 이 자 주 접 하 게 될 몇 가 지 종 류 의 데 이 터 는 다 음 과 같 습 니다 . • 이 미 지 : 스 마 트 폰 으 로 찍 거 나 웹 에 서 수 집 한 사 진 들 , 인 공 위 성 이 미 지 , 의 료 사 진 , 초 음 파 , CT 또 는 MRI 같 은 방 사 선 이 미 지 등 • 텍 스 트 : 이 메 일 , 고 등 학 교 에 세 이 , 트 윗 , 뉴 스 기 사 , 의 사 의 기 록 , 책 , 번 역 된 문 장 등 • 오 디 오 : Amazon Echo , 아 이 폰 , 또 는 안 드 로 이 드 폰 과 같 은 스 마 트 디 바 이 스 에 전 달 될 음 성 명 령 , 오 디 오 책 , 전 화 통 화 , 음 악 녹 음 등 • 비 디 오 : 텔 레 비 전 프 로 그 램 , 영 화 , 유 투 브 비 디 오 , 휴 대 전 화 수신 범 위 ( cell phone footage ) , 가 정 감 시 카 메 라 , 다 중 카 메 라 를 이 용 한 추 적 등 모 델 보 통 의 경 우 에 데 이 터 는 이 를 통 해 서 이 루 고 자 하 는 것과 는 아 주 다 릅 니다 . 예 를 들 면 사 람 들 의 사 진 을 가 지 고 있 고 , 사 람 이 행 복 한 지 아 닌 지 를 알아 내 고 자 합 니다 . 모 델 이 고 해 상 도 이 미 지 를 받 아 서 행 복 점 수 를 결과 로 내 도 록 할 수 있 습 니다 . 단 순 한 문 제 는 단 순 한 모 델 로 해 결 될 수 있 지 만 , 이 경 우 에 는 많 은 것 을 묻 고 있 습 니다 . 이 를 하 기 위 해 서 는 우 리 의 행 복 탐 지 기 가 수십 만 개 의 저 수 준 ( low - level ) 피 처 들 ( 픽 셀 값 들 ) 을 행 복 점 수 와 같 은 상 당 히 추 상 적 인 것 으 로 변 환 해 야 합 니다 . 정 확 한 모 델 을 선 택 하 는 것 은 어 려 운 일이 고 , 다 른 모 델 들 은 다 른 데 이 터 셋 에 더 적 합합 니다 . 이 책 에 서 우 리 는 대 부분 딥 뉴 럴 네 트 워 크 에 집중 할 예 정 입 니다 . 이 모 델 들 은 데 이 터 변 환 이 많 이 연 속 적 으 로 구 성 되 어 있 고 , 따 라 서 이 를 딥 러 닝 ( deep learning ) 이 라 고 합 니다 . 딥 넷 을 논 의 하 는 과 정 으 로 우 선 보 다 간 단 한 또 는 얕 은 모 델 을 먼 저 살 펴 보 겠 습 니다 . 손 실 함 수 ( loss function ) 우 리 의 모 델 이 얼 마 나 잘 하 고 있 는 지 평 가 하 기 위 해 서 모 델 의 결과 와 정 답 ( truth ) 을 비 교 할 필 요 가 있 습 니다 . Loss 함 수 는 우 리 결과가 얼 마 나 나 쁜 지 를 측 정 하 는 방법 을 제 공 합 니다 . 예 를 들 어 , 이 미 지 로 부 터 환 자의 심 장 박 동 을 추 론 하 는 모 델 을 학 습시 켰 다 고 하 겠 습 니다 . 환 자의 실 제 심 장 박 동 은 28 4 . 딥 러 닝 맛 보 기 60bpm 인 데 , 모 델 이 환 자의 심 장 박 동 을 100bpm 이 하 고 예 측 했 다 면 , 모 델 이 틀 린 일을 하 고 있 다는 것 을 이 야 기 할 방법 이 필 요 합 니다 . 다 른 예 로 , 모 델 이 이 메 일이 스 팸 일 가 능 성 을 점 수 로 알 려 준 다 면 , 예 측 이 틀 렸 을 때 모 델 에 게 알 려 줄 방법 이 필 요 합 니다 . 일 반 적 으 로 머 신 러 닝 의 학 습 부분 은 이 loss 함 수 를 최 소 화 하 는 것 으 로 구 성 됩 니 다 . 보 통 은 모 델 이 많 은 파 라 미 터 를 갖 습 니다 . 이 파 라 미 터 들 의 가 장 좋 은 값 은 관 찰 된 데 이 터 의 학 습 데 이 터 ( training data ) 에 대 한 loss 를 최 소 화 하 는 것 을 통 해 서 ’ 배 우 ’ 기 를 원 하 는 것 입 니다 . 불 행하 게 도 , 학 습 데 이 터 에 대 해 서 잘 하 는 것 이 ( 본 적 이 없 는 ) 테 스 트 데 이 터 에 도 잘 작 동 한 다는 것 이 보 장 되 지 않 습 니다 . 그 렇 기 때 문 에 , 우 리 는 두 값 을 추 적 해 야 합 니다 . • 학 습 오 류 ( training error ) : 학 습 데 이 터 에 대 해 loss 를 최 소 화 하 면 서 모 델 을 학 습시 킨 데 이 터 에 대 한 오 류 입 니다 . 비 유 하 자 면 , 실 제 시 험 을 준 비 하 는 학 생 이 연 습 시 험 에 대 해 서 모 두 잘 하 는 것과 동 일 합 니다 . 이 결과가 좋 긴 하 지 만 , 실 제 시 험 에 서 도 잘 본 다는 보 장은 없 습 니다 . • 테 스 트 오 류 ( test error ) : 보 지 않 은 테 스 트 셋 에 대 한 오 류 입 니다 . 학 습 오 류 와 는 상 당 히 다 를 수 있 습 니다 . 이 런 경 우 즉 보 지 않 은 데 이 터 에 대 한 일 반 화 를 실 패 한 경 우 를 우 리 는 오 버 피 팅 ( overﬁtting ) 이 라 고 합 니다 . 실 제 생 활 과 비 유 하 면 , 연 습 시 험 은 모 두 잘 했 는 데 실 제 시 험 은 망 친 것 입 니다 . 최 적 화 알 고 리 즘 마 지 막 으 로 loss 를 최 소 화 하 기 위 해 서 우 리 는 모 델 과 loss 함 수 를 사 용 해 서 loss 를 최 소 화 하 는 파 라 미 터 집 합 을 찾 는 방법 이 필 요 합 니다 . 뉴 럴 네 트 워 크 에 서 가 장 유 명 한 최 적 화 알 고 리 즘 은 gradient descent 라 고 불 리 는 방법 을 따 르 고 있 습 니다 . 간 략 하 게 말 하 면 , 각 파 라 미 터 에 대 해 서 파 라 미 터 를 조 금 바 꿨 을 때 학 습 셋 에 대 한 loss 가 어 느 방 향 으 로 움 직 이 는 지 를 봐 서 , loss 가 감 소 하 는 방 향 으 로 파 라 미 터 를 업 데 이 트 합 니다 . 지 금 부 터 는 우 리 는 머 신 러 닝 의 몇 가 지 종 류 에 대 해 서 조 금 자 세 히 살 펴 보 겠 습 니다 . 머 신 러 닝 이 할 수 있 는 것 들 을 나 열 하 는 것 으 로 시 작 합 니다 . 목 적 은 이 를 달 성 하 는 방법 에 대 한 기 술 집 합 ( 즉 , 학 습 , 데 이 터 종 류 등 ) 과 보 완 되 는 것 을 기 억 해 두 세 요 . 아 래 목 록 은 여 러 분 들 이 공 부 를 시 작 하 고 , 우 리 가 문 제 를 이 야 기 할 때 공 동 언어 를 쓸 수 있을 정 도 입 니다 . 더 많 은 문 제 는 앞 으 로 계 속 다 룰 예 정 입 니다 . 4 . 1 . 4 지 도 학 습 ( supervised learning ) 지 도 학 습 ( supervised learning ) 은 주 어 진 입 력 데 이 터 에 대 한 타 켓 ( target ) 을 예 측 하 는 문 제 를 푸 는 것 입 니다 . 타 겟 은 종종 레 이 블 ( label ) 이 라 고 불 리 고 , 기 호 로 는 y 로 표 기 합 니다 . 입 력 데 이 터 포 인 트 는 샘 플 ( sample ) 또 는 인 스 턴 스 ( instance ) 라 고 불 리 기 도 하 고 , 𝑥 로 표 기 됩 니다 . 입 력 𝑥 를 예 측 on 𝑓 𝜃 ( 𝑥 ) 로 매 핑하 는 모 델 𝑓 𝜃 을 생 성 하 는 것 이 목 표 입 니다 . 4 . 1 . 소 개 29 이 해 를 돕 기 위 해 서 예 를 들 어 보 겠 습 니다 . 여 러 분 이 의 료 분 야에 서 일을 한 다 면 , 어 떤 환 자 에 게 심 장 마 비 가 일 어 날 지 여 부 를 예 측 하 기 를 원 할 것 입 니다 . 이 관 찰 , 심 장 마 비 또 는 정 상 , 은 우 리 의 레 이 블 𝑦 가 됩 니다 . 입 력 𝑥 는 심 박 동 , 이 완 기 및 수 축 혈 압 등 바 이 탈 사 인 들 이 될 것 입 니다 . 파 라 미 터 𝜃 를 선 택 하 는 데 , 우 리 는 ( 감 독 자 ) 레 이 블 이 있 는 예 들 , ( 𝑥 𝑖 , 𝑦 𝑖 ) 을 모 델 에 게 제 공 하 기 때 문 에 감 독 이 작 동 합 니다 . 이 때 , 𝑥 𝑖 는 정 확 한 레 이 블 과 매 치 되 어 있 습 니다 . 확 률 용 어 로 는 조 건 부 확 률 𝑃 ( 𝑦 | 𝑥 ) 을 추 정 하 는 데 관 심 이 있 습 니다 . 이 것 은 머 신 러 닝 의 여 러 접 근 방법 중 에 하 나 이 지 만 , 지 도 학 습 은 실 제 로 사 용 되 는 머 신 러 닝 의 대 부분 을 설 명 합 니다 . 부분 적 으 로 , 많 은 중 요 한 작 업 들 이 몇 가 지 이 용 가 능 한 증 거가 주 어 졌 을 때 알 수 없 는 것 의 확 률 을 추 정 하 는 것 으 로 설 명 될 수 있 기 때 문 입 니다 : • CT 이 미 지 를 보 고 암 여 부 를 예 측 하 기 • 영 어 문 장 에 대 한 정 확 한 프 랑 스 어 번 역 을 예 측 하 기 • 이 번 달 의 제정 보 고 데 이 터 를 기 반 으 로 다 음 달 주 식 가격 을 예 측 하 기 ’ 입 력 으 로 부 터 타 겟 을 예 측 한 다 ’ 라 고 간 단 하 게 설 명 했 지 만 , 지 도 학 습 은 입 력 과 출 력 의 타 입 , 크 기 및 개 수 에 따 라 서 아 주 다 양 한 형 식 이 있 고 다 양 한 모 델 링 결 정 을 요 구 합 니다 . 예 를 들 면 , 텍 스 트 의 문 자 열 또 는 시 계 열 데 이 터 와 같 은 시 퀀 스 를 처 리 하 는 것과 고 정 된 백 처 표 현 을 처 리 하 는 데 다 른 모 델 을 사 용 합 니다 . 이 책 의 처 음 9 파 트 에 서 이 런 문 제 들 에 대 해 서 상 세 하 게 다 룹 니다 . 명 백 히 말 하 면 , 학 습 과 정 은 다 음 과 같 습 니다 . 예 제 입 력 을 많 이 수 집 해 서 , 임의 로 고 릅 니다 . 각각 에 대 해 서 ground truth 를 얻 습 니다 . 입 력 과 해 당 하 는 레 이 블 ( 원 하 는 결과 ) 을 합 쳐 서 학 습 데 이 터 를 구 성 합 니다 . 학 습 데 이 터 를 지 도 학 습 알 고 리 즘 에 입 력 합 니다 . 여 기 서 지 도 학 습 알 고 리 즘 ( supervised learning algorithm ) 은 데 이 터 셋 을 입 력 으 로 받 아 서 어 떤 함 수 ( 학 습 된 모 델 ) 를 결과 로 내 는 함 수 입 니 다 . 그 렇 게 얻어 진 학 습 된 모 델 을 이 용 해 서 이 전 에 보 지 않 은 새 로 운 입 력 에 대 해 서 해 당 하 는 레 이 블 을 예 측 합 니다 . 30 4 . 딥 러 닝 맛 보 기 회 귀 ( regression ) 아 마 도 여 러 분 의 머 리 에 떠 오 르 는 가 장 간 단 한 지 도 학 습 은 회 귀 ( regression ) 일 것 입 니다 . 주 택 판 매 데 이 터 베 이 스 에 서 추출 된 데 이 터 를 예 로 들 어 보 겠 습 니다 . 각 행 은 하 나 의 집 을 , 각 열 은 관 련 된 속성 ( 집 의 면 적 , 침 실 개 수 , 화 장 실 개 수 , 도 심 으 로 부 터 의 도 보 거 리 등 ) 을 갖 는 테 이 블 을 만 듭 니다 . 우 리 는 이 데 이 터 셋 의 하 나 의 행 을 속성 백 터 ( feature vector ) 라 고 부 르 고 , 이 와 연 관 된 객 체 는 예 제 ( example ) 이 라 고 부 릅 니다 . 만 약 여 러 분 이 뉴 욕 이 나 샌 프 란 시스 코 에 서 살 고 , 아 마 존 , 구 글 , 마 이 크 소 프 트 , 페 이 스 북 의 CEO 가 아 니 라 면 , 여 러 분 집 의 속성 백 터 ( 집 면 적 , 침 실수 , 화 장 실수 , 도 심 까 지 도 보 거 리 ) 는 아 마 도 [ 100 , 0 , . 5 , 60 ] 가 될 것 입 니다 . 하 지 만 , 피 츠 버 그 에 산 다 면 [ 3000 , 4 , 3 , 10 ] 와 가 까 울 것 입 니다 . 이 런 속성 백 터 는 모 든 전 통 적 인 머 신 러 닝 문 제 에 필 수 적 인 것 입 니다 . 우 리 는 일 반 적 으 로 어 떤 예 제 에 대 한 속성 백 터 를 x i 로 표 기 하 고 , 모 든 예 제 에 대 한 속성 백 터 의 집 합 은 𝑋 로 표 기 합 니다 . 결과 에 따 라 서 어 떤 문 제 가 회 귀 ( regression ) 인 지 를 결 정 됩 니다 . 새 집 을 사 기 위 해 서 부 동 산 을 돌 아 다 니 고 있 다 고 하 면 , 여 러 분 은 주 어 진 속성 에 대 해 서 합 당 한 집 가격 을 추 정 하 기 를 원 합 니다 . 타 겟 값 , 판 매 가격 , 은 실 제 숫 자 ( real number ) 가 됩 니다 . 샘 플 x i 에 대 응 하 는 각 타 겟 은 𝑦 𝑖 로 표 시 하 고 , 모 든 예 제 X 에 대 한 모 든 타 겟 들 은 y 로 적 습 니다 . 타 겟 이 어 떤 범 위 에 속 하 는 임의의 실수 값 을 갖 는 다 면 , 우 리 는 이 를 회 귀 문 제 라 고 부 릅 니다 . 우 리 의 모 델 의 목 표 는 실 제 타 겟 값 을 근 접 하 게 추 정 하 는 예 측 ( 이 경 우 에 는 집 가격 추 측 ) 을 생 성 하 는 것 입 니다 . 이 예 측 을 ˆ 𝑦 𝑖 로 표 기 합 니다 . 만 약 표 기 법 이 익 숙 하 지 않 다 면 , 다 음 장 들 에 서 더 자 세 히 설 명 할 것 이 기 때 문 에 지 금 은 그 냥 무 시 해 도 됩 니다 . 많 은 실 질 적 인 문 제 들 이 잘 정 의 된 회 귀 문 제 들 입 니다 . 관 객 이 영 화 에 줄 평 점 을 예 측 하 는 것 은 회 귀 의 문 제 인 데 , 여 러 분 이 2009 년 에 이 를 잘 예 측 하 는 대 단 한 알 고 리 즘 을 디 자인 했 다 면 $ 1 million Netﬂix prize 를 받 았 을 것 입 니다 . 환 자 가 입 원 일 수 를 예 측 하 는 것 또 한 회 귀 문 제 입 니다 . 문 제 가 회 귀 의 문 제 인 지 를 판 단 하 는 좋 은 경 험 의 법 칙 은 얼 마 나 만 큼 또 는 얼 마 나 많 이 로 대 답 이 되 는 지 보 는 것 입 니다 . • 이 수술 은 몇 시 간 이 걸 릴 까 요 ? - 회 귀 • 이 사 진 에 개가 몇 마리 있 나 요 ? - 회 귀 그 런 데 만 약 주 어 진 문 제 에 대 한 질 문 을 ‘ 이 것 은 . . . 인 가 요 ? ’ 라 고 쉽 게 바 꿀 수 있 다 면 , 분 류 의 문 제 입 니다 . 이 는 다 른 기 본 적 인 문 제 유 형 입 니다 . 머 신 러 닝 을 이 전 에 다 뤄 보 지 않 은 경 우 에 도 비 공 식 적 으 로 는 회 귀 의 문 제 들 을 다 뤄 왔 습 니다 . 예 를 들 어 , 여 러 분 의 집 의 배 수 구 를 수 리 하 고 , 수 리 공 이 𝑥 1 = 3 시 간 이 걸 려 서 하 수 관 에 서 덩 어 리를 제 거 했 습 니다 . 이 에 대 해 서 수 리 공 은 𝑦 1 = $ 350 청 구 를 합 니다 . 여 러 분 의 친 구 가 같 은 수 리 공 을 공 용 해 서 or 𝑥 2 = 2 시 간 걸 려 서 일 하 고 , 𝑦 2 = $ 250 를 청 구 했 습 니다 . 어 떤 사 람 이 하 수 관 에 서 덩 어 리를 제 거 하 는 데 비 용 이 얼 마 가 될 지 를 물 어 보 면 , 여 러 분 은 논 리 적 인 추 정 - 시 간 이 더 소 요 되 면 더 비 싸 다 - 을 할 것 입 니다 . 기 본 비 용 이 있 고 , 시 간 당 비 용 이 있을 것 이 라 고 까 지 추 정 할 것 입 니다 . 이 가 정 이 맞 다 면 , 위 두 데 이 터 포 인 트 를 활 용 해 서 수 리 공 의 가격 구 조 를 알아 낼 수 있 습 니다 : 시 간 당 100 달 러 및 기 본 비 용 50 달 러 . 여 러 분 이 여 기까 지 잘 따 라 왔 다 면 선 형 4 . 1 . 소 개 31 회 귀 에 대 한 고 차 원 의 아 이 디 어 를 이 미 이 해한 것 입 니다 . ( 선 형 모 델 을 bias 를 사 용 해 서 디 자인 했 습 니 다 . ) 위 예 에 서 는 수 리 공 의 가격 을 정 확 하 게 계 산 하 는 파 라 미 터 를 찾 아 낼 수 있 었 습 니다 . 때 로 는 불 가 능 한 데 , 예 를 들 면 만 약 어 떤 차 이 가 이 두 피 쳐 외 에 작 용 하 는 경 우 가 그 렇 습 니다 . 그 런 경 우 에 는 우 리 는 우 리 의 예 측 과 관 찰 된 값 의 차 이 를 최 소 화 하 는 모 델 을 학 습시 키 고 자 노 력 합 니다 . 대 부분 장 들 에 서 우 리 는 아 주 일 반 적 인 loss 둘 중 에 하 나 에 집중 할 것 입 니다 . 하 나 는 L1 loss 로 , 다 음 과 같고 , 𝑙 ( 𝑦 , 𝑦 ′ ) = ∑︁ 𝑖 | 𝑦 𝑖 − 𝑦 ′ 𝑖 | L2 loss where 다 른 하 나 는 최 소 평 균 제 곱 손 실 ( least mean square loss ) , 즉 L2 loss 입 니다 . 이 는 다 음 과 같 이 표 기 됩 니다 . 𝑙 ( 𝑦 , 𝑦 ′ ) = ∑︁ 𝑖 ( 𝑦 𝑖 − 𝑦 ′ 𝑖 ) 2 . 나 중 에 보 겠 지 만 , 𝐿 2 loss 는 우 리 의 데 이 터 가 가 우 시 안 노 이 즈 에 영 향 을 받 았 다 고 가 정 에 관 련 이 되 고 , 𝐿 1 loss 는 라 플 라 스 분 포 ( Laplace distribution ) 의 노 이 즈 를 가 정 합 니다 . 분 류 ( classiﬁcation ) 회 귀 모 델 은 얼 마 나 많 이 라 는 질 문 에 답 을 주 는 데 는 훌 륭 하 지 만 , 많 은 문 제 들 이 이 템 플 렛 에 잘 들 어 맞 지 않 습 니다 . 예 를 들 면 , 은 행 이 모 바 일 앱에 수 표 스 캐 닝 기 능 을 추 가 하 고 자 합 니다 . 이 를 위 해 서 고객 은 스 마 트 폰 의 카 메 라 로 수 표 를 찍 으 면 , 이 미 지 에 있 는 텍 스 트 를 자 동 으 로 이 해하 는 기 능 을 하 는 머 신 러 닝 모 델 이 필 요 합 니다 . 손 으 로 쓴 글 씨에 더 잘 동 작을 해 야 할 필 요 가 있 습 니다 . 이 런 시스 템 은 문 자인 식 ( OCR , optical character recognition ) 이 라 고 하 고 , 이 것 이 풀 려 는 문 제 의 종 류를 분 류 라 고 합 니다 . 회 귀 문 제 에 사 용 되 는 알 고 리 즘 과 는 아 주 다 른 알 고 리 즘 이 이 용 됩 니다 . 분 류 는 이 미 지 의 픽 셀 값과 같 은 속성 백 터 를 보 고 , 그 예 제 가 주 어 진 종 류 들 중 에 서 어 떤 카 테 고 리 에 속 하 는 지 를 예 측 합 니다 . 손 으 로 쓴 숫 자의 경 우 에 는 숫 자 0 부 터 9 까 지 10 개 의 클 래 스 가 있 습 니다 . 가 장 간 단 한 분 류 의 형 태 는 단 두 개 의 클 래 스 가 있 는 경 우 로 , 이 를 이 진 분 류 ( binary classiﬁcatio ) 이 라 고 부 릅 니다 . 예 를 들 어 , 데 이 터 셋 𝑋 가 동 물 들 의 사 진 이 고 , 이 에 대 한 레 이 블 𝑌 이 { 고 양 이 , 강 아 지 } 인 경 우 를 들 수 있 습 니다 . 회 귀 에 서 는 결과가 실수 값 ˆ 𝑦 가 되 지 만 , 분 류 에 서 는 결과가 예 측 된 클 래 스 인 분 류 기 를 만 들 고 자 합 니다 . 이 책 에 서 더 기 술 적 인 내 용 을 다 룰 때 , 고 정 된 카 테 고 리 - 예 를 들 면 고 양 이 또 는 개 - 에 대 한 결과 만 을 예 측 하 는 모 델 을 최 적 화 하 는 것 은 어 려 워 질 것 입 니다 . 대 신 확 률 에 기 반 한 모 델 로 표 현 하 는 것 이 훨 씬 32 4 . 딥 러 닝 맛 보 기 더 쉽습 니다 . 즉 , 예 제 𝑥 가 주 어 졌 을 때 , 모 델 은 각 레 이 블 𝑘 에 확 률 ˆ 𝑦 𝑘 를 할 당 하 는 것 입 니다 . 결과가 확 률 값 이 기 때 문 에 모 두 양 수 이 고 , 합 은 1 이 됩 니다 . 이 는 𝐾 개 의 카 테 고 리 에 대 한 확 률 을 구 하 기 위 해 서 는 𝐾 − 1 개 의 숫 자 만 필 요 하 다는 것 을 의 미 합 니다 . 이 진 분 류를 예 로 들 어 보 겠 습 니다 . 공 정 하 지 않 은 동 전 을 던 져 서 앞 면 이 나 올 확 률 이 0 . 6 ( 60 % ) 라 면 , 뒷 면 이 나 올 확 률 은 0 . 4 ( 40 % ) 다 됩 니다 . 동 물 분 류 의 예 로 돌 아 가 보 면 , 분 류 기 는 이 미 지 를 보 고 이 미 지 가 고 양 이일 확 률 Pr ( 𝑦 = cat | 𝑥 ) = 0 . 9 을 출 력 합 니다 . 우 리 는 이 숫 자 를 이 미 지 가 고 양 이 를 포 할 것 이 라 고 90 % 정 도 확 신 한 다 라 고 해 석 할 수 있 습 니다 . 예 측 된 클 래 스 에 대 한 확 률 의 정 도 는 신 뢰 에 대 한 개 념 을 나 타 냅 니다 . 신 뢰 의 개 념 일 뿐 만 아 니 라 , 고 급 내 용 을 다 루 는 장 에 서 는 여 러 비 신 뢰 의 개 념 도 논 의 하 겠 습 니다 . 두 개 보 다 많 은 클 래 스 가 있을 경 우 에 우 리 는 이 문 제 를 다 중 클 래 스 분 류 ( multiclass classiﬁcation ) 이 라 고 합 니다 . 흔 한 예 로 는 손 으 로 쓴 글 씨 - [ 0 , 1 , 2 , 3 . . . 9 , a , b , c , . . . ] - 를 인 식 하 는 예 제 가 있 습 니다 . 우 리 는 회 귀 문 제 를 풀 때 L1 또 는 L2 loss 함 수 를 최 소 화 하 는 시 도 를 했 는 데 , 분 류 문 제 에 서 cross - entropy 함 수 가 흔히 사 용 되 는 loss 함 수 는 입 니다 . MXNet Gluon 에 서 는 관 련 된 loss 함 수 에 대 한 내 용 을 여 기 에 서 볼 수 있 습 니다 . 가 장 그 럴 듯 한 클 래 스 가 결 정 을 위 해 서 사 용 하 는 것 이 꼭 아 닐 수 도 있 습 니다 . 여 러 분 의 뒷 뜰 에 서 이 아 름 다 운 버 섯 을 찾 는다 고 가 정 해 보 겠 습 니다 . 4 . 1 . 소 개 33 알 광 대 버 섯 ( death cap ) - 먹 지 마 세 요 ! 34 4 . 딥 러 닝 맛 보 기 자 , 사 진 이 주 어 졌 을 때 버 섯 이 독 이 있 는 것 인 지 를 예 측 하 는 분 류 기 를 만 들 어 서 학 습 했 다 고 가 정 합 니다 . 우 리 의 독 버 섯 탐 기 분 류 기 의 결과가 Pr ( 𝑦 = deathcap | image ) = 0 . 2 로 나 왔 습 니다 . 다 르 게 말 하 면 , 이 분 류 기 는 80 % 확 신 을 갖고 이 버 섯 이 알 광 대 버 섯 ( death cap ) 이 아 니다 라 고 말 하 고 있 습 니다 . 하 지 만 , 이 것 을 먹 지 는 않 을 것 입 니다 . 이 버 섯 으 로 만 들 어 질 멋 진 저 녁 식 사 의 가 치 가 독 버 섯 을 먹 고 죽 을 20 % 의 위 험 보 다 가 치 가 없 기 때 문 입 니다 . 이 것 을 수 학 적 으 로 살 펴 보 겠 습 니다 . 기 본 적 으 로 우 리 는 예 상 된 위 험 을 계 산 해 야 합 니다 . 즉 , 결과 에 대 한 확 률 에 그 결과 에 대 한 이익 ( 또 는 손 해 ) 를 곱 합 니다 . 𝐿 ( action | 𝑥 ) = E 𝑦 ∼ 𝑝 ( 𝑦 | 𝑥 ) [ loss ( action , 𝑦 ) ] 따 라 서 버 섯 을 먹 을 경 우 우 리 가 얻 는 loss 𝐿 은 𝐿 ( 𝑎 = eat | 𝑥 ) = 0 . 2 * ∞ + 0 . 8 * 0 = ∞ 인 반 면 에 , 먹 지 않 을 경 우 cost 또 는 loss 는 𝐿 ( 𝑎 = discard | 𝑥 ) = 0 . 2 * 0 + 0 . 8 * 1 = 0 . 8 이 됩 니다 . 우 리 의 주 의 깊 음이 옳 았 습 니다 . 균 학 자 들 은 위 버 섯 이 실 제 로 독 버 섯 인 알 광 대 버 섯 이 라 고 알 려 줄 것 이 기 때 문 입 니다 . 분 류 문 제 는 이 진 분 류 보 다 복 잡 해 질 수 있 습 니다 . 즉 , 다 중 클 래 스 분 류 문 제 이 거 나 더 나 아 가 서 는 다 중 레 이 블 분 류 의 문 제 일 수 있 습 니다 . 예 를 들 면 , 계 층 을 푸 는 분 류 의 종 류 들 이 있 습 니다 . 계 층 은 많 은 클 래 스 들 사 이 에 관 계가 있 는 것 을 가 정 합 니다 . 따 라 서 , 모 든 오 류 가 동 일 하 지 않 습 니다 . 즉 , 너 무 다 른 클 래 스 로 예 약 하 는 것 보 다는 관 련 된 클 래 스 로 예 측 하 는 것 을 더 선 호 합 니다 . 이 런 문 제 를 계 층 적 분 류 ( hierarchical classiﬁcation ) 이 라 고 합 니다 . 계 층 적 분 류 의 오 랜 예 는 Linnaeus 가 동 물 을 계 층 으 로 분 류 한 것 을 들 수 있 습 니다 . 4 . 1 . 소 개 35 동 물 분 류 의 경 우 푸 들 을 슈 나 이 저 라 고 실수 로 분 류 하 는 것 이 그 렇 게 나 쁘 지 않 을 수 있 지 만 , 푸 들 을 공 룡 이 라 고 분 류 한 다 면 그 영 향 이 클 수 도 있 습 니다 . 어 떤 계 층 이 적절 할 지 는 여 러 분 이 모 델 을 어 떻 게 사 용 할 것 인 지 에 달 려 있 습 니다 . 예 를 들 면 , 딸 랑 이 뱀 ( rattle snake ) 와 가 터 스 뱀 ( garter snake ) 은 계 통 트 리 에 서 는 가 까 울 수 있 지 만 , 딸 랑 이 뱀 을 가 터 스 뱀 으 로 잘 못 분 류 한 결과 는 치 명 적 일 수 있 기 때 문 입 니다 . 36 4 . 딥 러 닝 맛 보 기 태 깅 ( tagging ) 어 떤 분 류 의 문 제 는 이 진 또 는 다 중 클 래 스 분 류 형 태 로 딱 떨 어 지지 않 습 니다 . 예 를 들 자 면 , 고 양 이 와 강 아 지 를 구 분 하 는 정 상 적 인 이 진 분 류 기 를 학 습시 킬 수 있 습 니다 . 현 재의 컴 퓨 터 비 전 의 상 태 를 고 려 하 면 , 이 는 상 용 도 구 을 이 용 해 서 도 아 주 쉽 게 할 수 있 습 니다 . 그 럼 에 도 불 구 하 고 , 우 리 의 모 델 이 얼 마 나 정 확 하 든 지 상 관 없 이 브 레 맨 음 악 대 의 사 진지 주 어 진 다 면 문 제 가 발 생 할 수 도 있 습 니다 . 4 . 1 . 소 개 37 38 4 . 딥 러 닝 맛 보 기 사 진 에 는 고 양 이 , 수 닭 , 강 아 지 , 당 나 귀 그 리 고 배 경 에 는 나 무 들 이 있 습 니다 . 우 리 의 모 델 을 이 용 해 서 주 로 무 엇 을 할 것 인 지 에 따 라 서 , 이 문 제 를 이 진 분 류 의 문 제 로 다 룰 경 우 소 용 이 없어 질 수 있 습 니다 . 대 신 , 우 리 는 모 델 이 이 미 지 에 고 양 이 , 강 아 지 , 당 나 귀 그 리 고 수 닭 이 있 는 것 을 알 려 주 도 록 하 고 싶 을 것 입 니다 . 서 로 배 타 적 이 아 닌 ( not mutually exclusive ) 아 닌 클 래 스 들 을 예 측 하 는 문 제 를 멀 티 - 레 이 블 분 류 라 고 합 니다 . 자 동 태 깅 문 제 가 전 형 적 인 멀 티 레 이 블 분 류 문 제 입 니다 . 태 그 의 예 는 기 술 문 서 에 붙 이 는 태 그 - 즉 , ‘ 머 신 러 닝 ’ , ‘ 기 술 ’ , ‘ 가 젯 ’ , ‘ 프 로 그 램 언어 ’ , ‘ 리 눅 스 ’ , 클 라 우 드 컴 퓨 팅 ’ , ‘AWS’ - 를 생 각 해 봅 시 다 . 일 반 적 으 로 기 사 는 5 - 10 개 태 그 를 갖 는 데 , 그 이유 는 태 그 들 이 서 로 관 련 이 있 기 때 문 입 니다 . ‘ 클 라 우 드 컴 퓨 팅 ’ 에 대 한 글 은 ’AWS’ 를 언 급 할 가 능 성 이 높 고 , ’ 머 신 러 닝 ’ 관 련 글 은 ’ 프 로 그 램 언어 ’ 와 관 련 된 것 일 수 있 습 니다 . 우 리 는 연 구 자 들 이 리 뷰 를 많 이 할 수 있 도 록 하 기 위 해 서 올 바 른 태 그 를 다는 것 이 중 요 한 생 물 의 학 문 헌 을 다 룰 때 이 런 문 제 를 다 뤄 야 합 니다 . 의 학 국 립 도 서 관 에 는 많 은 전 문 주 석 자 들 이 PubMed 에 색 인 된 아 티 클 들 을 하 나 씩 보 면 서 MeSH ( 약 28 , 000 개 태 그 의 집 합 ) 중 에 관 련 된 태 그 를 연 관 시 키 는 일을 하 고 있 습 니다 . 이 것 은 시 간 이 많 이 소 모 되 는 일 로 서 , 주 석 자 들 이 태 그 를 다는 데 는 보 통 1 년 이 걸 립 니다 . 머 신 러 닝 을 사 용 해 서 임 시 태 그 를 달 고 , 이 후 에 매 뉴 얼 리 뷰 를 하 는 것 이 가 능 합 니다 . 실 제 로 몇 년 동 안 BioASQ 에 서 는 이 에 대 한 대 회 를 열었었 습 니다 . 검 색 ( search ) 과 랭 킹 ( ranking ) 때 로 는 각 예 제 들 에 대 해 서 어 떤 클 래 스 또 는 실 제 값 을 할 당 하 는 것 만 을 원 하 지 않 습 니다 . 정 보 검 색 분 야 의 경 우 에 는 아 이 템 집 합 에 순 위 를 매 기 고 싶 어 합 니다 . 웹 검 색 을 예 로 들 어 보 면 , 목 표 는 특 정 페 이 지 가 쿼 리 에 관 련 이 있 는 지 여 부 를 판 단 하 는 것 보 다는 검 색 결과 들 중 에 어 떤 것 이 사 용 자 에 게 먼 저 보 여 줘 야 하 는 것 에 있 습 니다 . 관 련 검 색 결과 의 순 서 에 대 해 서 관 심 이 많 고 , 우 리 의 러 닝 알 고 리 즘 은 큰 집 합 의 일 부 에 대 한 순 서 를 매 길 수 있 어야 합 니다 . 즉 , 알 파 벳 에 서 처 음 5 개 글 자 가 무 엇 인 지 를 물 어 봤 을 경 우 , A B C D E 를 결과 로 주 는 것과 C A B E D 를 결과 로 주 는 것 에 는 차 이 가 있 습 니다 . 결과 집 합 은 같 은 경 우 라 도 , 집 합 안에 서 순 서 도 중 요 합 니다 . 이 문 제 에 대 한 가 능 한 해 결 방법 은 가 능 한 집 합 의 원 소 들 에 관 련 성 점 수 를 부 여 하 고 , 점 수 가 높 은 항 목 들 을 검 색 하 는 것 입 니다 . PageRank 가 관 련 성 점 수 를 적 용 한 예 로 , 특 성 중 하 나 는 이 것 은 실 제 쿼 리 에 의 존 하 지 않 는다는 것 입 니다 . 대 신 , 쿼 리 단 어 들 을 포 함한 결과 들 을 순 서 를 부 여 하 는 것 을 합 니다 . 요 즘 의 검 색 엔 진 은 머 신 러 닝 과 행 동 모 델 을 이 용 해 서 쿼 리 와 관 련 된 관 련 성 점 수 를 얻 습 니다 . 이 주제 만 다 루 는 컨 퍼 런 스 가 있 습 니다 . 4 . 1 . 소 개 39 추 천 시스 템 추 천 시스 템 은 검 색 과 랭 킹 과 관 련 된 또 다 른 문 제 세 팅 입 니다 . 사 용 자 에 게 관 련 된 상 품 을 보 여 주 는 것 이 목 표 이 기 에 문 제 는 비 스 합 니다 . 주 요 차 이 점 은 추 천 시스 템 에 서 는 특 정 사 용 자 에 대 한 개 인 화 ( personalization ) 를 중 점 으 로 한 다는 것 입 니다 . 예 를 들 어 , 영 화 추 천 의 경 우 에 는 SciFi 에 대 한 결과 페 이 지 와 우 디 엘 런 코 미 디 에 대 한 결과 페 이 지 가 아 주 다 르 게 나 옵 니다 . 이 런 문 제 는 영 화 , 제 품 또 는 음 악 추 천 에 서 발 생 합 니다 . 어 떤 경 우 에 는 고객 은 얼 마 나 그 제 품 을 좋 아 하 는 지 를 직 접 알 려 주 기 도 합 니다 ( 예 를 들 면 아 마 존 의 제 품 리 뷰 ) . 어 떤 경 우 에 는 결과 에 만 족 하 지 못 한 경 우 피 드 백 을 간 단 하 게 주 기 도 합 니다 ( 재 생 목 록 의 타 이 틀 을 건 너 뛰 는 형 식 으 로 ) . 일 반 적 으 로 는 이 런 시스 템 은 어 떤 점 수 𝑦 𝑖𝑗 를 예 측 하 고 자 하 는 데 , 이 예 측 은 사 용 자 𝑢 𝑖 와 제 품 𝑝 𝑗 가 주 어 졌 을 때 예 상 된 평 점 또 는 구 매 확 률 이 될 수 있 습 니다 . 이 런 모 델 은 어 떤 사 용 자 에 대 해 서 가 장 큰 점 수 𝑦 𝑖𝑗 를 갖 는 객 체 들 의 집 합 을 찾 아 주 는 데 , 이 것 이 추 천 으 로 사 용 됩 니다 . 운영 시스 템 은 매 우 복 잡 하 고 , 점 수 를 계 산 할 때 자 세 한 사 용 자의 활 동 과 상 품 의 특 징 까 지 고 려 합 니다 . 아 래 이 미 지 는 아 마 존 이 저 자 들 의 관 심 을 반 영 한 개 인 화 알 고 리 즘 을 기 반 으 로 아 마 존 이 추 천 한 딥 러 닝 책 들 의 예 입 니다 . 40 4 . 딥 러 닝 맛 보 기 4 . 1 . 소 개 41 시 퀀 스 러 닝 ( sequence learning ) 지 금까 지 는 고 정 된 개 수 의 입 력 을 받 아 서 고 정 된 개 수 의 결과 를 출 력 하 는 문 제 를 봤 습 니다 . 면 적 , 침 실 개 수 , 욕 실 개 수 , 다 운 타 운 까 지 도 보 거 리 와 같 은 고 정 된 특 성 들 로 부 터 주 택 가격 을 예 측 하 는 것 을 고 려 하 기 앞 서서 , ( 고 정 된 차 원 의 ) 이 미 지 를 고 정 된 수 의 클 래 스 들 에 속 할 예 측 된 확 률 로 매 핑하 는 것 , 사 용 자 ID 와 제 품 ID 를 받 아 서 별 점 수 를 예 측 하 는 문 제 들 도 논 의 햇 습 니다 . 이 경 우 들 은 , 우 리 가 고 정 된 길 이의 입 력 을 모 델 에 넣 어 서 결과 를 얻 으 면 , 모 델 은 무 엇 을 봤 는 지 바 로 잊 어 버 립 니다 . 만 약 입 력 이 정 말 로 모 두 같 은 차 원 을 갖거 나 , 연 속 된 입 력 들 이 서 로 관 련 이 아 무 런 관 련 이 없 을 경 우 에 는 문 제 가 없 습 니다 . 하 지 만 , 비 디 오 영 상 의 단 편 을 다 뤄 야 한 다 면 어 떨 까 요 ? 이 경 우 에 는 각 단 편 은 서 로 다 른 여 러 프 레 임 들 로 구 성 되 어 있을 거 십 니다 . 각 프 레 임 에 서 무 엇 이 일 어 나 고 있 는 지 에 대 한 추 측 은 이 전 또 는 이 후 프 레 임을 고 려 할 경 우 에 더 확 실 할 것 입 니다 . 언어 도 마 찬 가 지 입 니다 . 기 계 번 역 은 유 명 한 딥 러 닝 문 제 들 중 에 하 나 입 니다 : 이 는 , 어 떤 언어 의 문 장을 받 아 서 , 다 른 언어 로 번 역 을 추 측 하 는 것 입 니다 . 이 와 같 은 문 제 는 의 학 에 서 도 찾 을 수 있 습 니다 . 우 리 는 중 환 자 실 의 환 자 를 모 니 터 링 하 면 서 , 24 시 간 안에 생 명 에 대 한 위 험 은 어 느 정 도 를 넘 을 경 우 경고 를 발 생 하 는 모 델 을 원 할 수 있 습 니다 . 당 연 히 이 모 델 이 사 용 했 던 지 난 몇 시 간 동 안 의 기 록 을 버 리 고 , 오 직 가 장 최 근 의 기 록 만 을 사 용 해 서 예 측 을 하 는 것 을 원 하 지 는 않 을 것 입 니다 . 머 신 러 닝 의 아 주 흥 미 로 운 응 용 들 이 이 런 문 제 들 에 속 합 니다 . 이 런 문 제 들 은 시 퀀 스 러 닝 ( sequence learning ) 의 예 들 입 니다 . 입 력 시 퀀 스 들 을 받 거 나 , 출 력 시 퀀 스 를 생 성 하 는 ( 또 는 모 두 ) 모 델 이 필 요 합 니다 . 종종 우 리 는 이 런 문 제 들 을 seq2seq 문 제 라 고 합 니다 . 언어 번 역 은 seq2seq 문 제 입 니다 . 음 성 으 로 부 터 텍 스 트 를 추출 하 는 것 또 한 seq2seq 문 제 입 니다 . 시 퀀 스 변 환 ( sequence transformation ) 의 모 든 종 류를 고 려 하 기 는 어 렵 지 만 , 특 별 한 몇 가 지 사 례 는 여 기 서 언 급 할 가 치 가 있 습 니다 . 태 깅 ( Tagging ) 과 파 싱 ( Parsing ) 이 것 은 텍 스 트 시 퀀 스 에 속성 들 로 주 석 을 다는 것 입 니다 . 이 때 , 입 력 과 출 력 의 개 수 가 정 확 하 게 같 습 니다 . 예 를 들 면 , 동 사 와 주 어 가 어 디 에 있 는 지 를 알 기 를 원 합 니다 . 또 는 , 어 떤 단 어 가 이 름 을 갖 는 개 체 인 지 를 알 고 자 할 수 있 습 니다 . 일 반 적 으 로 , 이 런 예 들 에 서 는 구 조 나 문 법 적 인 추 정 에 근 거 해 서 분 해 ( decompose ) 를 하 고 주 석 을 다는 것 이 목 표 입 니다 . 다 음은 문 장이 주 어 졌 을 때 어 떤 단 어 가 이 름 을 갖 는 개 체 를 가 르 키 는 지 를 태 그 로 주 석 을 다는 아 주 간 단 한 예 제 입 니다 . Tom has dinner in Washington with Sally . Ent • • • Ent • Ent 42 4 . 딥 러 닝 맛 보 기 자 동 음 성 인 식 ( automatic speech recognition ) 음 성 인 식 에 서 는 입 력 시 퀀 스 𝑥 는 화 자의 음 성 이 고 , 출 력 𝑦 는 화 자 가 말 한 원 문 의 기 록 입 니다 . 텍 스 트 보 다 오 디 오 프 레 임의 수 가 훨 신 더 많 다는 점 이 챌 린 지 입 니다 . 즉 , 수 천 개 의 오 디 오 샘 플 이 하 나 의 발 화 된 단 어에 해 당 되 기 때 문 에 , 오 디 오와 텍 스 트 의 1 : 1 대 응이 없 습 니다 . - D - e - e - p - L - ea - r - ni - ng - 텍 스 트 를 음 성 으 로 변 환 하 기 ( Text to Speech ) Text - to - Speech ( TTS ) 는 음 성 인 식 과 정 반 대 입 니다 . 즉 , 입 력 𝑥 는 텍 스 트 이 고 , 출 력 𝑦 는 오 디 오 파 일 입 니다 . 이 경 우 , 출 력 은 입 력 보 다 훨 신 깁 니다 . 사 람 이 잘 못 된 음 성 파 일을 알아 내 는 것 은 쉽 지 만 , 4 . 1 . 소 개 43 컴 퓨 터 에 게 는 쉬 운 일이 아 닙니다 . 기 계 번 역 ( Machine Translation ) 대 응 하 는 입 력 과 출 력 이 같 은 순 서 인 음 성 인 식 과 는 다 르 게 , 기 계 번 역 의 경 우 순 서 가 뒤 바 뀌 는 것 이 중 요 할 수 있 습 니다 . 즉 , 하 나 의 시 퀀 스 를 다 른 시 퀀 스 로 바 꾸 는 일을 하 지 만 , 입 력 과 출 력 의 개 수 나 대 응 하 는 데 이 터 포 인 트 들 의 순 서 가 같 다 고 가 정 하 지 않 습 니다 . 동 사 를 문 장의 맨 끝 에 놓 는 독 일인의 경 향 에 대 한 예 제 를 생 각 해 보 겠 습 니다 . 독 일 어 Haben Sie sich schon dieses grossartige Lehrwerk angeschaut ? 영 어 Did you already check out this excellent tutorial ? 잘 못 된 배 치 Did you yourself already this excellent tutorial looked - at ? 비 슷 한 문 제 는 아 주 많 이 존 재 합 니다 . 예 를 들 면 , 사 용 자 가 웹 페 이 지 를 어 떤 순 서 로 읽 는 지 를 결 정 하 는 것 은 2 차 원 적 인 레 이 아 웃 분 석 문 제 입 니다 . 비 슷 하 게 대 화 문 제 에 서 는 세 상 에 대 한 지 식 과 이 전 상 태 를 고 려 해 야 합 니다 . 이 것 들 은 활 발 한 연 구 영 역 입 니다 . 4 . 1 . 5 비 지 도 학 습 ( Unsupervised learning ) 지 금까 지 모 든 예 제 들 은 지 도 학 습 ( supervised learning ) 과 관 련 된 것 들 였 습 니다 . 즉 , 모 델 에 예 제 들 과 관 련 된 타 겟 값 들 을 입 력 하 는 학 습 였 습 니다 . 지 도 학 습 을 굉 장 히 특 화 된 일 과 매 우 분 석 적 인 상사 를 갖 는 것 으 로 생 각 할 수 있 습 니다 . 상사 는 여 러 분 의 어 깨 넘 어에 서 모 든 상 황 마 다 정 확 히 무 엇 을 해 야 하 는 지 를 알 려 주 며 , 이 는 여 러 분 이 상 황 을 행 동 으 로 연 결 하 는 것 을 배 울 때 까 지 지 속 됩 니다 . 그 런 상사 와 일 하 는 것 은 아 주 귀 찮 은 것 입 니다 . 다 른 한 편 으 로 는 이 런 상사 를 기 쁘 게 만 드 는 것 은 쉽습 니 다 . 패 턴 을 가 능 한 빨 리 인 지 해 서 그 들 의 행 동 을 모 방 하 면 됩 니다 . 이 와 는 완 전 히 반 대 인 경 우 , 즉 여 러 분 이 무 엇 을 해 야 할 지 를 전 혀 모 르 는 상사 와 일 하 는 것 이 실 망 스 러 울 수 있 습 니다 . 하 지 만 , 여 러 분 이 데 이 터 과 학 자 가 되 기 를 계 획 하 고 있 다 면 , 이 에 익 숙 해 져 야 합 니다 . 여 러 분 의 상사 는 엄 청 나 게 많 은 데 이 터 를 주 면 서 , 이 것 으 로 데 이 터 과 학 을 좀 해 봐 ! 라 고 할 수 도 있 습 니다 . 이 것 은 모 호 하 기 때 문 에 , 모 호 하 게 들 립 니다 . 이 런 문 제 종 류를 우 리 는 비 지 도 학 습 ( unsupervised learning ) 이 라 고 하 며 , 우 리 가 물 을 수 있 는 질 문 의 종 류 와 수 는 우 리 들 의 창 의 성 에 달 려 있 습 니다 . 다 음 장 들 에 서 다 양 한 비 지 도 학 습 기 법 에 대 해 서 알아 볼 예 정 이 나 , 여 러 분 의 궁 금 증 을 달 래 주 기 위 해 서 , 여 러 분 이 물 을 몇 가 지 질 문 들 에 대 해 서 설 명 하 겠 습 니다 . • 데 이 터 를 정 확 하 게 요 약 하 는 작은 개 수 의 프 로 토 타 입을 찾 을 수 있을 까 요 ? 사 진 들 이 주 어 졌 을 때 , 사 진 들 을 풍 경 사 진 , 강 아 지 사 진 , 아 기 들 , 고 양 이 들 , 산 정 상 등 으 로 그 룹 을 나 눌 수 있을 44 4 . 딥 러 닝 맛 보 기 까 요 ? 비 슷 하 게 , 사 용 자의 브 라 우 징 행 동 들 에 대 한 데 이 터 가 주 어 졌 을 때 , 비 슷 한 행 동 을 하 는 사 용 자 들 로 그 룹 을 나 눌 수 있 나 요 ? 이 런 문 제 는 일 반 적 으 로 클 러 스 터 링 ( clustering ) 이 라 고 합 니다 . • 데 이 터 에 대 해 서 관 련 있 는 특 성 을 정 확 하 게 포 착 하 는 몇 개 의 파 라 미 터 들 찾 을 수 있을 까 요 ? 공 의 궤 적 은 공 의 속 도 , 직 경 , 질 량 으 로 아 주 잘 설 명 됩 니다 . 재 봉 사 는 옷 을 맞 출 목 적 에 따 라 서 사 람 몸 모 양 을 꽤 정 확 하 게 설 명 하 는 몇 개 의 파 라 미 터 를 만 들 었 습 니다 . 이 문 제 들 은 부분 공 간 추 정 ( subspace estimation ) 문 제 로 알 려 져 있 습 니다 . 의 존 이 선 형 인 경 우 에 는 우 리 는 이 를 주 성 분 분 석 ( principal component analysis ) 이 라 고 합 니다 . • 유 클 리 디 인 공간 에 서 ( 임의 로 조 직 화 된 ) 객 체 에 대 해 서 심 볼 릭 성 질 이 잘 일 치 하 는 표 현 이 있 나 요 ? 이 는 표 현 학 습 ( representation learning ) 이 라 고 불 리 며 , 엔 터 티 들 과 그 것 들 의 관 계 를 설 명 하 는 데 사 용 돕 니다 . 예 를 들 면 , Rome - Italy + France = Paris . • 우 리 가 관 찰 한 많 은 양 의 데 이 터 에 대 한 주 원 인 ( root cause ) 에 대 한 설 명 이 있 나 요 ? 예 를 들 면 , 주 택 가격 , 공 해 , 범 죄 , 위 치 , 교 육 , 급 여 등 에 대 한 인 구 통 계 학 데 이 터 가 있을 때 , 단 순 히 경 험 적 인 데 이 터 를 기 반 으 로 이 것 들 이 어 떻 게 연 관 되 어 있 는 지 를 찾 을 수 있 나 요 ? 방 향 성 그 래 프 모 델 ( directed graphical model ) 과 인 과 관 계 ( causality ) 가 이 것 을 다 룹 니다 . • Generative adversarial network 는 최 근 에 개 발 된 중 요 하 고 흥 미 로 운 네 트 워 크 입 니다 . 기 본 적 으 로 는 이 것 은 데 이 터 를 합 성 하 는 단 계 적 인 방법 입 니다 . 근 본 적 인 통 계 적 메 카 니 즘 은 실 제 데 이 터 와 가 짜 데 이 터 가 같 은 지 를 점 검 하 는 테 스 트 들 입 니다 . 나 중 에 몇 개 의 노 트 북 에 걸 쳐 서 살 펴 보 도 록 하 겠 습 니다 . 4 . 1 . 6 환 경과 상 호 작 용 하 기 지 금까 지 는 데 이 터 가 실 제 로 어 디 서 왔 는 지 또 는 머 신 러 닝 모 델 이 결과 를 만 들 때 실 제 로 어 떤 일이 일 어 나 는 지 를 논 의 하 지 않았 습 니다 . 그 이유 는 지 도 학 습 과 비 지 도 학 습 은 이 런 이 슈 를 아 주 복 잡 한 방법 으 로 해 결 하 지 않 기 때 문 입 니다 . 둘 중 어 떤 경 우 에 도 , 우 리 는 많 은 양 의 데 이 터 를 받 아 서 , 환 경과 상 호 작 용 을 하 지 않 고 패 턴 인 식 을 수 행합 니다 . 이 런 모 든 학 습 은 알 고 리 즘 이 환 경 와 분 리 된 상 태 에 서 일 어 나 기 때 문 에 , 이 를 오 프 라 인 학 습 ( ofﬂine learning ) 이 라 고 합 니다 . 지 도 학 습 의 경 우 , 프 로 세 스 는 다 음 과 같 습 니다 . 4 . 1 . 소 개 45 오 프 라 인 학 습 의 간결 함 은 매 력 적 입 니다 . 좋 은 점 은 다 른 문 제 들 을 고 려 할 필 요 없 는 격 리 된 상 태 에 서 패 턴 인 식 을 하 면 된 다는 것 이 지 만 , 단 점 은 문 제 를 공 식 화 ( problem formulation ) 하 는 것 이 아 주 제 한 적 이 라 는 것 입 니다 . 여 러 분 이 더 의 욕 적 이 거 나 , Asimov’s Robot Series 를 읽으 면 서 자 랐 다 면 , 예 측 을 할 뿐 만 아 니 라 세 상 에 서 행 동 을 취 할 수 있 는 인 공 지 능 봇 을 떠 올 릴 수 도 있 습 니다 . 이 는 단 지 예 측 을 하 는 것 이 아 니 라 행 동 을 선 택 하 는 것 을 생 각 해 봐 야 한 다는 것 을 뜻 합 니다 . 나 아 가 예 측 과 는 달 리 행 동 은 실 제 로 환 경 에 영 향 을 미 칩 니다 . 우 리 가 지 능 적 인 에 이 전 트 를 학 습시 키 기 를 원 한 다 면 , 행 동 이 에 이 전 트 의 미 래 관 찰 에 미 치 는 영 향 에 대 해 서 도 설 명 해 야 합 니다 . 환 경과 의 상 호 작 용 을 고 려 하 는 것 은 완 전 히 새 로 운 모 델 링 에 대 한 질 문 을 가 져 옵 니다 . 환 경 이 , • 우 리 가 이 전 에 행한 것 을 기 억 하 나 요 ? • 우 리를 돕 기 를 원 하 나 요 ? 즉 , 사 용 자 가 음 성 인 식 기 에 텍 스 트 를 읽 는 경 우 . • 우 리를 이 기기 를 원 하 나 요 ? 즉 , 스 팸 필 터 ( 스 팸 발 송 자 에 대 항하 는 ) 같 은 적 대 적 설 정 또 는 ( 상 대 와 ) 게 임을 플 레 이 하 는 것 . • not care ( as in most cases ) ? • 변 하 는 역 동 성 을 가 지 고 있 나 요 ( 시 간 에 따 라 그 대 로 인 지 변 하 는 지 ) ? 마 지 막 질 문 은 공 변 량 변 화 ( covariate shift ) 문 제 를 일으 킵 니다 . ( 학 습 데 이 터 와 테 스 트 데 이 터 가 다 를 때 ) 숙 제 는 TA 가 제 출 하 는 반 면 에 , 시 험 은 강 의 를 하 는 사 람 이 낸 문 제 를 풀 때 이 런 것 을 경 험했 을 것 입 니다 . 환 경과 상 호 작 용 을 명 시 적 으 로 고 려 하 는 강 화 학 습 ( reinforcement learning ) 와 적 대 적 학 습 ( adversarial learning ) 에 대 해 서 간 단 히 살 펴 보 겠 습 니다 . 46 4 . 딥 러 닝 맛 보 기 강 화 학 습 ( reinforcement learning ) 환 경과 작 용 을 하 면 서 행 동 을 위 하 는 에 이 전 트 를 만 드 는 머 신 러 닝 에 관 심 이 있 다 면 , 여 러 분 은 아 맞 도 강 화 학 습 ( reinforcement learning , RL ) 에 집중 할 것 입 니다 . 로 보 틱 스 , 분 석 시스 템 심 지 어 는 비 디 오 게 임 에 대 한 AI 개 발 에 적 용 될 수 있 습 니다 . 딥 뉴 럴 네 트 워 크 를 RL 문 제 에 적 용 한 딥 강 화 학 습 ( deep reinforcement learning , DRL ) 이 인 기 가 높 습 니다 . deep Q - network that beat humans at Atari games using only the visual input 와 AlphaGo program that dethroned the world champion at the board game Go 이 유 명 한 두 예 입 니다 . 강 화 학 습 은 에 이 전 트 가 일 련 의 시 간 단 계 ( time steps ) 에 걸 쳐 서 환 경과 작 용 을 문 제 에 대 한 일 반 적 인 기 술 을 정 의 합 니다 . 각 시 간 단 계 𝑡 마 다 , 에 이 전 트 는 환 경 으 로 부 터 어 떤 관 찰 𝑜 𝑡 를 받 아 서 , 행 동 𝑎 𝑡 을 선 택 해 야 하 고 , 이 행 동 은 다 시 환 경 으 로 전 달 이 됩 니다 . 그 리 고 나 면 에 이 전 트 는 환 경 으 로 부 터 보 상 𝑟 𝑡 를 받 습 니다 . 그 후 , 에 이 전 트 는 다 음 관 찰 을 받 아 서 , 다 음 행 동 을 취 하 는 것 을 반복 합 니다 . RL 에 이 전 트 의 행 동 은 정 책 ( policy ) 에 의 해 서 정 의 됩 니다 . 간 단 하 게 말 하 면 , 정 책 ( policy ) 은 ( 환 경 으 로 부 터 얻 은 ) 관 찰 을 행 동 으 로 매 핑 시 키 는 함 수 입 니다 . 강 화 학 습 의 목 표 는 좋 은 정 책 을 만 드 는 것 입 니다 . RL 프 레 임 워 크 의 일 반 성 을 과 장 하 는 것 은 어 렵 습 니다 . 예 를 들 어 , 임의의 강 화 학 습 문 제 를 RL 문 제 로 바 꿀 수 있 습 니다 . 분 류 의 문 제 를 예 로 들 어 보 겠 습 니다 . 우 리 는 한 행 동 ( action ) 이 각 클 래 스 대 응 하 는 에 이 전 트 를 만 들 수 있 습 니다 . 그 리 고 는 원 래 의 지 도 학 습 에 서 사 용 하 는 손 실 함 수 ( loss function ) 와 완 전 히 같 은 보 상 을 주 는 환 경 을 생 성 합 니다 . 즉 , RL 은 지 도 학 습 이 해 결 하 지 못 하 는 많 은 문 제 를 해 결 할 수 있 습 니다 . 예 를 들 면 , 지 도 학 습 에 서 는 학 습 입 력 에 대 한 정 확 한 레 이 블 이 있 어야 합 니다 . 하 지 만 RL 의 경 우 에 는 관 찰 에 대 해 서 환 경 은 최 적 의 행 동 을 알 려 준 다 고 가 정 하 지 않 습 니다 . 일 반 적 으 로 우 리 는 어 떤 보 상 을 받 을 뿐 입 니다 . 즉 , 환 경 은 어 떤 행 동 이 보 상 을 받 을 수 있 는 지 조 차 알 려 주 지 않 을 수 도 있 습 니다 . 체 스 게 임을 생 각 해 보 세 요 . 오 직 실 제 보 상 은 게 임이 끝났 을 때 주 어 집 니다 . 즉 , 우 리 는 게 임을 이 겨 서 보 상 1 을 받 거 나 , 져 서 보 상 - 1 을 받 게 됩 니다 . 강 화 학 습 은 신 뢰 할 당 문 제 ( credit assignment problem ) 4 . 1 . 소 개 47 을 다 뤄 야 합 니다 . 10 월 11 일 에 승 진 을 하 는 직 원 의 경 우 도 같 은 것 이 적 용 됩 니다 . 승 진 은 지 난 해 에 잘 선 택 된 많 은 행 동 들 을 반 영 할 가 능 성 이 높 습 니다 . 미 래 에 더 많 은 승 진 을 하 기 위 해 서 는 어 떠 한 행 동 이 승 진 으 로 이 어 졌 었 는 지 를 알아 내 야 합 니다 . 강 화 학 습 은 일 부 관 찰 의 문 제 ( problem of partial observability ) 도 다 뤄 야 할 수 도 있 습 니다 . 즉 , 현 재 관 찰 은 현 재 상 태 에 대 한 모 든 것 을 알 려 주 지 않 을 수 도 있 습 니다 . 청 소 로 봇 이 집 에 서 비 슷 하 게 생 긴 여 러 옷 장 들 중 하 나 에 갇 혔 다 고 하 겠 습 니다 . 로 봇 의 정 확 한 위 치 ( 즉 상 태 ) 를 추 론 하 는 것 은 옷 장 에 들 어 가 기 전 에 이 전 상 태 를 고 려 해 야 할 것 입 니다 . 마 지 막 으 로 , 어 느 시 점 에 강 화 학 습 은 좋 은 정 책 하 나 를 알 게 될 수 도 있 지 만 , 에 이 전 트 가 전 혀 시 도 하 지 않 은 더 좋 은 정 책 들 이 많 이 있을 수 도 있 습 니다 . 강 화 학 습 은 정 책 이 주 는 현 재 알 려 진 최 고 의 전 략 을 개 척 ( exploit ) 하 거 나 지 식 을 얻 기 위 해 서 단 기 적 인 보 상 을 포 기 하 면 서 전 략 들 의 공간 을 탐 사 ( explore ) 할 지 를 지 속 적 으 로 선 택 해 야 합 니다 . MDP , 반 딧 ( bandit ) , 친 구 들 일 반 적 인 강 화 학 습 문 제 는 아 주 일 반 적 인 설 정 을 갖 습 니다 . 행 동 은 이 후 의 관 찰 에 영 향 을 줍 니다 . 보 상 은 선 택 된 행 동 에 대 해 서 만 관 찰 됩 니다 . 환 경 은 전 체 또 는 일 부 만 관 찰 될 수 있 습 니다 . 이 모 든 복 잡 성 을 한 번 에 설 명 하 기 에 는 연 구 자 들 에 게 너 무 많 은 것 을 묻 는 것 입 니다 . 그 결과 로 연 구 자 들 은 강 화 학 습 의 특 별 한 경 우 들 을 연 구 해 왔 습 니다 . 환 경 이 완 전 히 관 찰 되 는 경 우 에 우 리 는 RL 문 제 를 마 코 브 결 정 프 로 세 스 ( Markov Decision Process , MDP ) 라 고 부 릅 니다 . 상 태 가 이 전 의 행 동 에 의 존 하 지 않 을 경 우 , 우 리 는 이 런 문 제 를 문 맥 상 반 딧 문 제 ( contextual bandit problem ) 이 라 고 합 니다 . 상 태 가 없 고 , 초 기 에 알 려 지지 않 은 보 상 을 갖 는 가 능 한 행 동 의 집 합 만 있 는 경 우 , 이 런 문 제 를 우 리 는 고 전적 인 멀 티 - 암 드 반 딧 문 제 ( multi - armed bandit problem ) 이 라 고 합 니다 . 4 . 1 . 7 요 약 머 신 러 닝 은 광 대 합 니다 . 우 리 는 아 마 도 전 체 를 다 루 지 못 할 것 입 니다 . 하 지 만 , 뉴 럴 네 트 워 크 는 간 단 하 고 , 기 초 적 인 수 학 만 요 구 합 니다 . 그 럼 시 작 해 봅 시 다 . ( 하 지 만 먼 저 MXNet 을 설 치 하 세 요 ) 48 4 . 딥 러 닝 맛 보 기 4 . 1 . 8 Scan the QR Code to Discuss 4 . 2 Gluon 시 작 하 기 시 작 하 기 에 앞 서 노 트 북 들 을 수 행하 는 데 필 요 한 코 드 를 다 운 로 드 하 고 설 치 해 야 합 니다 . 이 절 을 그 냥 넘 어 가 도 다 음 절 들 에 서 설 명 하 는 이 론 적 인 내 용 을 이 해하 는 데 는 문 제 가 없 지 만 , 직 접 코 드 를 실 행해 보 는 것 을 꼭 권 장 합 니다 . 코 드 를 작 성 하 고 , 수 정 하 고 결과 를 보 는 것 이 이 책 으 로 부 터 더 많 은 것 을 얻 을 수 있 는 방법 이 기 때 문 입 니다 . 요 약 하 자 면 , 아 래 단 계 들 을 수 행하 면 됩 니다 . 1 . conda 설 치 하 기 2 . 이 책 에 필 요 한 코 드 를 다 운 로 드 하 기 3 . GPU 를 가 지 고 있 는 데 아 직 설 치 하 지 않았 다 면 GPU 드 라 이 버 설 치 하 기 4 . MXNet 와 이 책 의 예 제 들 을 수 행할 conda 환 경 빌 드 하 기 4 . 2 . 1 Conda 라 이 브 러 리를 설 치 의 간 편 함 을 위 해 서 , 유 명 한 Python 패 키 지 관 리 자인 conda 를 권 장 합 니다 . 1 . conda . io / en / latest / miniconda . html 를 방 문 해 서 여 러 분 의 OS 에 맞 는 Miniconda 를 다 운 로 드 한 후 설 치 하 세 요 . 2 . 리 눅 스 인 경 우 source ~ / . bashrc , MacOS 인 경 우 source ~ / . bash _ profile 를 수 행 해 서 쉘 을 업 데 이 트 합 니다 . PATH 환 경 변 수 에 Anaconda 를 꼭 추 가 하 세 요 . 3 . 이 책 의 노 트 북 들 이 담 긴 tarball 를 다 운 로 드 하 세 요 . 이 파 일은 www . d2l . ai / d2l - en - 1 . 0 . zip 에 있 습 니다 . 또 는 GitHub 리 포 지 토 리 에 서 최 신 버 전 을 복 사 해 도 됩 니다 . 4 . ZIP 파 일의 압 축 을 풀 어 서 원 하 는 디 렉 토 리 에 옮 겨 놓 으 세 요 . 리 눅 스 의 경 우 아 래 명 령 들 을 명 령 행 에 서 수 행하 면 됩 니다 . MacOS 를 사 용 하 는 경 우 , 처 음 두 줄 에 있 는 Linux 를 MacOS 로 바 꾸 면 됩 니다 . Windows 사 용 자 는 위 가 이 드 에 있 는 링 크 들 을 참 고 하 세 요 . 4 . 2 . Gluon 시 작 하 기 49 wget https : / / repo . anaconda . com / miniconda / Miniconda3 - latest - Linux - x86 _ 64 . sh sh Miniconda3 - latest - Linux - x86 _ 64 . sh mkdir d2l - en cd d2l - en curl https : / / www . d2l . ai / d2l - en - 1 . 0 . zip - o d2l - en . zip unzip d2l - en - 1 . 0 . zip rm d2l - en - 1 . 0 . zip 4 . 2 . 2 GPU 지 원 기 본 설 정 인 경 우 MXNet 은 모 든 컴 퓨 터 에 서 수 행 되 는 것 을 보 장 하 기 위 해 서 GPU 지 원 을 하 지 않 도 록 설 치 됩 니다 . 만 약 여 러 분 의 컴 퓨 터 에 GPU 가 있 다 면 , conda 환 경 을 수 정 해 서 CUDA 가 활 성 화 된 빌 드 를 다 운 로 드 해 야 합 니다 . 당 연 하 지 만 , 필 요 한 드 라 이 버 들 이 설 치 되 어 있 어야 합 니다 . 아 래 단 계 들 을 수 행하 세 요 . 1 . 여 러 분 이 가 지 고 있 는 GPU 에 맞 는 NVIDIA Drivers 가 설 치 되 어 있 는 지 확 인 하 세 요 . 2 . GPU 를 위 한 프 로 그 래 밍 언어 인 CUDA 를 설 치 하 세 요 . 3 . 딥 러 닝 을 위 한 다 양 한 최 적 화 라 이 브 러 리를 제 공 하 는 CUDNN 을 설 치 하 세 요 . 4 . 추 가 적 인 가 속 을 위 해 서 필 요 한 경 우 , TensorRT 를 설 치 합 니다 . 설 치 는 다 소 오 래 걸 리 고 , 라 이 센 스 동 의 를 해 야 하 기 도 하 고 여 러 설 치 스 크 립 트 를 사 용 해 야 합 니다 . 여 러 분 의 OS 나 하 드 웨 어에 따 라 설 치 방법 은 달 라 질 수 있 습 니다 . 위 과 정 을 마 친 후 , environment . yml 의 환 경 설 정 을 업 데 이 트 합 니다 . mxnet 을 mxnet - cu92 또 는 설 치 된 CUDA 버 전 에 맞 도 록 바 꿉 니다 . 만 약 CUDA 8 . 0 버 전 이 설 치 된 경 우 , mxnet - cu92 를 mxnet - cu80 으 로 바 꿔 야 합 니다 . 이 것 은 꼭 conda 환 경 을 만 들 기 전 에 해 야 합 니다 . 그 렇 지 않 은 경 우 , 빌 드 를 다 시 수 행해 야 하 기 때 문 입 니다 . 리 눅 스 사 용 자 는 아 래 명 령 으 로 수 정 할 수 있 습 니다 . ( Windows 사 용 자 는 Notepad 등 을 사 용 해 서 environment . yml 를 직 접 수 정 합 니다 . ) cd d2l emacs environment . yml 4 . 2 . 3 Conda 환 경 요 약 하 면 , conda 는 Python 라 이 브 러 리 들 을 반복 가 능 하 고 안 정적 인 방법 으 로 설 정 하 는 방법 을 제 공 합 니다 . 이 를 통 해 서 모 든 소 프 트 웨 어 의 의 존 성 을 만 족 시 킬 수 있 습 니다 . 시 작 하 기 에 필 요 한 것 을 다 음 과 50 4 . 딥 러 닝 맛 보 기 같 습 니다 . 1 . conda 를 이 용 해 서 환 경 을 생 성 하 고 활 성 화 합 니다 . 편 리 하 게 하 기 위 해 서 environment . yml 파 일 에 모 든 설 정 을 넣 어 놨 습 니다 . 2 . 환 경 을 활 성 화 합 니다 . 3 . Jupyter 노 트 북 을 열어 서 실 험 을 시 작 합 니다 . Windows 명 령 창 을 엽 니다 . conda env create - f environment . yml cd d2l - en activate gluon jupyter notebook 이 후 에 라 이 브 러 리 들 을 다 시 활 성 화 할 필 요 가 있 다 면 , 첫 번 째 줄 은 넘 어 가 세 요 . 이 는 여 러 분 의 설 정 이 활 성 화 되 어 있음을 확 인 시 켜 줍 니다 . Jupyter Notebook ( jupyter notebook ) 을 대 신 에 , JupyterLab ( jupyter lab ) 을 사 용 할 수 있음을 알아 두 세 요 . 활 성 화 된 conda gluon 환 경 에 서 conda install jupyterlab 을 실 행해 서 직 접 설 치 할 수 있 습 니다 . 웹 브 라 우 저 통 합 이 잘 작 동 한 다 면 , Jupyter 를 실 행하 면 웹 브 라 우 저 의 새 창 이 생 성 됩 니다 . 만 약 그 렇 지 않 다 면 , 직 접 http : / / localhost : 8888 를 열어 보 세 요 . 어 떤 노 트 북 은 필 요 한 데 이 터 와 사 전 - 학 습 ( pre - trained ) 모 델 을 자 동 으 로 다 운 로 드 하 기 도 합 니다 . MXNET _ GLUON _ REPO 변 수 를 수 정 해 서 리 포 지 토 리 위 치 를 바 꿀 수 도 있 습 니다 . Linux 와 MacOS 리 눅 스 에 서 도 비 슷 하 게 합 니다 . 단 지 , anaconda 명 령 옵 션 이 약 간 다 릅 니다 . conda env create - f environment . yml cd d2l - en source activate gluon jupyter notebook Windows 와 다 른 설 치 들 의 주 요 차 이 는 Windows 에 서 는 activate gluon 을 사 용 하 지 만 , Linux 나 MacOS 는 source activate gluon 을 사 용 한 다는 것 입 니다 . 그 외 에 는 Windows 에 서 했 던 것과 동 일 합 니다 . 더 강 력 한 환 경 을 원 한 다 면 JupyterLab 을 설 치 하 세 요 . 4 . 2 . Gluon 시 작 하 기 51 4 . 2 . 4 Gluon 업 데 이 트 하 기 새 로 운 CUDA 버 전 이 나 MXNet 을 설 치 해 서 리 포 지 토 리를 업 데 이 트 하 고 싶 다 면 , conda 명 령 으 로 간 단 하 게 할 수 있 습 니다 . 아 래 명 령 으 로 패 키 지 를 업 데 이 트 합 니다 . cd d2l - en conda env update - f environment . yml 4 . 2 . 5 요 약 • conda 는 모 든 소 프 트 웨 어 의 의 존 성 을 만 족 하 도 록 해 주 는 Python 패 키 지 매 니 저 입 니다 . • environment . yml 은 이 책 에 필 요 한 모 든 설 정 을 가 지 고 있 습 니다 . 모 든 노 트 북 에 대 한 다 운 로 드 링 크 를 제 공 하 고 , GitHub 를 통 해 서 도 제 공 합 니다 . • GPU 를 가 지 고 있 다 면 , GPU 드 라 이 버 와 관 련 설 정 을 업 데 이 트 하 세 요 . 학 습 시 간 을 아 주 많 이 줄 일 수 있 습 니다 . 4 . 2 . 6 문 제 1 . 이 책 의 코 드 를 다 운 로 드 하 고 , 실 행 환 경 을 설 치 하 세 요 . 2 . 질 문 이 나 도 움 이 필 요 하 면 이 절 의 아 래 에 있 는 링 크 를 따 라 포 럼 을 이 용 하 세 요 . 3 . 포 럼 의 계 정 을 만 들 고 여 러 분 을 소 개 하 세 요 . 4 . 2 . 7 Scan the QR Code to Discuss 52 4 . 딥 러 닝 맛 보 기 4 . 3 데 이 터 조 작 ( data manipulation ) 데 이 터 를 변 경 할 수 없 다 면 아 무 것 도 할 수 없 습 니다 . 일 반 적 으 로 우 리 는 데 이 터 를 사 용 해 서 두 가 지 중 요 한 일을 합 니다 . ( i ) 데 이 터 를 얻 고 , ( ii ) 컴 퓨 터 에 서 들 어 오 면 처 리 하 기 . 데 이 터 를 저 장 하 는 방법 을 모 른 다 면 데 이 터 를 얻 는 것 의 의 미 가 없 으 니 , 합 성 된 데 이 터 를 다 루 는 것 부 터 시 작 하 겠 습 니다 . MXNet 에 서 데 이 터 를 저 장 하 고 변 경 하 는 주 요 도 구 인 NDArray 를 소 개 하 겠 습 니다 . NumPy 를 사 용 해 봤 다 면 , NDArray 가 NumPy 의 다 차 원 배 열 과 디 자인 측 면 에 서 비 슷 하 다는 것 을 눈 치 챌 것 입 니다 . 하 지 만 , 주 요 장 점 들 이 있 습 니다 . 첫 번 째 로 는 NDArray 는 CPU , GPU 그 리 고 분 산 클 라 우 드 아 키 텍 처 에 서 비 동 기 연 산 을 지 원 합 니다 . 두 번 째 는 , 자 동 미 분 을 지 원 합 니다 . 이 특 징 들 때 문 에 NDArray 는 머 신 러 닝 에 이 상 적 인 요 소 라 고 할 수 있 습 니다 . 4 . 3 . 1 시 작 하 기 이 절 에 서 는 여 러 분 은 기 본 적 인 것 을 다 룰 것 입 니다 . 요 소 별 연 산 이 나 표 준 분 포 와 같 은 기 본 적 인 수 학 내 용 을 이 해하 지 못 해 도 걱 정 하 지 마 세 요 . 다 음 두 절 에 서 필 요 한 수 학 과 어 떻 게 코 드 로 구 현 하 는 지 를 다 룰 예 정 입 니다 . 수 학 에 대 해 서 더 알 고 싶 다 면 , 부 록 에 “Math” 를 참 고 하 세 요 . MXNet 과 MXNet 의 ndarray 모 듈 을 import 합 니다 . 여 기 서 는 ndarray 를 nd 라 고 별 칭 을 주 겠 습 니다 . [ 1 ] : import mxnet as mx from mxnet import nd 우 리 가 만 들 수 있 는 가 장 단 순 한 객 체 는 벡 터 입 니다 . arange 는 12 개 의 연 속 된 정 수 를 갖 는 행 벡 터 를 생 성 합 니다 . [ 2 ] : x = nd . arange ( 12 ) x [ 2 ] : [ 0 . 1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . ] < NDArray 12 @ cpu ( 0 ) > x 를 출 력 할 때 나 온 < NDArray 12 @ cpu ( 0 ) > 로 부 터 , 우 리 는 이 것 이 길 이 가 12 인 일 차 원 배 열 이 고 , CPU 의 메 인 메모 리 에 저 장 되 어 있 다는 것 을 알 수 있 습 니다 . @ cpu ( 0 ) 에 서 0 은 아 무 런 의 미 가 없 고 , 특 정 코 어 를 의 미 하 지 도 않 습 니다 . NDArray 인 스 턴 스 의 shape 은 shape 속성 으 로 얻 습 니다 . 4 . 3 . 데 이 터 조 작 ( data manipulation ) 53 [ 3 ] : x . shape [ 3 ] : ( 12 , ) size 속성 은 NDArray 인 스 턴 스 의 원 소 총 개 수 를 알 려 줍 니다 . 우 리 는 벡 터 를 다 루 고 있 기 때 문 에 두 결과 는 같 습 니다 . [ 4 ] : x . size [ 4 ] : 12 행 벡 터 를 3 행 , 4 열 의 행 렬로 바 꾸 기 위 해 서 , 즉 shape 을 바 꾸 기 위 해 서 reshape 함 수 를 사 용 합 니다 . 모 양 ( shape ) 이 바 뀌 는 것 을 제 외 하 고 는 x 의 원 소 와 크 기 는 변 하 지 않 습 니다 . [ 5 ] : x = x . reshape ( ( 3 , 4 ) ) x [ 5 ] : [ [ 0 . 1 . 2 . 3 . ] [ 4 . 5 . 6 . 7 . ] [ 8 . 9 . 10 . 11 . ] ] < NDArray 3x4 @ cpu ( 0 ) > 위 와 같 이 행 렬 의 모 양 을 바 꾸 는 것 은 좀 이 상 할 수 있 습 니다 . 결 국 , 3 개 행 을 갖 는 행 렬 을 원 한 다 면 총 원 소 의 개 수 가 12 개가 되 기 위 해 서 열 이 4 가 되 어야 한 다는 것 을 알아야 합 니다 . 또 는 , NDArray 에 게 행 의 개 수 가 몇 개 이 든 지 모 든 원 소 를 포 함하 는 열 이 4 개 인 행 렬 을 자 동 으 로 찾 아 내 도 록 요 청 하 는 것 도 가 능 합 니다 . 즉 , 위의 경 우 에 는 x . reshape ( ( 3 , 4 ) ) 는 x . reshape ( ( - 1 , 4 ) ) 와 x . reshape ( ( 3 , - 1 ) ) 와 같 습 니다 . [ 6 ] : nd . empty ( ( 3 , 4 ) ) [ 6 ] : [ [ 1 . 7943246e - 22 4 . 5868703e - 41 2 . 4476257e + 01 3 . 0850987e - 41 ] [ 0 . 0000000e + 00 0 . 0000000e + 00 0 . 0000000e + 00 0 . 0000000e + 00 ] [ 0 . 0000000e + 00 0 . 0000000e + 00 0 . 0000000e + 00 0 . 0000000e + 00 ] ] < NDArray 3x4 @ cpu ( 0 ) > empty 메 소 드 는 모 양 ( shape ) 에 따 른 메모 리를 잡 아 서 원 소 들 의 값 을 설 정 하 지 않 고 행 렬 를 반 환 합 니 다 . 이 는 아 주 유 용 하 지 만 , 원 소 들 이 어 떤 형 태 의 값 이 라 도 가 질 수 있 는 것 을 의 미 합 니다 . 이 는 매 우 큰 값 들 일 수 도 있 습 니다 . 하 지 만 , 일 반 적 으 로 는 행 렬 을 초 기 화 하 는 것 을 원 합 니다 . 보 통 은 모 두 0 으 로 초 기 화 하 기 를 원 합 니다 . 수 학 자 들 은 이 차 원 보 다 큰 객 체 들 에 대 해 서 는 특 별 한 이 름 을 쓰 지 않 지 만 , 우 리 는 이 것 들 은 텐 서 ( tensor ) 라 고 부 르 겠 습 니다 . 모 든 원 소 가 0 이 고 모 양 ( shape ) 이 54 4 . 딥 러 닝 맛 보 기 ( 2 , 3 , 4 ) 인 텐 서 를 하 나 만 들 기 위 해 서 다 음 과 같 이 합 니다 . [ 7 ] : nd . zeros ( ( 2 , 3 , 4 ) ) [ 7 ] : [ [ [ 0 . 0 . 0 . 0 . ] [ 0 . 0 . 0 . 0 . ] [ 0 . 0 . 0 . 0 . ] ] [ [ 0 . 0 . 0 . 0 . ] [ 0 . 0 . 0 . 0 . ] [ 0 . 0 . 0 . 0 . ] ] ] < NDArray 2x3x4 @ cpu ( 0 ) > NumPy 처 럼 , 모 든 원 소 가 1 인 텐 서 를 만 드 는 방법 은 다 음 과 같 습 니다 . [ 8 ] : nd . ones ( ( 2 , 3 , 4 ) ) [ 8 ] : [ [ [ 1 . 1 . 1 . 1 . ] [ 1 . 1 . 1 . 1 . ] [ 1 . 1 . 1 . 1 . ] ] [ [ 1 . 1 . 1 . 1 . ] [ 1 . 1 . 1 . 1 . ] [ 1 . 1 . 1 . 1 . ] ] ] < NDArray 2x3x4 @ cpu ( 0 ) > Python 리 스 트 를 이 용 해 서 NDArray 의 각 원 소 값 을 지 정 하 는 것 도 가 능 합 니다 . [ 9 ] : y = nd . array ( [ [ 2 , 1 , 4 , 3 ] , [ 1 , 2 , 3 , 4 ] , [ 4 , 3 , 2 , 1 ] ] ) y [ 9 ] : [ [ 2 . 1 . 4 . 3 . ] [ 1 . 2 . 3 . 4 . ] [ 4 . 3 . 2 . 1 . ] ] < NDArray 3x4 @ cpu ( 0 ) > 어 떤 경 우 에 는 , NDArray 의 값 을 임의 로 채 우 기 를 원 할 때 가 있 습 니다 . 이 는 특 히 뉴 럴 네 트 워 크 의 파 라 미 터 로 배 열 을 사 용 할 때 일 반 적 입 니다 . 아 래 코 드 는 shape 가 ( 3 , 4 ) NDArray 를 생 성 하 고 , 각 원 소 는 평 균 이 0 이 고 분 산 이 1 인 표 준 분 포 로 부 터 임의 로 추출 한 값 을 갖 습 니다 . 4 . 3 . 데 이 터 조 작 ( data manipulation ) 55 [ 10 ] : nd . random . normal ( 0 , 1 , shape = ( 3 , 4 ) ) [ 10 ] : [ [ 2 . 2122064 0 . 7740038 1 . 0434405 1 . 1839255 ] [ 1 . 8917114 - 1 . 2347414 - 1 . 771029 - 0 . 45138445 ] [ 0 . 57938355 - 1 . 856082 - 1 . 9768796 - 0 . 20801921 ] ] < NDArray 3x4 @ cpu ( 0 ) > 4 . 3 . 2 연 산 들 우 리 는 종종 배 열에 함 수 를 적 용 할 필 요 가 있 습 니다 . 아 주 간 단 하 고 , 굉 장 히 유 용 한 함 수 중 에 하 나 로 요 소 별 ( element - wise ) 함 수 가 있 습 니다 . 이 연 산 은 두 배 열 의 동 일 한 위 치 에 있 는 원 소 들 에 대 해 스 칼 라 연 산 을 수 행하 는 것 입 니다 . 스 칼 라 를 스 칼 라 로 매 핑하 는 함 수 를 사 용 하 면 언 제 나 요 소 별 ( element - wise ) 함 수 를 만 들 수 있 습 니다 . 수 학 기 호 로 는 이 런 함 수 를 𝑓 : R → R 로 표 현 합 니다 . 같 은 모 양 ( shape ) 의 두 벡 터 u 와 v 와 함 수 f 가 주 어 졌 을 때 , 모 든 𝑖 에 대 해 서 𝑐 𝑖 ← 𝑓 ( 𝑢 𝑖 , 𝑣 𝑖 ) 을 갖 는 벡 터 c = 𝐹 ( u , v ) 를 만 들 수 있 습 니다 . 즉 , 우 리 는 스 칼 라 함 수 를 벡 터 의 요 소 별 로 적 용 해 서 벡 터 함 수 𝐹 : R 𝑑 → R 𝑑 를 만 들 었 습 니다 . MXNet 에 서 는 일 반 적 인 표 준 산 술 연 산 자 들 ( + , - , / , * , * * ) 은 모 양 ( shape ) 이 무 엇 이 든 지 상 관 없 이 두 텐 서 의 모 양 ( shape ) 이 같 을 경 우 모 두 요 소 별 연 산 으 로 간 주 되 어 계 산 됩 니다 . 즉 , 행 렬 을 포 함한 같 은 모 양 ( shape ) 을 갖 는 임의의 두 텐 서 에 대 해 서 요 소 별 연 산 을 수 행할 수 있 습 니다 . [ 11 ] : x = nd . array ( [ 1 , 2 , 4 , 8 ] ) y = nd . ones _ like ( x ) * 2 print ( ' x = ' , x ) print ( ' x + y ' , x + y ) print ( ' x - y ' , x - y ) print ( ' x * y ' , x * y ) print ( ' x / y ' , x / y ) x = [ 1 . 2 . 4 . 8 . ] < NDArray 4 @ cpu ( 0 ) > x + y [ 3 . 4 . 6 . 10 . ] < NDArray 4 @ cpu ( 0 ) > x - y [ - 1 . 0 . 2 . 6 . ] < NDArray 4 @ cpu ( 0 ) > x * y ( continues on next page ) 56 4 . 딥 러 닝 맛 보 기 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) [ 2 . 4 . 8 . 16 . ] < NDArray 4 @ cpu ( 0 ) > x / y [ 0 . 5 1 . 2 . 4 . ] < NDArray 4 @ cpu ( 0 ) > 제 곱과 같 은 더 많 은 연 산 들 이 요 소 별 연 산 으 로 적 용 될 수 있 습 니다 . [ 12 ] : x . exp ( ) [ 12 ] : [ 2 . 7182817e + 00 7 . 3890562e + 00 5 . 4598148e + 01 2 . 9809580e + 03 ] < NDArray 4 @ cpu ( 0 ) > 요 소 별 연 산 과 더 불 어 서 , dot 함 수 를 이 용 한 행 렬 의 곱 처 럼 행 렬 의 연 산 들 도 수 행할 수 있 습 니다 . 행 렬 x 와 y 의 전 치 행 렬 에 대 해 서 행 렬 의 곱 을 수 행해 보 겠 습 니다 . x 는 행 이 3 , 열 이 4 인 행 렬 이 고 , y 는 행 이 4 개 열 이 3 개 를 갖 도 록 전 치 시 킵 니다 . 두 행 렬 을 곱 하 면 행 이 3 , 열 이 3 인 행 렬 이 됩 니다 . ( 이 것 이 어 떤 의 미 인 지 햇 갈 려 도 걱 정 하 지 마 세 요 . linear algebra 절 에 서 행 렬 의 연 산 에 대 한 것 들 을 설 명 할 예 정 입 니다 . ) [ 13 ] : x = nd . arange ( 12 ) . reshape ( ( 3 , 4 ) ) y = nd . array ( [ [ 2 , 1 , 4 , 3 ] , [ 1 , 2 , 3 , 4 ] , [ 4 , 3 , 2 , 1 ] ] ) nd . dot ( x , y . T ) [ 13 ] : [ [ 18 . 20 . 10 . ] [ 58 . 60 . 50 . ] [ 98 . 100 . 90 . ] ] < NDArray 3x3 @ cpu ( 0 ) > 여 러 NDArray 들 을 합 치 는 것 도 가 능 합 니다 . 이 를 위 해 서 는 어 떤 차 원 ( dimension ) 을 따 라 서 합 쳐 야 하 는 지 를 알 려 줘 야 합 니다 . 아 래 예 제 는 각각 의 차 원 0 ( 즉 , 행 들 ) 과 차 원 1 ( 열 들 ) 을 따 라 서 두 행 렬 을 합 칩 니다 . [ 14 ] : nd . concat ( x , y , dim = 0 ) nd . concat ( x , y , dim = 1 ) [ 14 ] : [ [ 0 . 1 . 2 . 3 . 2 . 1 . 4 . 3 . ] [ 4 . 5 . 6 . 7 . 1 . 2 . 3 . 4 . ] [ 8 . 9 . 10 . 11 . 4 . 3 . 2 . 1 . ] ] ( continues on next page ) 4 . 3 . 데 이 터 조 작 ( data manipulation ) 57 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) < NDArray 3x8 @ cpu ( 0 ) > NumPy 에 서 와 같 이 논 리 문 장을 사 용 해 서 이 진 NDArray 를 만 들 수 있 습 니다 . x = = y 를 예 로 들 어 보 겠 습 니다 . 만 약 x 와 y 가 같 은 원 소 가 있 다 면 , 새 로 운 NDArray 는 그 위 치 에 1 을 갖고 , 다 른 값 이 면 0 을 갖 습 니다 . [ 15 ] : x = = y [ 15 ] : [ [ 0 . 1 . 0 . 1 . ] [ 0 . 0 . 0 . 0 . ] [ 0 . 0 . 0 . 0 . ] ] < NDArray 3x4 @ cpu ( 0 ) > NDArray 의 모 든 요 소 를 더 하 면 하 나 의 원 소 를 갖 는 NDArray 가 됩 니다 . [ 16 ] : x . sum ( ) [ 16 ] : [ 66 . ] < NDArray 1 @ cpu ( 0 ) > asscalar 함 수 를 이 용 해 서 결과 를 Python 의 스 칼 라 로 바 꿀 수 있 습 니다 . 아 래 예 제 는 x 의 ℓ 2 놈 을 계 산 합 니다 . 이 결과 는 하 나 의 원 소 를 갖 는 NDArray 이 고 , 이 를 Python 의 스 칼 라 값 으 로 바 꿉 니다 . [ 17 ] : x . norm ( ) . asscalar ( ) [ 17 ] : 22 . 494442 표 기 를 편 하 게 하 기 위 해 서 y . exp ( ) , x . sum ( ) , x . norm ( ) , 등 을 각각 nd . exp ( y ) , nd . sum ( x ) , nd . norm ( x ) 처 럼 쓸 수 도 있 습 니다 . 4 . 3 . 3 브 로 드 케 스 트 메 카 니 즘 위 절 에 서 우 리 는 같 은 모 양 ( shape ) 의 두 NDArray 객 체 에 대 한 연 산 을 어 떻 게 수 행하 는 지 를 살 펴 봤 습 니다 . 만 약 모 양 ( shape ) 이 다 른 경 우 에 는 NumPy 와 같 이 브 로 드 케 스 팅 메 카 니 즘 이 적 용 됩 니다 : 즉 , 두 NDArray 가 같 은 모 양 ( shape ) 을 갖 도 록 원 소 들 이 복 사 된 후 , 요 소 별 로 연 산 을 수 행하 게 됩 니다 . [ 18 ] : a = nd . arange ( 3 ) . reshape ( ( 3 , 1 ) ) b = nd . arange ( 2 ) . reshape ( ( 1 , 2 ) ) a , b 58 4 . 딥 러 닝 맛 보 기 [ 18 ] : ( [ [ 0 . ] [ 1 . ] [ 2 . ] ] < NDArray 3x1 @ cpu ( 0 ) > , [ [ 0 . 1 . ] ] < NDArray 1x2 @ cpu ( 0 ) > ) a 와 b 는 각각 ( 3x1 ) , ( 1x2 ) 행 렬 이 기 때 문 에 , 두 행 렬 을 더 하 기 에 는 모 양 ( shape ) 이 일 치 하 지 않 습 니다 . NDArray 는 이 런 상 황 을 두 행 렬 의 원 소 들 을 더 큰 행 렬 ( 3x2 ) 로 ‘ 브 로 드 케 스 팅 ’ 해 서 해 결 합 니다 . 즉 , 행 렬 a 는 컬 럼 을 복 제 하 고 , 행 렬 b 는 열 을 복 제 한 후 , 요 소 별 덧 셈 을 수 행합 니다 . [ 19 ] : a + b [ 19 ] : [ [ 0 . 1 . ] [ 1 . 2 . ] [ 2 . 3 . ] ] < NDArray 3x2 @ cpu ( 0 ) > 4 . 3 . 4 인 덱 싱 과 슬 라 이 싱 ( slicing ) 다 른 Python 배 열 처 럼 NDArray 의 원 소 들 도 인 덱 스 를 통 해 서 지 정 할 수 있 습 니다 . Python 에 서 처 럼 첫 번 째 원 소 의 인 덱 스 는 0 이 고 , 범 위 는 첫 번 째 원 소 는 포 함하 고 마 지 막 은 포 함하 지 않 습 니다 . 즉 , 1 : 3 은 두 번 째 와 세 번 째 원 소 를 선 택 하 는 범 위입 니다 . 행 렬 에 서 행 들 을 선 택 하 는 예 는 다 음 과 같 습 니다 . [ 20 ] : x [ 1 : 3 ] [ 20 ] : [ [ 4 . 5 . 6 . 7 . ] [ 8 . 9 . 10 . 11 . ] ] < NDArray 2x4 @ cpu ( 0 ) > 값 을 읽 는 것 말 고 도 , 행 렬 의 원 소 값 을 바 꾸 는 것 도 가 능 합 니다 . [ 21 ] : x [ 1 , 2 ] = 9 x [ 21 ] : [ [ 0 . 1 . 2 . 3 . ] [ 4 . 5 . 9 . 7 . ] ( continues on next page ) 4 . 3 . 데 이 터 조 작 ( data manipulation ) 59 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) [ 8 . 9 . 10 . 11 . ] ] < NDArray 3x4 @ cpu ( 0 ) > 여 러 원 소 에 같 은 값 을 할 당 하 고 싶 을 경 우 에 는 , 그 원 소 들 에 대 한 인 덱 스 를 모 두 지 정 해 서 값 을 할 당 하 는 것 으 로 간 단 히 할 수 있 습 니다 . 예 를 들 어 [ 0 : 2 , : ] 는 첫 번 째 와 두 번 째 행 을 의 미 합 니다 . 행 렬 에 대 한 인 덱 싱 을 이 야 기 해 왔 지 만 , 벡 터 나 2 개 보 다 많 은 차 원 을 갖 는 텐 서 에 도 동 일 하 게 적 용 됩 니다 . [ 22 ] : x [ 0 : 2 , : ] = 12 x [ 22 ] : [ [ 12 . 12 . 12 . 12 . ] [ 12 . 12 . 12 . 12 . ] [ 8 . 9 . 10 . 11 . ] ] < NDArray 3x4 @ cpu ( 0 ) > 4 . 3 . 5 메모 리 절 약 하 기 앞 의 예 제 들 모 두 연 산 을 수 행할 때 마 다 새 로 운 메모 리를 할 당 해 서 결과 를 저 장 합 니다 . 예 를 들 어 , y = x + y 를 수 행하 면 , 원 래 의 행 렬 y 에 대 한 참 조 는 제 거 되 고 , 새 로 할 당 된 메모 리를 참 조 하 도 록 동 작 합 니다 . 다 음 예 제 에 서 는 , 객 체 의 메모 리 주 소 를 반 환 하 는 Python 의 id ( ) 함 수 를 이 용 해 서 이 를 확 인 해 보 겠 습 니다 . y = x + y 수 행 후 , id ( y ) 는 다 른 위 치 를 가 리 키 고 있 습 니다 . 이 렇 게 되 는 이유 는 Python 은 y + x 연 산 결과 를 새 로 운 메모 리 에 저 장 하 고 , y 가 새 로 운 메모 리를 참 조 하 도 록 작 동 하 기 때 문 입 니다 . [ 23 ] : before = id ( y ) y = y + x id ( y ) = = before [ 23 ] : False 이 는 두 가 지 이유 로 바 람 직 하 지 않 을 수 있 습 니다 . 첫 번 째 로 는 매 번 불 필 요 한 메모 리를 할 당 하 는 것 을 원 하 지 않 습 니다 . 머 신 러 닝 에 서 는 수 백 메 가 바 이 트 의 파 라 미 터 들 을 매 초 마 다 여 러 번 업 데 이 트 를 수 행합 니다 . 대 부분 의 경 우 우 리 는 이 업 데 이 트 를 같 은 메모 리 ( in - place ) 를 사 용 해 서 수 행하 기 를 원 합 니다 . 두 번 째 는 여 러 변 수 들 이 같 은 파 라 미 터 를 가 리 키 고 있을 수 있 습 니다 . 같 은 메모 리 에 업 데 이 트 를 하 지 않 을 경 우 , 메모 리 누 수 가 발 생 하 고 , 래 퍼 런 스 가 유 효 하 지 않 은 파 라 미 터 를 만 드 는 문 제 가 발 생 할 수 있 습 니다 . 60 4 . 딥 러 닝 맛 보 기 다 행 히 도 MXNet 에 서 같 은 메모 리 연 산 은 간 단 합 니다 . 슬 라 이 스 표 기 법 y [ : ] = < expression > 을 이 용 하 면 이 전 에 할 당 된 배 열에 연 산 의 결과 를 저 장 할 수 있 습 니다 . zeros _ like 함 수 를 사 용 해 서 동 일 한 모 양 ( shape ) 을 갖고 원 소 가 모 두 0 인 행 렬 을 하 나 복 사 해 서 이 것 이 어 떻 게 동 작 하 는 지 보 겠 습 니다 . [ 24 ] : z = y . zeros _ like ( ) print ( ' id ( z ) : ' , id ( z ) ) z [ : ] = x + y print ( ' id ( z ) : ' , id ( z ) ) id ( z ) : 140587583495640 id ( z ) : 140587583495640 멋 져 보 이 지 만 , x + y 는 결과값 을 계 산 하 고 이 를 y [ : ] 에 복 사 하 기 전 에 이 값 을 저 장 하 는 임 시 버 퍼 를 여 전 히 할 당 합 니다 . 메모 리를 더 잘 사 용 하 기 위 해 서 , ndarray 연 산 ( 이 경 우 는 elemwise _ add ) 을 직 접 호 출 해 서 임 시 버 퍼 의 사 용 을 피할 수 있 습 니다 . 모 든 ndarray 연 산 자 가 제 공 하 는 out 키 워 드 를 이 용 하 면 됩 니다 . [ 25 ] : before = id ( z ) nd . elemwise _ add ( x , y , out = z ) id ( z ) = = before [ 25 ] : True x 값 이 프 로 그 램 에 서 더 이 상 사 용 되 지 않 을 경 우 , x [ : ] = x + y 이 나 x + = y 로 연 산 으 로 인 한 메모 리 추 가 사 용 을 줄 일 수 있 습 니다 . [ 26 ] : before = id ( x ) x + = y id ( x ) = = before [ 26 ] : True 4 . 3 . 6 NDArray 와 NumPy 간 상 호 변 환 MXNet NDArray 를 NumPy 로 변 환 하 는 것 은 간 단 합 니다 . 변 환 된 배 열 은 메모 리를 공 유 하 지 않 습 니 다 . 이 것 은 사 소 하 지 만 아 주 중 요 합 니다 . CPU 또 는 GPU 하 나 를 사 용 해 서 연 산 을 수 행할 때 , NumPy 가 동 일 한 메모 리 에 서 다 른 일을 수 행하 는 것 을 MXNet 이 기 다 리 는 것 을 원 하 지 않 기 때 문 입 니다 . array 와 asnumpy 함 수 를 이 용 하 면 변 환 을 할 수 있 습 니다 . 4 . 3 . 데 이 터 조 작 ( data manipulation ) 61 [ 27 ] : import numpy as np a = x . asnumpy ( ) print ( type ( a ) ) b = nd . array ( a ) print ( type ( b ) ) < class ’numpy . ndarray’ > < class ’mxnet . ndarray . ndarray . NDArray’ > 4 . 3 . 7 문 제 1 . 이 절 의 코 드 를 실 행하 세 요 . 조 건 문 x = = y 를 x < y 이 나 x > y 로 바 꿔 서 결과가 어 떻 게 되 는 지 확 인 하 세 요 . 2 . 다 른 모 양 ( shape ) 의 행 렬 들 에 브 로 드 케 스 팅 이 적 용 되 는 연 산 을 수 행하 는 두 NDArray 를 바 꿔 보 세 요 . 예 를 들 면 3 차 원 텐 서 로 바 꿔 보 세 요 . 예 상 한 결과 도 같 나 요 ? 3 . 행 렬 3 개 a , b , c 가 있을 경 우 , c = nd . dot ( a , b . T ) + c 를 가 장 메모 리 가 효 율 적 인 코 드 로 바 꿔 보 세 요 . 4 . 3 . 8 Scan the QR Code to Discuss 4 . 4 선 형 대 수 자 , 이 제 데 이 터 를 저 장 하 고 조 작 하 는 방법 을 배 웠 으 니 , 모 델 에 대 한 대 부분 을 이 해하 는 데 필 요 한 기 초 적 인 선 형 대 수 일 부 를 간 단 하 게 살 펴 보 겠 습 니다 . 기 초 적 인 개 념 과 관 련 된 수 학 표 기 법 , 그 리 고 코 드 로 의 구 현 까 지 모 두 소 개 할 것 입 니다 . 기 본 선 형 대 수 에 익 숙 하 다 면 , 이 절 은 빨 리 읽 거 나 다 음 절 로 넘 어 가 도 됩 니다 . 62 4 . 딥 러 닝 맛 보 기 [ 1 ] : from mxnet import nd 4 . 4 . 1 스 칼 라 ( scalar ) 선 형 대 수 나 머 신 러 닝 을 배 워 본 적 이 없 다 면 , 아 마 도 한 번 에 하 나 의 숫 자 를 다 루 는 데 익 숙 할 것 입 니 다 . 예 를 들 어 , 팔 로 알 토 의 기 온 이 화 씨 52 도 입 니다 . 공 식 용 어 를 사 용 하 면 이 값 은 스 칼 라 ( scalar ) 입 니다 . 이 값 을 섭 씨 로 바 꾸 기 를 원 한 다 면 , 𝑐 = ( 𝑓 − 32 ) * 5 / 9 공 식 에 𝑓 값 으 로 52 대 입 하 면 됩 니다 . 이 공 식 에 서 각 항 들 32 , 5 , 9 은 스 칼 라 값 입 니다 . 플 래 이 스 홀 더 𝑐 와 𝑓 를 변 수 라 고 부 르 고 , 아 직 정 해 지지 않 은 스 칼 라 값 들 을 위 해 있 습 니다 . 수 학 적 인 표 기 법 으 로 는 우 리 는 스 칼 라 를 소 문 자 ( 𝑥 ﬀ , 𝑦 ﬀ , 𝑧 ﬀ ) 로 표 기 합 니다 . 또 한 모 든 스 칼 라 에 대 한 공간 은 ℛ ﬀ 로 적 습 니다 . 이 후 에 공간 이 정 확 히 무 엇 인 지 를 알아 보 겠 지 만 , 편 의 상 지 금 은 𝑥 ﬀ 가 스 칼 라라 고 이 야 기 하 는 것 은 𝑥 ∈ ℛ ﬀ 로 표 현 하 기 로 하 겠 습 니다 . MXNet 에 서 스 칼 라 는 하 나 의 원 소 를 갖 는 NDArray 로 표 현 됩 니다 . 아 래 코 드 에 서 는 두 개 의 스 칼 라 를 생 성 하 고 , 친 숙 한 수 치 연 산 - 더 하 기 , 빼 기 , 나 누 기 , 그 리 고 제 곱 을 수 행합 니다 . [ 2 ] : x = nd . array ( [ 3 . 0 ] ) y = nd . array ( [ 2 . 0 ] ) print ( ' x + y = ' , x + y ) print ( ' x * y = ' , x * y ) print ( ' x / y = ' , x / y ) print ( ' x * * y = ' , nd . power ( x , y ) ) x + y = [ 5 . ] < NDArray 1 @ cpu ( 0 ) > x * y = [ 6 . ] < NDArray 1 @ cpu ( 0 ) > x / y = [ 1 . 5 ] < NDArray 1 @ cpu ( 0 ) > x * * y = [ 9 . ] < NDArray 1 @ cpu ( 0 ) > asscalar 메 소 드 를 이 용 하 면 NDArray 를 Python 의 ﬂoat 형 으 로 변 환 할 수 있 습 니다 . 하 지 만 이 렇 4 . 4 . 선 형 대 수 63 게 하 는 것 은 좋 은 아 이 디 어 가 아 님 을 알아 두 세 요 . 그 이유 는 이 를 수 행하 는 동 안 , 프 로 세 스 제 어 를 Python 에 게 줘 야 하 기 때 문 에 NDArray 는 결과 를 내 기 위 한 다 른 것 들 을 모 두 멈 춰 야 합 니다 . 아 쉽 게 도 , Python 은 병 렬로 일을 처 리 하 는 데 좋 지 못 합 니다 . 이 런 연 산 을 코 드 나 네 트 워 크 에 서 수 행한 다 면 학 습 하 는 데 오 랜 시 간 이 걸 릴 것 입 니다 . [ 3 ] : x . asscalar ( ) [ 3 ] : 3 . 0 4 . 4 . 2 벡 터 ( vector ) 벡 터 를 [ 1 . 0 , 3 . 0 , 4 . 0 , 2 . 0 ] 처 럼 숫 자 들 의 리 스 트 로 생 각 할 수 있 습 니다 . 벡 터 의 각 숫 자 는 하 나 의 스 칼 라 변 수 로 이 루 어 져 있 습 니다 . 이 숫 자 들 을 우 리 는 벡 터 의 원 소 또 는 구 성 요 소 라 고 부 릅 니다 . 종종 우 리 는 실 제 세 상 에 서 중 요 한 값 을 담 은 벡 터 들 에 관 심 을 갖 습 니다 . 예 를 들 어 채 무 불 이 행 위 험 을 연 구 하 고 있 다 면 , 모 든 지 원 자 를 원 소 가 수 입 , 재 직 기 간 , 이 전 의 불 이 행 횟 수 등 을 포 함한 벡 터 와 연 관 을 지 을 지 도 모 릅 니다 . 병 원 내 환 자 들 의 심 장 마 비 위 험 을 연 구 하 는 사 람 은 , 환 자 들 을 최 근 바 이 탈 사 인 , 콜 레 스 테 롤 지 수 , 하 루 운 동 시 간 등 을 원 소 로 갖 는 벡 터 로 표 현 할 것 입 니다 . 수 학 적 인 표 기 법 을 이 용 할 때 벡 터 는 굵 은 글 씨 체 로 소 문 자 ( u , v , w ) 를 사 용 해 서 표 현 합 니다 . MXNet 에 서 는 임의의 숫 자 를 원 소 로 갖 는 1D NDArray 를 이 용 해 서 벡 터 를 다 루 게 됩 니다 . [ 4 ] : x = nd . arange ( 4 ) print ( ' x = ' , x ) x = [ 0 . 1 . 2 . 3 . ] < NDArray 4 @ cpu ( 0 ) > 첨 자 를 사 용 해 서 벡 터 의 요 소 를 가 리 킬 수 있 습 니다 . 즉 , u ﬀ 의 4 번 째 요 소 는 𝑢 4 ﬀ 로 표 현 합 니다 . 𝑢 4 ﬀ 는 스 칼 라 이 기 때 문 에 굵 은 글 씨 가 아 닌 폰 트 로 표 기 하 는 것 을 유의 하 세 요 . 코 드 에 는 NDArray 의 𝑖 ﬀ 번 째 인 덱 스 로 이 를 지 정 합 니다 . [ 5 ] : x [ 3 ] [ 5 ] : [ 3 . ] < NDArray 1 @ cpu ( 0 ) > 64 4 . 딥 러 닝 맛 보 기 4 . 4 . 3 길 이 , 차 원 ( dimensionality ) , 모 양 ( shape ) 앞 절 에 서 소 개 한 개 념 몇 개 를 다 시 살 펴 보 겠 습 니다 . 벡 터 는 숫 자 들 의 배 열 입 니다 . 모 든 배 열 은 길 이 를 갖 듯 이 , 벡 터 도 길 이 를 갖 습 니다 . 벡 터 x 가 𝑛 개 의 실수 값 을 갖 는 스 칼 라 들 로 구 성 되 어 있 다 면 , 이 는 수 학 적 인 표 현 으 로 x ∈ ℛ 𝑛 로 적 습 니다 . 벡 터 의 길 이 는 일 반 적 으 로 차 원 ( 𝑑𝑖𝑚𝑒𝑛𝑠𝑖𝑜𝑛 ) 이 라 고 합 니다 . Python 배 열 처 럼 , NDArray 의 길 이 도 Python 의 내 장 함 수 len ( ) 를 통 해 서 얻 을 수 있 습 니다 . 벡 터 의 길 이 는 . shape 속성 으 로 도 얻 을 수 있 습 니다 . shape 은 NDArray 객 체 의 각 축 에 대 한 차 원 의 목 록 으 로 표 현 됩 니다 . 벡 터 는 축 이 하 나 이 기 때 문 에 , 벡 터 의 모 양 ( shape ) 은 하 나 의 숫 자 로 표 현 됩 니다 . [ 6 ] : x . shape [ 6 ] : ( 4 , ) 차 원 ( dimension ) 이 라 는 단 어 가 여 러 가 지 의 미 로 사 용 되 기 때 문 에 , 헷 갈 릴 수 도 있 습 니다 . 어 떤 경 우 에 는 벡 터 의 차 원 ( dimensionality ) 을 벡 터 의 길 이 ( 원 소 들 의 수 ) 로 사 용 하 기 도 하 고 , 어 떤 경 우 에 는 배 열 의 축 의 개 수 로 사 용 되 기 도 합 니다 . 후 자의 경 우 에 는 스 칼 라 는 0 차 원 을 갖고 , 벡 터 는 1 차 원 을 갖 습 니다 . 혼 동 을 줄 이 기 위 해 서 , 우 리 는 2D 배 열 또 는 3D 배 열 이 라 고 말 할 때 , 축 이 각각 2 개 3 개 인 배 열 을 의 미 하 도 록 합 니다 . 하 지 만 만 약 n - 차 원 벡 터 라 고 하 는 경 우 에 는 , 길 이 가 n 인 벡 터 를 의 미 합 니다 . [ 7 ] : a = 2 x = nd . array ( [ 1 , 2 , 3 ] ) y = nd . array ( [ 10 , 20 , 30 ] ) print ( a * x ) print ( a * x + y ) [ 2 . 4 . 6 . ] < NDArray 3 @ cpu ( 0 ) > [ 12 . 24 . 36 . ] < NDArray 3 @ cpu ( 0 ) > 4 . 4 . 4 행 렬 ( matrix ) 벡 터 가 오 더 0 인 스 칼 라 를 오 더 1 로 일 반 화 하 는 것 처 럼 , 행 렬 은 1 𝐷 에 서 2 𝐷 로 벡 터 를 일 반 화 합 니다 . 일 반 적 으 로 대 문 자 ( 𝐴 , 𝐵 , 𝐶 ) 로 표 현 하 는 행 렬 은 코 드 에 서 는 축 이 2 개 인 배 열 로 표 현 합 니다 . 시 각 화 4 . 4 . 선 형 대 수 65 한 다 면 , 행 렬 은 원 소 𝑎 𝑖𝑗 가 𝑖 - 열 , 𝑗 - 행 에 속 하 는 표 로 그 릴 수 있 습 니다 . 𝐴 = ⎛ ⎜⎜⎜⎜⎝ 𝑎 11 𝑎 12 · · · 𝑎 1 𝑚 𝑎 21 𝑎 22 · · · 𝑎 2 𝑚 . . . . . . . . . . . . 𝑎 𝑛 1 𝑎 𝑛 2 · · · 𝑎 𝑛𝑚 ⎞ ⎟⎟⎟⎟⎠ ﬀ MXNet 에 서 는 𝑛 행 , 𝑚 열 을 갖 는 행 렬 을 만 드 는 방법 은 두 요 소 를 갖 는 ( n , m ) 모 양 ( shape ) 을 이 용 해 서 ones 또 는 zeros 함 수 를 호 출 을 통 해 ndarray 를 얻 는 것 입 니다 . [ 8 ] : A = nd . arange ( 20 ) . reshape ( ( 5 , 4 ) ) print ( A ) [ [ 0 . 1 . 2 . 3 . ] [ 4 . 5 . 6 . 7 . ] [ 8 . 9 . 10 . 11 . ] [ 12 . 13 . 14 . 15 . ] [ 16 . 17 . 18 . 19 . ] ] < NDArray 5x4 @ cpu ( 0 ) > 행 렬 은 유 용 한 자 료 구 조 입 니다 . 행 렬 을 이 용 해 서 서 로 다 른 양 식 의 변 형 을 갖 는 데 이 터 를 구 성 할 수 있 습 니다 . 예 를 들 어 보 면 , 행 렬 의 행 들 은 서 로 다 른 환 자 에 대 한 정 보 를 , 열 은 서 로 다 른 속성 에 대 한 정 보 를 의 미 할 수 있 습 니다 . 행 렬 𝐴 의 스 칼 라 원 소 𝑎 𝑖𝑗 을 지 정 하 는 방법 은 행 ( 𝑖 ) 과 열 ( 𝑗 ) 에 대 한 인 덱 스 를 지 정 하 면 됩 니다 . : 를 사 용 해 서 공 백 으 로 두 면 , 해 당 차 원 의 모 든 원 소 를 의 미 합 니다 . ( 앞 절 에 서 봤 던 방법 입 니다 . ) 행 렬 을 전 치 하 는 방법 은 T 를 이 용 합 니다 . 전 치 행 렬 은 만 약 𝐵 = 𝐴 𝑇 ﬀ 이 면 , 모 든 𝑖 ﬀ 과 𝑗 ﬀ 에 대 해 서 𝑏 𝑖𝑗 = 𝑎 𝑗𝑖 ﬀ 인 행 렬 을 의 미 합 니다 . [ 9 ] : print ( A . T ) [ [ 0 . 4 . 8 . 12 . 16 . ] [ 1 . 5 . 9 . 13 . 17 . ] [ 2 . 6 . 10 . 14 . 18 . ] [ 3 . 7 . 11 . 15 . 19 . ] ] < NDArray 4x5 @ cpu ( 0 ) > 66 4 . 딥 러 닝 맛 보 기 4 . 4 . 5 텐 서 ( tensor ) 벡 터 가 스 칼 라 를 일 반 화 하 고 , 행 렬 이 벡 터 를 일 반 화 하 는 것 처 럼 더 많 은 축 을 갖 는 자 료 구 조 를 만 들 수 있 습 니다 . 텐 서 ( tensor ) 는 임의의 개 수 의 축 을 갖 는 행 렬 을 표 현 하 는 일 반 적 인 방법 을 제 공 합 니다 . 예 를 들 어 벡 터 는 1 차 오 더 ( order ) 텐 서 이 고 , 행 렬 은 2 차 오 더 ( order ) 텐 서 입 니다 . 3D 자 료 구 조 를 갖 는 이 미 지 를 다 룰 때 텐 서 를 사 용 하 는 것 은 아 주 중 요 하 게 됩 니다 . 즉 , 각 축 이 높 이 , 넓 이 , 그 리 고 세 가 지 색 ( RGB ) 채 널 을 의 미 합 니다 . 지 금 은 이 것 에 대 한 내 용 은 생 략 하 고 , 기 본 적 인 것 을 확 실 하 게 아 는 것 을 목 표 로 하 겠 습 니다 . [ 10 ] : X = nd . arange ( 24 ) . reshape ( ( 2 , 3 , 4 ) ) print ( ' X . shape = ' , X . shape ) print ( ' X = ' , X ) X . shape = ( 2 , 3 , 4 ) X = [ [ [ 0 . 1 . 2 . 3 . ] [ 4 . 5 . 6 . 7 . ] [ 8 . 9 . 10 . 11 . ] ] [ [ 12 . 13 . 14 . 15 . ] [ 16 . 17 . 18 . 19 . ] [ 20 . 21 . 22 . 23 . ] ] ] < NDArray 2x3x4 @ cpu ( 0 ) > 4 . 4 . 6 텐 서 연 산 의 기 본 성 질 스 칼 라 , 벡 터 , 행 렬 , 그 리 고 어 떤 오 더 를 갖 는 텐 서 들 은 우 리 가 자 주 사 용 할 유 용 한 특 성 들 을 가 지 고 있 습 니다 . 요 소 별 연 산 ( element - wise operation ) 의 정 의 에 서 알 수 있 듯 이 , 같 은 모 양 ( shape ) 들 에 대 해 서 연 산 을 수 행하 면 , 요 소 별 연 산 의 결과 는 같 은 모 양 ( shape ) 을 갖 는 텐 서 입 니다 . 또 다 른 유 용 한 특 성 은 모 든 텐 서 에 대 해 서 스 칼 라 를 곱 하 면 결과 는 같 은 모 양 ( shape ) 의 텐 서 입 니다 . 수 학 적 으 로 표 현 하 면 , 같 은 모 양 ( shape ) 의 두 텐 서 𝑋 와 𝑌 가 있 다 면 𝛼𝑋 + 𝑌 는 같 은 모 양 ( shape ) 을 갖 습 니다 . [ 11 ] : a = 2 x = nd . ones ( 3 ) y = nd . zeros ( 3 ) print ( x . shape ) print ( y . shape ) ( continues on next page ) 4 . 4 . 선 형 대 수 67 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) print ( ( a * x ) . shape ) print ( ( a * x + y ) . shape ) ( 3 , ) ( 3 , ) ( 3 , ) ( 3 , ) 더 하 기 와 스 칼 라 곱 으 로 보 존 되 는 특 성 이 모 양 ( shape ) 뿐 만 은 아 닙니다 . 이 연 산 들 은 벡 터 공간 의 맴 버 쉽 을 보 존 해 줍 니다 . 하 지 만 , 여 러 분 의 첫 번 째 모 델 을 만 들 어 서 수 행하 는 데 중 요 하 지 않 기 때 문 에 , 이 장의 뒤 에 서 설 명 하 겠 습 니다 . 4 . 4 . 7 합 과 평 균 임의의 텐 서 들 로 수 행할 수 있 는 조 금 더 복 잡 한 것 은 각 요 소 의 합 을 구 하 는 것 입 니다 . 수 학 기 호 로 는 합 을 ∑︀ ﬀ 로 표 시 합 니다 . 길 이 가 𝑑 ﬀ 인 벡 터 u ﬀ 의 요 소 들 의 합 은 ∑︀ 𝑑𝑖 = 1 𝑢 𝑖 ﬀ 로 표 현 하 고 , 코 드 에 서 는 nd . sum ( ) 만 호 출 하 면 됩 니다 . [ 12 ] : print ( x ) print ( nd . sum ( x ) ) [ 1 . 1 . 1 . ] < NDArray 3 @ cpu ( 0 ) > [ 3 . ] < NDArray 1 @ cpu ( 0 ) > 임의의 모 양 ( shape ) 을 갖 는 텐 서 의 원 소 들 의 합 을 비 슷 하 게 표 현 할 수 있 습 니다 . 예 를 들 어 𝑚 × 𝑛 행 렬 𝐴 의 원 소 들 의 합 은 ∑︀ 𝑚𝑖 = 1 ∑︀ 𝑛𝑗 = 1 𝑎 𝑖𝑗 이 고 , 코 드 로 는 다 음 과 같 습 니다 . [ 13 ] : print ( A ) print ( nd . sum ( A ) ) [ [ 0 . 1 . 2 . 3 . ] [ 4 . 5 . 6 . 7 . ] [ 8 . 9 . 10 . 11 . ] [ 12 . 13 . 14 . 15 . ] ( continues on next page ) 68 4 . 딥 러 닝 맛 보 기 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) [ 16 . 17 . 18 . 19 . ] ] < NDArray 5x4 @ cpu ( 0 ) > [ 190 . ] < NDArray 1 @ cpu ( 0 ) > 합 과 관 련 된 것 으 로 평 균 ( mean ) 이 있 습 니다 . ( average 라 고 도 합 니다 . ) 평 균 은 원 소 들 의 합 을 원 소 들 의 개 수 로 나 눠 서 구 합 니다 . 어 떤 벡 터 u 의 평 균 을 수 학 기 호 로 표 현 하 면 1 𝑑 ∑︀ 𝑑𝑖 = 1 𝑢 𝑖 이 고 , 행 렬 𝐴 에 대 한 평 균 은 s 1 𝑛 · 𝑚 ∑︀ 𝑚𝑖 = 1 ∑︀ 𝑛𝑗 = 1 𝑎 𝑖𝑗 이 됩 니다 . 코 드 로 구 현 하 면 , 임의의 shape 을 갖 는 텐 서 의 평 균 은 nd . mean ( ) 을 호 출 해 서 구 합 니다 . [ 14 ] : print ( nd . mean ( A ) ) print ( nd . sum ( A ) / A . size ) [ 9 . 5 ] < NDArray 1 @ cpu ( 0 ) > [ 9 . 5 ] < NDArray 1 @ cpu ( 0 ) > 4 . 4 . 8 점 곱 ( dot product ) 지 금까 지 는 원 소 들 사 이 에 연 산 인 더 하 기 와 평 균 에 대 해 서 살 펴 봤 습 니다 . 이 연 산 들 이 우 리 가 할 수 있 는 전 부 라 면 , 선 형 대 수 를 별 도 의 절 로 만 들 어 서 설 명 할 필 요 가 없 을 것 입 니다 . 즉 , 가 장 기 본 적 인 연 산 들 중 에 하 나 로 점 곱 ( dot product ) 이 있 습 니다 . 두 벡 터 , u ﬀ 와 v ﬀ , 가 주 어 졌 을 때 , 점 곱 , u 𝑇 v ﬀ , 은 요 소 들 끼 리 곱 을 한 결과 에 대 한 합 이 됩 니다 . 즉 , u 𝑇 v = ∑︀ 𝑑𝑖 = 1 𝑢 𝑖 · 𝑣 𝑖 ﬀ . [ 15 ] : x = nd . arange ( 4 ) y = nd . ones ( 4 ) print ( x , y , nd . dot ( x , y ) ) [ 0 . 1 . 2 . 3 . ] < NDArray 4 @ cpu ( 0 ) > [ 1 . 1 . 1 . 1 . ] < NDArray 4 @ cpu ( 0 ) > ( continues on next page ) 4 . 4 . 선 형 대 수 69 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) [ 6 . ] < NDArray 1 @ cpu ( 0 ) > 두 벡 터 의 점 곱 nd . dot ( x , y ) , 은 원 소 들 끼 리 의 곱 을 수 행한 후 , 합 을 구 하 는 것과 동 일 합 니다 . [ 16 ] : nd . sum ( x * y ) [ 16 ] : [ 6 . ] < NDArray 1 @ cpu ( 0 ) > 점 곱 은 다 양 한 경 우 에 유 용 하 게 사 용 됩 니다 . 예 를 들 어 , 가 중 치 들 의 집 합 w 에 대 해 서 , 어 떤 값 𝑢 의 가 중 치 를 적 용 한 합 은 점 곱 인 u 𝑇 w 으 로 계 산 될 수 있 습 니다 . 가 중 치 들 이 0 또 는 양 수 이 고 , 합 이 1 (︁∑︀ 𝑑𝑖 = 1 𝑤 𝑖 = 1 )︁ 인 경 우 , 행 렬 의 곱 은 가 중 치 평 균 ( weighted average ) 을 나 타 냅 니다 . 길 이 가 1 인 두 벡 터 ( 길 이 가 무 엇 인 지 는 아 래 에 서 norm 을 설 명 할 때 다 룹 니다 ) 가 있을 때 , 점 곱 을 통 해 서 두 벡 터 사 이의 코 사 인 각 을 구 할 수 있 습 니다 . 4 . 4 . 9 행 렬 - 벡 터 곱 점 곱 을 어 떻 게 계 산 하 는 지 알아 봤 으 니 , 행 렬 - 벡 터 곱 을 알아 볼 준 비 가 되 었 습 니다 . 우 선 행 렬 𝐴 와 열 벡 터 x 를 시 각 적 으 로 표 현 하 는 것 으 로 시 작 합 니다 . 𝐴 = ⎛ ⎜⎜ ⎜ ⎜⎝ 𝑎 11 𝑎 12 · · · 𝑎 1 𝑚 𝑎 21 𝑎 22 · · · 𝑎 2 𝑚 . . . . . . . . . . . . 𝑎 𝑛 1 𝑎 𝑛 2 · · · 𝑎 𝑛𝑚 ⎞ ⎟⎟ ⎟ ⎟⎠ , x = ⎛ ⎜⎜ ⎜ ⎜⎝ 𝑥 1 𝑥 2 . . . 𝑥 𝑚 ⎞ ⎟⎟ ⎟ ⎟⎠ ﬀ 행 렬 을 다 시 행 벡 터 형 태 로 표 현 이 가 능 합 니다 . 𝐴 = ⎛ ⎜⎜⎜⎜⎝ a 𝑇 1 a 𝑇 2 . . . a 𝑇 𝑛 ⎞ ⎟⎟⎟⎟⎠ , ﬀ 여 기 서 각 a 𝑇𝑖 ∈ R 𝑚 는 행 렬 의 𝑖 번 째 행 을 표 시 하 는 행 벡 터 입 니다 . 70 4 . 딥 러 닝 맛 보 기 그 러 면 행 렬 - 벡 터 곱 y = 𝐴 x ﬀ 은 컬 럼 벡 터 y ∈ R 𝑛 ﬀ 이 며 , 각 원 소 𝑦 𝑖 ﬀ 는 점 곱 a 𝑇𝑖 x ﬀ 입 니다 . 𝐴 x = ⎛ ⎜⎜⎜⎜⎝ a 𝑇 1 a 𝑇 2 . . . a 𝑇𝑛 ⎞ ⎟⎟⎟⎟⎠ ⎛ ⎜⎜⎜⎜⎝ 𝑥 1 𝑥 2 . . . 𝑥 𝑚 ⎞ ⎟⎟⎟⎟⎠ = ⎛ ⎜⎜⎜⎜⎝ a 𝑇 1 x a 𝑇 2 x . . . a 𝑇𝑛 x ⎞ ⎟⎟⎟⎟⎠ 즉 , 행 렬 𝐴 ∈ R 𝑛 × 𝑚 ﬀ 로 곱 하 는 것 을 벡 터 를 R 𝑚 ﬀ 에 서 R 𝑛 ﬀ 로 사 영 시 키 는 변 환 으 로 도 생 각 할 수 있 습 니다 . 이 런 변 환 은 아 주 유 용 하 게 쓰 입 니다 . 예 를 들 면 , 회 전 을 정 사 각 행 렬 의 곱 으 로 표 현 할 수 있 습 니다 . 다 음 절 에 서 보 겠 지 만 , 행 렬 - 벡 터 곱 을 뉴 럴 네 트 워 크 의 각 층 의 연 산 을 표 현 하 는 데도 사 용 합 니다 . ndarray ' 를 이 용 해 서 행 렬 - 벡 터 의 곱 을 계 산 할 때 는 점 곱 에 서 사 용 했 던 nd . dot ( ) 함 수 를 동 일 하 게 사 용 합 니다 . 행 렬 A 와 벡 터 x 를 이 용 해 서 nd . dot ( A , x ) 를 호 출 하 면 , MXNet 은 행 렬 - 벡 터 곱 을 수 행해 야 한 다는 것 을 압 니다 . A 의 열 의 개 수 와 x 의 차 원 이 같 아야 한 다는 점 을 유의 하 세 요 . [ 17 ] : nd . dot ( A , x ) [ 17 ] : [ 14 . 38 . 62 . 86 . 110 . ] < NDArray 5 @ cpu ( 0 ) > 4 . 4 . 10 행 렬 - 행 렬 곱 점 곱과 행 렬 - 벡 터 곱 을 잘 이 해했 다 면 , 행 렬 - 행 렬 곱 은 아 주 간 단 할 것 입 니다 . 행 렬 𝐴 ∈ R 𝑛 × 𝑘 ﬀ , 𝐵 ∈ R 𝑘 × 𝑚 ﬀ 가 있 다 고 하 겠 습 니다 . 𝐴 = ⎛ ⎜⎜⎜⎜⎝ 𝑎 11 𝑎 12 · · · 𝑎 1 𝑘 𝑎 21 𝑎 22 · · · 𝑎 2 𝑘 . . . . . . . . . . . . 𝑎 𝑛 1 𝑎 𝑛 2 · · · 𝑎 𝑛𝑘 ⎞ ⎟⎟⎟⎟⎠ , 𝐵 = ⎛ ⎜⎜⎜⎜⎝ 𝑏 11 𝑏 12 · · · 𝑏 1 𝑚 𝑏 21 𝑏 22 · · · 𝑏 2 𝑚 . . . . . . . . . . . . 𝑏 𝑘 1 𝑏 𝑘 2 · · · 𝑏 𝑘𝑚 ⎞ ⎟⎟⎟⎟⎠ ﬀ 4 . 4 . 선 형 대 수 71 행 렬 의 곱 𝐶 = 𝐴𝐵 를 계 산 하 기 위 해 서 , 𝐴 를 행 벡 터 들 로 , 𝐵 를 열 벡 터 들 로 생 각 하 면 쉽습 니다 . 𝐴 = ⎛ ⎜⎜⎜⎜⎝ a 𝑇 1 a 𝑇 2 . . . a 𝑇𝑛 ⎞ ⎟⎟⎟⎟⎠ , 𝐵 = (︁ b 1 b 2 · · · b 𝑚 )︁ . 각 행 벡 터 a 𝑇𝑖 는 R 𝑘 에 속 하 고 , 각 열 벡 터 b 𝑗 는 R 𝑘 에 속 한 다는 것 을 주 의 하 세 요 . 그 러 면 , 행 렬 𝐶 ∈ R 𝑛 × 𝑚 의 각 원 소 𝑐 𝑖𝑗 는 a 𝑇𝑖 b 𝑗 로 구 해 집 니다 . 𝐶 = 𝐴𝐵 = ⎛ ⎜⎜⎜⎜⎝ a 𝑇 1 a 𝑇 2 . . . a 𝑇𝑛 ⎞ ⎟⎟⎟⎟⎠ (︁ b 1 b 2 · · · b 𝑚 )︁ = ⎛ ⎜⎜⎜⎜⎝ a 𝑇 1 b 1 a 𝑇 1 b 2 · · · a 𝑇 1 b 𝑚 a 𝑇 2 b 1 a 𝑇 2 b 2 · · · a 𝑇 2 b 𝑚 . . . . . . . . . . . . a 𝑇𝑛 b 1 a 𝑇𝑛 b 2 · · · a 𝑇𝑛 b 𝑚 ⎞ ⎟⎟⎟⎟⎠ 행 렬 - 행 렬 곱 𝐴𝐵 을 단 순 히 𝑚 개 의 행 렬 - 벡 터 의 곱 을 수 행한 후 , 결과 를 붙 여 서 𝑛 × 𝑚 행 렬로 만 드 는 것 으 로 생 각 할 수 도 있 습 니다 . 일 반 적 인 점 곱과 행 렬 - 벡 터 곱 을 계 산 하 는 것 처 럼 MXNet 에 서 행 렬 - 행 렬 의 곱 은 nd . dot ( ) 으 로 계 산 됩 니다 . [ 18 ] : B = nd . ones ( shape = ( 4 , 3 ) ) nd . dot ( A , B ) [ 18 ] : [ [ 6 . 6 . 6 . ] [ 22 . 22 . 22 . ] [ 38 . 38 . 38 . ] [ 54 . 54 . 54 . ] [ 70 . 70 . 70 . ] ] < NDArray 5x3 @ cpu ( 0 ) > 4 . 4 . 11 놈 ( norm ) 모 델 을 구 현 하 기 전 에 배 워 야 할 개 념 이 하 나 더 있 습 니다 . 선 형 대 수 에 서 가 장 유 용 한 연 산 중 에 놈 ( norm ) 이 있 습 니다 . 엄 밀 하 지 않 게 설 명 하 면 , 놈 은 벡 터 나 행 렬 이 얼 마 나 큰 지 를 알 려 주 는 개 념 입 니 다 . ‖ · ‖ 으 로 놈 을 표 현 하 는 데 , · 은 행 렬 이 나 벡 터 가 들 어 갈 자 리 입 니다 . 예 를 들 면 , 벡 터 x 나 행 렬 𝐴 를 각각 ‖ x ‖ or ‖ 𝐴 ‖ 로 적 습 니다 . 모 든 놈 은 다 음 특 성 을 만 족 시 켜 야 합 니다 . 72 4 . 딥 러 닝 맛 보 기 1 . ‖ 𝛼𝐴 ‖ = | 𝛼 | ‖ 𝐴 ‖ 2 . ‖ 𝐴 + 𝐵 ‖ ≤ ‖ 𝐴 ‖ + ‖ 𝐵 ‖ 3 . ‖ 𝐴 ‖ ≥ 0 4 . If ∀ 𝑖 , 𝑗 , 𝑎 𝑖𝑗 = 0 , then ‖ 𝐴 ‖ = 0 위 규 칙 을 말 로 설 명 하 면 , 첫 번 째 규 칙 은 행 렬 이 나 벡 터 의 모 든 원 소 에 상 수 𝛼 만 큼 스 캐 일을 바 꾸 면 , 놈 도 그 상 수 의 절 대 값 만 큼 스 캐 일이 바 뀐 다는 것 입 니다 . 두 번 째 규 칙 은 친 숙 한 삼 각 부 등 식 입 니다 . 세 번 째 는 놈 은 음 수 가 될 수 없 다는 것 입 니다 . 거 의 모 든 경 우 에 가 장 작은 크 기 가 0 이 기 에 이 규 칙 은 당 연 합 니다 . 마 지 막 규 칙 은 가 장 작은 놈 은 행 렬 또 는 벡 터 가 0 으 로 구 성 되 었 을 경 우 라 는 기 본 적 인 것 에 대 한 것 입 니다 . 0 이 아 닌 행 렬 에 놈 이 0 이 되 도 록 놈 을 정 의 하 는 것 이 가 능 합 니다 . 하 지 만 , 0 인 행 렬 에 0 이 아 닌 놈 이 되 게 하 는 놈 을 정 의 하 는 것 은 불 가 능 합 니다 . 길 게 설 명 했 지 만 , 이 것 을 이 해했 다 면 중 요 한 개 념 을 얻었 을 것 입 니다 . 수 학 시 간 에 배 운 유 클 리 디 안 거 리 ( Euclidean distance ) 를 기 억 한 다 면 , 0 이 아 닌 것과 삼 각 부 등 식 이 떠 오 를 것 입 니다 . 놈 이 거 리를 측 정 하 는 것과 비 슷 하 다는 것 을 인 지 했 을 것 입 니다 . 사 실 유 클 리 디 안 거 리 √︀ 𝑥 21 + · · · + 𝑥 2 𝑛 는 놈 입 니다 . 특 히 , 이 를 ℓ 2 - 놈 이 라 고 합 니다 . 행 렬 의 각 원 소 에 대 해 서 유 사 하 게 계 산 한 것 √︁∑︀ 𝑖 , 𝑗 𝑎 2 𝑖𝑗 을 푸 로 베 니 우 스 놈 ( Frobenius norm ) 이 라 고 합 니다 . 머 신 러 닝 에 서 는 자 주 제 곱 ℓ 2 놈 을 사 용 합 니다 . ( ℓ 22 로 표 현 합 니다 . ) ℓ 1 놈 도 흔히 사 용 합 니다 . ℓ 1 놈 은 절 대 값 들 의 합 으 로 , 이 상 치 ( outlier ) 에 덜 중 점 을 주 는 편 리 한 특 성 이 있 습 니다 . ℓ 2 ﬀ 놈 의 계 산 은 nd . norm ( ) 으 로 합 니다 . [ 19 ] : nd . norm ( x ) [ 19 ] : [ 3 . 7416573 ] < NDArray 1 @ cpu ( 0 ) > ℓ 1 ﬀ 놈 을 계 산 하 는 방법 은 각 원 소 의 절 대 값 을 구 한 후 , 모 두 합하 는 것 입 니다 . [ 20 ] : nd . sum ( nd . abs ( x ) ) [ 20 ] : [ 6 . ] < NDArray 1 @ cpu ( 0 ) > 4 . 4 . 선 형 대 수 73 4 . 4 . 12 놈 ( norm ) 와 목 적 ( objective ) 더 깊 이 나 가 지 는 않 겠 지 만 , 이 개 념 들 이 왜 중 요 한 지 궁 금 할 것 입 니다 . 머 신 러 닝 에 서 우 리 는 종종 최 적 화 문 제 를 풀 기 를 시 도 합 니다 - 즉 , 관 찰 된 데 이 터 에 할 당 된 확 률 을 최 대 화 하 기 , 예 측 된 값과 실 제 값 의 차 이 를 최 소 화 하 기 , 단 어 , 제 품 , 새 로 운 기 사 와 같 은 아 이 템 들 에 가 까 운 아 이 템 들 의 거 리 가 최 소 화 되 는 벡 터 를 할 당 하 기 등 을 시 도 합 니다 . 아 마 도 머 신 러 닝 알 고 리 즘 의 ( 데 이 터 를 제 외 한 ) 가 장 중 요 한 요 소 인 이 목 적 ( objective ) 들 은 자 주 놈 ( norm ) 으 로 표 현 됩 니다 . 4 . 4 . 13 중 급 선 형 대 수 여 러 분 이 여 기까 지 잘 따 라 오 면 서 모 든 내 용 을 이 해했 다 면 , 솔 직 하 게 여 러 분 은 모 델 을 시 작 할 준 비 가 되 었 습 니다 . 먄 약 조 급 함 을 느 낀 다 면 , 이 절 의 나 머 지 는 넘 어 가 도 됩 니다 . 실 제 로 적 용 할 수 있 는 유 용 한 모 델 들 을 구 현 하 는 데 필 요 한 모 든 선 형 대 수 에 대 해 서 알아 봤 고 , 더 알 고 싶 으 면 다 시 돌 아 올 수 있 습 니다 . 하 지 만 , 머 신 러 닝 만 고 려 해 봐 도 선 형 대 수 에 대 한 더 많 은 내 용 이 있 습 니다 . 이 후 어 느 시 점 에 여 러 분 이 머 신 러 닝 경 력 을 만 들 기 를 원 한 다 면 , 여 기 서 다 룬 것 보 다 더 많 은 것 을 알아야 할 것 입 니다 . 유 용 하 고 더 어 려 운 개 념 을 소 개 하 면 서 이 절 을 마 치 겠 습 니다 . 4 . 4 . 14 벡 터 의 기 본 성 질 들 벡 터 는 숫 자 를 담는 자 료 구 조 보 다 더 유 용 합 니다 . 벡 터 의 원 소 에 숫 자 를 읽 고 적 는 것 , 유 용 한 수 학 연 산 을 수 행하 는 것과 더 불 어 , 벡 터 를 재 미 있 는 방법 으 로 분 석 할 수 있 습 니다 . 벡 터 공간 의 개 념 은 중 요 한 개 념 입 니다 . 벡 터 공간 이 되 기 에 필 요 한 조 건 은 다 음 과 같 습 니다 . • 더 하 기 공 리 ( Additive axioms ) ( x , y , z 가 모 두 벡 터 라 고 가 정 합 니다 . ) : 𝑥 + 𝑦 = 𝑦 + 𝑥 ﬀ , ( 𝑥 + 𝑦 ) + 𝑧 = 𝑥 + ( 𝑦 + 𝑧 ) ﬀ , 0 + 𝑥 = 𝑥 + 0 = 𝑥 ﬀ 그 리 고 ( − 𝑥 ) + 𝑥 = 𝑥 + ( − 𝑥 ) = 0ﬀ . • 곱 하 기 공 리 ( Multiplicative axioms ) ( x 는 벡 터 이 고 a , b 는 스 칼 라 입 니다 . ) : 0 · 𝑥 = 0ﬀ , 1 · 𝑥 = 𝑥 ﬀ , ( 𝑎𝑏 ) 𝑥 = 𝑎 ( 𝑏𝑥 ) ﬀ . • 분 배 공 리 ( Distributive axioms ) ( x 와 y 는 벡 터 , a , b 는 스 칼 라 로 가 정 합 니다 . ) : 𝑎 ( 𝑥 + 𝑦 ) = 𝑎𝑥 + 𝑎𝑦 ﬀ and ( 𝑎 + 𝑏 ) 𝑥 = 𝑎𝑥 + 𝑏𝑥 ﬀ . 4 . 4 . 15 특 별 한 행 렬 들 이 책 에 서 사 용 할 특 별 한 행 렬 들 이 있 습 니다 . 그 행 렬 들 에 대 해 서 조 금 자 세 히 보 겠 습 니다 . 74 4 . 딥 러 닝 맛 보 기 • 대 칭 행 렬 ( Symmetric Matrix ) 이 행 렬 들 은 대 각 선 아 래 , 위의 원 소 들 이 같 은 값 을 갖 습 니다 . 즉 , 𝑀 ⊤ = 𝑀 입 니다 . 이 런 예 로 는 짝 들 의 ( pairwise ) 거 리를 표 현 하 는 행 렬 𝑀 𝑖𝑗 = ‖ 𝑥 𝑖 − 𝑥 𝑗 ‖ 이 있 습 니다 . 페 이 스 북 친 구 관 계 를 대 칭 행 렬로 표 현 할 수 있 습 니다 . 𝑖 와 𝑗 가 친 구 라 면 𝑀 𝑖𝑗 = 1 이 되 고 , 친 구 가 아 니 라 면 𝑀 𝑖𝑗 = 0 로 표 현 하 면 됩 니다 . 하 지 만 , 트 위 터 그 래 프 는 대 칭 이 아 님 을 주 목 해 세 요 . 𝑀 𝑖𝑗 = 1 , 즉 𝑖 가 𝑗 를 팔 로 우 하 는 것 이 꼭 𝑗 가 𝑖 를 팔 로 우 하 는 것 , 𝑀 𝑗𝑖 = 1 , 은 아 니 기 때 문 입 니다 . • 비 대 칭 행 렬 ( Antisymmetric Matrix ) 𝑀 ⊤ = − 𝑀 ﬀ 를 만 족 하 는 행 렬 입 니다 . 임의의 행 렬 은 대 칭 행 렬 과 비 대 칭 행 렬로 분 해 될 수 있 습 니다 . 즉 , 𝑀 = 12 ( 𝑀 + 𝑀 ⊤ ) + 12 ( 𝑀 − 𝑀 ⊤ ) ﬀ 로 표 현 될 수 있 습 니다 . • 대 각 지 배 행 렬 ( Diagonally Dominant Matrix ) 대 각 원 소 들 보 다 대 각 이 아 닌 원 소 들 이 작은 행 렬 입 니다 . 즉 , 𝑀 𝑖𝑖 ≥ ∑︀ 𝑗 ̸ = 𝑖 𝑀 𝑖𝑗 이 고 𝑀 𝑖𝑖 ≥ ∑︀ 𝑗 ̸ = 𝑖 𝑀 𝑗𝑖 입 니다 . 어 떤 행 렬 이 이 특 성 을 갖 는다 면 , 대 각 원 소 를 사 용 해 서 𝑀 을 추 정 할 수 있 고 , 이 를 diag ( 𝑀 ) 로 표 기 합 니다 . • 양 의 정 부 호 행 렬 ( Positive Deﬁnite Matrix ) 이 행 렬 은 𝑥 ̸ = 0 이 면 , 𝑥 ⊤ 𝑀𝑥 > 0 인 좋 은 특 성 을 갖 습 니다 . 직 관 적 으 로 설 명 하 면 , 벡 터 의 제 곱 놈 , ‖ 𝑥 ‖ 2 = 𝑥 ⊤ 𝑥 , 의 일 반 화 입 니다 . 𝑀 = 𝐴 ⊤ 𝐴 이 면 이 조 건 이 만 족 시 킨 다는 것 을 쉽 게 확 인 할 수 있 습 니다 . 이유 는 𝑥 ⊤ 𝑀𝑥 = 𝑥 ⊤ 𝐴 ⊤ 𝐴𝑥 = ‖ 𝐴𝑥 ‖ 2 이 기 때 문 입 니다 . 모 든 양 의 정 부 호 행 렬 은 이 런 형 태 로 표 현 될 수 있 다는 더 심 오 한 이 론 이 있 습 니다 . 4 . 4 . 16 요 약 몇 페 이 지 들 ( 또 는 Jupyter 노 트 북 한 개 ) 을 통 해 서 뉴 럴 네 트 워 크 의 중 요 한 부분 들 을 이 해하 는 데 필 요 한 모 든 선 형 대 수 에 대 해 서 알아 봤 습 니다 . 물 론 선 형 대 수 에 는 더 많 은 내 용 이 있 고 , 이 것 들 은 머 신 러 닝 에 유 용 하 게 쓰 입 니다 . 예 를 들 어 , 행 렬 을 분 해할 수 있 는 데 , 이 분 해 는 실 세 계 의 데 이 터 셋 의 아 래 차 원 의 구 조 를 알 려 주 기 도 합 니다 . 행 렬 분 해 를 이 용 하 는 데 집중 하 는 머 신 러 닝 의 별 도 의 분 야 가 있 습 니다 . 이 를 이 용 해 서 데 이 터 의 구 조 를 밝 히 고 예 측 문 제 를 풀 기 위 해 서 고 차 원 의 텐 서 를 일 반 화 하 기 도 합 니다 . 하 지 만 이 책 에 서 는 딥 러 닝 에 집중 합 니다 . 여 러 분 이 실 제 데 이 터 를 사 용 해 서 유 용 한 머 신 러 닝 모 델 을 만 들 기 시 작 한 다 면 , 수 학 에 대 해 서 더 관 심 을 갖게 될 것 이 라 고 믿 습 니다 . 하 지 만 수 학 적 인 내 용 은 나 중 에 더 설 명 하 기 로 하 고 , 이 절 은 여 기 서 마 무 리 하 겠 습 니다 . 선 형 대 수 에 대 해 서 더 배 우 기 를 원 한 다 면 , 유 용 한 교 재 들 이 있 습 니다 . • 탄탄 한 기 초 를 쌓 고 싶 으 면 , Gilbert Strang 의 책 Introduction to Linear Algebra 를 참 고 하 세 요 . • Zico Kolter’s Linear Algebra Review and Reference 4 . 4 . 선 형 대 수 75 4 . 4 . 17 Scan the QR Code to Discuss 4 . 5 자 동 미 분 ( automatic differentiation ) 머 신 러 닝 에 서 우 리 는 경 험 의 함 수 로 써 모 델 을 더 욱 좋 게 만 들 려 는 목 적 으 로 학 습 을 합 니다 . 보 통 은 더 좋 게 만 드 는 것 은 손 실 함 수 ( loss function ) , 즉 , “ 모 델 이 얼 마 나 못 하 냐 ” 에 대 한 점 수 를 최 소 화 하 는 것 을 의 미 합 니다 . 뉴 럴 네 트 워 크 에 서 는 파 라 미 터 에 대 해 서 미 분 이 가 능 한 손 실 함 수 ( loss function ) 를 선 택 합 니다 . 간 단 하 게 설 명 하 면 , 모 델 의 각 파 라 미 터 들 이 손 실 ( loss ) 을 얼 마만 큼 증 가 또 는 감 소 시 키 는 데 영 향 을 주 는 지 를 결 정 할 수 있 다는 것 입 니다 . 미 적 분 이 직 관 적 이 지 만 , 복 잡 한 모 델 들 에 대 한 미 분 을 직 접 계 산 하 는 것 은 너 무 힘 들 고 오 류를 범 할 수 있 는 일입 니다 . autograd 패 키 지 는 미 분 을 자 동 으 로 계 산 해 주 는 기 능 제 공 합 니다 . 다 른 대 부분 의 라 이 브 러 리 들 은 자 동 미 분 을 하 기 위 해 서 는 심 볼 그 래 프 컴 파 일을 우 선 수 행해 야 하 지 만 , autograd 는 일 반 적 인 명 령 형 코 드 ( imperative code ) 를 작 성 하 면 서 미 분 을 수 행할 수 있 게 해 줍 니다 . 모 델 에 값 을 대 입 할 때 마 다 , autograd 는 그 래 프 를 즉 석 으 로 만 들 고 , 이 를 사 용 해 서 그 래 디 언 트 ( gradient ) 를 역 으 로 계 산 합 니다 . 이 절 에 서 나 오 는 수 학 에 익 숙 하 지 않 은 독 자 들 은 부 록 의 “Mathematical Basics” 절 을 참 고 하 세 요 . [ 1 ] : from mxnet import autograd , nd 4 . 5 . 1 간 단 한 예 제 장 난 감 예 제 로 시 작 해 보 겠 습 니다 . 함 수 𝑦 = 2 x ⊤ x 를 컬 럼 벡 터 x ﬀ 에 대 해 서 미 분 을 하 고 싶습 니다 . 우 선 은 , 변 수 x 를 생 성 하 고 , 초 기 값 을 할 당 합 니다 . [ 2 ] : x = nd . arange ( 4 ) . reshape ( ( 4 , 1 ) ) print ( x ) [ [ 0 . ] [ 1 . ] [ 2 . ] ( continues on next page ) 76 4 . 딥 러 닝 맛 보 기 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) [ 3 . ] ] < NDArray 4x1 @ cpu ( 0 ) > x 에 대 한 y 의 미 분 을 계 산 한 결과 를 저 장 할 공간 이 필 요 합 니다 . attach _ grad 함 수 를 호 출 하 면 NDArray 는 그 래 디 언 트 ( gradient ) 를 저 장 할 공간 을 마 련 합 니다 . [ 3 ] : x . attach _ grad ( ) 자 이 제 y 를 계 산 합 니다 . MXNet 은 즉 석 에 서 연 산 그 래 프 를 생 성 해 줍 니다 . 이 는 마 치 MXNet 이 기 록 장 치 를 켜 고 , 각 변 수 들 이 생 성 되 는 정 확 한 경 로 를 모 두 집 어 내 는 것과 같 습 니다 . 연 산 그 래 프 를 생 성 하 는 데 에 는 적 지 않 은 연 산 량 이 필 요 함 을 기 억 해 두 세 요 . 따 라 서 , MXNet 은 요 청 을 받 았 을 때 만 그 래 프 를 생 성 하 도 록 되 어 있 습 니다 . 즉 , 이 는 with autograd . record ( ) : 블 록 안에 넣 었 을 때 작 동 합 니다 . [ 4 ] : with autograd . record ( ) : y = 2 * nd . dot ( x . T , x ) print ( y ) [ [ 28 . ] ] < NDArray 1x1 @ cpu ( 0 ) > x 의 모 양 ( shape ) 이 ( 4 , 1 ) 이 니 , y 는 스 칼 라 값 이 됩 니다 . 이 후 에 우 리 는 backward 함 수 를 호 출 해 서 그 래 디 언 트 ( gradient ) 를 자 동 으 로 구 합 니다 . 만 약 y 가 스 칼 라 타 입이 아 니 면 , 기 본 설 정 으 로 MXNet 은 우 선 y 의 모 든 항 목 을 더 해 서 새 로 운 값 을 얻 은 결과 의 x 에 대 한 그 래 디 언 트 ( gradient ) 를 찾 습 니다 . [ 5 ] : y . backward ( ) 함 수 𝑦 = 2 x ⊤ x 의 x 에 대 한 미 분 은 4 x 입 니다 . 미 분 이 올 바 르 게 계 산 되 는 지 확 인 해 보 겠 습 니다 . [ 6 ] : print ( ( x . grad - 4 * x ) . norm ( ) . asscalar ( ) = = 0 ) print ( x . grad ) True [ [ 0 . ] [ 4 . ] [ 8 . ] ( continues on next page ) 4 . 5 . 자 동 미 분 ( automatic differentiation ) 77 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) [ 12 . ] ] < NDArray 4x1 @ cpu ( 0 ) > 4 . 5 . 2 학 습 모 드 와 예 측 모 드 위 에 서 보 았 듯 이 record 함 수 를 호 출 하 면 MXNet 은 그 래 디 언 트 ( gradient ) 를 계 산 하 고 기 록 합 니다 . 더 불 어 기 본 설 정 으 로 는 autograd 는 실 행 모 드 를 예 측 모 드 에 서 학 습 모 드 로 바 꿉 니다 . 이 는 is _ training 함 수 를 호 출 해 서 확 인 할 수 있 습 니다 . [ 7 ] : print ( autograd . is _ training ( ) ) with autograd . record ( ) : print ( autograd . is _ training ( ) ) False True 어 떤 경 우 에 는 같 은 모 델 이 학 습 모 드 와 예 측 모 드 에 서 다 르 게 동 작 합 니다 . 배 치 정 규 ( normalization ) 가 그 렇 습 니다 . 다 른 어 떤 모 델 은 그 래 디 언 트 ( gradient ) 를 쉽 게 계 산 하 기 위 해 서 더 많 은 보 조 변 수 를 저 장 하 기 도 합 니다 . 다 음 장 들 에 서 차 이 에 대 해 서 자 세 하 게 다 룰 예 정 이 나 , 지 금 은 자 세 한 내 용 에 대 해 서 걱 정 할 필 요 는 없 습 니다 . 4 . 5 . 3 Python 제 어 흐 름 의 그 래 디 언 트 ( gradient ) 계 산 하 기 자 동 미 분 을 사 용 하 는 장 점 중 에 하 나 는 함 수 의 연 산 그 래 프 가 Python 제 어 흐 름 ( 조 건과 loop 제 어 ) 을 가 지 고 있 어 도 변 수 에 대 한 그 래 디 언 트 ( gradient ) 를 찾 아 낼 수 있 다는 것 입 니다 . 아 래 프 로 그 램 을 예 로 들 어 보 겠 습 니다 . while loop 의 반복 횟 수 와 if 조 건 문 의 수 행 횟 수 는 입 력 b 의 값 에 영 향 을 받 습 니다 . [ 8 ] : def f ( a ) : b = a * 2 while b . norm ( ) . asscalar ( ) < 1000 : b = b * 2 if b . sum ( ) . asscalar ( ) > 0 : c = b else : c = 100 * b return c 78 4 . 딥 러 닝 맛 보 기 그 래 디 언 트 ( gradient ) 를 계 산 하 기 위 해 서 record 함 수 를 호 출 해 야 하 고 , 그 래 디 언 트 ( gradient ) 를 찾 기 위 해 서 backward 함 수 를 호 출 합 니다 . [ 9 ] : a = nd . random . normal ( shape = 1 ) a . attach _ grad ( ) with autograd . record ( ) : d = f ( a ) d . backward ( ) 위 에 서 정 의 한 함 수 f 를 분 석 해 보 겠 습 니다 . 실 제 로 , 임의의 입 력 a 가 주 어 지 면 , 그 출 력 은 f ( a ) = x * a 의 형 태 여야 하 며 , 여 기 서 스 칼 라 계 수 x 의 값 은 입 력 a 에 의 존 합 니다 . c = f ( a ) 는 a 에 대 한 x 의 기 울 기 와 c / a 의 값 을 갖 기 때 문 에 다 음 과 같 이 이 예 에 서 제 어 흐 름 의 그 래 디 언 트 ( gradient ) 결과 의 정 확 성 을 확 인 할 수 있 습 니다 . [ 10 ] : print ( a . grad = = ( d / a ) ) [ 1 . ] < NDArray 1 @ cpu ( 0 ) > 4 . 5 . 4 헤 드 그 래 디 언 트 ( head gradient ) 와 체 인 룰 * 주 의 : 이 파 트 는 까 다 롭 지 만 , 이 책 의 나 머 지 부분 을 이 해하 는 데 꼭 필 요 한 내 용 은 아 닙니다 . 만 약 새 로 운 네 트 워 크 를 직 접 만 들 고 싶 은 경 우 에 필 요 한 내 용 이 기 때 문 에 , 이 책 을 처 음 읽을 때 는 넘 어 가 도 됩 니다 . y 가 x 의 함 수 이 고 , 이 함 수 의 backward 함 수 , y . backward ( ) 를 호 출 할 때 , 우 리 는 y 의 x 에 대 한 미 분 을 구 하 는 것 에 관 심 이 있 습 니다 . 수 학 자 들 은 이 를 𝑑𝑦 ( 𝑥 ) 𝑑𝑥 로 기 술 합 니다 . 어 떤 경 우 에 는 z 가 y 의 함 수 일 때 , z 의 x 에 대 한 미 분 을 구 해 야 합 니다 . 즉 , 𝑑𝑑𝑥 𝑧 ( 𝑦 ( 𝑥 ) ) 를 구 햐 야 합 니다 . 체 인 룰 을 떠 올 려 보 면 , 다 음 과 같 이 계 산 됩 니다 . 𝑑 𝑑𝑥𝑧 ( 𝑦 ( 𝑥 ) ) = 𝑑𝑧 ( 𝑦 ) 𝑑𝑦 𝑑𝑦 ( 𝑥 ) 𝑑𝑥 . y 가 더 큰 함 수 z 의 일 부 이 고 , x . grad 가 𝑑𝑧𝑑𝑥 를 저 장 하 도 록 하 려 면 , 헤 드 그 래 디 언 트 ( head gradient ) 𝑑𝑧 𝑑𝑦 를 backward ( ) 의 입 력 으 로 넣 으 면 됩 니다 . 기 본 인자 는 nd . ones _ like ( y ) 입 니다 . 자 세 한 내 용 은 Wikipedia 를 참 조 하 세 요 . 4 . 5 . 자 동 미 분 ( automatic differentiation ) 79 [ 11 ] : with autograd . record ( ) : y = x * 2 z = y * x head _ gradient = nd . array ( [ 10 , 1 . , . 1 , . 01 ] ) z . backward ( head _ gradient ) print ( x . grad ) [ [ 0 . ] [ 4 . ] [ 0 . 8 ] [ 0 . 12 ] ] < NDArray 4x1 @ cpu ( 0 ) > 4 . 5 . 5 요 약 • MXNet 은 미 분 계 산 을 자 동 화 해 주 는 autograd 패 키 지 를 제 공 합 니다 . • MXNet 의 autograd 패 키 지 는 일 반 적 인 명 령 형 ( imperative ) 프 로 그 램 형 식 으 로 사 용 할 수 있 습 니다 . • MXNet 의 실 행 모 드 로 는 학 습 모 드 와 예 측 모 드 가 있 습 니다 . 현 재의 실 행 모 드 는 autograd . is _ training ( ) 을 통 해 서 구 할 수 있 습 니다 . 4 . 5 . 6 문 제 1 . 이 절 에 서 예 로 들 었 던 제 어 흐 름 ( control ﬂow ) 의 그 래 디 언 트 ( gradient ) 를 찾 고 자 할 때 , 변 수 a 가 랜 덤 벡 터 또 는 행 렬 에 따 라 서 바 뀌 고 있 습 니다 . 이 때 , c 계 산 결과 는 더 이 상 스 칼 라 형 태 가 아 닙니다 . 결과 에 무 슨 일이 일 어 났나 요 ? 이 것 을 어 떻 게 분 석 할 수 있 나 요 ? 2 . 제 어 흐 름 ( control ﬂow ) 의 그 래 디 언 트 ( gradient ) 를 구 하 는 예 를 다 시 설 계 해 보 세 요 . 실 행하 고 결 과 를 분 석 해 보 세 요 . 3 . 세 컨 드 가격 경 매 ( 예 , eBay ) 에 서 는 두 번 째 로 높 은 가격 을 부 른 사 람 이 물 건 을 사 게 됩 니다 . autograd 를 이 용 해 서 물 건 의 가격 에 대 해 서 경 매 참 여에 대 한 그 래 디 언 트 ( gradient ) 를 계 산 하 세 요 . 왜 병 리 학 적 인 결과가 나 올 까 요 ? 이 결과 는 메 카 니 즘 에 대 해 서 무 엇 을 말 해 주 나 요 ? 더 자 세 한 내 용 은 Edelman , Ostrovski and Schwartz , 2005 페 이 퍼 를 읽 어 보 세 요 . 4 . 왜 일 차 미 분 보 다 이 차 미 분 이 훨 씬 더 많 은 연 산 량 이 필 요 하 나 요 ? 80 4 . 딥 러 닝 맛 보 기 5 . 체 인 룰 의 헤 드 그 래 디 언 트 ( head gradient ) 관 계 를 유 도 해 보 세 요 . 막 히 는 경 우 Wikipedia Chain Rule 를 참 고 하 세 요 . 6 . 함 수 𝑓 ( 𝑥 ) = sin ( 𝑥 ) 를 가 정 합 니다 . 심 볼 계 산 을 하 지 않 고 ( 즉 , 𝑓 ′ ( 𝑥 ) = cos ( 𝑥 ) 을 이 용 하 지 말 고 ) 𝑑𝑓 ( 𝑥 ) 𝑑𝑥 를 구 해 서 𝑓 ( 𝑥 ) 와 𝑑𝑓 ( 𝑥 ) 𝑑𝑥 의 그 래 프 를 그 려 보 세 요 . 4 . 5 . 7 Scan the QR Code to Discuss 4 . 6 확 률 과 통 계 머 신 러 닝 은 어 떤 방 식 이 든 지 결 국 예 측 을 수 행하 는 것 입 니다 . 어 떤 환 자의 의 료 기 록 을 바 탕 으 로 내 년 에 심 장 마 비 를 겪 을 확 률 예 측 하 기 를 예 로 들 어 볼 수 있 습 니다 . 비 정 상 탐 지 를 위 해 서 , 비 행 기 제 트 엔 진 의 센서 데 이 터 가 정 상 적 으 로 동 작 할 때 어 떤 값 을 갖게 될 지 예 측 을 할 수 도 있 습 니다 . 강 화 학 습 에 서 는 에 이 전 트 가 주 어 진 환 경 에 서 똑똑 하 게 동 작 하 게 만 드 는 것 이 목 표 입 니다 . 이 경 우 에 는 주 어 진 행 동 들 중 에 가 장 높 은 보 상 을 받 는 확 률 을 고 려 해 야 합 니다 . 추 천 시스 템 을 만 드 는 경 우 에 도 확 률 을 고 려 해 야 합 니다 . 예 를 들 어 여 러 분 이 대 형 온 라 인 서 점 에 서 일을 한 다 면 , 어 떤 책 을 홍 보 했 을 때 특 정 사 용 자 가 그 책 을 구 매 할 지 에 대 한 확 률 을 추 정 하 고 싶 어 할 것 입 니다 . 이 를 위 해 서 우 리 는 확 률 과 통 계 의 언어 를 사 용 할 필 요 가 있 습 니다 . 확 률 을 다 루 는 별 도 의 과 정 , 전 공 , 논 문 , 직 업 심 지 어 는 부 서 까 지 도 있 습 니다 . 이 책 의 목 표 는 이 모 든 주제 들 에 대 해 서 배 워 보 는 것 은 아 니 고 , 여 러 분 이 스스 로 머 신 러 닝 모 델 을 만 들 수 있을 정 도 의 내 용 을 알 려 주 고 , 이 후 에 스스 로 공 부 해 볼 수 있 는 주제 들 을 선 택 할 수 있 도 록 하 는 것 입 니다 . 지 금까 지 확 률 에 대 해 서 많 이 이 야 기 를 해 왔 지 만 , 확 률 에 정 확 하 게 무 엇 인 지 를 설 명 하 지 않았 고 구 체 적 인 예 제 를 들 지 는 않았 습 니다 . 동 물 의 사 진 이 주 어 졌 을 때 , 고 양 이인 지 개 인 지 를 구 분 하 는 문 제 를 조 금 자 세 하 게 살 펴 보 겠 습 니다 . 이 문 제 는 간 단 해 보 이 지 만 , 사 실 쉽 지 않 은 문 제 가 있 습 니다 . 우 선 은 문 제 의 난 이 도 가 이 미 지 의 해 상 도 에 따 라 차 이 가 있을 수 있 습 니다 . 4 . 6 . 확 률 과 통 계 81 10px 20px 40px 80px 160px 사 람 이 320 픽 셀 해 상 도 의 이 미 지 에 서 개 와 고 양 이 를 구 분 하 는 것 은 쉽습 니다 . 하 지 만 , 40 픽 셀 이 되 면 그 분 류 가 어 렵 고 , 10 픽 셀 로 줄 어 들 면 거 의 불 가 능 합 니다 . 즉 , 개 와 고 양 이 를 먼 거 리 에 서 판 별 하 는 것 은 ( 또 는 낮 은 해 상 도 의 이 미 지 에 서 ) 동 전 던 지 기 를 해 서 추 측 하 는 것과 동 일 해 집 니다 . 확 률 은 확 실 성 에 대 한 추 론 을 하 는 공 식 적 인 방법 을 제 공 합 니다 . 만 약 , 이 미 지 에 고 양 이 가 있 다는 것 을 완 벽 하 게 확 신 한 다 면 , 해 당 레 이 블 𝑙 이 고 양 이일 확 률 , 𝑃 ( 𝑙 = cat ) 는 1 . 0 이 라 고 말 합 니다 . 만 약 𝑙 = cat 인 지 𝑙 = dog 에 대 한 아 무 런 판 단 을 못 한 다 면 , 두 확 률 은 동 일 하 다 고 하 다 고 말 하 며 , 𝑃 ( 𝑙 = cat ) = 0 . 5 이 됩 니다 . 만 약 이 미 지 에 고 양 이 가 있 다는 것 을 확 실 하 지 는 않 지 만 어 느 정 도 확 신 한 다 면 , 확 률 은 . 5 < 𝑃 ( 𝑙 = cat ) < 1 . 0 로 주 어 질 것 입 니다 . 이 제 두 번 째 예 를 들 어 보 겠 습 니다 . 대 만 날 씨에 대 한 데 이 터 를 관 찰 한 데 이 터 가 있을 때 , 내 일 비 가 내 릴 확 률 을 예 측 하 고 자 합 니다 . 여 름 인 경 우 에 는 비 가 내 릴 확 률 이 0 . 5 정 도 가 될 것 입 니다 . 위 두 가 지 예 제 모 두 살 펴 볼 가 치 가 있 습 니다 . 두 경 우 모 두 결과 에 대 한 불 확 실 성 이 있 지 만 , 주 요 차 이 점 이 있 습 니다 . 첫 번 째 예 제 는 이 미 지 가 고 양 이인 지 개 이 지 만 , 우 리 가 어 떤 것 인 지 모 르 는 경 우 이 고 , 두 번 째 예 제 는 결과가 실 제 로 임의 로 일 어 나 는 이 벤 트 일 수 도 있 습 니다 . 즉 , 확 률 이 란 우 리 의 확 실 성 에 대 한 사 고 를 하 기 위 한 유 연 한 언어 이 며 , 다 양 한 경 우 에 효 과 적 으 로 적 용 될 수 있 습 니다 . 82 4 . 딥 러 닝 맛 보 기 4 . 6 . 1 기 초 확 률 이 론 주 사 위 를 던 져 서 다 른 숫 자 가 아 닌 1 일 나 오 는 확 률 이 얼 마 나 되 는 지 찾 는 경 우 를 생 각 해 보 겠 습 니 다 . 주 사 위 가 공 정 하 다 면 , 모 든 6 개 숫 자 들 , 𝒳 = { 1 , . . . , 6 } , 은 일 어 날 가 능 성 이 동 일 합 니다 . 학 술 용 어 로 는 “1 은 확 률 16 로 일 어 난 다 ” 라 고 말 합 니다 . 공 장 에 서 막 만 들 어 진 주 사 위 에 대 해 서 우 리 는 이 비 율을 알 지 못 할 수 있 고 , 주 사 위 가 공 정 한 지 확 인 해 야 할 필 요 가 있 습 니다 . 주 사 위 를 조 사 하 는 유일 한 방법 은 여 러 번 던 져 보 면 서 결과 를 기 록 하 는 것 입 니다 . 주 사 위 를 던 질 때 마 다 , 우 리 는 { 1 , 2 , . . . , 6 } 에 하 나 의 숫 자 를 얻 게 되 고 , 이 결과 들 이 주 어 지 면 , 각 숫 자 들 이 일 어 날 수 있 는 확 률 을 조 사 할 수 있 습 니다 . 가 장 자 연 스 러 운 방법 은 각 숫 자 들 이 나 온 횟 수 를 전 체 던 진 횟 수 로 나 누는 것 입 니다 . 이 를 통 해 서 우 리 는 특 정 이 벤 트 에 대 한 확 률 을 추 정 합 니다 . 큰 수 의 법 칙 ( the law of large numbers ) 에 따 라 , 던 지 는 횟 수 가 늘 어 날 수 록 이 추 정 은 실 제 확 률 과 계 속 가 까 워 집 니다 . 더 자 세 한 논 의 를 하 기 전 에 , 실 제 로 실 험 을 해 보 겠 습 니다 . 우 선 필 요 한 패 키 지 들 을 import 합 니다 . [ 1 ] : import mxnet as mx from mxnet import nd 다 음으 로 는 주 사 위 를 던 지 는 것 을 해 야 합 니다 . 통 계 에 서 는 확 률 분 포 에 서 샘 플 을 뽑 는 것 을 샘 플 링 이 라 고 합 니다 . 연 속 되 지 않 은 선 택 들 에 확 률 이 부 여 된 분 포 를 우 리 는 다 항 ( multinomial ) 분 포 라 고 합 니다 . 분 포 ( distribution ) 에 대 한 공 식 적 인 정 의 는 다 음 에 다 루 겠고 , 지 금 은 분 포 를 이 벤 트 들 에 확 률 을 할 당 하 는 것 정 도 로 생 각 하 겠 습 니다 . MXNet 에 서 nd . random . multinomial 함 수 를 이 용 하 면 다 항 분 포 에 서 샘 플 을 추출 할 수 있 습 니다 . [ 2 ] : probabilities = nd . ones ( 6 ) / 6 nd . random . multinomial ( probabilities ) [ 2 ] : [ 3 ] < NDArray 1 @ cpu ( 0 ) > 여 러 샘 플 을 뽑 아 보 면 , 매 번 임의의 숫 자 를 얻 는 것 을 확 인 할 수 있 습 니다 . 주 사 위의 공 정 성 을 추 정 하 는 예 제 에 서 우 리 는 같 은 분 포 에 서 많 은 샘 플 을 추출 하 기 를 원 합 니다 . Python 의 for loop 을 이 용 하 면 너 무 느 리 기 때 문 에 , random . multinomial 이 여 러 샘 플 을 한 번 째 뽑 아 주 는 기 능 을 이 용 해 서 우 리 가 원 하 는 모 양 ( shape ) 의 서 로 연 관 이 없 는 샘 플 들 의 배 열 을 얻 겠 습 니다 . [ 3 ] : print ( nd . random . multinomial ( probabilities , shape = ( 10 ) ) ) print ( nd . random . multinomial ( probabilities , shape = ( 5 , 10 ) ) ) 4 . 6 . 확 률 과 통 계 83 [ 3 4 5 3 5 3 5 2 3 3 ] < NDArray 10 @ cpu ( 0 ) > [ [ 2 2 1 5 0 5 1 2 2 4 ] [ 4 3 2 3 2 5 5 0 2 0 ] [ 3 0 2 4 5 4 0 5 5 5 ] [ 2 4 4 2 3 4 4 0 4 3 ] [ 3 0 3 5 4 3 0 2 2 1 ] ] < NDArray 5x10 @ cpu ( 0 ) > 이 제 주 사 위 를 던 지 는 샘 플 을 구 하 는 방법 을 알았 으 니 , 100 번 주 사 위 를 던 지 는 시 뮬 레 이 션 을 해 서 , 각 숫 자 들 이 나 온 횟 수 를 카 운 팅 합 니다 . [ 4 ] : rolls = nd . random . multinomial ( probabilities , shape = ( 1000 ) ) counts = nd . zeros ( ( 6 , 1000 ) ) totals = nd . zeros ( 6 ) for i , roll in enumerate ( rolls ) : totals [ int ( roll . asscalar ( ) ) ] + = 1 counts [ : , i ] = totals 1000 번 을 던 져 본 후 에 최 종 합 계 를 확 인 합 니다 . [ 5 ] : totals / 1000 [ 5 ] : [ 0 . 167 0 . 168 0 . 175 0 . 159 0 . 158 0 . 173 ] < NDArray 6 @ cpu ( 0 ) > 결과 에 따 르 면 , 모 든 숫 자 중 에 가 장 낮 게 추 정 된 확 률 은 약 0 . 15 이 고 , 가 장 높 은 추 정 확 률 은 0 . 188 입 니다 . 공 정 한 주 사 위 를 사 용 해 서 데 이 터 를 생 성 했 기 때 문 에 , 각 숫 자 들 은 1 / 6 즉 0 . 167 의 확 률 을 갖 는다는 것 을 알 고 있 고 , 예 측 도 매 우 좋 게 나 왔 습 니다 . 시 간 이 지 나 면 서 이 확 률 이 의 미 있 는 추 정 치 로 어 떻 게 수 렴 하 는 지 를 시 각 해 볼 수 도 있 습 니다 . 이 를 위 해 서 우 선 은 ( 6 , 1000 ) 의 모 양 ( shape ) 을 갖 는 counts 배 열 을 살 펴 봅 시 다 . 1000 번 을 수 행 하 는 각 단 계 마 다 , counts 는 각 숫 자 가 몇 번 나 왔 는 지 를 알 려 줍 니다 . 그 렇 다 면 , counts 배 열 의 𝑗 번 째 열 의 그 때 까 지 던 진 총 횟 수 로 표 준 화 해 서 , 그 시 점 에 서 의 추 정 확 률 current 를 계 산 합 니다 . counts 객 체 는 다 음 과 같 습 니다 . 84 4 . 딥 러 닝 맛 보 기 [ 6 ] : counts [ 6 ] : [ [ 0 . 0 . 0 . . . . 165 . 166 . 167 . ] [ 1 . 1 . 1 . . . . 168 . 168 . 168 . ] [ 0 . 0 . 0 . . . . 175 . 175 . 175 . ] [ 0 . 0 . 0 . . . . 159 . 159 . 159 . ] [ 0 . 1 . 2 . . . . 158 . 158 . 158 . ] [ 0 . 0 . 0 . . . . 173 . 173 . 173 . ] ] < NDArray 6x1000 @ cpu ( 0 ) > 던 진 총 횟 수 로 표 준 화 하 면 , [ 7 ] : x = nd . arange ( 1000 ) . reshape ( ( 1 , 1000 ) ) + 1 estimates = counts / x print ( estimates [ : , 0 ] ) print ( estimates [ : , 1 ] ) print ( estimates [ : , 100 ] ) [ 0 . 1 . 0 . 0 . 0 . 0 . ] < NDArray 6 @ cpu ( 0 ) > [ 0 . 0 . 5 0 . 0 . 0 . 5 0 . ] < NDArray 6 @ cpu ( 0 ) > [ 0 . 1980198 0 . 15841584 0 . 17821783 0 . 18811882 0 . 12871288 0 . 14851485 ] < NDArray 6 @ cpu ( 0 ) > 결과 에 서 보 이 듯 이 , 주 사 위 를 처 음 던 진 경 우 하 나 의 숫 자 에 대 한 확 률 이 1 . 0 이 고 나 머 지 숫 자 들 에 대 한 확 률 이 0 인 극 단 적 인 예 측 을 하 지 만 , 100 번 을 넘 어 서 면 결과가 상 당 히 맞 아 보 입 니다 . 플 롯 을 그 리 는 패 키 지 matplotlib 을 이 용 해 서 이 수 렴 과 정 을 시 각 화 해 봅 니다 . 이 패 키 지 를 아 직 설 치 하 지 않았 다 면 , install it 를 참 고 해 서 지 금 하 세 요 . [ 8 ] : % matplotlib inline from matplotlib import pyplot as plt from IPython import display display . set _ matplotlib _ formats ( ' svg ' ) plt . figure ( figsize = ( 8 , 6 ) ) for i in range ( 6 ) : plt . plot ( estimates [ i , : ] . asnumpy ( ) , label = ( " P ( die = " + str ( i ) + " ) " ) ) ( continues on next page ) 4 . 6 . 확 률 과 통 계 85 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) plt . axhline ( y = 0 . 16666 , color = ' black ' , linestyle = ' dashed ' ) plt . legend ( ) plt . show ( ) 각 선 은 주 사 위의 숫 자 중 에 하 나 를 의 미 하 고 , 1000 번 주 사 위 던 지 기 를 수 행하 면 서 각 횟 수 마 다 각 숫 자 가 나 올 확 률 의 추 정 값 을 나 타 내 는 그 래 프 입 니다 . 검 은 점 선 은 진짜 확 률 ( true probability , 1 / 6 ) 을 표 시 합 니다 . 횟 수 가 늘 어 가 면 선 들 이 진짜 확 률 에 수 렴 하 고 있 습 니다 . 주 사 위 던 지 기 예 를 통 해 서 확 률 변 수 ( random variable ) 라 는 개 념 을 소 개 했 습 니다 . 여 기 서 𝑋 로 표 현 할 확 률 변 수 는 어 떤 양 이 될 수 있 고 , 결 정적 이 지 않 을 수 있 습 니다 . 확 률 변 수 는 여 러 가 능 성 들 의 집 합 에 서 하 나 의 값 을 나 타 낼 수 도 있 습 니다 . 집 합 은 괄 호 를 이 용 해 서 표 현 합 니다 . 예 를 들 면 , { cat , dog , rabbit } 입 니다 . 집 합 에 속 한 아 이 템 들 은 원 소 ( element ) 라 고 하 고 , 어 떤 원 소 𝑥 가 집 합 𝑆 에 속 한 다 라 고 하 면 표 기 는 𝑥 ∈ 𝑆 로 합 니다 . 기 호 ∈ 는 “ 속 한 다 ” 라 고 읽 고 , 포 함 관 계 를 표 현 합 니다 . 예 를 들 어 , dog ∈ { cat , dog , rabbit } 입 니다 . 주 사 위 던 지 는 것 의 경 우 , 확 률 변 수 𝑋 ∈ { 1 , 2 , 3 , 4 , 5 , 6 } 86 4 . 딥 러 닝 맛 보 기 입 니다 연 속 적 이 지 않 은 확 률 변 수 ( 예 를 들 어 주 사 위의 6 면 ) 와 연 속 적 인 확 률 변 수 ( 예 를 들 어 사 람 의 몸 무 게 나 키 ) 사 이 에 는 미묘 한 차 이 점 이 있 다는 것 을 기 억 하 세 요 . 두 사 람 의 키 가 정 확 하 게 같 은 지 를 묻 는 경 우 는 드 물 것 입 니다 . 아 주 정 확 한 측 정 방법 이 있 어 서 이 를 적 용 한 다 면 , 이 세 상 에 키 가 완 전 하 게 같 은 사 람 두 사 람 이 없 습 니다 . 사 실 , 적 당 히 정 교 한 측 정 을 하 는 경 우 에 도 아 침 에 일 어 났 을 때 의 키 와 밤 에 잠 자 기 전 에 잰 키 는 다 르 게 나 옵 니다 . 즉 , 어 떤 사 람 의 키 가 2 . 00139278291028719210196740527486202 미 터 일 확 률 을 물 어 보 는 것 은 의 미 가 없 습 니다 . 전 체 인 구 에 대 해 서 도 이 확 률 은 거 의 0 입 니다 . 따 라 서 , 어 떤 사 람 의 키 가 어 느 구 간 ( 예 를 들 면 1 . 99 와 2 . 01 미 터 사 이 ) 에 속 하 는 지 를 묻 는 것 이 더 의 미 가 있 습 니다 . 이 런 경 우 들 에 는 우 리 는 어 떤 값 을 밀 도 ( density ) 로 볼 가 능 성 을 정 량 화 합 니다 . 정 확 하 게 2 . 0 미 터 인 키 에 대 한 확 률 은 없 지 만 , 밀 도 는 0 이 아 닙니다 . 서 로 다 른 두 키 의 구 간 에 대 해 서 는 확 률 값 이 0 이 아 닌 수 가 됩 니다 . 기 억 해 두 어야 할 몇 가 지 중 요 한 확 률 에 대 한 공 리 ( axiom ) 들 이 있 습 니다 . • 어 떤 이 벤 트 𝑧 에 대 해 서 , 확 률 은 절 대 로 음 수 가 아 닙니다 . 즉 , Pr ( 𝑍 = 𝑧 ) ≥ 0 • 두 이 벤 트 𝑍 = 𝑧 과 𝑋 = 𝑥 에 대 해 서 , 두 이 벤 트 의 합 집 합 ( union ) 에 대 한 확 률 은 각 이 벤 트 의 확 률 의 합 보 다 클 수 없 습 니다 . 즉 , Pr ( 𝑍 = 𝑧 ∪ 𝑋 = 𝑥 ) ≤ Pr ( 𝑍 = 𝑧 ) + Pr ( 𝑋 = 𝑥 ) ﬀ . • 어 떤 확 률 변 수 에 대 해 서 , 모 든 값 들 에 대 한 확 률 의 합 은 항 상 1 입 니다 . 즉 , ∑︀ 𝑛𝑖 = 1 Pr ( 𝑍 = 𝑧 𝑖 ) = 1 . • 서 로 겹 치 지 않 는 두 사 건 , 𝑍 = 𝑧 과 𝑋 = 𝑥 , t , 에 대 해 서 , 둘 중 에 한 사 건 이 일 어 날 확 률 은 각 사 건 의 확 률 의 합 과 같 습 니다 . 즉 , Pr ( 𝑍 = 𝑧 ∪ 𝑋 = 𝑥 ) = Pr ( 𝑍 = 𝑧 ) + Pr ( 𝑋 = 𝑥 ) . 4 . 6 . 2 여 러 확 률 변 수 다 루 기 종종 하 나 이 상 의 확 률 변 수 를 동 시 에 다 룰 필 요 가 생 깁 니다 . 질 병 과 증 상 의 관 계 를 모 델 링 하 는 경 우 를 들 수 있 습 니다 . 질 병 과 증 상 이 주 어 졌 을 때 , 예 를 들 면 ‘ 독 감 ’ 과 ’ 기 침 ’ , 두 개 는 어 떤 확 률 로 환 자 에 게 일 어 날 수 도 일 어 나 지 않 을 수 있 습 니다 . 이 둘 에 대 한 확 률 이 작 기 를 기 대 하 지 만 , 더 좋 은 의 료 처 방 을 할 수 있 도 록 확 률 과 둘 사 이의 관 계 를 예 측 하 고 자 합 니다 . 더 복 잡 한 예 로 , 수 백 만 픽 셀 로 이 루 어 진 이 미 지 를 들 어 보 겠 습 니다 . 즉 , 수 백 만 확 률 변 수 가 존 재 합 니다 . 많 은 경 우 에 이 미 지 들 은 이 미 지 에 있 는 객 체 를 지 칭 하 는 레 이 블 을 갖 습 니다 . 이 레 이 블 도 확 률 변 수 로 생 각 할 수 있 습 니다 . 더 나 아 가 서 는 , 위 치 , 시 간 , 구 경 ( apeture ) , 초 점 거 리 , ISO , 초 점 , 카 메 라 종 류 등 과 같 은 모 든 메 타 데 이 터 를 확 률 변 수 로 생 각 할 수 도 있 습 니다 . 이 모 든 것 은 연 관 되 어 발 생 하 는 확 률 변 수 들 입 니다 . 여 러 확 률 변 수 를 다 룰 때 몇 가 지 중 요 한 것 들 이 있 습 니다 . 첫 번 째 는 교 차 확 률 분 포 Pr ( 𝐴 , 𝐵 ) 입 니다 . 두 원 소 𝑎 와 𝑏 가 주 어 졌 을 때 , 교 차 확 률 분 포 는 동 시 에 𝐴 = 𝑎 이 고 𝐵 = 𝑏 4 . 6 . 확 률 과 통 계 87 일 확 률 이 얼 마 인 지 에 대 한 답 을 줍 니다 . 임의의 값 𝑎 와 𝑏 에 대 해 서 , Pr ( 𝐴 , 𝐵 ) ≤ Pr ( 𝐴 = 𝑎 ) 이 라 는 사 실 은 쉽 게 알 수 있 습 니다 . 𝐴 와 𝐵 가 일 어 났 기 때 문 에 , 𝐴 가 발 생 하 고 , 𝐵 또 한 발 생 해 야 합 니다 . ( 또 는 반 대 로 ) . 즉 , 𝐴 와 𝐵 가 동 시 에 일 어 나 는 것 은 𝐴 와 𝐵 가 별 도 로 일 어 나 는 것 보 다는 가 능 성 이 낮 습 니다 . 이 사 실 로 흥 미 로 운 비 율을 정 의 할 수 있 습 니다 . 즉 , 0 ≤ Pr ( 𝐴 , 𝐵 ) Pr ( 𝐴 ) ≤ 1 . 우 리 는 이 것 을 조 건 부 확 률 ( conditional probability ) 이 라 고 부 르 며 , Pr ( 𝐵 | 𝐴 ) 로 표 현 합 니다 . 다 시 말 하 면 , 𝐴 가 일 어 났 을 때 𝐵 가 일 어 날 확 률 입 니다 . 조 건 부 확 률 의 정 의 를 이 용 하 면 , 확 률 에 서 가 장 유 용 하 고 유 명 한 방 정 식 을 도 출 할 수 있 는 데 , 이 것 이 바 로 베 이 즈 이 론 ( Bayes’ theorem ) 입 니다 . 이 를 도 출 하 는 방법 으 로 Pr ( 𝐴 , 𝐵 ) = Pr ( 𝐵 | 𝐴 ) Pr ( 𝐴 ) 로 부 터 출 발 합 니다 . 대 칭 성 을 적 용 하 면 , Pr ( 𝐴 , 𝐵 ) = Pr ( 𝐴 | 𝐵 ) Pr ( 𝐵 ) 이 돕 니다 . 조 건 변 수 들 중 하 나 에 대 해 서 풀 어 보 면 다 음 공 식 을 얻 게 됩 니다 . Pr ( 𝐴 | 𝐵 ) = Pr ( 𝐵 | 𝐴 ) Pr ( 𝐴 ) Pr ( 𝐵 ) 어 떤 것 으 로 부 터 다 른 어 떤 것 을 추 론 ( 즉 원 인 과 효 과 ) 하 고 자 하 는 데 , 반 대 방 향 에 대 한 것 만 알 고 있을 경 우 에 아 주 유 용 합 니다 . 주 변 화 ( marginalization ) 는 이 것 이 작 동 하 게 만 드 는 데 아 주 중 요 한 연 산 입 니 다 . 이 연 산 은 Pr ( 𝐴 , 𝐵 ) 로 부 터 Pr ( 𝐴 ) 와 Pr ( 𝐵 ) 를 알아 내 는 연 산 입 니다 . 𝐴 가 일 어 날 확 률 은 모 든 𝐵 에 대 한 교 차 확 률 ( joint probability ) 의 값 으 로 계 산 됩 니다 . 즉 , Pr ( 𝐴 ) = ∑︁ 𝐵 ′ Pr ( 𝐴 , 𝐵 ′ ) and Pr ( 𝐵 ) = ∑︁ 𝐴 ′ Pr ( 𝐴 ′ , 𝐵 ) ﬀ 점 검 해 야 할 아 주 유 용 한 특 성 은 종 속 과 독 립 입 니다 . 독 립 은 하 나 의 사 건 의 발 생 이 다 른 사 건 의 발 생 에 영 향 을 주 지 않 는 것 을 의 미 합 니다 . 위 경 우 에 는 Pr ( 𝐵 | 𝐴 ) = Pr ( 𝐵 ) 를 의 미 합 니다 . 그 외 의 경 우 들 은 𝐴 와 𝐵 가 종 속 적 이 라 고 합 니다 . 주 사 위 를 두 번 연 속 으 로 던 지 는 것 은 독 립 적 이 나 , 방 의 전 등 스 위 치 의 위 치 와 방 의 밝 기 는 그 렇 지 않 습 니다 . ( 이 둘 이 완 전 히 결 정적 이 지 는 않 습 니다 . 왜 냐 하 면 , 전 구 가 망 가 질 수 도 있 고 , 전 원 이 나 갈 수 도 있 고 , 스 위 치 가 망 가 질 경 우 등 이 있 기 때 문 입 니다 . ) 그 럼 배 운 것 을 테 스 트 해 보 겠 습 니다 . 의 사 가 환 자 에 게 AIDS 테 스 트 를 하 는 것 을 가 정 하 겠 습 니다 . 이 테 스 트 는 상 당 히 정 확 해 서 , 환 자 가 음 성 일 경 우 이 를 틀 리 게 예 측 하 는 확 률 이 1 % 이 고 , 환 자 가 양 성 일 경 우 HIV 검 출 을 실 패 하 지 않 습 니다 . 𝐷 는 진 단 결과 를 𝐻 는 HIV 상 태 를 표 기 합 니다 . Pr ( 𝐷 | 𝐻 ) 결과 를 표 로 만 들 어 보 면 다 음 과 같 습 니다 . 결과 HIV 양 성 HIV 음 성 테 스 트 결과 - 양 성 1 0 . 01 테 스 트 결과 - 음 성 0 0 . 99 88 4 . 딥 러 닝 맛 보 기 같 은 열 의 값 을 더 하 면 1 이 나 , 행 으 로 더 하 면 그 렇 지 않 습 니다 . 그 이유 는 조 건 부 확 률 도 합 이 확 률 처 럼 1 이 여야 하 기 때 문 입 니다 . 테 스 트 결과가 양 성 일 경 우 환 자 가 AIDS 에 결 렸 을 확 률 을 계 산 해 보 겠 습 니 다 . 당 연 하 게 도 이 는 질 병 이 얼 마 나 일 반 적 인 가 에 따 라 달 라 집 니다 . 인 구 의 대 부분 이 건강 하 다 고 가 정 하 겠 습 니다 . 즉 Pr ( HIV positive ) = 0 . 0015 . 베 이 즈 이 론 ( Bayes’ Theorem ) 을 적 용 하 기 위 해 서 우 리 는 다 음을 결 정 해 야 합 니다 . Pr ( Test positive ) = Pr ( 𝐷 = 1 | 𝐻 = 0 ) Pr ( 𝐻 = 0 ) + Pr ( 𝐷 = 1 | 𝐻 = 1 ) Pr ( 𝐻 = 1 ) = 0 . 01 · 0 . 9985 + 1 · 0 . 0015 = 0 . 011485 따 라 서 , 우 리 가 얻 는 것 은 다 음 과 같 습 니다 . Pr ( 𝐻 = 1 | 𝐷 = 1 ) = Pr ( 𝐷 = 1 | 𝐻 = 1 ) Pr ( 𝐻 = 1 ) Pr ( 𝐷 = 1 ) = 1 · 0 . 0015 0 . 011485 = 0 . 131 이 결과 는 99 % 정 확 도 로 테 스 트 결과가 양 성 으 로 나 올 지 라 도 환 자 가 실 제 로 AIDS 에 걸 렸 을 확 률 은 13 . 1 % 밖 에 되 지 않 는 다는 것 을 의 미 입 니다 . 이 결과 에 서 보 듯 이 , 통 계 는 매 우 직 관 적 이 지 않 을 수 있 습 니다 . 4 . 6 . 3 조 건 부 독 립 성 그 렇 다 면 , 환 자 가 이 렇 게 무 서 운 결과 를 받 았 을 때 어 떻 게 해 야 할 까 요 ? 아 마 도 환 자 는 의 사 에 게 테 스 트 를 다 시 해 봐 달 라 고 요 청 할 것 입 니다 . 두 번 째 테 스 트 는 다 르 게 나 왔 다 고 하 겠 습 니다 . ( 즉 , 첫 번 째 만 큼 좋 지 않 습 니다 . ) 결과 HIV 양 성 HIV 음 성 테 스 트 결과 - 양 성 0 . 98 0 . 03 테 스 트 결과 - 음 성 0 . 02 0 . 97 안 타 깝 게 도 두 번 째 테 스 트 역 시 양 성 으 로 나 오 고 있 습 니다 . 베 이 즈 이 론 ( Bayes’ Theorom ) 을 적 용 하 기 위 한 필 요 한 확 률 값 들 을 계 산 해 봅 니다 . • Pr ( 𝐷 1 = 1 and 𝐷 2 = 1 ) = 0 . 0003 · 0 . 9985 + 0 . 98 · 0 . 0015 = 0 . 00176955 • Pr ( 𝐻 = 1 | 𝐷 1 = 1 and 𝐷 2 = 1 ) = 0 . 98 · 0 . 0015 0 . 00176955 = 0 . 831 4 . 6 . 확 률 과 통 계 89 즉 , 두 번 째 테 스 트 결과 는 좋 지 않 다는 것 에 더 확 신 하 게 만 듭 니다 . 두 번 째 결과 는 첫 번 째 보 다 덜 정 확 함 에 도 불 구 하 고 , 예 측 결과 를 더 향 상 시 켰 습 니다 . 그 렇 다 면 , 첫 번 째 테 스 트 를 두 번 하 지 않 을 까 요 ? 결 국 , 첫 번 째 테 스 트 가 더 정 확 했 습 니다 . 두 번 째 테 스 트 가 필 요 한 이유 는 첫 번 째 테 스 트 를 독 립 적 으 로 확 인 하 기 위 함 입 니다 . 즉 , Pr ( 𝐷 1 , 𝐷 2 | 𝐻 ) = Pr ( 𝐷 1 | 𝐻 ) Pr ( 𝐷 2 | 𝐻 ) 이 라 는 암 묵 적 인 가 정 을 했 습 니다 . 통 계 학 에 서 는 이 런 확 률 변 수 를 조 건 에 독 립 적 이 라 고 하 며 , 𝐷 1 ⊥⊥ 𝐷 2 | 𝐻 라 고 표 현 합 니다 . 4 . 6 . 4 요 약 이 절 에 서 우 리 는 확 률 , 독 립 , 조 건 독 립 , 그 리 고 기 본 적 인 결 론 을 도 출 하 는 데 이 것 들 을 어 떻 게 사 용 하 는 지 를 알아 봤 습 니다 . 이 개 념 들 은 아 주 유 용 합 니다 . 다 음 절 에 서 는 나 이 브 베 이 즈 분 류 기 ( Naive Nayes ) 를 사 용 한 기 본 적 인 예 측 을 하 는 데 이 개 념 들 이 어 떻 게 사 용 되 는 지 살 펴 보 겠 습 니다 . 4 . 6 . 5 문 제 1 . Pr ( 𝐴 ) 과 Pr ( 𝐵 ) 확 률 로 두 사 건 이 주 어 졌 을 때 , Pr ( 𝐴 ∪ 𝐵 ) 와 Pr ( 𝐴 ∩ 𝐵 ) 의 상 한 과 하한 을 구 하 세 요 . 힌 트 - Venn Diagram 을 사 용 하 는 상 황 을 그 려 보 세 요 . 2 . 연 속 적 인 사 건 , 즉 𝐴 , 𝐵 , 𝐶 , 들 이 있 는 데 , 𝐵 는 𝐴 에 만 의 존 하 고 , 𝐶 는 𝐵 에 만 의 존 한 다 고 가 정 합 니다 . 이 경 우 교 차 확 률 ( joint probability ) 를 간 단 하 게 할 수 있을 까 요 ? 힌 트 - 이 는 Markov Chain 입 니다 . 4 . 6 . 6 Scan the QR Code to Discuss 4 . 7 나 이 브 베 이 즈 분 류 ( Naive Nayes Classiﬁcation ) 조 건 에 독 립 적 인 것 은 데 이 터 를 다 루 는 데 있 어 서 많 은 공 식 을 간 단 하 게 해 주 기 때 문 에 유 용 합 니다 . 간 단 하 고 유 명 한 알 고 리 즘 으 로 나 이 브 베 이 즈 분 류 ( Naive Bayes Classiﬁer ) 가 있 습 니다 . 이 알 고 리 즘 의 주 요 가 정 은 주 어 진 레 이 블 들 에 대 해 서 모 든 속성 들 이 서 로 영 향 을 주 지 않 는다는 것 입 니다 . 즉 , 90 4 . 딥 러 닝 맛 보 기 다 음 과 같 은 수식 을 만 족 시 킵 니다 . 𝑝 ( x | 𝑦 ) = ∏︁ 𝑖 𝑝 ( 𝑥 𝑖 | 𝑦 ) 베 이 즈 이 론 을 적 용 해 보 면 , 𝑝 ( 𝑦 | x ) = ∏︀ 𝑖 𝑝 ( 𝑥 𝑖 | 𝑦 ) 𝑝 ( 𝑦 ) 𝑝 ( x ) 분 류를 얻 을 수 있 습 니다 . 하 지 만 아 쉽 게 도 , 𝑝 ( 𝑥 ) 를 모 르 기 때 문 에 다 루 기 어 렵 습 니다 . 다 행 인 것 은 , ∑︀ 𝑦 𝑝 ( 𝑦 | x ) = 1 인 것 을 알 고 있 기 때 문 에 , 𝑝 ( 𝑥 ) 가 필 요 가 없 습 니다 . 따 라 서 , 우 리 는 항 상 표 준 화 ( normalization ) 를 구 할 수 있 습 니다 . 𝑝 ( 𝑦 | x ) ∝ ∏︁ 𝑖 𝑝 ( 𝑥 𝑖 | 𝑦 ) 𝑝 ( 𝑦 ) . ﬀ 스 팸 메 일 과 일 반 메 일을 분 류 하 는 것 을 예 로 들 어 서 설 명 해 보 겠 습 니다 . Nigeria , prince , money , rich 와 같 은 단 어 들 이 이 메 일 에 있 다는 것 은 그 이 메 일이 스 팸 일 가 능 성 이 있 다는 것 을 암 시 한 다 고 할 수 있 습 니다 . 반 면 에 , theorem , network , Bayes , statistics 같 은 단 어 는 메 시 지 가 실 질 적 인 내 용 을 담 고 있 는 것 을 암 시 한 다 고 할 수 있 습 니다 . 따 라 서 , 이 런 단 어 들 이 속 한 클 래 스 가 주 어 졌 을 때 각각 에 대 한 확 률 에 대 한 모 델 을 만 들 고 , 문 장이 스 팸 일 가 능 성 에 대 한 점 수 를 매 기 는 데 사 용 하 는 것 이 가 능 합 니다 . 실 제 로 이 방법 은 많 은 오 랫 동 안 Bayesian spam ﬁlters 가 사 용 하 는 방법 입 니다 . 4 . 7 . 1 광 학 문 자 인 지 ( optical character recognition ) 이 미 지 가 다 루 기 더 쉽 기 때 문 에 , MNIST 데 이 터 셋 의 숫 자 를 구 분 하 는 문 제 에 나 이 브 베 이 즈 분 류를 적 용 해 보 겠 습 니다 . 여 기 서 문 제 는 𝑝 ( 𝑦 ) 와 𝑝 ( 𝑥 𝑖 | 𝑦 ) 를 모 른 다는 것 입 니다 . 그 렇 게 때 문 에 우 리 가 해 야 할 일은 주 어 진 학 습 데 이 터 를 사 용 해 서 이 확 률 들 을 추 정 해 야 합 니다 . 즉 , 모 델 을 학 습시 키 는 것 이 필 요 합 니다 . 𝑝 ( 𝑦 ) 를 추 정 하 는 것 은 어 려 운 일이 아 닙니다 . 우 리 가 다 루 는 클 래 스 의 개 수 가 10 개 이 기 때 문 에 , 아 주 쉽 게 추 정 할 수 있 습 니다 - 즉 , 각 숫 자 가 나 오 는 것 을 카 운 팅 한 수 𝑛 𝑦 전 체 데 이 터 개 수 𝑛 으 로 나 누 면 됩 니다 . 예 를 들 어 숫 자 8 이 나 온 횟 수 가 𝑛 8 = 5 , 800 이 고 전 체 데 이 터 개 수 가 of 𝑛 = 60 , 000 개 이 면 , 추 정 확 률 은 𝑝 ( 𝑦 = 8 ) = 0 . 0967 입 니다 . 이 제 조 금 더 어 려 운 𝑝 ( 𝑥 𝑖 | 𝑦 ) 에 대 해 서 이 야 기 해 보 겠 습 니다 . 이 미 지 가 흑 백 으 로 되 어 있 기 때 문 에 , 𝑝 ( 𝑥 𝑖 | 𝑦 ) 는 픽 셀 𝑖 가 클 래 스 𝑦 에 속 할 확 률 을 나 타 냅 니다 . 앞에 서 적 용 한 방법 을 이 용 해 서 어 떤 이 벤 트 가 발 생 한 횟 수 𝑛 𝑖𝑦 를 카 운 트 하 고 , y 가 일 어 난 전 체 횟 수 𝑛 𝑦 로 나 눌 수 있 습 니다 . 그 러 나 약 간 까 다 로 운 문 제 가 있 습 니다 - 어 떤 픽 셀 은 절 대 로 검 정 색 이 지 않 을 수 있 습 니다 . ( 만 약 , 이 미 지 를 잘 잘 라 내 면 , 코 너 픽 셀 들 은 항 상 흰 색 일 것 이 기 때 문 입 니다 . ) 통 계 학 자 들 이 이 런 문 제 를 다 루 는 편 리 한 방법 으 로 의 사 ( pseudo ) 카 운 트 를 모 든 것 에 더 하 는 것 입 니다 . 즉 , 𝑛 𝑖𝑦 를 사 용 하 는 대 신 𝑛 𝑖𝑦 + 1 를 , 𝑛 𝑦 대 신 𝑛 𝑦 + 1 를 사 용 하 는 것 입 니다 . 이 방법 은 Laplace Smoothing 이 라 고 불 리 는 것 입 니다 . 4 . 7 . 나 이 브 베 이 즈 분 류 ( Naive Nayes Classiﬁcation ) 91 [ 1 ] : % matplotlib inline from matplotlib import pyplot as plt from IPython import display display . set _ matplotlib _ formats ( ' svg ' ) import mxnet as mx from mxnet import nd import numpy as np # We go over one observation at a time ( speed doesn ' t matter here ) def transform ( data , label ) : return ( nd . floor ( data / 128 ) ) . astype ( np . float32 ) , label . astype ( np . float32 ) mnist _ train = mx . gluon . data . vision . MNIST ( train = True , transform = transform ) mnist _ test = mx . gluon . data . vision . MNIST ( train = False , transform = transform ) # Initialize the counters xcount = nd . ones ( ( 784 , 10 ) ) ycount = nd . ones ( ( 10 ) ) for data , label in mnist _ train : y = int ( label ) ycount [ y ] + = 1 xcount [ : , y ] + = data . reshape ( ( 784 ) ) # using broadcast again for division py = ycount / ycount . sum ( ) px = ( xcount / ycount . reshape ( 1 , 10 ) ) 모 든 픽 셀 들 에 대 해 서 픽 셀 별 로 등 장 하 는 횟 수 를 계 산 했 으 니 , 우 리 의 모 델 이 어 떻 게 동 작 하 는 지 그 림 을 그 려 서 보 겠 습 니다 . 이 미 지 를 이 용 하 면 아 주 편 한 점 은 시 각 화 가 가 능 하 다는 것 입 니다 . 28x28x10 확 률 을 시 각 화 하 는 것 은 무 의 미 한 일이 니 , 이 미 지 형 태 로 그 려 서 빠 르 게 살 펴 보 겠 습 니다 . 눈 치 가 빠 른 분 은 숫 자 를 닯 은 어 떤 평 균 임을 알아 차 렸 을 지 도 모 르 겠 습 니다 . [ 2 ] : import matplotlib . pyplot as plt fig , figarr = plt . subplots ( 1 , 10 , figsize = ( 10 , 10 ) ) for i in range ( 10 ) : figarr [ i ] . imshow ( xcount [ : , i ] . reshape ( ( 28 , 28 ) ) . asnumpy ( ) , cmap = ' hot ' ) figarr [ i ] . axes . get _ xaxis ( ) . set _ visible ( False ) figarr [ i ] . axes . get _ yaxis ( ) . set _ visible ( False ) plt . show ( ) print ( ' Class probabilities ' , py ) 92 4 . 딥 러 닝 맛 보 기 Class probabilities [ 0 . 09871688 0 . 11236461 0 . 09930012 0 . 10218297 0 . 09736711 0 . 09035161 0 . 09863356 0 . 10441593 0 . 09751708 0 . 09915014 ] < NDArray 10 @ cpu ( 0 ) > 우 리 의 모 델 에 근 거 해 서 이 미 지 의 가 능 성 들 에 대 한 계 산 을 할 수 있 게 되 었 습 니다 . 통 계 용 어 로 말 하 면 , 𝑝 ( 𝑥 | 𝑦 ) ﬀ 을 계 산 할 수 있 습 니다 . 즉 , 이 는 주 어 진 이 미 지 가 특 정 레 이 블 에 속 할 확 률 이 됩 니다 . ‘’ 모 든 픽 셀 이 독 립 적 이 다 ” 라 고 가 정 하 는 나 이 브 베 이 즈 모 델 에 따 르 면 , 다 음 공 식 을 얻 습 니다 . 𝑝 ( x | 𝑦 ) = ∏︁ 𝑖 𝑝 ( 𝑥 𝑖 | 𝑦 ) ﬀ 따 라 서 , 베 이 즈 법 칙 을 사 용 하 면 , 𝑝 ( 𝑦 | x ) 의 값 은 다 음 식 으 로 구 할 수 있 습 니다 . 𝑝 ( 𝑦 | x ) = 𝑝 ( x | 𝑦 ) 𝑝 ( 𝑦 ) ∑︀ 𝑦 ′ 𝑝 ( x | 𝑦 ′ ) ﬀ 그 럼 코 드 를 실 행해 보 겠 습 니다 . [ 3 ] : # Get the first test item data , label = mnist _ test [ 0 ] data = data . reshape ( ( 784 , 1 ) ) # Compute the per pixel conditional probabilities xprob = ( px * data + ( 1 - px ) * ( 1 - data ) ) # Take the product xprob = xprob . prod ( 0 ) * py print ( ' Unnormalized Probabilities ' , xprob ) # Normalize xprob = xprob / xprob . sum ( ) print ( ' Normalized Probabilities ' , xprob ) Unnormalized Probabilities [ 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . ] < NDArray 10 @ cpu ( 0 ) > Normalized Probabilities [ nan nan nan nan nan nan nan nan nan nan ] ( continues on next page ) 4 . 7 . 나 이 브 베 이 즈 분 류 ( Naive Nayes Classiﬁcation ) 93 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) < NDArray 10 @ cpu ( 0 ) > 완 전 히 틀 린 결과 를 얻었 습 니다 . 이유 를 찾 기 위 해 각 픽 셀 의 확 률 을 살 펴 보 겠 습 니다 . 일 반 적 으 로 확 률 값 은 0 . 001 와 1 사 이의 값 입 니다 . 우 리 는 784 개 의 값 을 곱 했 습 니다 . 바 로 이 부분 에 서 문 제 가 발 생 합 니다 . 즉 , 고 정 소 수 점 연 산 을 수 행하 면 서 수 치 적 인 언 더 플 로 우 ( 𝑛𝑢𝑚𝑒𝑟𝑖𝑎𝑙𝑢𝑛𝑑𝑒𝑟𝑓𝑙𝑜𝑤 ) 가 발 생 합 니다 . 작은 수 들 을 계 속 곱 하 면 결 국 0 이 되 는 되 고 , 결 국 0 을 나 누는 일이 발 생 하 기 때 문 에 결과가 nan 이 되 는 것 입 니다 . 이 문 제 를 해 결 하 기 위 해 서 log 𝑎𝑏 = log 𝑎 + log 𝑏 이 라 는 것 을 사 용 하 겠 습 니다 . 즉 , 로 그 ( logarithm ) 합 으 로 바 꾸 기 입 니다 . 이 를 적 용 하 면 log - 공간 에 서 비 표 준 화 된 확 률 을 얻 게 되 니 , 표 준 화 하 기 위 해 서 다 음 과 같 은 공 식 을 활 용 합 니다 . exp ( 𝑎 ) exp ( 𝑎 ) + exp ( 𝑏 ) = exp ( 𝑎 + 𝑐 ) exp ( 𝑎 + 𝑐 ) + exp ( 𝑏 + 𝑐 ) ﬀ 분 모 항 의 하 나 는 1 이 되 도 록 𝑐 = − max ( 𝑎 , 𝑏 ) ﬀ 로 선 택 합 니다 . [ 4 ] : logpx = nd . log ( px ) logpxneg = nd . log ( 1 - px ) logpy = nd . log ( py ) def bayespost ( data ) : # We need to incorporate the prior probability p ( y ) since p ( y | x ) is # proportional to p ( x | y ) p ( y ) logpost = logpy . copy ( ) logpost + = ( logpx * data + logpxneg * ( 1 - data ) ) . sum ( 0 ) # Normalize to prevent overflow or underflow by subtracting the largest # value logpost - = nd . max ( logpost ) # Compute the softmax using logpx post = nd . exp ( logpost ) . asnumpy ( ) post / = np . sum ( post ) return post fig , figarr = plt . subplots ( 2 , 10 , figsize = ( 10 , 3 ) ) # Show 10 images ctr = 0 for data , label in mnist _ test : x = data . reshape ( ( 784 , 1 ) ) y = int ( label ) ( continues on next page ) 94 4 . 딥 러 닝 맛 보 기 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) post = bayespost ( x ) # Bar chart and image of digit figarr [ 1 , ctr ] . bar ( range ( 10 ) , post ) figarr [ 1 , ctr ] . axes . get _ yaxis ( ) . set _ visible ( False ) figarr [ 0 , ctr ] . imshow ( x . reshape ( ( 28 , 28 ) ) . asnumpy ( ) , cmap = ' hot ' ) figarr [ 0 , ctr ] . axes . get _ xaxis ( ) . set _ visible ( False ) figarr [ 0 , ctr ] . axes . get _ yaxis ( ) . set _ visible ( False ) ctr + = 1 if ctr = = 10 : break plt . show ( ) 보 이 는 것 처 럼 , 이 분 류 기 가 많 은 경 우 잘 동 작 하 고 있 습 니다 . 하 지 만 , 뒤 에 서 두 번 째 숫 자 는 예 측 이 틀 리 기 도 하 고 그 잘 못 된 예 측 에 대 해 서 너 무 높 은 확 신 값 을 주 고 있 습 니다 . 즉 , 완 전 히 틀 린 추 측 일 경 우 에 도 확 률 을 0 또 는 1 에 아 주 가 깝 게 출 력 하 고 있 습 니다 . 이 런 모 델 은 사 용 할 수 있 는 수 준 이 아 닙니다 . 이 분 류 기 의 전 반 적 인 정 확 도 를 계 산 해 서 이 모 델 이 얼 마 나 좋 은 지 확 인 합 니다 . [ 5 ] : # Initialize counter ctr = 0 err = 0 for data , label in mnist _ test : ctr + = 1 x = data . reshape ( ( 784 , 1 ) ) y = int ( label ) ( continues on next page ) 4 . 7 . 나 이 브 베 이 즈 분 류 ( Naive Nayes Classiﬁcation ) 95 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) post = bayespost ( x ) if ( post [ y ] < post . max ( ) ) : err + = 1 print ( ' Naive Bayes has an error rate of ' , err / ctr ) Naive Bayes has an error rate of 0 . 1574 현 대 딥 네 트 워 크 는 0 . 01 보 다 낮 은 에 러 율을 달 성 합 니다 . 나 이 브 베 이 즈 분 류 기 는 80 년 대 나 90 년 대 에 스 팸 필 터 등 을 만 드 는 데 많 이 사 용 되 었 지 만 , 이 제 는 더 이 상 사 용 되 지 않 습 니다 . 성 능 이 나 쁜 이유 는 우 리 가 모 델 을 만 들 때 했 던 통 계 적 인 가 정 이 틀 렸 기 때 문 입 니다 - 즉 , 모 든 픽 셀 은 서 로 연 관 이 없 고 , 오 직 레 이 블 에 만 관 련 이 있 다 고 가 정 한 것 이 틀 렸 기 때 문 입 니다 . 사 람 들 이 숫 자 를 적 는 방법 이 다 양 하 다는 것 을 반 영 하 지 못 하 는 틀 린 가 정 이 잘 작 동 하 지 않 는 분 류 기 를 만 들 어 낸 것 입 니다 . 자 이 제 부 터 딥 네 트 워 크 를 만 드 는 것 을 시 작 해 보 겠 습 니다 . 4 . 7 . 2 요 약 • 나 이 브 베 이 즈 는 𝑝 ( x | 𝑦 ) = ∏︀ 𝑖 𝑝 ( 𝑥 𝑖 | 𝑦 ) 를 가 정 하 는 분 류 기 를 만 들 기 쉽습 니다 . • 분 류 기 는 학 습시 키 기 쉽 지 만 , 예 측 이 많 이 틀 리 기 쉽습 니다 . • 전 반 적 인 신 뢰 수 준 과 틀 린 예 측 을 해 결 하 기 위 해 서 , 𝑝 ( 𝑥 𝑖 | 𝑦 ) 확 률 을 Laplace smoothing 과 같 은 방법 을 적 용 할 수 있 습 니다 . 즉 , 모 든 카 운 트 에 상 수 를 더 하 는 방법 을 적 용 할 수 있 습 니다 . • 나 이 브 베 이 즈 분 류 기 는 관 찰 ( observation ) 들 사 이의 관 계 를 고 려 하 지 않 습 니다 . 4 . 7 . 3 문 제 1 . 𝑝 ( 𝑥 𝑖 | 𝑦 ) 가 표 준 분 포 를 따 를 때 , 나 이 브 베 이 즈 회 귀 모 델 ( Naive Bayes regression estimator ) 을 만 들 어 보 세 요 . 2 . 어 떤 경 우 나 이 브 베 이 즈 가 잘 동 작 할 까 요 ? 3 . 어 떤 목 격 자 가 가 해 자 를 다 시 봤 을 경 우 , 90 % 정 확 도 로 그 사 람 을 인 식 할 수 있 다 고 확 신 합 니다 . • 5 명 의 용 의자 가 있을 경 우 , 이 사 실 이 유 용 할 까 요 ? • 50 명 일 경 우 에 도 유 용 할 까 요 ? 96 4 . 딥 러 닝 맛 보 기 4 . 7 . 4 Scan the QR Code to Discuss 4 . 8 샘 플 링 난 수 는 확 률 변 수 의 한 형 태 인 데 , 컴 퓨 터 는 숫 자 를 다 루 는 것 을 아 주 잘 하 기 때 문 에 코 드 를 제 외 한 거 의 모 든 것 은 결 국 숫 자 로 바 뀝 니다 . 난 수 를 만 드 는 데 필 요 한 기 본 도 구 중 에 하 나 는 어 떤 분 포 에 서 샘 플 을 추출 하 는 것 입 니다 . 그 럼 난 수 생 성 기 를 사 용 했 을 때 어 떤 일이 벌 어 지 는 지 보 겠 습 니다 . 우 선 필 요 한 모 듈 을 import 하 겠 습 니다 . [ 1 ] : % matplotlib inline from matplotlib import pyplot as plt import mxnet as mx from mxnet import nd import numpy as np [ 2 ] : import random for i in range ( 10 ) : print ( random . random ( ) ) 0 . 21785526546058442 0 . 4875898504605559 0 . 4926469792095761 0 . 5163976417187927 0 . 5522929801883227 0 . 5210376921832638 0 . 5505350452267133 0 . 05428584699457617 0 . 6542585258797543 0 . 6479446877975654 4 . 8 . 샘 플 링 97 4 . 8 . 1 균 등 분 포 ( uniform distribution ) 위 결과 의 숫 자 들 은 굉 장 히 임의 로 선 택 된 것 들 입 니다 . 이 숫 자 들 은 0 과 1 사 이의 범 위 에 서 잘 분 포 되 어 있 습 니다 . 즉 , 어 떤 특 정 구 간 에 숫 자 들 이 몰 려 있 지 않 습 니다 . ( 사 실 진짜 난 수 발 생 기 가 아 니 기 때 문 에 그 럴 수 도 있 습 니다 . ) 어 떤 숫 자 가 [ 0 . 2 , 0 . 3 ) 구 간 에 서 선 택 될 가 능 성 과 [ . 593264 , . 693264 ) 구 간 에 서 선 택 될 가 능 성 이 비 슷 하 다는 의 미 입 니다 . 이 난 수 발 생 기 는 내 부 적 으 로 임의의 정 수 를 먼 저 만 들 고 , 그 다 음 에 이 숫 자 를 최 대 구 간 의 숫 자 로 나 누는 방 식 으 로 동 작 합 니다 . 정 수 를 얻 고 싶 은 경 우 에 는 , 다 음 과 같 이 하 면 됩 니다 . 아 래 코 드 는 0 과 100 사 이의 임의의 정 수 를 생 성 합 니다 . [ 3 ] : for i in range ( 10 ) : print ( random . randint ( 1 , 100 ) ) 33 43 48 9 85 92 16 66 47 37 randint 가 정 말 로 균 일 하 다는 것 을 확 인을 어 떻 게 할 수 있을 까 요 ? 직 관 적 으 로 가 장 좋 은 방법 은 100 만 개 의 난 수 를 생 성 해 서 각 숫 자 가 몇 번 나 오 는 지 계 산 하 고 , 이 분 포 가 균 일 한 지 를 확 인 하 는 것 입 니다 . [ 4 ] : import math counts = np . zeros ( 100 ) fig , axes = plt . subplots ( 2 , 3 , figsize = ( 15 , 8 ) , sharex = True ) axes = axes . reshape ( 6 ) # Mangle subplots such that we can index them in a linear fashion rather than # a 2d grid for i in range ( 1 , 1000001 ) : counts [ random . randint ( 0 , 99 ) ] + = 1 if i in [ 10 , 100 , 1000 , 10000 , 100000 , 1000000 ] : axes [ int ( math . log10 ( i ) ) - 1 ] . bar ( np . arange ( 1 , 101 ) , counts ) plt . show ( ) 98 4 . 딥 러 닝 맛 보 기 위 결과 로 나 온 그 림 으 로 확 인 하 면 , 초 기 의 숫 자 는 균 등 해 보 이 지 않 습 니다 . 100 개 의 정 수 에 대 한 분 포 에 서 100 개 보 다 적 은 개 수 를 뽑 는 경 우 는 당 연 한 결과 입 니다 . 하 지 만 , 1000 샘 플 을 뽑 아 도 여 전 히 상 당 한 변 동 이 있 습 니다 . 우 리 가 원 하 는 결과 는 숫 자 를 뽑 았 을 때 의 확 률 이 𝑝 ( 𝑥 ) 가 되 는 것 입 니다 . 4 . 8 . 2 카 테 고 리 분 포 ( categorical distribution ) 사 실 , 100 개 중 에 서 균 일 한 분 포 로 뽑 기 를 하 는 것 은 아 주 간 단 합 니다 . 만 약에 불 균 일 한 확 률 을 사 용 해 야 한 다 면 어 떻 게 해 야 할 까 요 ? 동 전 을 던 졌 을 때 앞 면 이 나 올 확 률 이 0 . 35 이 고 뒷 면 이 나 올 확 률 이 0 . 65 가 나 오 는 편 향 된 ( biased ) 동 전 을 간 단 한 예 로 들 어 보 겠 습 니다 . 이 를 구 현 하 는 간 단 한 방법 은 [ 0 , 1 ] 구 간 에 서 균 일 하 게 숫 자 를 선 택 하 고 그 수 가 0 . 35 보 다 작으 면 앞 면 으 로 , 크 면 뒷 면 으 로 하 는 것 입 니다 . 그 럼 코 드 를 보 겠 습 니다 . [ 5 ] : # Number of samples n = 1000000 y = np . random . uniform ( 0 , 1 , n ) x = np . arange ( 1 , n + 1 ) # Count number of occurrences and divide by the number of total draws p0 = np . cumsum ( y < 0 . 35 ) / x p1 = np . cumsum ( y > = 0 . 35 ) / x ( continues on next page ) 4 . 8 . 샘 플 링 99 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) plt . figure ( figsize = ( 15 , 8 ) ) plt . semilogx ( x , p0 ) plt . semilogx ( x , p1 ) plt . show ( ) 결과 에 서 보 이 듯 이 , 평 균 적 으 로 보 면 35 % 가 0 이 고 , 65 % 가 1 입 니다 . 두 개 이 상 의 결과가 있 다 면 어 떻 게 할 까 요 ? 위 아 이 디 어 를 다 음 과 같 이 일 반 화 하 면 됩 니다 . 예 를 들 면 𝑝 = [ 0 . 1 , 0 . 2 , 0 . 05 , 0 . 3 , 0 . 25 , 0 . 1 ] 와 같 은 분 포 가 있 다 고 하 면 , 누 적 된 분 포 를 계 산 해 서 𝐹 = [ 0 . 1 , 0 . 3 , 0 . 35 , 0 . 65 , 0 . 9 , 1 ] 를 얻 습 니다 . ( 이 는 Python 의 cumsum 함 수 를 이 용 하 면 간 단 히 할 수 있 습 니다 . ) 이 전 과 동 일 하 게 𝑈 [ 0 , 1 ] 범 위의 균 일 분 포 에 서 난 수 𝑥 를 뽑 고 , 𝐹 [ 𝑖 − 1 ] ≤ 𝑥 < 𝐹 [ 𝑖 ] 를 만 족 시 키 는 구 간 을 찾 아 서 𝑖 를 샘 플 결과 로 리 턴 합 니다 . 이 렇 게 하 면 난 수 가 [ 𝐹 [ 𝑖 − 1 ] , 𝐹 [ 𝑖 ] ) 구 간 에 속 할 확 률 이 𝑝 ( 𝑖 ) 가 됩 니다 . 위 방법보 다 훨 씬 더 효 율 적 인 알 고 리 즘 이 많 이 있 습 니다 . 예 를 들 면 , 𝑛 개 의 랜 덤 변 수 에 대 해 서 𝐹 에 대 한 이 진 검 색 을 수 행하 면 𝑂 ( log 𝑛 ) 시 간 이 걸 립 니다 . 𝑂 ( 𝑛 ) ﬀ 만 큼 의 전 처 리를 하 면 샘 플 링 에 상 수 의 시 간 이 걸 리 는 Alias Method 와 같 은 더 좋 은 알 고 리 즘 들 이 있 습 니다 . 4 . 8 . 3 표 준 분 포 ( normal distribution ) 표 준 분 포 ( 또 는 가 우 시 안 분 포 ) 는 𝑝 ( 𝑥 ) = 1 √ 2 𝜋 exp (︀ − 12 𝑥 2 )︀ 로 정 의 됩 니다 . 그 림 으 로 확 인 해 보 겠 습 니다 . 100 4 . 딥 러 닝 맛 보 기 [ 6 ] : x = np . arange ( - 10 , 10 , 0 . 01 ) p = ( 1 / math . sqrt ( 2 * math . pi ) ) * np . exp ( - 0 . 5 * x * * 2 ) plt . figure ( figsize = ( 10 , 5 ) ) plt . plot ( x , p ) plt . show ( ) 이 분 포 에 서 샘 플 링 을 하 는 것 은 그 리 간 단 하 지 않 습 니다 . 우 선 서 포 트 가 무 한합 니다 . 즉 , 어 떤 𝑥 값 이 주 어 지 든 지 확 률 밀 도 𝑝 ( 𝑥 ) 값 이 양 수 입 니다 . 두 번 째 특 징 은 확 률 밀 도 는 균 일 하 지 않 습 니다 . 이 분 포 에 서 샘 플 링 을 수 행하 는 많 은 기 법 이 있 는 데 , 모 든 알 고 리 즘 에 사 용 되 는 주 요 아 이 디 어 는 𝑝 ( 𝑥 ) 를 계 층 화 해 서 균 일 한 분 포 𝑈 [ 0 , 1 ] 로 매 핑 시 키 는 것 입 니다 . 확 률 적 분 변 환 이 그 방법 중 에 하 나 입 니다 . 𝑝 의 누 적 분 포 함 수 ( cumulative distribution function , CDF ) 를 𝐹 ( 𝑥 ) = ∫︀ 𝑥 −∞ 𝑝 ( 𝑧 ) 𝑑𝑧 로 표 기 합 니다 . 이 방법 은 앞에 서 누 적 합 의 연 속 된 버 전 이 라 고 할 수 있 습 니다 . 이 전 과 같 은 방법 으 로 균 일 하 게 뽑 은 𝜉 에 대 해 서 역 으 로 매 핑하 는 𝐹 − 1 ( 𝜉 ) 를 정 의 할 수 있 습 니다 . 벡 터 𝐹 에 대 한 정 확 한 구 간 을 찾 는 이 전 의 문 제 와 다 르 게 , 우 리 는 𝐹 ( 𝑥 ) 의 역 을 구 해 야 합 니다 . 실 제 로 가 우 시 안 의 경 우 CDF 의 역 을 구 하 는 것 은 다 소 까 다 롭 습 니다 . 두 개 의 균 등 한 분 포 들 을 다 뤄 야 하 지 만 이 차 원 적 분 이 더 다 루 기 쉽 기 때 문 에 두 개 의 표 준 확 률 변 수 로 만 듭 니다 . 지 금 은 이 문 제 를 해 결 해 주 는 알 고 리 즘 이 있 다 고 해 두 겠 습 니다 . 표 준 분 포 는 또 다 른 중 요 한 특 징 이 있 습 니다 . 만 약 어 떤 분 포 에 서 충 분 히 많 은 뽑 기 를 해 서 평 균 을 구 한 다 면 , 모 든 분 포 는 표 준 분 포 에 수 렴 합 니다 . 이 를 더 자 세 히 이 해하 기 위 해 서 , 세 가 지 중 요 한 개 념 들 인 기 대 값 ( expected value ) , 평 균 , 그 리 고 분 산 을 소 개 하 겠 습 니다 . 4 . 8 . 샘 플 링 101 • 분 포 𝑝 를 따 르 는 함 수 𝑓 에 대 한 기 대 값 E 𝑥 ∼ 𝑝 ( 𝑥 ) [ 𝑓 ( 𝑥 ) ] 은 적 분 ∫︀ 𝑥 𝑝 ( 𝑥 ) 𝑓 ( 𝑥 ) 𝑑𝑥 으 로 계 산 됩 니다 . 즉 , 이 는 𝑝 에 따 라 주 어 지 는 모 든 결과 에 대 한 평 균 값 입 니다 . • 함 수 𝑓 ( 𝑥 ) = 𝑥 에 대 한 기 대 값 은 굉 장 히 중 요 합 니다 . 이 함 수 의 기 대 값 은 𝜇 : = E 𝑥 ∼ 𝑝 ( 𝑥 ) [ 𝑥 ] 입 니다 . 이 는 전 형 적 인 𝑥 에 대 한 아 이 디 어 를 제 공 해 주 기 때 문 입 니다 . • 중 요 한 다 른 개 념 으 로 는 분 산 이 있 습 니다 . 이 는 𝜎 2 : = E 𝑥 ∼ 𝑝 ( 𝑥 ) [ ( 𝑥 − 𝜇 ) 2 ] 으 로 표 현 되 며 , 평 균 으 로 부 터 얼 마 나 떨 어 져 있 는 지 를 알 려 줍 니다 . 간 단 한 계 산 을 하 면 분 산 은 𝜎 2 = E 𝑥 ∼ 𝑝 ( 𝑥 ) [ 𝑥 2 ] − E 2 𝑥 ∼ 𝑝 ( 𝑥 ) [ 𝑥 ] 로 표 현 되 기 도 합 니다 . 위 개 념 은 확 률 변 수 의 평 균 과 분 산 을 바 꿀 수 있 게 해 줍 니다 . 예 를 들 면 확 률 변 수 𝑥 의 평 균 이 𝜇 일 경 우 확 률 변 수 𝑥 + 𝑐 의 평 균 은 𝜇 + 𝑐 이 됩 니다 . 또 한 , 확 률 변 수 가 𝛾𝑥 일 경 우 에 는 분 산 은 𝛾 2 𝜎 2 이 됩 니다 . 평 균 𝜇 이 고 분 산 이 𝜎 2 인 확 률 변 수 에 표 준 분 포 ( normal distribution ) 를 적 용 하 면 𝑝 ( 𝑥 ) = 1 √ 2 𝜎 2 𝜋 exp (︀ − 12 𝜎 2 ( 𝑥 − 𝜇 ) 2 )︀ 의 형 태 가 됩 니다 . 스 캐 일 팩 터 1 𝜎 가 적 용 된 것 에 주 의 하 세 요 . 이 렇 게 한 이유 는 이 분 포 를 𝜎 만 큼 늘 릴 경 우 , 같 은 확 률 값 을 갖게 하 기 위 해 서 1 𝜎 만 큼 줄 여야 할 필 요 가 있 기 때 문 입 니다 . ( 왜 냐 하 면 분 포 의 가 중 치 들 의 합 은 항 상 1 이 어야 하 기 때 문 입 니다 . ) 자 이 제 통 계 학 에 서 가 장 기 본 적 인 이 론 중 에 하 나 에 대 해 서 알아 볼 준 비 가 되 었 습 니다 . 이 는 Central Limit Theorem 입 니다 . 이 이 론 은 충 분 히 잘 행 동 하 는 확 률 변 수 에 대 해 서 , 특 히 잘 정 의 된 평 균 과 분 산 을 가 지 고 있 는 확 률 변 수 , 전 체 합 은 표 준 분 포 로 근 접 합 니다 . 이 해 를 돕 기 위 해 서 시 작 할 때 사 용 했 던 실 험 을 정 수 값 { 0 , 1 , 2 } 을 갖 는 확 률 변 수 를 사 용 해 봅 니다 . [ 7 ] : # Generate 10 random sequences of 10 , 000 uniformly distributed random variables tmp = np . random . uniform ( size = ( 10000 , 10 ) ) x = 1 . 0 * ( tmp > 0 . 3 ) + 1 . 0 * ( tmp > 0 . 8 ) mean = 1 * 0 . 5 + 2 * 0 . 2 variance = 1 * 0 . 5 + 4 * 0 . 2 - mean * * 2 print ( ' mean { } , variance { } ' . format ( mean , variance ) ) # Cumulative sum and normalization y = np . arange ( 1 , 10001 ) . reshape ( 10000 , 1 ) z = np . cumsum ( x , axis = 0 ) / y plt . figure ( figsize = ( 10 , 5 ) ) for i in range ( 10 ) : plt . semilogx ( y , z [ : , i ] ) plt . semilogx ( y , ( variance * * 0 . 5 ) * np . power ( y , - 0 . 5 ) + mean , ' r ' ) plt . semilogx ( y , - ( variance * * 0 . 5 ) * np . power ( y , - 0 . 5 ) + mean , ' r ' ) plt . show ( ) mean 0 . 9 , variance 0 . 49 102 4 . 딥 러 닝 맛 보 기 위 결과 를 보 면 많 은 변 수 들 의 평 균 만 을 보 면 처 음 예 제 와 아 주 비 슷 하 게 보 입 니다 . 즉 , 이 론 이 맞 다는 것 을 보 여 줍 니다 . 확 률 변 수 의 평 균 과 분 산 은 다 음 과 같 이 표 현 됩 니다 . 𝜇 [ 𝑝 ] : = E 𝑥 ∼ 𝑝 ( 𝑥 ) [ 𝑥 ] 와 𝜎 2 [ 𝑝 ] : = E 𝑥 ∼ 𝑝 ( 𝑥 ) [ ( 𝑥 − 𝜇 [ 𝑝 ] ) 2 ] 그 러 면 , lim 𝑛 →∞ 1 √ 𝑛 ∑︀ 𝑛𝑖 = 1 𝑥 𝑖 − 𝜇 𝜎 → 𝒩 ( 0 , 1 ) 이 됩 니다 . 즉 , 어 떤 값 으 로 부 터 시 작 했 는 지 상 관 없 이 , 가 우 시 안 분 포 에 항 상 수 렴 하 게 됩 니다 . 이 것 이 통 계 에 서 가 우 시 안 분 포 가 유 명 한 이유 들 중 에 하 나 입 니다 . 4 . 8 . 4 여 러 분 포 들 그 외 에 도 유 용 한 분 산 들 이 많 이 있 습 니다 . 더 자 세 한 내 용 은 통 계 책 이 나 위 키 피 디 아 를 참 조 하 세 요 . • 이 항 분 포 ( Binomial Distribution ) 같 은 분 포 에 서 여 러 번 뽑 을 때 의 분 포 를 설 명 하 는 데 사 용 됩 니다 . 즉 , 편 향 된 동 전 ( 동 전 앞 면 이 나 올 확 률 이 𝜋 ∈ [ 0 , 1 ] 인 동 전 을 사 용 할 때 ) 을 10 번 던 져 서 앞 면 이 나 오 는 횟 수 . 이 산 분 포 는 𝑝 ( 𝑥 ) = (︀ 𝑛𝑥 )︀ 𝜋 𝑥 ( 1 − 𝜋 ) 𝑛 − 𝑥 입 니다 . • 다 항 분 포 ( Multinomial Distribution ) 두 개 보 다 많 은 결과가 있을 경 우 에 해 당 합 니다 . 즉 , 주 사 위 를 여 러 번 던 지 는 경 우 를 예 로 들 수 있 습 니다 . 이 경 우 분 포 는 𝑝 ( 𝑥 ) = 𝑛 ! ∏︀ 𝑘𝑖 = 1 𝑥 𝑖 ! ∏︀ 𝑘𝑖 = 1 𝜋 𝑥 𝑖 𝑖 ﬀ 로 주 어 집 니다 . 4 . 8 . 샘 플 링 103 • 포 아 송 분 포 ( Poisson Distribution ) 주 어 진 속 도 ( rate ) 에 따 라 서 일 어 나 는 이 벤 트 를 모 델 링 할 때 사 용 됩 니다 . 예 를 들 면 , 어 느 공간 에 일 정 시 간 동 안 떨 어 지 는 빗 방 울 의 수 가 됩 니다 . ( 특 이 한 사 실 은 , 프 러 시 안 군 인 들 이 말 의 발 길 에 치 여 서 죽 은 수 가 이 분 포 를 따 르 고 있 습 니다 . ) 속 도 𝜆 에 대 해 서 , 일이 일 어 날 확 률 은 𝑝 ( 𝑥 ) = 1 𝑥 ! 𝜆 𝑥 𝑒 − 𝜆 로 표 현 됩 니다 . • 베 타 , 디 리 치 ( Dirichlet ) , 감 마 , 위 샤 트 ( Wishart ) 분 포 통 계 학 자 들 은 이 것 들 을 각각 이 산 , 다 항 , 포 아 송 , 그 리 고 가 우 시 안 분 포 의 변 종 이 라 고 설 명 하 고 있 습 니다 . 이 분 포 들 은 분 포 들 의 집 합 에 대 한 계 수 를 위 한 사 전 순 위 로 사 용 되 는 데 , 자 세 한 설 명 은 생 략 하 겠 습 니다 . 이 산 결과 들 의 확 률 을 모 델 링 하 는 데 사 전 순 위 로 의 베 타 분 포 같 은 것 입 니다 . 4 . 8 . 5 Scan the QR Code to Discuss 4 . 9 문 서 ( documentation ) 이 책 에 서 MXNet 함 수 와 클 래 스 를 모 두 설 명 하 기 는 불 가 능 하 니 , API 문 서 나 추 가 적 인 튜 토 리 얼 과 예 제 를 참 고 하 면 이 책 에 서 다 루 지 못 한 많 은 내 용 을 찾 아 볼 수 있 습 니다 . 4 . 9 . 1 모 듈 의 모 든 함 수 와 클 래 스 찾 아 보 기 모 듈 에 서 어 떤 함 수 와 클 래 스 가 제 공 되 는 지 알 기 위 해 서 dir 함 수 를 이 용 합 니다 . 예 를 들 어 , nd . random 모 듈 의 모 든 맴 버 와 속성 을 다 음 과 같 이 조 회 할 수 있 습 니다 . [ 1 ] : from mxnet import nd print ( dir ( nd . random ) ) [ ’NDArray’ , ’ _ Null’ , ’ _ _ all _ _ ’ , ’ _ _ builtins _ _ ’ , ’ _ _ cached _ _ ’ , ’ _ _ doc _ _ ’ , ˓ → ’ _ _ file _ _ ’ , ’ _ _ loader _ _ ’ , ’ _ _ name _ _ ’ , ’ _ _ package _ _ ’ , ’ _ _ spec _ _ ’ , ˓ → ’ _ internal’ , ’ _ random _ helper’ , ’current _ context’ , ’exponential’ , ˓ → ’exponential _ like’ , ’gamma’ , ’gamma _ like’ , ’generalized _ negative _ binomial’ , ˓ → ’generalized _ negative _ binomial _ like’ , ’multinomial’ , ’negative _ binomial’ , ˓ → ’negative _ binomial _ like’ , ’normal’ , ’normal _ like’ , ’numeric _ types’ , ˓ → ’poisson’ , ’poisson _ like’ , ’randint’ , ’randn’ , ’shuffle’ , ’uniform’ , ˓ → ’uniform _ like’ ] ( continues on next page ) 104 4 . 딥 러 닝 맛 보 기 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) 일 반 적 으 로 이 름 이 _ _ 로 시 작 하 는 함 수 ( Python 에 서 특 별 한 객 체 를 나 타 냄 ) 나 _ 로 시 작 하 는 함 수 ( 보 통 은 내 부 함 수 들 ) 는 무 시 해 도 됩 니다 . 나 머 지 맴 버 들 에 대 해 서 는 이 름 을 통 해 추 측 해 보 면 , 다 양 한 난 수 를 생 성 하 는 메 소 드들 로 추 측 할 수 있 습 니다 . 즉 , 균 일 한 분 포 에 서 난 수 를 생 성 하 는 uniform , 표 준 분 산 에 서 난 수 를 생 성 하 는 normal 그 리 고 Poisson 샘 플 링 인 poisson 등 의 기 능 을 제 공 함 을 알 수 있 습 니다 . 4 . 9 . 2 특 정 함 수 들 과 클 래 스 들 의 사 용 법 찾 아 보 기 help 함 수 를 이 용 하 면 특 정 함 수 나 클 래 스 의 사 용 법 확 인 할 수 있 습 니다 . NDArray 의 ones _ like 함 수 를 예 로 살 펴 봅 니다 . [ 2 ] : help ( nd . ones _ like ) Help on function ones _ like : ones _ like ( data = None , out = None , name = None , * * kwargs ) Return an array of ones with the same shape and type as the input array . Examples : : x = [ [ 0 . , 0 . , 0 . ] , [ 0 . , 0 . , 0 . ] ] ones _ like ( x ) = [ [ 1 . , 1 . , 1 . ] , [ 1 . , 1 . , 1 . ] ] Parameters - - - - - - - data : NDArray The input out : NDArray , optional ( continues on next page ) 4 . 9 . 문 서 ( documentation ) 105 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) The output NDArray to hold the result . Returns - - - - - out : NDArray or list of NDArrays The output of this function . 문 서 를 보 면 , ones _ like 함 수 는 NDArray 객 체 와 모 두 1 로 설 정 된 같 은 모 양 ( shape ) 의 새 로 운 객 체 를 만 들 어 줍 니다 . 확 인 해 보 겠 습 니다 . [ 3 ] : x = nd . array ( [ [ 0 , 0 , 0 ] , [ 2 , 2 , 2 ] ] ) y = x . ones _ like ( ) y [ 3 ] : [ [ 1 . 1 . 1 . ] [ 1 . 1 . 1 . ] ] < NDArray 2x3 @ cpu ( 0 ) > Jupyter 노 트 북 에 서 는 ? 를 이 용 해 서 다 른 윈 도 우 에 문 서 를 표 시 할 수 있 습 니다 . 예 를 들 어 nd . random . uniform ? 를 수 행하 면 help ( nd . random . uniform ) 과 거 의 동 일 한 내 용 이 다 른 윈 도 우 에 나 옵 니다 . 그 리 고 , nd . random . uniform ? ? 와 같 이 ? 를 두 개 사 용 하 면 , 함 수 를 구 현 하 는 코 드 도 함 께 출 력 됩 니다 . 4 . 9 . 3 API 문 서 API 에 대 한 더 자 세 한 내 용 은 MXNet 웹 사 이 트 http : / / mxnet . apache . org / 를 확 인 하 세 요 . Python 및 이 외 의 다 른 프 로 그 램 언어에 대 한 내 용 들 을 웹 사 이 트 에 서 찾 을 수 있 습 니다 . 4 . 9 . 4 문 제 1 . API 문 서 에 서 ones _ like 와 autograd 를 찾 아 보 세 요 . 106 4 . 딥 러 닝 맛 보 기 4 . 9 . 5 Scan the QR Code to Discuss 4 . 9 . 문 서 ( documentation ) 107 108 4 . 딥 러 닝 맛 보 기 5 딥 러 닝 기 초 이 장 에 서 는 딥 러 닝 의 기 본 적 인 내 용 들 을 소 개 합 니다 . 네 트 워 크 아 키 텍 처 , 데 이 터 , 손 실 함 수 ( loss functino ) , 최 적 화 , 그 리 고 용 량 제 어 를 포 함합 니다 . 이 해 를 돕 기 위 해 서 , 선 형 함 수 , 선 형 회 귀 , 그 리 고 확 률 적 경 사 하 강 법 ( stochastic gradient descent ) 과 같 은 간 단 한 개 념 부 터 시 작 합 니다 . 이 것 들 은 soft - max 나 다 층 퍼 셉 트 론 ( multilayer perceptron ) 와 같 은 보 다 복 잡 한 개 념 의 기 초 가 됩 니다 . 우 리 는 이 미 상 당 히 강 력 한 네 트 워 크 를 디 자인 할 수 있 지 만 , 필 수 적 인 제 어 나 기 교 는 배 우 지 않았 습 니다 . 이 를 위 해 서 , 용 량 제 어 , 오 버 피 팅 ( overﬁtting ) 과 언 더 피 팅 ( underﬁtting ) 에 대 한 개 념 을 이 해할 필 요 가 있 습 니다 . 드 롭 아 웃 ( dropout ) , 수 치 안 정 화 ( numerical stability ) , 그 리 고 초 기 화 에 대 한 설 명 으 로 이 장을 마 무 리 할 예 정 입 니다 . 우 리 는 실 제 데 이 터 에 모 델 을 적 용 하 는 방법 에 집중 하 겠 습 니다 . 이 를 통 해 서 여 러 분 은 기 본 개 념 뿐 만 아 니 라 딥 네 트 워 크 를 실 제 문 제 에 적 용 할 수 있 도 록 할 예 정 입 니다 . 성 능 , 확 장 성 그 리 고 효 율 성 은 다 음 장 들 에 서 다 룹 니다 . 5 . 1 선 형 회 귀 ( Linear Regression ) 우 선 간 단 한 문 제 인 회 귀 문 제 ( regression ) 를 살 펴 보 겠 습 니다 . 회 귀 문 제 는 주 어 진 데 이 터 포 인 트 𝑥 에 해 당 하 는 실 제 값 으 로 주 어 지 는 타 겟 𝑦 를 예 측 하 는 과 제 입 니다 . 회 귀 문 제 는 현 실 에 서 많 이 보 이 는 문 제 입 니다 . 예 를 들 면 , 주 택 가격 , 기 온 , 판 매 량 등 과 같 은 연 속 된 값 을 예 측 하 는 문 제 들 을 들 수 있 109 습 니다 . 이 는 결과 값 이 이 미 지 분 류 와 같 이 과 일의 종 류를 예 측 하 는 이 산 적 인 ( discrete ) 구 분 문 제 ( classiﬁcation ) 와 는 다 릅 니다 . 5 . 1 . 1 선 형 회 귀 의 기 본 요 소 들 가 장 간 단 하 지 만 가 장 유 용 한 접 근 방법 인 선 형 회 귀 를 정 의 하 는 방법 으 로 예 측 함 수 가 입 력 피 처 들 의 선 형 조 합 으 로 표 현 된 다 고 가 정 합 니다 . 이 때 문 에 , 이 를 선 형 회 귀 ( linear regression ) 라 는 이 름 으 로 부 릅 니다 . 5 . 1 . 2 선 형 모 델 간 단 한 예 로 , 집 의 면 적 ( 제 곱 미 터 ) 과 지 어 진 후 몇 년 이 되 었 는 지 를 입 력 으 로 사 용 해 서 주 택 가격 을 예 측 하 는 문 제 를 들 어 보 겠 습 니다 . 이 경 우 모 델 을 다 음 과 같 은 수식 으 로 표 현 할 수 있 습 니다 . price = 𝑤 area · area + 𝑤 age · age + 𝑏 이 공 식 은 명 확 해 보 이 는 데 , 두 개 이 상 의 입 력 변 수 가 사 용 되 는 경 우 는 굉 장 히 긴 공 식 이 됩 니다 . ( 변 수 이 름 을 지 정 하 는 것 조 차 지 루 한 일입 니다 . ) 하 지 만 , 수 학 자 들 이 발 명 한 벡 터 를 사 용 하 면 간 단 하 게 표 현 이 가 능 합 니다 . 𝑑 개 의 변 수 가 있 다 고 하 면 , 모 델 은 아 래 와 같 이 표 현 됩 니다 . ˆ 𝑦 = 𝑤 1 · 𝑥 1 + . . . + 𝑤 𝑑 · 𝑥 𝑑 + 𝑏 데 이 터 포 인 트 들 을 𝑋 로 , 타 겟 변 수 는 𝑦 로 표 현 합 니다 . 각 데 이 터 포 인 트 𝑥 𝑖 와 이 에 대 한 label 값 인 𝑦 𝑖 를 추 정 해 서 연 관 시 켜 주 는 Γ 𝑋 ( 𝑤𝑒𝑖𝑔ℎ𝑡 ) 벡 터 𝑤 와 𝑏𝑖𝑎𝑠 𝑏 를 찾 아 보 는 것 을 시 도 해 봅 니다 . 이 를 조 금 전 문 적 인 수 학 기 호 로 표 현 하 면 , 위 긴 합 은 ˆ 𝑦 = w ⊤ x + 𝑏 이 됩 니다 . 마 지 막 으 로 , 데 이 터 포 인 트 들 의 집 합 𝑋 와 예 측 값 ˆ y 은 아 래 와 같 은 행 렬 - 벡 터 곱 의 공 식 이 됩 니다 . ˆ y = Xw + 𝑏 𝑥 와 𝑦 의 관 계가 대 략 선 형 적 이 라 고 가 정 하 는 것 은 상 당 히 합 리 적 입 니다 . 측 정 하 는 데 다 소 오 류 가 발 생 할 수 도 있 습 니다 . 예 를 들 면 , 주 택 가격 은 일 반 적 으 로 하 락 하 지 만 , 오 래 될 수 록 가 치 가 더 해 지 는 , 오 래 된 역 사 적 인 주 택 의 경 우 는 해 당 되 지 않 을 수 있 습 니다 . 파 라 메 터 𝑤 를 찾 기 위 해 서 는 두 가 지 가 더 필 요 합 니다 . 하 나 는 , 현 재 모 델 의 품 질 ( quality ) 를 측 정 하 는 방법 과 두 번 째 는 품 질 을 향 상 시 킬 수 있 는 방법 입 니다 . 110 5 . 딥 러 닝 기 초 5 . 1 . 3 학 습 데 이 터 우 선 필 요 한 것 은 데 이 터 입 니다 . 예 를 들 면 , 여 러 집 들 의 실 제 판 매 가격과 그 집 들 의 크 기 와 지 어 진 후 몇 년 이 지 났 는 지 에 대 한 데 이 타 가 필 요 합 니다 . 우 리 가 하 고 자 하 는 것 은 모 델 이 예 측 한 집 가격과 실 제 가격 의 차 이 ( 오 류 ) 를 최 소 화 하 는 모 델 파 라 미 터 를 찾 는 것 입 니다 . 머 신 러 닝 의 용 어 로 는 , 데 이 터 셋 은 ‘ 학 습 데 이 터 ’ 또 는 ‘ 학 습 셋 ’ 이 라 고 하 고 , 하 나 의 집 ( 집 과 판 매 가격 ) 을 ’ 샘 플 ’ , 그 리 고 그 집 의 판 매 가격 을 ‘ 레 이 블 ( label ) ’ 이 라 고 합 니다 . 레 이 블 을 예 측 하 기 위 해 서 사 용 된 두 값 은 ’ 피 처 ( feature ) ’ 또 는 ’ 공 변 량 ( covariate ) ’ 이 라 고 합 니다 . 피 처 는 샘 플 의 특 징 을 표 현 하 는 데 사 용 됩 니다 . 일 반 적 으 로 수 집 한 샘 플 의 개 수 를 𝑛 으 로 표 기 하 고 , 각 샘 플 은 인 덱 스 𝑖 를 사 용 해 서 𝑥 ( 𝑖 ) = [ 𝑥 ( 𝑖 ) 1 , 𝑥 ( 𝑖 ) 2 ] 와 레 이 블 은 𝑦 ( 𝑖 ) 로 표 현 합 니다 . 5 . 1 . 4 Loss 함 수 모 델 학 습 을 위 해 서 는 모 델 이 예 측 된 가격과 실 제 가격 의 오 차 를 측 정 해 야 합 니다 . 보 통 오 차 는 0 또 는 양 수 값 을 선 택 하 고 , 값 이 작을 수 록 , 오 차 가 적 음을 의 미 합 니다 . 일 반 적 으 로 제 곱 함 수 를 사 용 하 며 , index 𝑖 의 샘 플 에 대 한 오 차 계 산 은 다 음 과 같 이 합 니다 . 𝑙 ( 𝑖 ) ( w , 𝑏 ) = 1 2 (︁ ˆ 𝑦 ( 𝑖 ) − 𝑦 ( 𝑖 ) )︁ 2 , 수식 에 곱 해 진 1 / 2 상 수 값 은 2 차 원 항 목 을 미 분 했 을 때 값 이 1 이 되 게 만 들 어 서 조 금 더 간 단 하 게 수식 을 만 들 기 위 해 서 사 용 된 값 입 니다 . 이 를 사 용 하 면 , 오 류 값 이 작을 수 록 예 측 된 값 이 실 제 가격과 더 비 슷 해 지 고 , 두 값 이 같 으 면 오 류 는 0 이 됩 니다 . 학 습 데 이 터 셋 이 주 어 졌 을 때 , 이 오 류 는 모 델 파 라 미 터 들 에 만 의 존 합 니다 . 즉 , 이 함 수 를 모 델 파 라 미 터 를 파 라 미 터 로 갖 는 함 수 로 생 각 할 수 있 습 니다 . 머 신 러 닝 에 서 는 이 와 같 이 오 류를 측 정 하 는 함 수 를 ‘loss function’ 이 라 고 부 릅 니다 . 위 에 서 정 의 한 제 곱 오 류 함 수 는 ’square loss’ 라 고 부 릅 니다 . 조 금 더 구 체 적 으 로 살 펴 보 기 위 해 서 , 집 값 이 집 크 기 에 만 의 존 한 다는 모 델 을 가 정 해 서 일 차 원 문 제 로 회 귀 문 제 를 도 식 화 한 것 을 예 로 들 어 보 겠 습 니다 . 위 그 래 프 에 서 보 이 는 것 처 럼 , 이 차 의 존 성 ( quadratic dependence ) 으 로 인 해 서 예 측 값 ˆ 𝑦 ( 𝑖 ) 과 실 제 값 5 . 1 . 선 형 회 귀 ( Linear Regression ) 111 𝑦 ( 𝑖 ) 의 큰 차 이 는 loss 에 서 는 더 크 게 반 영 됩 니다 . 전 체 데 이 터 셋 에 대 해 서 모 델 의 품 질 을 측 정 하 기 위 해 서 학 습 셋 에 대 한 loss 의 평 균 값 을 사 용 할 수 있 습 니다 . 𝐿 ( w , 𝑏 ) = 1 𝑛 𝑛 ∑︁ 𝑖 = 1 𝑙 ( 𝑖 ) ( w , 𝑏 ) = 1 𝑛 𝑛 ∑︁ 𝑖 = 1 1 2 (︁ w ⊤ x ( 𝑖 ) + 𝑏 − 𝑦 ( 𝑖 ) )︁ 2 . 학 습 샘 플 들 의 평 균 loss 를 최 소 화 하 는 모 델 파 라 미 터 w * 와 𝑏 * 를 찾 는 것 이 모 델 을 학 습시 키 는 것 입 니다 . w * , 𝑏 * = argmin w , 𝑏 𝐿 ( w , 𝑏 ) . 5 . 1 . 5 최 적 화 알 고 리 즘 모 델 과 loss 함 수 가 상 대 적 으 로 간 단 하 게 표 현 되 는 경 우 앞에 서 정 의 한 loss 를 최 소 화 하 는 방법 은 역 행 렬 을 사 용 해 서 명 확 한 수식 으 로 표 현 할 수 있 습 니다 . 이 수식 은 다 양 하 고 좋 은 수 학 적 분 석 을 적 용 할 수 있 어 서 좋 지 만 , 적 은 경 우 에 만 적 용 할 수 있 는 제 약 이 있 습 니다 . ( 즉 , multilayer perceptron 이 나 비 선 형 레 이 어 가 있으 면 적 용 할 수 없 습 니다 . ) 대 부분 딥 러 닝 모 델 은 위 분 석 적 방법 을 적 용 할 수 없 습 니다 . loss 함 수 의 값 은 점 진 적 인 최 적 화 알 고 리 즘 을 사 용 해 서 모 델 파 라 미 터 를 유 한한 횟 수 로 업 데 이 트 하 면 서 줄 이 는 방법 을 적 용 해 야 만 합 니다 . 딥 러 닝 에 서 는 산 술 적 인 솔 루 션 으 로 미 니 배 치 를 적 용 한 stochastic gradient descent 방법 이 널 리 사 용 되 고 있 습 니다 . 사 용 되 는 알 고 리 즘 은 간 단 합 니다 : 우 선 , 일 반 적 으 로 는 난 수 를 이 용 해 서 모 델 파 라 미 터 를 초 기 화 합 니다 . 그 후 , 데 이 터 를 반복 적 으 로 사 용 해 서 loss 함 수 의 값 을 줄 이 는 것 을 반복 합 니다 . 각 반 복 에 서 는 학 습 데 이 터 에 서 미 리 정 한 개 수 만 큼 의 샘 플 들 을 임의 로 또 균 일 하 게 뽑 아 서 미 니 배 치 ℬ 를 구 성 하 고 , 미 니 배 치 의 값 들 에 대 한 평 균 loss 값 의 모 델 파 라 미 터 에 대 한 미 분 을 구 합 니다 . 마 지 막 으 로 이 결과 와 미 리 정 의 된 스 탭 크 기 𝜂 > 0 를 곱 해 서 loss 값 이 최 소 화 되 는 방 향 으 로 파 라 미 터 를 변 경 합 니다 . 수식 으 로 표 현 하 면 다 음 과 같 습 니다 . ( w , 𝑏 ) ← ( w , 𝑏 ) − 𝜂 | ℬ | ∑︁ 𝑖 ∈ℬ 𝜕 ( w , 𝑏 ) 𝑙 ( 𝑖 ) ( w , 𝑏 ) 이 차 원 loss 및 선 형 함 수 에 대 해 서 는 아 래 와 같 이 명 시 적 으 로 이 를 계 산 할 수 있 습 니다 . 여 기 서 w 와 x 는 벡 터 입 니다 . 벡 터 를 잘 사 용 하 면 𝑤 1 , 𝑤 2 , . . . 𝑤 𝑑 와 같 은 계 수 를 읽 기 쉬 운 수식 으 로 표 현 할 수 112 5 . 딥 러 닝 기 초 있 습 니다 . w ← w − 𝜂 | ℬ | ∑︁ 𝑖 ∈ℬ 𝜕 w 𝑙 ( 𝑖 ) ( w , 𝑏 ) = 𝑤 − 𝜂 | ℬ | ∑︁ 𝑖 ∈ℬ x ( 𝑖 ) (︁ w ⊤ x ( 𝑖 ) + 𝑏 − 𝑦 ( 𝑖 ) )︁ , 𝑏 ← 𝑏 − 𝜂 | ℬ | ∑︁ 𝑖 ∈ℬ 𝜕 𝑏 𝑙 ( 𝑖 ) ( w , 𝑏 ) = 𝑏 − 𝜂 | ℬ | ∑︁ 𝑖 ∈ℬ (︁ w ⊤ x ( 𝑖 ) + 𝑏 − 𝑦 ( 𝑖 ) )︁ . 위 수식 에 서 | ℬ | 는 각 미 니 배 치 의 샘 플 개 수 를 의 미 하 고 , 𝜂 는 ’ 학 습 속 도 ( learning rate ) ’ 를 의 미 합 니 다 . 학 습 속 도 는 양 수 값 을 사 용 합 니다 . 여 기 서 중 요 한 점 은 미 니 배 치 크 기 와 학 습 속 도 는 모 델 학 습 을 통 해 서 찾 아 지 는 값 이 아 니 라 여 러 분 이 직 접 선 택 을 해 야 하 는 값 들 입 니다 . 따 라 서 , 우 리 는 이 값 들 을 hyper - parameters 라 고 부 릅 니다 . 우 리 가 흔히 hyper - parameters 튜 닝 이 라 고 하 는 일은 이 값 들 을 조정 하 는 것 을 의 미 합 니다 . 아 주 나 쁜 경 우 에 는 적 당 한 hyper - parameters 를 찾 기까 지 반복 된 실 험 을 수 행해 야 할 수 도 있 습 니다 . 더 좋 은 방법 으 로 는 모 델 학 습 의 일 부 로 이 값 들 을 찾 는 것 이 있 습 니다 만 , 심 화 주제 이 기 때 문 에 여 기 서 는 다 루 지 않 겠 습 니다 . 5 . 1 . 6 모 델 을 이 용 한 예 측 모 델 학 습 이 끝나 면 모 델 파 라 미 터 w , 𝑏 에 해 당 하 는 값 ˆ w @ ˆ 𝑏 을 저 장 합 니다 . 학 습 을 통 해 서 loss 함 수 를 최 소 화 시 키 는 최 적 의 값 w * , 𝑏 * 를 구 할 필 요 는 없 습 니다 . 다 만 , 이 최 적 의 값 에 근 접 하 는 값 을 학 습 을 통 해 서 찾 는 것 입 니다 . 이 후 , 학 습 된 선 형 회 귀 모 델 ˆ w ⊤ 𝑥 + ˆ 𝑏 을 이 용 해 서 학 습 데 이 터 셋 에 없 는 집 정 보 에 대 한 집 가격 을 추 정 합 니다 . “ 추 정 ” 을 “ 모 델 예 측 ( prediction ) ” 또 는 “ 모 델 추 론 ( inference ) ” 라 고 합 니다 . “ 추 론 ( inference ) ” 이 라 는 용 어 는 실 제 로 는 잘 못 선 택 된 용 어 지 만 , 딥 러 닝 에 서 는 많 이 사 용 하 는 용 어 로 자 리 잡 았 습 니다 . 통 계 에 서 추 론 은 다 른 데 이 터 를 기 반 으 로 파 라 미 터 들 과 결과 를 추 정 하 는 것 을 의 미 하 기 때 문 에 , 통 계 학 자 들 과 이 야 기 할 때 이 용 어 로 인 해 서 혼 동 을 가 져 오 기 도 합 니다 . 하 지 만 , 이 미 보 편 적 으 로 사 용 되 고 있 기 때 문 에 , 학 습 된 모 델 에 새 로 운 데 이 터 를 적 용 하 는 것 을 추 론 이 라 는 용 어 로 사 용 하 겠 습 니다 . ( 수 백 년 을 걸 친 통 계 학 자 들 에 게 미 안 함 을 표 합 니다 . ) 5 . 1 . 7 선 형 회 귀 에 서 딥 네 트 워 크 로 지 금까 지 선 형 함 수 만 을 이 야 기 했 는 데 , 뉴 럴 네 트 워 크 는 이 보 다 많 은 것 을 다 룹 니다 . 물 론 선 형 함 수 는 중 요 한 구 성 요 소 입 니다 . 이 제 모 든 것 을 ‘ 층 ( layer ) ’ 표 기 법 으 로 다 시 기 술 해 보 겠 습 니다 . 5 . 1 . 선 형 회 귀 ( Linear Regression ) 113 5 . 1 . 8 뉴 럴 네 트 워 크 다 이 어 그 램 딥 러 닝 에 서 는 모 델 의 구 조 를 뉴 럴 네 트 워 크 다 이 어 그 램 으 로 시 각 화 할 수 있 습 니다 . 뉴 럴 네 트 워 크 구 조 로 선 형 회 귀 를 좀 더 명 확 하 게 표 현 해 보 면 , 그 림 3 . 1 에 서 와 같 이 뉴 럴 네 트 워 크 다 이 어 그 램 을 이 용 해 서 이 절 에 서 사 용 하 고 있 는 선 형 회 귀 모 델 을 도 식 화 할 수 있 습 니다 . 이 뉴 럴 네 트 워 크 다 이 어 그 램 에 서 는 모 델 파 라 미 터 인 가 중 치 와 bias 를 직 접 표 현 하 지 않 습 니다 . 위 뉴 럴 네 트 워 크 에 서 입 력 값 은 𝑥 1 , 𝑥 2 , . . . 𝑥 𝑑 입 니다 . 때 로 는 입 력 값 의 개 수 를 피 처 차 원 ( feature di - mension ) 이 라 고 부 르 기 도 합 니다 . 이 경 우 에 는 입 력 값 의 개 수 는 𝑑 이 고 , 출 력 값 의 개 수 는 1 입 니다 . 출 력 값 을 선 형 회 귀 의 결과 를 직 접 결과 로 사 용 한 다는 것 을 기 억 해 두 세 요 . 입 력 레 이 어에 는 어 떤 비 선 형 이 나 어 떤 계 산 이 적 용 되 지 않 기 때 문 에 , 이 네 트 워 크 의 총 레 이 어 의 개 수 는 1 개 입 니다 . 종종 이 런 네 트 워 크 를 단 일 뉴 론 이 라 고 부 르 기 도 합 니다 . 모 든 입 력 들 이 모 든 출 력 ( 이 경 우 는 한 개 의 출 력 ) 과 연 결 되 어 있 기 때 문 에 , 이 레 이 어 는 fully connected layer 또 는 dense layer 라 고 불 립 니다 . 5 . 1 . 9 생 물 학 으 로 우 회 뉴 럴 네 트 워 크 라 는 이 름 은 신 경과 학 으 로 부 터 나 왔 습 니다 . 얼 마 나 많 은 네 트 워 크 구 조 가 만 들 어 졌 는 지 잘 이 해하 기 위 해 서 , 우 선 뉴 론 ( neuron ) 의 기 본 적 인 구 조 를 살 펴 볼 필 요 가 있 습 니다 . 비 유 하 자 면 , 입 력 단 자인 수 상 돌 기 ( dendrities ) , CPU 인 핵 ( nucleu ) , 출 력 연 결 인 축 삭 ( axon ) , 그 리 고 , 시 냅 스 를 통 해 서 다 른 뉴 런 과 연 결 하 는 축 삭 단 자 ( axon terminal ) 라 고 하 면 충 분 합 니다 . 114 5 . 딥 러 닝 기 초 Dendrite Cell body Node of Ranvier Axon Terminal Schwann cell Myelin sheath Axon Nucleus 수 상 돌 기 는 다 른 뉴 론 들 로 부 터 온 정 보 𝑥 𝑖 를 받 습 니다 . 구 체 적 으 로 는 그 정 보 는 시 텝 틱 가 중 치 𝑤 𝑖 가 적 용 된 정 보 값 입 니다 . 이 가 중 치 는 입 력 에 얼 마 나 반 응을 해 야 하 는 지 정 의 합 니다 . ( 즉 , 𝑥 𝑖 𝑤 𝑖 를 통 해 서 활 성 화 됨 ) 이 모 든 값 들 은 핵 에 서 𝑦 = ∑︀ 𝑖 𝑥 𝑖 𝑤 𝑖 + 𝑏 , 로 통 합 되 고 , 이 정 보 는 축 삭 ( axon ) 으 로 보 내 져 서 다 른 프 로 세 스 를 거 치 는 데 , 일 반 적 으 로 는 𝜎 ( 𝑦 ) 를 통 해 서 비 선 형 처 리 가 됩 니다 . 이 후 , 최 종 목 적 지 ( 예 를 들 면 근 육 ) 또 는 수 상 돌 기 를 거 쳐 서 다 른 뉴 론 으 로 보 내 집 니다 . 뇌 의 구 조 는 아 주 다 양 합 니다 . 어 떤 것 들 은 다 소 임의 적 으 로 보 이 지 만 , 어 떤 것 들 은 아 주 규 칙 적 인 구 조 를 가 지 고 있 습 니다 . 예 를 들 면 , 여 러 곤 충 들 의 시 각 시스 템 은 아 주 구 조적 입 니다 . 이 구 조 들 에 대 한 분 석 을 통 해 서 신 경과 학 자 들 은 새 로 운 아 키 텍 처 를 제 안 하 는 데 영 감 을 받 기 도 하 고 , 어 떤 경 우 에 는 아 주 성 공 적 이 었 습 니다 . 하 지 만 , 비 행 기 가 새 로 부 터 영 감 을 받 아 서 만 들 어 졌 지 만 차 이 가 많 은 것과 같 이 , 이 둘 의 직 접적 인 관 계 를 찾 아 보 는 것 은 오 류 가 되 기 도 합 니다 . 수 학 과 컴 퓨 터 과 학 이 영 감 의 같 은 근 원 이 라 고 볼 수 있 습 니다 5 . 1 . 10 벡 터 화 로 빠 르 게 만 들 기 모 델 학 습 및 예 측 을 수 행할 때 , 벡 터 연 산 을 사 용 하 고 이 를 통 해 서 여 러 값 들 은 한 번 에 처 리 합 니다 . 이 것 이 왜 중 요 한 지 설 명 하 기 위 해 서 벡 터 들 을 더 하 는 두 가 지 방법 을 생 각 해 봅 시 다 . 우 선 1000 차 원 의 벡 터 두 개 를 생 성 합 니다 . [ 1 ] : from mxnet import nd from time import time a = nd . ones ( shape = 10000 ) b = nd . ones ( shape = 10000 ) 5 . 1 . 선 형 회 귀 ( Linear Regression ) 115 두 벡 터 를 더 하 는 방법 중 에 하 나 는 for loop 을 이 용 해 서 벡 터 의 각 값 들 을 하 나 씩 더 하 는 것 입 니다 . [ 2 ] : start = time ( ) c = nd . zeros ( shape = 10000 ) for i in range ( 10000 ) : c [ i ] = a [ i ] + b [ i ] time ( ) - start [ 2 ] : 1 . 4912874698638916 다 른 방법 으 로 는 두 벡 터 를 직 접 더 할 수 있 습 니다 . [ 3 ] : start = time ( ) d = a + b time ( ) - start [ 3 ] : 0 . 00019621849060058594 당 연 하 게 도 벡 터 를 직 접 더 하 는 방법 이 훨 씬 더 빠 릅 니다 . 코 드 를 벡 터 화 하 는 것 은 연 산 속 도 를 빠 르 게 하 는 좋 은 방법 입 니다 . 마 찬 가 지 로 연 산 식 을 간 단 하 게 하 고 , 표 기 에 있 어 서 잠재 적 인 오 류를 줄 여 주 는 효 과 도 있 습 니다 . 5 . 1 . 11 표 준 분 포 와 제 곱 Loss 아 래 내 용 은 옵 션 이 니 , 다 음으 로 넘 어 가 도 됩 니다 . 하 지 만 , 딥 러 닝 모 델 을 구 성 에 있 어 서 디 자인 선 택 에 대 한 이 해 를 높 이 는 데 도 움 이 됩 니다 . 위 에 서 봤 듯 이 , squared loss 𝑙 ( 𝑦 , ˆ 𝑦 ) = 12 ( 𝑦 − ˆ 𝑦 ) 2 ﬀ 는 간 단 한 편 미 분 𝜕 ^ 𝑦 𝑙 ( 𝑦 , ˆ 𝑦 ) = ( ˆ 𝑦 − 𝑦 ) ﬀ 과 같 은 좋 은 특 징 들 을 가 지 고 있 습 니다 . 즉 , gradient 가 예 측 값과 실 제 값 의 차 이 로 계 산 됩 니다 . 선 형 회 귀 는 전 통 적 인 통 계 모 델 입 니다 . Legendre 가 처 음으 로 least squares regression 을 1805 년 에 개 발 했 고 , 1809 년 에 Gauss 에 의 해 서 재 발 견 되 었 습 니다 . 이 를 조 금 더 잘 이 해 하 기 위 해 서 평 균 이 𝜇 ﬀ 이 고 분 산 이 𝜎 2 ﬀ 인 정 규 분 포 ( normal distribution ) 를 떠 올 려 봅 시 다 . 𝑝 ( 𝑥 ) = 1 √ 2 𝜋𝜎 2 exp (︂ − 1 2 𝜎 2 ( 𝑥 − 𝜇 ) 2 )︂ ﬀ 이 는 다 음 과 같 이 시 각 화 될 수 있 습 니다 . [ 4 ] : % matplotlib inline from matplotlib import pyplot as plt from IPython import display from mxnet import nd import math ( continues on next page ) 116 5 . 딥 러 닝 기 초 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) x = nd . arange ( - 7 , 7 , 0 . 01 ) # Mean and variance pairs parameters = [ ( 0 , 1 ) , ( 0 , 2 ) , ( 3 , 1 ) ] # Display SVG rather than JPG display . set _ matplotlib _ formats ( ' svg ' ) plt . figure ( figsize = ( 10 , 6 ) ) for ( mu , sigma ) in parameters : p = ( 1 / math . sqrt ( 2 * math . pi * sigma * * 2 ) ) * nd . exp ( - ( 0 . 5 / sigma * * 2 ) * ( x - mu ) * * 2 ) plt . plot ( x . asnumpy ( ) , p . asnumpy ( ) , label = ' mean ' + str ( mu ) + ' , variance ' + ˓ → str ( sigma ) ) plt . legend ( ) plt . show ( ) 위 그 림 에 서 보 이 듯 이 , 평 균 을 변 경 하 면 함 수 를 이 동 시 키 고 , 편 차 를 증 가 시 키 면 피 크 는 낮 추 고 더 펼 쳐 지 게 만 듭 니다 . least mean square loss 를 적 용 한 선 형 회 귀 에 서 중 요 한 가 정 은 관 찰 들 은 노 이 즈 가 있 는 관 찰 에 서 얻어 지 고 , 이 노 이 즈 들 은 데 이 터 에 더 해 진 다는 것 입 니다 . 𝑦 = w ⊤ x + 𝑏 + 𝜖 where 𝜖 ∼ 𝒩 ( 0 , 𝜎 2 ) ﬀ 5 . 1 . 선 형 회 귀 ( Linear Regression ) 117 이 는 주 어 진 𝑥 ﬀ 에 대 해 서 특 정 𝑦 ﬀ 를 얻 을 가 능 성 ( likelihood ) 는 다 음 과 같 이 표 현 됩 니다 . 𝑝 ( 𝑦 | x ) = 1 √ 2 𝜋𝜎 2 exp (︂ − 1 2 𝜎 2 ( 𝑦 − w ⊤ x − 𝑏 ) 2 )︂ ﬀ 가 장 근 접 한 𝑏 ﬀ 와 w ﬀ 값 을 찾 는 좋 은 방법 은 전 체 데 이 터 셋 에 대 한 likelihood 를 최 대 화 하 는 것 입 니 다 . 𝑝 ( 𝑌 | 𝑋 ) = 𝑛 ∏︁ 𝑖 = 1 𝑝 ( 𝑦 ( 𝑖 ) | x ( 𝑖 ) ) ﬀ 파 라 미 터 들 에 대 해 서 데 이 터 의 likelihood 를 최 대 화 하 는 것 은 Maximum Likelihood Principle 로 잘 알 려 져 있 고 , 이 런 estimator 들 은 Maximum Likelihood Estimators ( MLE ) 라 고 불 립 니다 . 아 쉽 게 도 , 많 은 지 수 함 수 들 의 곱 을 최 적 화 하 는 것 은 구 현 하 는 것 이 나 , 종 이 에 적 어 보 는 것 이 나 아 주 어 렵 습 니다 . 대 신 , 더 좋 은 방법 은 Negative Log - Likelihood − log 𝑃 ( 𝑌 | 𝑋 ) ﬀ 를 최 소 화 하 는 것 입 니다 . 위 예 는 다 음 수식 으 로 표 현 됩 니다 . − log 𝑃 ( 𝑌 | 𝑋 ) = 𝑛 ∑︁ 𝑖 = 1 1 2 log ( 2 𝜋𝜎 2 ) + 1 2 𝜎 2 (︁ 𝑦 ( 𝑖 ) − w ⊤ x ( 𝑖 ) − 𝑏 )︁ 2 ﬀ 이 공 식 을 잘 살 펴 보 면 − log 𝑃 ( 𝑌 | 𝑋 ) 를 최 소 화 할 때 는 첫 번 째 항 목 을 무 시 할 수 있 습 니다 . 왜 냐 하 면 , 첫 번 째 항 목 은 𝑤 , 𝑏 그 리 고 데 이 터 와 도 연 관 이 없 기 때 문 입 니다 . 두 번 째 항 목 은 우 리 가 이 전 에 봤 던 objective 와 상 수 1 𝜎 2 가 곱 해 진 것 을 빼 면 동 일 합 니다 . 이 값 은 가 장 근 접 한 솔 루 션 을 찾 는 것 만 원 한 다 면 무 시 할 수 있 고 , 이 렇 게 하 면 additive Gaussian noise 를 갖 는 선 형 모 델 의 likelihood 를 최 대 화 하 는 것 은 squared loss 를 적 용 한 선 형 회 귀 와 동 일 한 문 제 로 정 의 됩 니다 . 5 . 1 . 12 요 약 • 머 신 러 닝 에 서 중 요 한 요 소 는 학 습 데 이 터 , loss 함 수 , 최 적 화 알 고 리 즘 , 그 리 고 당 연 하 지 만 모 델 자 체 입 니다 . • 벡 터 화 는 모 든 것 ( 수 학 ) 을 좋 게 만 들 고 , ( 코 드 를 ) 빠 르 게 만 들 어 줍 니다 . • objective 함 수 를 최 소 화 하 는 것과 maximum likelihood 를 구 하 는 것 은 같 은 것 입 니다 . • 선 형 모 델도 뉴 럴 모 델 입 니다 . 118 5 . 딥 러 닝 기 초 5 . 1 . 13 문 제 1 . 데 이 터 𝑥 1 , . . . 𝑥 𝑛 ∈ R 가 있 다 고 가 정 합 니다 . 우 리 의 목 표 는 ∑︀ 𝑖 ( 𝑥 𝑖 − 𝑏 ) 2 를 최 소 화 시 키 는 상 수 𝑏 를 찾 는 것 입 니다 . • 최 적 의 닫 힌 형 식 의 해 ( closed form solution ) 을 찾 아 보 세 요 . • 표 준 분 포 의 용 어 로 이 것 은 무 엇 을 의 미 하 나 요 ? 2 . Quadratic loss 를 사 용 한 선 형 회 귀 의 최 적 화 문 제 를 닫 힌 형 식 으 로 풀 기 를 원 한 다 고 가 정 합 니다 . 간 단 하 게 하 기 위 해 서 , 문 제 에 서 편 향 ( bias ) 𝑏 는 생 략 해 도 됩 니다 . • 문 제 를 행 렬 과 벡 터 표 기 를 사 용 해 서 다 시 기 술 해 보 세 요 . ( 힌 트 - 모 든 데 이 터 를 하 나 의 행 렬로 취 급 합 니다 . ) • 최 적 화 문 제 를 𝑤 에 대 한 경 사 ( gradient ) 를 계 산 하 세 요 . • 행 렬 방 적 식 을 풀 어 서 닫 힌 형 식 의 해 ( closed form solution ) 를 구 하 세 요 . • 언 제 이 방법 이 확 률 적 경 사 하 강 법 ( stochastic gradient descent ) ( 즉 , 위 에 서 우 리 가 논 의 한 점 진 적 최 적 화 접 근 법 ) 보 다 좋 을 까 요 ? 언 제 이 방법 이 틀 릴 까 요 ? ( 힌 트 - 𝑥 가 고 차 원 이 면 어 떻 게 될 까 요 ? 만 약 많 은 관 찰 들 이 아 주 작으 면 어 떻 게 될 까 요 ? ) 3 . 부 가 노 이 즈 𝜖 을 결 정 하 는 노 이 즈 모 델 이 지 수 분 포 라 고 가 정 합 니다 . 즉 , 𝑝 ( 𝜖 ) = 12 exp ( − | 𝜖 | ) . • 모 델 − log 𝑝 ( 𝑌 | 𝑋 ) 의 데 이 터 에 대 한 네 가 티 브 로 그 - 가 능 도 ( log - likelihood ) 를 적 어 보 세 요 . • 닫 힌 형 식 의 해 를 찾 을 수 있 나 요 ? • 이 문 제 를 푸 는 확 률 적 경 사 하 강 법 ( stochastic gradient descent ) 알 고 리 즘 을 제 안 하 세 요 . 무 엇 이 잘 못 될 가 능 성 이 있 나 요 ? ( 힌 트 - 파 라 미 터 를 업 데 이 트 할 때 , 정 류 점 ( stationary point ) 근 처 에 서 어 떤 일이 일 어 날 까 요 ? ) 4 . NumPy 와 같 은 다 른 패 키 지 들 이 나 MATLAB 과 같 은 다 른 프 로 그 램 언어 를 사 용 해 서 두 벡 터 를 더 할 때 의 실 행 시 간 들 을 비 교 해 보 세 요 . 5 . 1 . 14 Scan the QR Code to Discuss 5 . 1 . 선 형 회 귀 ( Linear Regression ) 119 5 . 2 선 형 회 귀 를 처 음 부 터 구 현 하 기 선 형 회 귀 에 대 한 어 느 정 도 의 배 경 지 식 을 습 득 했 으 니 이 제 실 제 구 현 을 해 보 도 록 하 겠 습 니다 . 좋 은 딥 러 닝 프 레 임 워 크 를 이 용 하 면 반복 적 인 일을 줄 일 수 있 지 만 , 일을 쉽 게 하 기 위 해 서 프 레 임 워 크 에 너 무 의 존 하 면 딥 러 닝 이 어 떻 게 동 작 하 는 지 이 해하 기 어 렵 게 될 수 있 습 니다 . 예 를 들 면 , 이 후 에 레 이 어 , loss 함 수 등 을 직 접 정 의 해 야 하 는 경 우 에 특 히 그 렇 습 니다 . 따 라 서 , NDArray 와 autograd 만 을 이 용 해 서 선 형 회 귀 학 습 을 직 접 구 현 해 보 겠 습 니다 . 시 작 하 기 앞 서 , 이 절 에 서 필 요 한 패 키 지 와 모 듈 을 import 합 니다 . matplotlib 는 도 표 를 그 리 고 화 면 에 표 시 하 는 데 사 용 됩 니다 . [ 1 ] : % matplotlib inline from IPython import display from matplotlib import pyplot as plt from mxnet import autograd , nd import random 5 . 2 . 1 데 이 터 셋 생 성 하 기 간 단 한 학 습 데 이 터 셋 을 직 접 만 들 어 서 학 습 된 파 라 미 터 와 실 제 모 델 의 파 라 미 터 의 차 이 를 시 각 적 으 로 비 교 해 볼 수 있 습 니다 . 학 습 데 이 터 셋 의 샘 플 개 수 는 1000 개 로 하 고 , 특 성 ( feature ) 개 수 는 2 개 로 합 니다 . 임의 로 생 성 한 배 치 샘 플 특 성 ( feature ) X ∈ R 1000 × 2 와 실 제 가 중 치 값 w = [ 2 , − 3 . 4 ] ⊤ 와 편 향 ( bias ) 𝑏 = 4 . 2 를 사 용 하 겠 습 니다 . 그 리 고 , 임의의 노 이 즈 값 𝜖 도 사 용 합 니다 . y = Xw + 𝑏 + 𝜖 ﬀ 노 이 즈 항 목 𝜖 은 평 균 이 0 이 고 표 준 편 차 가 0 . 01 인 정 규 분 포 를 따 르 도 록 정 의 합 니다 . 아 래 코 드 로 실 제 데 이 터 셋 을 생 성 합 니다 . [ 2 ] : num _ inputs = 2 num _ examples = 1000 true _ w = nd . array ( [ 2 , - 3 . 4 ] ) true _ b = 4 . 2 features = nd . random . normal ( scale = 1 , shape = ( num _ examples , num _ inputs ) ) labels = nd . dot ( features , true _ w ) + true _ b labels + = nd . random . normal ( scale = 0 . 01 , shape = labels . shape ) features 의 각 행 은 2 차 원 데 이 터 포 인 트 로 구 성 되 고 , labels 의 각 행 은 1 차 원 타 겟 값 으 로 구 성 됩 니다 . 120 5 . 딥 러 닝 기 초 [ 3 ] : features [ 0 ] , labels [ 0 ] [ 3 ] : ( [ 2 . 2122064 0 . 7740038 ] < NDArray 2 @ cpu ( 0 ) > , [ 6 . 000587 ] < NDArray 1 @ cpu ( 0 ) > ) features [ : , 1 ] 과 labels 를 이 용 해 서 scatter plot 을 생 성 해 보 면 , 둘 사 이의 선 형 관 계 를 명 확 하 게 관 찰 할 수 있 습 니다 . [ 4 ] : def use _ svg _ display ( ) : # Display in vector graphics display . set _ matplotlib _ formats ( ' svg ' ) def set _ figsize ( figsize = ( 3 . 5 , 2 . 5 ) ) : use _ svg _ display ( ) # Set the size of the graph to be plotted plt . rcParams [ ' figure . figsize ' ] = figsize set _ figsize ( ) plt . figure ( figsize = ( 10 , 6 ) ) plt . scatter ( features [ : , 1 ] . asnumpy ( ) , labels . asnumpy ( ) , 1 ) ; 5 . 2 . 선 형 회 귀 를 처 음 부 터 구 현 하 기 121 plot 을 그 려 주 는 함 수 plt , use _ svg _ display 함 수 와 set _ figsize 함 수 는 g2l 패 키 지 에 정 의 되 어 있 습 니다 . 앞 으 로 는 plot 을 그 리 기 위 해 서 g2l . plt 를 직 접 호 출 하 겠 습 니다 . plt 은 g2l 패 키 지 의 전 역 변 수 로 정 의 되 어 있 기 때 문 에 , 벡 터 다 이 어 그 램 과 크 기 를 정 하 기 위 해 서 는 plot 을 그 리 기 전 에 g2l . set _ figsize ( ) 를 호 출 하 면 됩 니다 . 5 . 2 . 2 데 이 터 읽 기 모 델 을 학 습시 킬 때 , 전 체 데 이 터 셋 을 반복 적 으 로 사 용 하 면 서 각 데 이 터 의 미 니 배 치 를 얻어야 합 니 다 . 이 를 위 해 서 함 수 를 하 나 정 의 하 겠 습 니다 . 이 함 수 는 임의 로 선 택 된 특 성 ( feature ) 들 과 태 그 ( tag ) 들 을 배 치 크 기 ( batch size ) 의 개 수 만 큼 리 턴 해 주 는 역 할 을 합 니다 . 왜 한 번 에 하 나 의 샘 플 을 사 용 하 지 않 고 여 러 샘 플 을 리 턴 하 는 iterator 를 작 성 할 까 요 ? 이유 는 최 적 화 를 효 율 적 으 로 하 기 위 함 입 니다 . 한 번 에 하 나 의 1 - 차 원 값 을 처 리 했 을 때 성 능 이 아 주 느 렸 던 것 을 기 억 해 볼 까 요 . 하 나 의 샘 플 을 처 리 하 는 것 처 럼 , 하 나 의 벡 터 가 아 닌 행 렬로 표 현 된 샘 플 들 의 전 체 배 치 를 한 번 에 처 리 하 는 것 도 동 일 하 게 할 수 있 습 니다 . 특 히 , GPU 는 행 렬 을 다 룰 때 아 주 빠 른 속 도 로 연 산 을 수 행합 니다 . 이 것 이 딥 러 닝 에 서 보 통 하 나 의 샘 플 보 다는 미 니 배 치 단 위 로 연 산 을 하 는 이유 중 에 하 나 입 니다 . 122 5 . 딥 러 닝 기 초 [ 5 ] : # This function has been saved in the d2l package for future use def data _ iter ( batch _ size , features , labels ) : num _ examples = len ( features ) indices = list ( range ( num _ examples ) ) # The examples are read at random , in no particular order random . shuffle ( indices ) for i in range ( 0 , num _ examples , batch _ size ) : j = nd . array ( indices [ i : min ( i + batch _ size , num _ examples ) ] ) yield features . take ( j ) , labels . take ( j ) # The “take” function will then return the corresponding element based # on the indices 첫 번 째 작은 배 치 를 읽 어 서 출 력 해 보 겠 습 니다 . 각 배 치 의 특 성 ( feature ) 들 의 모 양 ( shape ) 은 배 치 크 기 와 입 력 차 원 의 수 와 연 관 됩 니다 . 마 찬 가 지 로 , 배 치 크 기 와 동 일 한 레 이 블 ( label ) 들 을 얻 습 니다 . [ 6 ] : batch _ size = 10 for X , y in data _ iter ( batch _ size , features , labels ) : print ( X , y ) break [ [ 2 . 045332 - 0 . 8686574 ] [ - 0 . 10930543 - 1 . 217784 ] [ - 0 . 3311225 0 . 53222805 ] [ - 0 . 5480731 - 0 . 75966454 ] [ 0 . 1177411 - 1 . 0699357 ] [ 2 . 310595 1 . 1056191 ] [ - 0 . 11902292 1 . 1829549 ] [ - 0 . 31461143 1 . 6538868 ] [ - 0 . 5227964 - 0 . 649588 ] [ - 0 . 572525 - 0 . 80434114 ] ] < NDArray 10x2 @ cpu ( 0 ) > [ 11 . 24293 8 . 119462 1 . 7325351 5 . 684718 8 . 061257 5 . 0523906 - 0 . 04490374 - 2 . 0525527 5 . 3611174 5 . 7827954 ] < NDArray 10 @ cpu ( 0 ) > 당 연 하 겠 지 만 , iterator 를 다 시 수 행하 면 , 전 체 데 이 터 를 모 두 소 진 할 때 까 지 다 른 미 니 배 치 를 반 환 합 니다 . 위 에 서 구 현 한 iterator 는 다 소 비 효 율 적 입 니다 ( 모 든 데 이 터 를 메모 리 에 로 딩 한 후 , 메모 리를 접 근 하 는 것 을 반복 하 기 때 문 입 니다 . ) 패 키 지 에 서 제 공 하 는 iterator 는 더 효 율 적 으 로 구 현 되 어 있 고 , 파 일 에 저 장 된 데 이 터 를 접 근 하 거 나 데 이 터 스 트 림 을 통 한 접 근 도 가 능 합 니다 . 5 . 2 . 선 형 회 귀 를 처 음 부 터 구 현 하 기 123 5 . 2 . 3 모 델 파 라 미 터 들 초 기 화 하 기 가 중 치 는 평 균 값 이 0 이 고 표 준 편 차 가 0 . 01 인 정 규 분 포 를 따 르 는 난 수 값 들 로 초 기 화 합 니다 . 편 향 ( bias ) 𝑏 는 0 으 로 설 정 합 니다 . [ 7 ] : w = nd . random . normal ( scale = 0 . 01 , shape = ( num _ inputs , 1 ) ) b = nd . zeros ( shape = ( 1 , ) ) 이 후 , 모 델 이 데 이 터 를 잘 예 측 할 수 있 도 록 이 파 라 미 터 들 을 업 데 이 트 할 것 입 니다 . 이 를 위 해 서 손 실 함 수 ( loss function ) 의 파 라 미 터 에 대 한 그 래 디 언 트 ( gradient ) , 즉 다 변 수 미 분 을 구 해 야 합 니다 . 손 실 ( loss ) 값 을 줄 이 는 방 향 으 로 각 파 라 미 터 를 업 데 이 트 할 것 입 니다 . autograd 가 적 당 한 데 이 터 구 조 를 준 비 하 고 , 변 경 을 추 적 할 수 있 도 록 , 그 래 디 언 트 ( gradient ) 들 을 명 시 적 으 로 붙 여 줘 야 합 니다 . [ 8 ] : w . attach _ grad ( ) b . attach _ grad ( ) 5 . 2 . 4 모 델 정 의 하 기 다 음으 로 는 모 델 을 정 의 합 니다 . 아 주 간 단 하 고 유 용 한 뉴 럴 네 트 워 크 인 선 형 모 델 을 정 의 하 겠 습 니 다 . 선 형 모 델 의 결과 를 계 산 하 기 위 해 서 , 입 력 값과 모 델 의 가 중 치 𝑤 를 곱 하 고 오 프 셋 ( offset ) 𝑏 를 더 합 니다 . [ 9 ] : # This function has been saved in the d2l package for future use def linreg ( X , w , b ) : return nd . dot ( X , w ) + b 5 . 2 . 5 손 실 함 수 ( loss function ) 정 의 하 기 이 전 절 에 서 선 형 회 귀 손 실 ( loss ) 를 정 의 하 는 데 사 용 한 제 곱 손 실 함 수 ( squared loss function ) 를 사 용 하 겠 습 니다 . 이 를 구 현 하 기 위 해 서 우 선 실 제 값 y 의 모 양 ( shape ) 을 예 측 값 y _ hat 의 모 양 ( shape ) 과 동 일 하 게 변 형 합 니다 . 다 음 함 수 의 리 턴 값 은 y _ hat 의 모 양 ( shape ) 과 동 일 하 게 바 꿉 니다 . [ 10 ] : # This function has been saved in the d2l package for future use def squared _ loss ( y _ hat , y ) : return ( y _ hat - y . reshape ( y _ hat . shape ) ) * * 2 / 2 124 5 . 딥 러 닝 기 초 5 . 2 . 6 최 적 화 알 고 리 즘 정 의 하 기 선 형 회 귀 문 제 는 잘 정 의 된 솔 루 션 이 있 습 니다 . 하 지 만 , 우 리 가 다 루 게 될 많 은 재 미 있 는 모 델 은 분 석 적 인 방법 으 로 풀 릴 수 없 습 니다 . 그 렇 기 때 문 에 , 이 문 제 를 확 률 적 경 사 하 강 법 ( stochastic gradient descent ) sgd 를 이 용 해 서 풀 어 볼 것 입 니다 . 각 단 계 에 서 데 이 터 셋 중 임의 로 선 택 한 배 치 를 이 용 해 서 , 가 중 치 ( weight ) 들 에 대 한 손 실 ( loss ) 의 그 래 디 언 트 ( gradient ) 를 추 정 합 니다 . 그 리 고 , 손 실 ( loss ) 을 줄 이 는 방 향 으 로 파 라 미 터 들 을 조 금 씩 업 데 이 트 합 니다 . 여 기 서 , 자 동 미 분 모 듈 로 계 산 된 그 래 디 언 트 ( gradient ) 는 샘 플 들 의 배 치 의 그 래 디 언 트 ( gradient ) 합 입 니다 . 평 균 을 구 하 기 위 해 서 , 이 값 을 배 치 크 기 로 나 누 고 , 학 습 속 도 ( learning rate ) lr 로 정 의 된 값 에 비 례 해 서 업 데 이 트 를 하 게 됩 니다 . [ 11 ] : # This function has been saved in the d2l package for future use def sgd ( params , lr , batch _ size ) : for param in params : param [ : ] = param - lr * param . grad / batch _ size 5 . 2 . 7 학 습 학 습 은 데 이 터 를 반복 해 서 사 용 하 면 서 모 델 의 파 라 미 터 를 최 적 화 시 키 는 것 입 니다 . 각 반복 에 서 는 현 재 얻어 진 미 니 배 치 의 데 이 터 샘 플 들 ( 피 처 X 와 레 이 블 y ) 을 이 용 해 서 역 함 수 인 backward 함 수 를 호 출 해 서 미 니 배 치 에 대 한 확 률 적 경 사 ( stochastic gradient ) 를 계 산 합 니다 . 이 후 , 최 적 화 알 고 리 즘 sgd 을 호 출 해 서 모 델 파 라 미 터 들 을 업 데 이 트 하 게 됩 니다 . 앞 의 소 스 코 드 에 서 배 치 크 기 batch _ size 를 10 으 로 설 정 했 으 니 , 각 미 니 배 치 별 로 손 실 ( loss ) l 의 모 양 ( shape ) 은 ( 10 , 1 ) 이 됩 니다 . • 파 라 미 터 ( w , 𝑏 ) 를 초 기 화 합 니다 . • 끝날 때 까 지 다 음을 반복 합 니다 . – 그 래 디 언 트 ( gradient ) 계 산 하 기 g ← 𝜕 ( w , 𝑏 ) 1 ℬ ∑︀ 𝑖 ∈ℬ 𝑙 ( x 𝑖 , 𝑦 𝑖 , w , 𝑏 ) – 파 라 미 터 업 데 이 트 하 기 ( w , 𝑏 ) ← ( w , 𝑏 ) − 𝜂 g 그 래 디 언 트 ( gradient ) 를 계 산 하 는 코 드 를 직 접 작 성 해 야 한 다 면 , 지 루 하 기 도 하 고 코 드 에 오 류 가 있 을 수 있 기 때 문 에 , 𝑔 를 계 산 해 주 는 자 동 미 분 ( automatic differentiation ) 을 이 용 합 니다 . 이 에 대 한 자 세 한 내 용 은 “Automatic Gradient” 절 을 참 조 하 세 요 . loss l 은 스 칼 라 변 수 가 아 니 기 때 문 에 , l . backward ( ) 를 수 행하 면 l 의 모 든 항 목 들 을 합해 서 새 로 운 변 수 를 만 들 고 , 이 를 이 용 해 서 다 양 한 모 델 파 라 미 터 의 gradient 를 계 산 합 니다 . 매 epoch 마 다 , data _ iter 함 수 를 이 용 해 서 학 습 데 이 터 셋 의 전 체 샘 플 을 한 번 씩 학 습 에 사 용 합 니다 . 이 때 , 전 체 샘 플 의 개 수 는 배 치 크 기 의 배 수 라 고 가 정 합 니다 . 하 이 퍼파 라 미 터 ( hyper - parameter ) 인 총 epoch 수 num _ epochs 와 학 습 속 도 ( learning rate ) lr 는 각 각 3 , 0 . 03 으 로 설 정 합 니다 . 실 제 5 . 2 . 선 형 회 귀 를 처 음 부 터 구 현 하 기 125 상 황 에 서 는 하 이 퍼파 라 미 터 ( hyper - parameter ) 들 은 반복 된 실 험 을 통 해 서 가 장 좋 은 값 을 직 접 찾 아 야 합 니다 . 예 를 들 면 , 어 떤 모 델 은 학 습 을 오 래 하 면 정 확 도 가 높 아 지 기 도 합 니다 . 물 론 이 때 , 연 산 비 용 이 증 가 하 게 됩 니다 . 마 찬 가 지 로 , 학 습 을 수 행하 는 중 간 에 학 습 속 도 ( learning rate ) 를 변 경 하 고 싶 은 경 우 도 있 습 니다 . 이 에 대 한 내 용 은 “ 최 적 화 알 고 리 즘 , Optimization Algorithms” 에 서 자 세 히 살 펴 보 겠 습 니다 . [ 12 ] : lr = 0 . 03 # Learning rate num _ epochs = 3 # Number of iterations net = linreg # Our fancy linear model loss = squared _ loss # 0 . 5 ( y - y ' ) ^ 2 for epoch in range ( num _ epochs ) : # Assuming the number of examples can be divided by the batch size , all # the examples in the training data set are used once in one epoch # iteration . The features and tags of mini - batch examples are given by X # and y respectively for X , y in data _ iter ( batch _ size , features , labels ) : with autograd . record ( ) : l = loss ( net ( X , w , b ) , y ) # Minibatch loss in X and y l . backward ( ) # Compute gradient on l with respect to [ w , b ] sgd ( [ w , b ] , lr , batch _ size ) # Update parameters using their gradient train _ l = loss ( net ( features , w , b ) , labels ) print ( ' epoch % d , loss % f ' % ( epoch + 1 , train _ l . mean ( ) . asnumpy ( ) ) ) epoch 1 , loss 0 . 040530 epoch 2 , loss 0 . 000152 epoch 3 , loss 0 . 000050 학 습 된 모 델 을 평 가 하 는 방법 으 로 실 제 파 라 미 터 와 학 습 을 통 해 서 찾 아 낸 파 라 미 터 를 비 교 해 보 면 , 이 것 들 이 매 우 비 슷 해 졌 습 니다 . [ 13 ] : print ( ' Error in estimating w ' , true _ w - w . reshape ( true _ w . shape ) ) print ( ' Error in estimating b ' , true _ b - b ) Error in estimating w [ 3 . 671646e - 04 - 7 . 772446e - 05 ] < NDArray 2 @ cpu ( 0 ) > Error in estimating b [ 6 . 866455e - 05 ] < NDArray 1 @ cpu ( 0 ) > 모 델 의 파 라 미 터 를 아 주 정 확 하 게 찾 아 내 는 것 이 간 단 한 일은 아 닙니다 . 아 주 특 별 한 분 류 의 문 제 에 서 만 간 단 히 찾 는 것 이 가 능 합 니다 . 예 를 들 면 , 노 이 즈 가 있 는 샘 플 들 을 이 용 해 도 숨 겨 진 상 관관 126 5 . 딥 러 닝 기 초 계 를 정 확 하 게 찾 아 낼 수 있을 정 도 로 많 은 양 의 데 이 터 가 주 어 진 강 한 볼 록 최 적 화 ( strongly convex optimzation ) 문 제 가 그 런 경 우 입 니다 . 대 부분 의 경 우 , 문 제 는 이 런 분 류 가 아 닙니다 . 실 제 로 는 학 습 데 이 터 가 사 용 되 는 순 서 를 포 함해 서 모 든 경 우 가 동 일 하 지 않 은 경 우 가 아 니 면 딥 네 트 워 크 의 파 라 미 터 들 이 비 슷 하 거 나 같게 나 오 지 않 습 니다 . 그 럼 에 도 불 구 하 고 , 뉴 럴 네 트 워 크 는 좋 은 모 델 을 만 들 어 내 는 데 , 이 는 예 측 을 잘 하 는 여 러 파 라 미 터 집 합 들 이 존 재 하 기 때 문 입 니다 . 5 . 2 . 8 요 약 이 절 에 서 NDArray 와 autograd 만 을 사 용 해 서 레 이 어 정 의 나 멋 진 옵 티 마 이 져 ( optimizer ) 를 위 한 별 도 의 도 구 없 이 도 딥 네 트 워 크 구 현 및 최 적 화 를 어 떻 게 할 수 있 는 지 살 펴 봤 습 니다 . 이 예 제 는 어 떤 것 들 이 가 능 한 지 에 대 한 기 본 만 다 뤘 고 , 다 음 절 에 서 는 지 금까 지 배 운 것 들 을 기 반 으 로 다 양 한 딥 러 닝 모 델 들 을 살 펴 보 고 , 보 다 간결 하 게 구 현 하 는 방법 에 대 해 서 살 펴 보 겠 습 니다 . 5 . 2 . 9 문 제 1 . 가 중 치 ( weight ) 들 을 0 으 로 ( w = 0 ) 초 기 화 를 하 면 어 떤 일이 발 생 할 까 요 ? 알 고 리 즘 이 여 전 히 동 작 할 까 요 ? 2 . 여 러 분 이 전 압 과 전 류 간 의 모 델 을 만 들 고 자 하 는 Georg Simon Ohm 이 라 고 가 정 해 보 세 요 . 여 러 분 의 모 델 파 라 미 터 를 학 습시 키 기 위 해 서 autograd 를 사 용 할 수 있을 까 요 ? 3 . 스 펙 트 럼 에 너 지 밀 도 를 사 용 해 서 물 체 의 온 도 를 결 정 하 는 데 Planck’s Law 를 사 용 할 수 있 나 요 ? 4 . autograd 를 이 차 미 분 으 로 확 장 한 다 면 어 떤 문 제 를 만 날 수 있을 까 요 ? 5 . squared _ loss 함 수 에 서 reshape 함 수 가 왜 필 요 한 가 요 ? 6 . 다 양 한 학 습 속 도 ( learning rate ) 들 을 사 용 해 서 실 험하 고 , 그 결과 손 실 함 수 ( loss function ) 값 이 얼 마 나 빠 르 게 감 소 하 는 지 알아 보 세 요 . 7 . 예 제 들 의 개 수 가 배 치 크 기 로 나 누 어 떨 어 지지 않 을 경 우 에 , data _ iter 함 수 는 어 떻 게 동 작 할 까 요 ? 5 . 2 . 선 형 회 귀 를 처 음 부 터 구 현 하 기 127 5 . 2 . 10 Scan the QR Code to Discuss 5 . 3 선 형 회 귀 의 간결 한 구 현 딥 러 닝 프 레 임 워 크 들 의 개 발 로 인 해 서 , 딥 러 닝 어 플 리 케 이 션 을 개 발 하 는 것 이 나날 이 쉬 워 지 고 있 습 니다 . 이 번 에 는 앞 절 에 서 구 현 한 모 델 을 보 다 간결 하 게 구 현 하 고 학 습시 키 는 방법 을 MXNet 에 서 제 공 하 는 Gluon 인 터 페 이 스 를 이 용 해 서 구 현 해 보 도 록 하 겠 습 니다 . 5 . 3 . 1 데 이 터 셋 만 들 기 이 전 절 에 서 사 용 한 것 처 럼 동 일 한 데 이 터 셋 을 생 성 합 니다 . [ 1 ] : from mxnet import autograd , nd num _ inputs = 2 num _ examples = 1000 true _ w = nd . array ( [ 2 , - 3 . 4 ] ) true _ b = 4 . 2 features = nd . random . normal ( scale = 1 , shape = ( num _ examples , num _ inputs ) ) labels = nd . dot ( features , true _ w ) + true _ b labels + = nd . random . normal ( scale = 0 . 01 , shape = labels . shape ) 5 . 3 . 2 데 이 터 읽 기 Gluon 은 데 이 터 를 읽 는 데 사 용 할 수 있 는 data 모 듈 을 제 공 합 니다 . data 라 는 이 름 은 변 수 이 름 으 로 많 이 사 용 하 기 때 문 에 , gdata 라 고 별 명 을 붙 여 서 사 용 하 겠 습 니다 . 매 반복 ( iteration ) 마 다 , 10 개 데 이 터 인 스 턴 스 를 갖 는 미 니 배 치 를 읽 어 보 겠 습 니다 . [ 2 ] : from mxnet . gluon import data as gdata batch _ size = 10 ( continues on next page ) 128 5 . 딥 러 닝 기 초 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) # Combine the features and labels of the training data dataset = gdata . ArrayDataset ( features , labels ) # Randomly reading mini - batches data _ iter = gdata . DataLoader ( dataset , batch _ size , shuffle = True ) data _ iter 는 이 전 절 에 서 사 용 한 방 식 과 같 습 니다 . 이 를 이 용 해 서 첫 번 째 미 니 배 치 를 읽 어 서 내 용 을 출 력 합 니다 . [ 3 ] : for X , y in data _ iter : print ( X , y ) break [ [ - 0 . 15826425 0 . 36227608 ] [ 0 . 8295173 0 . 39740315 ] [ - 0 . 23048365 0 . 7954761 ] [ 0 . 39320782 1 . 5395709 ] [ 0 . 85316896 1 . 0389048 ] [ - 0 . 17634277 0 . 35655284 ] [ 0 . 8817903 0 . 9748315 ] [ 0 . 7465509 - 0 . 5054337 ] [ 0 . 4231157 0 . 64803934 ] [ 1 . 0674734 0 . 79578614 ] ] < NDArray 10x2 @ cpu ( 0 ) > [ 2 . 6514182 4 . 5125256 1 . 0299395 - 0 . 25150746 2 . 3738737 2 . 6176562 2 . 6379745 7 . 4012175 2 . 8509607 3 . 6091642 ] < NDArray 10 @ cpu ( 0 ) > 5 . 3 . 3 모 델 정 의 하 기 선 형 회 귀 모 델 을 직 접 구 현 했 을 때 는 모 델 파 라 미 터 를 정 의 하 고 , 모 델 을 수 행하 는 매 단 계 를 직 접 작 성 했 는 데 , 더 복 잡 한 모 델 을 이 렇 게 만 들 어야 한 다 면 구 현 이 더 복 잡 해 질 것 입 니다 . Gluon 은 이 미 정 의 된 다 양 한 레 이 어 를 제 공 하 고 있 어 서 , 레 이 어 를 어 떻 게 구 현 하 는 지 가 아 니 라 , 레 이 어 를 사 용 한 모 델 을 설 계 하 는 데 집중 할 수 있 도 록 해 줍 니다 . 선 형 모 델 을 구 현 하 는 방법 은 , 우 선 nn 모 듈 을 import 하 는 것 으 로 시 작 합 니다 . nn 은 neural network 의 약 자입 니다 . 이 름 이 의 미 하 듯 이 이 모 듈 은 많 은 종 류 의 뉴 럴 네 트 워 크 층 들 을 바 로 사 용 할 수 있 도 록 제 공 합 니다 . Sequential 인 스 턴 스 인 net 모 델 변 수 를 정 의 합 니다 . Gluon 에 서 Sequential 인 스 5 . 3 . 선 형 회 귀 의 간결 한 구 현 129 턴 스 는 다 양 한 레 이 어 를 순 차 적 으 로 담는 컨 테 이 너 로 사 용 됩 니다 . 모 델 을 정 의 할 때 , 이 컨 터 이 너 에 층 을 원 하 는 순 서 대 로 추 가 합 니다 . 입 력 값 이 주 어 지 면 , 컨 테 이 너 의 각 층 순 서 대 로 계 산 이 이 뤄 지 고 , 각 층 의 결과값 은 다 음 층 의 입 력 으 로 사 용 됩 니다 . [ 4 ] : from mxnet . gluon import nn net = nn . Sequential ( ) 단 일 층 네 트 워 크 를 다 시 생 각 해 봅 시 다 . 층 은 모 든 입 력 들 과 모 든 출 력 들 을 연 결 하 는 완 전 연 결 ( fully connected ) 로 구 성 되 어 있 고 , 이 는 행 렬 - 벡 터 곱 으 로 표 현 했 습 니다 . Gluon 의 Dense 인 스 턴 스 는 완 전 연 결 층 ( fully connected layer ) 를 의 미 합 니다 . 한 개 의 스 칼 라 출 력 을 생 성 해 야 하 기 때 문 에 갯 수 를 1 로 정 의 합 니다 . [ 5 ] : net . add ( nn . Dense ( 1 ) ) Gluon 의 특 별 한 점 은 , 각 레 이 어 의 입 력 shape 를 별 도 로 지 정 할 필 요 가 없 다는 것 입 니다 . 예 를 들 면 , 선 형 회 귀 의 입 력 개 수 같 은 것 . 예 를 들 면 net ( X ) 가 수 행 되 면 , 모 델 이 데 이 터 를 보 면 서 각 레 이 어 의 입 력 개 수 를 자 동 으 로 계 산 해 줍 니다 . 이 원 리 에 대 한 내 용 은 “ 딥 러 닝 계 산 ” 장 에 서 자 세 히 다 루 겠 습 니다 . Gluon 의 이 런 디 자인은 모 델 개 발 을 더 쉽 게 만 들 어 줍 니다 . 5 . 3 . 4 모 델 파 라 미 터 들 초 기 화 하 기 net 을 사 용 하 기 전 에 선 형 회 귀 모 델 ( linear regression model ) 의 가 중 치 와 편 향 ( bias ) 같 은 모 델 파 라 미 터 들 을 초 기 화 를 해 야 합 니다 . 이 를 위 해 서 MXNet 의 initializer 모 듈 을 import 합 니다 . 이 모 듈 을 이 용 하 면 다 양 한 방법 으 로 모 델 파 라 미 터 를 초 기 화 할 수 있 습 니다 . import 한 init 은 initializer 의 간 략 한 이 름 입 니다 . init . Normal ( sigma = 0 . 01 ) 을 통 해 서 평 균 이 0 이 고 표 준 편 차 가 0 . 01 인 정 규 분 포 를 따 르 는 난 수 값 들 로 각 가 중 치 파 라 미 터 들 을 초 기 화 합 니다 . 편 향 ( bias ) 는 0 으 로 기 본 설 정 되 게 둡 니다 . [ 6 ] : from mxnet import init net . initialize ( init . Normal ( sigma = 0 . 01 ) ) 130 5 . 딥 러 닝 기 초 위 코 드 는 매 우 직 관 적 으 로 보 이 지 만 , 실 제 로 는 상 당 히 이 상 한 일이 일 어 납 니다 . 이 단 계 에 서 우 리 는 아 직 Gluon 에 게 입 력 이 어 떤 차 원 을 갖 는 지 를 알 려 주 지 않 은 상 태 에 서 파 라 미 터 를 초 기 화 하 고 있 습 니다 . 네 트 워 크 를 어 떻 게 정 의 하 는 지 에 따 라 서 따 라 서 2 가 될 수 도 , 2 , 000 이 될 수 도 있 기 때 문 에 , 이 시 점 에 서 메모 리를 미 리 할 당 할 수 도 없 습 니다 . 파 라 미 터 할 당 및 초 기 화 는 최초 의 데 이 터 가 네 트 워 크 에 입 력 되 는 시 점 으 로 미 뤄 지 는 방 식 으 로 동 작 합 니다 . 이 런 방 식 으 로 모 든 설 정 을 미 리 준 비 한 후 , 사 용 자 는 설 정 들 에 대 해 서 걱 정 하 지 않아 도 됩 니다 . 단 지 유의 해 야 할 점 은 파 라 미 터 들 이 아 직 초 기 화 가 되 지 않았 기 때 문 에 값 을 바 꾸 는 일을 못 한 다는 것 입 니다 . 5 . 3 . 5 손 실 함 수 ( loss function ) 정 의 하 기 Gluon 의 loss 모 듈 은 다 양 한 손 실 함 수 ( loss function ) 를 정 의 하 고 있 습 니다 . loss 모 듈 을 import 할 때 이 름 을 gloss 바 꾸 고 , 제 곱 손 실 ( squared loss ) 을 모 델 의 손 실 함 수 ( loss function ) 로 사 용 하 도 록 합 니다 . [ 7 ] : from mxnet . gluon import loss as gloss loss = gloss . L2Loss ( ) # The squared loss is also known as the L2 norm loss 5 . 3 . 6 최 적 화 알 고 리 즘 정 의 하 기 마 찬 가 지 로 미 니 배 치 에 대 한 확 률 적 경 사 하 강 법 ( stochastic gradient descent ) 을 직 접 구 현 할 필 요 가 없 습 니다 . Gluon 을 import 한 후 , Trainer 인 스 턴 스 를 만 들 면 서 , 학 습 속 도 ( learning rate ) 가 0 . 03 를 갖 는 미 니 배 치 확 률 적 경 사 하 강 법 ( stochastic gradient descent ) 을 최 적 화 ( optimization ) 알 고 리 즘 으 로 설 정 하 면 됩 니다 . 이 최 적 화 알 고 리 즘 은 add 함 수 를 이 용 해 서 net 인 스 턴 스 에 추 가 된 레 이 어 들 의 모 든 파 라 미 터 들 에 적 용 할 것 인 데 , 이 파 라 미 터 들 은 collect _ params 함 수 를 통 해 서 얻 습 니다 . [ 8 ] : from mxnet import gluon trainer = gluon . Trainer ( net . collect _ params ( ) , ' sgd ' , { ' learning _ rate ' : 0 . 03 } ) 5 . 3 . 7 학 습 Gluon 을 이 용 해 서 모 델 을 표 현 하 는 것 이 보 다 간결 하 다는 것 을 눈 치 챘 을 것 입 니다 . 예 를 들 면 , 파 라 미 터 들 을 일일이 선 언 하 지 않았 고 , 손 실 함 수 ( loss function ) 를 직 접 정 의 하 지 않았 고 , 확 률 적 경 사 하 강 법 ( stochastic gradient descent ) 을 직 접 구 현 할 필 요 가 없 습 니다 . 더 욱 복 잡 한 형 태 의 모 델 을 다 루 기 시 작 하 면 , Gluon 의 추 상 화 를 사 용 하 는 것 이 얼 마 나 많 은 이 득 이 되 는 지 알 게 될 것 입 니다 . 기 본 적 인 것 들 이 모 두 설 정 된 후 에 는 학 습 loop 은 앞에 서 직 접 구 현 한 것과 비 슷 합 니다 . 5 . 3 . 선 형 회 귀 의 간결 한 구 현 131 앞 서 구 현 했 던 모 델 학 습 단 계 를 다 시 짚 어 보 면 , 전 체 학 습 데 이 터 ( train _ data ) 를 정 해 진 에 포 크 ( epoch ) 만 큼 반복 해 서 학 습 을 수 행합 니다 . 하 나 의 에 포 크 ( epoch ) 는 다 시 미 니 배 치 로 나 눠 지 는 데 미 니 배 치 는 입 력 과 입 력 에 해 당 하 는 진 실 값 ( ground - truth label ) 들 로 구 성 됩 니다 . 각 미 니 배 치 에 서 는 다 음 단 계 들 이 수 행 됩 니다 . • 네 트 워 크 를 따 라 순 전 파 ( forward pass ) 을 수 행해 서 예 측 값 net ( x ) 을 생 성 하 고 , 손 실 ( loss ) l 을 계 산 합 니다 . • l . backward ( ) 수 행 으 로 역 전 파 ( backward pass ) 를 실 행해 서 그 래 티 언 트 ( gradient ) 들 을 계 산 합 니다 . • SGD 옵 티 마 이 저 ( optimizer ) 를 수 행해 서 모 델 파 라 미 터 들 을 업 데 이 트 합 니다 . ( trainer . step ( ) 을 보 면 , 어 떤 파 라 미 터 를 업 데 이 트 해 야 하 는 지 를 명 시 하 고 있 지 않 고 학 습 할 데 이 터 만 전 달 하 고 있 습 니다 . 이유 는 , trainer 를 초 기 화 할 때 이 미 학 습 할 파 라 미 터 를 명 시 했 기 때 문 입 니 다 . ) 학 습 상 태 를 관 찰 하 기 위 해 서 , 매 에 포 크 ( epoch ) 마 다 전 체 특 성 ( feature ) 들 에 대 해 서 손 실 ( loss ( ) 값 을 계 산 해 서 출 력 합 니다 . [ 9 ] : num _ epochs = 3 for epoch in range ( 1 , num _ epochs + 1 ) : for X , y in data _ iter : with autograd . record ( ) : l = loss ( net ( X ) , y ) l . backward ( ) trainer . step ( batch _ size ) l = loss ( net ( features ) , labels ) print ( ' epoch % d , loss : % f ' % ( epoch , l . mean ( ) . asnumpy ( ) ) ) epoch 1 , loss : 0 . 040472 epoch 2 , loss : 0 . 000162 epoch 3 , loss : 0 . 000050 학 습 된 모 델 파 라 미 터 들 과 실 제 모 델 파 라 미 터 를 비 교 해 봅 니다 . 모 델 의 파 라 미 터 들 의 값 을 확 인 하 는 방법 은 , 우 선 net 인 스 턴 스 로 부 터 층 을 얻어 내 고 , 그 층 의 가 중 치 ( weight ) 변 수 와 편 향 ( bias ) 변 수 를 접 근 하 는 것 입 니다 . 학 습 된 파 라 미 터 들 과 실 제 파 라 미 터 들 이 매 우 비 슷 한 것 을 볼 수 있 습 니다 . [ 10 ] : w = net [ 0 ] . weight . data ( ) print ( ' Error in estimating w ' , true _ w . reshape ( w . shape ) - w ) b = net [ 0 ] . bias . data ( ) print ( ' Error in estimating b ' , true _ b - b ) 132 5 . 딥 러 닝 기 초 Error in estimating w [ [ - 4 . 9829483e - 05 - 3 . 9315224e - 04 ] ] < NDArray 1x2 @ cpu ( 0 ) > Error in estimating b [ 0 . 00032949 ] < NDArray 1 @ cpu ( 0 ) > 5 . 3 . 8 요 약 • Gluon 을 이 용 해 서 모 델 을 매 우 간 단 하 게 구 현 할 수 있 습 니다 . • Gluon 에 서 data 모 듈 은 데 이 터 프 로 세 싱 하 는 도 구 를 , nn 모 듈 을 많 은 종 류 의 뉴 럴 네 트 워 크 레 이 어 들 의 정 의 를 그 리 고 loss 모 듈 은 다 양 한 손 실 함 수 ( loss function ) 를 제 공 합 니다 . • MXNet 의 initializer 모 듈 은 모 델 파 라 미 터 를 초 기 화 하 는 다 양 한 방법 을 제 공 합 니다 . • 모 델 파 라 미 터 의 차 원 ( dimensionality ) 과 저 장 공간 은 할 당 은 실 제 사 용 될 때 까 지 미 뤄 집 니다 . ( 따 라 서 , 초 기 화 되 기 전 에 파 라 미 터 를 접 근 하 는 경 우 에 주 의 해 야 합 니다 . ) 5 . 3 . 9 문 제 1 . l = loss ( output , y ) 를 l = loss ( output , y ) . mean ( ) 로 바 꿀 경 우 , trainer . step ( batch _ size ) 를 trainer . step ( 1 ) 바 꿔 야 합 니다 . 왜 일 까 요 ? 2 . gluon . loss 와 init 모 듈 에 어 떤 손 실 함 수 ( loss function ) 와 초 기 화 방법 들 이 포 함 되 어 있 는 지 MXNet 문 서 를 읽 어 보 세 요 . 손 실 함 수 ( loss function ) 를 Huber’s loss 로 도 바 꿔 보 세 요 . 3 . dense . weight 의 그 래 디 언 트 ( gradient ) 를 어 떻 게 접 근 할 수 있을 까 요 ? 5 . 3 . 10 Scan the QR Code to Discuss 5 . 3 . 선 형 회 귀 의 간결 한 구 현 133 5 . 4 Softmax 회 귀 ( regression ) 앞 두 절 , from scratch 와 using Gluon 을 통 해 서 선 형 회 귀 모 델 을 직 접 구 현 해 보 기 도 했 고 , Gluon 을 이 용 해 서 구 현 해 보 았 습 니다 . Gluon 을 이 용 하 면 파 라 미 터 정 의 나 초 기 화 , 손 실 함 수 ( loss function ) 정 의 , optimizer 구 현 과 같 은 반복 된 일을 자 동 화 할 수 있 었 습 니다 . 회 귀 ( regression ) 는 몇 개 인 지 , 얼 마 인 지 등 에 대 한 답 을 구 할 때 사 용 하 는 도 구 로 , 예 를 들 면 집 가격 이 얼 마 인 지 , 어 떤 야 구 팀 이 몇 번 승 리를 할 것 인 지 등 을 예 측 하 는 데 사 용 할 수 있 는 방법 입 니다 . 다 른 예 로 는 , 환 자 가 몇 일 만 에 퇴 원 할 것 인 지 예 측 하 는 것 도 회 귀 ( regression ) 문 제 입 니다 . 하 지 만 , 현 실 에 서 는 어 떤 카 테 고 리 에 해 당 하 는 지 를 예 측 하 는 문 제 를 더 많 이 접 하 게 됩 니다 . • 메 일이 스 팸 인 지 아 닌 지 • 고객 이 구 독 서 비 스 에 가 입 할 지 아 닐 지 • 이 미 지 에 있 는 객 체 가 무 엇 인 지 ( 원 숭 이 , 강 아 지 , 고 양 이 , 닭 등 ) • 고객 이 어 떤 물 건 을 구 매 할 것 인 지 카 테 고 리 별 로 값 을 할 당 하 거 나 , 어 떤 카 테 고 리 에 속 할 확 률 이 얼 마 나 되 는 지 를 예 측 하 는 것 은 분 류 ( classiﬁcation ) 라 고 부 릅 니다 . 앞 절 들 에 서 살 펴 본 모 델 은 확 률 을 예 측 하 는 문 제 에 적 용 하 기 어 렵 습 니다 . 5 . 4 . 1 분 류 문 제 들 입 력 이 미 지 의 높 이 와 넓 이 가 2 픽 셀 이 고 , 색 은 회 색 인 이 미 지 를 입 력 으 로 다 루 는 간 단 한 문 제 부 터 시 작 해 보 겠 습 니다 . 이 미 지 의 4 개 픽 셀 의 값 은 𝑥 1 , 𝑥 2 , 𝑥 3 , 𝑥 4 으 로 표 현 하 고 , 각 이 미 지 의 실 제 레 이 블 ( label ) 는 “ 고 양 이 ” , “ 닭 ” , “ 강 아 지 ” 중 에 하 나 로 정 의 되 어 있 다 고 하 겠 습 니다 . ( 4 픽 셀 로 구 성 된 이 미 지 가 3 개 동 물 중 에 어 떤 것 인 지 를 구 별 할 수 있 다 고 가 정 합 니다 . ) 이 레 이 블 ( label ) 들 을 표 현 하 는 데 두 가 지 방법 이 있 습 니다 . 첫 번 째 방법 은 { 강 아 지 , 고 양 이 , 닭 } 을 각 각 𝑦 ∈ { 1 , 2 , 3 } ﬀ 으 로 정 의 합 니다 . 이 방법 은 컴 퓨 터 에 정 보 를 저 장 하 는 좋 은 방법 이 지 만 , 이 방법 은 회 귀 문 제 에 적 합합 니다 . 더 구 나 이 숫 자 들 의 순 서 가 분 류 의 문 제 에 서 는 의 미 가 없 습 니다 . 우 리 의 간 단 한 예 제 에 서 는 적 어 도 수 학 적 으 로 는 고 양 이 가 강 아 지 보 다는 닭 과 더 비 슷 하 다는 것 을 의 미 할 수 도 있 게 됩 니다 . 하 지 만 , 실 제 문 제 들 에 서 이 런 비 교 가 잘 되 지 않 습 니다 . 그 렇 기 때 문 에 , 통 계 학 자 들 은 원 - 핫 - 인 코 딩 ( one hot encoding ) 을 통 해 서 표 현 하 는 방법 을 만 들 었 습 니다 . 𝑦 ∈ { ( 1 , 0 , 0 ) , ( 0 , 1 , 0 ) , ( 0 , 0 , 1 ) } 134 5 . 딥 러 닝 기 초 즉 , 𝑦 는 3 차 원 벡 터 로 ( 1 , 0 , 0 ) 은 고 양 이 를 , ( 0 , 1 , 0 ) 은 닭 은 , ( 0 , 0 , 1 ) 은 강 아 지 를 의 미 합 니다 . 5 . 4 . 2 네 트 워 크 아 키 텍 처 여 러 클 래 스 들 에 대 한 분 류를 예 측 할 때 는 카 테 고 리 개 수 와 같 은 수 의 출 력 들 이 필 요 합 니다 . 이 점 이 회 귀 문 제 와 가 장 다 른 점 입 니다 . 4 개 특 성 ( feature ) 들 과 3 개 의 동 물 카 테 고 리 출 력 ( output ) 들 이 있으 니 , 가 중 치 ( 𝑤 ) 는 12 개 의 스 칼 라 들 로 구 성 되 고 편 향 ( bias ) ( 𝑏 ) 는 3 개 의 스 칼 라 로 정 의 됩 니다 . 각 입 력 에 대 해 서 3 개 의 출 력 ( 𝑜 1 , 𝑜 2 , 𝑜 3 ) 는 다 음 과 같 이 계 산 됩 니다 . 𝑜 1 = 𝑥 1 𝑤 11 + 𝑥 2 𝑤 21 + 𝑥 3 𝑤 31 + 𝑥 4 𝑤 41 + 𝑏 1 , 𝑜 2 = 𝑥 1 𝑤 12 + 𝑥 2 𝑤 22 + 𝑥 3 𝑤 32 + 𝑥 4 𝑤 42 + 𝑏 2 , 𝑜 3 = 𝑥 1 𝑤 13 + 𝑥 2 𝑤 23 + 𝑥 3 𝑤 33 + 𝑥 4 𝑤 43 + 𝑏 3 . 아 래 뉴 럴 네 트 워 크 다 이 어 그 램 은 위 연 산 을 표 현 하 고 있 습 니다 . 선 형 회 귀 처 럼 , softmax 회 귀 는 단 일 층 의 뉴 럴 네 트 워 크 로 구 성 됩 니다 . 출 력 ( 𝑜 1 , 𝑜 2 , 𝑜 3 ) 는 모 든 입 력 ( 𝑥 1 , 𝑥 2 , 𝑥 3 , 𝑥 4 ) 값 들 과 연 관 되 서 계 산 되 기 때 문 에 , softmax 회 귀 의 출 력 층 은 완 전 연 결 층 입 니다 . 5 . 4 . 3 Softmax 연 산 위 표 기 법 은 다 소 장 황 해 보 입 니다 . 이 를 벡 터 표 현 으 로 하 면 o = Wx + b 와 같 이 쓰 기 도 간 단 하 고 코 딩 하 기 도 간 단 해 집 니다 . 하 지 만 , 분 류 문 제 는 이 산 ( discrete ) 예 측 결과가 필 요 하 기 때 문 에 , 𝑖 번 째 카 테 고 리 에 대 한 확 신 수 준 ( conﬁdence level ) 을 표 현 하 기 위 해 서 출 력 을 𝑜 𝑖 로 표 현 하 는 간 단 한 방법 을 사 용 합 니다 . 이 렇 게 구 성 하 면 , 어 떤 카 테 고 리 에 속 하 는 지 를 결과 값 들 중 에 가 장 큰 값 의 클 래 스 로 선 택 하 면 되 고 , argmax 𝑖 𝑜 𝑖 로 간 단 히 계 산 할 수 있 습 니다 . 예 를 들 면 , 결과 𝑜 1 , 𝑜 2 , 𝑜 3 가 각 각 0 . 1 , 10 , 0 . 1 이 라 면 , 예 측 된 카 테 고 리 는 2 , 즉 “ 닭 ” 이 됩 니다 . 하 지 만 , 출 력 층 의 값 을 직 접 사 용 하 기 에 는 두 가 지 문 제 가 있 습 니다 . 첫 번 째 는 출 력 값 의 범 위 가 불 확 실 해 서 , 시 각 적 으 로 이 값 들 의 의 미 를 판 단 하 기 어 렵 다는 것 입 니다 . 예 를 들 어 , 이 전 예 에 서 결과 10 은 주 어 진 이 미 지 가 “ 닭 ” 카 테 고 리 에 속 할 것 이 라 고 “ 매 우 확 신 ” 한 다는 것 을 의 미 합 니다 . 왜 냐 하 면 , 5 . 4 . Softmax 회 귀 ( regression ) 135 다 른 두 카 테 고 리 들 의 값 보 다 100 배 크 기 때 문 입 니다 . 만 약에 𝑜 1 = 𝑜 3 = 10 3 이 라 면 , 10 이 라 는 출 력 값 은 이 미 지 가 “ 닭 ” 카 테 고 리 에 속 할 가 능 성 이 매 우 낮 다는 것 의 의 미 하 게 됩 니다 . 두 번 째 문 제 는 실 제 레 이 블 ( label ) 은 이 산 ( discrete ) 값 을 갖 기 때 문 에 , 불 특 정 범 위 를 갖 는 출 력 값과 레 이 블 값 의 오 류를 측 정 하 는 것 이 매 우 어 렵 다는 것 입 니다 . 출 력 값 들 이 확 률 값 으 로 나 오 도 록 해 볼 수 있 겠 지 만 , 새 로 운 데 이 터 가 주 어 졌 을 때 확 률 값 이 0 또 는 양 수 이 고 , 전 체 합 이 1 이 된 다는 것 을 보 장 할 수 는 없 습 니다 . 이 런 이 산 값 ( discrete value ) 예 측 문 제 를 다 루 기 위 해 서 통 계 학 자 들 은 ( softmax ) 로 지 스 틱 회 귀 ( logistic regression ) 이 라 는 분 류 모 델 을 만 들 었 습 니다 . 선 형 회 귀 ( linear regression ) 과 는 다 르 게 , softmax 회 귀 ( regression ) 의 결과 는 모 든 결과값 들 의 합 이 1 이 되 도 록 하 는 비 선 형 성 에 영 향 을 받 고 , 각 결과 값 는 0 또 는 양 수 값 을 갖 습 니다 . 비 선 형 변 환 은 다 음 공 식 으 로 이 뤄 집 니다 . ˆ y = softmax ( o ) where ˆ 𝑦 𝑖 = exp ( 𝑜 𝑖 ) ∑︀ 𝑗 exp ( 𝑜 𝑗 ) 모 든 𝑖 에 대 해 서 0 ≤ ˆ 𝑦 𝑖 ≤ 1 이 고 ˆ 𝑦 1 + ˆ 𝑦 2 + ˆ 𝑦 3 = 1 를 만 족 하 는 것 을 쉽 게 확 인 할 수 있 습 니다 . 따 라 서 , ˆ 𝑦 은 적절 한 확 률 분 포 이 고 , 𝑜 값 은 쉽 게 측 정 할 수 있 는 값 으 로 간 주 할 수 있 습 니다 . 아 래 공 식 은 가 장 가 능 성 있 는 클 래 스 를 찾 아 줍 니다 . ˆ 𝚤 ( o ) = argmax 𝑖 𝑜 𝑖 = argmax 𝑖 ˆ 𝑦 𝑖 즉 , softmax 연 산 은 예 측 하 는 카 테 고 리 의 결과 를 바 꾸 지 않 으 면 서 , 결과 𝑜 에 대 한 적절 한 의 미 를 부 여 해 줍 니다 . 이 것 을 벡 터 표 현 법 으 로 요 약 해 보 면 , get o ( 𝑖 ) = Wx ( 𝑖 ) + b , ˆ y ( 𝑖 ) = softmax ( o ( 𝑖 ) ) 이 됩 니다 . 5 . 4 . 4 미 니 배 치 를 위 한 벡 터 화 연 산 효 율을 더 높 이 기 위 해 서 , 데 이 터 의 미 니 배 치 에 대 한 연 산 을 벡 터 화 합 니다 . 차 원 이 𝑑 이 고 배 치 크 기 가 𝑛 인 데 이 터 들 의 미 니 배 치 X 가 있 고 , 결과 로 𝑞 개 의 카 테 고 리 가 있 다 고 가 정 하 겠 습 니다 . 그 러 면 , 미 니 배 치 feature X 는 R 𝑛 × 𝑑 에 속 하 고 , 가 충 치 들 W 는 R 𝑑 × 𝑞 에 , 편 향 ( bias ) b 는 R 𝑞 에 속 합 니다 . O = XW + b ˆ Y = softmax ( O ) 이 렇 게 정 의 하 면 가 장 많 이 차 지 하 는 연 산 을 가 속 화 할 수 있 습 니다 . 즉 , WX 이 형 렬 - 벡 터 의 곱 에 서 행 렬 - 행 렬 의 곱 으 로 변 환 됩 니다 . softmax 는 결과 O 의 모 든 항 목 에 지 수 함 수 를 적 용 하 고 , 지 수 136 5 . 딥 러 닝 기 초 함 수 들 의 값 의 합 으 로 정 규 화 ( normalize ) 하 는 것 으 로 계 산 됩 니다 . 5 . 4 . 5 손 실 함 수 ( loss function ) 확 률 결과 를 출 력 하 는 방법 을 정 의 했 으 니 , 이 값 이 얼 마 나 정 확 한 지 를 측 정 하 는 값 으 로 변 환 하 는 것 이 필 요 합 니다 . 즉 , 손 실 함 수 ( loss function ) 가 필 요 합 니다 . 선 형 회 귀 에 서 사 용 했 던 것과 동 일 한 개 념 을 사 용 하 는 데 , 이 는 가 능 도 최 대 화 ( likelihood maxmization _ 이 라 고 합 니다 . 로 그 가 능 도 ( Log - Likelihood ) softmax 함 수 는 결과 o 를 여 러 결과 들 에 대 한 확 률 , 𝑝 ( 𝑦 = cat | x ) , 들 의 벡 터 로 변 환 합 니다 . 이 는 , 예 측 된 값 이 얼 마 나 잘 예 측 하 고 있 는 지 를 확 인 하 는 것 으 로 실 제 값과 예 측 결과 에 대 한 비 교 를 할 수 있 습 니다 . 𝑝 ( 𝑌 | 𝑋 ) = 𝑛 ∏︁ 𝑖 = 1 𝑝 ( 𝑦 ( 𝑖 ) | 𝑥 ( 𝑖 ) ) 이 고 , 그 러 므 로 − log 𝑝 ( 𝑌 | 𝑋 ) = 𝑛 ∑︁ 𝑖 = 1 − log 𝑝 ( 𝑦 ( 𝑖 ) | 𝑥 ( 𝑖 ) ) 잘 예 측 하 는 것 은 − log 𝑝 ( 𝑌 | 𝑋 ) 를 최 소 화 하 는 것 을 의 미 합 니다 . 이 를 통 해 서 손 실 함 수 ( loss function ) 를 다 음 과 같 이 정 의 할 수 있 습 니다 . ( 표 기 를 간 단 하 게 하 기 위 해 서 𝑖 는 제 외 했 습 니다 . ) 𝑙 = − log 𝑝 ( 𝑦 | 𝑥 ) = − ∑︁ 𝑗 𝑦 𝑗 log ˆ 𝑦 𝑗 여 기 서 ˆ 𝑦 = softmax ( o ) 이 고 , 벡 터 y 는 해 당 하 는 레 이 블 이 아 닌 위 치 에 는 모 두 0 을 갖 습 니다 . ( 예 를 들 면 ( 1 , 0 , 0 ) ) . 따 라 서 , 모 든 𝑗 에 대 한 합 을 하 면 , 하 나 의 항 목 만 남 게 됩 니다 . 모 든 ˆ 𝑦 𝑗 는 확 률 값 이 기 때 문 에 , 이 에 대 한 로 그 를 적 용 한 ( logarithm ) 값 은 0 보 다 커 질 수 없 습 니다 . 그 결과 , 주 어 진 x 에 대 해 서 y 를 잘 예 측 하 는 경 우 라 면 ( 즉 , 𝑝 ( 𝑦 | 𝑥 ) = 1 ) , 손 실 함 수 ( loss function ) 는 최 소 화 될 것 입 니다 . 5 . 4 . 6 Softmax 와 미 분 ( derivative ) Softmax 와 이 에 대 한 손 실 ( loss ) 는 많 이 사 용 되 기 때 문 에 , 어 떻 게 계 산 되 는 지 자 세 히 살 펴 볼 필 요 가 있 습 니다 . 𝑜 를 손 실 𝑙 의 정 의 에 대 입 하 고 , softmax 의 정 의 를 이 용 하 면 , 다 음 과 같 이 표 현 을 얻 습 니다 . 𝑙 = − ∑︁ 𝑗 𝑦 𝑗 log ˆ 𝑦 𝑗 = ∑︁ 𝑗 𝑦 𝑗 log ∑︁ 𝑘 exp ( 𝑜 𝑘 ) − ∑︁ 𝑗 𝑦 𝑗 𝑜 𝑗 = log ∑︁ 𝑘 exp ( 𝑜 𝑘 ) − ∑︁ 𝑗 𝑦 𝑗 𝑜 𝑗 5 . 4 . Softmax 회 귀 ( regression ) 137 어 떤 일이 일 어 나 는 지 더 살 펴 보 기 위 해 서 , 손 실 함 수 ( loss function ) 를 𝑜 에 대 해 서 미 분 을 해 보 면 아 래 공 식 을 유 도 할 수 있 습 니다 . 𝜕 𝑜 𝑗 𝑙 = exp ( 𝑜 𝑗 ) ∑︀ 𝑘 exp ( 𝑜 𝑘 ) − 𝑦 𝑗 = softmax ( o ) 𝑗 − 𝑦 𝑗 = Pr ( 𝑦 = 𝑗 | 𝑥 ) − 𝑦 𝑗 다 르 게 설 명 해 보 면 , 그 래 디 언 트 ( gradient ) 는 모 델 이 𝑝 ( 𝑦 | 𝑥 ) 확 률 표 현 식 으 로 예 측 한 것과 실 제 값 𝑦 𝑗 의 차 이입 니다 . 이 는 회 귀 문 제 에 서 보 았 던 것과 아 주 비 슷 합 니다 . 회 귀 문 제 에 서 그 래 디 언 트 ( gradient ) 가 관 찰 된 실 제 값 𝑦 와 예 측 된 값 ˆ 𝑦 의 차 이 로 계 산 되 었 습 니다 . 이 는 너 무 우 연 으 로 보 이 는 데 , 사 실 은 그 렇 지 않 습 니다 . exponential 계 열 의 모 델 의 경 우 에 는 , 로 그 가 능 도 ( log - likelihood ) 의 그 래 디 언 트 ( gradient ) 는 정 확 하 게 이 항 목 으 로 주 어 집 니다 . 이 로 인 해 서 그 래 디 언 트 ( gradient ) 를 구 하 는 것 이 실 제 적 용 할 때 매 우 간 단 해 집 니다 . 크 로 스 - 엔 트 로 피 손 실 ( cross - entropy loss ) 자 이 제 는 하 나 의 결과 에 대 한 관 찰 을 하 는 경 우 가 아 니 라 , 결과 들 에 대 한 전 체 분 포 를 다 루 는 경 우 를 생 각 해 봅 시 다 . 𝑦 에 대 한 표 기 를 이 전 과 동 일 하 게 사 용 할 수 있 습 니다 . 오 직 다 른 점 은 ( 0 , 0 , 1 ) 과 같 이 이 진 ( binary ) 값 을 갖 는 것 이 아 니 라 ( 0 . 1 , 0 . 2 , 0 . 7 ) 과 같 이 일 반 적 인 확 률 벡 터 를 사 용 한 다는 것 입 니 다 . 손 실 𝑙 의 정 의 도 동 일 한 수 학 을 사 용 하 지 만 , 이 에 대 한 해 석 은 조 금 더 일 반 적 입 니다 . 레 이 블 들 의 분 포 에 대 한 손 실 의 기 대 값 을 의 미 합 니다 . 𝑙 ( y , ˆ y ) = − ∑︁ 𝑗 𝑦 𝑗 log ˆ 𝑦 𝑗 이 렇 게 정 의 된 손 실 는 크 로 스 - 엔 트 로 피 손 실 ( cross - entropy loss ) 이 라 고 부 릅 니다 . 이 것 은 다 중 클 래 스 분 류 에 가 장 흔히 사 용 되 는 손 실 입 니다 . 이 이 름 에 대 해 서 알아 보 기 위 해 서 는 정 보 이 론 ( information theory ) 에 대 한 설 명 이 필 요 하 며 , 지 금 부 터 설 명 하 겠 습 니다 . 다 음 내 용 은 넘 어 가 도 됩 니다 . 5 . 4 . 7 정 보 이 론 ( Information theory ) 기 초 정 보 이 론 ( information theory ) 는 정 보 ( 또 는 데 이 터 ) 를 가 능 한 한 간결 한 형 식 으 로 인 코 딩 , 디 코 딩 , 전 송 , 및 변 조 하 는 문 제 를 다 룹 니다 . 138 5 . 딥 러 닝 기 초 엔 트 로 피 ( Entropy ) 데 이 터 ( 또 는 난 수 ) 에 몇 개 의 정 보 비 트 들 이 담 겨 있 는 지 가 중 요 한 개 념 입 니다 . 이 는 분 표 𝑝 의 entropy 로 다 음 과 같 이 수 치 화 할 수 있 습 니다 . 𝐻 [ 𝑝 ] = ∑︁ 𝑗 − 𝑝 ( 𝑗 ) log 𝑝 ( 𝑗 ) 정 보 이 론 의 근 본 적 인 이 론 중 에 하 나 로 분 포 𝑝 로 부 터 임의 로 추출 된 데 이 터 를 인 코 드 하 기 위 해 서 는 최 소 𝐻 [ 𝑝 ] 개 의 ’nat’ 이 필 요 하 다는 것 이 있 습 니다 . 여 기 서 ’nat’ 은 비 트 와 동 일 하 나 , 베 이 스 ( base ) 2 가 아 니 라 베 이 스 ( base ) 𝑒 를 이 용 합 니다 . 즉 , 1 nat 은 1 log ( 2 ) ≈ 1 . 44 비 트 이 고 , 𝐻 [ 𝑝 ] / 2 는 종종 이 진 앤 트 로 피 ( binary entropy ) 라 고 불 립 니다 . 조 금 더 이 론 적 으 로 들 어 가 보 겠 습 니다 . 𝑝 ( 1 ) = 12 이 고 , 𝑝 ( 2 ) = 𝑝 ( 3 ) = 14 인 분 포 를 가 정 하 겠 습 니 다 . 이 경 우 , 이 분 포 에 서 추출 한 데 이 터 에 대 한 최 적 의 코 드 를 굉 장 히 쉽 게 설 계 할 수 있 습 니다 . 즉 , 1 의 인 코 딩 은 0 , 2 와 3 에 대 한 인 코 딩 은 각 각 10 , 11 로 정 의 하 면 됩 니다 . 예 상 되 는 비 트 개 수 는 1 . 5 = 0 . 5 * 1 + 0 . 25 * 2 + 0 . 25 * 2 이 고 , 이 숫 자 는 이 진 앤 트 로 피 ( binary entropy ) 𝐻 [ 𝑝 ] / log 2ﬀ 와 같 다는 것 을 쉽 게 확 인 할 수 있 습 니다 . Kullback Leibler Divergence 두 분 포 간 에 차 이 를 측 정 하 는 방법 중 에 하 나 로 앤 트 로 피 ( entropy ) 를 이 용 하 는 방법 이 있 습 니다 . 𝐻 [ 𝑝 ] 는 분 포 𝑝 를 따 르 는 데 이 터 를 인 코 드 하 는 데 필 요 한 최 소 비 트 수 를 의 미 하 기 때 문 에 , 틀 린 분 포 𝑞 에 서 뽑 았 을 때 얼 마 나 잘 인 코 딩 이 되 었 는 지 를 물 어 볼 수 있 습 니다 . 𝑞 를 인 코 딩 하 는 데 추 가 로 필 요 한 비 트 수 는 두 분 표 가 얼 마 나 다 른 지 에 대 한 아 이 디 어 를 제 공 합 니다 . 직 접 계 산 해 보 겠 습 니다 . 분 포 𝑞 에 대 해 최 적 인 코 드 를 이 용 해 서 𝑗 를 인 코 딩 하 기 위 해 서 는 − log 𝑞 ( 𝑗 ) nat 이 필 요 하 고 , 𝑝 ( 𝑗 ) 인 모 든 경 우 에 서 이 를 사 용 하 면 , 다 음 식 을 얻 습 니다 . 𝐷 ( 𝑝 ‖ 𝑞 ) = − ∑︁ 𝑗 𝑝 ( 𝑗 ) log 𝑞 ( 𝑗 ) − 𝐻 [ 𝑝 ] = ∑︁ 𝑗 𝑝 ( 𝑗 ) log 𝑝 ( 𝑗 ) 𝑞 ( 𝑗 ) 𝑞 에 대 해 서 𝐷 ( 𝑝 ‖ 𝑞 ) 를 최 소 화 하 는 것 은 크 로 스 - 엔 트 로 피 손 실 ( cross - entropy loss ) 을 최 소 화 하 는 것 과 같 습 니다 . 이 는 𝑞 에 의 존 하 지 않 는 𝐻 [ 𝑝 ] 를 빼 버 리 면 바 로 얻 을 수 있 습 니다 . 이 를 통 해 서 우 리 는 softmax 회 귀 ( regression ) 는 예 측 된 값 ˆ 𝑦 이 아 니 라 실 제 레 이 블 𝑦 를 봤 을 때 얻 는 놀 라 움 ( 비 트 수 ) 을 최 소 화 하 려 는 것 임을 증 명 했 습 니다 . 5 . 4 . Softmax 회 귀 ( regression ) 139 5 . 4 . 8 모 델 예 측 및 평 가 학 습 된 softmax 회 귀 ( regression ) 모 델 을 사 용 하 면 , 새 로 운 특 성 ( feature ) 가 주 어 졌 을 때 , 각 결과 카 테 고 리 에 속 할 확 률 값 을 예 측 할 수 있 습 니다 . 일 반 적 으 로 는 가 장 크 게 예 측 된 확 률 값 을 갖 는 카 테 고 리를 결과 카 테 고 리 라 고 정 의 합 니다 . 실 제 카 테 고 리 ( label ) 와 일 치 하 는 경 우 에 예 측 이 정 확 하 다 고 합 니다 . 다 음 에 는 모 델 의 성 능 을 평 가 하 는 방법 으 로 accuracy 정 확 도 를 사 용 할 예 정 입 니다 . 이 는 정 확 하 게 예 측 한 개 수 와 전 체 예 측 의 개 수 의 비 율 과 같 습 니다 . 5 . 4 . 9 요 약 • 벡 터 를 확 률 로 변 환 하 는 softmax 연 산 을 알아 봤 습 니다 . • softmax 회 귀 ( regression ) 은 분 류 의 문 제 에 적 용 할 수 있 습 니다 . softmax 연 산 을 이 용 해 서 얻 은 결과 카 테 고 리 의 확 률 분 포 를 이 용 합 니다 . • 크 로 스 엔 트 로 피 ( cross entropy ) 는 두 확 률 분 포 의 차 이 를 측 정 하 는 좋 은 방법 입 니다 . 이 는 주 어 진 모 델 이 데 이 터 를 인 코 드 하 는 데 필 요 한 비 트 수 를 나 타 냅 니다 . 5 . 4 . 10 문 제 1 . Kullback - Leibler divergence 𝐷 ( 𝑝 ‖ 𝑞 ) 가 모 든 음 수 가 분 포 𝑝 , 𝑞 에 대 해 서 음 수 가 아 님 을 증 명 하 세 요 . 힌 트 - Jensen 의 부 등 식 을 이 용 하 세 요 . 예 를 들 여 , − log 𝑥 가 볼 록 함 수 ( convex function ) 이 라 는 사 실 을 사 용 하 세 요 . 2 . log ∑︀ 𝑗 exp ( 𝑜 𝑗 ) 가 𝑜 에 서 볼 록 함 수 ( convex function ) 임을 증 명 하 세 요 . 3 . 지 수승 집 단 과 softmax 를 관 계 를 심 도 있 게 알아 볼 수 있 습 니다 . • softmax 에 대 한 크 로 스 엔 트 로 피 손 실 𝑙 ( 𝑦 , ˆ 𝑦 ) 의 이 차 미 분 을 계 산 하 세 요 . • softmax ( 𝑜 ) 로 주 어 지 는 분 포 의 분 산 을 계 산 하 고 , 위 에 서 계 산 한 이 차 이 분 과 같 음을 증 명 하 세 요 . 4 . 동 일 한 확 률 로 일 어 나 는 3 개 클 래 스 가 있 다 고 가 정 합 니다 . 즉 , 확 률 벡 터 가 ( 13 , 13 , 13 ) 입 니다 . • 이 문 제 를 위 한 이 진 코 드 ( binary code ) 를 설 계 하 고 자 하 면 어 떤 문 제 가 있을 까 요 ? 앤 트 로 피 의 하한 ( lower bound ) 를 필 요 한 비 트 수 로 같게 할 수 있 나 요 ? • 더 좋 은 코 드 를 설 계 할 수 있 나 요 ? 힌 트 - 두 독 립 적 인 관 찰 을 인 코 딩 하 려 면 어 떤 일이 생 기 나 요 ? 𝑛 개 의 관 찰 을 연 관 해 서 인 코 드 하 면 어 떨 까 요 ? 140 5 . 딥 러 닝 기 초 5 . Softmax 는 위 에 서 소 개 된 매 핑 에 대 한 잘 못 된 이 름 입 니다 ( 하 지 만 딥 러 닝 에 서 많 은 사 람 들 이 쓰 고 있 습 니다 . ) 실 제 softmax 는 RealSoftMax ( 𝑎 , 𝑏 ) = log ( exp ( 𝑎 ) + exp ( 𝑏 ) ) 로 정 의 됩 니다 . • RealSoftMax ( 𝑎 , 𝑏 ) > max ( 𝑎 , 𝑏 ) 임을 증 명 하 세 요 . • 𝜆 > 0 일 경 우 , 모 든 𝜆 − 1 RealSoftMax ( 𝜆𝑎 , 𝜆𝑏 ) 에 대 해 서 이 것 이 성 립 함 을 증 명 하 세 요 • 𝜆 → ∞ 이 면 , 𝜆 − 1 RealSoftMax ( 𝜆𝑎 , 𝜆𝑏 ) → max ( 𝑎 , 𝑏 ) 임을 증 명 하 세 요 . • soft - min 은 어 떻 게 생 겼 을 까 요 ? • 이 를 두 개 이 상 의 숫 자 들 로 확 장 해 보 세 요 . 5 . 4 . 11 Scan the QR Code to Discuss 5 . 5 이 미 지 분 류 데 이 터 ( Fashion - MNIST ) softmax 회 귀 ( regression ) 구 현 에 앞 서 적절 한 데 이 터 셋 이 필 요 합 니다 . 시 각 적 으 로 돋 보 이 게 만 들 기 위 해 서 , 분 류 문 제 중 에 서 선 택 해 보 겠 습 니다 . 다 음 장 들 에 서 모 델 정 확 도 의 차 이 를 관 찰 하 거 나 , 비 교 알 고 리 즘 의 연 산 효 율 성 에 대 한 이 야 기 를 할 때 에 도 반복 해 서 사 용 할 예 제 입 니다 . 가 장 흔 한 이 미 지 분 류 데 이 터 셋 은 MNIST 손 글 씨 숫 자 인 식 데 이 터 셋 이 있 습 니다 . 이 데 이 터 셋 은 1990 년 대 에 Lecun , Cortes 와 Burges 에 의 해 서 제 안 되 었 습 니다 . 하 지 만 , 거 의 모 든 모 델 이 MNIST 데 이 터 셋 에 대 해 서 95 % 이 상 의 정 확 도 를 보 여 주 기 때 문 에 , 모 델 들 사 이의 차 이 를 설 명 하 기 에 적 합하 지 않 습 니다 . 알 고 리 즘 들 의 차 이 를 보 다 직 관 적 으 로 보 여 주 기 위 해 서 , 더 복 잡 한 데 이 터 셋 을 사 용 하 겠 습 니다 . 이 데 이 터 셋 은 Fashion - MNIST 라 는 것 으 로 2017 년 에 Xio , Rasul 그 리 고 Vollgraf 가 제 안 했 습 니다 . 5 . 5 . 1 데 이 터 구 하 기 우 선 , 이 절 에 서 필 요 한 패 키 지 와 모 듈 을 import 합 니다 . 5 . 5 . 이 미 지 분 류 데 이 터 ( Fashion - MNIST ) 141 [ 1 ] : import sys sys . path . insert ( 0 , ' . . ' ) % matplotlib inline import d2l from mxnet . gluon import data as gdata import sys import time 다 음으 로 , Gluon 의 data 패 키 지 를 이 용 해 서 이 데 이 터 셋 을 다 운 로 드 합 니다 . 데 이 터 셋 은 처 음 불 렸 을 때 , 인 터 넷 으 로 부 터 자 동 으 로 다 운 로 드됩 니다 . train 파 라 미 터 를 통 해 서 학 습 데 이 터 셋 을 받 을 것 인 지 테 스 트 데 이 터 셋 을 받 을 것 인 지 를 정 할 수 있 습 니다 . 테 스 트 데 이 터 셋 또 는 테 스 팅 데 이 터 셋 은 모 델 의 성 능 을 평 가 할 때 만 쓰 이 고 , 학 습 에 는 사 용 되 지 않 는 데 이 터 입 니다 . [ 2 ] : mnist _ train = gdata . vision . FashionMNIST ( train = True ) mnist _ test = gdata . vision . FashionMNIST ( train = False ) 학 습 데 이 터 셋 과 테 스 트 데 이 터 셋 은 각 카 테 고 리 별 로 각각 6 , 000 개 와 1 , 000 개 의 이 미 지 들 로 구 성 되 어 있 습 니다 . 카 테 고 리 개 수 는 10 개 이 기 에 , 학 습 데 이 터 는 총 60 , 000 개 의 이 미 지 들 로 테 스 팅 셋 은 10 , 000 개 이 미 지 들 을 가 지 고 있 습 니다 . [ 3 ] : len ( mnist _ train ) , len ( mnist _ test ) [ 3 ] : ( 60000 , 10000 ) [ ] 을 이 용 하 면 각 샘 플 을 접 근 할 수 있 습 니다 . 첫 번 째 데 이 터 의 이 미 지 와 레 이 블 을 얻어 보 겠 습 니다 . [ 4 ] : feature , label = mnist _ train [ 0 ] feature 변 수 는 높 이 와 넓 이 가 모 두 28 픽 셀 인 이 미 지 데 이 터 를 가 지 고 있 습 니다 . 각 픽 셀 은 8 - bit 부 호 없 는 정 수 ( uint8 ) 이 고 , 0 부 터 255 사 이의 값 을 갖 습 니다 . 이 는 3 차 원 NDArray 에 저 장 됩 니다 . 마 지 막 차 원 은 채 널 의 개 수 를 의 미 합 니다 . 데 이 터 셋 이 회 색 이 미 지 이 기 때 문 에 , 채 널 의 수 는 1 이 됩 니다 . 간 단 하 게 하 기 위 해 서 , 이 미 지 의 모 양 이 높 이 h , 넓 이 는 w 픽 셀 인 경 우 이 미 지 의 모 양 ( shape ) 을 ℎ × 𝑤 또 는 ( h , w ) 로 표 기 하 도 록 하 겠 습 니다 . [ 5 ] : feature . shape , feature . dtype [ 5 ] : ( ( 28 , 28 , 1 ) , numpy . uint8 ) 각 이 미 지 에 대 한 레 이 블 은 NumPy 의 스 칼 라 로 저 장 되 어 있 고 , 이 는 32 - bit 정 수 형 태 입 니다 . 142 5 . 딥 러 닝 기 초 [ 6 ] : label , type ( label ) , label . dtype [ 6 ] : ( 2 , numpy . int32 , dtype ( ' int32 ' ) ) Fashion - MNIST 에 는 10 개 의 카 테 고 리 가 있 는 데 , 이 들 은 티 셔 츠 , 바 지 , 풀 오 버 , 드 레 스 , 코 드 , 센 달 , 셔 츠 , 스 니 커 , 가 방 , 발 목 부 츠 입 니다 . 숫 자 형 태 의 레 이 블 을 텍 스 트 레 이 블 로 바 꿔 주 는 함 수 를 아 래 와 같 이 정 의 합 니다 . [ 7 ] : # This function has been saved in the d2l package for future use def get _ fashion _ mnist _ labels ( labels ) : text _ labels = [ ' t - shirt ' , ' trouser ' , ' pullover ' , ' dress ' , ' coat ' , ' sandal ' , ' shirt ' , ' sneaker ' , ' bag ' , ' ankle boot ' ] return [ text _ labels [ int ( i ) ] for i in labels ] 아 래 함 수 는 한 줄 에 여 러 이 미 지 와 그 이 미 지 의 레 이 블 을 그 리 는 것 을 정 의 합 니다 . [ 8 ] : # This function has been saved in the d2l package for future use def show _ fashion _ mnist ( images , labels ) : d2l . use _ svg _ display ( ) # Here _ means that we ignore ( not use ) variables _ , figs = d2l . plt . subplots ( 1 , len ( images ) , figsize = ( 12 , 12 ) ) for f , img , lbl in zip ( figs , images , labels ) : f . imshow ( img . reshape ( ( 28 , 28 ) ) . asnumpy ( ) ) f . set _ title ( lbl ) f . axes . get _ xaxis ( ) . set _ visible ( False ) f . axes . get _ yaxis ( ) . set _ visible ( False ) 학 습 데 이 터 셋 의 처 음 9 개 의 샘 플 들 에 대 한 이 미 지 와 텍 스 트 레 이 블 을 살 펴 보 겠 습 니다 . [ 9 ] : X , y = mnist _ train [ 0 : 9 ] show _ fashion _ mnist ( X , get _ fashion _ mnist _ labels ( y ) ) 5 . 5 . 2 미 니 배 치 읽 기 학 습 데 이 터 나 테 스 트 데 이 터 를 읽 는 코 드 를 “ 선 형 회 귀 를 처 음 부 터 구 현 하 기 ” 에 서 처 럼 직 접 작 성 하 지 않 고 DataLoad 를 사 용 하 도 록 하 겠 습 니다 . 데 이 터 로 더 는 매 번 batch _ size 개 수 의 샘 플 을 5 . 5 . 이 미 지 분 류 데 이 터 ( Fashion - MNIST ) 143 갖 는 미 니 배 치 를 읽 습 니다 . 실 제 수 행 을 할 때 , 데 이 터 를 읽 는 것 이 성 능 의 병 목 이 되 는 것 을 볼 수 있 습 니다 . 특 히 , 모 델 이 간 단 하 거 나 컴 퓨 터 가 빠 를 경 우 에 더 욱 그 렇 습 니다 . DataLoader 의 유 용 한 특 징 은 데 이 터 읽 기 속 도 를 빠 르 게 하 기 위 해 서 멀 티 프 로 세 스 들 사 용 할 수 있 다는 것 입 니다 . ( 단 , 현 재 Windows 에 서 는 지 원 되 지 않 습 니다 ) 예 를 들 면 , num _ workers 설 정 을 통 해 서 4 개 의 프 로 세 스 가 데 이 터 를 읽 도 록 만 들 수 있 습 니다 . 추 가 적 으 로 ToTensor 클 래 스 를 이 용 해 서 이 미 지 데 이 터 를 uint8 에 서 32bit 부 동 소 수 점 숫 자 로 변 환 합 니다 . 이 후 , 모 든 숫 자 를 255 로 나 눠 서 모 든 픽 셀 의 값 이 0 과 1 사 이 가 되 도 록 합 니다 . ToTensor 클 래 스 는 이 미 지 채 널 을 마 지 막 차 원 에 서 첫 번 째 차 원 으 로 바 꿔 주 는 기 능 이 있 는 데 , 이 는 다 음 에 소 개 할 컨 볼 루 션 뉴 럴 네 트 워 크 ( convolutional neural network ) 계 산 과 관 련 이 있 습 니다 . 데 이 터 셋 의 transform _ first 함 수 를 이 용 하 면 , ToTensor 의 변 환 을 각 데 이 터 샘 플 ( 이 미 지 와 레 이 블 ) 의 첫 번 째 원 소 인 이 미 지 에 적 용 할 수 있 습 니다 . [ 10 ] : batch _ size = 256 transformer = gdata . vision . transforms . ToTensor ( ) if sys . platform . startswith ( ' win ' ) : # 0 means no additional processes are needed to speed up the reading of # data num _ workers = 0 else : num _ workers = 4 train _ iter = gdata . DataLoader ( mnist _ train . transform _ first ( transformer ) , batch _ size , shuffle = True , num _ workers = num _ workers ) test _ iter = gdata . DataLoader ( mnist _ test . transform _ first ( transformer ) , batch _ size , shuffle = False , num _ workers = num _ workers ) Fashion - MNIST 데 이 터 셋 을 가 지 고 와 서 읽 는 로 직 은 g2l . load _ data _ fashion _ mnist 함 수 내 부 에 구 현 되 어 있 습 니다 . 이 함 수 는 다 음 장 들 에 서 사 용 될 예 정 입 니다 . 이 함 수 는 train _ iter 와 test _ iter 두 변 수 를 리 턴 합 니다 . 이 책 에 서 는 내 용 이 깊 어 짐 에 따 라 이 함 수 를 향 상 시 켜 보 겠 습 니다 . 전 체 구 현 에 대 한 자 세 한 내 용 은 “Deep Convolutional Neural Networks ( AlexNet ) ” 절 에 서 설 명 하 겠 습 니다 . 학 습 데 이 터 를 읽 는 데 걸 리 는 시 간 을 측 정 해 보 겠 습 니다 . [ 11 ] : start = time . time ( ) for X , y in train _ iter : ( continues on next page ) 144 5 . 딥 러 닝 기 초 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) continue ' % . 2f sec ' % ( time . time ( ) - start ) [ 11 ] : ' 1 . 21 sec ' 5 . 5 . 3 요 약 • Fashion - MNIST 는 의 류 분 류 데 이 터 셋 으 로 10 개 의 카 테 고 리 로 분 류 되 어 있 습 니다 . 다 음 장 들 에 서 다 양 한 알 고 리 즘 의 성 능 을 테 스 트 하 는 데 사 용 할 예 정 입 니다 . • 이 미 지 의 모 양 ( shape ) 은 높 이 h 픽 셀 , 넓 이 w 픽 셀 을 이 용 해 서 ℎ × 𝑤 나 ( h , w ) 로 저 장 됩 니다 . • 데 이 터 이 터 레 이 터 ( iterator ) 는 효 율 적 인 성 능 을 위 한 중 요 한 컴 포 넌 트 입 니다 . 가 능 하 면 제 공 되 는 것 들 을 사 용 하 세 요 . 5 . 5 . 4 문 제 1 . batch _ size 를 줄 이 면 ( 예 를 들 면 1 ) 읽 기 성 능 에 영 향 을 미 칠 까 요 ? 2 . Windows 사 용 자 가 아 니 라 면 , num _ workers 을 바 꾸 면 서 읽 기 성 능 이 어 떻 게 영 향 을 받 는 지 실 험해 보 세 요 . 3 . mxnet . gluon . data . vision 에 서 어 떤 데 이 터 셋 들 이 제 공 되 는 지 MXNet 문 서 를 통 해 서 확 인 해 보 세 요 . 4 . mxnet . gluon . data . vision . transforms 에 서 어 떤 변 환 들 이 제 공 되 는 지 MXNet 문 서 를 통 해 서 확 인 해 보 세 요 . 5 . 5 . 5 Scan the QR Code to Discuss 5 . 5 . 이 미 지 분 류 데 이 터 ( Fashion - MNIST ) 145 5 . 6 Softmax 회 귀 ( regression ) 를 처 음 부 터 구 현 하 기 선 형 회 귀 를 직 접 구 현 해 본 것 처 럼 , softmax 회 귀 ( regression ) 도 직 접 구 현 해 보 는 것 이 도 움 이 될 것 입 니다 . 이 후 에 , 같 은 내 용 을 Gluon 을 사 용 해 서 구 현 하 면 서 비 교 를 해 보 겠 습 니다 . 필 요 한 패 키 지 와 모 듈 을 import 하 는 것 으 로 시 작 합 니다 . [ 1 ] : import sys sys . path . insert ( 0 , ' . . ' ) % matplotlib inline import d2l from mxnet import autograd , nd Fashion - MNIST 데 이 터 셋 을 사 용 하 고 , 배 치 크 기 는 256 으 로 하 겠 습 니다 . [ 2 ] : batch _ size = 256 train _ iter , test _ iter = d2l . load _ data _ fashion _ mnist ( batch _ size ) 5 . 6 . 1 모 델 파 라 미 터 초 기 화 하 기 선 형 회 귀 처 럼 샘 플 들 을 벡 터 로 표 현 합 니다 . 각 예 제 가 28 × 28 픽 셀 의 이 미 지 이 기 때 문 에 784 차 원 의 벡 터 에 저 장 합 니다 . 그 리 고 10 개 의 카 테 고 리 가 있으 니 단 일 레 이 어 를 갖 는 네 트 워 크 의 output 차 원 은 10 으 로 정 의 합 니다 . 이 렇 게 하 면 , softmax 회 귀 ( regression ) 의 가 중 치 와 편 향 ( bias ) 파 라 미 터 들 은 각각 크 기 가 784 × 10 , 1 × 10 인 행 렬 이 됩 니다 . 𝑊 를 가 우 시 안 노 이 즈 를 이 용 해 서 초 기 화 합 니다 . [ 3 ] : num _ inputs = 784 num _ outputs = 10 W = nd . random . normal ( scale = 0 . 01 , shape = ( num _ inputs , num _ outputs ) ) b = nd . zeros ( num _ outputs ) 이 전 처 럼 모 델 파 라 미 터 에 그 래 디 언 트 ( gradient ) 를 붙 이 겠 습 니다 . [ 4 ] : W . attach _ grad ( ) b . attach _ grad ( ) 146 5 . 딥 러 닝 기 초 5 . 6 . 2 Softmax softmax 회 귀 ( regression ) 을 정 의 하 기 에 앞 서 , sum 과 같 은 연 산 이 NDArray 의 특 정 차 원 에 서 어 떻 게 동 작 하 는 지 를 보 도 록 하 겠 습 니다 . 행 렬 x 의 같 은 열 ( asix = 0 ) 또 는 같 은 행 ( axis = 1 ) 의 값 들 을 모 두 더 할 수 있 습 니다 . 합 을 수 행한 후 결과 의 차 원 수 를 줄 이 지 않 고 그 대 로 유 지 하 는 것 도 가 능 합 니다 . 이 를 위 해 서 keepdims = True 파 라 미 터 값 을 설 정 하 면 됩 니다 . [ 5 ] : X = nd . array ( [ [ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] ] ) X . sum ( axis = 0 , keepdims = True ) , X . sum ( axis = 1 , keepdims = True ) [ 5 ] : ( [ [ 5 . 7 . 9 . ] ] < NDArray 1x3 @ cpu ( 0 ) > , [ [ 6 . ] [ 15 . ] ] < NDArray 2x1 @ cpu ( 0 ) > ) 자 이 제 우 리 는 softmax 함 수 를 정 의 할 준 비 가 되 었 습 니다 . 우 선 각 항 에 exp 를 적 용 해 서 지 수 값 을 구 하 고 , 정 규 화 상 수 ( normalization constant ) 를 구 하 기 위 해 서 각 행 의 값 들 을 모 두 더 합 니다 . 각 행 을 정 규 화 상 수 ( normalization contatnt ) 로 나 누 고 그 결과 를 리 턴 합 니다 . 코 드 를 보 기 전 에 수식 을 먼 저 보 겠 습 니다 . softmax ( X ) 𝑖𝑗 = exp ( 𝑋 𝑖𝑗 ) ∑︀ 𝑘 exp ( 𝑋 𝑖𝑘 ) 분 모 는 파 티 션 ( partition ) 함 수 라 고 불 리 기 도 합 니다 . 이 이 름 은 파 티 클 의 앙 상 블 에 대 한 분 포 를 모 델 링 하 는 통 계 물 리 에 서 기 원 합 니다 . Naive Bayes 에 서 그 랬 던 것 처 럼 행 렬 의 항 목 들 이 너 무 크 거 나 작 아 서 생 기 는 , 숫 자 가 너 무 커 지 는 오 버 플 로 우 ( overﬂow ) 나 너 무 작 아 지 는 언 더 플 로 우 ( underﬂow ) 를 고 려 하 지 않 고 함 수 를 구 현 하 겠 습 니다 . [ 6 ] : def softmax ( X ) : X _ exp = X . exp ( ) partition = X _ exp . sum ( axis = 1 , keepdims = True ) return X _ exp / partition # The broadcast mechanism is applied here 보 는 것 처 럼 , 임의의 난 수 입 력 에 대 해 서 , 각 항 목 을 0 또 는 양 의 숫 자 로 변 환 합 니다 . 또 한 , 확 률 에 서 요 구 하 는 것 처 럼 각 행 의 합 은 1 이 됩 니다 . [ 7 ] : X = nd . random . normal ( shape = ( 2 , 5 ) ) X _ prob = softmax ( X ) X _ prob , X _ prob . sum ( axis = 1 ) 5 . 6 . Softmax 회 귀 ( regression ) 를 처 음 부 터 구 현 하 기 147 [ 7 ] : ( [ [ 0 . 21324193 0 . 33961776 0 . 1239742 0 . 27106097 0 . 05210521 ] [ 0 . 11462264 0 . 3461234 0 . 19401033 0 . 29583326 0 . 04941036 ] ] < NDArray 2x5 @ cpu ( 0 ) > , [ 1 . 0000001 1 . ] < NDArray 2 @ cpu ( 0 ) > ) 5 . 6 . 3 모 델 Softmax 연 산 을 이 용 해 서 softmax 회 귀 ( regresssion ) 모 델 을 정 의 하 겠 습 니다 . reshape 함 수 를 이 용 해 서 원 본 이 미 지 를 길 이 가 num inputs 인 벡 터 로 변 환 합 니다 . [ 8 ] : def net ( X ) : return softmax ( nd . dot ( X . reshape ( ( - 1 , num _ inputs ) ) , W ) + b ) 5 . 6 . 4 손 실 함 수 ( loss function ) 앞 절 에 서 softmax 회 귀 ( regression ) 에 서 사 용 하 는 크 로 스 - 엔 트 로 피 손 실 함 수 ( cross - entropy loss func - tion ) 를 소 개 했 습 니다 . 이 는 모 든 딥 러 닝 에 서 등 장 하 는 손 실 함 수 ( loss function ) 들 중 에 가 장 일 반 적 인 손 실 함 수 ( loss function ) 입 니다 . 이유 는 회 귀 ( regression ) 문 제 보 다는 분 류 문 제 가 더 많 기 때 문 입 니다 . 크 로 스 - 엔 트 로 피 ( cross - entropy ) 의 계 산 은 레 이 블 ( label ) 의 예 측 된 확 률 값 을 얻 고 , 이 값 에 로 그 ( lo - girithm ) − log 𝑝 ( 𝑦 | 𝑥 ) 을 적 용 하 는 것 임을 기 억 해 두 세 요 . Python 의 for loop 을 사 용 하 지 않 고 ( 비 효 율 적 임 ) , softmax 를 적 용 한 행 렬 에 서 적 당 한 항 목 을 뽑 아 주 는 pick 함 수 를 이 용 하 겠 습 니다 . 3 개 의 카 테 고 리 와 2 개 의 샘 플 의 경 우 아 래 와 같 이 구 할 수 있 습 니다 . [ 9 ] : y _ hat = nd . array ( [ [ 0 . 1 , 0 . 3 , 0 . 6 ] , [ 0 . 3 , 0 . 2 , 0 . 5 ] ] ) y = nd . array ( [ 0 , 2 ] , dtype = ' int32 ' ) nd . pick ( y _ hat , y ) [ 9 ] : [ 0 . 1 0 . 5 ] < NDArray 2 @ cpu ( 0 ) > 이 를 이 용 해 서 크 로 스 - 엔 트 로 피 손 실 함 수 ( cross - entropy loss function ) 를 다 음 과 같 이 정 의 합 니다 . [ 10 ] : def cross _ entropy ( y _ hat , y ) : return - nd . pick ( y _ hat , y ) . log ( ) 148 5 . 딥 러 닝 기 초 5 . 6 . 5 분 류 정 확 도 예 측 된 확 률 분 포 들 y _ hat 주 어 졌 을 때 , 가 장 높 은 예 측 확 률 을 갖 는 것 을 결과 카 테 고 리 로 사 용 합 니다 . 실 제 카 테 고 리 y 와 일 치 하 는 경 우 , 이 예 측 은 ’ 정 확 하 다 ’ 라 고 합 니다 . 분 류 정 확 도 는 정 확 한 예 측 들 과 전 체 예 측 수 의 비 율 로 정 의 됩 니다 . 정 확 도 를 계 산 하 기 위 해 서 accuracy 함 수 를 다 음 과 같 이 정 의 합 니다 . y _ hat . argmax ( axis = 1 ) 는 행 렬 y _ hat 에 서 가 장 큰 원 소 의 인 덱 스 를 리 턴 하 고 , 그 결과 의 shape 은 y 변 수 의 shape 과 동 일 합 니다 . 이 제 해 야 할 일은 두 개가 일 치 하 는 지 확 인 하 는 것 입 니다 . 동 등 연 산 자 = = 는 데 이 터 타 입 에 민 감 하 기 때 문 에 , 두 개 를 동 일 한 타 입으 로 바 꿔 야 합 니다 . float32 로 하 겠 습 니다 . 결과 는 각 항 목 이 거 짓 일 경 우 0 , 참 일 경 우 1 의 값 을 갖 는 NDArray 가 됩 니다 . 이 에 대 한 평 균 을 계 산 하 면 원 하 는 결과 를 얻 을 수 있 습 니다 . [ 11 ] : def accuracy ( y _ hat , y ) : return ( y _ hat . argmax ( axis = 1 ) = = y . astype ( ' float32 ' ) ) . mean ( ) . asscalar ( ) 예 측 된 확 률 분 표 와 레 이 블 ( label ) 에 대 한 변 수 로 pick 함 수 에 서 정 의 했 던 y _ hat 과 y 를 계 속 사 용 하 겠 습 니다 . 첫 번 째 샘 플 의 예 측 카 테 고 리 는 2 ( 첫 번 째 행 에 서 가 장 큰 값 은 0 . 6 이 고 이 값 의 인 덱 스 는 2 ) 임을 확 인 할 수 있 고 , 이 는 실 제 레 이 블 0 과 일 치 하 지 않 습 니다 . 두 번 째 샘 플 의 예 측 카 테 고 리 는 2 ( 두 번 째 행 에 서 가 장 큰 값 이 0 . 5 이 고 이 값 의 인 덱 스 는 2 ) 이 고 , 이 는 실 제 레 이 블 2 와 일 치 합 니다 . 따 라 서 , 이 두 예 들 에 대 한 분 류 정 확 도 는 0 . 5 입 니다 . [ 12 ] : accuracy ( y _ hat , y ) [ 12 ] : 0 . 5 마 찬 가 지 로 , data _ iter 로 주 어 지 는 데 이 터 셋 에 대 한 모 델 net 결과 에 대 한 정 확 도 를 평 가 해 볼 수 있 습 니다 . [ 13 ] : # The function will be gradually improved : the complete implementation will be # discussed in the " Image Augmentation " section def evaluate _ accuracy ( data _ iter , net ) : acc _ sum , n = 0 . 0 , 0 for X , y in data _ iter : y = y . astype ( ' float32 ' ) acc _ sum + = ( net ( X ) . argmax ( axis = 1 ) = = y ) . sum ( ) . asscalar ( ) n + = y . size return acc _ sum / n 이 모 델 net 은 난 수 값 으 로 가 중 치 값 들 이 초 기 화 되 어 있 기 때 문 에 , 정 확 도 는 임의 로 추 측 하 는 것과 유 사 한 0 . 1 ( 10 개 의 클 래 스 ) 로 나 올 것 입 니다 . 5 . 6 . Softmax 회 귀 ( regression ) 를 처 음 부 터 구 현 하 기 149 [ 14 ] : evaluate _ accuracy ( test _ iter , net ) [ 14 ] : 0 . 0925 5 . 6 . 6 모 델 학 습 softmax 회 귀 ( regression ) 학 습 은 선 형 회 귀 학 습 과 아 주 유 사 합 니다 . 모 델 의 손 실 함 수 ( loss function ) 를 최 적 화 하 기 위 해 서 미 니 배 치 확 률 적 경 사 하 강 법 ( stochastic gradient descent ) 를 이 용 합 니다 . 모 델 학 습 에 서 num _ epochs 에 포 크 ( epoch ) 횟 수 와 lr 학 습 속 도 ( learning rate ) 는 모 두 바 꿀 수 있 는 하 이 퍼파 라 미 터 ( hyper - parameter ) 입 니다 . 이 값 을 바 꾸 면 서 , 모 델 의 분 류 정 확 도 를 높 일 수 있 습 니다 . [ 15 ] : num _ epochs , lr = 5 , 0 . 1 # This function has been saved in the d2l package for future use def train _ ch3 ( net , train _ iter , test _ iter , loss , num _ epochs , batch _ size , params = None , lr = None , trainer = None ) : for epoch in range ( num _ epochs ) : train _ l _ sum , train _ acc _ sum , n = 0 . 0 , 0 . 0 , 0 for X , y in train _ iter : with autograd . record ( ) : y _ hat = net ( X ) l = loss ( y _ hat , y ) . sum ( ) l . backward ( ) if trainer is None : d2l . sgd ( params , lr , batch _ size ) else : # This will be illustrated in the next section trainer . step ( batch _ size ) y = y . astype ( ' float32 ' ) train _ l _ sum + = l . asscalar ( ) train _ acc _ sum + = ( y _ hat . argmax ( axis = 1 ) = = y ) . sum ( ) . asscalar ( ) n + = y . size test _ acc = evaluate _ accuracy ( test _ iter , net ) print ( ' epoch % d , loss % . 4f , train acc % . 3f , test acc % . 3f ' % ( epoch + 1 , train _ l _ sum / n , train _ acc _ sum / n , test _ acc ) ) train _ ch3 ( net , train _ iter , test _ iter , cross _ entropy , num _ epochs , batch _ size , [ W , b ] , lr ) epoch 1 , loss 0 . 7902 , train acc 0 . 746 , test acc 0 . 804 epoch 2 , loss 0 . 5756 , train acc 0 . 810 , test acc 0 . 823 epoch 3 , loss 0 . 5292 , train acc 0 . 824 , test acc 0 . 833 ( continues on next page ) 150 5 . 딥 러 닝 기 초 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) epoch 4 , loss 0 . 5053 , train acc 0 . 830 , test acc 0 . 828 epoch 5 , loss 0 . 4901 , train acc 0 . 834 , test acc 0 . 836 5 . 6 . 7 예 측 학 습 이 완 료 되 었 으 면 , 모 델 을 이 용 해 서 이 미 지 를 분 류 해 보 겠 습 니다 . 이 미 지 들 이 주 어 졌 을 때 , 실 제 레 이 블 들 ( 텍 스 트 결과 의 첫 번 째 줄 ) 과 모 델 예 측 ( 텍 스 트 결과 의 두 번 째 줄 ) 을 비 교 해 보 세 요 . [ 16 ] : for X , y in test _ iter : break true _ labels = d2l . get _ fashion _ mnist _ labels ( y . asnumpy ( ) ) pred _ labels = d2l . get _ fashion _ mnist _ labels ( net ( X ) . argmax ( axis = 1 ) . asnumpy ( ) ) titles = [ truelabel + ' \ n ' + predlabel for truelabel , predlabel in zip ( true _ labels , pred _ labels ) ] d2l . show _ fashion _ mnist ( X [ 0 : 9 ] , titles [ 0 : 9 ] ) 5 . 6 . 8 요 약 softmax 회 귀 ( regression ) 을 이 용 해 서 다 중 카 테 고 리 분 류를 할 수 있 습 니다 . 학 습 은 선 형 회 귀 와 비 슷 하 게 수 행 됩 니다 : 데 이 터 를 획 득 하 고 , 읽 고 , 모 델 과 손 실 함 수 ( loss function ) 를 정 의 한 후 , 최 적 화 알 고 리 즘 을 이 용 해 서 모 델 을 학 습시 킵 니다 . 사 실 은 거 의 모 든 딥 러 닝 모 델 의 학 습 절 차 는 이 와 비 슷 합 니다 . 5 . 6 . 9 문 제 1 . 이 절 에 서 softmax 연 산 의 수 학 적 인 정 의 에 따 라 softmax 함 수 를 직 접 정 의 해 봤 습 니다 . 이 경 우 어 떤 문 제 가 발 생 할 수 있을 까 요 ? ( 힌 트 - exp ( 50 ) 의 크 기 를 계 산 해 보 세 요 ) 5 . 6 . Softmax 회 귀 ( regression ) 를 처 음 부 터 구 현 하 기 151 2 . 이 절 의 cross _ entropy 함 수 크 로 스 - 엔 트 로 피 손 실 함 수 ( cross - entropy loss function ) 의 정 의 를 따 라 서 구 현 되 었 습 니다 . 이 구 현 에 어 떤 문 제 가 있을 까 요 ? ( 힌 트 - logarithm 의 도 메 인을 고 려 해 보 세 요 ) 3 . 위 두 가 지 문 제 를 어 떻 게 해 결 할 수 있 는 지 생 각 해 보 세 요 4 . 가 장 유 사 한 레 이 블 을 리 턴 하 는 것 이 항 상 좋 은 아 이 디 어 일 까 요 ? 예 를 들 면 , 의 료 진 단 에 서 그 렇 게 하 겠 나 요 ? 5 . 어 떤 특 성 ( feature ) 들 을 기 반 으 로 다 음 단 어 를 예 측 하 기 위 해 서 softmax 회 귀 ( regression ) 을 사 용 하 기 를 원 한 다 고 가 정 하 겠 습 니다 . 단 어 수 가 많 은 경 우 어 떤 문 제 가 있을 까 요 ? 5 . 6 . 10 Scan the QR Code to Discuss 5 . 7 Softmax 회 귀 ( regression ) 의 간결 한 구 현 우 리 는 이 미 선 형 회 귀 구 현 에 서 Gluon 을 이 용 하 는 것 이 아 주 편 리 하 다는 것 을 확 인 했 습 니다 . 이 제 Gluon 이 분 류 에 어 떻 게 적 용 되 는 지 보 도 록 하 겠 습 니다 . 역 시 몇 가 지 패 키 지 와 모 듈 을 import 하 는 것 으 로 시 작 합 니다 . [ 1 ] : import sys sys . path . insert ( 0 , ' . . ' ) % matplotlib inline import d2l from mxnet import gluon , init from mxnet . gluon import loss as gloss , nn 앞 절 과 동 일 하 게 Fashion - MNIST 데 이 터 셋 과 같 은 배 치 크 기 를 사 용 합 니다 . [ 2 ] : batch _ size = 256 train _ iter , test _ iter = d2l . load _ data _ fashion _ mnist ( batch _ size ) 152 5 . 딥 러 닝 기 초 5 . 7 . 1 모 델 파 라 미 터 초 기 화 하 기 앞 절 에 서 언 급 했 듯 이 softmax regression 의 output 레 이 어 는 fully connected 레 이 어 입 니다 . 따 라 서 , 10 개 의 output 을 갖 는 fully connected 레 이 어 를 추 가 하 고 , weight 를 평 균 이 0 이 고 표 준 편 차 가 0 . 01 인 분 포 에 서 난 수 를 뽑 아 서 초 기 화 를 합 니다 . [ 3 ] : net = nn . Sequential ( ) net . add ( nn . Dense ( 10 ) ) net . initialize ( init . Normal ( sigma = 0 . 01 ) ) 5 . 7 . 2 The Softmax 이 전 예 제 에 서 는 모 델 의 결과 를 계 산 하 고 , 이 결과 에 크 로 스 - 엔 트 로 피 손 실 ( cross - entropy loss ) 을 적 용 했 었 습 니다 . 이 를 위 해 서 - nd . pick ( y _ hat , y ) . log ( ) 를 이 용 했 습 니다 . 수 학 적 으 로 는 이 렇 게 하 는 것 이 매 우 논 리 적 입 니다 . 하 지 만 , 이 미 수 차 례 언 급 하 였 듯 이 연 산 의 관 점 에 서 보 면 어 려 운 문 제 가 될 수 있 습 니다 . ( 예 를 들 면 , Naive Bayes 의 예 또 는 이 전 장의 문 제 들 처 럼 ) . yhat 의 j 번 째 원 소 를 ˆ 𝑦 𝑗 라 고 하 고 , 입 력 인 y _ linear 변 수 의 j 번 째 원 소 를 𝑧 𝑗 라 고 할 때 , softmax 함 수 는 ˆ 𝑦 𝑗 = 𝑒 𝑧𝑗 ∑︀ 𝑛𝑖 = 1 𝑒 𝑧𝑖 ﬀ 를 계 산 합 니다 . 만 약 몇 개 의 𝑧 𝑖 가 매 우 큰 값 을 갖 는다 면 , 𝑒 𝑧 𝑖 값 이 float 변 수 가 표 현 할 수 있 는 값 보 다 훨 씬 커 질 수 있 습 니다 ( overﬂow ) . 따 라 서 , 분 모 ( 또 는 분 자 ) 가 inf 가 돼 서 결과 ˆ 𝑦 𝑗 가 0 , inf 또 는 nan 가 될 수 있 습 니다 . 어 떤 경 우 에 든 지 cross _ entropy 는 잘 정 의 된 값 을 리 턴 하 지 못 할 것 입 니다 . 이 런 문 제 때 문 에 , softmax 함 수 에 서 는 모 든 𝑧 𝑖 에 서 max ( 𝑧 𝑖 ) 를 뺍 니다 . 이 렇 게 𝑧 𝑖 를 이 동 시 키 는 것 이 softmax 의 리 턴 값 을 변 화 시 키 지 않 는다는 사 실 을 확 인 해 볼 수 도 있 습 니다 . 위 에 서 설 명 한 빼 기 와 정 규 화 ( normalization ) 단 계 를 거 친 후 에 도 𝑧 𝑗 가 여 전 히 매 우 작은 음 수 값 이 될 가 능 성 이 있 습 니다 . 따 라 서 , 𝑒 𝑧 𝑗 가 0 과 매 우 근 접 해 지 거 나 유 한한 프 리 시 전 ( ﬁnite precision ) ( 즉 , underﬂow ) 때 문 에 0 으 로 반 올 림 될 수 도 있 습 니다 . 이 렇 게 되 면 , ˆ 𝑦 𝑗 는 0 이 되 고 , log ( ˆ 𝑦 𝑗 ) 는 - inf 가 됩 니다 . 역 전 파 ( backpropagation ) 를 몇 번 거 치 면 , 화 면 에 not - a - number ( nan ) 결과가 출 력 되 는 것 을 보 게 될 것 입 니다 . 이 문 제 에 대 한 해 결 책 은 지 수 함 수 를 계 산 함 에 도 불 구 하 고 , 크 로 스 - 엔 트 로 피 ( cross - entropy ) 함 수 에 서 이 값 에 대 한 로 그 ( log ) 값 을 취 하 도 록 합 니다 . 이 두 연 산 softmax 와 cross _ entropy 를 함 께 사 용 해 서 수 치 안 정 성 문 제 를 해 결 하 고 , 역 전 파 ( backpropagation ) 과 정 에 서 위 문 제 를 만 나 지 않 게 할 수 있 습 니다 . 아 래 공 식 에 서 보 이 는 것 처 럼 , log ( exp ( · ) ) 를 사 용 해 서 𝑒 𝑧 𝑗 를 계 산 하 는 것 을 피하 고 , 𝑧 𝑗 5 . 7 . Softmax 회 귀 ( regression ) 의 간결 한 구 현 153 를 직 접 사 용 할 수 있 습 니다 . log ( ˆ 𝑦 𝑗 ) = log (︂ 𝑒 𝑧 𝑗 ∑︀ 𝑛𝑖 = 1 𝑒 𝑧 𝑖 )︂ = log ( 𝑒 𝑧 𝑗 ) − log (︃ 𝑛 ∑︁ 𝑖 = 1 𝑒 𝑧 𝑖 )︃ = 𝑧 𝑗 − log (︃ 𝑛 ∑︁ 𝑖 = 1 𝑒 𝑧 𝑖 )︃ 모 델 의 확 률 결과 에 대 한 평 가 를 해 야 하 는 경 우 에 사 용 되 는 이 런 전 형 적 인 softmax 함 수 를 간 편 하 게 만 들 고 싶습 니다 . 하 지 만 , softmax 확 률 들 을 새 로 운 손 실 함 수 ( loss function ) 에 대 입 하 는 것 보 다는 , ˆ 𝑦 만 전 달 하 면 , softmax 와 log 값 들 이 모 두 softmax _ cross _ entropy 손 실 함 수 ( loss function ) 에 서 계 산 되 도 록 하 겠 습 니다 . 이 는 log - sum - exp 트 릭 ( see on Wikipedia ) 과 같 이 스 마 트 한 것 을 하 는 것과 같 다 고 볼 수 있 습 니다 . [ 4 ] : loss = gloss . SoftmaxCrossEntropyLoss ( ) 5 . 7 . 3 최 적 화 알 고 리 즘 최 적 화 알 고 리 즘 으 로 학 습 속 도 ( learning rate ) 를 0 . 1 로 하 는 미 니 배 치 확 률 적 경 사 하 강 법 ( stochastic gradient descent ) 를 사 용 하 겠 습 니다 . 이 는 선 형 회 귀 에 서 도 동 일 하 게 사 용 했 는 데 , 옵 티 마 이 져 ( opti - mizer ) 들 의 이 식 성 보 여 줍 니다 . [ 5 ] : trainer = gluon . Trainer ( net . collect _ params ( ) , ' sgd ' , { ' learning _ rate ' : 0 . 1 } ) 5 . 7 . 4 학 습 다 음으 로 , 앞 절 에 서 정 의 된 학 습 함 수 를 이 용 해 서 모 델 을 학 습 시 킵 니다 . [ 6 ] : num _ epochs = 5 d2l . train _ ch3 ( net , train _ iter , test _ iter , loss , num _ epochs , batch _ size , None , None , trainer ) epoch 1 , loss 0 . 7907 , train acc 0 . 744 , test acc 0 . 805 epoch 2 , loss 0 . 5748 , train acc 0 . 810 , test acc 0 . 819 epoch 3 , loss 0 . 5296 , train acc 0 . 823 , test acc 0 . 831 ( continues on next page ) 154 5 . 딥 러 닝 기 초 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) epoch 4 , loss 0 . 5054 , train acc 0 . 831 , test acc 0 . 837 epoch 5 , loss 0 . 4891 , train acc 0 . 835 , test acc 0 . 835 이 전 과 같 이 이 알 고 리 즘 은 매 우 쓸 만 한 정 확 도 인 83 . 7 % 로 수 렴 하 는 데 , 이 전 보 다 훨 씬 더 적 은 코 드 를 가 지 고 가 능 합 니다 . Gluon 은 단 순 하 게 구 현 할 경 우 만 날 수 있 는 수 치 안 정 성 을 넘 어 서 특 별 한 예 방 을 포 함하 고 있 기 에 , 모 델 을 직 접 구 현 할 때 만 날 수 있 는 많 은 일 반 적 인 위 험 을 피할 수 있 게 해 줍 니다 . 5 . 7 . 5 문 제 1 . 배 치 크 기 ( batch size ) , 에 포 크 ( epoch ) , 학 습 속 도 ( learning rate ) 와 같 은 하 이 퍼파 라 미 터 ( hyper - parameter ) 를 변 경 하 면 서 어 떤 결과가 나 오 는 지 보 세 요 . 2 . 학 습 이 진 행 될 때 어 느 정 도 지 나 면 테 스 트 정 확 도 가 감 소 할 까 요 ? 어 떻 게 고 칠 수 있 나 요 ? 5 . 7 . 6 Scan the QR Code to Discuss 5 . 8 다 층 퍼 셉 트 론 ( Multilayer Perceptron ) 이 전 절 들 에 서 옷 이 미 지 를 10 개 의 카 테 고 리 중 에 어 디 에 속 하 는 지 를 예 측 하 는 멀 티 클 래 스 로 지 스 틱 리 그 레 션 ( multiclass logistic regression ) ( 또 는 softmax regression ) 을 구 현 해 봤 습 니다 . 여 기 서 부 터 재 미 있 는 것 들 이 시 작 됩 니다 . 데 이 터 를 다 루 고 , 출 력 값 을 유 효 한 확 률 분 포 로 바 꾸 고 , 적 합한 loss 함 수 를 적 용 하 고 , 파 라 미 터 를 최 적 화 하 는 방법 에 대 해 서 알아 봤 습 니다 . 기 본 적 인 것 들 을 익 혔 으 니 , 이 제 딥 뉴 럴 네 트 워 크 를 포 함하 도 록 우 리 의 도 구 상 자 를 확 장 해 보 겠 습 니다 . 5 . 8 . 다 층 퍼 셉 트 론 ( Multilayer Perceptron ) 155 5 . 8 . 1 은 닉 층 ( hidden layer ) 이 전 의 것 을 기 억 해 보 면 , 단 일 선 형 변 환 ( single linear transformation ) 을 통 해 서 입 력 들 을 바 로 출 력 으 로 매 핑 을 했 고 , 이 는 아 래 과 같 이 표 현 할 수 있 습 니다 . ˆ o = softmax ( Wx + b ) 만 약 레 이 블 ( label ) 들 이 대 략 적 인 선 형 함 수 로 입 력 데 이 터 와 연 관 을 지 을 수 있 다 면 , 이 방법 은 적절 할 수 있 습 니다 . 하 지 만 , 이 선 형 성 은 너 무 강 한 가 정 입 니다 . 선 형 성 은 각 입 력 에 대 해 서 , 입 력 값 이 증 가 하 면 다 른 입 력 값과 는 상 관 없 이 결과값 이 커 지 거 나 작 아 지 는 것 을 의 미 합 니다 . 5 . 8 . 2 하 나 에 서 여 러 개 로 검 정 색 이 나 희 색 이 미 지 을 이 용 해 서 강 아 지 나 고 양 이 를 분 류 하 는 케 이 스 를 생 각 해 봅 시 다 . 각 픽 셀 의 값 을 증 가 시 키 면 강 아 지 라 고 판 별 할 확 률 값 을 높 이 거 나 내 려 가 는 경 우 를 생 각 해 봅 시 다 . 이 는 합 당 하 지 않 습 니다 . 왜 냐 하 면 , 이 렇 게 된 다 면 결 국 강 아 지 는 모 두 검 정 색 이 고 고 양 이 는 모 두 흰 색 이 거 나 그 반 대 라 는 것 을 의 미 하 기 때 문 입 니다 . 이 미 지 에 무 엇 이 있 는 지 알아 내 기 위 해 서 는 입 력 과 출 력 간 의 매 우 복 잡 한 관 계 와 이 를 위 해 서 는 패 턴 이 많 은 특 성 ( feature ) 들 사 이의 관 계 를 통 해 서 특 정 지 어 지 는 가 능 성 을 고 려 해 야 합 니다 . 이 런 경 우 에 는 , 선 형 모 델 의 정 확 도 는 낮 을 것 입 니다 . 우 리 는 한 개 이 상 의 은 닉 층 ( hidden layer ) 을 함 께 사 용 해 서 더 일 반 적 인 함 수 들 을 이 용 한 모 델 을 만 들 수 있 습 니다 . 이 를 구 현 하 는 가 장 쉬 운 방법 은 각 층 위 에 다 른 층 들 을 쌓 는 것 입 니다 . 각 층 의 결과 는 그 위의 층 의 입 력 으 로 연 결 되 는 데 , 이 는 마 지 막 출 력 층 까 지 반복 됩 니다 . 이 런 아 키 텍 쳐 는 일 반 적 으 로 “ 다 층 퍼 셉 트 론 ( multilayer perceptron ) ” 이 라 고 불 립 니다 . 즉 , MLP 는 여 러 층 를 연 속 해 서 쌓 아 올 립 니다 . 예 를 들 면 다 음 과 같 습 니다 . 156 5 . 딥 러 닝 기 초 위 다 층 퍼 셉 트 론 ( multilayer perceptron ) 에 서 는 입 력 이 4 개 , 출 력 이 3 개 , 중 간 의 은 닉 층 ( hidden layer ) 은 5 개 의 은 닉 유 닛 ( hidden unit ) 이 있 습 니다 . 입 력 층 은 어 떤 연 산 을 수 행하 지 않 기 때 문 에 , 이 다 층 퍼 셉 트 론 ( multilayer perceptron ) 은 총 2 개 의 층 ( layer ) 을 갖 습 니다 . 은 닉 층 ( hidden layer ) 의 뉴 런 ( neuron ) 들 은 입 력 층 의 입 력 들 과 모 두 연 결 되 어 있 습 니다 . 출 력 층 ( output layer ) 의 뉴 런 과 은 닉 층 의 뉴 런 들 도 모 두 연 결 되 어 있 습 니다 . 따 라 서 , 이 다 층 퍼 셉 트 론 의 은 닉 층 과 출 력 층 은 모 두 완 전 연 결 층 ( fully connected layer ) 입 니다 . 5 . 8 . 3 선 형 에 서 비 선 형 으 로 다 중 클 래 스 분 류 의 경 우 위 그 림 이 수 학 적 으 로 어 떻 게 정 의 되 는 지 보 겠 습 니다 . h = W 1 x + b 1 o = W 2 h + b 2 ˆ y = softmax ( o ) 위 방법 의 문 제점 은 은 닉 층 을 W = W 2 W 1 과 b = W 2 b 1 + b 2 를 사 용 해 서 단 일 층 퍼 셉 트 론 ( single layer perceptron ) 식 으 로 재 구 성 할 수 있 기 때 문 에 , 다 층 ( mutilayer ) 이 아 닌 단 순 한 단 일 층 퍼 셉 트 론 ( single layer perceptron ) 이 라 는 점 입 니다 . o = W 2 h + b 2 = W 2 ( W 1 x + b 1 ) + b 2 = ( W 2 W 1 ) x + ( W 2 b 1 + b 2 ) = Wx + b 5 . 8 . 다 층 퍼 셉 트 론 ( Multilayer Perceptron ) 157 이 를 해 결 하 는 방법 은 모 든 층 의 다 음 에 max ( 𝑥 , 0 ) 와 같 은 비 선 형 함 수 𝜎 를 추 가 하 는 것 입 니다 . 이 렇 게 하 면 , 층 들 을 합 치 는 것 이 더 이 상 불 가 능 해 집 니다 . 즉 , h = 𝜎 ( W 1 x + b 1 ) o = W 2 h + b 2 ˆ y = softmax ( o ) 이 렇 게 하 면 , 여 러 개 의 은 닉 층 들 을 쌓 는 것 도 가 능 합 니다 . 즉 , h 1 = 𝜎 ( W 1 x + b 1 ) 과 h 2 = 𝜎 ( W 2 h 1 + b 2 ) 를 각각 연 결 해 서 진짜 다 층 퍼 셉 트 론 을 만 들 수 있 습 니다 . 은 닉 뉴 런 들 이 각 입 력 의 값 에 의 존 하 고 있 기 때 문 에 다 층 퍼 셉 트 론 은 입 력 들 사 이의 복 잡 한 상 호 작 용 을 설 명 할 수 있 습 니다 . 예 를 들 면 , 입 력 들 에 대 한 논 리 연 산 과 같 이 임의의 연 산 을 수 행하 는 은 닉 노 드 ( hidden node ) 를 만 드 는 것 도 쉽습 니다 . 다 층 퍼 셉 트 론 이 보 편 적 인 approximator 라 는 것 이 잘 알 려 져 있 습 니다 . 이 것 의 의 미 는 다 음 과 같 습 니다 . 한 개 의 은 닉 층 을 갖 는 다 층 퍼 셉 트 론 이 라 도 , 충 분 히 많 은 노 드 와 , 정 확 한 가 중 치 를 설 정 할 수 있 다 면 모 든 함 수 에 대 한 모 델 을 만 들 수 있 다는 것 입 니다 . 사 실 그 함 수 를 학 습 하 는 것 은 매 우 어 려 운 부분 입 니다 . 그 리 고 , 더 깊 은 ( 또 는 더 넓 은 ) 뉴 럴 네 트 워 크 를 이 용 한 다 면 함 수 를 더 욱 더 간결 하 게 추 정 할 수 도 있 습 니다 . 수 학 적 인 부분 은 이 어 질 장 들 에 서 다 루 겠 고 , 이 장 에 서 는 MLP 를 실 제 로 만 들 어 보 겠 습 니다 . 아 래 예 제 에 서 는 2 개 의 은 닉 층 과 1 개 의 출 력 층 을 갖 는 다 층 퍼 셉 트 론 을 구 현 해 보 겠 습 니다 . 5 . 8 . 4 벡 터 화 와 미 니 배 치 샘 플 들 이 미 니 배 치 로 주 어 지 는 경 우 에 는 벡 터 화 를 통 해 서 구 현 의 효 율 성 을 높 일 수 있 습 니다 . 요 약 하 면 , 벡 터 를 행 렬로 바 꿀 것 입 니다 . X 는 미 니 배 치 의 입 력 행 렬 에 대 한 표 기 입 니다 . 2 개 의 은 닉 층 ( hidden layer ) 를 갖 는 MLP 는 다 음 과 같 이 표 현 됩 니다 . H 1 = 𝜎 ( W 1 X + b 1 ) H 2 = 𝜎 ( W 2 H 1 + b 2 ) O = softmax ( W 3 H 2 + b 3 ) 위 MLP 는 구 현 을 쉽 게 할 수 있 고 , 최 적 화 도 쉽 게 할 수 있 습 니다 . 표 기 법 을 조 금 남 용 해 서 , 비 선 형 ( nonlinearity ) 𝜎 를 정 의 하 고 , 이 를 행 단 위 로 입 력 에 적 용 하 겠 습 니다 . 즉 , 한 번 에 하 나 의 관 찰 또 는 한 번 에 한 좌 표 씩 적 용 합 니다 . 실 제 대 부분 활 성 화 함 수 는 이 렇 게 적 용 합 니다 . ( batch normalization 은 이 규 칙 에 대 한 예외 중 에 하 나 입 니다 . ) 158 5 . 딥 러 닝 기 초 5 . 8 . 5 활 성 화 함 수 ( activation function ) 활 성 화 함 수 ( activation function ) 의 예 를 더 알아 보 겠 습 니다 . 결 국 딥 네 트 워 트 를 동 작 시 키 는 것 은 선 형 ( linear ) 항 목 과 비 선 형 ( nonlinear ) 항 목 들 을 서 로 교 차 시 키 는 것 입 니다 . 구 현 하 기 간 단 하 고 좋 은 효 과 로 유 명 한 ReLU 함 수 가 있 습 니다 . 5 . 8 . 6 ReLU 함 수 ReLU ( rectiﬁed linear unit ) 함 수 는 아 주 간 단 한 비 선 형 변 환 입 니다 . 주 어 진 𝑥 에 대 해 서 , ReLU 함 수 는 다 음 과 같 이 정 의 됩 니다 . ReLU ( 𝑥 ) = max ( 𝑥 , 0 ) . ﬀ ReLU 함 수 는 양 수 만 그 대 로 두 고 , 음 수 는 버 리 고 0 으 로 바 꿔 주 는 역 할 을 합 니다 . 이 해 를 돕 기 위 해 서 도 표 로 그 려 보 겠 습 니다 . 간 단 한 방법 으 로 , 도 표 를 그 리 는 함 수 xyplot 를 정 의 하 겠 습 니다 . [ 1 ] : import sys sys . path . insert ( 0 , ' . . ' ) % matplotlib inline import d2l from mxnet import autograd , nd def xyplot ( x _ vals , y _ vals , name ) : d2l . set _ figsize ( figsize = ( 5 , 2 . 5 ) ) d2l . plt . plot ( x _ vals . asnumpy ( ) , y _ vals . asnumpy ( ) ) d2l . plt . xlabel ( ' x ' ) d2l . plt . ylabel ( name + ' ( x ) ' ) 그 리 고 , 이 를 사 용 해 서 ReLU 함 수 를 NDArray 에 서 제 공 하 는 relu 함 수 를 이 용 해 서 도 식 화 합 니다 . 보 이 는 것 처 럼 활 성 화 함 수 ( activation function ) 은 두 개 의 선 형 함 수 로 보 입 니다 . [ 2 ] : x = nd . arange ( - 8 . 0 , 8 . 0 , 0 . 1 ) x . attach _ grad ( ) with autograd . record ( ) : y = x . relu ( ) xyplot ( x , y , ' relu ' ) 5 . 8 . 다 층 퍼 셉 트 론 ( Multilayer Perceptron ) 159 당 연 히 입 력 이 음 수 일 경 우 ReLU 함 수 의 미 분 값 은 0 이 고 , 입 력 이 양 수 면 ReLU 함 수 의 미 분 값 이 1 이 됩 니다 . 하 지 만 , 입 력 이 0 일 경 우 에 는 ReLU 함 수 는 미 분 이 불 가 능 하 기 때 문 에 , 입 력 이 0 일 때 는 left - hand - side ( LHS ) 미 분 인 0 으 로 선 택 합 니다 . ReLU 함 수 의 미 분 은 다 음 과 같 습 니다 . [ 3 ] : y . backward ( ) xyplot ( x , x . grad , ' grad of relu ' ) ReLU 함 수 는 다 양 한 변 형 이 있 는 데 , 예 를 들 면 He et al . , 2015 . parameterized ReLU ( pReLU ) 가 있 습 니다 . 이 는 ReLU 선 형 항 목 을 추 가 해 서 , 입 력 이 음 수 일 경 우 에 도 정 보 가 전 달 될 수 있 도 록 만 들 고 160 5 . 딥 러 닝 기 초 있 습 니다 . pReLU ( 𝑥 ) = max ( 0 , 𝑥 ) − 𝛼𝑥 ﬀ ReLU 를 사 용 하 는 이유 는 값 이 사 라 지 거 나 그 대 로 전 달 하 게 하 는 식 으 로 미 분 이 아 주 잘 작 동 하 기 때 문 입 니다 . 이 러 한 특 징 으 로 인 해 최 적 화 가 더 잘 되 고 , ( 나 중 에 설 명 할 ) vanishing gradient 문 제 를 줄 여 줍 니다 . 5 . 8 . 7 Sigmoid 함 수 sigmoid 함 수 는 실수 값 을 ( 0 , 1 ) 사 이의 값 으 로 변 환 해 줍 니다 . sigmoid ( 𝑥 ) = 1 1 + exp ( − 𝑥 ) . ﬀ sigmoid 함 수 는 이 전 의 뉴 럴 네 트 워 크 에 서 일 반 적 으 로 사 용 되 었 으 나 , 현 재 는 더 간 단 한 ReLU 함 수 로 대 체 되 었 습 니다 . “Recurrent Neural Network” 장 에 서 는 이 함 수 가 0 과 1 사 이의 값 으 로 변 환 해 주 는 특 징 을 이 용 해 서 뉴 럴 네 트 워 크 에 서 정 보 의 전 달 을 제 어 하 는 데 어 떻 게 사 용 하 는 지 보 겠 습 니다 . Sigmoid 함 수 의 미 분 은 아 래 그 림 과 같 습 니다 . 입 력 이 0 에 가 까 워 지 면 , Sigmoid 함 수 는 선 형 변 환 에 가 까 워 집 니다 . [ 4 ] : with autograd . record ( ) : y = x . sigmoid ( ) xyplot ( x , y , ' sigmoid ' ) 5 . 8 . 다 층 퍼 셉 트 론 ( Multilayer Perceptron ) 161 Sigmoid 함 수 의 미 분 은 아 래 와 같 습 니다 . 𝑑 𝑑𝑥 sigmoid ( 𝑥 ) = exp ( − 𝑥 ) ( 1 + exp ( − 𝑥 ) ) 2 = sigmoid ( 𝑥 ) ( 1 − sigmoid ( 𝑥 ) ) . Sigmoid 함 수 의 미 분 은 아 래 와 같 이 생 겼 습 니다 . 입 력 이 0 이 면 , Sigmoid 함 수 의 미 분 의 최 대 값 인 0 . 25 가 됩 니다 . 입 력 값 이 0 에 서 멀 어 지 면 , Sigmoid 함 수 의 미 분 값 은 0 으 로 접 근 합 니다 . [ 5 ] : y . backward ( ) xyplot ( x , x . grad , ' grad of sigmoid ' ) 5 . 8 . 8 Tanh 함 수 Tanh ( Hyperbolic Tangent ) 함 수 는 값 을 - 1 와 1 사 이 값 으 로 변 환 합 니다 . tanh ( 𝑥 ) = 1 − exp ( − 2 𝑥 ) 1 + exp ( − 2 𝑥 ) . Tanh 함 수 를 도 식 화 를 아 래 와 같 이 할 수 있 습 니다 . 입 력 이 0 에 가 까 워 지 면 , Tanh 함 수 는 선 형 변 환 에 가 까 워 집 니다 . 생 긴 모 양 이 Sigmoid 함 수 와 비 슷 하 지 만 , Tanh 함 수 는 좌 표 의 원 점 을 기 준 으 로 대 칭 인 형 태 를 띕 니다 . [ 6 ] : with autograd . record ( ) : y = x . tanh ( ) xyplot ( x , y , ' tanh ' ) 162 5 . 딥 러 닝 기 초 Tanh 함 수 의 미 분 은 다 음 과 같 습 니다 . 𝑑 𝑑𝑥 tanh ( 𝑥 ) = 1 − tanh 2 ( 𝑥 ) . ﬀ Tanh 함 수 의 미 분 은 아 래 와 같 이 그 려 지 는 데 , 입 력 이 0 과 가 까 우 면 Tanh 함 수 의 미 분 은 최 대 값 이 1 에 근 접 하 게 됩 니다 . 입 력 값 이 0 에 서 멀 어 지 면 , Tanh 함 수 의 미 분 은 0 에 접 근 합 니다 . [ 7 ] : y . backward ( ) xyplot ( x , x . grad , ' grad of tanh ' ) 5 . 8 . 다 층 퍼 셉 트 론 ( Multilayer Perceptron ) 163 요 약 하 면 , 여 러 종 류 의 비 선 형 을 살 펴 봤 고 , 아 주 강 력 한 네 트 워 크 아 키 텍 처 를 만 들 기 위 해 서 어 떻 게 사 용 해 야 하 는 지 도 알아 봤 습 니다 . 부 수 적 으 로 말 하 자 면 , 여 기까 지 의 소 개 한 기 법 을 사 용 하 면 1990 년 대 의 최 첨 단 의 딥 러 닝 을 만 들 수 있 습 니다 . 이 전 과 다 른 점 은 예 전 에 는 C 또 는 Fortran 언어 를 이 용 해 서 수 천 줄 의 코 드 로 만 들 어야 했 던 모 델 을 지 금 은 강 력 한 딥 러 닝 프 레 임 워 크 가 있 어 서 몇 줄 의 코 드 로 만 들 수 있 다는 점 입 니다 . 5 . 8 . 9 요 약 • 다 층 퍼 셉 트 론 은 입 력 과 출 력 층 에 한 개 이 상 의 완 전 연 결 은 닉 층 ( hidden layer ) 를 추 가 하 고 , 각 은 닉 층 ( hidden layer ) 의 결과 에 활 성 화 함 수 ( activation function ) 를 적 용 하 는 것 입 니다 . • 일 반 적 으 로 사 용 되 는 활 성 화 함 수 ( activation function ) 는 ReLU 함 수 , Sigmoid 함 수 , Tanh 함 수 가 있 습 니다 . 5 . 8 . 10 문 제 1 . Tanh , pReLU 활 성 화 함 수 ( activation function ) 의 미 분 을 구 하 시 오 . 2 . ReLU ( 또 는 pReLU ) 만 을 사 용 해 서 만 든 multlayer perceptron 은 연 속 된 piecewise linear function 임을 증 명 하 세 요 . 3 . tanh ( 𝑥 ) + 1 = 2sigmoid ( 2 𝑥 ) 임을 증 명 하 세 요 . 4 . 층 들 사 이 에 비 선 형 없 이 만 든 다 층 퍼 셉 트 론 이 있 다 고 가 정 합 니다 . 𝑑 입 력 차 원 , 𝑑 출 력 차 원 , 그 리 고 다 른 layer 는 𝑑 / 2 차 원 을 찾 는다 고 했 을 때 , 이 네 트 워 크 는 단 층 퍼 셉 트 론 ( single layer perceptron ) 보 다 강 하 지 않 다는 것 을 증 명 하 세 요 . 5 . 한 번 에 하 나 의 미 니 배 치 에 적 용 하 는 비 선 형 이 있 다 고 가 정 합 니다 . 이 렇 게 해 서 발 생 하 는 문 제 가 무 엇 이 있을 까 요 ? 5 . 8 . 11 Scan the QR Code to Discuss 164 5 . 딥 러 닝 기 초 5 . 9 다 층 퍼 셉 트 론 ( multilayer perceptron ) 을 처 음 부 터 구 현 하 기 다 층 퍼 셉 트 론 ( multilayer perceptron , MLP ) 가 어 떻 게 작 동 하 는 지 이 론 적 으 로 배 웠 으 니 , 직 접 구 현 해 보 겠 습 니다 . 우 선 관 련 패 키 지 와 모 듈 을 import 합 니다 . [ 1 ] : import sys sys . path . insert ( 0 , ' . . ' ) % matplotlib inline import d2l from mxnet import nd from mxnet . gluon import loss as gloss 이 예 제 에 서 도 Fashion - MNIST 데 이 터 셋 을 사 용 해 서 , 이 미 지 를 분 류 하 는 데 다 층 퍼 셉 트 론 ( multilayer perceptron ) 을 사 용 하 겠 습 니다 . [ 2 ] : batch _ size = 256 train _ iter , test _ iter = d2l . load _ data _ fashion _ mnist ( batch _ size ) 5 . 9 . 1 모 델 파 라 미 터 초 기 화 하 기 이 데 이 터 셋 은 10 개 의 클 래 스 로 구 분 되 어 있 고 , 각 이 미 지 는 28 × 28 = 784 픽 셀 의 해 상 도 를 가 지 고 있 습 니다 . 따 라 서 , 입 력 은 784 개 이 고 , 출 력 은 10 개가 됩 니다 . 우 리 는 한 개 의 은 닉 층 ( hidden layer ) 을 갖 는 MLP 를 만 들 어 보 겠 는 데 , 이 은 닉 층 ( hidden layer ) 은 256 개 의 은 닉 유 닛 ( hidden unit ) 을 갖 도 록 하 겠 습 니다 . 만 약 원 한 다 면 하 이 퍼파 라 미 터 인 은 닉 유 닛 ( hidden unit ) 개 수 를 다 르 게 설 정 할 수 도 있 습 니다 . 일 반 적 으 로 , 유 닛 ( unit ) 의 개 수 는 메모 리 에 잘 배 치 될 수 있 도 록 2 의 지 수승 의 숫 자 로 선 택 합 니다 . [ 3 ] : num _ inputs , num _ outputs , num _ hiddens = 784 , 10 , 256 W1 = nd . random . normal ( scale = 0 . 01 , shape = ( num _ inputs , num _ hiddens ) ) b1 = nd . zeros ( num _ hiddens ) W2 = nd . random . normal ( scale = 0 . 01 , shape = ( num _ hiddens , num _ outputs ) ) b2 = nd . zeros ( num _ outputs ) params = [ W1 , b1 , W2 , b2 ] for param in params : param . attach _ grad ( ) 5 . 9 . 다 층 퍼 셉 트 론 ( multilayer perceptron ) 을 처 음 부 터 구 현 하 기 165 5 . 9 . 2 활 성 화 함 수 ( activation function ) 여 기 서 ReLU 를 직 접 호 출 하 는 대 신 , ReLU 를 maximum 함 수 를 이 용 해 서 정 의 합 니다 . [ 4 ] : def relu ( X ) : return nd . maximum ( X , 0 ) 5 . 9 . 3 모 델 Softmax 회 귀 ( regression ) 에 서 그 랬 던 것 처 럼 , reshape 함 수 를 이 용 해 서 원 래 이 미 지 를 num _ inputs 크 기 의 벡 터 로 변 환 한 다 음 에 앞에 서 설 명 한 대 로 MLP 를 구 현 합 니다 . [ 5 ] : def net ( X ) : X = X . reshape ( ( - 1 , num _ inputs ) ) H = relu ( nd . dot ( X , W1 ) + b1 ) return nd . dot ( H , W2 ) + b2 5 . 9 . 4 손 실 함 수 ( loss function ) 더 나 은 계 산 의 안 정 성 을 위 해 서 , softmax 계 산 과 크 로 스 - 엔 트 로 피 손 실 ( cross - entropy loss ) 계 산 은 Gluon 함 수 를 이 용 하 겠 습 니다 . 왜 그 런 지 는 앞 절 에 서 이 함 수 의 구 현 에 대 한 복 잡 성 을 이 야 기 했 으 니 참 고 바 랍 니다 . Gluon 함 수 를 이 용 하 면 코 드 를 안 전 하 게 구 현 하 기 위 해 서 신 경 을 써 야 하 는 많 은 세 밀 한 것 들 을 간 단 하 게 피할 수 있 습 니다 . ( 자 세 한 내 용 이 궁 금 하 다 면 소 스 코 드 를 살 펴 보 기 바 랍 니다 . 소 스 코 드 을 보 면 다 른 관 련 함 수 를 구 현 하 는 데 유 용 한 것 들 을 배 울 수 도 있 습 니다 . ) [ 6 ] : loss = gloss . SoftmaxCrossEntropyLoss ( ) 5 . 9 . 5 학 습 다 층 퍼 셉 트 론 ( multilayer perceptron ) 을 학 습시 키 는 단 계 는 softmax 회 귀 ( regression ) 학 습 과 같 습 니다 . g2l 패 키 지 에 서 제 공 하 는 train _ ch3 함 수 를 직 접 호 출 합 니다 . 이 함 수 의 구 현 은 여 기 를 참 고 하 세 요 . 총 에 포 크 ( epoch ) 수 는 10 으 로 학 습 속 도 ( learning rate ) 는 0 . 5 로 설 정 합 니다 . [ 7 ] : num _ epochs , lr = 10 , 0 . 5 d2l . train _ ch3 ( net , train _ iter , test _ iter , loss , num _ epochs , batch _ size , params , lr ) 166 5 . 딥 러 닝 기 초 epoch 1 , loss 0 . 8116 , train acc 0 . 696 , test acc 0 . 818 epoch 2 , loss 0 . 4935 , train acc 0 . 818 , test acc 0 . 842 epoch 3 , loss 0 . 4327 , train acc 0 . 839 , test acc 0 . 859 epoch 4 , loss 0 . 3948 , train acc 0 . 854 , test acc 0 . 860 epoch 5 , loss 0 . 3732 , train acc 0 . 861 , test acc 0 . 855 epoch 6 , loss 0 . 3523 , train acc 0 . 869 , test acc 0 . 867 epoch 7 , loss 0 . 3387 , train acc 0 . 875 , test acc 0 . 873 epoch 8 , loss 0 . 3315 , train acc 0 . 877 , test acc 0 . 878 epoch 9 , loss 0 . 3170 , train acc 0 . 883 , test acc 0 . 884 epoch 10 , loss 0 . 3066 , train acc 0 . 885 , test acc 0 . 879 학 습 이 잘 되 었 는 지 확 인 하 기 위 해 서 , 모 델 을 테 스 트 데 이 터 에 적 용 해 보 겠 습 니다 . 이 모 델 의 성 능 이 궁 금 하 다 면 , 동 일 한 분 류를 수 행하 는 linear 모 델 의 결과 와 비 교 해 보 세 요 . [ 8 ] : for X , y in test _ iter : break true _ labels = d2l . get _ fashion _ mnist _ labels ( y . asnumpy ( ) ) pred _ labels = d2l . get _ fashion _ mnist _ labels ( net ( X ) . argmax ( axis = 1 ) . asnumpy ( ) ) titles = [ truelabel + ' \ n ' + predlabel for truelabel , predlabel in zip ( true _ labels , pred _ labels ) ] d2l . show _ fashion _ mnist ( X [ 0 : 9 ] , titles [ 0 : 9 ] ) 이 전 보 다 조 금 성 능 이 좋 아 보 이 는 것 으 로 보 아 MLP 를 사 용 하 는 것 이 좋 은 것 임을 알 수 있 습 니다 . 5 . 9 . 6 요 약 간 단 한 MLP 는 직 접 구 현 하 는 것 이 아 주 쉽 다는 것 을 확 인 했 습 니다 . 하 지 만 , 많 은 수 의 층 을 갖 는 경 우 에 는 굉 장 히 복 잡 해 질 수 있 습 니다 . ( 예 를 들 면 모 델 파 라 미 터 이 름 을 정 하 는 것 등 ) 5 . 9 . 다 층 퍼 셉 트 론 ( multilayer perceptron ) 을 처 음 부 터 구 현 하 기 167 5 . 9 . 7 문 제 1 . num _ hiddens 하 이 퍼파 라 미 터 를 변 경 해 서 결과가 어 떻 게 영 향 을 받 는 지 확 인 해 보 세 요 . 2 . 새 로 운 은 닉 층 ( hidden layer ) 를 추 가 해 서 어 떤 영 향 을 미 치 는 지 확 인 해 보 세 요 . 3 . 학 습 속 도 ( learning rate ) 를 변 경 하 면 결과가 어 떻 게 되 나 요 ? 4 . 모 든 하 이 퍼파 라 미 터 ( 학 습 속 도 ( learing rate ) , 에 포 크 ( epoch ) 수 , 은 닉 층 ( hidden layer ) 개 수 , 각 층 의 은 닉 유 닛 ( hidden unit ) 개 수 ) 의 조 합 을 통 해 서 얻 을 수 있 는 가 장 좋 은 결과 는 무 엇 인 가 요 ? 5 . 9 . 8 Scan the QR Code to Discuss 5 . 10 다 층 퍼 셉 트 론 ( multilayer perceptron ) 의 간결 한 구 현 다 층 퍼 셉 트 론 ( multilayer perceptron , MLP ) 가 어 떻 게 작 동 하 는 지 이 론 적 으 로 배 웠 으 니 , 이 제 직 접 구 현 해 보 겠 습 니다 . 우 선 관 련 패 키 지 와 모 듈 을 import 합 니다 . [ 1 ] : import sys sys . path . insert ( 0 , ' . . ' ) import d2l from mxnet import gluon , init from mxnet . gluon import loss as gloss , nn 5 . 10 . 1 모 델 Softmax 회 귀 ( regression ) 와 유일 하 게 다 른 점 은 은 닉 층 ( hidden layer ) 으 로 완 전 연 결 층 ( fully connected layer ) 을 추 가 한 다는 점 입 니다 . 이 은 닉 층 ( hidden layer ) 은 256 개 의 은 닉 유 닛 ( hidden unit ) 을 갖고 , 활 성 화 함 수 ( activation function ) 로 ReLU 를 사 용 합 니다 . 168 5 . 딥 러 닝 기 초 [ 2 ] : net = nn . Sequential ( ) net . add ( nn . Dense ( 256 , activation = ' relu ' ) ) net . add ( nn . Dense ( 10 ) ) net . initialize ( init . Normal ( sigma = 0 . 01 ) ) net . add ( ) 를 호 출 할 때 세 밀 하 게 살 펴 봐 야 할 점 이 있 는 데 , 이 함 수 를 이 용 하 면 한 개 이 상 의 층 을 네 트 워 크 에 추 가 할 수 있 다는 것 입 니다 . 즉 , 위 코 드들 로 정 의 된 뉴 럴 네 트 워 크 는 net . add ( nn . Dense ( 256 , activation = ' relu ' ) , nn . Dense ( 10 ) ) 코 드 한 줄 로 정 의 되 어 네 트 워 크 와 동 일 합 니다 . 또 한 , Gluon 은 명 시 되 지 않 은 파 라 미 터 들 을 자 동 으 로 알아 냅 니다 . 예 를 들 면 , 두 번 째 층 은 256 × 10ﬀ 크 기 의 행 렬 이 필 요 한 데 , 이 는 네 트 워 크 가 처 음 실 행 될 때 자 동 으 로 찾 아 내 집 니다 . Softmax 회 귀 ( regression ) 학 습 과 거 의 같 은 절 차 로 데 이 터 를 읽 고 모 델 을 학 습 시 킵 니다 . [ 3 ] : batch _ size = 256 train _ iter , test _ iter = d2l . load _ data _ fashion _ mnist ( batch _ size ) loss = gloss . SoftmaxCrossEntropyLoss ( ) trainer = gluon . Trainer ( net . collect _ params ( ) , ' sgd ' , { ' learning _ rate ' : 0 . 5 } ) num _ epochs = 10 d2l . train _ ch3 ( net , train _ iter , test _ iter , loss , num _ epochs , batch _ size , None , None , trainer ) epoch 1 , loss 0 . 7839 , train acc 0 . 706 , test acc 0 . 818 epoch 2 , loss 0 . 4902 , train acc 0 . 819 , test acc 0 . 847 epoch 3 , loss 0 . 4337 , train acc 0 . 840 , test acc 0 . 855 epoch 4 , loss 0 . 3982 , train acc 0 . 854 , test acc 0 . 864 epoch 5 , loss 0 . 3700 , train acc 0 . 863 , test acc 0 . 873 epoch 6 , loss 0 . 3518 , train acc 0 . 871 , test acc 0 . 876 epoch 7 , loss 0 . 3397 , train acc 0 . 875 , test acc 0 . 879 epoch 8 , loss 0 . 3273 , train acc 0 . 878 , test acc 0 . 881 epoch 9 , loss 0 . 3141 , train acc 0 . 883 , test acc 0 . 878 epoch 10 , loss 0 . 3035 , train acc 0 . 887 , test acc 0 . 879 5 . 10 . 2 문 제 1 . 은 닉 층 ( hidden layer ) 들 을 더 추 가 해 서 결과가 어 떻 게 변 하 는 지 확 인 하 세 요 . 2 . 다 른 활 성 화 함 수 ( activation function ) 를 적 용 해 보 세 요 . 어 떤 것 이 가 장 좋 게 나 오 나 요 ? 3 . 가 중 치 에 대 한 초 기 화 를 다 르 게 해 보 세 요 . 5 . 10 . 다 층 퍼 셉 트 론 ( multilayer perceptron ) 의 간결 한 구 현 169 5 . 10 . 3 Scan the QR Code to Discuss 5 . 11 모 델 선 택 , 언 더 피 팅 ( underﬁtting ) , 오 버 피 팅 ( overﬁtting ) 머 신 러 닝 에 서 우 리 의 목 표 는 일 반 적 인 패 턴 을 발 견 하 는 것 입 니다 . 예 를 들 면 , 유 전 자 표 지 와 성 인 기 의 치 매 발병 간 의 관 련 성 을 배 우 기 를 원 할 수 있 습 니다 . 여 기 서 우 리 의 원 하 는 것 은 전 인 류 에 대 한 위 험 을 평 가 하 기 위 해 서 성 공 적 으 로 적 용 될 수 있 는 패 턴 을 발 견 하 는 것 입 니다 . 하 지 만 , 우 리 가 모 델 을 학 습시 킬 때 , 전 체 인 구 에 대 한 정 보 를 사 용 하 는 것 은 불 가 능 합 니다 . 대 신 , 우 리 는 작은 유 한한 샘 플 만 을 사 용 할 수 있 습 니다 . 대 형 병 원 시스 템 에 서 조 차 도 수십 만 건 의 의 료 기 록 정 도 를 얻 을 수 도 있 습 니다 . 이 렇 게 한 정 된 표 본 의 크 기 를 감 안 하 면 , 보 이 지 않 은 데 이 터 에 존 재 하 는 않 는 가 짜 연 관 성 을 발 견 할 수 있 습 니다 . 극 단 적 인 병 리 학 적 사 례 를 생 각 해 보 겠 습 니다 . 어 떤 사 람 들 이 대 출 금 을 상 환 할 것 인 지 를 예 측 하 는 것 을 배 우 기 를 원 한 다 고 상상 해 보 십시 오 . 대 출 기 관 이 케 이 스 를 조 사 하 기 위 해 서 당 신 을 데 이 터 과 학 자 로 고 용 해 서 , 100 명 의 지 원 자 에 대 한 완 전 한 파 일을 제 공 합 니다 . 100 명 의 지 원 자 중 에 5 명 이 3 년 간 채 무 불 이 행 을 했 었 습 니다 . 제 공 된 파 일 에 는 수 입 , 직 업 , 신 용 점 수 , 취 업 기 간 등 을 포 함한 100 여 개 의 특 성 ( feature ) 들 이 있 습 니다 . 더 불 어 대 출 기 관 과 의 인 터 뷰 비 디 오 를 추 가 로 제 공 한 다 고 상상 해 보 겠 습 니다 . 이 것 들 이 아 주 많 은 데 이 터 처 럼 보 일 수 있 습 니다 ! 엄 청 난 양 의 특 성 ( feature ) 세 트 를 만 든 후 , 채 무 불 이 행 을 한 5 명 이 모 두 인 터 뷰 중 에 파 란 셔 츠 를 입 었 다는 것 을 발 견 했 다 고 가 정 하 겠 습 니다 . 반 면 에 일 반 인 구 의 40 % 만 이 파 란 셔 츠 를 입 었 습 니다 . 여 러 분 이 학 습시 킨 모 델 이 이 신 호 를 포 착 해 서 , 학 습 한 패 턴 의 중 요 한 부분 으 로 사 용 할 가 능 성 이 큽 니다 . 채 무 불 이 행 자 가 더 이 상 파 란 셔 츠 를 입 지 않 을 지 라 도 , 모 든 5 명 의 채 무 불 이 행 자 가 파 란 셔 츠 를 입을 것 이 라 고 관 찰 할 확 률 이 1 % 입 니다 . 수 백 또 는 수 천 개 의 특 성 ( feature ) 들 을 가 지 고 있으 면 서 샘 플 의 크 기 를 작 게 유 지 한 다 면 , 아 주 많 은 가 짜 상 관 관 계 를 관 찰 할 것 입 니다 . 수 조 개 의 학 습 샘 플 이 주 어 진 다 면 , 이 잘 못 된 연 관 성 은 사 라 질 것 입 니다 . 하 지 만 , 실 제 그 렇 게 많 은 데 이 터 를 얻 을 수 있 는 경 우 가 드 뭅 니다 . 모 델 이 실 제 분 포 보 다 학 습 샘 플 들 분 포 에 더 근 접 하 게 학 습 되 는 현 상 을 오 버 피 팅 ( overﬁtting ) 이 라 고 170 5 . 딥 러 닝 기 초 하 며 , 오 버 피 팅 ( overﬁtting ) 을 피하 는 방법 을 정 규 화 ( regularization ) 라 고 합 니다 . 더 정 확 하 게 는 이 전 절 의 Fashion - MNIST 데 이 서셋 에 서 이 현 상 이 나 왔 었 습 니다 . 실 험 중 에 모 델 의 구 조 나 하 이 퍼파 라 미 터 ( hyperparameter ) 들 을 바 꾸 면 , 어 떤 조 합 에 서 는 모 델 이 학 습 데 이 터 와 비 교 해 서 테 스 팅 데 이 터 셋 을 사 용 했 을 때 정 확 하 지 않 게 나 오 는 현 상 을 찾 아 냈 을 수 있 습 니다 . 5 . 11 . 1 학 습 오 류 와 일 반 화 오 류 이 현 상 에 대 해 서 설 명 하 기 전 에 , 학 습 오 류 와 일 반 화 오 류 에 대 해 서 구 분 할 필 요 가 있 습 니다 . Layman 의 용 어에 의 하 면 , 학 습 오 류 는 학 습 데 이 터 셋 을 사 용 했 을 때 나 오 는 오 류 이 고 , 일 반 화 오 류 는 기 본 데 이 터 분 포 에 서 추 가 로 데 이 터 를 뽑 아 낸 가 상 의 스 트 림 에 모 델 을 적 용 할 때 예 상 되 는 오 류를 의 미 합 니다 . 종종 일 반 화 오 류 는 테 스 트 셋 에 모 델 을 적 용 해 서 추 정 합 니다 . 예 를 들 면 , 이 전 에 논 의 한 손 실 함 수 ( loss function ) 들 중 에 선 형회 귀 에 사 용 된 제 곱 손 실 함 수 ( squared loss function ) 나 softmax 회 귀 ( regression ) 에 사 용 된 크 로 스 - 엔 트 로 피 손 실 함 수 ( cross - entropy loss function ) 를 이 용 해 서 학 습 오 류 와 일 반 화 오 류 비 율을 계 산 할 수 있 습 니다 . 다 음 세 가 지 사 고 실 험 이 이 상 황 을 더 잘 설 명 하 는 데 도 움 이 될 것 입 니다 . 학 기 말 시 험 을 준 비 하 는 대 학 생 을 생 각 해 봅 시 다 . 근 면 한 학 생 은 준 비 를 잘 하 고 , 이 전 년 도 의 시 험 문 제 를 통 해 서 본 인의 능 력 을 테 스 트 하 는 등 의 노 력 할 것 입 니다 . 하 지 만 , 이 전 시 험 을 잘 푸 는 것 이 꼭 그 학 생 이 실 제 시 험 을 잘 본 다는 것 을 보 장 하 지 는 못 합 니다 . 예 를 들 면 , 그 학 생 은 시 험 문 제 의 답 을 기 계 적 으 로 학 습 하 면 서 준 비 하 려 고 노 력 할 지 도 모 릅 니다 . 이 렇 게 하 면 그 학 생 은 많 은 것 을 외워 야 합 니다 . 이 렇 게 해 서 그 학 생 은 이 전 시 험 의 답 을 완 벽 하 게 암 기 할 수 도 있 습 니다 . 반 면 에 , 다 른 학 생 은 문 제 에 대 한 특 정 답 이 나 오 게 되 는 이유 를 이 해하 려 고 노 력 하 면 서 준 비 했 습 니다 . 대 부분 의 경 우 에 는 , 후 자의 경 우 에 실 제 시 험 에 서 더 좋 은 성 적 을 냅 니다 . 마 찬 가 지 로 , 질 문 에 대 한 답 을 테 이 블 에 서 조 회 하 는 역 할 을 수 행하 는 모 델 을 생 각 해 보 겠 습 니다 . 입 력 이 이 산 적 ( discrete ) 인 경 우 , 많 은 샘 플 을 보 는 것 을 통 해 서 잘 동 작 할 수 있 습 니다 . 하 지 만 , 이 모 델 은 데 이 터 가 실수 값 이 거 나 우 리 가 원 하 는 것 보 다 더 부 족 한 경 우 실 제 상 황 에 서 는 잘 동 작 하 지 않 을 가 능 성 이 높 습 니다 . 더 군 다 나 , 우 리 는 모 델 을 저 장 할 수 있 는 한 정 된 양 의 메모 리만 가 지 고 있 습 니다 . 마 지 막 으 로 , 간 단 한 분 류 문 제 를 고 려 해 보 겠 습 니다 . 공 정 한 동 전 을 던 져 서 앞 면 이 나 오 면 0 , 뒷 면 이 나 오 면 1 로 분 류 한 레 이 블 을 갖 는 학 습 데 이 터 가 있 습 니다 . 우 리 가 무 엇 을 하 던 지 상 관 없 이 , 일 반 화 오 류 는 항 상 12 입 니다 . 하 지 만 , 학 습 오 류 는 동 전 을 던 지 는 운 에 따 라 서 더 작 아 질 수 있 습 니다 . 예 를 들 어 { 0 , 1 , 1 , 1 , 0 , 1 } 인 학 습 데 이 터 를 사 용 하 는 경 우 에 는 , 1 을 예 측 한 다 면 , 13 의 오 류 가 발 생 하 는 데 , 이 는 실 제 보 다 더 좋 은 값 입 니다 . 데 이 터 양 을 늘 릴 수 록 , 확 률 의 편 차 가 1 2 로 부 터 감 소 하 고 , 학 습 오 류 도 이 에 근 접 하 게 될 것 입 니다 . 그 이유 는 우 리 의 모 델 이 데 이 터 에 오 버 핏 ( overﬁt ) 되 어 있 고 , 데 이 터 의 양 을 늘 리 면 모 든 것 이 평 균 상 태 가 되 기 때 문 입 니다 . 5 . 11 . 모 델 선 택 , 언 더 피 팅 ( underﬁtting ) , 오 버 피 팅 ( overﬁtting ) 171 통 계 적 학 습 이 론 ( statistical learning theory ) 이 현 상 에 대 한 공 식 이 론 이 있 습 니다 . Glivenko 와 Cantelli 는 그 들 의 eponymous theorem 에 서 학 습 오 류 가 일 반 화 오 류 로 수 렴 하 는 비 율을 도 출 했 습 니다 . Vapnik 와 Chervonenkis 는 여 러 논 문 을 통 해 서 이 것 을 더 일 반 적 인 함 수 의 클 래 스 들 로 확 장 했 고 , Statistical Learning Theory 의 기 초 가 되 었 습 니다 . 특 별 히 이 야 기 하 지 않 으 면 , 학 습 데 이 터 셋 과 테 스 트 데 이 터 셋 은 동 일 한 분 포 로 부 터 독 립 적 으 로 추 출 되 었 다 고 가 정 합 니다 . 즉 , 이 분 포 로 부 터 추출 을 할 때 , 추출 들 간 의 어 떤 기 억 도 없 다는 것 을 의 미 합 니다 . 더 나 아 가 , 두 경 우 모 두 동 일 한 분 포 를 사 용 하 는 것 을 의 미 합 니다 . 이 를 위 반 하 는 명 확 한 사 례로 초 등 학 교 학 생 들 의 얼 굴 데 이 터 로 학 습 한 얼 굴 인 식 모 델 을 이 용 해 서 일 반 인 구 에 적 용 하 는 것 을 들 수 있 습 니다 . 초 등 학 교 학 생 들 과 일 반 사 람 들 은 아 주 다 르 게 보 일 것 이 기 때 문 에 잘 동 작 하 지 않 을 가 능 성 이 높 습 니다 . 학 습 을 통 해 서 우 리 는 학 습 데 이 터 에 잘 맞 는 함 수 를 찾 으 려 고 노 력 합 니다 . 만 약 학 습 데 이 터 의 자 세 한 것 에 아 주 잘 적 응 할 정 도 로 유 연 하 다 면 , 지 나 치 게 잘 하 게 될 것 입 니다 . 이 것 이 바 로 우 리 가 피하 려 고 또 는 통 제 하 려 는 것 입 니다 . 대 신 , 우 리 는 일 반 화 오 류를 줄 이 는 모 델 을 찾 는 것 을 원 합 니다 . 딥 러 닝 의 많 은 튜 닝 은 이 런 것 이 일 어 나 지 않 도 록 하 는 데 이 용 됩 니다 . 모 델 복 잡 도 우 리 는 간 단 한 모 델 들 과 많 은 데 이 터 가 있을 경 우 , 일 반 화 오 류 가 학 습 오 류 와 비 슷 해 지 기 를 예 상 합 니다 . 반 면 에 모 델 이 복 잡 하 고 데 이 터 가 적 을 때 는 , 학 습 오 류 는 작 아 지지 만 , 일 반 화 오 류 는 커 질 것 을 예 상 합 니다 . 무 엇 이 정 확 하 게 모 델 의 복 잡 성 을 구 성 하 는 지 는 복 잡 한 문 제 입 니다 . 모 델 이 일 반 화 를 잘 할 수 있을 지 는 많 은 것 들 에 의 해 서 영 향 을 받 습 니다 . 예 를 들 면 , 더 많 은 파 라 미 터 를 갖 는 모 델 이 더 복 잡 하 다 고 여 기 질 수 있 고 , 값 의 범 위 가 더 넓 은 파 라 미 터 를 갖 는 모 델 이 더 복 잡 하 다 고 여 겨 질 수 도 있 습 니다 . 뉴 럴 네 트 워 크 의 경 우 에 는 학 습 을 더 오 래 한 모 델 이 더 복 잡 한 것 이 라 고 생 각 될 수 도 있 고 , 일 찍 학 습 을 종 료 한 모 델 은 덜 복 잡 하 다 고 생 각 될 수 도 있 습 니다 . 다 양 한 모 델 종 류 들 간 의 복 잡 성 을 비 교 하 는 것 은 어 려 운 일 수 있 습 니다 . 예 를 들 면 결 정 트 리 ( decision tree ) 와 뉴 럴 네 트 워 크 의 복 잡 성 을 비 교 하 는 것 은 어 렵 습 니다 . 이 런 경 우 , 간 단 한 경 험 의 법 칙 을 적 용 하 는 것 이 유 용 합 니다 . 통 계 학 자 들 은 임의의 사 실 을 잘 설 명 하 는 모 델 을 복 잡 하 다 고 하 고 , 제 한 적 인 설 명 을 하 는 능 력 을 갖 으 나 데 이 터 를 여 전 히 잘 설 명 하 는 모 델 은 진 실 에 좀 더 가 깝 다 고 합 니다 . 철 학 에 서 이 것 은 포퍼 의 과 학 이 론 의 허 위 진 술 성 ( falsiﬁability ) 과 밀 접 한 관 련 이 있 습 니다 . 어 떤 이 론 이 데 이 터 에 적 합하 고 , 오 류를 입 증 할 수 있 는 특 정 테 스 트 가 있 다 면 , 그 이 론 을 좋 다 고 합 니다 . 모 든 통 계 적 추 정 이 post - hoc 이 기 에 이 는 매 우 중 요 합 니다 . 즉 , 우 리 는 어 떤 사 실 을 관 찰 한 후 에 추 정 을 합 니다 . 따 라 서 , 관 련 오 류 에 취 약 하 게 됩 니다 . 자 , 철 학 에 대 해 서 는 충 분 히 이 야 기 했 으 니 , 더 구 체 적 인 이 슈 를 살 펴 보 겠 습 니다 . 여 러 분 이 이 장 에 대 한 직 관 을 가 질 수 있 도 록 , 모 델 클 래 스 의 일 반 화 에 영 향 을 줄 수 있 는 몇 가 지 요 소 들 에 집중 하 겠 습 니다 . 172 5 . 딥 러 닝 기 초 1 . 튜 닝 이 가 능 한 파 라 미 터 의 개 수 . 자유 도 라 고 불 리 기 도 하 는 튜 닝 가 능 한 파 라 미 터 의 수 가 많 을 경 우 , 모 델 이 오 버 피 팅 ( overﬁtting ) 에 더 취 약 한 경 향 이 있 습 니다 . 2 . 파 라 미 터 에 할 당 된 값 . 가 중 치 들 이 넓 은 범 위의 값 을 갖 을 경 우 , 모 델 은 오 버 피 팅 ( overﬁtting ) 에 더 취 약 할 수 있 습 니다 . 3 . 학 습 예 제 의 개 수 . 모 델 이 간 단 할 지 라 도 학 습 데 이 터 가 한 개 또 는 두 개 인 경 우 에 는 오 버 핏 ( overﬁt ) 되 기 가 아 주 쉽습 니다 . 하 지 만 , 수 백 만 개 의 학 습 데 이 터 를 이 용 해 서 모 델 을 오 버 피 팅 ( overﬁtting ) 시 키 기 위 해 서 는 모 델 이 아 주 복 잡 해 야 합 니다 . 5 . 11 . 2 모 델 선 택 머 신 러 닝 에 서 , 우 리 는 보 통 여 러 후 보 모 델 들 의 성 능 을 평 가 해 서 모 델 을 선 정 합 니다 . 이 과 정 을 모 델 선 택 ( model selection ) 이 라 고 합 니다 . 후 보 모 델 들 은 다 른 하 이 퍼파 라 미 터 ( hyper - parameter ) 들 을 적 용 한 간 단 한 모 델 들 일 수 있 습 니다 . 다 층 퍼 셉 트 론 ( multilayer perceptron ) 을 예 로 들 면 , 우 리 는 은 닉 층 ( hidden layer ) 의 개 수 , 은 닉 유 닛 ( hidden unit ) 의 개 수 , 각 은 닉 층 ( hidden layer ) 의 활 성 화 함 수 ( activation function ) 를 선 택 할 수 있 습 니다 . 효 과 적 인 모 델 을 찾 기 위 해 서 는 모 델 선 택 에 상 당 한 노 력 이 필 요 합 니 다 . 다 음 절 에 서 모 델 선 택 에 종종 사 용 되 는 검 증 데 이 터 셋 ( validation data set ) 에 대 해 서 설 명 하 겠 습 니다 . 검 증 데 이 터 셋 엄 밀 하 게 이 야 기 하 면 , 테 스 트 셋 은 모 든 하 이 퍼파 라 미 터 ( hyper - parameter ) 들 과 모 델 파 라 미 터 들 이 선 택 된 후 에 만 사 용 되 어야 합 니다 . 특 히 , 테 스 트 데 이 터 셋 은 하 이 퍼파 라 미 터 ( hyper - parameter ) 선 택 과 같 은 모 델 선 택 과 정 에 서 사 용 되 서 는 안 됩 니다 . 모 델 선 택 과 정 에 서 학 습 데 이 터 에 만 의 존 해 서 도 안 됩 니다 . 그 이유 는 일 반 화 오 류 율이 학 습 오 류 율 로 예 상 될 수 없 기 때 문 입 니다 . 이 를 고 려 해 서 , 학 습 데 이 터 와 테 스 트 데 이 터 이 외 의 데 이 터 를 확 보 해 서 모 델 선 택 에 사 용 할 수 있 습 니다 . 이 렇 게 확 보 한 데 이 터 는 검 증 데 이 터 셋 ( validation data set ) 또 는 검 증 셋 ( validation set ) 이 라 고 합 니다 . 예 를 들 면 , 학 습 데 이 터 에 서 임의 로 선 택 한 일 부 의 데 이 터 를 검 증 셋 으 로 사 용 하 고 , 나 머 지 를 실 제 학 습 데 이 터 로 사 용 할 수 있 습 니다 . 하 지 만 , 실 제 응 용 의 경 우 에 는 테 스 트 데 이 터 를 구 하 기 어 렵 기 때 문 에 한 번 사 용 하 고 버 리 는 경 우 가 드 뭅 니다 . 따 라 서 , 실 제 의 경 우 에 는 검 증 데 이 터 와 테 스 트 데 이 터 셋 의 구 분 이 명 확 하 지 않 을 수 도 있 습 니다 . 명 시 적 으 로 별 도 로 언 급 하 지 않 는 경 우 이 책 에 서 실 험 으 로 사 용 하 는 테 스 트 데 이 터 셋 은 검 증 데 이 터 셋 이 라 고 하 고 , 실 험 결과 의 테 스 트 정 확 도 는 검 증 정 확 도 를 의 미 하 겠 습 니다 . 좋 은 소 식 은 검 증 셋 에 아 주 많 은 데 이 터 가 필 요 하 지 않 다는 것 입 니다 . 우 리 의 예 측 의 불 명 확 성 은 𝑂 ( 𝑛 − 12 ) 오 더 로 보 여 질 수 있 습 니다 . 5 . 11 . 모 델 선 택 , 언 더 피 팅 ( underﬁtting ) , 오 버 피 팅 ( overﬁtting ) 173 𝐾 - 겹 교 차 검 증 ( 𝐾 ﬀ - Fold Cross - Validation ) 학 습 데 이 터 가 충 분 하 지 않 을 경 우 검 증 데 이 터 를 많 이 확 보 하 는 것 은 과 하 다 고 간 주 됩 니다 . 왜 냐 하 면 , 검 증 데 이 터 는 모 델 학 습 에 어 떤 역 할 도 할 수 없 기 때 문 입 니다 . 이 에 대 한 해 결 책 으 로 𝐾 - 겹 교 차 검 증 ( 𝐾 - Fold Cross - Validation ) 방법 이 있 습 니다 . 에 𝐾 - 겹 교 차 검 증 ( 𝐾 - Fold Cross - Validation ) 에 서 는 원 래 학 습 데 이 터 를 겹 치 지 않 는 K 개 의 부분 데 이 터 셋 으 로 나 누 고 , 모 델 학 습 과 검 증 을 𝐾 번 반복 합 니다 . 검 증 이 수 행 될 때 마 다 𝐾 − 1 개 의 부분 데 이 터 셋 으 로 학 습 을 하 고 , 1 개 의 부분 데 이 터 셋 으 로 검 증 을 수 행합 니다 . 모 델 을 검 증 하 는 데 사 용 하 는 부분 데 이 터 셋 은 계 속 바 꾸 면 서 𝐾 번 학 습 과 검 증 을 수 행하 게 됩 니다 . 마 지 막 으 로 , 𝐾 번 의 학 습 과 검 증 오 류 에 대 한 평 균 을 각각 구 합 니다 . 5 . 11 . 3 언 더 피 팅 ( underﬁtting ) 과 오 버 피 팅 ( overﬁtting ) 다 음으 로 는 모 델 학 습 을 진 행하 면 서 만 나 게 되 는 일 반 적 인 두 가 지 문 제 에 대 해 서 살 펴 보 겠 습 니다 . 첫 번 째 문 제 는 모 델 이 너 무 간 단 하 기 때 문 에 학 습 오 류 가 줄 어 들 지 않 는 것 입 니다 . 이 현 상 을 언 더 피 팅 ( underﬁtting ) 이 라 고 합 니다 . 두 번 째 문 제 는 앞에 서 이 야 기 했 던 오 버 피 팅 ( overﬁtting ) 으 로 , 이 는 학 습 오 류 가 테 스 트 데 이 터 셋 에 대 한 오 류 보 다 아 주 작은 경 우 입 니다 . 실 제 로 이 두 문 제 는 가 능 한 경 우 항 상 동 시 에 해 결 이 되 어야 합 니다 . 이 두 문 제 의 원 인은 여 러 요 소 들 이 있 지 만 , 여 기 서 는 두 가 지 요 소 에 대 해 서 집중 하 겠 습 니다 . 이 두 가 지 는 모 델 복 잡 성 과 학 습 데 이 터 셋 의 크 기 입 니다 . 모 델 복 잡 도 이 이 슈 를 설 명 하 기 위 해 서 다 항 식 을 예 로 들 겠 습 니다 . 스 칼 라 데 이 터 특 성 ( feature ) 𝑥 와 이 에 대 한 스 칼 라 레 이 블 ( label ) 𝑦 로 구 성 된 학 습 데 이 터 가 주 어 진 경 우 , 𝑦 를 추 정 하 는 𝑑 차 원 다 항 식 을 찾 는다 고 하 겠 습 니다 . ˆ 𝑦 = 𝑑 ∑︁ 𝑖 = 0 𝑥 𝑖 𝑤 𝑖 여 기 서 𝑤 𝑖 는 모 델 의 가 중 치 파 라 미 터 를 의 미 하 고 , 편 향 ( bias ) 은 𝑥 0 = 1 이 기 때 문 에 𝑤 0 이 됩 니다 . 간 단 하 게 하 기 위 해 서 , 선 형 회 귀 ( linear regression ) 의 경 우와 같 이 제 곱 손 실 ( squared loss ) 을 사 용 하 겠 습 니다 . ( 사 실 𝑑 = 1 인 경 우 이 모 델 은 선 형 회 귀 ( linear regression ) 입 니다 . ) 고 차 원 의 다 항 함 수 는 저 차 원 의 다 항 함 수 보 다 더 복 잡 합 니다 . 이유 는 차 원 이 더 높 아 지 면 , 더 많 은 파 라 미 터 를 갖게 되 고 , 모 델 함 수 의 선 택 범 위 가 더 넓 어 지 기 때 문 입 니다 . 따 라 서 , 같 은 학 습 데 이 터 셋 을 사 용 하 는 경 우 , 더 높 은 차 원 의 다 항 함 수 에 대 한 학 습 오 류 는 그 보 다 낮 은 차 원 의 다 항 함 수 의 오 류 보 다 낮 을 것 입 니다 . 이 를 염 두 하 면 , 학 습 데 이 터 셋 이 고 정 되 어 있을 때 모 델 의 복 잡 도 와 오 류 의 일 반 적 인 상 관관 계 는 아 래 그 림 으 로 설 명 됩 니다 . 데 이 터 에 비 해 서 모 델 이 너 무 간 단 하 면 , 언 더 피 팅 174 5 . 딥 러 닝 기 초 ( underﬁtting ) 이 발 생 하 고 , 모 델 을 너 무 복 잡 하 게 선 택 하 면 오 버 피 팅 ( overﬁtting ) 이 발 생 합 니다 . 데 이 터 에 대 한 모 델 을 적절 한 복 잡 성 을 선 택 하 는 것 이 오 버 피 팅 ( overﬁtting ) 과 언 더 피 팅 ( underﬁtting ) 문 제 를 피하 는 방법 중 에 하 나 입 니다 . 데 이 터 셋 의 크 기 다 른 원 인은 학 습 데 이 터 의 양 입 니다 . 일 반 적 으 로 학 습 데 이 터 셋 의 샘 플 개 수 가 충 분 하 지 않 은 경 우 , 특 히 모 델 의 파 라 미 터 개 수 보 다 적 은 수 의 샘 플 을 사 용 하 는 경 우 , 오 버 피 팅 ( overﬁtting ) 이 쉽 게 발 생 합 니다 . 학 습 데 이 터 의 양 을 늘 리 면 , 일 반 화 오 류 는 일 반 적 으 로 줄 어 듭 니다 . 즉 , 더 많 은 데 이 터 는 모 델 학 습 에 나 쁜 영 향 을 미 치 지 않 다는 것 을 의 미 합 니다 . 더 나 아 가 서 , 이 는 충 분 한 데 이 터 가 있 다 면 , 일 반 적 으 로 많 은 층 들 을 갖 는 복 잡 한 모 델 을 사 용 해 야 한 다는 것 을 의 미 합 니다 . 5 . 11 . 4 다 항 식 회 귀 ( Polynomial Regression ) 데 이 터 를 이 용 해 서 다 항 식 을 학 습시 켜 보 면 서 이 것 이 어 떻 게 동 작 하 는 지 보 겠 습 니다 . 우 선 몇 가 지 모 듈 을 import 합 니다 . [ 1 ] : import sys sys . path . insert ( 0 , ' . . ' ) % matplotlib inline import d2l ( continues on next page ) 5 . 11 . 모 델 선 택 , 언 더 피 팅 ( underﬁtting ) , 오 버 피 팅 ( overﬁtting ) 175 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) from mxnet import autograd , gluon , nd from mxnet . gluon import data as gdata , loss as gloss , nn 데 이 터 셋 생 성 하 기 우 선 데 이 터 가 필 요 합 니다 . 주 어 진 𝑥 에 대 해 서 , 다 음 3 차 원 방 정 식 을 사 용 해 서 학 습 데 이 터 와 테 스 트 데 이 터 로 사 용 할 레 이 블 ( label ) 만 들 겠 습 니다 . 𝑦 = 5 + 1 . 2 𝑥 − 3 . 4 𝑥 2 2 ! + 5 . 6 𝑥 3 3 ! + 𝜖 where 𝜖 ∼ 𝒩 ( 0 , 0 . 1 ) 노 이 즈 항 인 𝜖 은 평 균 이 0 이 고 표 준 편 차 가 0 . 1 인 정 규 분 포 를 따 릅 니다 . 학 습 과 테 스 트 데 이 터 셋 의 샘 플 의 개 수 는 각각 100 개 , 1000 개 로 하 겠 습 니다 . [ 2 ] : maxdegree = 20 # Maximum degree of the polynomial n _ train , n _ test = 100 , 1000 # Training and test data set sizes true _ w = nd . zeros ( maxdegree ) # Allocate lots of empty space true _ w [ 0 : 4 ] = nd . array ( [ 5 , 1 . 2 , - 3 . 4 , 5 . 6 ] ) features = nd . random . normal ( shape = ( n _ train + n _ test , 1 ) ) features = nd . random . shuffle ( features ) poly _ features = nd . power ( features , nd . arange ( maxdegree ) . reshape ( ( 1 , - 1 ) ) ) poly _ features = poly _ features / ( nd . gamma ( nd . arange ( maxdegree ) + 1 ) . reshape ( ( 1 , - 1 ) ) ) labels = nd . dot ( poly _ features , true _ w ) labels + = nd . random . normal ( scale = 0 . 1 , shape = labels . shape ) 최 적 화 를 위 해 서 , 그 래 디 언 트 ( gradient ) , 손 실 ( loss ) 등 이 큰 값 을 갖 는 것 을 피해 야 합 니다 . poly _ features 에 저 장 되 는 단 항 들 이 𝑥 𝑖 에 서 1 𝑖 ! 𝑥 𝑖 로 스 케 일을 조정 하 는 이유입 니다 . 이 렇 게 하 면 큰 차 원 𝑖 의 값 들 이 아 주 커 지 는 것 을 방 지 할 수 있 습 니다 . 팩 토 리 얼 은 Gluon 의 Gamma 함 수 를 이 용 해 서 구 현 합 니다 . ( 𝑛 ! = Γ ( 𝑛 + 1 ) ) 생 성 된 데 이 터 셋 에 서 처 음 두 샘 플 을 확 인 해 봅 니다 . 값 1 도 기 술 적 으 로 보 면 하 나 의 특 성 ( feature ) 으 로 , bias 에 대 한 상 수 특 성 ( feature ) 라 고 볼 수 있 습 니다 . [ 3 ] : features [ : 2 ] , poly _ features [ : 2 ] , labels [ : 2 ] [ 3 ] : ( [ [ - 0 . 5095612 ] [ 0 . 34202248 ] ] ( continues on next page ) 176 5 . 딥 러 닝 기 초 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) < NDArray 2x1 @ cpu ( 0 ) > , [ [ 1 . 00000000e + 00 - 5 . 09561181e - 01 1 . 29826277e - 01 - 2 . 20514797e - 02 2 . 80914456e - 03 - 2 . 86286173e - 04 2 . 43133891e - 05 - 1 . 76987987e - 06 1 . 12732764e - 07 - 6 . 38269260e - 09 3 . 25237282e - 10 - 1 . 50662070e - 11 6 . 39762874e - 13 - 2 . 50767950e - 14 9 . 12725858e - 16 - 3 . 10059752e - 17 9 . 87465261e - 19 - 2 . 95984643e - 20 8 . 37901598e - 22 - 2 . 24716890e - 23 ] [ 1 . 00000000e + 00 3 . 42022479e - 01 5 . 84896803e - 02 6 . 66826218e - 03 5 . 70173899e - 04 3 . 90024616e - 05 2 . 22328640e - 06 1 . 08630559e - 07 4 . 64426186e - 09 1 . 76493528e - 10 6 . 03647601e - 12 1 . 87691835e - 13 5 . 34956872e - 15 1 . 40744067e - 16 3 . 43840286e - 18 7 . 84007342e - 20 1 . 67592618e - 21 3 . 37178999e - 23 6 . 40682210e - 25 1 . 15330363e - 26 ] ] < NDArray 2x20 @ cpu ( 0 ) > , [ 3 . 8980482 5 . 3267784 ] < NDArray 2 @ cpu ( 0 ) > ) 모 델 정 의 , 학 습 , 그 리 고 테 스 트 우 선 그 래 프 를 그 리 는 함 수 semilogy 를 정 의 합 니다 . 𝑦 축 은 로 그 ( logarithm ) 단 위 를 사 용 합 니다 . [ 4 ] : # This function has been saved in the d2l package for future use def semilogy ( x _ vals , y _ vals , x _ label , y _ label , x2 _ vals = None , y2 _ vals = None , legend = None , figsize = ( 3 . 5 , 2 . 5 ) ) : d2l . set _ figsize ( figsize ) d2l . plt . xlabel ( x _ label ) d2l . plt . ylabel ( y _ label ) d2l . plt . semilogy ( x _ vals , y _ vals ) if x2 _ vals and y2 _ vals : d2l . plt . semilogy ( x2 _ vals , y2 _ vals , linestyle = ' : ' ) d2l . plt . legend ( legend ) 선 형 회 귀 ( Linear regression ) 와 비 슷 하 게 , 다 항 함 수 학 습 에 제 곱 손 실 함 수 ( squared loss function ) 를 이 용 하 겠 습 니다 . 생 성 된 데 이 터 를 이 용 해 서 여 러 복 잡 도 를 갖 는 모 델 들 을 학 습시 킬 것 이 기 때 문 에 , 모 델 정 의 를 fit _ and _ plot 함 수 에 전 달 하 도 록 하 겠 습 니다 . 다 항 함 수 에 대 한 학 습 과 테 스 트 단 계 는 softmax 회 귀 ( regression ) 와 비 슷 합 니다 . [ 5 ] : num _ epochs , loss = 200 , gloss . L2Loss ( ) def fit _ and _ plot ( train _ features , test _ features , train _ labels , test _ labels ) : net = nn . Sequential ( ) # Switch off the bias since we already catered for it in the polynomial ( continues on next page ) 5 . 11 . 모 델 선 택 , 언 더 피 팅 ( underﬁtting ) , 오 버 피 팅 ( overﬁtting ) 177 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) # features net . add ( nn . Dense ( 1 , use _ bias = False ) ) net . initialize ( ) batch _ size = min ( 10 , train _ labels . shape [ 0 ] ) train _ iter = gdata . DataLoader ( gdata . ArrayDataset ( train _ features , train _ labels ) , batch _ size , shuffle = True ) trainer = gluon . Trainer ( net . collect _ params ( ) , ' sgd ' , { ' learning _ rate ' : 0 . 01 } ) train _ ls , test _ ls = [ ] , [ ] for _ in range ( num _ epochs ) : for X , y in train _ iter : with autograd . record ( ) : l = loss ( net ( X ) , y ) l . backward ( ) trainer . step ( batch _ size ) train _ ls . append ( loss ( net ( train _ features ) , train _ labels ) . mean ( ) . asscalar ( ) ) test _ ls . append ( loss ( net ( test _ features ) , test _ labels ) . mean ( ) . asscalar ( ) ) print ( ' final epoch : train loss ' , train _ ls [ - 1 ] , ' test loss ' , test _ ls [ - 1 ] ) semilogy ( range ( 1 , num _ epochs + 1 ) , train _ ls , ' epochs ' , ' loss ' , range ( 1 , num _ epochs + 1 ) , test _ ls , [ ' train ' , ' test ' ] ) print ( ' weight : ' , net [ 0 ] . weight . data ( ) . asnumpy ( ) ) 3 차 다 항 함 수 피 팅 ( Third - order Polynomial Function Fitting ( Normal ) ) 우 선 , 데 이 터 를 생 성 한 것과 같 은 3 차 원 다 항함 수 를 이 용 해 보 겠 습 니다 . 테 스 트 데 이 터 를 이 용 해 서 얻 은 모 델 의 오 류 는 낮 게 나 오 는 것 이 보 여 집 니다 . 학 습 된 모 델 파 라 미 터 역 시 실 제 값 𝑤 = [ 5 , 1 . 2 , − 3 . 4 , 5 . 6 ] 과 비 슷 합 니다 . [ 6 ] : num _ epochs = 1000 # Pick the first four dimensions , i . e . 1 , x , x ^ 2 , x ^ 3 from the polynomial # features fit _ and _ plot ( poly _ features [ : n _ train , 0 : 4 ] , poly _ features [ n _ train : , 0 : 4 ] , labels [ : n _ train ] , labels [ n _ train : ] ) final epoch : train loss 0 . 0045331586 test loss 0 . 005139432 weight : [ [ 4 . 9952927 1 . 2209384 - 3 . 3926976 5 . 561578 ] ] 178 5 . 딥 러 닝 기 초 선 형 함 수 피 팅 ( 언 더 피 팅 , underﬁtting ) 선 형 함 수 의 경 우 를 보 겠 습 니다 . 초 기 에 포 크 ( epoch ) 를 수 행하 면 서 학 습 오 류 가 감 소 한 후 로 더 이 상 모 델 학 습 의 오 류 가 감 소 하 지 않 는 것 은 자 연 스 러 운 현 상 입 니다 . 마 지 막 epoch 까 지 마 친 후 에 도 학 습 오 류 는 여 전 히 높 습 니다 . 선 형 모 델 은 비 선 형 모 델 ( 3 차 다 항 함 수 ) 로 만 들 어 진 데 이 터 셋 에 대 해 서 언 더 피 팅 ( underﬁtting ) 에 민 감 합 니다 . [ 7 ] : num _ epochs = 1000 # Pick the first four dimensions , i . e . 1 , x from the polynomial features fit _ and _ plot ( poly _ features [ : n _ train , 0 : 3 ] , poly _ features [ n _ train : , 0 : 3 ] , labels [ : n _ train ] , labels [ n _ train : ] ) final epoch : train loss 0 . 86323565 test loss 3 . 34696 weight : [ [ 4 . 9270425 3 . 3825665 - 2 . 672712 ] ] 5 . 11 . 모 델 선 택 , 언 더 피 팅 ( underﬁtting ) , 오 버 피 팅 ( overﬁtting ) 179 부 족 한 학 습 ( 오 버 피 팅 , overﬁtting ) 실 제 상 황 에 서 , 데 이 터 를 생 성 할 때 사 용 한 것과 같 은 3 차 다 항 함 수 를 이 용 할 경 우 에 도 학 습 을 충 분 히 오 래 하 지 않 은 경 우 에 는 오 버 핏 ( overﬁt ) 이 쉽 게 발 생 할 수 있 습 니다 . 아 주 높 은 차 원 의 다 항 식 을 사 용 해 서 모 델 을 학 습시 켜 보 겠 습 니다 . 모 든 높 은 차 수 의 계 수 들 이 0 에 가 깝 다는 사 실 을 학 습 하 기 에 는 데 이 터 가 너 무 적 습 니다 . 이 경 우 에 는 모 델 이 너 무 복 잡 해 서 학 습 데 이 터 의 노 이 즈 에 쉽 게 영 향 을 받 는 결과가 나 옵 니다 . 학 습 오 류 가 낮 을 지 라 도 , 테 스 트 오 류 는 여 전 히 높 습 니다 . 다 른 모 델 의 복 잡 도 ( n _ degreee ) 와 학 습 셋 크 기 ( n _ subset ) 를 적 용 해 서 어 떤 일이 발 생 하 는 지 에 대 한 직 감 을 얻어 보 세 요 . [ 8 ] : num _ epochs = 1000 n _ subset = 100 # Subset of data to train on n _ degree = 20 # Degree of polynomials fit _ and _ plot ( poly _ features [ 1 : n _ subset , 0 : n _ degree ] , poly _ features [ n _ train : , 0 : n _ degree ] , labels [ 1 : n _ subset ] , labels [ n _ train : ] ) final epoch : train loss 0 . 00519652 test loss 0 . 03173446 weight : [ [ 4 . 9595180e + 00 1 . 2578486e + 00 - 3 . 2017822e + 00 5 . 2782393e + 00 - 6 . 3643193e - 01 1 . 3321676e + 00 - 1 . 6474346e - 02 2 . 0417929e - 01 - 6 . 1874263e - 02 5 . 8486674e - 02 - 3 . 7163932e - 02 - 6 . 7995742e - 02 3 . 7113793e - 02 - 1 . 9592559e - 02 6 . 2177807e - 02 3 . 2198679e - 02 3 . 4999892e - 02 - 4 . 5971844e - 02 - 2 . 2483468e - 02 2 . 9451251e - 03 ] ] 180 5 . 딥 러 닝 기 초 다 음 장 들 에 서 오 버 피 팅 ( overﬁtting ) 문 제 들 을 계 속 논 의 하 고 , 이 를 해 결 하 기 위 한 가 중 치 감 쇠 ( weight decay ) 와 드 롭 아 웃 ( dropout ) 과 같 은 방법 을 알아 보 겠 습 니다 . 5 . 11 . 5 요 약 • 일 반 화 오 류 율은 학 습 오 류 율을 이 용 해 서 추 정 될 수 없 기 때 문 에 , 단 순 히 학 습 오 류 율을 줄 이 는 것 이 일 반 화 오 류를 줄 이 는 것 을 의 미 하 지 않 습 니다 . 머 신 러 닝 모 델 은 일 반 화 오 류를 줄 이 기 를 통 해 서 오 버 피 팅 ( overﬁtting ) 에 조 심스 럽 게 대 비 해 야 합 니다 . • 검 증 셋 은 모 델 선 택 에 사 용 됩 니다 . ( 너 무 남 용 되 지 않 는다는 가 정 에 서 ) • 언 더 피 팅 ( underﬁtting ) 은 모 델 이 학 습 오 류를 줄 이 지 못 하 는 상 황 을 의 미 하 고 , 오 버 피 팅 ( over - ﬁtting ) 은 모 델 학 습 오 류 가 테 스 트 데 이 터 의 오 류 보 다 훨 씬 작은 경 우 를 의 미 합 니다 . • 우 리 는 적절 한 모 델 의 복 잡 성 을 선 택 해 야 하 고 , 부 족 한 학 습 샘 플 을 이 용 하 는 것 을 피해 야 합 니다 . 5 . 11 . 6 문 제 1 . 다 항 회 귀 문 제 를 정 확 하 게 풀 수 있 나 요 ? 힌 트 - 선 형 대 수 를 이 용 합 니다 . 2 . 다 항 식 에 대 한 모 델 선 택 에 대 해 서 • 학 습 오 류 와 모 델 복 잡 도 ( 다 항 식 의 차 원 수 ) 를 도 식 화 해 보 세 요 . 무 엇 이 관 찰 되 나 요 ? 5 . 11 . 모 델 선 택 , 언 더 피 팅 ( underﬁtting ) , 오 버 피 팅 ( overﬁtting ) 181 • 이 경 우 테 스 트 오 류를 도 식 화 해 보 세 요 . • 같 은 그 래 프 를 데 이 터 양에 따 라 서 그 려 보 세 요 . 3 . 다 항 식 의 특 성 ( feature ) 𝑥 𝑖 에 적 용 한 정 규 화 1 / 𝑖 ! 를 제 거 하 면 어 떤 일이 일 어 날 까 요 ? 다 른 방법 으 로 이 를 해 결 할 수 있 나 요 ? 4 . 학 습 오 류를 0 으 로 줄 이 기 위 해 서 몇 차 원 을 사 용 하 나 요 ? 5 . 일 반 화 오 류를 0 으 로 줄 이 는 것 이 가 능 한 가 요 ? 5 . 11 . 7 Scan the QR Code to Discuss 5 . 12 가 중 치 감 쇠 ( weight decay ) 앞 절 에 서 우 리 는 오 버 피 팅 ( overﬁtting ) 에 대 해 서 알아 봤 고 , 이 를 해 결 하 기 위 해 서 용 량 제 어 ( capacity control ) 의 필 요 성 에 대 해 서 도 이 야 기 했 습 니다 . 학 습 데 이 터 셋 의 양 을 늘 리 는 것 은 오 버 피 팅 ( overﬁt - ting ) 문 제 를 해 결 할 수 도 있 지 만 , 학 습 데 이 터 를 추 가 로 확 보 하 는 것 은 일 반 적 으 로 어 려 운 일입 니다 . 그 렇 기 때 문 에 , 사 용 하 는 함 수 의 복 잡 도 를 조정 하 는 것 을 더 선 호 합 니다 . 구 체 적 으 로 는 차 수 를 조정 해 서 다 항 식 의 복 잡 도 를 조절 할 수 있 는 것 을 확 인 했 습 니다 . 이 방법 은 일 차 원 데 이 터 를 다 루 는 문 제 에 대 해 서 는 좋 은 전 략 이 될 수 있 지 만 , 이 방법 은 쉽 게 복 잡 해 지 기 때 문 에 관 리 가 어 려 워 질 수 있 고 , 너 무 투 박 한 방법 입 니다 . 예 를 들 면 , 𝐷 차 원 벡 터 의 경 우 , 𝑑 차 수 에 대 한 단 항 의 개 수 는 (︀ 𝐷 − 1 + 𝑑 𝐷 − 1 )︀ 가 됩 니다 . 따 라 서 , 여 러 함 수 에 대 한 제 어 를 하 는 것 보 다는 함 수 의 복 잡 도 를 조절 할 수 있 는 보 다 정 교 한 툴 이 필 요 합 니다 . 5 . 12 . 1 제 곱 놈 정 규 화 ( squared norm regularization ) 가 장 많 이 사 용 하 는 기 법 중 에 하 나 로 가 중 치 감 쇠 ( weight decay ) 가 있 습 니다 . 이 방법 은 모 든 함 수 𝑓 ﬀ 들 중 에 서 𝑓 = 0ﬀ 이 가 장 간 단 한 형 태 라 는 것 에 착 안 하 고 있 습 니다 . 따 라 서 , 0 과 얼 마 나 가 까 운 가 를 이 용 해 서 함 수 에 대 한 측 정 을 할 수 있 습 니다 . 이 를 측 정 하 는 방법 은 다 양 한 데 별 도 의 수 학 분 야 가 존 182 5 . 딥 러 닝 기 초 재 하 기까 지 합 니다 . 예 를 들 면 , 이 문 제 에 대 한 답 을 찾 는 것 에 목 적 을 두 고 있 는 함 수 분 석 과 Banach 공간 이 론 ( the theory of Banach spaces ) 를 들 수 있 습 니다 . 우 리 의 목 적 을 위 해 서 는 아 주 간 단 한 것 을 사 용 해 도 충 분 합 니다 : 선 형 함 수 𝑓 ( x ) = w ⊤ x ﬀ 에 서 가 중 치 벡 터 ( weight vector ) 가 작을 경 우 ‘’ 이 함 수 는 간 단 하 다 ” 라 고 간 주 합 니다 . 이 것 은 ‖ w ‖ 2 ﬀ 로 측 정 될 수 있 습 니다 . 가 중 치 벡 터 ( weight vector ) 를 작 게 유 지 하 는 방법 중 에 하 나 는 손 실 ( loss ) 을 최 소 화 하 는 문 제 에 이 값 을 패 널 티 ( penalty ) 로 더 하 는 것 입 니다 . 이 렇 게 하 면 , 가 중 치 벡 터 ( weight vector ) 가 너 무 커 지 면 , 학 습 알 고 리 즘 은 학 습 오 류를 최 소 화 하 는 것 보 다 w ﬀ 를 최 소 화 하 는 데 우 선 순 위 를 둘 것 입 니다 . 이 것 이 바 로 우 리 가 원 하 는 것 입 니다 . 코 드 에 서 이 를 설 명 하 기 위 해 서 , 앞 절 의 “Linear Regression” 를 고 려 해 보 면 , 손 실 ( loss ) 은 다 음 과 같 이 주 어 집 니다 . 𝑙 ( w , 𝑏 ) = 1 𝑛 𝑛 ∑︁ 𝑖 = 1 1 2 (︁ w ⊤ x ( 𝑖 ) + 𝑏 − 𝑦 ( 𝑖 ) )︁ 2 . 위 수식 에 서 x ( 𝑖 ) 는 관 찰 들 이 고 , 𝑦 ( 𝑖 ) 는 label , ( w , 𝑏 ) 는 가 중 치 와 편 향 ( bias ) 파 라 미 터 들 입 니다 . 가 중 치 벡 터 ( weight vector ) 의 크 기 에 대 한 패 널 티 를 주 는 새 로 운 손 실 함 수 ( loss function ) 를 만 들 기 위 해 서 , ‖ w ‖ 2 를 더 합 니다 . 하 지 만 , 얼 마 나 더 해 야 할 까 요 ? 이 를 조절 하 는 정 규 화 상 수 ( regularization constant ) 인 𝜆 하 이 퍼파 라 미 터 ( hyperparameter ) 가 그 역 할 을 합 니다 . 𝑙 ( w , 𝑏 ) + 𝜆 2 ‖ 𝑤 ‖ 2 𝜆 ≥ 0 는 정 규 화 ( regularization ) 의 정 도 를 조절 합 니다 . 𝜆 = 0 인 경 우 , 원 래 의 손 실 함 수 ( loss function ) 가 되 고 , 𝜆 > 0 이 면 , w 가 너 무 커 지지 않 도 록 강 제 합 니다 . 통 찰 력 이 있 는 분 은 가 중 치 벡 터 ( weight vector ) 를 왜 제 곱 을 하 는 지 의 아 해할 것 입 니다 . 이 는 두 가 지 이유 때 문 인 데 , 하 나 는 미 분 계 산 이 쉬 워 지 기 때 문 에 연 산 의 편 의 성 을 위 함 이 고 , 다 른 하 나 는 작은 가 중 치 벡 터 ( weight vector ) 들 보 다 큰 가 중 치 벡 터 ( weight vector ) 에 더 많 은 패 널 티 를 부 여 하 는 것 으 로 통 계 적 인 성 능 향 상 을 얻 기 위 하 는 것 입 니다 . 확 률 적 경 사 하 강 법 ( Stochastic gradient descent ) 업 데 이 트 는 다 음 과 같 이 이 뤄 집 니다 . 𝑤 ← (︂ 1 − 𝜂𝜆 | ℬ | )︂ w − 𝜂 | ℬ | ∑︁ 𝑖 ∈ℬ x ( 𝑖 ) (︁ w ⊤ x ( 𝑖 ) + 𝑏 − 𝑦 ( 𝑖 ) )︁ , 이 전 과 같 이 , 관 찰 된 값과 예 측 된 값 의 차 이 에 따 라 서 w 를 업 데 이 트 합 니다 . 하 지 만 , w 의 크 기 를 0 과 가 까 워 지 게 줄 이 고 있 습 니다 . 즉 , 가 중 치 를 감 쇠 하 게 ( decay ) 만 듭 니다 . 이 것 은 다 항 식 에 파 라 미 터 개 수 를 선 택 하 는 것 보 다 더 편 한 방법 입 니다 . 특 히 , 𝑓 의 복 잡 도 를 조절 하 는 연 속성 이 있 는 방법 을 갖게 되 었 습 니다 . 작은 𝜆 값 은 w 를 적 게 제 약 하 는 반 면 , 큰 값 은 w 를 많 이 제 약 합 니다 . 편 향 ( bias ) 항 역 시 큰 값 을 갖 기 를 원 하 지 않 기 때 문 에 , 𝑏 2 를 패 널 티 로 더 하 기 도 합 니다 . 5 . 12 . 가 중 치 감 쇠 ( weight decay ) 183 5 . 12 . 2 고 차 원 선 형 회 귀 고 차 원 회 귀 ( regression ) 에 서 생 략 할 정 확 한 차 원 을 선 택 하 기 어 려 운 데 , 가 중 치 감 쇠 정 규 화 ( weight - decay regularization ) 는 아 주 간 편 한 대 안 이 됩 니다 . 왜 그 런 지 를 지 금 부 터 설 명 하 겠 습 니다 . . 우 선 , 아 래 공 식 을 사 용 해 서 데 이 터 를 생 성 합 니다 . 𝑦 = 0 . 05 + 𝑑 ∑︁ 𝑖 = 1 0 . 01 𝑥 𝑖 + 𝜖 where 𝜖 ∼ 𝒩 ( 0 , 0 . 01 ) 즉 , 이 식 에 서 는 평 균 이 0 이 고 표 준 편 차 가 0 . 01 인 가 우 시 안 ( Gaussian ) 노 이 즈 를 추 가 했 습 니다 . 오 버 피 팅 ( overﬁtting ) 을 더 잘 재 현 하 기 위 해 서 , 차 원 𝑑 가 200 인 고 차 원 문 제 를 선 택 하 고 , 적 은 양 의 학 습 데 이 터 ( 20 개 ) 를 사 용 하 겠 습 니다 . 이 전 과 같 이 필 요 한 패 키 지 를 import 합 니다 . [ 1 ] : import sys sys . path . insert ( 0 , ' . . ' ) % matplotlib inline import d2l from mxnet import autograd , gluon , init , nd from mxnet . gluon import data as gdata , loss as gloss , nn n _ train , n _ test , num _ inputs = 20 , 100 , 200 true _ w , true _ b = nd . ones ( ( num _ inputs , 1 ) ) * 0 . 01 , 0 . 05 features = nd . random . normal ( shape = ( n _ train + n _ test , num _ inputs ) ) labels = nd . dot ( features , true _ w ) + true _ b labels + = nd . random . normal ( scale = 0 . 01 , shape = labels . shape ) train _ features , test _ features = features [ : n _ train , : ] , features [ n _ train : , : ] train _ labels , test _ labels = labels [ : n _ train ] , labels [ n _ train : ] 5 . 12 . 3 처 음 부 터 구 현 하 기 다 음으 로 는 가 중 치 감 쇠 ( weight decay ) 를 직 접 구 현 해 보 겠 습 니다 . 이 를 위 해 서 , 간 단 하 게 타 켓 ( target ) 함 수 다 음 에 ℓ 2 패 널 티 를 추 가 손 실 항 목 으 로 더 합 니다 . 제 곱 놈 ( squared norm ) 패 널 티 라 는 이 름 은 제 곱 수 를 더 하 는 것 , ∑︀ 𝑖 𝑥 2 𝑖 , 으 로 부 터 왔 습 니다 . 이 외 에 도 여 러 가 지 패 널 티 들 이 있 습 니다 . ℓ 𝑝 놈 ( norm ) 은 다 음 과 같 이 정 의 됩 니다 . ‖ x ‖ 𝑝𝑝 : = 𝑑 ∑︁ 𝑖 = 1 | 𝑥 𝑖 | 𝑝 184 5 . 딥 러 닝 기 초 파 라 미 터 초 기 화 하 기 우 선 모 델 파 라 미 터 를 임의 로 초 기 화 하 는 함 수 를 정 의 합 니다 . 이 함 수 는 각 파 라 미 터 에 그 래 디 언 트 ( gradient ) 를 붙 입 니다 . [ 2 ] : def init _ params ( ) : w = nd . random . normal ( scale = 1 , shape = ( num _ inputs , 1 ) ) b = nd . zeros ( shape = ( 1 , ) ) w . attach _ grad ( ) b . attach _ grad ( ) return [ w , b ] ℓ 2 놈 페 널 티 ( Norm Penalty ) 정 의 하 기 이 페 널 티 를 정 의 하 는 간 단 한 방법 은 각 항 을 모 두 제 곱 하 고 이 를 더 하 는 것 입 니다 . 수식 이 멋 지 고 간 단 하 게 보 이 기 위 해 서 2 로 나 눕니다 . [ 3 ] : def l2 _ penalty ( w ) : return ( w * * 2 ) . sum ( ) / 2 학 습 및 테 스 트 정 의 하 기 아 래 코 드 는 학 습 데 이 터 셋 과 테 스 트 데 이 터 셋 을 이 용 해 서 모 델 을 학 습시 키 고 테 스 트 하 는 함 수 를 정 의 합 니다 . 이 전 절 의 예와 는 다 르 게 , 여 기 서 는 ℓ 2 놈 패 널 티 ( norm penalty ) 를 최 종 손 실 함 수 ( loss function ) 를 계 산 할 때 더 합 니다 . 선 형 네 트 워 크 와 제 곱 손 실 ( squared loss ) 은 이 전 과 같 기 때 문 에 , d2l . linreg 와 d2l . squared _ loss 를 import 해 서 사 용 하 겠 습 니다 . [ 4 ] : batch _ size , num _ epochs , lr = 1 , 100 , 0 . 003 net , loss = d2l . linreg , d2l . squared _ loss train _ iter = gdata . DataLoader ( gdata . ArrayDataset ( train _ features , train _ labels ) , batch _ size , shuffle = True ) def fit _ and _ plot ( lambd ) : w , b = init _ params ( ) train _ ls , test _ ls = [ ] , [ ] for _ in range ( num _ epochs ) : for X , y in train _ iter : with autograd . record ( ) : # The L2 norm penalty term has been added ( continues on next page ) 5 . 12 . 가 중 치 감 쇠 ( weight decay ) 185 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) l = loss ( net ( X , w , b ) , y ) + lambd * l2 _ penalty ( w ) l . backward ( ) d2l . sgd ( [ w , b ] , lr , batch _ size ) train _ ls . append ( loss ( net ( train _ features , w , b ) , train _ labels ) . mean ( ) . asscalar ( ) ) test _ ls . append ( loss ( net ( test _ features , w , b ) , test _ labels ) . mean ( ) . asscalar ( ) ) d2l . semilogy ( range ( 1 , num _ epochs + 1 ) , train _ ls , ' epochs ' , ' loss ' , range ( 1 , num _ epochs + 1 ) , test _ ls , [ ' train ' , ' test ' ] ) print ( ' l2 norm of w : ' , w . norm ( ) . asscalar ( ) ) 정 규 화 ( regularization ) 없 이 학 습 하 기 자 이 제 고 차 원 의 선 형 회 귀 ( linear regression ) 모 델 을 학 습시 키 고 테 스 트 해 봅 니다 . lambd = 0 인 경 우 에 는 가 중 치 감 쇠 ( weight decay ) 를 사 용 하 지 않 습 니다 . 그 결과 로 , 학 습 오 류 가 줄 어 드 는 반 면 , 테 스 트 오 류 는 줄 어 들 지 않 게 됩 니다 . 즉 , 오 버 피 팅 ( overﬁtting ) 의 완 벽 한 예 제 가 만 들 어 졌 습 니다 . [ 5 ] : fit _ and _ plot ( lambd = 0 ) l2 norm of w : 11 . 611941 186 5 . 딥 러 닝 기 초 가 중 치 감 쇠 ( weight decay ) 사 용 하 기 아 래 예 는 학 습 오 류 는 증 가 하 는 반 면 , 테 스 트 오 류 는 감 소 하 는 것 을 보 여 줍 니다 . 이 것 은 가 중 치 감 쇠 ( weight decay ) 를 사 용 하 면 서 예 상 한 개 선 된 결과 입 니다 . 완 벽 하 지 는 않 지 만 , 오 버 피 팅 ( overﬁtting ) 문 제 가 어 느 정 도 해 결 되 었 습 니다 . 추 가 로 , 가 중 치 w 에 대 한 ℓ 2 놈 ( norm ) 도 가 중 치 감 쇠 ( weight decay ) 를 사 용 하 지 않 을 때 보 다 작 아 졌 습 니다 . [ 6 ] : fit _ and _ plot ( lambd = 3 ) l2 norm of w : 0 . 041905243 5 . 12 . 4 간결 한 구 현 Gluon 에 는 최 적 화 알 고 리 즘 에 가 중 치 감 쇠 ( weight decay ) 가 통 합 되 어 있 어 더 편 하 게 적 용 할 수 있 습 니다 . 그 이유 는 옵 티 마 이 져 ( optimizer ) 가 모 든 파 라 미 터 를 직 접 다 루 기 때 문 에 , 옵 티 마 이 져 ( optimizer ) 가 가 중 치 감 쇠 ( weight decay ) 를 직 접 관 리 하 고 , 관 련 된 것 을 최 적 화 알 고 리 즘 에 서 다 루 는 것 이 실 행 속 도 면 에 서 더 빠 르 기 때 문 입 니다 . 아 래 예 제 에 서 는 Trainer 인 스 턴 스 를 생 성 할 때 , wd 파 라 미 터 를 통 해 서 가 중 치 감 쇠 ( weight decay ) 하 이 퍼파 라 미 터 ( hyperparameter ) 를 직 접 지 정 합 니다 . Gluon 의 기 본 설 정 은 가 중 치 와 편 향 ( bias ) 을 모 두 감 쇠 ( decay ) 시 킵 니다 . 다 른 종 류 의 파 라 미 터 에 대 해 서 다 른 옵 티 마 이 져 ( optimizer ) 를 사 용 할 수 있 습 니다 . 예 를 들 면 , w 에 는 가 중 치 감 쇠 ( weight decay ) 를 적 용 하 는 Trainer 를 하 나 만 들 고 , 𝑏 에 는 가 중 치 감 쇠 ( weight decay ) 를 적 용 하 지 않 은 다 른 Trainer 를 각각 만 들 수 있 습 니다 . 5 . 12 . 가 중 치 감 쇠 ( weight decay ) 187 [ 7 ] : def fit _ and _ plot _ gluon ( wd ) : net = nn . Sequential ( ) net . add ( nn . Dense ( 1 ) ) net . initialize ( init . Normal ( sigma = 1 ) ) # The weight parameter has been decayed . Weight names generally end with # " weight " . trainer _ w = gluon . Trainer ( net . collect _ params ( ' . * weight ' ) , ' sgd ' , { ' learning _ rate ' : lr , ' wd ' : wd } ) # The bias parameter has not decayed . Bias names generally end with " bias " trainer _ b = gluon . Trainer ( net . collect _ params ( ' . * bias ' ) , ' sgd ' , { ' learning _ rate ' : lr } ) train _ ls , test _ ls = [ ] , [ ] for _ in range ( num _ epochs ) : for X , y in train _ iter : with autograd . record ( ) : l = loss ( net ( X ) , y ) l . backward ( ) # Call the step function on each of the two Trainer instances to # update the weight and bias separately trainer _ w . step ( batch _ size ) trainer _ b . step ( batch _ size ) train _ ls . append ( loss ( net ( train _ features ) , train _ labels ) . mean ( ) . asscalar ( ) ) test _ ls . append ( loss ( net ( test _ features ) , test _ labels ) . mean ( ) . asscalar ( ) ) d2l . semilogy ( range ( 1 , num _ epochs + 1 ) , train _ ls , ' epochs ' , ' loss ' , range ( 1 , num _ epochs + 1 ) , test _ ls , [ ' train ' , ' test ' ] ) print ( ' L2 norm of w : ' , net [ 0 ] . weight . data ( ) . norm ( ) . asscalar ( ) ) 그 래 프 는 가 중 치 감 쇠 ( weight decay ) 를 직 접 구 현 해 서 얻었 던 것과 아 주 비 슷 하 게 생 겼 습 니다 . 하 지 만 , 더 빠 르 고 더 구 현 하 기 쉬 웠 고 , 큰 문 제 의 경 우 에 는 더 욱 그 렇 습 니다 . [ 8 ] : fit _ and _ plot _ gluon ( 0 ) 188 5 . 딥 러 닝 기 초 L2 norm of w : 13 . 311796 [ 9 ] : fit _ and _ plot _ gluon ( 3 ) L2 norm of w : 0 . 032878112 지 금까 지 우 리 는 간 단 한 선 형 함 수 를 구 성 하 는 것 들 만 을 다 뤘 습 니다 . 비 선 형 함 수 에 대 해 서 이 것 들 을 다 루 는 것 은 훨 씬 더 복 잡 합 니다 . 예 를 들 어 , Reproducing Kernel Hilbert Spaces 라 는 것 이 있 는 데 , 이 를 이 용 하 면 선 형 함 수 에 서 사 용 한 많 은 도 구 들 을 비 선 형 에 서 사 용 할 수 있 게 해 줍 니다 . 하 지 만 안 타 깝 게 도 , 사 용 되 는 알 고 리 즘 들 이 데 이 터 가 매 우 많 은 경 우 잘 동 작 하 지 않 는 확 장 성 문 제 가 있 습 니다 . 따 라 서 , 이 책 의 목 적 을 위 해 서 우 리 는 각 층 의 가 중 치 들 을 단 순 히 더 하 는 방법 , ∑︀ 𝑙 ‖ w 𝑙 ‖ 2 을 사 용 하 5 . 12 . 가 중 치 감 쇠 ( weight decay ) 189 겠 습 니다 . 이 렇 게 하 는 것 은 전 체 층 들 에 가 중 치 감 쇠 ( weight decay ) 를 적 용 하 는 것과 같 습 니다 . 5 . 12 . 5 요 약 • 정 규 화 ( regularization ) 은 오 버 피 팅 ( overﬁtting ) 을 다 루 는 일 반 적 인 방법 입 니다 . 학 습 된 모 델 의 복 잡 도 를 줄 이 기 위 해 서 학 습 데 이 터 에 대 한 손 실 함 수 ( loss function ) 의 값 에 패 널 티 항 목 을 더 합 니다 . • 모 델 을 간 단 하 게 유 지 하 는 방법 으 로 ℓ 2 놈 패 널 티 ( norm penalty ) 를 사 용 하 는 가 중 치 감 쇠 ( weight decay ) 를 선 택 했 습 니다 . 이 를 통 해 서 , 학 습 알 고 리 즘 의 업 데 이 트 단 계 에 서 가 중 치 감 쇠 ( weight decay ) 가 적 용 됩 니다 . • Gluon 은 옵 티 마 이 저 ( optimizer ) 에 하 이 퍼파 라 미 터 ( hyperparameter ) wd 를 설 정 하 는 것 으 로 가 중 치 감 쇠 ( weight decay ) 기 능 을 자 동 으 로 추 가 할 수 있 습 니다 . • 같 은 학 습 에 서 파 라 미 터 마 다 다 른 옵 티 마 이 저 ( optimizer ) 를 적 용 할 수 있 습 니다 . 5 . 12 . 6 문 제 1 . 이 장의 예 측 문 제 에 서 𝜆 값 을 실 험해 보 세 요 . 𝜆 에 대 한 함 수 의 형 태 로 학 습 정 확 도 와 테 스 트 정 확 도 를 도 식 화 해 보 세 요 . 어 떤 것 이 관 찰 되 나 요 ? 2 . 검 증 데 이 터 셋 을 이 용 해 서 최 적 의 𝜆 값 을 찾 아 보 세 요 . 찾 은 값 이 진짜 최 적 값 인 가 요 ? 진짜 값 을 찾 는 것 이 중 요 한 가 요 ? 3 . 패 널 티 항 목 으 로 ‖ w ‖ 2 대 신 ∑︀ 𝑖 | 𝑤 𝑖 | 를 사 용 하 면 업 데 이 트 공 식 이 어 떻 게 될 까 요 ? ( 이 는 ℓ 1 정 규 화 ( regularization ) 라 고 합 니다 . ) 4 . ‖ w ‖ 2 = w ⊤ w 입 니다 . 행 렬 에 서 비 슷 한 공 식 을 찾 아 볼 수 있 나 요 ? ( 수 학 자 들 은 이 를 Frobenius norm 이 라 고 합 니다 ) 5 . 학 습 오 류 와 일 반 화 오 류 의 관 계 를 복 습 해 보 세 요 . 가 중 치 감 쇠 ( weight decay ) , 학 습 데 이 터 셋 늘 리 기 , 적 당 한 복 잡 도 를 갖 는 모 델 사 용 하 기 외 에 , 오 버 피 팅 ( overﬁtting ) 을 다 룰 수 있 는 방법 이 어 떤 것 들 이 있을 까 요 ? 6 . 베 이 시 안 통 계 에 서 , prior 와 likelihood 곱 을 이 용 해 서 posterior 를 구 할 수 있 습 니다 . 𝑝 ( 𝑤 | 𝑥 ) ∝ 𝑝 ( 𝑥 | 𝑤 ) 𝑝 ( 𝑤 ) . 𝑝 ( 𝑤 ) 가 정 규 화 ( regularization ) 와 어 떻 게 동 일 할 까 요 ? 190 5 . 딥 러 닝 기 초 5 . 12 . 7 Scan the QR Code to Discuss 5 . 13 드 롭 아 웃 ( dropout ) 앞에 서 우 리 는 통 계 적 인 모 델 을 정 규 화 ( regularize ) 하 는 전 통 적 인 방법 을 알아 봤 습 니다 . 가 중 치 의 크 기 ( ℓ 2 ﬀ norm ) 을 패 널 티 로 사 용 해 서 , 가 중 치 값 이 강 제 로 작 아 지 도 록 했 습 니다 . 확 률 적 인 용 어 로 말 하 자 면 , 가 우 시 안 프 리 어 ( Gaussian prior ) 를 가 중 치 값 에 적 용 한 다 고 할 수 있 습 니다 . 하 지 만 , 더 직 관 적 인 함 수 의 용 어 를 사 용 하 면 , 가 중 치 값 들 이 다 른 특 성 ( feature ) 들 사 이 에 더 확 산 되 도 록 하 고 , 잠재 적 으 로 가 짜 연 관 들 에 더 적 게 의 존 되 도 록 모 델 을 학 습시 킨 다 고 할 수 있 습 니다 . 5 . 13 . 1 오 버 피 팅 다 시 살 펴 보 기 뛰 어 난 유 연 성 은 오 버 피 팅 ( overﬁtting ) 에 대 한 책 임이 따 릅 니다 . 샘 플 들 보 다 더 많 은 특 성 ( feature ) 들 이 주 어 지 면 , 선 형 모 델 은 오 버 핏 ( overﬁt ) 될 수 있 습 니다 . 반 면 에 특 성 ( feature ) 수 보 다 샘 플 이 더 많 은 경 우 에 는 선 형 모 델 은 일 반 적 으 로 오 버 핏 ( overﬁt ) 되 지 않 습 니다 . 아 쉽 게 도 , 일 반 화 를 잘 하 기 위 해 서 는 그 에 따 른 비 용 이 들 어 갑 니다 . 매 특 성 ( feature ) 에 대 해 서 , 선 형 모 델 은 양 수 또 는 음 수 의 가 중 치 를 할 당 해 야 합 니다 . 선 형 모 델 은 특 성 ( feature ) 들 사 이의 미묘 한 상 호 작 용 을 설 명 하 지 못 합 니다 . 좀 더 공 식 적 인 용 어 로 이 야 기 하 면 , 편 향 - 분 산 트 레 이 드 오 프 ( bias - variance tradeoff ) 로 논 의 되 는 현 상 을 볼 것 입 니다 . 선 형 모 델 은 높 은 편 향 ( bias ) ( 표 현 할 수 있 는 함 수 의 개 수 가 적 습 니다 ) 를 보 이 나 , 분 산 ( variance ) 은 낮 습 니다 ( 다 른 랜 덤 샘 플 데 이 터 에 대 해 서 비 슷 한 결과 를 줍 니다 ) 반 면 에 딥 뉴 럴 네 트 워 크 는 편 향 - 분 산 ( bias - variance ) 스 팩 트 럼 에 서 반 대 의 현 상 을 보 입 니다 . 뉴 럴 네 트 워 크 는 각 특 성 ( feature ) 을 독 립 적 으 로 보 는 제 약 이 없 기 때 문 에 유 연 합 니다 . 대 신 , 특 성 ( feature ) 들 의 그 룹 들 에 서 복 잡 한 상 관관 계 를 학 습 할 수 있 습 니다 . 예 를 들 면 , 뉴 럴 네 트 워 크 는 “Nigeria” 와 “Western Union” 이 이 메 일 에 함 께 나 오 면 그 이 메 일을 스 팸 으 로 판 단 하 고 , “Western Union” 이 라 는 단 어 가 없 이 “Nigeria” 가 등 장 하 는 이 메 일은 스 팸 이 아 니 라 고 판 단 할 수 있 습 니다 . 특 성 ( feature ) 들 의 개 수 가 적 은 경 우 에 도 , 딥 뉴 럴 네 트 워 크 는 오 버 피 팅 ( overﬁtting ) 될 수 있 습 니다 . 뉴 럴 네 트 워 크 의 뛰 어 난 유 연 성 을 보 여 주 는 예 로 , 연 구 자 들 은 임의 로 레 이 블 ( label ) l 이 할 당 된 데 이 터 5 . 13 . 드 롭 아 웃 ( dropout ) 191 를 완 벽 하 게 분 류 하 는 것 을 입 증 했 습 니다 . 이 것 이 무 엇 을 뜻 하 는 지 생 각 해 봅 시 다 . 10 개 의 분 류 로 된 레 이 블 들 이 균 일 하 게 임의 로 부 여 되 어 있 는 경 우 , 어 떤 분 류 기 도 10 % 이 상 의 정 확 도 를 얻 을 수 없 습 니다 . 이 렇 게 패 턴 을 학 습 할 수 없 는 경 우 에 도 , 뉴 럴 네 트 워 크 는 학 습 레 이 블 들 에 완 벽 하 게 맞 춰 질 수 있 습 니다 . 5 . 13 . 2 변 화 를 통 한 견고 함 좋 은 통 계 적 인 모 델 로 부 터 무 엇 을 기 대 할 수 있 는 지 에 대 해 서 간 단 히 알아 보 겠 습 니다 . 당 연 하 게 우 리 는 이 모 델 이 보 지 않 은 테 스 트 데 이 터 에 대 해 서 잘 작 동 하 기 를 기 대 합 니다 . 이 를 달 성 하 는 방법 중 에 하 나 로 어 떤 것 이 “ 간 단 한 ” 모 델 을 만 드 는 지 를 묻 는 것 입 니다 . 단 항 함 수 ( monomial basis function ) 을 사 용 해 서 모 델 을 학 습시 키 면 서 언 급 했 던 것 처 럼 차 원 의 수 가 적 은 것 으 로 부 터 간 단 함 이 유 도 될 수 있 습 니다 . 또 한 간 단 함 은 기 본 이 되 는 함 수 의 작은 놈 ( norm ) 의 형 태 로 만 들 어 질 수 도 있 습 니다 . 즉 , 가 중 치 감 쇠 ( weight decay ) 와 ℓ 2 정 규 화 ( regularization ) 이 그 런 예 입 니다 . 간 단 함 을 만 드 는 또 다 른 요 소 는 입 력 의 완 만 한 변 화 에 도 큰 영 향 을 받 지 않 는 함 수 를 들 수 있 습 니다 . 예 를 들 어 이 미 지 를 분 류 할 때 , 몇 개 의 픽 셀 들 의 변 경 으 로 인 해 서 결과 에 영 향 을 미 치 지 않 기 를 기 대 하 는 것 입 니다 . 사 실 이 개 념 은 1995 년 Bishop 이 Training with Input Noise is Equivalent to Tikhonov Regularization 를 증 명 하 면 서 공 식 화 되 었 습 니다 . 즉 , 그 는 부 드 러 운 ( 따 라 서 간 단 한 ) 함 수 의 개 념 을 입 력 의 변 화 에 탄 력 적 인 것과 연 관 을 시 켰 습 니다 . 2014 년 으 로 흘 러 가 서 , 여 러 층 을 갖 는 딥 네 트 워 크 의 복 잡 도 가 주 어 졌 을 때 , 입 력 에 부 드 러 움 을 강 제 하 는 것 은 꼭 다 음 층 들 에 서 도 보 장 되 지 는 않 습 니다 . Srivastava et al . , 2014 에 서 발 표 된 독 창 적 인 아 이 디 어 는 Bishop 의 아 이 디 어 를 네 트 워 크 의 내 부 층 들 에 적 용 했 습 니다 . 이 는 , 학 습 과 정 에 네 트 워 크 연 산 경 로 에 노 이 즈 를 집 어 넣 는 것 입 니다 . 여 기 서 주 요 과 제 는 지 나 친 편 향 ( bias ) 을 추 가 하 지 않 으 면 서 어 떻 게 노 이 즈 를 추 가 하 는 지 입 니다 . 입 력 x 에 대 해 서 는 노 이 즈 를 추 가 하 는 것 은 상 대 적 으 로 간 단 합 니다 . 즉 , 𝜖 ∼ 𝒩 ( 0 , 𝜎 2 ) 노 이 즈 를 입 력 에 더 한 후 x ′ = x + 𝜖 이 것 을 학 습 데 이 터 로 사 용 하 면 됩 니다 . 이 렇 게 했 을 때 주 요 특 징 은 E [ x ′ ] = x 을 갖 는 것 입 니다 . 하 지 만 , 중 간 층 들 에 서 는 이 노 이 즈 의 스 캐 일이 적절 하 지 않 을 수 있 기 때 문 에 이 특 징 을 기 대 하 기 어 렵 습 니다 . 대 안 은 다 음 과 같 이 좌 표 를 뒤 틀 어 놓 는 것 입 니다 . ℎ ′ = ⎧⎨ ⎩ 0 확 률 𝑝 인 경 우 ℎ 1 − 𝑝 그 외 의 경 우 설 계 상 으 로 는 기 대 값 이 변 하 지 않 습 니다 . 즉 , E [ ℎ ′ ] = ℎ 입 니다 . 중 간 레 이 어 들 에 적 용 되 는 활 성 화 ( activation ) ℎ 를 같 은 기 대 값 을 갖 는 랜 덤 변 수 ℎ ′ 로 바 꾸 는 것 이 드 롭 아 웃 ( dropout ) 의 핵 심 아 이 디 어 입 니다 . ‘ 드 롭 아 웃 ( dropout ) ’ 이 라 는 이 름 은 마 지 막 결과 를 계 산 하 기 위 해 서 사 용 되 는 연 산 의 몇몇 뉴 런 들 을 누 락 ( drop out ) 시 킨 다는 개 념 에 서 왔 습 니다 . 학 습 과 정 에 서 , 중 간 의 활 성 화 ( activation ) 들 을 활 률 변 수 로 바 꿉 니다 . 192 5 . 딥 러 닝 기 초 5 . 13 . 3 드 롭 아 웃 ( dropout ) 실 제 적 용 하 기 5 개 의 은 닉 유 닛 ( hidden unit ) 을 갖 는 한 개 의 은 닉 층 을 사 용 하 는 다 층 퍼 셉 트 론 ( multilayer perceptron ) 의 예 를 다 시 들 어 보 겠 습 니다 . 이 네 트 워 크 의 아 키 텍 처 는 다 음 과 같 이 표 현 됩 니다 . ℎ = 𝜎 ( 𝑊 1 𝑥 + 𝑏 1 ) 𝑜 = 𝑊 2 ℎ + 𝑏 2 ˆ 𝑦 = softmax ( 𝑜 ) 은 닉 층 에 드 롭 아 웃 ( dropout ) 을 확 률 𝑝 로 적 용 하 는 경 우 , 은 닉 유 닛 들 을 𝑝 확 률 로 제 거 하 는 것 이 됩 니다 . 이유 는 , 그 확 률 을 이 용 해 서 출 력 을 0 으 로 설 정 하 기 때 문 입 니다 . 이 를 적 용 한 네 트 워 크 는 아 래 그 림 과 같 습 니다 . 여 기 서 ℎ 2 와 ℎ 5 가 제 거 되 었 습 니다 . 결과 적 으 로 𝑦 를 계 산 할 때 , ℎ 2 와 ℎ 5 는 사 용 되 지 않 게 되 고 , 역 전 파 ( backprop ) 을 수 행할 때 이 것 들 에 대 한 그 래 디 언 트 ( gradient ) 들 도 적 용 되 지 않 습 니다 . 이 렇 게 해 서 출 력 층 으 ㄹ 계 산 할 때 ℎ 1 , . . . , ℎ 5 중 어 느 하 나 에 전적 으 로 의 존 되 지 않 게 합 니다 . 이 것 이 오 버 피 팅 ( overﬁtting ) 문 제 를 해 결 하 는 정 규 화 ( regularization ) 목 적 을 위 해 서 필 요 한 것 입 니다 . 테 스 트 시 에 는 , 더 확 실 한 결과 를 얻 기 위 해 서 드 롭 아 웃 ( dropout ) 을 사 용 하 지 않 는 것 이 일 반 적 입 니다 . 5 . 13 . 4 직 접 구 현 하 기 드 롭 아 웃 ( dropout ) 를 구 현 하 기 위 해 서 는 입 력 개 수 만 큼 의 확 률 변 수 를 균 일 한 분 포 𝑈 [ 0 , 1 ] ﬀ 에 서 추 출 해 야 합 니다 . 드 롭 아 웃 ( dropout ) 의 정 의 에 따 르 면 , 이 를 간 단 하 게 구 현 할 수 있 습 니다 . 다 음 드 롭 아 웃 ( dropout ) 함 수 는 NDArray 입 력 x 의 원 소 들 을 drop _ prob 확 률 로 누 락 시 킵 니다 . 5 . 13 . 드 롭 아 웃 ( dropout ) 193 [ 1 ] : import sys sys . path . insert ( 0 , ' . . ' ) import d2l from mxnet import autograd , gluon , init , nd from mxnet . gluon import loss as gloss , nn def dropout ( X , drop _ prob ) : assert 0 < = drop _ prob < = 1 # In this case , all elements are dropped out if drop _ prob = = 1 : return X . zeros _ like ( ) mask = nd . random . uniform ( 0 , 1 , X . shape ) > drop _ prob return mask * X / ( 1 . 0 - drop _ prob ) 몇 가 지 예 제 에 적 용 해 서 어 떻 게 동 작 하 는 지 살 펴 보 겠 습 니다 . 드 롭 아 웃 ( dropout ) 확 률 을 각각 0 , 0 . 5 , 그 리 고 1 로 설 정 해 봅 니다 . [ 2 ] : X = nd . arange ( 16 ) . reshape ( ( 2 , 8 ) ) print ( dropout ( X , 0 ) ) print ( dropout ( X , 0 . 5 ) ) print ( dropout ( X , 1 ) ) [ [ 0 . 1 . 2 . 3 . 4 . 5 . 6 . 7 . ] [ 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . ] ] < NDArray 2x8 @ cpu ( 0 ) > [ [ 0 . 0 . 0 . 0 . 8 . 10 . 12 . 0 . ] [ 16 . 0 . 20 . 22 . 0 . 0 . 0 . 30 . ] ] < NDArray 2x8 @ cpu ( 0 ) > [ [ 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . ] [ 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . ] ] < NDArray 2x8 @ cpu ( 0 ) > 5 . 13 . 5 모 델 파 라 미 터 정 의 하 기 “Softmax 회 귀 ( regression ) 를 처 음 부 터 구 현 하 기 ” 절 에 서 사 용 한 Fashion - MNIST 데 이 터 셋 을 다 시 사 용 합 니다 . 두 개 의 은 닉 층 들 을 갖 는 다 층 퍼 셉 트 론 ( multilayer perceptron ) 을 정 의 하 는 데 , 각 은 닉 층 은 194 5 . 딥 러 닝 기 초 256 개 의 결과 를 출 력 합 니다 . [ 3 ] : num _ inputs , num _ outputs , num _ hiddens1 , num _ hiddens2 = 784 , 10 , 256 , 256 W1 = nd . random . normal ( scale = 0 . 01 , shape = ( num _ inputs , num _ hiddens1 ) ) b1 = nd . zeros ( num _ hiddens1 ) W2 = nd . random . normal ( scale = 0 . 01 , shape = ( num _ hiddens1 , num _ hiddens2 ) ) b2 = nd . zeros ( num _ hiddens2 ) W3 = nd . random . normal ( scale = 0 . 01 , shape = ( num _ hiddens2 , num _ outputs ) ) b3 = nd . zeros ( num _ outputs ) params = [ W1 , b1 , W2 , b2 , W3 , b3 ] for param in params : param . attach _ grad ( ) 5 . 13 . 6 모 델 정 의 하 기 정 의 하 는 모 델 은 각 활 성 화 함 수 ( activation function ) 의 결과 에 드 롭 아 웃 ( dropout ) 을 적 용 하 면 서 완 전 연 결 층 ( fully connected layer ) 와 활 성 화 함 수 ( activation function ) ReLU 를 연 결 하 도 록 되 어 있 습 니다 . 각 층 에 서 로 다 른 드 롭 아 웃 ( dropout ) 확 률 을 설 정 할 수 있 습 니다 . 일 반 적 으 로 는 입 력 층 에 가 까 울 수 록 낮 은 드 롭 아 웃 ( dropout ) 확 률 값 을 사 용 하 는 것 을 권 장 합 니다 . 아 래 모 델 에 서 는 첫 번 째 층 에 는 0 . 2 를 두 번 째 층 에 는 0 . 5 를 적 용 하 고 있 습 니다 . “Autograd” 절 에 서 정 의 한 is _ training 을 사 용 하 면 , 학 습 할 때 만 드 롭 아 웃 ( dropout ) 이 적 용 될 수 있 게 할 수 있 습 니다 . [ 4 ] : drop _ prob1 , drop _ prob2 = 0 . 2 , 0 . 5 def net ( X ) : X = X . reshape ( ( - 1 , num _ inputs ) ) H1 = ( nd . dot ( X , W1 ) + b1 ) . relu ( ) # Use dropout only when training the model if autograd . is _ training ( ) : # Add a dropout layer after the first fully connected layer H1 = dropout ( H1 , drop _ prob1 ) H2 = ( nd . dot ( H1 , W2 ) + b2 ) . relu ( ) if autograd . is _ training ( ) : # Add a dropout layer after the second fully connected layer H2 = dropout ( H2 , drop _ prob2 ) return nd . dot ( H2 , W3 ) + b3 5 . 13 . 드 롭 아 웃 ( dropout ) 195 5 . 13 . 7 학 습 및 테 스 트 다 층 퍼 셉 트 론 ( multilayer perceptron ) 의 학 습 과 테 스 트 는 이 전 에 설 명 한 것과 비 슷 합 니다 . [ 5 ] : num _ epochs , lr , batch _ size = 10 , 0 . 5 , 256 loss = gloss . SoftmaxCrossEntropyLoss ( ) train _ iter , test _ iter = d2l . load _ data _ fashion _ mnist ( batch _ size ) d2l . train _ ch3 ( net , train _ iter , test _ iter , loss , num _ epochs , batch _ size , params , lr ) epoch 1 , loss 1 . 1833 , train acc 0 . 543 , test acc 0 . 771 epoch 2 , loss 0 . 5987 , train acc 0 . 779 , test acc 0 . 822 epoch 3 , loss 0 . 4963 , train acc 0 . 818 , test acc 0 . 836 epoch 4 , loss 0 . 4512 , train acc 0 . 835 , test acc 0 . 846 epoch 5 , loss 0 . 4273 , train acc 0 . 844 , test acc 0 . 861 epoch 6 , loss 0 . 4088 , train acc 0 . 851 , test acc 0 . 869 epoch 7 , loss 0 . 3922 , train acc 0 . 856 , test acc 0 . 866 epoch 8 , loss 0 . 3720 , train acc 0 . 863 , test acc 0 . 870 epoch 9 , loss 0 . 3604 , train acc 0 . 869 , test acc 0 . 878 epoch 10 , loss 0 . 3554 , train acc 0 . 869 , test acc 0 . 881 5 . 13 . 8 간결 한 구 현 Gluon 을 이 용 하 면 , 완 전 연 결 층 ( fully connected layer ) 다 음 에 드 롭 아 웃 ( dropout ) 확 률 값 을 주 면 서 드 롭 아 웃 ( dropout ) 층 을 추 가 하 기 만 하 면 됩 니다 . 모 델 을 학 습시 킬 때 드 롭 아 웃 ( dropout ) 층 은 명 시 된 드 롭 아 웃 ( dropout ) 확 률 에 따 라 서 결과 원 소 들 을 임의 로 누 락 시 켜 주 고 , 테 스 트 를 수 행할 때 는 데 이 터 를 그 냥 통 과 시 킵 니다 . [ 6 ] : net = nn . Sequential ( ) net . add ( nn . Dense ( 256 , activation = " relu " ) , # Add a dropout layer after the first fully connected layer nn . Dropout ( drop _ prob1 ) , nn . Dense ( 256 , activation = " relu " ) , # Add a dropout layer after the second fully connected layer nn . Dropout ( drop _ prob2 ) , nn . Dense ( 10 ) ) net . initialize ( init . Normal ( sigma = 0 . 01 ) ) 다 음으 로 모 델 을 학 습시 키 고 테 스 트 를 수 행합 니다 . 196 5 . 딥 러 닝 기 초 [ 7 ] : trainer = gluon . Trainer ( net . collect _ params ( ) , ' sgd ' , { ' learning _ rate ' : lr } ) d2l . train _ ch3 ( net , train _ iter , test _ iter , loss , num _ epochs , batch _ size , None , None , trainer ) epoch 1 , loss 1 . 1507 , train acc 0 . 552 , test acc 0 . 774 epoch 2 , loss 0 . 6168 , train acc 0 . 773 , test acc 0 . 807 epoch 3 , loss 0 . 5205 , train acc 0 . 809 , test acc 0 . 849 epoch 4 , loss 0 . 4542 , train acc 0 . 832 , test acc 0 . 858 epoch 5 , loss 0 . 4279 , train acc 0 . 845 , test acc 0 . 859 epoch 6 , loss 0 . 4049 , train acc 0 . 852 , test acc 0 . 865 epoch 7 , loss 0 . 3863 , train acc 0 . 859 , test acc 0 . 865 epoch 8 , loss 0 . 3769 , train acc 0 . 862 , test acc 0 . 868 epoch 9 , loss 0 . 3619 , train acc 0 . 868 , test acc 0 . 874 epoch 10 , loss 0 . 3520 , train acc 0 . 871 , test acc 0 . 873 5 . 13 . 9 요 약 • 차 원 수 를 조절 하 고 가 중 치 벡 터 ( weight vector ) 의 크 기 를 제 어 하 는 것 이 외 에 , 드 롭 아 웃 ( dropout ) 은 오 버 피 팅 ( overﬁtting ) 을 해 결 하 는 또 다 른 방법 입 니다 . 이 세 가 지 는 종종 함 께 사 용 됩 니다 . • 드 롭 아 웃 ( dropout ) 은 ℎ 를 같 은 기 대 값 ℎ 를 갖 는 확 률 변 수 ℎ ′ 로 드 롭 아 웃 ( dropout ) 확 률 𝑝 만 큼 바 꾸 는 것 입 니다 . • 드 롭 아 웃 ( dropout ) 은 학 습 에 만 적 용 합 니다 . 5 . 13 . 10 문 제 1 . 층 1 과 2 에 서 드 롭 아 웃 ( dropout ) 확 률 값 을 바 꾸 면 서 그 결과 를 관 찰 해 보 세 요 . 특 히 , 두 층 에 대 한 드 롭 아 웃 ( dropout ) 확 률 을 동 시 에 바 꾸 면 어 떻 게 될 까 요 ? 2 . 에 포 크 ( epoch ) 수 를 늘 리 면 서 드 롭 아 웃 ( dropout ) 을 적 용 할 때 와 적 용 하 지 않 을 때 의 결과 를 비 교 해 보 세 요 . 3 . 드 롭 아 웃 ( dropout ) 을 적 용 한 후 , 활 성 화 ( activation ) 확 률 변 수 의 편 차 를 계 산 해 보 세 요 . 4 . 왜 일 반 적 으 로 드 롭 아 웃 ( dropout ) 을 사 용 하 지 않아야 하 나 요 ? 5 . 은 닉 층 유 닛 ( hideen layer unit ) 을 추 가 하 는 것 처 럼 모 델 의 복 잡 도 를 높 이 는 변 경 을 할 때 , 드 롭 아 웃 ( dropout ) 을 사 용 하 는 효 과가 오 버 피 팅 ( overﬁtting ) 문 제 를 해 결 하 는 더 확 실 한 가 요 ? 5 . 13 . 드 롭 아 웃 ( dropout ) 197 6 . 위 예 제 를 이 용 해 서 드 롭 아 웃 ( dropout ) 과 가 중 치 감 쇠 ( weight decay ) 효 과 를 비 교 해 보 세 요 . 7 . 활 성 화 결과가 아 니 라 가 중 치 행 렬 ( weight matrix ) 의 각 가 중 치 에 적 용 하 면 어 떻 게 될 까 요 ? 8 . [ 0 , 𝛾 / 2 , 𝛾 ] 에 서 추출 한 값 을 갖 도 록 드 롭 아 웃 ( dropout ) 을 바 꿔 보 세 요 . 이 진 드 롭 아 웃 ( binary dropout ) 함 수 보 다 더 좋 은 것 을 만 들 어 볼 수 있 나 요 ? 왜 그 런 방법 을 사 용 할 것 인 가 요 ? 왜 아 닌 가 요 ? 5 . 13 . 11 참 고 자 료 [ 1 ] Srivastava , N . , Hinton , G . , Krizhevsky , A . , Sutskever , I . , & Salakhutdinov , R . ( 2014 ) . JMLR 5 . 13 . 12 Scan the QR Code to Discuss 5 . 14 순 전 파 ( forward propagation ) , 역 전 파 ( back propagation ) , 연 산 그 래 프 앞에 서 우 리 는 모 델 을 학 습 시 키 는 방법 으 로 미 니 배 치 확 률 적 경 사 강 하 법 ( stochastic gradient descent ) 최 적 화 알 고 리 즘 을 사 용 했 습 니다 . 이 를 구 현 할 때 , 우 리 는 모 델 의 순 전 파 ( forward propagation ) 을 계 산 하 면 서 입 력 에 대 한 모 델 의 결과 만 을 계 산 했 습 니다 . 그 리 고 , 자 동 으 로 생 성 된 backward 함 수 를 호 출 함 으 로 autograd 을 이 용 해 서 gradient 를 계 산 합 니다 . 역 전 파 ( back - propagation ) 을 이 용 하 는 경 우 자 동 으 로 그 래 디 언 트 ( gradient ) 를 계 산 하 는 함 수 를 이 용 함 으 로 딥 러 닝 학 습 알 고 리 즘 구 현 이 굉 장 히 간 단 해 졌 습 니다 . 이 절 에 서 는 순 전 파 ( forward propagation ) 와 역 전 파 ( back propagation ) 를 수 학 적 이 고 연 산 적 인 그 래 프 를 사 용 해 서 설 명 하 겠 습 니다 . 더 정 확 하 게 는 한 개 의 은 닉 층 ( hidden layer ) 를 갖 는 다 층 퍼 셉 트 론 ( multilayer perceptron ) 에 ℓ 2 놈 정 규 화 ( norm regularization ) 를 적 용 한 간 단 한 모 델 을 이 용 해 서 순 전 파 ( forward propagation ) 와 역 전 파 ( back propagation ) 를 설 명 합 니다 . 이 절 은 딥 러 닝 을 수 행할 때 어 떤 일이 일 어 나 고 있 는 지 에 대 해 서 더 잘 이 해할 수 있 도 록 해 줄 것 입 니다 . 198 5 . 딥 러 닝 기 초 5 . 14 . 1 순 전 파 ( forward propagation ) 순 전 파 ( forward propagation ) 은 뉴 럴 네 트 워 크 모 델 의 입 력 층 부 터 출 력 층 까 지 순 서 대 로 변 수 들 을 계 산 하 고 저 장 하 는 것 을 의 미 합 니다 . 지 금 부 터 한 개 의 은 닉 층 ( hidden layer ) 을 갖 는 딥 네 트 워 크 를 예 로 들 어 단 계 별 로 어 떻 게 계 산 되 는 지 설 명 하 겠 습 니다 . 다 소 지 루 할 수 있 지 만 , backward 를 호 출 했 을 때 , 어 떤 일이 일 어 나 는 지 논 의 할 때 도 움 이 될 것 입 니다 . 간 단 하 게 하 기 위 해 서 , 입 력 은 𝑑 차 원 의 실수 공간 x ∈ R 𝑑 으 로 부 터 선 택 되 고 , 편 향 ( bias ) 항 목 은 생 략 하 겠 습 니다 . 중 간 변 수 는 다 음 과 같 이 정 의 됩 니다 . z = W ( 1 ) x W ( 1 ) ∈ R ℎ × 𝑑 은 은 닉 층 ( hidden layer ) 의 가 중 치 파 라 미 터 입 니다 . 중 간 변 수 z ∈ R ℎ 를 활 성 화 함 수 ( activation functino ) 𝜑 에 입 력 해 서 벡 터 길 이 가 ℎ 인 은 닉 층 ( hidden layer ) 변 수 를 얻 습 니다 . h = 𝜑 ( z ) . 은 닉 변 수 h 도 중 간 변 수 입 니다 . 출 력 층 의 가 중 치 W ( 2 ) ∈ R 𝑞 × ℎ 만 을 사 용 한 다 고 가 정 하 면 , 벡 터 길 이 가 𝑞 인 출 력 층 의 변 수 를 다 음 과 같 이 계 산 할 수 있 습 니다 . o = W ( 2 ) h . 손 실 함 수 ( loss function ) 를 𝑙 이 라 고 하 고 , 샘 플 레 이 블 을 𝑦 라 고 가 정 하 면 , 하 나 의 데 이 터 샘 플 에 대 한 손 실 ( loss ) 값 을 다 음 과 같 이 계 산 할 수 있 습 니다 . 𝐿 = 𝑙 ( o , 𝑦 ) . ℓ 2 놈 정 규 화 ( norm regularization ) 의 정 의 에 따 라 서 , 하 이 퍼파 라 미 터 ( hyper - parameter ) 𝜆 가 주 어 졌 을 때 , 정 규 화 ( regularization ) 항 목 은 다 음 과 같 습 니다 . 𝑠 = 𝜆 2 (︁ ‖ W ( 1 ) ‖ 2 𝐹 + ‖ W ( 2 ) ‖ 2 𝐹 )︁ , 여 기 서 행 렬 의 Frobenius norm 은 행 렬 을 벡 터 로 바 꾼 후 계 산 하 는 𝐿 2 놈 ( norm ) 과 같 습 니다 . 마 지 막 으 로 , 한 개 의 데 이 터 샘 플 에 대 한 모 델 의 정 규 화 된 손 실 ( regularized loss ) 값 을 계 산 합 니다 . 𝐽 = 𝐿 + 𝑠 . 5 . 14 . 순 전 파 ( forward propagation ) , 역 전 파 ( back propagation ) , 연 산 그 래 프 199 𝐽 를 주 어 진 데 이 터 샘 플 에 대 한 목 표 함 수 ( objective function ) 라 고 하 며 , 앞 으 로 이 를 ’ 목 표 함 수 ( ob - jective function ) ’ 라 고 하 겠 습 니다 . 5 . 14 . 2 순 전 파 ( forward propagation ) 의 연 산 그 래 프 연 산 그 래 프 를 도 식 화 하 면 연 산 에 포 함 된 연 산 자 와 변 수 들 사 이의 관 계 를 시 각 화 하 는 데 도 움 이 됩 니 다 . 아 래 그 림 은 위 에 서 정 의 한 간 단 한 네 트 워 크 의 그 래 프 입 니다 . 왼 쪽 아 래 는 입 력 이 고 , 오 른 쪽 위 는 출 력 입 니다 . 데 이 터 의 흐 름 을 표 시 하 는 화 살 표 의 방 향 이 오 른 쪽 과 위 로 향해 있 습 니다 . 5 . 14 . 3 역 전 파 ( back propagation ) 역 전 파 ( back propagation ) 는 뉴 럴 네 트 워 크 의 파 라 미 터 들 에 대 한 그 래 디 언 트 ( gradient ) 를 계 산 하 는 방 법 을 의 미 합 니다 . 일 반 적 으 로 는 역 전 파 ( back propagation ) 은 뉴 럴 네 트 워 크 의 각 층 과 관 련 된 목 적 함 수 ( objective function ) 의 중 간 변 수 들 과 파 라 미 터 들 의 그 래 디 언 트 ( gradient ) 를 출 력 층 에 서 입 력 층 순 으 로 계 산 하 고 저 장 합 니다 . 이 는 미 적 분 의 ’ 체 인 룰 ( chain rule ) ’ 을 따 르 기 때 문 입 니다 . 임의의 모 양 을 갖 는 입 력 과 출 력 텐 서 ( tensor ) X , Y , Z 들 을 이 용 해 서 함 수 Y = 𝑓 ( X ) 와 Z = 𝑔 ( Y ) = 𝑔 ∘ 𝑓 ( X ) 를 정 의 했 다 고 가 정 하 고 , 체 인 룰 ( chain rule ) 을 사 용 하 면 , X 에 대 한 Z 의 미 분 은 다 음 과 같 이 정 의 됩 니다 . 𝜕 Z 𝜕 X = prod (︂ 𝜕 Z 𝜕 Y , 𝜕 Y 𝜕 X )︂ . 여 기 서 prod 연 산 은 전 치 ( transposotion ) 나 입 력 위 치 변 경과 같 이 필 요 한 연 산 을 수 항한 후 곱 을 수 행 하 는 것 을 의 미 합 니다 . 벡 터 의 경 우 에 는 이 것 은 직 관 적 입 니다 . 단 순 히 행 렬 - 행 렬 곱 셈 이 고 , 고 차 원 의 텐 서 의 경 우 에 는 새 로 대 응 하 는 원 소 들 간 에 연 산 을 수 행합 니다 . prod 연 산 자 는 이 모 든 복 잡 한 개 념 을 감 춰 주 는 역 할 을 합 니다 . 하 나 의 은 닉 층 ( hidden layer ) 를 갖 는 간 단 한 네 트 워 크 의 파 라 매 터 는 W ( 1 ) 와 W ( 2 ) 이 고 , 역 전 파 ( back propagation ) 는 미 분 값 𝜕𝐽 / 𝜕 W ( 1 ) 와 𝜕𝐽 / 𝜕 W ( 2 ) 를 계 산 하 는 것 입 니다 . 이 를 위 해 서 우 리 는 체 인 룰 ( chain rule ) 을 적 용 해 서 각 중 간 변 수 와 파 라 미 터 에 대 한 그 래 디 언 트 ( gradient ) 를 계 산 합 니다 . 연 산 200 5 . 딥 러 닝 기 초 그 래 프 의 결과 로 부 터 시 작 해 서 파 라 미 터 들 에 대 한 그 래 디 언 트 ( gradient ) 를 계 산 해 야 하 기 때 문 에 , 순 전 파 ( forward propagation ) 와 는 반 대 방 향 으 로 연 산 을 수 행합 니다 . 첫 번 째 단 계 는 손 실 ( loss ) 항 목 𝐿 과 정 규 화 ( regularization ) 항 목 𝑠 에 대 해 서 목 적 함 수 ( objective function ) 𝐽 = 𝐿 + 𝑠 의 그 래 디 언 트 ( gradient ) 를 계 산 하 는 것 입 니다 . 𝜕𝐽 𝜕𝐿 = 1 and 𝜕𝐽 𝜕𝑠 = 1 그 다 음 , 출 력 층 𝑜 의 변 수 들 에 대 한 목 적 함 수 ( objective function ) 의 그 래 디 언 트 ( gradient ) 를 체 인 룰 ( chain rule ) 을 적 용 해 서 구 합 니다 . 𝜕𝐽 𝜕 o = prod (︂ 𝜕𝐽 𝜕𝐿 , 𝜕𝐿 𝜕 o )︂ = 𝜕𝐿 𝜕 o ∈ R 𝑞 이 제 두 파 라 메 터 에 대 해 서 정 규 화 ( regularization ) 항 목 의 그 래 디 언 트 ( gradient ) 를 계 산 합 니다 . 𝜕𝑠 𝜕 W ( 1 ) = 𝜆 W ( 1 ) and 𝜕𝑠 𝜕 W ( 2 ) = 𝜆 W ( 2 ) 이 제 우 리 는 출 력 층 와 가 장 가 까 운 모 델 파 라 미 터 들 에 대 해 서 목 적 함 수 ( objective function ) 의 그 래 디 언 트 ( gradient ) 𝜕𝐽 / 𝜕 W ( 2 ) ∈ R 𝑞 × ℎ 를 계 산 할 수 있 습 니다 . 체 인 룰 ( chain rule ) 을 적 용 하 면 다 음 과 같 이 계 산 됩 니다 . 𝜕𝐽 𝜕 W ( 2 ) = prod (︂ 𝜕𝐽 𝜕 o , 𝜕 o 𝜕 W ( 2 ) )︂ + prod (︂ 𝜕𝐽 𝜕𝑠 , 𝜕𝑠 𝜕 W ( 2 ) )︂ = 𝜕𝐽 𝜕 o h ⊤ + 𝜆 W ( 2 ) W ( 1 ) 에 대 한 그 래 디 언 트 ( gradient ) 를 계 산 하 기 위 해 서 , 출 력 층 으 로 부 터 은 닉 층 까 지 역 전 파 ( back propagation ) 를 계 속 해 야 합 니다 . 은 닉 층 ( hidden layer ) 변 수 에 대 한 그 래 디 언 트 ( gradient ) 𝜕𝐽 / 𝜕 h ∈ R ℎ 는 다 음 과 같 습 니다 . 𝜕𝐽 𝜕 h = prod (︂ 𝜕𝐽 𝜕 o , 𝜕 o 𝜕 h )︂ = W ( 2 ) ⊤ 𝜕𝐽 𝜕 o . 활 성 화 함 수 ( activation function ) 𝜑 는 각 요 소 별 로 적 용 되 기 때 문 에 , 중 간 변 수 z 에 대 한 그 래 디 언 트 ( gradient ) 𝜕𝐽 / 𝜕 z ∈ R ℎ 를 계 산 하 기 위 해 서 는 요 소 별 곱 하 기 ( element - wise multiplication ) 연 산 자 를 사 용 해 야 합 니다 . 우 리 는 이 연 산 을 ⊙ 로 표 현 하 겠 습 니다 . 𝜕𝐽 𝜕 z = prod (︂ 𝜕𝐽 𝜕 h , 𝜕 h 𝜕 z )︂ = 𝜕𝐽 𝜕 h ⊙ 𝜑 ′ ( z ) . 마 지 막 으 로 , 입 력 층 과 가 장 가 까 운 모 델 파 라 미 터 에 대 한 그 래 디 언 트 ( gradient ) 𝜕𝐽 / 𝜕 W ( 1 ) ∈ R ℎ × 𝑑 를 5 . 14 . 순 전 파 ( forward propagation ) , 역 전 파 ( back propagation ) , 연 산 그 래 프 201 체 인 룰 ( chain rule ) 을 적 용 해 서 다 음 과 같 이 계 산 합 니다 . 𝜕𝐽 𝜕 W ( 1 ) = prod (︂ 𝜕𝐽 𝜕 z , 𝜕 z 𝜕 W ( 1 ) )︂ + prod (︂ 𝜕𝐽 𝜕𝑠 , 𝜕𝑠 𝜕 W ( 1 ) )︂ = 𝜕𝐽 𝜕 z x ⊤ + 𝜆 W ( 1 ) . 5 . 14 . 4 모 델 학 습시 키 기 네 트 워 크 를 학 습시 킬 때 , 순 전 파 ( forward propagation ) 과 역 전 파 ( backward propagation ) 은 서 로 의 존 하 는 관 계 입 니다 . 특 히 역 전 파 ( forward propagation ) 는 연 관 되 는 관 계 를 따 라 서 그 래 프 를 계 산 하 고 , 그 경 로 의 모 든 변 수 를 계 산 합 니다 . 이 것 들 은 연 산 이 반 대 방 향 인 역 전 파 ( back propagation ) 에 서 다 시 사 용 됩 니다 . 그 결과 중 에 하 나 로 역 전 파 ( back propagation ) 을 완 료 할 때 까 지 중 간 값 들 을 모 두 가 지 고 있 어야 하 는 것 이 있 습 니다 . 이 것 이 역 전 파 ( back propagation ) 가 단 순 예 측 을 수 행할 때 보 다 훨 씬 더 많 은 메모 리를 사 용 하 는 이유 들 중 에 하 나 입 니다 . 즉 , 체 인 룰 ( chain rule ) 을 적 용 하 기 위 해 서 모 든 중 간 변 수 를 저 장 하 고 있 어야 , 그 래 디 언 트 ( gradient ) 인 텐 서 ( tensor ) 들 을 계 산 할 수 있 습 니다 . 메모 리를 더 많 이 사 용 하 는 다 른 이유 는 모 델 을 학 습 시 킬 때 미 니 배 치 형 태 로 하 기 때 문 에 , 더 많 은 중 간 활 성 화 ( activation ) 들 을 저 장 해 야 하 는 것 이 있 습 니다 . 5 . 14 . 5 요 약 • 순 전 파 ( forwards propagation ) 은 뉴 럴 네 트 워 크 의 그 래 프 를 계 산 하 기 위 해 서 중 간 변 수 들 을 순 서 대 로 계 산 하 고 저 장 합 니다 . 즉 , 입 력 층 부 터 시 작 해 서 출 력 층 까 지 처 리 합 니다 . • 역 전 파 ( back propagation ) 은 중 간 변 수 와 파 라 미 터 에 대 한 그 래 디 언 트 ( gradient ) 를 반 대 방 향 으 로 계 산 하 고 저 장 합 니다 . • 딥 러 닝 모 델 을 학 습시 킬 때 , 순 전 파 ( forward propagation ) 과 역 전 파 ( back propagation ) 는 상 호 의 존적 입 니다 . • 학 습 은 상 당 히 많 은 메모 리 와 저 장 공간 을 요 구 합 니다 . 5 . 14 . 6 문 제 1 . 입 력 x 가 행 렬 이 라 고 가 정 하 면 , 그 래 디 언 트 ( gradient ) 의 차 원 이 어 떻 게 되 나 요 ? 2 . 이 절 에 서 설 명 한 모 델 의 은 닉 층 ( hidden layer ) 에 편 향 ( bias ) 을 추 가 하 고 , • 연 산 그 래 프 를 그 려 보 세 요 • 순 전 파 ( forward propagation ) 와 역 전 파 ( backward propagation ) 공 식 을 유 도 해 보 세 요 . 202 5 . 딥 러 닝 기 초 3 . 이 절 에 사 용 한 모 델 에 대 해 서 학 습 과 예 측 에 사 용 되 는 메모 리 양 을 계 산 해 보 세 요 . 4 . 2 차 미 분 을 계 산 을 해 야 한 다 고 가 정 합 니다 . 그 래 프 연 산 에 어 떤 일이 생 길까 요 ? 좋 은 아 이 디 어 인 가 요 ? 5 . 연 산 그 래 프 가 사 용 중 인 GPU 에 비 해 서 너 무 크 다 고 가 정 합 니다 . • 한 개 이 상 의 GPU 로 나 눌 수 있 나 요 ? • 작은 미 니 배 치 로 학 습 을 할 경 우 장 점 과 단 점 이 무 엇 인 가 요 ? 5 . 14 . 7 Scan the QR Code to Discuss 5 . 15 수 치 안 정 성 ( numerical stability ) 및 초 기 화 지 금까 지 우 리 는 다 층 퍼 셉 트 론 ( multilayer perception ) 을 구 현 하 는 데 필 요 한 도 구 , 회 귀 와 분 류 의 문 제 를 어 떻 게 풀 수 있 는 지 , 그 리 고 모 델 의 용 량 을 어 떻 게 제 어 해 야 하 는 지 에 대 해 서 다 뤘 습 니다 . 하 지 만 , 파 라 미 터 의 초 기 화 는 당 연 한 것 으 로 간 주 하 면 서 , 특 별 히 중 요 하 지 않 은 것 으 로 단 순 하 게 가 정 했 습 니 다 . 이 절 에 서 는 이 것 들 에 대 해 서 자 세 히 살 펴 보 고 , 유 용 한 경 험 적 방법 론 에 대 해 서 논 의 하 겠 습 니다 . 두 번 째 로 우 리 는 활 성 화 함 수 ( activation function ) 선 택 에 큰 관 심 을 두 지 않았 습 니다 . 실 제 로 얕 은 네 트 워 크 에 서 는 크 게 중 요 하 지 않 지 만 , 딥 네 트 워 크 ( deep network ) 에 서 는 비 선 형 성 과 초 기 화 의 선 택 이 최 적 화 알 고 리 즘 을 빠 르 게 수 렴 시 키 는 데 중 요 한 역 할 을 합 니다 . 이 이 슈 들 을 중 요 하 게 생 각 하 지 않 으 면 그 래 디 언 트 소 실 ( vanishing ) 또 는 폭 발 ( exploding ) 이 발 생 할 수 있 습 니다 . 5 . 15 . 1 그 래 디 언 트 소 실 ( vanishing ) 과 폭 발 ( exploding ) 입 력 이 x , 출 력 이 o 이 고 𝑑 층 을 갖 는 딥 네 트 워 크 를 예 로 들 겠 습 니다 . 각 층 은 다 음을 만 족 합 니다 . h 𝑡 + 1 = 𝑓 𝑡 ( h 𝑡 ) 이 고 , 따 라 서 o = 𝑓 𝑑 ∘ . . . ∘ 𝑓 1 ( x ) 5 . 15 . 수 치 안 정 성 ( numerical stability ) 및 초 기 화 203 모 든 활 성 화 ( activation ) 들 과 입 력 들 이 벡 터 인 경 우 , 𝑡 번 째 층 의 함 수 𝑓 𝑡 와 관 련 된 파 라 미 터 W 𝑡 의 임의의 세 트 에 대 한 o 의 그 래 디 언 트 ( gradient ) 는 다 음 과 같 이 표 현 됩 니다 . 𝜕 W 𝑡 o = 𝜕 h 𝑑 − 1 h 𝑑 ⏟ ⏞ : = M 𝑑 · . . . · 𝜕 h 𝑡 h 𝑡 + 1 ⏟ ⏞ : = M 𝑡 𝜕 W 𝑡 h 𝑡 ⏟ ⏞ : = v 𝑡 . 다 르 게 말 하 면 , 위 공 식 은 𝑑 − 𝑡 개 의 행 렬 M 𝑑 · . . . · M 𝑡 과 그 래 디 언 트 ( gradient ) 벡 터 v 𝑡 의 곱 입 니다 . 너 무 많 은 확 률 을 곱 할 때 산 술 적 인 언 더 플 로 우 ( underﬂow ) 를 경 험할 때 와 비 슷 한 상 황 이 발 생 합 니다 . 이 문 제 를 로 그 공간 으 로 전 환 시 켜 서 , 즉 문 제 를 가 수 ( mantissa ) 에 서 수 치 표 현 의 지 수 로 이 동 시 켜 서 완 화 할 수 있 었 습 니다 . 처 음 에 행 렬 들 𝑀 𝑡 은 다 양 한 고 유 값 ( eigenvalue ) 들 을 갖 을 것 입 니다 . 어 떤 것 들 은 작을 수 도 , 어 떤 것 은 클 수 도 있 습 니다 . 특 히 그 것 들 의 곱 이 아 주 크 거 나 아 주 작을 수 도 있 습 니다 . 이 것 은 수 치 적 인 표 현 의 문 제 일 뿐 만 아 니 라 최 적 화 알 고 리 즘 이 수 렴 되 지 않 을 수 있 다는 것 을 의 미 합 니다 . 아 주 큰 그 래 디 언 트 ( gradient ) 가 되 거 나 , 너 무 조 금 씩 업 데 이 트 가 되 기 도 합 니다 . 앞 의 경 우 에 는 , 파 라 미 터 가 너 무 커 질 것 이 고 , 후 자의 경 우 에 는 그 래 디 언 트 소 실 ( vanishing gradient ) 이 되 어 버 려 서 더 이 상 의 미 있 는 진 척 을 만 들 어 내 지 못 하 게 됩 니다 . 그 래 디 언 트 폭 발 ( exploding gradient ) 좀 더 자 세 히 설 명 해 보 겠 습 니다 . 하 나 의 행 렬 을 선 택 한 후 100 개 의 가 우 시 안 랜 덤 행 렬 을 선 택 해 서 모 두 곱 합 니다 . 우 리 가 선 택 한 스 캐 일 링 으 로 인 해 서 행 렬 의 곱 은 너 무 커 지 게 됩 니다 . 이 러 한 일이 딥 네 트 워 크 에 서 발 생 한 다 면 , 알 고 리 즘 을 수 렴 하 게 만 들 기 어 려 워 집 니다 . [ 1 ] : % matplotlib inline import mxnet as mx from mxnet import nd , autograd from matplotlib import pyplot as plt M = nd . random . normal ( shape = ( 4 , 4 ) ) print ( ' A single matrix ' , M ) for i in range ( 100 ) : M = nd . dot ( M , nd . random . normal ( shape = ( 4 , 4 ) ) ) print ( ' After multiplying 100 matrices ' , M ) A single matrix [ [ 2 . 2122064 0 . 7740038 1 . 0434405 1 . 1839255 ] [ 1 . 8917114 - 1 . 2347414 - 1 . 771029 - 0 . 45138445 ] [ 0 . 57938355 - 1 . 856082 - 1 . 9768796 - 0 . 20801921 ] [ 0 . 2444218 - 0 . 03716067 - 0 . 48774993 - 0 . 02261727 ] ] ( continues on next page ) 204 5 . 딥 러 닝 기 초 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) < NDArray 4x4 @ cpu ( 0 ) > After multiplying 100 matrices [ [ 3 . 1575275e + 20 - 5 . 0052276e + 19 2 . 0565092e + 21 - 2 . 3741922e + 20 ] [ - 4 . 6332600e + 20 7 . 3445046e + 19 - 3 . 0176513e + 21 3 . 4838066e + 20 ] [ - 5 . 8487235e + 20 9 . 2711797e + 19 - 3 . 8092853e + 21 4 . 3977330e + 20 ] [ - 6 . 2947415e + 19 9 . 9783660e + 18 - 4 . 0997977e + 20 4 . 7331174e + 19 ] ] < NDArray 4x4 @ cpu ( 0 ) > 그 래 디 언 트 소 실 ( vanishing gradient ) 반 대 의 문 제 인 그 래 디 언 트 소 실 ( vanishing gradient ) 도 나 쁜 경 우 입 니다 . 주 요 원 인 중 의 하 나 는 각 층 의 선 형 연 산 과 함 께 엮 이 는 활 성 화 함 수 ( activation function ) 𝜎 입 니다 . 역 사 적 으 로 는 Multilayer Perceptrons 절 에 서 소 개 했 던 sigmoid 함 수 ( 1 + exp ( − 𝑥 ) ) 가 유 명 한 활 성 화 함 수 ( activation function ) 였 습 니다 . 이 함 수 를 비 선 형 활 성 화 함 수 ( activation function ) 로 사 용 했 을 때 문 제 가 될 수 있 는 지 를 보 기 위 해 서 , 이 함 수 에 대 해 서 간 단 하 게 살 펴 보 겠 습 니다 . [ 2 ] : x = nd . arange ( - 8 . 0 , 8 . 0 , 0 . 1 ) x . attach _ grad ( ) with autograd . record ( ) : y = x . sigmoid ( ) y . backward ( ) plt . figure ( figsize = ( 8 , 4 ) ) plt . plot ( x . asnumpy ( ) , y . asnumpy ( ) ) plt . plot ( x . asnumpy ( ) , x . grad . asnumpy ( ) ) plt . legend ( [ ' sigmoid ' , ' gradient ' ] ) plt . show ( ) 5 . 15 . 수 치 안 정 성 ( numerical stability ) 및 초 기 화 205 위 그 림 에 서 보 이 는 것 처 럼 sigmoid 의 그 래 디 언 트 는 아 주 큰 수 나 아 주 작은 수 에 서 소 멸 합 니다 . 체 인 룰 ( chain rule ) 로 인 해 서 , 활 성 화 ( activation ) 들 이 [ − 4 , 4 ] 범 위 에 들 어 가 지 않 지 않 으 면 전 체 곱 의 그 래 디 언 트 ( gradient ) 는 소 멸 될 수 있 다는 것 을 의 미 합 니다 . 층 을 많 이 사 용 하 는 경 우 , 이 현 상 이 어 떤 층 에 서 일 어 날 가 능 성 이 높 습 니다 . ReLU max ( 0 , 𝑥 ) 가 소 개 되 기 전 까 지 는 이 문 제 가 딥 네 트 워 크 학 습 의 단 점 이 었 습 니다 . 그 결과 ReLU 가 활 성 화 ( activation ) 로 설 계 할 때 기 본 선 택 이 되 었 습 니다 . 대 칭 성 딥 네 트 워 크 디 자인의 마 지 막 문 제 는 파 라 미 터 화 에 내 재 된 대 칭 입 니다 . 두 개 의 은 닉 유 닛 ( hidden unit ) , ℎ 1 and ℎ 2 을 갖 는 한 개 의 은 닉 층 ( hidden layer ) 를 가 지 고 있 는 딥 네 트 워 크 를 가 정 하 겠 습 니다 . 이 경 우 , 첫 번 째 층 의 가 중 치 W 1 를 뒤 집 고 , 두 번 째 층 의 결과 도 뒤 집 을 경 우 , 동 일 한 함 수 를 얻 게 됩 니다 . 좀 더 일 반 적 으 로 각 층 의 은 닉 유 닛 ( hidden unit ) 간 에 는 치 환 대 칭 성 ( permutation symmetry ) 이 존 재 합 니다 . 이 것 은 이 론 적 으 로 만 말 썽 이 되 는 것 이 아 닙니다 . 어 떤 층 의 파 라 미 터 를 모 두 0 으 로 초 기 화 를 하 거 나 ( W 𝑙 = 0 ) , W 𝑙 의 모 든 값 을 동 일 하 게 설 정 한 다 고 가 정 합 니다 . 이 경 우 , 모 든 차 원 의 그 래 디 언 트 ( gradient ) 들 이 같게 되 고 , 주 어 진 층 에 내 재 된 표 현 력 을 전 혀 사 용 할 수 없 게 됩 니다 . 사 실 , 그 은 닉 층 ( hidden layer ) 는 단 일 유 닛 ( single unit ) 처 럼 동 작 합 니다 . 5 . 15 . 2 파 라 미 터 초 기 화 위 문 제 를 해 결 하 거 나 최 소 한 완 화 시 키 는 방법 은 가 중 치 벡 터 의 초 기 화 를 조 심 하 게 하 는 것 입 니다 . 이 렇 게 해 서 적 어 도 초 기 의 그 래 디 언 트 ( gradient ) 가 소 멸 되 지 않 게 하 고 , 네 트 워 크 가 중 치 들 이 너 무 206 5 . 딥 러 닝 기 초 커 지지 않 게 합 리 적 인 범 위 에 존 재 하 게 할 수 있 습 니다 . 최 적 화 에 서 의 추 가 적 조 치 나 적 합한 정 규 화 ( regularization ) 을 통 해 서 너 무 나 빠 지 는 것 을 막 을 수 있 습 니다 . 이 제 방법 들 에 대 해 서 알아 보 겠 습 니다 . 기 본 초 기 화 “Concise Implementation of Linear Regression” 절 에 서 우 리 는 net . initialize ( init . Normal ( sigma = 0 . 01 ) ) 을 이 용 해 서 가 중 치 의 초 기 값 으 로 정 규 분 포 에 서 임의의 수 선 택 하 는 방법 을 사 용 했 습 니다 . 초 기 화 방법 을 명 시 하 지 않 은 경 우 , 즉 , net . initialize ( ) 를 호 출 하 는 경 우 에 MXNet 은 기 본 랜 덤 초 기 화 방법 을 적 용 합 니다 . 이 는 , 가 중 치 의 각 원 소 는 𝑈 [ − 0 . 07 , 0 . 07 ] 범 위의 균 일 분 포 에 서 선 택 된 값 을 갖고 , 편 향 ( bias ) 파 라 미 터 는 모 두 0 으 로 설 정 됩 니다 . 일 반 적 인 문 제 의 크 기 에 서 이 두 방법 은 상 당 히 잘 작 동 합 니다 . Xavier 초 기 화 어 떤 층 의 은 닉 유 닛 ( hidden unit ) ℎ 𝑖 에 적 용 된 활 성 화 ( activation ) 의 범 위 분 포 를 살 펴 보 겠 습 니다 . 이 값 들 은 다 음 과 같 이 계 산 됩 니다 . ℎ 𝑖 = 𝑛 in ∑︁ 𝑗 = 1 𝑊 𝑖𝑗 𝑥 𝑗 가 중 치 𝑊 𝑖𝑗 들 은 같 은 분 포 에 서 서 로 독 립 적 으 로 선 택 됩 니다 . 이 분 포 는 평 균 이 0 이 고 분 산 이 𝜎 2 라 고 가 정 하 겠 습 니다 . ( 하 지 만 , 이 분 포 가 가 우 시 안 ( Gaussian ) 이 어야 한 다는 것 은 아 니 고 , 단 지 평 균 과 분 산 이 필 요 할 뿐 입 니다 . ) 층 의 입 력 𝑥 𝑗 를 제 어 할 수 있 는 방법 이 없 지 만 , 그 값 들 의 평 균 이 0 이 고 분 산 이 𝛾 2 이 고 , W 과 는 독 립 적 이 라 는 다 소 비 현 실 적 인 가 정 을 하 겠 습 니다 . 이 경 우 , ℎ 𝑖 의 평 균 과 분 산 을 다 음 과 같 이 계 산 할 수 있 습 니다 . E [ ℎ 𝑖 ] = 𝑛 in ∑︁ 𝑗 = 1 E [ 𝑊 𝑖𝑗 𝑥 𝑗 ] = 0 E [ ℎ 2 𝑖 ] = 𝑛 in ∑︁ 𝑗 = 1 E [ 𝑊 2 𝑖𝑗 𝑥 2 𝑗 ] = 𝑛 in ∑︁ 𝑗 = 1 E [ 𝑊 2 𝑖𝑗 ] E [ 𝑥 2 𝑗 ] = 𝑛 in 𝜎 2 𝛾 2 5 . 15 . 수 치 안 정 성 ( numerical stability ) 및 초 기 화 207 𝑛 in 𝜎 2 = 1 을 적 용 하 면 분 산 을 고 정 시 킬 수 있 습 니다 . 이 제 back propagation 을 고 려 해 봅 니다 . 가 장 상 위 층 들 로 부 터 전 달 되 는 그 래 디 언 트 ( gradient ) 와 함 께 비 슷 한 문 제 를 만 나 게 됩 니다 . 즉 , Ww 대 신 에 W ⊤ g 를 다 뤄 야 합 니다 . 여 기 서 g 는 상 위 층 으 로 부 터 전 달 되 는 그 래 디 언 트 ( gradient ) 를 의 미 합 니다 . 포 워 드 프 로 퍼 게 이 션 ( forward propagation ) 에 서 와 같 은 논 리 로 , 𝑛 out 𝜎 2 = 1 이 아 닐 경 우 에 는 그 래 디 언 트 ( gradient ) 의 분 산 이 너 무 커 질 수 있 습 니다 . 이 상 황 이 우 리를 다 음 과 같 은 딜 레 마 에 빠 지 게 합 니다 . 즉 , 우 리 는 두 조 건 을 동 시 에 만 족 시 킬 수 없 습 니다 . 대 신 , 다 음은 조 건 은 쉽 게 만 족 시 킬 수 있 습 니다 . 1 2 ( 𝑛 in + 𝑛 out ) 𝜎 2 = 1 또 는 동 일 하 게 𝜎 = √︂ 2 𝑛 in + 𝑛 out 이 것 이 2010 년 에 Xavier Glorot and Yoshua Bengio 이 제 안 한 Xavier 초 기 화 의 기 본 이 되 는 논 리 입 니다 . 이 방법 은 실 제 로 충 분 이 잘 작 동 합 니다 . 가 우 시 안 ( Gaussian ) 확 률 변 수 에 서 Xavier 초 기 화 는 평 균 이 0 이 고 분 산 이 𝜎 2 = 2 / ( 𝑛 in + 𝑛 out ) 인 정 규 분 포 에 서 값 을 선 택 합 니다 . 𝑈 [ − 𝑎 , 𝑎 ] 에 균 등 하 게 분 포 한 확 률 변 수 의 경 우 , 분 산 이 𝑎 2 / 3 이 됩 니다 . 𝑎 2 / 3 을 𝜎 2 에 대 한 조 건 에 대 입 하 면 다 음 과 같 은 분 포 의 초 기 화 를 할 수 있 습 니다 . 𝑈 [︁ − √︀ 6 / ( 𝑛 in + 𝑛 out ) , √︀ 6 / ( 𝑛 in + 𝑛 out ) ]︁ . 그 외 의 것 들 위 내 용 은 아 주 일 부 입 니다 . MXNet 의 mxnet . initializer 모 듈 은 10 가 지 이 상 의 경 험 적 방법 들 을 제 공 합 니다 . 이 방법 들 은 슈 퍼 해 상 도 ( superresolution ) , 시 퀀 스 ( sequence ) 모 델 또 는 관 련 된 문 제 들 에 서 파 라 미 터 들 이 연 관 된 경 우 에 사 용 할 수 있 습 니다 . 이 모 듈 이 제 공 하 는 것 들 을 살 펴 보 는 것 을 권 장 합 니다 . 5 . 15 . 3 요 약 • 그 래 디 언 트 소 멸 ( Vanishing gradient ) 와 그 래 디 언 트 폭 발 ( exploding gradient ) 은 아 주 깊 은 네 트 워 크 에 서 발 생 하 는 흔 한 문 제 입 니다 . 이 를 위 해 서 그 래 디 언 트 ( gradient ) 와 파 라 미 터 가 잘 통 제 되 도 록 하 는 것 이 중 요 합 니다 . • 초 기 화 방법 은 최 소 한 초 기 의 그 래 디 언 트 ( gradient ) 들 이 너 무 커 지 거 나 너 무 작 아 지지 않 도 록 하 는 데 필 요 합 니다 . • ReLU 는 그 래 디 언 트 소 멸 ( vanishing gradient ) 문 제 중 에 하 나 를 해 결 합 니다 . 즉 , 매 우 큰 입 력 에 대 해 서 그 래 디 언 트 ( gradient ) 가 사 라 지 는 것 을 해 결 합 니다 . 이 는 수 렴 을 아 주 빠 르 게 가 속 화 해 줍 니다 . 208 5 . 딥 러 닝 기 초 • 랜 덤 초 기 화 는 최 적 화 를 수 행하 기 전 대 칭 을 깨 주 는 데 중 요 합 니다 . 5 . 15 . 4 문 제 1 . 치 환 대 칭 성 ( permutation symmetry ) 이 외 에 대 칭 성 ( symmetry ) 를 깨 는 다 른 사 례 를 디 자인 할 수 있 나 요 ? 2 . 선 형 회 귀 나 softmax 회 귀 에 서 모 든 가 중 치 파 라 미 터 를 같 은 값 으 로 초 기 화 할 수 있 나 요 ? 3 . 두 행 렬 의 곱 에 서 고 유 값 ( eigenvalue ) 들 의 해 석 적 범 위 ( analytic bound ) 를 찾 아 보 세 요 . 그 래 디 언 트 ( gradient ) 들 을 잘 제 어 되 도 록 하 는 것 에 대 해 서 어 떤 것 을 알 수 있 나 요 ? 4 . 어 떤 항 들 의 값 이 커 지 고 있 는 것 을 알 게 된 경 우 , 이 후 에 이 를 고 칠 수 있 나 요 ? You , Gitman and Ginsburg , 2017 의 LARS 논 문 을 참 고 해 보 세 요 . 5 . 15 . 5 Scan the QR Code to Discuss 5 . 16 환 경 지 금까 지 우 리 는 데 이 터 가 어 디 서 왔 는 지 모 델 이 어 떻 게 배 포 되 는 지 에 대 해 서 는 걱 정 하 지 않았 습 니다 . 하 지 만 , 이 것 들 을 고 려 하 지 않 는 것 은 문 제 가 됩 니다 . 실 패 한 많 은 머 신 러 닝 배 포 들 의 원 인을 추 적 해 보 면 이 런 상 황 이 원 인이 됩 니다 . 이 절 에 서 는 이 상 황 을 초 기 에 발 견 하 고 , 완 화 하 는 방법 을 알아 봅 니다 . 상 황 에 따 라 서 , 정 확 한 데 이 터 를 사 용 하 면 되 는 다 소 간 단 한 문 제 일 수 있 기 도 하 지 만 , 강 화 학 습 시스 템 을 만 드 는 것과 같 이 어 려 운 문 제 이 기 도 합 니다 . 5 . 16 . 1 공 변 량 변 화 ( covariate shift ) 이 해하 는 것 은 쉽 지 만 , 놓 치 기 쉬 운 문 제 가 있 습 니다 . 강 아 지 와 고 양 을 구 분 하 는 문 제 를 생 각 해 봅 시 다 . 학 습 데 이 터 는 다 음 과 같 이 주 어 졌 습 니다 . 5 . 16 . 환 경 209 고 양 이 고 양 이 강 아 지 강 아 지 210 5 . 딥 러 닝 기 초 테 스 트 에 서 는 다 음 그 림 을 분 류 하 도 록 요 청 받 습 니다 . 5 . 16 . 환 경 211 고 양 이 고 양 이 강 아 지 강 아 지 212 5 . 딥 러 닝 기 초 당 연 하 게 이 것 은 잘 작 동 하 지 않 습 니다 . 학 습 데 이 터 는 실 제 사 진 으 로 구 성 되 어 있 지 만 , 테 스 트 셋 은 만 화 그 림 으 로 되 어 있 습 니다 . 색상 도 정 확 하 지 않 습 니다 . 새 로 운 도 메 인 에 어 떻 게 적 용 할 지 계 획 이 없 이 테 스 트 셋 과 다 른 데 이 터 로 학 습 을 시 키 는 것 은 나 쁜 아 이 디 어 입 니다 . 불 행하 게 도 이 것 은 흔 한 함 정 입 니다 . 통 계 학 자 들 은 이 것 을 공 변 량 변 화 ( covariate shift ) 라 고 합 니다 . 즉 , 공 변 량 ( covariates ) ( 학 습 데 이 터 ) 의 분 포 가 테 스 트 데 이 터 의 분 포 가 다 른 상 황 을 의 미 합 니다 . 수 학 적 으 로 말 하 자 면 , 𝑝 ( 𝑥 ) 는 변 화 하 는 데 , 𝑝 ( 𝑦 | 𝑥 ) 는 그 대 로 있 는 경 우 를 의 미 합 니다 . 5 . 16 . 환 경 213 214 5 . 딥 러 닝 기 초 5 . 16 . 2 개 념 변 화 ( concept shift ) 5 . 16 . 환 경 215 기 계 번 역 시스 템 을 만 든 다 면 , 분 포 𝑝 ( 𝑦 | 𝑥 ) 는 지 역에 따 라 서 다 를 수 있 습 니다 . 이 문 제 를 집 어 내 기 에 는 상 당 이 까 다 롭 습 니다 . 다 행한 것 은 많 은 경 우 에 𝑝 ( 𝑦 | 𝑥 ) 는 조 금 씩 만 변 화 한 다는 것 입 니다 . 더 자 세 히 살 펴 보 기 전 에 , 공 변 량 변 화 ( covariate shift ) 와 개 념 변 화 ( concept shift ) 가 명 백 하 게 드 러 나 지 않 는 많 은 상 황 에 대 해 서 살 펴 보 겠 습 니다 . 5 . 16 . 3 예 제 의 학 분 석 암 을 진 단 하 는 알 고 리 즘 을 설 계 하 는 것 을 상상 해 보 세 요 . 건강 한 사 람 과 아 픈 사 람 의 데 이 터 를 얻 은 후 , 알 고 리 즘 을 학 습시 킵 니다 . 학 습 된 모 델 이 높 은 정 확 도 를 보 여 주 면 서 잘 동 작 합 니다 . 당 신 은 이 제 의 료 분 석 분 야에 서 성 공 적 인 경 력 을 시 작 할 수 있 다 고 판 단 합 니다 . 하 지 만 너 무 이 릅 니다 . 많 은 것 들 이 잘 못 될 수 있 습 니다 . 특 히 , 학 습 에 사 용 한 분 포 와 실 제 분 포 는 상 당 히 다 를 수 있 습 니다 . 실 제 로 수 년 전 에 스 타 트 업 회 사 를 컨 설 팅 하 면 서 겪 었 던 일입 니다 . 이 회 사 는 주 로 나 이 많 은 남 성 에 서 발 견 되 는 질 병 에 대 한 혈 액 테 스 트 를 개 발 하 고 있 었 습 니다 . 이 를 위 해 서 환 자 들 로 부 터 상 당 히 많 은 샘 플 을 수 집 할 수 있 었 습 니다 . 하 지 만 , 윤 리 적 인 이유 로 건강 한 남 자의 혈 액 샘 플 을 구 하 는 것 은 상 당 히 어 려 웠 습 니다 . 이 를 해 결 하 기 위 해 서 , 캠 퍼 스 의 학 생 들 에 게 혈 액 을 기 증 받 아 서 테 스 트 를 수 행했 습 니다 . 그 리 고 , 그 회 사 는 나 에 게 질 병 을 분 류 하 는 모 델 을 만 드 는 것 에 대 한 도 움 을 요 청 했 습 니다 . 거 의 완 벽 한 정 확 도 의 확 률 로 두 데 이 터 셋 을 분 류 하 는 것 은 아 주 쉽 다 고 알 려 줬 습 니다 . 결 국 , 모 든 테 스 트 대 상 은 나 이 , 호 르 몬 레 벨 , 신 체 활 동 , 식 이 상 태 , 알 콜 섭 취 , 그 리 고 질 병 과 연 관 이 없 는 아 주 많 은 요 소 들 이 달 랐 습 니다 . 하 지 만 , 이 는 실 제 환 자의 경 우와 차 이 가 있 습 니다 . 이 들 이 사 용 한 샘 플 링 절 차 는 아 주 심 한 공 변 량 변 화 ( covariate shift ) 를 가 지 고 와 서 , 어 떤 전 통 적 인 방법 으 로 고 쳐 질 수 가 없었 습 니다 . 달 리 말 하 면 , 학 습 데 이 터 와 테 스 트 데 이 터 가 너 무 나 달 라 서 어 떤 유 용 한 일 도 할 수 없었 고 , 결 국 상 당 히 많 은 돈 을 낭 비 만 했 습 니다 . 자율 주 행 자 동 차 자율 주 행 차 를 위 한 머 신 러 닝 시스 템 을 만 들 고 자 하 는 한 회 사 가 있 습 니다 . 도 로 를 탐 지 하 는 것 이 중 요 한 컴 포 넌 트 중 에 하 나 입 니다 . 실 제 답 을 다는 것 이 너 무 비 싸 기 때 문 에 , 게 임 렌 더 링 엔 진 을 사 용 해 서 생 성 한 데 이 터 를 추 가 학 습 데 이 터 로 사 용 하 기 로 했 습 니다 . 이 렇 게 학 습 된 모 델 은 렌 더 링 엔 진 으 로 만 들 어 진 ’ 테 스 트 데 이 터 ’ 에 는 잘 동 작 했 습 니다 . 하 지 만 , 실 제 차 에 서 는 재 앙 이 었 습 니다 . 이유 는 렌 더 링 된 도 로 가 너 무 단 순 한 텍 스 처 를 사 용 했 기 때 문 이 었 습 니다 . 더 중 요 한 것 은 모 든 도 로 경계가 같 은 텍 스 터 로 렌 더 되 었 기 에 , 도 로 탐 지 기 는 이 ’ 특 징 ’ 을 너 무 빨 리 배 워 버 렸 습 니다 . 미 국 군 대 에 서 숲 속 에 서 있 는 탱크 를 탐 지 하 는 것 을 하 려 고 했 을 때 도 비 슷 한 문 제 가 발 생 했 습 니다 . 탱크 가 없 는 숲 의 항 공 사 진 을 찍 고 , 탱크 를 숲 으 로 몰 고 가 서 다 른 사 진 을 찍 었 습 니다 . 이 렇 게 학 습 216 5 . 딥 러 닝 기 초 된 분 류 기 는 아 주 완 벽 하 게 동 작 했 습 니다 . 하 지 만 불 행 히 도 이 모 델 은 그 늘 이 있 는 나 무 들 과 그 늘 이 없 는 나 무 를 구 분 하 고 있 었 습 니다 . 이유 는 첫 번 째 사 진 은 이 른 아 침 에 찍 었 고 , 두 번 째 사 진 은 정 오 에 찍 었 기 때 문 이 었 습 니다 . 정적 이 지 않 은 분 포 ( nonstationary distribution ) 더 알아 내 기 힘 든 상 황 은 분 포 가 천천 히 변 화 하 는 상 황 에 서 모 델 을 적절 하 게 업 데 이 트 를 하 지 않 는 경 우 입 니다 . 전 형 적 인 사 례로 는 다 음 과 같 은 경 우 가 있 습 니다 . • 광 고 모 델 을 학 습시 킨 후 , 자 주 업 데 이 트 하 는 것 을 실 패 한 경 우 . ( 예 를 들 면 , iPad 라 는 새 로 운 디 바 이 스 가 막 출 시 된 것 을 반 영 하 는 것 을 잊은 경 우 ) • 스 팸 필 더 를 만 들 었 습 니다 . 이 스 팸 필 터 는 우 리 가 봤 던 모 든 스 팸 을 모 두 잘 탐 지 합 니다 . 하 지 만 , 스 팸 을 보 내 는 사 람 들 이 이 를 알 고 이 전 에 봐 왔 던 것과 는 아 무 다 른 새 로 운 메 시 지 를 만 듭 니다 . • 상 품 추 천 시스 템 을 만 들 었 습 니다 . 겨 울 에 는 잘 동 작 합 니다 . 하 지 만 , 크 리 스 마 스 가 지 난 후 에 도 산 타 모 자 를 계 속 추 천 하 고 있 습 니다 . 더 많 은 예 제 들 • “ 업 무 에 부 적 합 또 는 안 전 한 ( Not suitable / safe for work ( NSFW ) ) ” 이 미 지 판 별 기 를 만 들 고 있 습 니다 . 쉽 게 하 기 위 해 서 , Subreddit 에 서 이 미 지 를 수 집 합 니다 . 불 행하 게 도 실 제 생 활 데 이 터 에 대 한 정 확 도 는 낮 게 나 옵 니다 . ( Reddit 에 올 라 와 있 는 사 진 은 전 문 사 진 가가 찍 은 품 질 이 좋 은 사 진 들 인 반 면 에 실 제 NSFW 이 미 지 는 품 질 이 좋 지 않 습 니다 . ) • 얼 굴 인 식 기 를 만 듭 니다 . 모 든 밴 치 마 크 에 서 잘 동 작 합 니다 . 하 지 만 , 테 스 트 데 이 터 에 서 는 그 렇 지 못 합 니다 . 실 패 한 이 미 지 를 보 니 이 미 지 전 체 를 얼 굴 이 차 지 하 는 클 로 즈 업 사 진 들 입 니다 . • 미 국 마 켓 을 위 한 웹 검 색 엔 진 을 만 들 어 서 영 국 에 배 포 하 고 싶습 니다 . 요 약 하 면 , 학 습 데 이 터 의 분 포 와 테 스 트 데 이 터 의 분 포 가 다 른 다 양 한 사 례 가 있 습 니다 . 어 떤 경 우 에 는 운 이 좋 아 서 covariate shift 가 있음 에 도 불 구 하 고 모 델 이 잘 동 작 할 수 있 습 니다 . 자 지 금 부 터 원 칙 적 인 해 결 전 략 에 대 해 서 이 야 기 하 겠 습 니다 . 경고 - 약 간 의 수 학 과 통 계가 필 요 합 니다 . 5 . 16 . 4 공 변 량 변 화 ( covariate shift ) 교 정 레 이 블 을 달 아 놓 은 데 이 터 ( 𝑥 𝑖 , 𝑦 𝑖 ) 에 대 한 의 존 도 𝑝 ( 𝑦 | 𝑥 ) 를 추 정 하 는 것 을 한 다 고 가 정 합 니다 . 그 런 데 , 𝑥 𝑖 가 올 바 른 분 포 인 𝑝 ( 𝑥 ) 가 아 닌 다 른 분 포 𝑞 ( 𝑥 ) 를 갖 는 곳 에 서 추출 됩 니다 . 먼 저 , 우 리 는 학 습 과 정 에 정 확 하 게 어 떤 일이 일 어 나 는 지 에 대 해 서 잘 생 각 해 볼 필 요 가 있 습 니다 . 즉 , 학 습 데 이 터 와 5 . 16 . 환 경 217 연 관 된 레 이 블 을 반복 하 면 서 , 매 미 니 배 치 이 후 에 모 델 의 가 중 치 벡 터 ( weight vector ) 들 을 업 데 이 트 합 니다 . 경 우 에 따 라 서 우 리 는 파 라 미 터 에 가 중 치 감 쇠 ( weight decay ) , 드 롭 아 웃 ( dropout ) , 존 아 웃 ( zoneout ) 또 는 유 사 한 패 널 티 를 적 용 합 니다 . 즉 , 학 습 은 대 부분 손 실 ( loss ) 을 최 소 화 하 는 것 을 의 미 합 니다 . minimize 𝑤 1 𝑛 𝑛 ∑︁ 𝑖 = 1 𝑙 ( 𝑥 𝑖 , 𝑦 𝑖 , 𝑓 ( 𝑥 𝑖 ) ) + some penalty ( 𝑤 ) 통 계 학 자 들 은 첫 번 째 항 을 경 험 적 인 평 균 ( empirical average ) 이 라 고 합 니다 . 즉 , 이 것 은 𝑝 ( 𝑥 ) 𝑝 ( 𝑦 | 𝑥 ) 확 률 로 선 택 된 데 이 터 에 구 해 진 평 균 을 의 미 합 니다 . 만 약 데 이 터 가 잘 못 된 분 포 𝑞 에 서 선 택 된 다 면 , 다 음 과 같 이 간 단 한 아 이 덴 터 티 ( identity ) 를 사 용 해 서 수 정 할 수 있 습 니다 . ∫︁ 𝑝 ( 𝑥 ) 𝑓 ( 𝑥 ) 𝑑𝑥 = ∫︁ 𝑝 ( 𝑥 ) 𝑓 ( 𝑥 ) 𝑞 ( 𝑥 ) 𝑝 ( 𝑥 ) 𝑑𝑥 = ∫︁ 𝑞 ( 𝑥 ) 𝑓 ( 𝑥 ) 𝑝 ( 𝑥 ) 𝑞 ( 𝑥 ) 𝑑𝑥 다 르 게 설 명 해 보 면 , 데 이 터 가 추출 되 어야 하 는 올 바 른 분 포 에 대 한 확 률 의 비 율을 곱 ( 𝛽 ( 𝑥 ) : = 𝑝 ( 𝑥 ) / 𝑞 ( 𝑥 ) ) 해 서 각 샘 플 의 가 중 치 를 조절 하 면 됩 니다 . 하 지 만 안 타 깝 게 도 이 비 율을 알 지 못 합 니다 . 따 라 서 , 우 선 해 야 하 는 일은 이 값 을 추 정 하 는 것 입 니다 . 이 를 추 정 하 는 다 양 한 방법 이 존 재 합 니다 . 예 로 는 다 소 멋 진 이 론 적 인 연 산 방법 이 있 습 니다 . 이 는 예 상 치 를 계 산 하 는 연 산 을 재 조정 하 는 것 으 로 , 이 는 최 소 - 놈 ( minimum - norm ) 이 나 최 대 엔 트 로 피 ( maximum entropy ) 원 칙 을 직 접 이 용 하 는 방법 입 니다 . 이 런 방법 들 은 두 분 포 에 서 샘 플 들 을 수 집 해 야 하 는 것 을 염 두 해 두 세 요 . 즉 , 학 습 데 이 터 를 이 용 해 서 진짜 𝑝 , 그 리 고 학 습 데 이 터 셋 을 𝑞 를 만 드 는 데 사 용 한 분 포 를 의 미 합 니다 . 이 경 우 좋 은 결과 를 주 는 효 과 적 인 방법 이 있 는 데 , 그 것 은 바 로 로 지 스 틱 회 귀 ( logistic regression ) 입 니다 . 로 지 스 틱 회 귀 를 이 용 하 면 확 률 비 율을 계 산 해 낼 수 있 습 니다 . 𝑝 ( 𝑥 ) ﬀ 로 부 터 추출 된 데 이 터 와 𝑞 ( 𝑥 ) ﬀ 로 부 터 추출 된 데 이 터 를 구 분 하 기 위 한 분 리 모 델 을 학 습 시 키 실 수 있 습 니다 . 두 분 포 를 구 별 하 는 것 이 불 가 능 하 다 면 , 샘 플 들 은 두 분 포 중 에 하 나 에 서 나 왔 다는 것 을 의 미 합 니다 . 반 면 에 분 류 가 잘 되 는 샘 플 들 은 오 버 웨 이 트 ( overweighted ) 되 었 거 나 언 더 웨 이 트 ( underweight ) 되 어 있을 것 입 니다 . 간 단 하 게 설 명 하 기 위 해 서 , 두 분 포 로 부 터 같 은 개 수 만 큼 샘 플 을 추출 했 다 고 가 정 하 겠 습 니다 . 이 를 각각 𝑥 𝑖 ∼ 𝑝 ( 𝑥 ) ﬀ 와 𝑥 𝑖 2 ∼ 𝑞 ( 𝑥 ) ﬀ 로 표 기 합 니다 . 𝑝 ﬀ 로 부 터 추출 된 경 우 𝑧 𝑖 ﬀ 를 1 로 , 𝑞 ﬀ 로 부 터 추출 된 경 우 에 는 - 1 로 값 을 할 당 합 니다 . 그 러 면 , 섞 인 데 이 터 셋 의 확 률 은 다 음 과 같 이 표 현 됩 니다 . 𝑝 ( 𝑧 = 1 | 𝑥 ) = 𝑝 ( 𝑥 ) 𝑝 ( 𝑥 ) + 𝑞 ( 𝑥 ) 이 고 따 라 서 𝑝 ( 𝑧 = 1 | 𝑥 ) 𝑝 ( 𝑧 = − 1 | 𝑥 ) = 𝑝 ( 𝑥 ) 𝑞 ( 𝑥 ) 따 라 서 , 𝑝 ( 𝑧 = 1 | 𝑥 ) = 1 1 + exp ( ` 𝑓 ( 𝑥 ) 를 만 족 시 키 는 로 지 스 틱 회 귀 ( logistic regression ) 방법 을 사 용 하 면 , 218 5 . 딥 러 닝 기 초 이 비 율은 아 래 와 같 은 수식 으 로 계 산 됩 니다 . 𝛽 ( 𝑥 ) = 1 / ( 1 + exp ( − 𝑓 ( 𝑥 ) ) ) exp ( − 𝑓 ( 𝑥 ) / ( 1 + exp ( − 𝑓 ( 𝑥 ) ) ) = exp ( 𝑓 ( 𝑥 ) ) 결 론 적 으 로 우 리 는 두 문 제 를 풀 어야 합 니다 . 첫 번 째 문 제 는 두 분 포 에 서 추출 된 데 이 터 를 구 분 하 는 것 이 고 , 두 번 째 는 가 중 치 를 다 시 적 용 한 최 소 화 문 제 입 니다 . 가 중 치 조정 은 𝛽 를 이 용 하 는 데 , 이 는 헤 드 그 래 디 언 트 ( head gradient ) 를 이 용 합 니다 . 레 이 블 이 없 는 학 습 셋 𝑋 와 테 스 트 셋 𝑍 을 사 용 하 는 프 로 토 타 입의 알 고 리 즘 은 아 래 와 같 습 니다 . 1 . 학 습 셋 { ( 𝑥 𝑖 , − 1 ) . . . ( 𝑧 𝑗 , 1 ) } 을 생 성 합 니다 . 2 . 로 지 스 틱 회 귀 ( Logistic regression ) 를 이 용 해 서 이 진 ( binary ) 분 류 기 를 학 습시 킵 니다 . 이 를 함 수 𝑓 라 고 하 겠 습 니다 . 3 . 𝛽 𝑖 = exp ( 𝑓 ( 𝑥 𝑖 ) ) 또 는 𝛽 𝑖 = min ( exp ( 𝑓 ( 𝑥 𝑖 ) ) , 𝑐 ) 를 이 용 해 서 학 습 데 이 터 에 가 중 치 를 적 용 합 니다 . 4 . 데 이 터 𝑋 와 이 에 대 한 레 이 블 𝑌 에 대 한 학 습 을 수 행할 때 , 가 중 치 𝛽 𝑖 를 이 용 합 니다 . Generative Adversarial Networks 는 위 에 서 설 명 한 아 이 디 어 를 이 용 해 서 , 참 조 데 이 터 셋 과 구 분 이 어 려 운 데 이 터 를 만 드 는 데 이 터 생 성 기 ( data generator ) 를 만 듭 니다 . 네 트 워 크 𝑓 는 진짜 와 가 짜 데 이 터 는 구 분 하 고 , 다 른 네 트 워 크 𝑔 는 판 정 하 는 역 할 을 하 는 𝑓 를 속 이 는 역 할 , 즉 가 짜 데 이 터 를 진짜 라 고 판 별 하 도 록 하 는 역 할 을 수 행합 니다 . 이 에 대 한 자 세 한 내 용 은 다 시 다 루 겠 습 니다 . 5 . 16 . 5 개 념 변 화 ( concept shift ) 교 정 개 념 변 화 ( concept shift ) 는 개 념 적 으 로 해 결 하 기 훨 씬 어 렵 습 니다 . 예 를 들 면 , 고 양 이 와 강 아 지 를 구 분 하 는 문 제 에 서 흰 색 과 검 은 색 동 물 을 구 분 하 는 문 제 로 갑 자 기 바 뀌 었 다 고 하 면 , 새 로 운 레 이 블 을 이 용 해 서 새 로 학 습 을 시 키 는 것 보 다 더 잘 동 작 시 키 는 것 을 기 대 하 는 것 은 무 리 일 것 입 니다 . 다 행 히 , 실 제 상 황 에 서 는 이 렇 게 심 한 변 화 는 발 생 하 지 않 습 니다 . 대 신 , 변 화 가 천천 히 일 어 나 는 것 이 보 통 의 경 우 입 니다 . 더 정 확 하 게 하 기 위 해 서 , 몇 가 지 예 를 들 어 보 겠 습 니다 . • 광 고 에 서 새 로 운 상 품 이 출 시 되 고 , 이 전 상 품 의 인 기 는 떨 어 집 니다 . 즉 , 광 고 의 분 포 와 인 기 도 는 서서 히 변 화 되 기 때 문 에 , click - through rate 예 측 모 델 은 그 에 따 라 서 서서 히 바 뀌 어야 합 니다 . • 교 통 카 메 라 렌 즈 는 환 경 의 영 향 으 로 서서 히 성 능 이 떨 어 지 게 되 고 , 그 결과 이 미 지 품 질 에 영 향 을 미 칩 니다 . • 뉴 스 내 용 이 서서 히 바 뀝 니다 . ( 즉 , 대 부분 의 뉴 스 는 바 뀌 지 않 지 만 , 새 로 운 이 야 기 가 추 가 됩 니 다 . ) 5 . 16 . 환 경 219 이 런 경 우 에 네 트 워 크 학 습 에 사 용 한 것과 같 은 방법 을 데 이 터 의 변 화 에 적 응 시 키 는 데 사 용 할 수 있 습 니다 . 즉 , 네 트 워 크 를 처 음 부 터 다 시 학 습시 키 는 것 이 아 니 라 , 현 재 가 중 치 값 을 갖 는 네 트 워 크 에 새 로 이 추 가 된 데 이 터 를 이 용 해 서 학 습시 키 는 것 입 니다 . 5 . 16 . 6 학 습 문 제 의 분 류 𝑝 ( 𝑥 ) 과 𝑝 ( 𝑦 | 𝑥 ) 이 바 뀔 때 어 떻 게 다 뤄 야 하 는 지 에 대 해 서 알아 봤 으 니 , 머 신 러 닝 을 이 용 해 서 풀 수 있 는 여 러 가 지 문 제 들 에 대 해 서 알아 보 겠 습 니다 . • 배 치 러 닝 . 학 습 데 이 터 와 레 이 블 쌍 { ( 𝑥 1 , 𝑦 1 ) , . . . ( 𝑥 𝑛 , 𝑦 𝑛 ) } 을 사 용 해 서 네 트 워 크 𝑓 ( 𝑥 , 𝑤 ) 를 학 습시 킨 다 고 생 각 해 봅 니다 . 모 델 을 학 습시 킨 후 , 학 습 데 이 터 와 같 은 분 포 에 서 새 로 운 데 이 터 ( 𝑥 , 𝑦 ) 를 뽑 아 서 이 모 델 에 적 용 합 니다 . 우 리 가 여 기 서 논 의 하 는 대 부분 의 문 제 는 이 기 본 적 인 가 정 을 포 함하 고 있 습 니다 . 예 를 들 면 , 고 양 이 와 강 아 지 사 진 을 사 용 해 서 고 양 이 탐 지 모 델 을 학 습시 킵 니다 . 모 델 을 학 습시 킨 후 , 고 양 이 만 들 어 올 수 있 도 록 하 는 컴 퓨 터 비 전 을 이 용 한 고 양 이 전 용 문 시스 템 에 이 모 델 을 사 용 합 니다 . 이 시스 템 을 고객 의 가 정 에 설 치 한 후 에 모 델 을 다 시 업 데 이 트 하 지 않 습 니다 . • 온 라 인 러 닝 . 데 이 터 ( 𝑥 𝑖 , 𝑦 𝑖 ) ﬀ 가 한 번 에 하 나 씩 들 어 오 는 것 을 가 정 합 니다 . 조 금 더 명 확 하 게 말 하 자 면 , 우 선 𝑥 𝑖 ﬀ 가 관 찰 되 면 , 𝑓 ( 𝑥 𝑖 , 𝑤 ) ﬀ 를 통 해 서 추 측 을 수 행 한 이 후 에 만 𝑦 𝑖 ﬀ 를 알 수 있 는 경 우 를 가 정 합 니다 . 이 후 , 추 측 결과 에 대 한 보 상 또 는 loss 를 계 산 합 니다 . 많 은 실 제 문 제 가 이 러 한 분 류 에 속 합 니다 . 예 를 들 면 , 다 음 날 의 주 식 가격 을 예 측 하 는 경 우 를 생 각 해 보 면 , 예 측 된 주 가 에 근 거 해 서 거 래 를 하 고 , 그 날 의 주 식시 장이 끝나 면 예 측 이 수 익을 가 져 다 줬 는 지 알 수 있 습 니다 . 달 리 말 하 면 , 새 로 운 관 찰 을 통 해 서 모 델 을 지 속 적 으 로 발 전 시 키 는 다 음 과 같 은 사 이 클 을 만 들 수 있 습 니다 . model 𝑓 𝑡 −→ data 𝑥 𝑡 −→ estimate 𝑓 𝑡 ( 𝑥 𝑡 ) −→ observation 𝑦 𝑡 −→ loss 𝑙 ( 𝑦 𝑡 , 𝑓 𝑡 ( 𝑥 𝑡 ) ) −→ model 𝑓 𝑡 + 1 • 반 딧 . 반 딧 은 위 문 제 의 특 별 한 경 우 입 니다 . 대 부분 의 학 습 문 제 는 연 속 된 값 을 출 력 하 는 함 수 𝑓 의 파 라 미 터 ( 예 를 들 면 딥 네 트 워 크 ) 를 학 습 하 는 경 우 이 지 만 , 반 딧 문 제 는 선 택 할 수 있 는 종 류 가 유 한한 경 우 ( 즉 , 취 할 수 있 는 행 동 이 유 한한 경 우 ) 입 니다 . 이 간 단 한 문 제 의 경 우 , 최 적 화 의 측 면 에 서 강 력 한 이 론 적 인 보 증 을 얻 을 수 있 다는 것 이 당 연 합 니다 . 이 문 제 를 별 도 로 분 류 한 이유 는 이 문 제 를 종종 distinct learning 과 혼 동 하 기 때 문 입 니다 . • 제 어 ( 그 리 고 비 대 립 적 강 화 학 습 ) . 많 은 경 우 에 환 경 은 우 리 가 취 한 행 동 을 기 억 합 니다 . 적 의 적 인 의 도 가 아 닌 경 우 에 도 , 단 순 히 기 억 하 고 , 이 전 에 일 어 난 일 에 근 거 해 서 반 응 하 는 환 경 들 이 있 습 니다 . 즉 , 커 피 포 트 제 어 기 의 경 우 이 전 에 데 웠 는 지 여 부 에 따 라 서 다 른 온 도 를 감 지 하 기 도 합 니다 . PID ( Propotional Integral Derivative ) 제 어 알 고 리 즘 도 유 명 한 예 입 니다 . 비 슷 한 예 로 , 뉴 스 사 이 트 에 대 한 사 용 자의 행 동 은 이 전 에 무 엇 을 무 엇 이 었 는 지 영 향 을 받 습 니다 . 이 런 종 류 220 5 . 딥 러 닝 기 초 의 많 은 알 고 리 즘 들 은 그 결 정 들 이 임의의 선 택 으 로 보 이 지 않 도 록 모 델 을 만 들 어 냅 니다 . ( 즉 , 분 산 을 줄 이 는 방 향 으 로 ) • 강 화 학 습 . 기 억 을 하 는 환 경 의 더 일 반 적 인 예 로 우 리 와 협 력 을 시 도 하 는 환 경 ( non - zero - sum 게 임 과 같 이 협 력 적 인 게 임 ) 이 나 이 기 려 고 하 는 환 경 이 있 습 니다 . 체 스 나 , 바 둑 , 서 양 주 사 위 놀 이 ( Backgammon ) 또 는 스 타크 래 프 트 가 경 쟁 하 는 환 경 의 예 들 입 니다 . 마 찬 가 지 로 , 자율 주 행 차 를 위 한 좋 은 제 어 기 를 만 드 는 것 도 생 각 해 볼 수 있 습 니다 . 이 경 우 다 른 차 량 들 은 자율 주 행 차 의 운 전 스 타 일 에 여 러 가 지 로 반 응을 합 니다 . 때 로 는 피하 려 고 하 거 나 , 사 고 를 내 려 고 하 거 나 , 같 이 잘 주 행하 려 고 하 는 등 여 러 반 응을 보 일 것 입 니다 . 위 에 설 명 한 다 양 한 상 황 들 간 의 주 요 차 이 점 은 안 정적 인 환 경 에 서 잘 작 동 하 는 전 략 이 환 경 이 변 화 는 상 황 에 서 는 잘 작 동 하 지 않 을 수 있 다는 것 입 니다 . 예 를 들 면 , 거 래 자 가 발 견 한 차 익 거 래 기 회 는 한 번 실 행 되 면 사 라 질 가 능 성 이 높 습 니다 . 환 경 이 변 화 하 는 속 도 나 형 태 는 계 속 해 서 사 용 할 수 있 는 알 고 리 즘 의 형 태 를 많 이 제 약 합 니다 . 예 를 들 면 , 어 떤 것 이 천천 히 변 화 할 것 이 라 고 알 고 있을 경 우 , 예 측 모 델 또 한 천천 히 바 뀌 도 록 할 수 있 습 니다 . 만 약 , 환 경 이 불 규 적 으 로 순 간 적 으 로 바 뀐 다 고 알 고 있 는 경 우 에 는 , 이 에 대 응 하 도 록 만 들 수 있 습 니다 . 이 런 종 류 의 지 식 은 풀 고 자 하 는 문 제 가 시 간 에 따 라 서 바 뀌 는 상 황 , 즉 개 념 변 화 ( concept shit ) 를 다 루 는 야 심 찬 데 이 터 사 이 언 티 스 트 에 게 아 주 중 요 합 니다 . 5 . 16 . 7 요 약 • 많 은 경 우 에 학 습 셋 과 테 스 트 셋 은 같 은 분 포 로 부 터 얻어 지지 않 습 니다 . 이 런 상 황 을 우 리 는 공 변 량 변 화 ( covariate shift ) 라 고 합 니다 . • 공 변 량 변 화 ( covariate shift ) 는 변 화 가 아 주 심 하 지 않 을 경 우 에 탐 지 하 고 이 를 교 정 할 수 있 습 니 다 . 만 약 그 렇 게 하 지 못 하 면 , 테 스 트 시 점 에 좋 지 않 은 결과가 나 옵 니다 . • 어 떤 경 우 에 는 환 경 이 우 리 가 취 한 것 을 기 억 하 고 , 예 상 하 지 못 한 방법 으 로 결과 를 줄 수 도 있 습 니다 . 모 델 을 만 들 에 이 점 을 유의 해 야 합 니다 . 5 . 16 . 8 문 제 1 . 검 색 엔 진 의 행 동 을 바 꾸 면 어 떤 일이 일 어 날 까 요 ? 사 용 자 는 어 떻 게 반 응 할 까 요 ? 광 고 주 는 어 떨 까 요 ? 2 . 공 변 량 변 화 ( covariate shift ) 탐 기 지 를 구 현 해 보 세 요 . 힌 트 - 분 류 기 를 만 들 어 봅 니다 . 3 . 공 변 량 변 화 ( covariate shift ) 교 정 기 를 구 현 해 보 세 요 . 5 . 16 . 환 경 221 4 . 학 습 세 트 와 테 스 트 세 트 가 많 이 다 를 경 우 무 엇 이 잘 못 될 수 있을 까 요 ? 샘 플 weight 들 에 는 어 떤 일이 일 어 날 까 요 ? 5 . 16 . 9 Scan the QR Code to Discuss 5 . 17 Kaggle 의 주 택 가격 예 측 하 기 앞 절 들 에 서 딥 네 트 워 크 를 만 들 고 차 원 과 가 중 치 감 쇠 ( weight decay ) 그 리 고 드 롭 아 웃 ( dropout ) 을 사 용 해 서 용 량 을 제 어 하 는 다 양 한 기 본 적 인 도 구 들 을 소 개 했 습 니다 . 이 제 앞에 서 배 운 내 용 들 을 잘 활 용 해 서 Kaggle 대 회 에 참 여 해 보 겠 습 니다 . 집 가격 예 측 문 제 는 상 당 히 일 반 적 이 고 , 텍 스 트 나 이 미 지 데 이 터 처 럼 규 칙 적 인 구 조 도 없 기 때 문 에 시 작 하 기 좋 은 문 제 입 니다 . 사 용 할 데 이 터 는 1978 년 의 Harrison 과 Rubinfeld 의 보 스 턴 집 데 이 터 세 트 가 아 니 고 , 더 크 고 더 많 은 특 성 ( feature ) 을 지 닌 데 이 터 세 트 로 2006 년 부 터 2010 년 까 지 의 Ames , IA 의 집 가격 데 이 터 입 니다 . 이 데 이 터 는 2011 년 에 Bart de Cock 이 수 집 한 것 입 니다 . 더 크 기 때 문 에 조 금 더 흥 미 있 는 예 측 문 제 를 다 루 게 됩 니다 . 이 절 에 서 는 우 리 가 배 운 것 들 을 적 용 해 볼 예 정 입 니다 . 특 히 , 데 이 터 전 처 리 , 모 델 설 계 , 하 이 퍼파 라 미 터 ( hyperparameter ) 선 택 과 튜 닝 에 대 한 자 세 한 내 용 들 을 살 펴 봅 니다 . 직 접 수 행하 면 서 용 량 제 어 , 특 성 ( feature ) 추출 등 의 영 향 이 어 떻 게 되 는 지 실 제 로 알아 보 게 되 는 데 , 이 경 험 은 숙 련 된 데 이 터 사 이 언 티 스 트 가 되 기 위 해 서 는 꼭 필 요 한 것 입 니다 . 5 . 17 . 1 Kaggle Kaggle 은 머 신 러 닝 대 회 로 유 명 한 플 랫 폼 으 로 , 데 이 터 와 코 드 를 사 용 해 서 사 용 자 간 에 협 력 을 하 거 나 경 쟁을 하 는 곳 입 니다 . 예 를 들 어 , 경 쟁자 가 제 출 한 코 드 를 볼 수 있 고 , 다 른 참 여 자 들 과 대 비 해 서 여 러 분 이 얼 마 나 잘 하 고 있 는 지 를 볼 수 도 있 습 니다 . 대 회 중 에 하 나 에 참 여 하 기 위 해 서 는 , 계 정 을 등 록 해 야 합 니다 . 자 그 럼 지 금 계 정 을 만 들 어 보 겠 습 니다 . 222 5 . 딥 러 닝 기 초 5 . 17 . Kaggle 의 주 택 가격 예 측 하 기 223 집 가격 예 측 페 이 지 에 서 , 데 이 터 탭 을 눌 러 보 면 데 이 터 세 트 를 찾 을 수 있 고 , 예 측 을 제 출 해 서 여 러 분 의 순 위 를 확 인 할 수 있 습 니다 . 아 래 URL 을 방 문 해 보 세 요 . https : / / www . kaggle . com / c / house - prices - advanced - regression - techniques 224 5 . 딥 러 닝 기 초 5 . 17 . Kaggle 의 주 택 가격 예 측 하 기 225 5 . 17 . 2 데 이 터 셋 에 접 근 하 고 읽 기 대 회 데 이 터 는 학 습 셋 과 테 스 트 셋 으 로 나 눠 져 있 습 니다 . 각 레 코 드 는 집 에 대 한 특 징 값 들 과 도 로 종 류 , 지 어 진 연 도 , 지 붕 형 태 , 지 하 실 상 태 등 에 대 한 속성 이 포 함 되 어 있 습 니다 . 데 이 터 는 다 양 한 데 이 터 형 으 로 구 성 되 어 있 습 니다 . 예 를 들 면 , 지 어 진 연 도 는 정 수 , 지 붕 형 태 는 이 산 레 이 블 ( discrete label ) , 다 른 속성 들 은 실수 등 으 로 되 어 있 습 니다 . 어 떤 데 이 터 는 누 락 된 경 우 가 있 는 데 이 는 ‘na’ 로 표 기 되 어 있 습 니다 . 각 집 의 가격 ( 즉 , 레 이 블 ) 은 학 습 데 이 터 세 트 ( 경 진 대 회 이 기 때 문 에 ) 에 만 포 함 되 어 있 습 니다 . ’Competition’ 탭 의 ‘Data’ 탭 을 눌 러 보 면 데 이 터 를 다 운 로 드 할 수 있 는 링 크 를 찾 을 수 있 습 니다 . 데 이 터 를 읽 고 처 리 하 는 데 효 과 적 인 데 이 터 분 석 툴 킷 인 pandas 를 이 용 하 겠 습 니다 . 이 절 을 수 행 하 기 위 해 서 pandas 를 우 선 설 치 하 세 요 . [ 1 ] : # If pandas is not installed , please uncomment the following line : # ! pip install pandas import sys sys . path . insert ( 0 , ' . . ' ) % matplotlib inline import d2l from mxnet import autograd , gluon , init , nd from mxnet . gluon import data as gdata , loss as gloss , nn import numpy as np import pandas as pd 편 의 를 위 해 서 데 이 터 를 미 리 다 운 로 드 해 서 . . / data 디 렉 토 리 저 장 해 놓 았 습 니다 . 학 습 데 이 터 와 테 스 트 데 이 터 가 담 겨 있 는 두 개 의 CSV ( Comma Separated Values ) 파 일을 각각 Pandas 를 이 용 해 서 읽 습 니다 . [ 2 ] : train _ data = pd . read _ csv ( ' . . / data / kaggle _ house _ pred _ train . csv ' ) test _ data = pd . read _ csv ( ' . . / data / kaggle _ house _ pred _ test . csv ' ) 학 습 데 이 터 셋 은 1 , 460 개 의 샘 플 을 가 지 고 있 고 , 각 샘 플 은 80 개 의 특 성 ( feature ) 들 과 1 개 의 label 을 가 지 고 있 습 니다 . 테 스 트 데 이 터 는 1 , 459 개 샘 플 과 각 80 개 의 특 성 ( feature ) 들 이 있 습 니다 . [ 3 ] : print ( train _ data . shape ) print ( test _ data . shape ) ( 1460 , 81 ) ( 1459 , 80 ) 226 5 . 딥 러 닝 기 초 맨 앞 4 개 의 샘 플 들 의 처 음 4 개 의 특 성 ( feature ) 들 과 마 지 막 2 개 의 특 성 ( feature ) 들 , 그 리 고 판 매 가 레 이 블 을 확 인 해 봅 시 다 . [ 4 ] : train _ data . iloc [ 0 : 4 , [ 0 , 1 , 2 , 3 , - 3 , - 2 , - 1 ] ] [ 4 ] : Id MSSubClass MSZoning LotFrontage SaleType SaleCondition SalePrice 0 1 60 RL 65 . 0 WD Normal 208500 1 2 20 RL 80 . 0 WD Normal 181500 2 3 60 RL 68 . 0 WD Normal 223500 3 4 70 RL 60 . 0 WD Abnorml 140000 각 샘 플 을 확 인 한 결과 , 첫 번 째 특 성 ( feature ) 는 ID 라 는 것 을 확 인 할 수 있 습 니다 . 이 값 은 모 델 이 학 습 데 이 터 를 구 분 하 는 데 활 용 할 수 있 겠 습 니다 . 편 리 한 특 성 ( feature ) 이 긴 하 지 만 , 예 측 이 라 는 목 적 에 어 떤 정 보 를 제 공 하 지 는 않 습 니다 . 따 라 서 , 네 트 워 크 에 데 이 터 를 넣 기 전 에 우 리 는 이 값 을 데 이 터 셋 에 서 제 거 하 겠 습 니다 . [ 5 ] : all _ features = pd . concat ( ( train _ data . iloc [ : , 1 : - 1 ] , test _ data . iloc [ : , 1 : ] ) ) 5 . 17 . 3 데 이 터 전 처 리 하 기 앞 서 설 명 했 듯 이 , 이 데 이 터 는 다 양 한 데 이 터 형 을 가 지 고 있 습 니다 . 데 이 터 를 딥 네 트 워 크 에 대 입 하 기 전 에 , 상 당 한 처 리를 해 야 합 니다 . 수 치 형 의 특 성 ( feature ) 부 터 시 작 해 봅 시 다 . 누 락 된 값 은 평 균 값 으 로 채 워 넣 는 것 으 로 시 작 합 니다 . 이 는 특 성 ( feature ) 이 규 칙 없 이 누 락 된 경 우 에 는 의 미 있 는 전 략 입 니다 . 공 통 적 인 스 케 일 로 조정 하 기 위 해 서 , 평 균 이 0 이 고 분 산 이 1 이 되 도 록 조정 을 하 겠 습 니다 . 이 를 위 한 방법 은 다 음 과 같 습 니다 . 𝑥 ← 𝑥 − 𝜇 𝜎 이 변 환 이 𝑥 를 평 균 이 0 이 고 분 산 이 1 인 데 이 터 로 변 환 하 는 방법 은 E [ ( 𝑥 − 𝜇 ) / 𝜎 ] = ( 𝜇 − 𝜇 ) / 𝜎 = 0 간 단 히 계 산 해 보 면 됩 니다 . 분 산 을 확 인 하 기 위 해 서 , E [ ( 𝑥 − 𝜇 ) 2 ] = 𝜎 2 을 사 용 하 면 , 변 환 된 분 산 이 1 을 갖 는다는 것 을 확 인 할 수 있 습 니다 . 데 이 터 를 표 준 화 ( normalizing ) 하 는 이유 는 모 든 특 성 ( feature ) 값 을 동 일 한 크 기 정 도 로 변 환 해 주 기 때 문 입 니다 . 결 국 에 는 우 리 는 어 떤 특 성 ( feature ) 이 관 련 이 있 는 지 에 대 한 선 험 적 정 보 ( priori ) 를 모 릅 니다 . 따 라 서 , 그 값 들 은 동 일 하 게 다 루 는 것 은 의 미 가 있 습 니다 . [ 6 ] : numeric _ features = all _ features . dtypes [ all _ features . dtypes ! = ' object ' ] . index all _ features [ numeric _ features ] = all _ features [ numeric _ features ] . apply ( lambda x : ( x - x . mean ( ) ) / ( x . std ( ) ) ) # After standardizing the data all means vanish , hence we can set missing ( continues on next page ) 5 . 17 . Kaggle 의 주 택 가격 예 측 하 기 227 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) # values to 0 all _ features = all _ features . fillna ( 0 ) 다 음으 로 는 , 불 연 속 된 값 ( discrete value ) 들 을 다 뤄 보 겠 습 니다 . 이 것 은 ’MSZoning’ 과 같 은 변 수 들 을 포 함합 니다 . 멀 티 클 래 스 분 류 데 이 터 를 0 과 1 의 벡 터 로 변 환 한 것과 같 은 방법 으 로 이 값 들 을 원 - 핫 - 인 코 딩 ( one - hot - encoding ) 을 이 용 해 서 변 환 합 니다 . 예 를 들 면 , ’MSZoning’ 에 대 한 값 이 ’RL’ 과 ’RM’ 을 가 질 수 있 다 고 하 면 , 이 들 은 각각 ( 1 , 0 ) 과 ( 0 , 1 ) 벡 터 로 매 핑하 는 것 을 의 미 합 니다 . Pandas 는 이 를 자 동 으 로 해 주 는 기 능 을 제 공 합 니다 . [ 7 ] : # Dummy _ na = True refers to a missing value being a legal eigenvalue , and # creates an indicative feature for it all _ features = pd . get _ dummies ( all _ features , dummy _ na = True ) all _ features . shape [ 7 ] : ( 2919 , 354 ) 이 변 환 을 수 행하 면 특 성 ( feature ) 의 개 수 가 79 개 에 서 331 개 로 증 가 합 니다 . 마 지 막 으 로 , values 속성 을 통 해 서 , Pandas dataframe 을 NumPy 형 태 로 추출 하 고 , 이 를 학 습 에 사 용 하 기 위 해 서 다 시 MXNet 의 기 본 표 현 인 NDArray 로 바 꿉 니다 . [ 8 ] : n _ train = train _ data . shape [ 0 ] train _ features = nd . array ( all _ features [ : n _ train ] . values ) test _ features = nd . array ( all _ features [ n _ train : ] . values ) train _ labels = nd . array ( train _ data . SalePrice . values ) . reshape ( ( - 1 , 1 ) ) 5 . 17 . 4 학 습 하 기 우 선 제 곱 손 실 ( squared loss ) 을 사 용 해 서 선 형 모 델 을 학 습시 켜 보 겠 습 니다 . 이 모 델 은 당 연 히 이 대 회 에 서 우 승 을 할 정 도 로 좋 은 모 델 이 될 수 는 없 지 만 , 데 이 터 에 의 미 있 는 정 보 가 있 는 지 를 점 검 하 는 데 에 도 움 을 줍 니다 . 또 한 , 이 모 델 은 더 멋 진 모 델 이 되 기 위 해 얼 마 나 좋 을 결과 를 만 들 어 내 야 하 는 지 에 대 한 최 소 한 의 기 준 점 ( baseline ) 을 주 기 도 합 니다 . [ 9 ] : loss = gloss . L2Loss ( ) def get _ net ( ) : net = nn . Sequential ( ) net . add ( nn . Dense ( 1 ) ) net . initialize ( ) return net 228 5 . 딥 러 닝 기 초 집 가격 은 주 식 과 같 이 상 대 적 입 니다 . 즉 , 절 대 오 류 보 다는 상 대 오 류 𝑦 − ^ 𝑦𝑦 가 더 의 미 가 있을 것 입 니다 . 예 를 들 면 , 실 제 집 가격 이 125 , 000 달 러 인 Rural Ohio 에 서 가격 을 100 , 000 달 러 만 큼 틀 리 게 예 측 하 는 것 은 아 주 나 쁜 예 측 이 되 지 만 , 평 균 집 값 이 4 백 만 달 러 가 넘 는 캘 리 포 니 아 Los Altos Hills 의 집 가격 을 같 은 오 차 로 예 산 했 다 면 , 이 모 델 을 충 분 히 정 확 한 것 으 로 간 주 될 것 입 니다 . 이 런 문 제 를 해 결 하 는 방법 중 에 하 나 는 예 측 된 가격 에 로 그 ( logarithm ) 를 취 한 값 의 차 이 로 측 정 하 는 것 입 니다 . 사 실 , 이 대 회 에 서 품 질 을 측 정 하 는 방법 으 로 사 용 되 는 오 류 이 기 도 합 니다 . 결 국 , log 𝑦 − log ˆ 𝑦 의 작은 값 𝛿 는 𝑒 − 𝛿 ≤ ^ 𝑦𝑦 ≤ 𝑒 𝛿 로 해 석 되 고 , 다 음 과 같 은 loss 함 수 를 정 의 할 수 있 습 니다 . 𝐿 = ⎯⎸⎸⎷ 1 𝑛 𝑛 ∑︁ 𝑖 = 1 ( log 𝑦 𝑖 − log ˆ 𝑦 𝑖 ) 2 ﬀ [ 10 ] : def log _ rmse ( net , features , labels ) : # To further stabilize the value when the logarithm is taken , set the # value less than 1 as 1 clipped _ preds = nd . clip ( net ( features ) , 1 , float ( ' inf ' ) ) rmse = nd . sqrt ( 2 * loss ( clipped _ preds . log ( ) , labels . log ( ) ) . mean ( ) ) return rmse . asscalar ( ) 이 전 절 들 과 는 다 르 게 , 아 래 학 습 함 수 에 서 는 Adam 최 적 화 알 고 리 즘 을 사 용 합 니다 . 앞에 서 사 용 한 미 니 배 치 확 률 적 경 사 하 강 법 ( stochastic gradient descent ) 와 비 교 하 자 면 , Adam 최 적 화 알 고 리 즘 은 학 습 속 도 ( learning rate ) 에 상 대 적 으 로 덜 민 감 합 니다 . 이 에 대 한 자 세 한 설 명 은 Optimization Algorithms 장 에 서 하 겠 습 니다 . [ 11 ] : def train ( net , train _ features , train _ labels , test _ features , test _ labels , num _ epochs , learning _ rate , weight _ decay , batch _ size ) : train _ ls , test _ ls = [ ] , [ ] train _ iter = gdata . DataLoader ( gdata . ArrayDataset ( train _ features , train _ labels ) , batch _ size , shuffle = True ) # The Adam optimization algorithm is used here trainer = gluon . Trainer ( net . collect _ params ( ) , ' adam ' , { ' learning _ rate ' : learning _ rate , ' wd ' : weight _ decay } ) for epoch in range ( num _ epochs ) : for X , y in train _ iter : with autograd . record ( ) : l = loss ( net ( X ) , y ) l . backward ( ) trainer . step ( batch _ size ) train _ ls . append ( log _ rmse ( net , train _ features , train _ labels ) ) ( continues on next page ) 5 . 17 . Kaggle 의 주 택 가격 예 측 하 기 229 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) if test _ labels is not None : test _ ls . append ( log _ rmse ( net , test _ features , test _ labels ) ) return train _ ls , test _ ls 5 . 17 . 5 𝐾 - 겹 교 차 검 증 ( 𝐾 ff - fold cross - validation ) k - 겹 교 차 검 증 ( k - fold cross - validation ) 은 “ 모 델 선 택 , 언 더 피 팅 ( underﬁtting ) , 오 버 피 팅 ( overﬁtting ) ” 절 에 서 어 떻 게 다 뤄 야 하 는 지 를 소 개 한 개 념 입 니다 . 우 리 는 이 방법 을 모 델 디 자인을 선 택 하 고 , 하 이 퍼파 라 미 터 ( hyperparameter ) 를 조정 하 는 데 사 용 하 겠 습 니다 . 우 선 , k - 겹 교 차 검 증 ( k - fold cross - validation ) 절 차 에 사 용 될 , i - 번 째 데 이 터 겹 ( fold ) 을 반 환 하 는 함 수 가 필 요 합 니다 . 데 이 터 를 다 루 는 가 장 효 과 적 인 구 현 이 아 님 을 명 시 해 주 세 요 . 이 후 에 우 리 는 아 주 많 은 데 이 터 를 더 똑똑 하 게 다 루 는 방법 을 사 용 할 예 정 이 지 만 , 함 수 의 구 현 코 드 를 간결 하 게 하 기 위 해 서 지 금 은 사 용 하 않 겠 습 니다 . [ 12 ] : def get _ k _ fold _ data ( k , i , X , y ) : assert k > 1 fold _ size = X . shape [ 0 ] / / k X _ train , y _ train = None , None for j in range ( k ) : idx = slice ( j * fold _ size , ( j + 1 ) * fold _ size ) X _ part , y _ part = X [ idx , : ] , y [ idx ] if j = = i : X _ valid , y _ valid = X _ part , y _ part elif X _ train is None : X _ train , y _ train = X _ part , y _ part else : X _ train = nd . concat ( X _ train , X _ part , dim = 0 ) y _ train = nd . concat ( y _ train , y _ part , dim = 0 ) return X _ train , y _ train , X _ valid , y _ valid 다 음 함 수 는 k - 겹 교 차 검 증 ( k - fold cross - validation ) 에 서 학 습 을 𝑘 번 수 행했 을 때 학 습 오 류 의 평 균 과 검 증 ( validation ) 오 류 의 평 균 을 반 환 합 니다 . [ 13 ] : def k _ fold ( k , X _ train , y _ train , num _ epochs , learning _ rate , weight _ decay , batch _ size ) : train _ l _ sum , valid _ l _ sum = 0 , 0 for i in range ( k ) : data = get _ k _ fold _ data ( k , i , X _ train , y _ train ) net = get _ net ( ) train _ ls , valid _ ls = train ( net , * data , num _ epochs , learning _ rate , ( continues on next page ) 230 5 . 딥 러 닝 기 초 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) weight _ decay , batch _ size ) train _ l _ sum + = train _ ls [ - 1 ] valid _ l _ sum + = valid _ ls [ - 1 ] if i = = 0 : d2l . semilogy ( range ( 1 , num _ epochs + 1 ) , train _ ls , ' epochs ' , ' rmse ' , range ( 1 , num _ epochs + 1 ) , valid _ ls , [ ' train ' , ' valid ' ] ) print ( ' fold % d , train rmse : % f , valid rmse : % f ' % ( i , train _ ls [ - 1 ] , valid _ ls [ - 1 ] ) ) return train _ l _ sum / k , valid _ l _ sum / k 5 . 17 . 6 모 델 선 택 하 기 아 래 하 이 퍼파 라 미 터 ( hyperparameter ) 는 튜 닝 되 지 않 은 값 을 사 용 했 으 니 , 여 러 분 이 이 값 을 변 경 해 서 모 델 의 성 능 을 높 여 보 기 를 바 랍 니다 . 몇 개 를 조정 할 것 인 지 에 따 라 좋 은 값 들 을 찾 는 데 상 당 히 많 은 시 간 이 걸 릴 수 도 있 습 니다 . 왜 냐 하 면 k - 겹 교 차 검 증 ( k - fold cross - validation ) 방법 은 테 스 트 를 여 러 번 수 행하 는 것 에 도 영 향 을 받 지 않 기 때 문 입 니다 . 하 지 만 , 너 무 많 은 오 션 들 을 시 도 해 볼 려 고 한 다 면 , 실 패 할 수 도 있 습 니다 . 그 이유 는 검 증 데 이 터 셋 에 특 정 하 이 퍼파 라 미 터 가 좋 게 나 오 는 것 이 있을 수 있 기 때 문 입 니다 . [ 14 ] : k , num _ epochs , lr , weight _ decay , batch _ size = 5 , 100 , 5 , 0 , 64 train _ l , valid _ l = k _ fold ( k , train _ features , train _ labels , num _ epochs , lr , weight _ decay , batch _ size ) print ( ' % d - fold validation : avg train rmse : % f , avg valid rmse : % f ' % ( k , train _ l , valid _ l ) ) 5 . 17 . Kaggle 의 주 택 가격 예 측 하 기 231 fold 0 , train rmse : 0 . 169553 , valid rmse : 0 . 156965 fold 1 , train rmse : 0 . 162164 , valid rmse : 0 . 188608 fold 2 , train rmse : 0 . 163758 , valid rmse : 0 . 168080 fold 3 , train rmse : 0 . 167453 , valid rmse : 0 . 154536 fold 4 , train rmse : 0 . 162620 , valid rmse : 0 . 182973 5 - fold validation : avg train rmse : 0 . 165110 , avg valid rmse : 0 . 170232 어 떤 하 이 퍼파 라 미 터 ( hyperparameter ) 세 트 들 을 사 용 하 면 학 습 오 류 가 상 당 히 작 게 나 오 나 , 𝐾 - 겹 교 차 검 증 ( 𝐾 - fold cross - validation ) 오 류 는 상 당 히 크 게 나 오 는 현 상 을 발 견 하 게 될 것 입 니다 . 이 것 은 대 부 분 오 버 피 팅 ( overﬁtting ) 의 결과 입 니다 . 따 라 서 , 학 습 오 류를 줄 일 때 , 𝐾 - 겹 교 차 검 증 ( 𝐾 - fold cross - validation ) 오 류 도 함 께 감 소 하 고 있 는 지 를 확 인 하 는 것 이 필 요 합 니다 . 5 . 17 . 7 예 측 하 고 제 출 하 기 하 이 퍼파 라 미 터 ( hyperparameter ) 의 좋 은 조 합 을 찾 았 으 면 ( 학 습 데 이 터 의 1 − 1 / 𝑘 만 큼 사 용 하 는 것 이 아 니 라 ) 모 든 학 습 데 이 터 를 사 용 해 서 모 델 을 학 습시 킵 니다 . 이 렇 게 학 습 된 모 델 을 테 스 트 셋 에 적 용 하 고 , 예 측 결과 를 CSV 파 일 에 저 장 해 서 Kaggle 에 업 로 드 를 할 것 입 니다 . [ 15 ] : def train _ and _ pred ( train _ features , test _ feature , train _ labels , test _ data , num _ epochs , lr , weight _ decay , batch _ size ) : net = get _ net ( ) train _ ls , _ = train ( net , train _ features , train _ labels , None , None , num _ epochs , lr , weight _ decay , batch _ size ) d2l . semilogy ( range ( 1 , num _ epochs + 1 ) , train _ ls , ' epochs ' , ' rmse ' ) ( continues on next page ) 232 5 . 딥 러 닝 기 초 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) print ( ' train rmse % f ' % train _ ls [ - 1 ] ) # Apply the network to the test set preds = net ( test _ features ) . asnumpy ( ) # Reformat it for export to Kaggle test _ data [ ' SalePrice ' ] = pd . Series ( preds . reshape ( 1 , - 1 ) [ 0 ] ) submission = pd . concat ( [ test _ data [ ' Id ' ] , test _ data [ ' SalePrice ' ] ] , axis = 1 ) submission . to _ csv ( ' submission . csv ' , index = False ) 자 모 델 을 수 행해 보 겠 습 니다 . 잘 되 고 있 는 지 확 인 하 는 좋 은 방법 은 𝐾 - 겹 교 차 검 증 ( 𝐾 - fold cross - validation ) 으 로 예 측 한 것과 테 스 트 셋 에 대 한 예 측 이 비 슷 하 게 나 오 는 지 확 인 해 보 는 것 입 니다 . 만 약 비 슷 하 게 나 온 다 면 , 결과 를 Kaggle 에 업 로 드 하 세 요 . [ 16 ] : train _ and _ pred ( train _ features , test _ features , train _ labels , test _ data , num _ epochs , lr , weight _ decay , batch _ size ) train rmse 0 . 162739 위 코 드 를 수 행하 면 submission . csv 파 일이 생 성 됩 니다 . ( CSV 는 Kaggle 에 서 결과 파 일 로 받 는 형 식 중 에 하 나 임 ) 그 다 음 , Kaggle 에 예 측 값 을 제 출 해 서 테 스 트 데 이 터 셋 에 대 한 실 제 집 가격과 비 교 해 서 오 류를 확 인 해 보 는 것 입 니다 . 방법 은 아 주 간 단 합 니다 . • Kaggle 웹 사 이 트 에 로 그 인 하 고 , 집 값 예 측 대 회 페 이 지 를 방 문 합 니다 . • 오 른 쪽 의 “Submit Predictions” 또 는 “Late Submission” 을 클 릭 합 니다 . • 점 선 박 스 안 의 “Upload Submission File” 버 튼 을 클 릭 하 고 , 업 로 드 할 예 측 파 일을 선 택 합 니다 . 5 . 17 . Kaggle 의 주 택 가격 예 측 하 기 233 • 페 이 지 아 래 에 있 는 “Make Submission” 버 튼 을 클 릭 해 서 여 러 분 의 결과 를 보 세 요 . 234 5 . 딥 러 닝 기 초 5 . 17 . Kaggle 의 주 택 가격 예 측 하 기 235 5 . 17 . 8 요 약 • 실 제 데 이 터 는 종종 다 양 한 데 이 터 타 입의 값 들 을 갖고 있 기 때 문 에 , 전 처 리 가 꼭 필 요 합 니다 . • 실수 값 을 평 균 이 0 이 고 분 산 이 1 로 변 환 을 기 본 으 로 선 택 하 는 것 은 좋 은 방법 이 고 , 누 락 된 값 을 평 균 값 으 로 채 워 넣 는 것 도 그 렇 습 니다 . • 카 테 고 리 변 수 를 지 표 변 수 ( indicator variable ) 로 변 환 해 서 이 값 들 을 벡 터 처 럼 다 룰 수 있 습 니다 . • 모 델 을 선 택 하 고 하 이 퍼파 라 미 터 ( hyper - parameter ) 를 선 택 하 기 위 해 서 𝐾 - 겹 교 차 검 증 ( 𝐾 - fold cross - validation ) 을 사 용 할 수 있 습 니다 . • 로 그 ( Logarithm ) 는 상 대 적 인 손 실 ( loss ) 를 구 하 는 데 유 용 합 니다 . 5 . 17 . 9 문 제 1 . 여 러 분 이 수 행한 예 측 결과 를 Kaggle 에 제 출 하 세 요 . 여 러 분 의 예 측 이 얼 마 나 좋 은 가 요 ? 2 . log 가격 을 직 접 최 소 화 하 는 방법 으 로 모 델 을 향 상 시 킬 수 있 나 요 ? 가격 이 아 닌 log 가격 을 예 측 하 도 록 하 면 어 떻 게 될 까 요 ? 3 . 누 락 된 값 을 평 균 값 으 로 채 우 는 방법 이 항 상 좋 은 아 이 디 어 일 까 요 ? 힌 트 - 값 들 이 불 규 칙 하 게 누 락 되 지 않 은 경 우 를 생 각 해 보 세 요 . 4 . 누 락 된 값 을 다 루 는 더 좋 은 표 현 법 을 찾 아 보 세 요 . 힌 트 - 지 표 변 수 를 추 가 하 면 어 떻 게 될 까 요 ? 5 . 𝐾 - 겹 교 차 검 증 ( 𝐾 - fold cross - validation ) 을 이 용 해 서 하 이 퍼파 라 미 터 ( hyper - parameter ) 를 튜 닝 하 고 더 Kaggle 에 서 좋 은 점 수 를 획 득 해 보 세 요 . 6 . 층 추 가 , 정 규 화 적 용 , 드 롭 아 웃 ( dropout ) 적 용 등 을 통 해 서 모 델 을 향 상 시 켜 서 점 수 를 높 여 보 세 요 . 7 . 연 속 된 수 치 특 성 ( feature ) 이 절 에 서 한 것 처 럼 표 준 화 하 지 않 은 경 우 어 떤 일이 일 어 날 까 요 ? 5 . 17 . 10 Scan the QR Code to Discuss 236 5 . 딥 러 닝 기 초 6 딥 러 닝 계 산 앞 장 에 서 는 간 단 한 딥 러 닝 모 델 을 위 한 원 칙 과 구 현 에 대 해 서 알아 봤 습 니다 . 이 장 에 서 우 리 는 딥 러 닝 연 산 의 주 요 요 소 들 ( 모 델 생 성 , 파 라 미 터 접 근 , 초 기 화 , 커 스 텀 층 , 읽 기 / 저 장 하 기 , GPU 사 용 ) 을 다 룰 예 정 입 니다 . 이 장을 통 해 서 여 러 분 은 모 델 구 현 및 연 산 상 세 에 대 한 중 요 한 통 찰 을 얻 고 , 다 음 장 들 에 서 설 명 한 더 복 잡 한 모 델 구 현 을 위 한 견고 한 기 본 을 다 질 수 있 습 니다 . 6 . 1 층 ( layer ) 과 블 럭 ( Block ) 딥 러 닝 이 유 명 해 질 수 있 었 던 중 요 요 소 들 중 에 하 나 는 바 로 강 력 한 소 프 트 웨 어 입 니다 . 반 도 체 설 계 를 하 는 데 엔 지 니 어 들 이 논 리 회 로 를 트 랜 지 스 터 로 구 현 하 던 것 에 서 코 드 를 작 성 하 는 것 으 로 넘 어 간 것과 같 은 일이 딥 네 트 워 크 설 계 에 도 비 슷 하 게 일 어 나 고 있 습 니다 . 앞 장 들 은 단 일 뉴 런 으 로 부 터 뉴 런 으 로 구 성 된 전 체 층 들 로 옮 겨가 는 것 을 보 여 줬 습 니다 . 하 지 만 , 컴 퓨 터 비 전 문 제 를 풀 기 위 해 서 2016 년 에 He et al . 에 의 해 서 제 안 된 ResNet - 152 의 경 우 처 럼 152 개 의 층 들 을 갖 는 네 트 워 크 층 들 을 사 용 한 네 트 워 크 설 계 방법 조 차 도 지 루 할 수 있 습 니다 . 이 런 네 트 워 크 는 많 은 정 도 로 반복 되 는 부분 을 갖고 , 반복 되 는 ( 또 는 비 슷 하 게 설 계 된 ) 층 들 의 블 럭 들 로 구 성 됩 니다 . 이 들 블 럭 들 은 더 복 잡 한 네 트 워 크 디 자인을 구 성 하 는 기 본 요 소 가 됩 니다 . 간 략 하 게 237 말 하 면 , 블 럭 은 하 나 또 는 그 이 상 의 층 의 조 합 입 니다 . 마 치 레 고 공 장이 만 든 블 럭 을 이 용 해 서 멋 진 구 조 물 을 만 들 수 있 는 것 처 럼 , 이 디 자인은 요 청 에 따 라 서 블 럭 을 생 성 하 는 코 드 의 도 움 으 로 만 들 어 질 수 있 습 니다 . 아 주 간 단 한 블 럭 부 터 살 펴 보 겠 습 니다 . 이 블 럭 은 앞 장 에 서 본 다 층 퍼 셉 트 론 ( multilayer perception ) 을 위 한 것 입 니다 . 일 반 적 인 방법 으 로 두 개 의 층 을 갖 는 네 트 워 크 를 다 음 과 같 이 만 들 수 있 습 니다 . [ 1 ] : from mxnet import nd from mxnet . gluon import nn x = nd . random . uniform ( shape = ( 2 , 20 ) ) net = nn . Sequential ( ) net . add ( nn . Dense ( 256 , activation = ' relu ' ) ) net . add ( nn . Dense ( 10 ) ) net . initialize ( ) net ( x ) [ 1 ] : [ [ 0 . 09543004 0 . 04614332 - 0 . 00286654 - 0 . 07790349 - 0 . 05130243 0 . 02942037 0 . 08696642 - 0 . 0190793 - 0 . 04122177 0 . 05088576 ] [ 0 . 0769287 0 . 03099705 0 . 00856576 - 0 . 04467199 - 0 . 06926839 0 . 09132434 0 . 06786595 - 0 . 06187842 - 0 . 03436673 0 . 04234694 ] ] < NDArray 2x10 @ cpu ( 0 ) > 이 코 드 는 256 개 의 유 닛 ( unit ) 들 을 갖 는 은 닉 층 ( hidden layer ) 한 개 를 포 함한 네 트 워 크 를 생 성 합 니 다 . 은 닉 층 ( hidden layer ) 은 ReLU 활 성 화 ( activation ) 로 연 결 되 어 있 고 , 결과 층 의 10 개 유 닛 ( unit ) 들 로 연 결 되 어 있 습 니다 . 여 기 서 우 리 는 nn . Sequential 생 성 자 를 사 용 해 서 빈 네 트 워 크 를 만 들 고 , 그 다 음 에 층 들 을 추 가 했 습 니다 . 아 직 은 nn . Sequential 내 부 에 서 어 떤 일이 벌 어 지 는 지 는 미 스 테 리 로 남 아 있 습 니다 . 아 래 내 용 을 통 해 서 이 것 은 실 제 로 블 럭 을 생 성 하 고 있 는 것 을 확 인 할 것 입 니다 . 이 블 럭 들 은 더 큰 결과 물 로 합 쳐 지 는 데 때 로 는 재 귀 적 으 로 합 쳐 지 기 도 합 니다 . 아 래 그 림 은 이 것 이 어 떻 게 일 어 나 는 지 보 여 줍 니다 . 238 6 . 딥 러 닝 계 산 층 ( layer ) 을 정 의 하 는 것 부 터 ( 하 나 또 는 그 이 상 이 층 들 을 갖 는 ) 블 럭 을 정 의 하 는 데 필 요 한 다 양 한 절 차 에 대 해 서 설 명 하 겠 습 니다 . 블 럭 은 멋 진 층 과 비 슷 하 게 동 작 합 니다 . 즉 , 블 럭 은 아 래 기 능 을 제 공 합 니다 . 1 . 데 이 터 ( 입 력 을 ) 를 받 아야 합 니다 . 2 . 의 미 있 는 결과 를 출 력 해 야 합 니다 . 이 는 forward 라 고 불 리 는 함 수 에 서 처 리 합 니다 . 원 하 는 결과 을 얻 기 위 해 서 net ( x ) 를 통 해 서 블 럭 을 수 행할 수 도 있 는 데 , 실 제 로 는 순 전 파 ( forward propagation ) 을 수 행하 는 forward 함 수 를 호 출 합 니다 . 3 . backward 함 수 가 호 출 되 면 입 력 에 대 해 서 그 래 디 언 트 ( gradient ) 를 생 성 해 야 합 니다 . 일 반 적 으 로 이 것 은 자 동 으 로 이 뤄 집 니다 . 4 . 블 럭 에 속 한 파 라 미 터 들 을 저 장 해 야 합 니다 . 예 를 들 면 , 위 블 럭 은 두 개 의 은 닉 층 ( hidden layer ) 을 갖 는 데 , 파 라 미 터 를 저 장 할 공간 이 있 어야 합 니다 . 6 . 1 . 1 커 스 텀 블 럭 nn . Block 클 래 스 는 우 리 가 필 요 로 하 는 기 능 들 을 제 공 합 니다 . nn 모 듈 에 서 제 공 하 는 모 델 생 성 자 로 , 우 리 가 원 하 는 모 델 을 정 의 하 기 위 해 서 상 속 하 는 클 래 스 입 니다 . 아 래 코 드 는 이 절 을 시 작 할 때 언 급 한 다 층 퍼 셉 트 론 ( multilayer perceptron ) 을 생 성 하 기 위 해 서 Block 클 래 스 를 상 속 하 고 있 습 니다 . 여 기 서 MLP 클 래 스 는 Block 클 래 스 의 _ _ init _ _ 과 forward 함 수 를 오 버 라 이 드 하 고 있 습 니다 . 이 함 수 들 은 각각 모 델 파 라 미 터 들 을 생 성 하 고 forward 계 산 을 정 의 하 는 함 수 입 니다 . Forward 연 산 은 역 전 파 ( forward propagation ) 을 의 미 합 니다 . 6 . 1 . 층 ( layer ) 과 블 럭 ( Block ) 239 [ 2 ] : from mxnet import nd from mxnet . gluon import nn class MLP ( nn . Block ) : # Declare a layer with model parameters . Here , we declare two fully # connected layers def _ _ init _ _ ( self , * * kwargs ) : # Call the constructor of the MLP parent class Block to perform the # necessary initialization . In this way , other function parameters can # also be specified when constructing an instance , such as the model # parameter , params , described in the following sections super ( MLP , self ) . _ _ init _ _ ( * * kwargs ) self . hidden = nn . Dense ( 256 , activation = ' relu ' ) # Hidden layer self . output = nn . Dense ( 10 ) # Output layer # Define the forward computation of the model , that is , how to return the # required model output based on the input x def forward ( self , x ) : return self . output ( self . hidden ( x ) ) 조 금 더 자 세 히 살 펴 보 겠 습 니다 . forward 메 소 드 는 은 닉 층 ( hidden layer ) self . hidden ( x ) 를 계 산 하 고 , 그 값 을 이 용 해 서 결과 층 self . output ( . . . ) 을 계 산 합 니다 . 이 것 이 이 블 럭 의 forward 연 산 에 서 해 야 하 는 일입 니다 . 블 럭 이 어 떤 값 을 사 용 해 서 계 산 을 수 행해 야 하 는 지 를 알 기 위 해 서 , 우 리 는 우 선 층 들 을 정 의 해 야 합 니 다 . 이 는 _ _ init _ _ 메 소 드 가 하 는 일입 니다 . 블 럭 과 관 련 된 모 든 파 라 미 터 들 을 초 기 화 하 고 , 필 요 한 층 을 생 성 합 니다 . 그 리 고 , 관 련 층 들 과 클 래 스 에 필 요 한 파 라 미 터 들 을 정 의 합 니다 . 시스 템 은 그 래 디 언 트 ( gradient ) 를 자 동 으 로 계 산 해 주 는 backward 메 소 드 를 자 동 으 로 생 성 해 줍 니다 . initialize 메 소 드 도 자 동 으 로 생 성 됩 니다 . 한 번 수 행해 보 겠 습 니다 . [ 3 ] : net = MLP ( ) net . initialize ( ) net ( x ) [ 3 ] : [ [ 0 . 00362228 0 . 00633332 0 . 03201144 - 0 . 01369375 0 . 10336449 - 0 . 03508018 - 0 . 00032164 - 0 . 01676023 0 . 06978628 0 . 01303309 ] [ 0 . 03871715 0 . 02608213 0 . 03544959 - 0 . 02521311 0 . 11005433 - 0 . 0143066 - 0 . 03052466 - 0 . 03852827 0 . 06321152 0 . 0038594 ] ] < NDArray 2x10 @ cpu ( 0 ) > 위 에 서 설 명 했 듯 이 , 블 럭 클 래 스 는 무 엇 을 하 는 지 에 따 라 서 아 주 다 르 게 정 의 될 수 있 습 니다 . 예 를 들 어 , 그 것 의 하 위 클 래 스 가 ( Gluon 에 서 제 공 하 는 Dense 클 래 스 와 같 은 ) 층 이 될 수 도 있 고 , ( 우 리 가 240 6 . 딥 러 닝 계 산 막 정 의 한 MLP 클 래 스 와 같 은 ) 모 델 이 될 수 도 있 습 니다 . 또 는 다 른 모 델 의 일 부 가 될 수 도 있 습 니다 . 이 는 아 주 깊 은 네 트 워 크 를 디 자인 할 때 사 용 되 는 방법 입 니다 . 이 장을 통 해 서 우 리 는 이 것 을 아 주 유 연 하 게 사 용 할 수 있 는 방법 에 대 해 서 알아 보 겠 습 니다 . 6 . 1 . 2 Sequential 블 럭 Block 클 래 스 는 데 이 터 흐 름 을 기 술 하 는 일 반 컴 포 넌 트 입 니다 . 사 실 Sequential 클 래 스 는 Block 클 래 스 로 부 터 정 의 됩 니다 . 모 델 을 forward 연 산 은 각 층 에 대 한 연 산 의 단 순 한 연 결 이 기 때 문 에 , 우 리 는 모 델 을 아 주 간 단 한 방법 으 로 정 의 할 수 있 습 니다 . Sequential 클 래 스 의 목 적 은 유 용 한 편 의 함 수 들 을 제 공 하 는 것 에 있 습 니다 . 특 히 , add 메 소 드 는 연 결 된 Block 하 위 클 래 스 의 인 스 턴 스 를 하 나 씩 더 할 수 있 게 해 주 고 , 모 델 의 forward 연 산 은 이 인 스 턴 스 들 을 더 하 기 순 서 대 로 계 산 합 니다 . 아 래 코 드 에 서 MySequential 클 래 스 를 정 의 했 는 데 , 이 는 Sequential 클 래 스 와 같 은 기 능 을 제 공 합 니다 . 이 를 통 해 서 Sequential 클 래 스 가 어 떻 게 동 작 하 는 이 해하 는 데 도 움 이 될 것 입 니다 . [ 4 ] : class MySequential ( nn . Block ) : def _ _ init _ _ ( self , * * kwargs ) : super ( MySequential , self ) . _ _ init _ _ ( * * kwargs ) def add ( self , block ) : # Here , block is an instance of a Block subclass , and we assume it has # a unique name . We save it in the member variable _ children of the # Block class , and its type is OrderedDict . When the MySequential # instance calls the initialize function , the system automatically # initializes all members of _ children self . _ children [ block . name ] = block def forward ( self , x ) : # OrderedDict guarantees that members will be traversed in the order # they were added for block in self . _ children . values ( ) : x = block ( x ) return x add 메 소 드 가 핵 심 입 니다 . 이 메 소 드 는 순 서 가 있 는 사 전 ( dictionary ) 에 블 럭 을 추 가 하 는 일을 합 니다 . 순 전 파 ( forward propagation ) 가 호 출 되 면 이 블 럭 들 은 순 서 대 로 수 행 됩 니다 . MLP 가 어 떻 게 구 현 되 는 지 보 겠 습 니다 . [ 5 ] : net = MySequential ( ) net . add ( nn . Dense ( 256 , activation = ' relu ' ) ) ( continues on next page ) 6 . 1 . 층 ( layer ) 과 블 럭 ( Block ) 241 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) net . add ( nn . Dense ( 10 ) ) net . initialize ( ) net ( x ) [ 5 ] : [ [ 0 . 07787765 0 . 00216401 0 . 01682201 0 . 03059879 - 0 . 00702019 0 . 01668714 0 . 04822845 0 . 00394321 - 0 . 09300036 - 0 . 044943 ] [ 0 . 08891079 - 0 . 00625484 - 0 . 01619132 0 . 03807178 - 0 . 01451489 0 . 02006172 0 . 0303478 0 . 02463485 - 0 . 07605445 - 0 . 04389167 ] ] < NDArray 2x10 @ cpu ( 0 ) > 실 제 로 , “ 다 층 퍼 셉 트 론 ( multilayer perceptron ) 의 간결 한 구 현 ” 에 서 Sequential 클 래 스 를 사 용 한 것과 MySequential 클 래 스 를 사 용 한 것 이 다 르 지 않 다는 것 을 볼 수 있 습 니다 . 6 . 1 . 3 코 드 와 블 록 ( Block ) Sequential 클 래 스 가 모 델 생 성 을 쉽 게 해 주 고 forward 메 소 드 를 별 도 로 구 현 할 필 요 가 없 게 해 주 지 만 , Block 클 래 스 를 직 접 상 속 하 면 더 유 연 한 모 델 생 성 을 할 수 있 습 니다 . 특 히 , forward 메 소 드 에 서 Python 의 제 어 흐 름 을 이 용 하 는 것 을 예 로 들 어 보 겠 습 니다 . 설 명 하 기 에 앞 서서 constant 파 라 미 터 라 는 개 념 에 대 해 서 알아 보 겠 습 니다 . 이 파 라 미 터 들 은 역 전 파 ( back propagation ) 이 호 출 되 었 을 때 사 용 되 지 는 않 습 니다 . 추 상 적 으 로 들 릴 수 있 지 만 , 실 제 일 어 나 는 일이 그 렇 습 니다 . 어 떤 함 수 가 있 다 고 가 정 합 니다 . 𝑓 ( x , w ) = 3 · w ⊤ x . 이 경 우 , 3 이 상 수 ( constant ) 파 라 미 터 입 니다 . 우 리 는 3 을 다 른 값 , 예 를 들 어 𝑐 로 바 꿔 서 다 음 과 같 이 표 현 할 수 있 습 니다 . 𝑓 ( x , w ) = 𝑐 · w ⊤ x . 𝑐 의 값 을 조절 할 수 있 게 된 것 이 외 에 는 바 뀐 것 이 없 습 니다 . w 와 x 만 을 생 각 해 보 면 여 전 히 상 수 입 니다 . 하 지 만 , Gluon 은 이 것 을 미 리 알 지 못 하 기 때 문 에 , 도 움 을 주 는 것 이 필 요 합 니다 . 이 렇 게 하 는 것 은 Gluon 이 변 하 지 않 는 파 라 미 터 에 대 해 서 는 신 경 쓰 지 않 도 록 할 수 있 기 때 문 에 코 드 가 더 빠 르 게 수 행 되 게 해 줍 니다 . get _ constant 메 소 드 을 이 용 하 면 됩 니다 . 실 제 어 떻 게 구 현 되 는 지 살 펴 보 겠 습 니다 . 242 6 . 딥 러 닝 계 산 [ 6 ] : class FancyMLP ( nn . Block ) : def _ _ init _ _ ( self , * * kwargs ) : super ( FancyMLP , self ) . _ _ init _ _ ( * * kwargs ) # Random weight parameters created with the get _ constant are not # iterated during training ( i . e . constant parameters ) self . rand _ weight = self . params . get _ constant ( ' rand _ weight ' , nd . random . uniform ( shape = ( 20 , 20 ) ) ) self . dense = nn . Dense ( 20 , activation = ' relu ' ) def forward ( self , x ) : x = self . dense ( x ) # Use the constant parameters created , as well as the relu and dot # functions of NDArray x = nd . relu ( nd . dot ( x , self . rand _ weight . data ( ) ) + 1 ) # Reuse the fully connected layer . This is equivalent to sharing # parameters with two fully connected layers x = self . dense ( x ) # Here in Control flow , we need to call asscalar to return the scalar # for comparison while x . norm ( ) . asscalar ( ) > 1 : x / = 2 if x . norm ( ) . asscalar ( ) < 0 . 8 : x * = 10 return x . sum ( ) FancyMLP 모 델 에 서 rand _ weight 라 는 상 수 가 중 치 를 정 의 했 습 니다 . ( 이 변 수 는 모 델 파 라 미 터 는 아 니다 라 는 것 을 알아 두 세 요 ) . 그 리 고 , 행 렬 곱 하 기 연 산 ( nd . dot ( ) ) 을 수 행하 고 , 같 은 Dense 층 을 재 사 용 합 니다 . 서 로 다 른 파 라 미 터 세 트 를 사 용 한 두 개 의 덴 스 층 ( dense layer ) 를 사 용 했 던 것과 다 른 형 태 로 구 현 되 었 음을 주 목 하 세 요 . 우 리 는 대 신 , 같 은 네 트 워 크 를 두 번 사 용 했 습 니다 . 네 트 워 크 의 여 러 부분 이 같 은 파 라 미 터 를 공 유 하 는 경 우 딥 네 트 워 크 에 서 이 것 을 파 라 미 터 가 서 로 묶 여 있 다 ( tied ) 라 고 말 하 기 도 합 니다 . 이 클 래 스 에 대 한 인 스 턴 스 를 만 들 어 서 데 이 터 를 입 력 하 면 어 떤 일이 일 어 나 는 지 보 겠 습 니다 . [ 7 ] : net = FancyMLP ( ) net . initialize ( ) net ( x ) [ 7 ] : [ 25 . 522684 ] < NDArray 1 @ cpu ( 0 ) > 네 트 워 크 를 만 들 때 이 런 방법 을 섞 어 서 사 용 하 지 않 을 이유 가 없 습 니다 . 아 래 예 제 를 보 면 어 쩌 면 키 메 라 와 닮 아 보 일 수 도 있 고 조 금 다 르 게 말 하 면 , Rube Goldberg Machine 과 비 슷 하 다 고 할 수 도 있 6 . 1 . 층 ( layer ) 과 블 럭 ( Block ) 243 습 니다 . 즉 , 개 별 적 인 블 럭 을 합 쳐 서 블 럭 을 만 들 고 이 렇 게 만 들 어 진 블 럭 이 다 시 블 럭 으 로 사 용 될 수 있 는 것 의 예 제 를 다 음 과 같 이 만 들 어 볼 수 있 습 니다 . 더 나 아 가 서 는 같 은 forward 함 수 안에 서 여 러 전 략 을 합 치 는 것 도 가 능 합 니다 . 아 래 코 드 가 그 런 예 입 니다 . [ 8 ] : class NestMLP ( nn . Block ) : def _ _ init _ _ ( self , * * kwargs ) : super ( NestMLP , self ) . _ _ init _ _ ( * * kwargs ) self . net = nn . Sequential ( ) self . net . add ( nn . Dense ( 64 , activation = ' relu ' ) , nn . Dense ( 32 , activation = ' relu ' ) ) self . dense = nn . Dense ( 16 , activation = ' relu ' ) def forward ( self , x ) : return self . dense ( self . net ( x ) ) chimera = nn . Sequential ( ) chimera . add ( NestMLP ( ) , nn . Dense ( 20 ) , FancyMLP ( ) ) chimera . initialize ( ) chimera ( x ) [ 8 ] : [ 30 . 518448 ] < NDArray 1 @ cpu ( 0 ) > 6 . 1 . 4 컴 파 일 여 러 분 이 관 심 이 많 다 면 이 런 접 근 방법 의 효 율 에 대 한 의 심 을 할 것 입 니다 . 결 국 에 는 많 은 사 전 ( dictio - nary ) 참 조 , 코 드 수 행 과 다 른 Python 코 드들 수 행하 면 서 성 능 이 높 은 딥 러 닝 라 이 브 러 리를 만 들 어야 합 니다 . Python 의 Global Interpreter Lock 은 아 주 잘 알 려 진 문 제 로 , 아 주 성 능 이 좋 은 GPU 를 가 지 고 있을 지 라 도 단 일 CPU 코 어에 서 수 행 되 는 Python 프 로 그 램 이 다 음 에 무 엇 을 해 야 할 지 를 알 려 주 기 를 기 다 려 야 하 기 때 문 에 딥 러 닝 환 경 에 서 성 능 에 안 좋 은 영 향 을 미 칩 니다 . 당 연 하 게 도 아 주 나 쁜 상 황 이 지 만 , 이 를 우 회 하 는 여 러 방법 들 이 존 재 합 니다 . Python 속 도 를 향 상 시 키 는 방법 은 이 모 든 것 을 모 두 제 거 하 는 것 이 최 선 입 니다 . Gluon 은 Hybridization 기 능 을 통 해 서 해 결 하 고 있 습 니다 . Python 코 드 블 럭 이 처 음 수 행 되 면 Gluon 런 타 임은 무 엇 이 수 행 되 었 는 지 를 기 록 하 고 , 이 후 에 수 행 될 때 는 Python 을 호 출 하 지 않 고 빠 른 코 드 를 수 행합 니다 . 이 방법 은 속 도 를 상 당 히 빠 르 게 해 주 지 만 , 제 어 흐 름 을 다 루 는 데 주 의 를 기 울 여야 합 니 다 . 하 이 브 리 드 화 ( Hybridization ) 와 컴 파 일 ( compilation ) 에 대 해 서 더 관 심 이 있 다 면 이 장을 마 치 고 , 해 당 내 용 이 있 는 절 을 읽 어 보 세 요 . 244 6 . 딥 러 닝 계 산 6 . 1 . 5 요 약 • 층 들 은 블 럭 입 니다 . • 많 은 층 들 이 하 나 의 블 럭 이 될 수 있 습 니다 . • 많 은 블 럭 들 이 하 나 의 블 럭 이 될 수 있 습 니다 . • 코 드 도 블 럭 이 될 수 있 습 니다 . • 블 럭 은 파 라 미 터 초 기 화 , 역 전 파 ( back propagation ) 또 는 관 련 된 일을 대 신 처 리 해 줍 니다 . • 층 들 과 블 럭 들 을 순 차 적 으 로 연 결 하 는 것 은 Sequential 블 럭 에 의 해 서 처 리 됩 니다 . 6 . 1 . 6 문 제 1 . What kind of error message will you get when calling an _ _ init _ _ method whose parent class not in the _ _ init _ _ function of the parent class ? 2 . FancyMLP 클 래 스 에 서 asscalar 함 수 를 삭 제 하 면 어 떤 문 제 가 발 생 하 나 요 ? 3 . NestMLP 클 래 스 에 서 Sequential 클 래 스 의 인 스 턴 스 로 정 의 된 self . net 을 self . net = [ nn . Dense ( 64 , activation = ' relu ' ) , nn . Dense ( 32 , activation = ' relu ' ) ] 로 바 꾸 면 어 떤 문 제 가 발 생 하 나 요 ? 4 . 두 블 럭 ( net1 과 net2 ) 를 인자 로 받 아 서 forward pass 의 두 네 트 워 크 의 결과 를 연 결 해 서 반 환 하 는 블 럭 을 작 성 해 보 세 요 . ( 이 는 parallel 블 럭 이 라 고 합 니다 ) 5 . 같 은 네 트 워 크 의 여 러 인 스 턴 스 를 연 결 하 고 자 가 정 합 니다 . 같 은 블 럭 의 여 러 인 스 턴 스 를 생 성 하 는 factory 함 수 를 작 성 하 고 , 이 를 사 용 해 서 더 큰 네 트 워 크 를 만 들 어 보 세 요 . 6 . 1 . 7 Scan the QR Code to Discuss 6 . 1 . 층 ( layer ) 과 블 럭 ( Block ) 245 6 . 2 파 라 미 터 관 리 딥 네 트 워 크 학 습 의 최 종 목 표 는 주 어 진 아 키 텍 처 에 가 장 잘 맞 는 파 라 미 터 값 들 을 찾 는 것 입 니다 . 일 반 적 인 것 또 는 표 준 에 준 하 는 것 들 을 다 룰 때 는 nn . Sequential 클 래 스 가 이 를 위 한 완 벽 한 도 구 가 될 수 있 습 니다 . 하 지 만 , 소 수 의 모 델 이 완 전 히 표 준 이 고 , 대 부분 의 과 학 자 들 은 독 창 적 인 것 을 만 들 기 를 원 합 니다 . 이 절 에 서 는 파 라 미 터 를 다 루 는 방법 에 대 해 서 살 펴 보 겠 습 니다 . 좀 더 자 세 하 게 는 아 래 와 같 은 것 들 을 포 함합 니다 . • 디 버 깅 이 나 분 석 을 위 해 서 파 라 미 터 를 접 근 하 고 , 그 것 들 을 시 각 화 하 거 나 저 장 하 는 것 을 통 해 서 커 스 텀 모 델 을 어 떻 게 만 들 어야 하 는 지 이 해 를 시 작 하 겠 습 니다 . • 다 음으 로 는 초 기 화 목 적 등 을 위 해 서 특 별 한 방법 으 로 파 라 미 터 들 을 설 정 해 야 하 는 데 , 이 를 위 해 서 파 라 미 터 초 기 화 도 구 의 구 조 에 대 해 서 논 의 합 니다 . • 마 지 막 으 로 일 부 파 라 미 터 를 공 유 하 는 네 트 워 크 를 만 들 면 서 이 내 용 들 이 어 떻 게 적 용 되 는 지 보 겠 습 니다 . 지 금까 지 그 랬 듯 이 은 닉 층 ( hidden layer ) 을 갖 는 다 층 퍼 셉 트 론 ( multilayer perceptron ) 으 로 부 터 시 작 하 겠 습 니다 . 이 를 이 용 해 서 다 양 한 특 징 들 을 살 펴 봅 니다 . [ 1 ] : from mxnet import init , nd from mxnet . gluon import nn net = nn . Sequential ( ) net . add ( nn . Dense ( 256 , activation = ' relu ' ) ) net . add ( nn . Dense ( 10 ) ) net . initialize ( ) # Use the default initialization method x = nd . random . uniform ( shape = ( 2 , 20 ) ) net ( x ) # Forward computation [ 1 ] : [ [ 0 . 09543004 0 . 04614332 - 0 . 00286654 - 0 . 07790349 - 0 . 05130243 0 . 02942037 0 . 08696642 - 0 . 0190793 - 0 . 04122177 0 . 05088576 ] [ 0 . 0769287 0 . 03099705 0 . 00856576 - 0 . 04467199 - 0 . 06926839 0 . 09132434 0 . 06786595 - 0 . 06187842 - 0 . 03436673 0 . 04234694 ] ] < NDArray 2x10 @ cpu ( 0 ) > 246 6 . 딥 러 닝 계 산 6 . 2 . 1 파 라 미 터 접 근 Sequential 클 래 스 의 경 우 , 네 트 워 크 의 각 층 의 인 덱 스 를 사 용 해 서 파 라 미 터 를 쉽 게 접 근 할 수 있 습 니 다 . params 변 수 가 필 요 한 데 이 터 를 가 지 고 있 습 니다 . 자 그 럼 첫 번 째 층 의 파 라 미 터 를 조 사 하 는 것 을 직 접 해 보 겠 습 니다 . [ 2 ] : print ( net [ 0 ] . params ) print ( net [ 1 ] . params ) dense0 _ ( Parameter dense0 _ weight ( shape = ( 256 , 20 ) , dtype = float32 ) Parameter dense0 _ bias ( shape = ( 256 , ) , dtype = float32 ) ) dense1 _ ( Parameter dense1 _ weight ( shape = ( 10 , 256 ) , dtype = float32 ) Parameter dense1 _ bias ( shape = ( 10 , ) , dtype = float32 ) ) 위 코 드 의 수 행 결과 는 많 은 것 을 우 리 에 게 알 려 줍 니다 . 첫 번 째 정 보 는 예 상 대 로 이 층 은 파 라 미 터 들 의 두 개 의 세 트 , dense0 _ weight 와 dense0 _ bias , 로 구 성 되 어 있 는 것 을 확 인 할 수 있 습 니다 . 이 값 들 은 모 두 싱 글 프 리 시 전 ( single precision ) 이 고 , 입 력 차 원 이 20 이 고 출 력 차 원 이 256 인 첫 번 째 층 에 필 요 한 모 양 ( shape ) 을 갖고 있 습 니다 . 특 히 , 파 라 미 터 들 의 이 름 이 주 어 지 는 데 이 는 아 주 유 용 합 니다 . 이 름 을 사 용 하 면 간 단 하 지 않 은 구 조 를 갖 는 수 백 개 의 층 들 로 구 성 된 네 트 워 크 에 서 파 라 미 터 를 쉽 게 지 정 할 수 있 기 때 문 입 니다 . 두 번 째 층 도 같 은 방 식 으 로 구 성 되 어 있 는 것 을 확 인 할 수 있 습 니다 . 지 정 된 파 라 미 터 파 라 미 터 를 가 지 고 뭔 가 유 용 한 일을 하 기 를 원 한 다 면 이 값 들 을 접 근 할 수 있 어야 합 니다 . 간 단 한 방법 부 터 일 반 적 인 방법 까 지 다 양 한 방법 이 있 는 데 , 몇 가 지 를 살 펴 보 겠 습 니다 . [ 3 ] : print ( net [ 1 ] . bias ) print ( net [ 1 ] . bias . data ( ) ) Parameter dense1 _ bias ( shape = ( 10 , ) , dtype = float32 ) [ 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . ] < NDArray 10 @ cpu ( 0 ) > 첫 번 째 코 드 는 두 번 째 층 의 편 향 ( bias ) 를 출 력 합 니다 . 이 는 데 이 터 , 그 래 디 언 트 ( gradient ) 그 리 고 추 가 적 인 정 보 를 가 지 고 있 는 객 체 이 기 에 , 우 리 는 데 이 터 를 명 시 적 으 로 접 근 해 야 합 니다 . 우 리 는 편 향 6 . 2 . 파 라 미 터 관 리 247 ( bias ) 을 모 두 0 으 로 초 기 화 했 기 때 문 에 편 향 ( bias ) 이 모 두 0 임을 기 억 해 두 기 바 랍 니다 . 이 값 은 파 라 미 터 의 이 름 , dense0 _ weight , 을 이 용 해 서 직 접 접 근 할 수 도 있 습 니다 . 이 렇 게 할 수 있 는 이유 는 모 든 레 이 어 는 직 접 접 근 할 수 있 는 고 유의 파 라 미 터 사 전 ( dictionary ) 를 갖고 있 기 때 문 입 니다 . 이 두 방법 은 완 전 이 동 일 하 나 , 첫 번 째 방법 이 조 금 더 읽 기 쉽습 니다 . [ 4 ] : print ( net [ 0 ] . params [ ' dense0 _ weight ' ] ) print ( net [ 0 ] . params [ ' dense0 _ weight ' ] . data ( ) ) Parameter dense0 _ weight ( shape = ( 256 , 20 ) , dtype = float32 ) [ [ 0 . 06700657 - 0 . 00369488 0 . 0418822 . . . - 0 . 05517294 - 0 . 01194733 - 0 . 00369594 ] [ - 0 . 03296221 - 0 . 04391347 0 . 03839272 . . . 0 . 05636378 0 . 02545484 - 0 . 007007 ] [ - 0 . 0196689 0 . 01582889 - 0 . 00881553 . . . 0 . 01509629 - 0 . 01908049 - 0 . 02449339 ] . . . [ 0 . 00010955 0 . 0439323 - 0 . 04911506 . . . 0 . 06975312 0 . 0449558 - 0 . 03283203 ] [ 0 . 04106557 0 . 05671307 - 0 . 00066976 . . . 0 . 06387014 - 0 . 01292654 0 . 00974177 ] [ 0 . 00297424 - 0 . 0281784 - 0 . 06881659 . . . - 0 . 04047417 0 . 00457048 0 . 05696651 ] ] < NDArray 256x20 @ cpu ( 0 ) > 가 중 치 들 이 모 두 0 이 아 닌 값 으 로 되 어 있음을 주 목 하 세 요 . 우 리 가 네 트 워 크 를 만 들 때 , 이 값 들 은 난 수 값 으 로 초 기 화 했 기 때 문 에 그 렇 습 니다 . data 함 수 만 있 는 것 이 아 닙니다 . 예 를 들 어 파 라 미 터 에 대 해 서 그 래 디 언 트 ( gradient ) 를 계 산 하 고 자 할 수 도 있 습 니다 . 이 결과 는 가 중 치 와 같 은 모 양 ( shape ) 을 갖게 됩 니다 . 하 지 만 , 역 전 파 ( back propagation ) 을 아 직 실 행하 지 않았 기 때 문 에 이 값 들 은 모 두 0 으 로 보 여 질 것 입 니다 . [ 5 ] : net [ 0 ] . weight . grad ( ) [ 5 ] : [ [ 0 . 0 . 0 . . . . 0 . 0 . 0 . ] [ 0 . 0 . 0 . . . . 0 . 0 . 0 . ] [ 0 . 0 . 0 . . . . 0 . 0 . 0 . ] . . . [ 0 . 0 . 0 . . . . 0 . 0 . 0 . ] [ 0 . 0 . 0 . . . . 0 . 0 . 0 . ] [ 0 . 0 . 0 . . . . 0 . 0 . 0 . ] ] ( continues on next page ) 248 6 . 딥 러 닝 계 산 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) < NDArray 256x20 @ cpu ( 0 ) > 한 번 에 모 든 파 라 미 터 지 정 위 방법 으 로 파 라 미 터 를 접 근 하 는 것 은 다 소 지 루 할 수 있 습 니다 . 특 히 , 더 복 잡 한 블 럭 들 을 갖거 나 , 블 럭 들 로 구 성 된 블 럭 ( 심 지 어 는 블 럭 들 을 블 럭 들 의 블 럭 ) 으 로 구 성 된 네 트 워 크 인 경 우 , 블 럭 들 이 어 떻 게 생 성 되 었 는 지 알 기 위 해 서 전 체 트 리를 모 두 뒤 져 봐 야 하 는 경 우 가 그 런 예 입 니다 . 이 를 피하 기 위 해 서 , 블 럭 은 collect _ params 라 는 메 소 드 를 제 공 하 는 데 이 를 이 용 하 면 네 트 워 크 의 모 든 파 라 미 터 를 하 나 의 사 전 ( dictionary ) 에 담 아 주 고 , 쉽 게 조 회 할 수 있 습 니다 . 이 는 내 부 적 으 로 블 럭 의 모 든 구 성 요 소 들 을 방 문 하 면 서 필 요 한 경 우 서 브블 럭 들 에 collect _ params 함 수 를 호 출 하 는 식 으 로 동 작 합 니다 . 차 이 를 확 인 하 기 위 해 서 아 래 코 드 를 살 펴 보 겠 습 니다 . [ 6 ] : # parameters only for the first layer print ( net [ 0 ] . collect _ params ( ) ) # parameters of the entire network print ( net . collect _ params ( ) ) dense0 _ ( Parameter dense0 _ weight ( shape = ( 256 , 20 ) , dtype = float32 ) Parameter dense0 _ bias ( shape = ( 256 , ) , dtype = float32 ) ) sequential0 _ ( Parameter dense0 _ weight ( shape = ( 256 , 20 ) , dtype = float32 ) Parameter dense0 _ bias ( shape = ( 256 , ) , dtype = float32 ) Parameter dense1 _ weight ( shape = ( 10 , 256 ) , dtype = float32 ) Parameter dense1 _ bias ( shape = ( 10 , ) , dtype = float32 ) ) 이 렇 게 해 서 네 트 워 크 의 파 라 미 터 를 접 근 하 는 세 번 째 방법 을 배 웠 습 니다 . 두 번 째 층 의 편 향 ( bias ) 값 을 확 인 하 는 코 드 는 아 래 와 같 이 간 단 하 게 작 성 할 수 있 습 니다 . [ 7 ] : net . collect _ params ( ) [ ' dense1 _ bias ' ] . data ( ) [ 7 ] : [ 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . ] < NDArray 10 @ cpu ( 0 ) > 이 책 에 서 설 명 을 계 속 하 면 서 , 블 럭 들 의 하 위 블 럭 에 이 름 이 어 떻 게 부 여 되 는 지 보 게 될 것 입 니다 . ( 그 6 . 2 . 파 라 미 터 관 리 249 중 에 , Sequential 의 경 우 는 숫 자 를 할 당 합 니다 . ) 이 름 할 당 규 칙 은 필 요 한 파 라 미 터 만 필 터 링 하 는 정 규 식 을 사 용 할 수 있 게 해 서 아 주 편 리 합 니다 . [ 8 ] : print ( net . collect _ params ( ' . * weight ' ) ) print ( net . collect _ params ( ' dense0 . * ' ) ) sequential0 _ ( Parameter dense0 _ weight ( shape = ( 256 , 20 ) , dtype = float32 ) Parameter dense1 _ weight ( shape = ( 10 , 256 ) , dtype = float32 ) ) sequential0 _ ( Parameter dense0 _ weight ( shape = ( 256 , 20 ) , dtype = float32 ) Parameter dense0 _ bias ( shape = ( 256 , ) , dtype = float32 ) ) 루 브 골 드 버 그 가 다 시 공격 하 다 . 블 럭 들 이 중 첩 되 어 있 는 경 우 파 라 미 터 의 이 름 이 어 떤 식 으 로 매 겨 지 는 지 보 겠 습 니다 . 이 를 위 해 서 우 리 는 블 럭 들 을 생 성 하 는 함 수 ( block factory 라 고 불 릴 수 있 는 ) 를 정 의 하 고 , 이 를 이 용 해 서 더 큰 블 럭 들 이 블 럭 을 포 함 시 켜 보 겠 습 니다 . [ 9 ] : def block1 ( ) : net = nn . Sequential ( ) net . add ( nn . Dense ( 32 , activation = ' relu ' ) ) net . add ( nn . Dense ( 16 , activation = ' relu ' ) ) return net def block2 ( ) : net = nn . Sequential ( ) for i in range ( 4 ) : net . add ( block1 ( ) ) return net rgnet = nn . Sequential ( ) rgnet . add ( block2 ( ) ) rgnet . add ( nn . Dense ( 10 ) ) rgnet . initialize ( ) rgnet ( x ) [ 9 ] : [ [ 1 . 0116727e - 08 - 9 . 4839003e - 10 - 1 . 1526797e - 08 1 . 4917443e - 08 ( continues on next page ) 250 6 . 딥 러 닝 계 산 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) - 1 . 5690811e - 09 - 3 . 9257650e - 09 - 4 . 1441655e - 09 9 . 3013472e - 09 3 . 2393586e - 09 - 4 . 8612452e - 09 ] [ 9 . 0111598e - 09 - 1 . 9115812e - 10 - 8 . 9595842e - 09 1 . 0745880e - 08 1 . 4963460e - 10 - 2 . 2272872e - 09 - 3 . 9153973e - 09 7 . 0595711e - 09 3 . 4854222e - 09 - 4 . 5807327e - 09 ] ] < NDArray 2x10 @ cpu ( 0 ) > 네 트 워 크 를 설 계 했 으 니 , 어 떻 게 구 성 되 는 지 확 인 해 봅 니다 . collect _ params 를 이 용 하 면 이 름 과 논 리 적 인 구 조 에 대 한 정 보 를 얻 을 수 있 습 니다 . [ 10 ] : print ( rgnet . collect _ params ) print ( rgnet . collect _ params ( ) ) < bound method Block . collect _ params of Sequential ( ( 0 ) : Sequential ( ( 0 ) : Sequential ( ( 0 ) : Dense ( 20 - > 32 , Activation ( relu ) ) ( 1 ) : Dense ( 32 - > 16 , Activation ( relu ) ) ) ( 1 ) : Sequential ( ( 0 ) : Dense ( 16 - > 32 , Activation ( relu ) ) ( 1 ) : Dense ( 32 - > 16 , Activation ( relu ) ) ) ( 2 ) : Sequential ( ( 0 ) : Dense ( 16 - > 32 , Activation ( relu ) ) ( 1 ) : Dense ( 32 - > 16 , Activation ( relu ) ) ) ( 3 ) : Sequential ( ( 0 ) : Dense ( 16 - > 32 , Activation ( relu ) ) ( 1 ) : Dense ( 32 - > 16 , Activation ( relu ) ) ) ) ( 1 ) : Dense ( 16 - > 10 , linear ) ) > sequential1 _ ( Parameter dense2 _ weight ( shape = ( 32 , 20 ) , dtype = float32 ) Parameter dense2 _ bias ( shape = ( 32 , ) , dtype = float32 ) Parameter dense3 _ weight ( shape = ( 16 , 32 ) , dtype = float32 ) Parameter dense3 _ bias ( shape = ( 16 , ) , dtype = float32 ) ( continues on next page ) 6 . 2 . 파 라 미 터 관 리 251 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) Parameter dense4 _ weight ( shape = ( 32 , 16 ) , dtype = float32 ) Parameter dense4 _ bias ( shape = ( 32 , ) , dtype = float32 ) Parameter dense5 _ weight ( shape = ( 16 , 32 ) , dtype = float32 ) Parameter dense5 _ bias ( shape = ( 16 , ) , dtype = float32 ) Parameter dense6 _ weight ( shape = ( 32 , 16 ) , dtype = float32 ) Parameter dense6 _ bias ( shape = ( 32 , ) , dtype = float32 ) Parameter dense7 _ weight ( shape = ( 16 , 32 ) , dtype = float32 ) Parameter dense7 _ bias ( shape = ( 16 , ) , dtype = float32 ) Parameter dense8 _ weight ( shape = ( 32 , 16 ) , dtype = float32 ) Parameter dense8 _ bias ( shape = ( 32 , ) , dtype = float32 ) Parameter dense9 _ weight ( shape = ( 16 , 32 ) , dtype = float32 ) Parameter dense9 _ bias ( shape = ( 16 , ) , dtype = float32 ) Parameter dense10 _ weight ( shape = ( 10 , 16 ) , dtype = float32 ) Parameter dense10 _ bias ( shape = ( 10 , ) , dtype = float32 ) ) 층 들 이 계 층 적 으 로 생 성 되 어 있으 니 , 우 리 도 층 들 을 그 렇 게 접 근 할 수 있 습 니다 . 예 를 들 어 서 , 첫 번 째 큰 블 럭 의 두 번 째 하 위 블 럭 의 첫 번 째 층 의 편 향 ( bias ) 값 은 다 음 과 같 이 접 근 이 가 능 합 니다 . [ 11 ] : rgnet [ 0 ] [ 1 ] [ 0 ] . bias . data ( ) [ 11 ] : [ 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . ] < NDArray 32 @ cpu ( 0 ) > 6 . 2 . 2 파 라 미 터 초 기 화 자 이 제 파 라 미 터 를 어 떻 게 접 근 할 수 있 는 지 알 게 되 었 으 니 , 파 라 미 터 를 어 떻 게 적절 하 게 초 기 화 할 수 있을 지 를 살 펴 볼 차 례 입 니다 . 이 전 장 에 서 초 기 화 가 왜 필 요 한 지 를 설 명 했 습 니다 . 기 본 설 명 으 로 는 MXNet 은 가 중 치 행 렬 은 𝑈 [ − 0 . 07 , 0 . 07 ] 을 따 르 는 균 일 한 난 수 로 , 편 향 ( bias ) 파 라 미 터 는 모 두 0 으 로 설 정 합 니다 . 하 지 만 , 때 로 는 가 중 치 값 을 다 르 게 초 기 화 해 야 할 필 요 가 있 습 니다 . MXNet 의 init 모 듈 은 미 리 설 정 된 다 양 한 초 기 화 방법 들 을 제 공 하 는 데 , 만 약 특 별 한 방법 으 로 초 기 화 하 는 것 이 필 요 하 다 면 몇 가 지 추 가 적 인 일이 필 요 합 니다 . 252 6 . 딥 러 닝 계 산 제 공 되 는 초 기 화 빌 트 인 초 기 화 방법 들 을 우 선 살 펴 보 겠 습 니다 . 아 래 코 드 는 모 든 파 라 미 터 를 Gaussian 확 률 변 수 로 초 기 화 하 는 예 제 입 니다 . [ 12 ] : # force _ reinit ensures that the variables are initialized again , regardless of # whether they were already initialized previously net . initialize ( init = init . Normal ( sigma = 0 . 01 ) , force _ reinit = True ) net [ 0 ] . weight . data ( ) [ 0 ] [ 12 ] : [ - 0 . 008166 - 0 . 00159167 - 0 . 00273115 0 . 00684697 0 . 01204039 0 . 01359703 0 . 00776908 - 0 . 00640936 0 . 00256858 0 . 00545601 0 . 0018105 - 0 . 00914027 0 . 00133803 0 . 01070259 - 0 . 00368285 0 . 01432678 0 . 00558631 - 0 . 01479764 0 . 00879013 0 . 00460165 ] < NDArray 20 @ cpu ( 0 ) > 만 약 파 라 미 터 들 을 모 두 1 로 초 기 화 하 고 싶 다 면 , 초 기 화 방법 을 Constant ( 1 ) 로 바 꾸 기 만 하 면 됩 니다 . [ 13 ] : net . initialize ( init = init . Constant ( 1 ) , force _ reinit = True ) net [ 0 ] . weight . data ( ) [ 0 ] [ 13 ] : [ 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . ] < NDArray 20 @ cpu ( 0 ) > 만 약 특 정 파 라 미 터 만 다 른 방법 으 로 초 기 화 를 하 고 싶 다 면 , 해 당 하 는 서 브블 럭 에 초 기 화 함 수 를 지 정 하 는 것 으 로 간 단 히 구 현 할 수 있 습 니다 . 예 를 들 어 , 아 래 코 드 는 두 번 째 층 을 42 라 는 값 으 로 초 기 화 하 고 , 첫 번 째 층 의 가 중 치 들 은 Xavier 초 기 화 방법 을 적 용 하 고 있 습 니다 . [ 14 ] : net [ 1 ] . initialize ( init = init . Constant ( 42 ) , force _ reinit = True ) net [ 0 ] . weight . initialize ( init = init . Xavier ( ) , force _ reinit = True ) print ( net [ 1 ] . weight . data ( ) [ 0 , 0 ] ) print ( net [ 0 ] . weight . data ( ) [ 0 ] ) [ 42 . ] < NDArray 1 @ cpu ( 0 ) > [ - 0 . 14511706 - 0 . 01173057 - 0 . 03754489 - 0 . 14020921 0 . 00900492 0 . 01712246 0 . 12447387 - 0 . 04094418 - 0 . 12105145 0 . 00079902 - 0 . 0277361 - 0 . 10213967 - 0 . 14027238 - 0 . 02196661 - 0 . 04641148 0 . 11977354 0 . 03604397 - 0 . 14493202 ( continues on next page ) 6 . 2 . 파 라 미 터 관 리 253 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) - 0 . 06514931 0 . 13826048 ] < NDArray 20 @ cpu ( 0 ) > 커 스 텀 초 기 화 때 로 는 우 리 가 필 요 한 초 기 화 방법 이 init 모 듈 에 없 을 수 도 있 습 니다 . 이 경 우 에 는 , Initializer 클 래 스 의 하 위 클 래 스 를 정 의 해 서 다 른 초 기 화 메 소 드 와 같 은 방법 으 로 사 용 할 수 있 습 니다 . 보 통 은 , _ init _ weight 함 수 만 구 현 하 면 됩 니다 . 이 함 수 는 입 력 받 은 NDArray 를 원 하 는 초 기 값 으 로 바 꿔 줍 니다 . 아 래 예 제 에 서 는 이 를 잘 보 여 주 기 위 해 서 다 소 이 상 하 고 특 이 한 분 포 를 사 용 해 서 값 을 초 기 화 합 니다 . 𝑤 ∼ ⎧⎪⎪⎪⎨ ⎪⎪⎪⎩ 𝑈 [ 5 , 10 ] with probability 14 0 with probability 12 𝑈 [ − 10 , − 5 ] with probability 14 [ 15 ] : class MyInit ( init . Initializer ) : def _ init _ weight ( self , name , data ) : print ( ' Init ' , name , data . shape ) data [ : ] = nd . random . uniform ( low = - 10 , high = 10 , shape = data . shape ) data * = data . abs ( ) > = 5 net . initialize ( MyInit ( ) , force _ reinit = True ) net [ 0 ] . weight . data ( ) [ 0 ] Init dense0 _ weight ( 256 , 20 ) Init dense1 _ weight ( 10 , 256 ) [ 15 ] : [ - 5 . 44481 6 . 536484 - 0 . 0 . 0 . 7 . 7452965 7 . 739216 7 . 6021366 0 . - 0 . - 7 . 3307705 - 0 . 9 . 611603 0 . 7 . 4357147 0 . 0 . - 0 . 8 . 446959 0 . ] < NDArray 20 @ cpu ( 0 ) > 이 기 능 이 충 분 하 지 않 을 경 우 에 는 , 파 라 미 터 값 을 직 접 설 정 할 수 도 있 습 니다 . data ( ) 는 NDArray 를 반 환 하 기 때 문 에 , 이 를 이 용 하 면 일 반 적 인 행 렬 처 럼 사 용 하 면 됩 니다 . 고 급 사 용 자 들 을 위 해 서 조 금 더 설 명 하 면 , autograd 범 위 안에 서 파 라 미 터 를 조정 하 는 경 우 에 는 , 자 동 미 분 기 능 이 오 작 동 하 지 254 6 . 딥 러 닝 계 산 않 도 록 set _ data 를 사 용 해 야 하 는 것 을 기 억 해 두 세 요 . [ 16 ] : net [ 0 ] . weight . data ( ) [ : ] + = 1 net [ 0 ] . weight . data ( ) [ 0 , 0 ] = 42 net [ 0 ] . weight . data ( ) [ 0 ] [ 16 ] : [ 42 . 7 . 536484 1 . 1 . 1 . 8 . 7452965 8 . 739216 8 . 602137 1 . 1 . - 6 . 3307705 1 . 10 . 611603 1 . 8 . 435715 1 . 1 . 1 . 9 . 446959 1 . ] < NDArray 20 @ cpu ( 0 ) > 6 . 2 . 3 묶 인 ( Tied ) 파 라 미 터 들 다 른 어 떤 경 우 에 는 , 여 러 층 들 이 모 델 파 라 미 터 를 공 유 하 는 것 이 필 요 하 기 도 합 니다 . 예 를 들 면 , 좋 은 단 어 임 베 딩 을 찾 는 경 우 , 단 어 인 코 딩 과 디 코 딩 에 같 은 파 라 미 터 를 사 용 하 도 록 하 는 결 정 할 수 있 습 니다 . 이 런 경 우 는 Blocks 에 서 도 소 개 되 었 습 니다 . 이 것 을 보 다 깔 끔 하 게 구 현 하 는 방법 을 알아 보 겠 습 니다 . 아 래 코 드 에 서 는 덴 스 층 ( dense layer ) 을 하 나 정 의 하 고 , 다 른 층 에 파 라 미 터 값 을 동 일 하 게 설 정 하 는 것 을 보 여 주 고 있 습 니다 . [ 17 ] : net = nn . Sequential ( ) # We need to give the shared layer a name such that we can reference its # parameters shared = nn . Dense ( 8 , activation = ' relu ' ) net . add ( nn . Dense ( 8 , activation = ' relu ' ) , shared , nn . Dense ( 8 , activation = ' relu ' , params = shared . params ) , nn . Dense ( 10 ) ) net . initialize ( ) x = nd . random . uniform ( shape = ( 2 , 20 ) ) net ( x ) # Check whether the parameters are the same print ( net [ 1 ] . weight . data ( ) [ 0 ] = = net [ 2 ] . weight . data ( ) [ 0 ] ) net [ 1 ] . weight . data ( ) [ 0 , 0 ] = 100 # Make sure that they ' re actually the same object rather than just having the # same value print ( net [ 1 ] . weight . data ( ) [ 0 ] = = net [ 2 ] . weight . data ( ) [ 0 ] ) 6 . 2 . 파 라 미 터 관 리 255 [ 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . ] < NDArray 8 @ cpu ( 0 ) > [ 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . ] < NDArray 8 @ cpu ( 0 ) > 위 예 제 는 두 번 째 , 세 번 째 층 의 파 라 미 터 가 묶 여 있 는 것 ( tied ) 을 보 여 줍 니다 . 이 파 라 미 터 들 은 값 이 같 은 수 준 이 아 니 라 , 동 일 합 니다 . 즉 , 하 나 의 파 라 미 터 를 바 꾸 면 다 른 파 라 미 터 의 값 도 함 께 바 뀝 니다 . 그 래 디 언 트 ( gradient ) 들 에 일 어 나 는 현 상 은 아 주 독 창 적 입 니다 . 모 델 은 파 라 미 터 는 그 래 디 언 트 ( gradient ) 를 갖고 있 기 때 문 에 , 두 번 째 와 세 번 째 층 의 그 래 디 언 트 ( gradient ) 들 은 역 전 파 ( back propagation ) 단 계 에 서 shared . params . grad ( ) 함 수 에 의 해 서 누 적 됩 니다 . 6 . 2 . 4 요 약 • 모 델 파 라 미 터 를 접 근 하 고 , 초 기 화 하 고 , 서 로 묶 는 다 양 한 방법 이 있 습 니다 . • 커 스 텀 초 기 화 를 사 용 할 수 있 습 니다 . • Gluon 은 독 특 하 고 계 층 적 인 방법 으 로 파 라 미 터 에 접 근 하 는 정 교 한 방법 을 제 공 합 니다 . 6 . 2 . 5 문 제 1 . 이 전 절 의 FancyMLP 정 의 를 사 용 해 서 , 다 양 한 레 이 어 의 파 라 미 터 에 접 근 해 보 세 요 . 2 . MXNet documentation 의 다 양 한 초 기 화 방법 들 을 살 펴 보 세 요 . 3 . net . initialize ( ) 수 행 후 와 net ( x ) 수 행 전 에 모 델 파 라 미 터 를 확 인 해 서 , 모 델 파 라 미 터 들 의 모 양 ( shape ) 를 관 찰 해 보 세 요 . 무 엇 바 뀌 어 있 고 , 왜 그 럴 까 요 ? 4 . 파 라 미 터 를 공 유 하 는 레 이 어 를 갖 는 다 층 퍼 셉 트 론 ( multilayer perceptron ) 을 만 들 어 서 학 습 을 시 켜 보 세 요 . 학 습 과 정 을 수 행하 면 서 모 델 각 층 의 파 라 미 터 들 과 그 래 디 언 트 ( gradient ) 값 을 관 찰 해 보 세 요 . 256 6 . 딥 러 닝 계 산 6 . 2 . 6 Scan the QR Code to Discuss 6 . 3 초 기 화 지 연 ( deferred Initialization ) 앞 의 예 제 들 에 서 는 네 트 워 크 들 을 빠 르 게 그 리 고 조 금 은 느 슨 하 게 만 들 어 왔 습 니다 . 특 히 다 음 과 같 이 동 작 하 지 않 을 것 처 럼 보 일 수 있 는 것 들 을 했 습 니다 . • 입 력 의 차 원 을 고 려 하 지 않 고 네 트 워 크 아 키 텍 처 를 정 의 했 습 니다 . • 이 전 층 ( layer ) 의 출 력 차 원 을 고 려 하 지 않 고 다 음 층 에 추 가 했 습 니다 . • 얼 마 나 많 은 파 라 미 터 들 이 있을 지 모 르 는 상 태 에 서 이 파 라 미 터 들 을 초 기 화 까 지 했 습 니다 . 이 모 든 것 이 불 가 능 하 게 들 리 고 , 실 제 로 도 불 가 능 합 니다 . 사 실 MXNet 이 나 다 른 프 레 임 워 크 들 이 네 트 워 크 에 들 어 올 입 력 값 의 차 원 을 예 측 할 수 있 는 방법 은 없 습 니다 . 이 후 에 살 펴 볼 , 컨 볼 루 션 ( con - volutional ) 네 트 워 크 나 이 미 지 를 다 룰 때 이 문 제 는 더 욱 그 렇 게 보 일 것 입 니다 . 그 이유 는 이 미 지 의 해 상 도 같 은 입 력 의 차 원 은 네 트 워 크 의 연 속 된 층 들 의 차 원 에 영 향 을 미 치 기 때 문 입 니다 . 따 라 서 코 드 를 작 성 할 때 차 원 이 무 엇 인 지 미 리 알 필 요 없 이 파 라 미 터 를 설 정 할 수 있 는 능 력 은 통 계 적 인 모 델 링 을 아 주 간 단 하 게 해 줄 수 있 습 니다 . 지 금 부 터 초 기 화 를 예 로 어 떻 게 동 작 하 는 지 살 펴 보 겠 습 니다 . 결 국 에 는 존 재 하 는 지 모 르 는 변 수 를 초 기 화 하 는 것 은 불 가 능 합 니다 . 6 . 3 . 1 네 트 워 크 생 성 하 기 네 트 워 크 에 대 한 인 스 턴 스 를 만 들 면 일 어 나 는 일을 살 펴 보 겠 습 니다 . 앞에 서 와 같 이 MLP 를 사 용 합 니다 . [ 1 ] : from mxnet import init , nd from mxnet . gluon import nn def getnet ( ) : net = nn . Sequential ( ) net . add ( nn . Dense ( 256 , activation = ' relu ' ) ) ( continues on next page ) 6 . 3 . 초 기 화 지 연 ( deferred Initialization ) 257 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) net . add ( nn . Dense ( 10 ) ) return net net = getnet ( ) 이 시 점 에 서 네 트 워 크 는 여 러 파 라 미 터 들 의 차 원 이 어 떻 게 되 는 지 를 알 수 있 는 방법 이 없 습 니다 . 이 상 태 에 서 말 할 수 있 는 사 실 은 각 층 의 차 원 이 무 엇 이 되 던 가 중 치 들 과 편 향 들 이 필 요 하 다는 것 입 니다 . 파 라 미 터 들 읽 어 보 면 이 것 을 알 수 있 습 니다 . [ 2 ] : print ( net . collect _ params ) print ( net . collect _ params ( ) ) < bound method Block . collect _ params of Sequential ( ( 0 ) : Dense ( None - > 256 , Activation ( relu ) ) ( 1 ) : Dense ( None - > 10 , linear ) ) > sequential0 _ ( Parameter dense0 _ weight ( shape = ( 256 , 0 ) , dtype = float32 ) Parameter dense0 _ bias ( shape = ( 256 , ) , dtype = float32 ) Parameter dense1 _ weight ( shape = ( 10 , 0 ) , dtype = float32 ) Parameter dense1 _ bias ( shape = ( 10 , ) , dtype = float32 ) ) net [ 0 ] . weight . data ( 0 ) 을 수 행하 면 무 언 가 를 하 기 위 해 서 는 네 트 워 크 가 초 기 화 되 어야 한 다는 런 타 임 에 러 를 만 나 게 됩 니다 . 파 라 미 터 를 초 기 화 한 후 , 무 엇 인 바 뀌 는 지 를 확 인 해 보 겠 습 니다 . [ 3 ] : net . initialize ( ) net . collect _ params ( ) [ 3 ] : sequential0 _ ( Parameter dense0 _ weight ( shape = ( 256 , 0 ) , dtype = float32 ) Parameter dense0 _ bias ( shape = ( 256 , ) , dtype = float32 ) Parameter dense1 _ weight ( shape = ( 10 , 0 ) , dtype = float32 ) Parameter dense1 _ bias ( shape = ( 10 , ) , dtype = float32 ) ) 결과 에 서 볼 수 있 듯 이 , 아 무 것 도 바 뀐 것 이 없 습 니다 . 네 트 워 크 에 데 이 터 를 입 력 하 는 경 우 에 비 로 소 변 화 가 생 기 게 됩 니다 . 한 번 해 보 겠 습 니다 . 258 6 . 딥 러 닝 계 산 [ 4 ] : x = nd . random . uniform ( shape = ( 2 , 20 ) ) net ( x ) # Forward computation net . collect _ params ( ) [ 4 ] : sequential0 _ ( Parameter dense0 _ weight ( shape = ( 256 , 20 ) , dtype = float32 ) Parameter dense0 _ bias ( shape = ( 256 , ) , dtype = float32 ) Parameter dense1 _ weight ( shape = ( 10 , 256 ) , dtype = float32 ) Parameter dense1 _ bias ( shape = ( 10 , ) , dtype = float32 ) ) 이 전 에 대 비 해 서 주 요 차 이 점 은 입 력 에 대 한 차 원 , x ∈ R 20 을 알 게 되 면 , 첫 번 째 층 의 가 중 치 행 렬 을 정 의 할 수 있 다는 것 입 니다 . 이 렇 게 되 면 , 우 리 는 두 번 째 층 에 대 한 차 원 을 10 × 256 으 로 결 정 할 수 있 게 됩 니다 . 계 속 연 산 그 래 프 를 따 라 서 차 원 이 결 정 되 게 됩 니다 . 이 것 이 끝나 면 , 파 라 미 터 를 초 기 화 하 는 것 을 진 행할 수 있 습 니다 . 이 방법 이 위 세 가 지 문 제 의 해 결 책 입 니다 . 6 . 3 . 2 초 기 화 지 연 의 적 용 이 론 적 으 로 어 떻 게 동 작 하 는 지 를 배 웠 으 니 , 초 기 화 가 실 제 로 언 제 일 어 나 는 지 보 겠 습 니다 . 이 를 확 인 하 기 위 해 서 어 떤 파 라 미 터 를 초 기 화 할 지 를 지 정 하 면 서 호 출 되 면 아 무 것 도 하 지 않 지 만 디 버 그 메 시 지 를 출 력 하 는 초 기 화 클 래 스 를 하 나 정 의 합 니다 . [ 5 ] : class MyInit ( init . Initializer ) : def _ init _ weight ( self , name , data ) : print ( ' Init ' , name , data . shape ) # The actual initialization logic is omitted here net = getnet ( ) net . initialize ( init = MyInit ( ) ) MyInit 은 호 출 되 면 모 델 파 라 미 터 에 대 한 정 보 를 출 력 하 게 만 들 어 졌 는 데 , initialize 함 수 를 호 출 해 도 아 무 런 정 보 가 출 력 되 지 않 고 있 습 니다 . 즉 , initialize 함 수 가 호 출 이 되 어 도 실 제 파 라 미 터 초 기 화 가 일 어 나 지 않 습 니다 . 이 제 입 력 값 을 정 의 하 고 forward 연 산 을 수 행해 봅 니다 . [ 6 ] : x = nd . random . uniform ( shape = ( 2 , 20 ) ) y = net ( x ) Init dense2 _ weight ( 256 , 20 ) Init dense3 _ weight ( 10 , 256 ) 6 . 3 . 초 기 화 지 연 ( deferred Initialization ) 259 이 제 야 모 델 파 라 미 터 들 에 대 한 정 보 가 화 면 에 출 력 됩 니다 . 주 어 진 입 력 x 에 대 한 forward 연 산 을 수 행할 때 , 시스 템 은 입 력 의 모 양 ( shape ) 을 기 반 으 로 모 든 층 의 가 중 치 파 라 미 터 의 모 양 ( shape ) 을 추 론 해 냅 니다 . 시스 템 이 이 파 라 미 터 들 을 생 성 하 고 나 면 , MyInit 인 스 턴 스 를 호 출 해 서 파 라 미 터 들 을 초 기 화 한 후 , forward 연 산 을 수 행하 게 됩 니다 . 물 론 이 초 기 화 는 최초 forward 연 산 을 수 행할 때 만 일 어 납 니다 . 즉 , 이 후 에 net ( x ) 을 호 출 해 서 forward 연 산 이 수 행 되 면 재 초 기 화 가 수 행 되 지 않 고 , 따 라 서 MyInit 인 스 턴 스 의 결과 는 다 시 출 력 되 지 않 습 니다 . [ 7 ] : y = net ( x ) 이 절 을 시 작 하 면 서 언 급 했 듯 이 , 지 연 된 초 기 화 는 혼 동 을 가 져 올 수 도 있 습 니다 . 예 를 들 면 , 첫 번 째 forward 연 산 전 에 는 , 모 델 파 라 미 터 를 직 접 바 꾸 는 것 이 불 가 능 합 니다 . 즉 , data 나 set _ data 함 수 를 호 출 해 서 모 델 파 라 미 터 의 값 을 얻 거 나 바 꾸 는 것 이 불 가 능 합 니다 . 따 라 서 , 필 요 한 경 우 에 는 샘 플 입 력 을 이 용 해 서 네 트 워 크 를 강 제 초 기 화 하 기 도 합 니다 . 6 . 3 . 3 강 제 초 기 화 initialize 함 수 가 수 행 되 는 시 점 에 시스 템 에 모 든 파 라 미 터 의 모 양 ( shape ) 을 하 는 경 우 에 는 지 연 된 초 기 화 가 일 어 나 지 않 습 니다 . 아 래 두 가 지 가 그 런 경 우 입 니다 . • 이 미 어 떤 데 이 터 를 봤 고 , 파 라 미 터 를 재 설 정 하 고 싶 은 경 우 • 네 트 워 크 를 정 의 할 때 , 모 든 입 력 과 출 력 의 차 원 을 알 고 있 는 경 우 첫 번 째 경 우 는 아 래 예 제 코 드 처 럼 간 단 히 할 수 있 습 니다 . [ 8 ] : net . initialize ( init = MyInit ( ) , force _ reinit = True ) Init dense2 _ weight ( 256 , 20 ) Init dense3 _ weight ( 10 , 256 ) 두 번 째 경 우 는 레 이 어 를 생 성 할 때 파 라 미 터 에 대 한 정 보 를 명 시 해 줘 야 합 니다 . 예 를 들 어 , 덴 스 층 ( dense layer ) 의 경 우 에 는 in _ units 에 대 한 값 을 명 시 하 면 initialize 가 호 출 되 면 파 라 미 터 가 바 로 초 기 화 됩 니다 . [ 9 ] : net = nn . Sequential ( ) net . add ( nn . Dense ( 256 , in _ units = 20 , activation = ' relu ' ) ) net . add ( nn . Dense ( 10 , in _ units = 256 ) ) net . initialize ( init = MyInit ( ) ) 260 6 . 딥 러 닝 계 산 Init dense4 _ weight ( 256 , 20 ) Init dense5 _ weight ( 10 , 256 ) 6 . 3 . 4 요 약 • 지 연 된 초 기 화 는 좋 습 니다 . 지 연 된 초 기 화 는 Gluon 이 많 은 것 들 을 자 동 으 로 설 정 할 수 있 게 해 주 고 , 새 로 운 네 트 워 크 아 키 텍 처 를 정 의 할 때 발 생 할 수 있 는 많 은 오 류 요 소 를 제 거 해 줍 니다 . • 간 접적 으 로 정 의 된 변 수 의 값 을 할 당 하 면 이 기 능 은 우 회 할 수 있 습 니다 . • force _ reinit = True 플 래 그 를 사 용 해 서 초 기 화 를 강 제 하 거 나 다 시 수 행하 게 할 수 있 습 니다 . 6 . 3 . 5 문 제 1 . 입 력 들 의 일 부 만 차 원 을 명 시 하 면 어 떤 일이 일 어 날 까 요 ? 이 경 우 에 도 즉 시 초 기 화 가 수 행 되 나 요 ? 2 . 잘 못 된 자 원 을 명 시 하 면 어 떻 게 될 까 요 ? 3 . 차 원 이 변 하 는 입 력 이 있을 때 어 떻 게 해 야 할 까 요 ? 힌 트 - 파 라 미 터 묶 기 를 참 조 하 세 요 . 6 . 3 . 6 Scan the QR Code to Discuss 6 . 4 커 스 텀 층 ( custom layer ) 딥 러 닝 의 성 공 요 인 중 에 하 나 는 딥 네 트 워 크 에 서 사 용 할 수 있 는 다 양 한 종 류 의 층 ( layer ) 이 있 다 는 점 에 서 찾 아 볼 수 있 습 니다 . 즉 , 다 양 한 형 태 의 층 을 사 용 해 서 많 은 종 류 의 커 스 터 마 이 징 과 다 양 한 문 제 에 적 용 이 가 능 하 게 되 었 습 니다 . 예 를 들 면 , 과 학 자 들 이 이 미 지 , 텍 스 트 , 풀 링 , loop , 동 적 프 로 그 래 밍 , 그 리 고 심 지 어 는 컴 퓨 터 프 로 그 램 을 위 한 층 을 발 명 해 왔 습 니다 . 앞 으 로 도 Gluon 에 현 재 존 재 하 지 6 . 4 . 커 스 텀 층 ( custom layer ) 261 않 은 새 로 운 층 을 만 나 게 될 것 이 고 , 어 쩌 면 여 러 분 이 만 난 문 제 를 해 결 하 기 위 해 서 새 로 운 층 을 직 접 발 명 할 지 도 모 릅 니다 . 자 그 럼 커 스 텀 층 을 만 들 어 보 는 것 을 이 절 에 서 배 워 보 겠 습 니다 . 6 . 4 . 1 파 라 미 터 가 없 는 층 ( layer ) 커 스 텀 층 을 만 드 는 것 은 다 소 복 잡 할 수 있 기 때 문 에 , 파 라 미 터 를 계 승 받 지 않 는 커 스 텀 층 ( 또 는 Block ) 을 만 드 는 것 부 터 시 작 해 보 겠 습 니다 . 첫 번 째 시 작은 이 전 에 introduced blocks 에 서 소 개 했 던 것과 비 슷 합 니다 . 아 래 CenteredLayer 클 래 스 는 입 력 에 서 평 균 을 빼 는 것 을 계 산 하 는 층 을 정 의 합 니다 . 우 리 는 이 것 을 Block 클 래 스 를 상 속 하 고 , forward 메 소 드 를 구 현 해 서 만 듭 니다 . [ 1 ] : from mxnet import gluon , nd from mxnet . gluon import nn class CenteredLayer ( nn . Block ) : def _ _ init _ _ ( self , * * kwargs ) : super ( CenteredLayer , self ) . _ _ init _ _ ( * * kwargs ) def forward ( self , x ) : return x - x . mean ( ) 어 떻 게 동 작 하 는 지 보 기 위 해 서 , 데 이 터 를 층 에 입 력 해 봅 니다 . [ 2 ] : layer = CenteredLayer ( ) layer ( nd . array ( [ 1 , 2 , 3 , 4 , 5 ] ) ) [ 2 ] : [ - 2 . - 1 . 0 . 1 . 2 . ] < NDArray 5 @ cpu ( 0 ) > 우 리 는 이 를 사 용 해 서 더 복 잡 한 모 델 을 만 들 수 도 있 습 니다 . [ 3 ] : net = nn . Sequential ( ) net . add ( nn . Dense ( 128 ) , CenteredLayer ( ) ) net . initialize ( ) 그 럼 이 가 운 데 로 만 들 어 주 는 층 이 잘 작 동 하 는 지 보 겠 습 니다 . 이 를 위 해 서 난 수 데 이 터 를 생 성 하 고 , 네 트 워 크 에 입 력 한 후 평 균 만 큼 값 이 조정 되 는 지 확 인 합 니다 . 우 리 가 다 루 는 변 수 가 실수 형 이 기 때 문 에 , 아 주 작 지 만 0 이 아 닌 숫 자 를 보 게 될 것 임을 염 두 하 세 요 . [ 4 ] : y = net ( nd . random . uniform ( shape = ( 4 , 8 ) ) ) y . mean ( ) . asscalar ( ) 262 6 . 딥 러 닝 계 산 [ 4 ] : - 7 . 212293e - 10 6 . 4 . 2 파 라 미 터 가 있 는 층 층 을 어 떻 게 정 의 하 는 지 원 리를 알 게 되 었 으 니 , 파 라 미 터 를 갖 는 층 을 정 의 해 보 겠 습 니다 . 이 파 라 미 터 들 은 학 습 을 통 해 서 조정 될 값 들 입 니다 . 딥 러 닝 연 구 자 들 의 일을 편 하 게 만 들 어 주 기 위 해 서 , Parameter 클 래 스 와 ParameterDict 사 전 ( dictionary ) 은 많 이 사 용 하 는 기 능 을 제 공 하 고 있 습 니 다 . 이 클 래 스 들 은 접 근 을 관 리 하 고 , 초 기 화 를 하 고 , 공 유 를 하 고 , 모 델 파 라 미 터 를 저 장 하 고 로 딩 하 는 기 능 을 관 리 해 줍 니다 . 예 를 들 면 , 새 로 운 커 스 텀 층 을 만 든 때 매 번 직 렬 화 ( serialization ) 루 틴 을 작 성 할 필 요 가 없 습 니다 . 다 른 예 로 는 , Block 클 래 스 와 함 께 제 공 되 는 ParameterDict 타 입인 params 를 사 용 할 수 도 있 습 니다 . 이 사 전 ( dictionary ) 는 문 자 타 입의 파 라 미 터 이 름 을 Parameter 타 입의 모 델 파 라 미 터 로 매 핑하 는 기 능 을 제 공 합 니다 . ParameterDict 의 get 함 수 를 사 용 해 서 Parameter 인 스 턴 스 를 생 성 하 는 것 도 가 능 합 니다 . [ 5 ] : params = gluon . ParameterDict ( ) params . get ( ' param2 ' , shape = ( 2 , 3 ) ) params [ 5 ] : ( Parameter param2 ( shape = ( 2 , 3 ) , dtype = < class ' numpy . float32 ' > ) ) 덴 스 층 ( dense layer ) 을 직 접 구 현 해 보 겠 습 니다 . 이 층 은 두 파 라 미 터 , 가 중 치 과 편 향 ( bias ) 을 갖 습 니다 . 약 간 특 별 하 게 만 들 기 위 해 서 , ReLU 활 성 화 함 수 를 기 본 으 로 적 용 하 도 록 만 들 어 봅 니다 . 가 중 치 와 편 향 파 라 미 터 를 갖 는 완 전 연 결 층 ( fully connected layer ) 을 구 현 하 고 , ReLU 를 활 성 화 함 수 로 추 가 합 니다 . in _ units 와 units 는 각각 입 력 과 출 력 의 개 수 입 니다 . [ 6 ] : class MyDense ( nn . Block ) : # units : the number of outputs in this layer ; in _ units : the number of # inputs in this layer def _ _ init _ _ ( self , units , in _ units , * * kwargs ) : super ( MyDense , self ) . _ _ init _ _ ( * * kwargs ) self . weight = self . params . get ( ' weight ' , shape = ( in _ units , units ) ) self . bias = self . params . get ( ' bias ' , shape = ( units , ) ) def forward ( self , x ) : linear = nd . dot ( x , self . weight . data ( ) ) + self . bias . data ( ) return nd . relu ( linear ) 6 . 4 . 커 스 텀 층 ( custom layer ) 263 파 라 미 터 에 이 름 을 부 여 하 는 것 은 이 후 에 사 전 ( dictionary ) 조 회 를 통 해 서 원 하 는 파 라 미 터 를 직 접 접 근 할 수 있 도 록 해 줍 니다 . 그 렇 기 때 문 에 , 잘 설 명 하 는 이 름 을 정 하 는 것 이 좋 은 생 각 입 니다 . 자 이 제 MyDense 클 래 스 의 인 스 턴 스 를 만 들 고 모 델 파 라 미 터 들 을 직 접 확 인 해 봅 니다 . [ 7 ] : dense = MyDense ( units = 3 , in _ units = 5 ) dense . params [ 7 ] : mydense0 _ ( Parameter mydense0 _ weight ( shape = ( 5 , 3 ) , dtype = < class ' numpy . float32 ' > ) Parameter mydense0 _ bias ( shape = ( 3 , ) , dtype = < class ' numpy . float32 ' > ) ) 커 스 텀 층 의 forward 연 산 을 수 행합 니다 . [ 8 ] : dense . initialize ( ) dense ( nd . random . uniform ( shape = ( 2 , 5 ) ) ) [ 8 ] : [ [ 0 . 06917784 0 . 01627153 0 . 01029644 ] [ 0 . 02602214 0 . 0453731 0 . ] ] < NDArray 2x3 @ cpu ( 0 ) > 커 스 텀 층 을 이 용 해 서 모 델 은 만 들 어 보 겠 습 니다 . 만 들 어 진 모 델 은 기 본 으 로 제 공 되 는 덴 스 층 ( dense layer ) 처 럼 사 용 할 수 있 습 니다 . 하 나 다 른 점 은 입 력 , 출 력 의 크 기 를 자 동 으 로 계 산 하 는 것 이 없 다는 점 입 니다 . 어 떻 게 이 기 능 을 구 현 할 수 있 는 지 는 MXNet documentation 을 참 고 하 세 요 . [ 9 ] : net = nn . Sequential ( ) net . add ( MyDense ( 8 , in _ units = 64 ) , MyDense ( 1 , in _ units = 8 ) ) net . initialize ( ) net ( nd . random . uniform ( shape = ( 2 , 64 ) ) ) [ 9 ] : [ [ 0 . 03820474 ] [ 0 . 04035058 ] ] < NDArray 2x1 @ cpu ( 0 ) > 6 . 4 . 3 요 약 • Block 클 래 스 를 이 용 해 서 커 스 텀 층 ( layer ) 를 만 들 수 있 습 니다 . 이 방법 은 블 럭 팩 토 리를 정 의 하 는 것 보 다 더 강 력 한 방법 인 데 , 그 이유 는 다 양 한 컨 텍 스 트 ( context ) 들 에 서 불 려 질 수 있 기 때 문 입 니다 . 264 6 . 딥 러 닝 계 산 • 블 럭 들 은 로 컬 파 라 미 터 를 가 질 수 있 습 니다 . 6 . 4 . 4 문 제 1 . 데 이 터 에 대 해 서 afﬁne 변 환 을 학 습 하 는 층 을 디 자인 하 세 요 . 예 를 들 면 , 평 균 값 을 빼 고 , 대 신 더 할 파 라 미 터 를 학 습 합 니다 . 2 . 입 력 을 받 아 서 텐 서 축 소 를 하 는 층 을 만 들 어 보 세 요 . 즉 , 𝑦 𝑘 = ∑︀ 𝑖 , 𝑗 𝑊 𝑖𝑗𝑘 𝑥 𝑖 𝑥 𝑗 를 반 환 합 니다 . 3 . 데 이 터 에 대 한 퓨 리 에 계 수 의 앞에 서 반 을 리 턴 하 는 층 을 만 들 어 보 세 요 . 힌 트 - MXNet 의 fft 함 수 를 참 고 하 세 요 . 6 . 4 . 5 Scan the QR Code to Discuss 6 . 5 파 일 입 / 출 력 지 금까 지 우 리 는 데 이 터 를 처 리 하 고 , 딥 러 닝 모 델 을 만 들 고 , 학 습시 키 고 , 테 스 트 하 는 방법 들 을 알아 봤 습 니다 . 실 험 을 진 행하 다 가 어 느 시 점 에 는 얻 은 결과가 만 족 스 러 워 서 , 나 중 에 도 활 용 하 고 배 포 하 기 위 해 서 결과 를 저 장 할 필 요 가 생 깁 니다 . 또 한 긴 학 습 을 수 행할 때 서 버 의 전 원 에 문 제 가 생 겼 을 때 며 칠 동 안 수 행한 내 용 을 잃 지 않 기 위 해 서 중 간 결과 들 을 저 장 하 는 것 이 가 장 최 선 의 방법 이 기 도 합 니다 . 예 를 들 면 , 영 어 단 어 임 베 딩 을 가 지 고 , 멋 진 스 팸 분 류 기 를 만 들 고 자 하 는 경 우 , 미 리 학 습 된 ( pretrained ) 모 델 을 읽 어야 하 는 경 우 도 있 습 니다 . 이 모 든 경 우 를 수 행하 기 위 해 서 는 , 개 별 가 중 치 벡 터 들 또 는 전 체 모 델 을 저 장 하 고 읽 어야 합 니다 . 이 번 절 에 서 는 이 두 가 지 에 대 해 서 알아 보 겠 습 니다 . 6 . 5 . 1 NDArray 가 장 간 단 한 방법 은 save 와 load 함 수 를 직 접 호 출 해 서 NDArray 를 하 나 씩 저 장 하 고 읽을 수 있 습 니다 . 이 는 다 음 과 같 이 저 장 하 는 것 을 간 단 히 구 현 할 수 있 습 니다 . 6 . 5 . 파 일 입 / 출 력 265 [ 1 ] : from mxnet import nd from mxnet . gluon import nn x = nd . arange ( 4 ) nd . save ( ' x - file ' , x ) 그 리 고 , 우 리 는 이 파 일을 메모 리 로 다 시 읽 습 니다 . [ 2 ] : x2 = nd . load ( ' x - file ' ) x2 [ 2 ] : [ [ 0 . 1 . 2 . 3 . ] < NDArray 4 @ cpu ( 0 ) > ] 하 나 의 NDArray 객 체 뿐 만 아 니 라 , NDArray 들 의 리 스 트 도 저 장 하 고 다 시 메모 리 로 읽 기 도 가 능 합 니다 . [ 3 ] : y = nd . zeros ( 4 ) nd . save ( ' x - files ' , [ x , y ] ) x2 , y2 = nd . load ( ' x - files ' ) ( x2 , y2 ) [ 3 ] : ( [ 0 . 1 . 2 . 3 . ] < NDArray 4 @ cpu ( 0 ) > , [ 0 . 0 . 0 . 0 . ] < NDArray 4 @ cpu ( 0 ) > ) 문 자 를 NDArray 로 매 핑하 는 사 전 ( dictionary ) 를 저 장 하 고 읽 는 것 도 가 능 합 니다 . 이 방법 은 모 델 의 전 체 가 중 치 들 을 한 꺼 번 에 저 장 하 고 읽을 때 유 용 합 니다 . [ 4 ] : mydict = { ' x ' : x , ' y ' : y } nd . save ( ' mydict ' , mydict ) mydict2 = nd . load ( ' mydict ' ) mydict2 [ 4 ] : { ' x ' : [ 0 . 1 . 2 . 3 . ] < NDArray 4 @ cpu ( 0 ) > , ' y ' : [ 0 . 0 . 0 . 0 . ] < NDArray 4 @ cpu ( 0 ) > } 266 6 . 딥 러 닝 계 산 6 . 5 . 2 Gluon 모 델 파 라 미 터 들 가 중 치 벡 터 를 하 나 씩 ( 또 는 NDArray 텐 서 들 ) 저 장 하 는 것 이 유 용 하 지 만 , 모 델 전 체 를 저 장 하 고 이 후 에 읽 는 데 는 매 우 불 편 한 방법 입 니다 . 왜 냐 하 면 모 델 전 체 에 걸 쳐 서 수 백 개 의 파 라 미 터 그 룹 들 이 있을 수 있 기 때 문 입 니다 . 만 약 모 든 값 을 모 아 서 아 키 텍 처 에 매 핑 시 키 는 스 크 립 트 를 작 성 한 다 면 매 우 많 은 일을 해 야 합 니다 . 이 런 이유 로 Gluon 은 개 별 가 중 치 벡 터 를 저 장 하 는 것 보 다는 전 체 네 트 워 크 를 저 장 하 고 읽을 수 있 는 기 능 을 제 공 합 니다 . 유의 해 야 할 점 은 전 체 모 델 을 저 장 하 는 것 이 아 니 라 , 모 델 의 파 라 미 터 들 을 저 장 한 다는 것 입 니다 . 만 약에 3 개 층 을 갖 는 MLP 가 있 다 면 , 네 트 워 크 의 아 키 텍 처 는 별 도 록 명 시 해 줘 야 합 니다 . 이 렇 게 한 이유 는 모 델 들 자 체 는 임의의 코 드 를 담 고 있을 수 있 는 데 이 경 우 에 는 코 드 가 쉽 게 직 렬 화 ( serialization ) 되 지 않 을 수 있 기 때 문 입 니다 . ( 단 , 컴 파 일 된 모 델 의 경 우 에 는 방법 이 있 는 데 , 기 술 적 인 자 세 한 내 용 은 MXNet documentation 을 참 고 하 세 요 . ) 결 국 , 모 델 을 다 시 만 들 려 면 , 아 키 텍 처 를 코 드 형 태 로 만 들 고 , 디 스 크 로 부 터 파 라 미 터 를 로 딩 해 야 합 니다 . 지 연 된 초 기 화 는 실 제 값 을 할 당 할 필 요 없 이 모 델 을 정 의 할 수 있 기 때 문 에 이 런 방 식 에 아 주 도 움 이 됩 니다 . 역 시 우 리 의 MLP 를 사 용 해 서 설 명 하 겠 습 니다 . [ 5 ] : class MLP ( nn . Block ) : def _ _ init _ _ ( self , * * kwargs ) : super ( MLP , self ) . _ _ init _ _ ( * * kwargs ) self . hidden = nn . Dense ( 256 , activation = ' relu ' ) self . output = nn . Dense ( 10 ) def forward ( self , x ) : return self . output ( self . hidden ( x ) ) net = MLP ( ) net . initialize ( ) x = nd . random . uniform ( shape = ( 2 , 20 ) ) y = net ( x ) 모 델 파 라 미 터 들 을 ’mlp . params’ 라 는 이 름 의 파 일 에 저 장 합 니다 . [ 6 ] : net . save _ parameters ( ' mlp . params ' ) 모 델 을 복 원 할 수 있 는 지 확 인 하 기 위 해 서 , 원 본 MLP 모 델 의 복 사 본 을 만 듭 니다 . 모 델 파 라 미 터 를 난 수 로 초 기 화 하 는 것 이 아 니 라 , 파 일 에 저 장 했 던 파 라 미 터 들 을 직 접 읽 습 니다 . [ 7 ] : clone = MLP ( ) clone . load _ parameters ( ' mlp . params ' ) 두 모 델 의 인 스 턴 스 가 같 은 모 델 파 라 미 터 를 갖고 있 기 때 문 에 , 같 은 입 력 x 를 가 지 고 계 산 한 결과 는 6 . 5 . 파 일 입 / 출 력 267 같 아야 합 니다 . 확 인 해 보 겠 습 니다 . [ 8 ] : yclone = clone ( x ) yclone = = y [ 8 ] : [ [ 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . ] [ 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . 1 . ] ] < NDArray 2x10 @ cpu ( 0 ) > 6 . 5 . 3 요 약 • save 와 load 함 수 를 이 용 해 서 NDArray 객 체 들 을 파 일 에 저 장 하 고 읽을 수 있 습 니다 . • load _ parameters 와 save _ parameters 함 수 는 Gluon 의 네 트 워 크 전 체 파 라 미 터 를 저 장 하 고 읽 는 데 사 용 됩 니다 . • 아 키 텍 처 를 저 장 하 는 것 은 파 라 미 터 와 는 별 도 로 코 드 로 해 야 합 니다 . 6 . 5 . 4 문 제 1 . 다 른 디 바 이 스 에 학 습 된 모 델 을 배 포 할 필 요 가 없 을 경 우 라 도 , 모 델 파 라 미 터 를 저 장 할 수 있을 때 얻 을 수 있 는 실 용 적 인 이 점 은 무 엇 인 가 요 ? 2 . 네 트 워 크 의 일 부 를 다 른 아 키 텍 처 의 네 트 워 크 에 포 함해 야 한 다 고 가 정 합 니다 . 예 를 들 어 이 전 네 트 워 크 의 처 음 두 개 층 을 새 로 운 네 트 워 크 에 서 어 떻 게 사 용 할 수 있을 까 요 ? 3 . 네 트 워 크 아 키 텍 처 와 파 라 미 터 를 저 장 하 는 방법 이 무 엇 이 있을 까 요 ? 네 트 워 크 아 키 텍 처 에 어 떤 제 약 을 둬 야 할 까 요 ? 6 . 5 . 5 Scan the QR Code to Discuss 268 6 . 딥 러 닝 계 산 6 . 6 GPU 이 책 의 소 개 에 서 우 리 는 지 난 이 십 년 간 연 산 능 력 의 급 격 한 증 가 에 대 해 서 논 의 했 습 니다 . 간 단 하 게 말 하 면 , GPU 성 능 이 2000 년 부 터 10 년 마 다 약 1000 배 씩 증 가 해 왔 습 니다 . 이 런 것 이 우 리 에 게 엄 청 난 기 회 를 주 기 도 하 고 , 그 러 한 성 능 을 제 공 할 필 요 성 을 제 시 하 기 도 합 니다 . 연 대 데 이 터 셋 메모 리 초 당 부 동 소 수 점 연 산 수 1970 100 ( Iris ) 1 KB 100 KF ( Intel 8080 ) 1980 1 K ( House prices in Boston ) 100 KB 1 MF ( Intel 80186 ) 1990 10 K ( optical character recognition ) 10 MB 10 MF ( Intel 80486 ) 2000 10 M ( web pages ) 100 MB 1 GF ( Intel Core ) 2010 10 G ( advertising ) 1 GB 1 TF ( NVIDIA C2050 ) 2020 1 T ( social network ) 100 GB 1 PF ( NVIDIA DGX - 2 ) 여 러 분 의 연 구 를 위 해 서 이 컴 퓨 팅 성 능 을 활 용 하 는 방법 에 대 해 서 논 의 하 는 것 으 로 시 작 해 보 겠 습 니다 . 우 선 은 하 나 의 GPU 를 사 용 해 보 겠고 , 이 후 에 는 여 러 GPU 및 ( 여 러 GPU 를 갖 는 ) 여 러 서 버 를 사 용 하 는 방법 에 대 해 서 다 루 겠 습 니다 . 이 미 눈 치 챘 겠 지 만 , MXNet NDArray 는 NumPy 와 거 의 유 사 합 니다 . 하 지 만 , 몇 가 지 중 요 한 차 이 점 들 이 있 습 니다 . MXNet 를 NumPy 와 다 르 게 만 드 는 중 요 한 특 징 중 하 나 는 다 양 한 하 드 웨 어 디 바 이 스 를 지 원 한 다는 점 입 니다 . MXNet 의 모 든 배 열 은 컨 텍 스 트 ( context ) 를 갖 습 니다 . 사 실 , 설 명 을 하 지 는 않았 지 만 지 금까 지 NDAr - ray 를 출 력 할 때 마 다 , @ cpu ( 0 ) 라 는 이 상 한 내 용 이 결과 에 함 께 출 력 되 었 습 니다 . 이 것 이 의 미 하 는 것 은 해 당 연 산 이 CPU 에 서 수 행 되 었 다는 것 입 니다 . 다 른 컨 텍 스 트 ( context ) 들 로 는 다 양 한 GPU 들 이 될 수 있 습 니다 . 작 업 을 여 러 서 버 에 배 포 하 는 경 우 에 는 상 황 이 더 어 려 워 질 수 있 습 니다 . 배 열 을 컨 텍 스 트 ( context ) 들 에 지 능 적 으 로 할 당 하 면 , 디 바 이 스 간 에 데 이 터 가 전 송 되 는 시 간 을 최 소 화 할 수 있 습 니 다 . 예 를 들 면 , GPU 하 나 를 가 지 고 있 는 서 버 에 서 뉴 럴 네 트 워 크 를 학 습시 키 는 경 우 , 모 델 파 라 미 터 가 GPU 에 상 주 하 는 것 이 유 리 합 니다 . 요 약 하 면 , 복 잡 한 뉴 럴 네 트 워 크 나 큰 스 케 일의 데 이 터 를 다 룰 때 , CPU 만 을 사 용 해 서 연 산 을 수 행하 는 것 은 비 효 율 적 일 수 있 습 니다 . 이 절 에 서 우 리 는 하 나 의 NVIDIA GPU 를 사 용 해 서 연 산 을 수 행하 는 것 을 설 명 하 겠 습 니다 . 우 선 , 여 러 분 의 시스 템 에 적 어 도 한 개 의 NVIDIA GPU 가 설 치 되 어 있 는 지 확 인 하 세 요 . 다 음 , CUDA 를 다 운 로 드 하 고 경 로 를 적절 히 설 정 하 세 요 . 준 비 가 끝났 다 면 , nvidia - smi 명 령 을 사 용 해 서 그 래 픽 카 드 정 보 를 조 회 해 볼 수 있 습 니다 . [ 1 ] : ! nvidia - smi 6 . 6 . GPU 269 Mon May 20 16 : 19 : 46 2019 + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + | NVIDIA - SMI 410 . 48 Driver Version : 410 . 48 ˓ → | | - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - + | GPU Name Persistence - M | Bus - Id Disp . A | Volatile Uncorr . ˓ → ECC | | Fan Temp Perf Pwr : Usage / Cap | Memory - Usage | GPU - Util Compute M . ˓ → | | = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = + = = = = = = = = = = = = = = = = = = = = = = + = = = = = = = = = = = = = = = = = = = = = = | | 0 Tesla V100 - SXM2 . . . On | 00000000 : 00 : 1B . 0 Off | ˓ → 0 | | N / A 42C P0 38W / 300W | 0MiB / 16130MiB | 0 % ˓ → Default | + - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - + | 1 Tesla V100 - SXM2 . . . On | 00000000 : 00 : 1C . 0 Off | ˓ → 0 | | N / A 43C P0 54W / 300W | 1106MiB / 16130MiB | 0 % ˓ → Default | + - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - + | 2 Tesla V100 - SXM2 . . . On | 00000000 : 00 : 1D . 0 Off | ˓ → 0 | | N / A 43C P0 43W / 300W | 0MiB / 16130MiB | 0 % ˓ → Default | + - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - + | 3 Tesla V100 - SXM2 . . . On | 00000000 : 00 : 1E . 0 Off | ˓ → 0 | | N / A 42C P0 42W / 300W | 11MiB / 16130MiB | 0 % ˓ → Default | + - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - + - - - - - - - - - - - - - - - + + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + | Processes : GPU ˓ → Memory | | GPU PID Type Process name Usage ˓ → | | = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = | | 1 13783 C / home / ubuntu / miniconda3 / bin / python ˓ → 1095MiB | ( continues on next page ) 270 6 . 딥 러 닝 계 산 ( ì˙It’ì˘aˇD í ˝OŸì˙It’ì˘g ˘Aì ˚UˇRìˇDIJ ê¸sˇDì ˛E ) + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + 그 다 음 , GPU 버 전 의 MXNet 이 설 치 되 어 있 는 지 확 인 하 세 요 . 만 약 CPU 버 전 의 MXNet 이 이 미 설 치 되 어 있 는 경 우 에 는 우 선 MXNet 을 제 거 해 야 합 니다 . 즉 , pip uninstall mxnet 명 령 으 로 제 거 하 고 , 시스 템 에 설 치 된 CUDA 버 전 에 대 응 하 는 MXNet 을 설 치 합 니다 . CUDA 9 . 0 이 설 치 되 어 있 다 고 가 정 하 면 , CUDA 9 . 0 을 지 원 하 는 MXNet 버 전 설 치 는 pip install mxnet - cu90 명 령 으 로 합 니다 . 이 절 의 프 로 그 램 들 을 수 행하 기 위 해 서 는 최 소 두 개 이 상 의 GPU 들 이 필 요 합 니다 . 대 부분 의 데 스 크탑 컴 퓨 터 에 GPU 두 개가 설 치 된 경 우 는 드 물 지 만 , 클 라 우 드 에 서 는 이 런 시스 템 을 구 하 기 쉽습 니다 . 예 를 들 면 , AWS 클 라 우 드 의 멀 티 GPU 를 제 공 하 는 EC2 인 스 턴 스 를 사 용 할 수 있 습 니다 . 거 의 모 든 다 른 절 들 에 서 는 다 중 GPU 를 필 요 로 하 지 는 않 습 니다 . 여 기 서 는 데 이 터 가 서 로 다 른 디 바 이 스 간 에 어 떻 게 이 동 하 는 지 를 설 명 하 기 위 해 서 여 러 GPU 가 필 요 합 니다 . 6 . 6 . 1 컴 퓨 팅 디 바 이 스 MXNet 은 값 의 저 장 과 연 산 에 사 용 할 CPU 나 GPU 와 같 은 디 바 이 스 를 지 정 할 수 있 습 니다 . 기 본 설 정 으 로 MXNet 은 메 인 메모 리 에 데 이 터 를 생 성 하 고 , CPU 를 사 용 해 서 연 산 을 수 행합 니다 . MXNet 에 서 는 CPU 와 GPU 는 각각 cpu ( ) 와 gpu ( ) 로 표 현 됩 니다 . mx . cpu ( ) ( 또 는 괄 호 안에 아 무 정 수 를 사 용 ) 는 모 든 물 리 적 인 CPU 들 과 메모 리를 의 미 한 다는 것 을 기 억 해 두 세 요 . 즉 , MXNet 은 연 산 을 수 행할 때 모 든 CPU 코 어 를 사 용 하 려 고 합 니다 . 반 면 에 mx . gpu ( ) 는 하 나 의 그 래 픽 카 드 와 그 카 드 의 메모 리를 지 정 합 니다 . 만 약 여 러 GPU 를 가 지 고 있 다 면 , 𝑖 번 째 GPU 를 ( 𝑖 는 0 부 터 시 작 ) 지 정 하 는 방법 은 mx . gpu ( i ) 라 고 명 시 하 면 됩 니다 . 참 고 로 mx . gpu ( 0 ) 과 mx . gpu ( ) 는 같 은 표 현 입 니다 . [ 2 ] : import mxnet as mx from mxnet import nd from mxnet . gluon import nn mx . cpu ( ) , mx . gpu ( ) , mx . gpu ( 1 ) [ 2 ] : ( cpu ( 0 ) , gpu ( 0 ) , gpu ( 1 ) ) 6 . 6 . 2 NDArray 와 GPU 앞에 서 도 말 했 듯 이 기 본 설 정 은 NDArray 객 체 를 CPU 에 생 성 합 니다 . 따 라 서 , NDArray 를 출 력 할 때 , @ cpu ( 0 ) 라 는 식 별 자 를 보 게 됩 니다 . 6 . 6 . GPU 271 [ 3 ] : x = nd . array ( [ 1 , 2 , 3 ] ) x [ 3 ] : [ 1 . 2 . 3 . ] < NDArray 3 @ cpu ( 0 ) > NDArray 의 context 속성 을 사 용 해 서 NDArray 객 체 가 위 치 한 디 바 이 스 를 확 인 할 수 있 습 니다 . 여 러 객 체 에 대 한 연 산 을 수 행할 때 는 항 상 그 객 체 이 모 두 같 은 컨 텍 스 트 ( context ) 에 있 어야 한 다는 것 을 명 심 하 세 요 . 즉 , 두 변 수 를 더 하 는 경 우 , 두 변 수 가 같 은 디 바 이 스 에 있 어야 한 다는 의 미 입 니다 . 그 렇 지 않 을 경 우 에 는 MXNet 은 결과 를 어 느 곳 에 저 장 할 지 또 는 연 산 을 어 느 곳 에 서 수 행해 야 할 지 를 알 수 없 습 니다 . [ 4 ] : x . context [ 4 ] : cpu ( 0 ) GPU 의 저 장 소 GPU 에 NDArray 를 저 장 하 는 방법 은 여 러 가 지 가 있 습 니다 . NDArray 객 체 를 생 성 할 때 , ctx 파 라 미 터 를 이 용 해 서 저 장 할 디 바 이 스 지 정 이 가 능 합 니다 . 예 를 들 어 , gpu ( 0 ) 에 NDArray 변 수 a 를 생 성 합 니다 . a 를 출 력 하 면 , 디 바 이 스 정 보 가 @ gpu ( 0 ) 으 로 나 오 는 것 을 확 인 해 보 세 요 . GPU 에 서 만 들 어 진 NDArray 는 그 GPU 의 메모 리만 사 용 합 니다 . GPU 메모 리 사 용 량 은 nvidia - smi 명 령 으 로 확 인이 가 능 합 니다 . 일 반 적 우 리 는 GPU 메모 리 크 기 를 넘 어 서 데 이 터 를 생 성 하 지 않 도 록 해 야 합 니다 . [ 5 ] : x = nd . ones ( ( 2 , 3 ) , ctx = mx . gpu ( ) ) x [ 5 ] : [ [ 1 . 1 . 1 . ] [ 1 . 1 . 1 . ] ] < NDArray 2x3 @ gpu ( 0 ) > 최 소 한 두 개 의 GPU 가 있 다 고 하 면 , 아 래 코 드 는 난 수 배 열 을 gpu ( 1 ) 에 생 성 합 니다 . [ 6 ] : y = nd . random . uniform ( shape = ( 2 , 3 ) , ctx = mx . gpu ( 1 ) ) y [ 6 ] : [ [ 0 . 59119 0 . 313164 0 . 76352036 ] [ 0 . 9731786 0 . 35454726 0 . 11677533 ] ] < NDArray 2x3 @ gpu ( 1 ) > 272 6 . 딥 러 닝 계 산 복 사 x + y 를 계 산 하 고 자 한 다 면 , 이 연 산 을 어 느 디 바 이 스 에 서 수 행할 지 를 결 정 해 야 합 니다 . x 를 gpu ( 1 ) 로 옮 기 고 , 연 산 을 거 기 서 수 행할 수 있 습 니다 . 단 순 히 ‘‘x + y‘‘ 를 수 행하 지 마 세 요 . 만 약 그 렇 게 할 경 우 , 예외 가 발 생 할 것 입 니다 . 왜 냐 하 면 , 런 타 임 엔 진 은 무 엇 을 해 야 할 지 모 르 고 , 같 은 디 바 이 스 에 서 데 이 터 를 찾 을 수 없어 서 연 산 이 실 패 하 기 때 문 입 니다 . copyto 메 소 드 는 데 이 터 를 다 른 디 바 이 스 로 복 사 해 서 , 연 산 을 할 수 있 도 록 해 줍 니다 . y 는 두 번 째 GPU 에 있으 니 , 우 리 는 연 산 을 수 행하 기 전 에 x 를 그 디 바 이 스 로 옮 겨 야 합 니다 . [ 7 ] : z = x . copyto ( mx . gpu ( 1 ) ) print ( x ) print ( z ) [ [ 1 . 1 . 1 . ] [ 1 . 1 . 1 . ] ] < NDArray 2x3 @ gpu ( 0 ) > [ [ 1 . 1 . 1 . ] [ 1 . 1 . 1 . ] ] < NDArray 2x3 @ gpu ( 1 ) > 자 이 제 데 이 터 가 모 두 같 은 GPU 에 있으 니 , 두 값 을 더 할 수 있 습 니다 . MXNet 은 연 산 결과 를 다 시 같 은 디 바 이 스 에 저 장 합 니다 . 지 금 예 의 경 우 는 @ gpu ( 1 ) 입 니다 . [ 8 ] : y + z [ 8 ] : [ [ 1 . 59119 1 . 313164 1 . 7635204 ] [ 1 . 9731786 1 . 3545473 1 . 1167753 ] ] < NDArray 2x3 @ gpu ( 1 ) > 변 수 z 는 두 번 째 GPU , gpu ( 1 ) , 에 있 는 데 , 만 약 z . copyto ( gpu ( 1 ) ) 을 수 행하 면 어 떻 게 될 까 요 ? 답 6 . 6 . GPU 273 은 이 미 같 은 GPU 에 값 이 있 더 라 도 새 로 운 메모 리를 할 당 해 서 값 을 복 사 합 니다 . 프 로 그 램 이 수 행 되 는 환 경 에 따 라 서 두 변 수 가 이 미 같 은 디 바 이 스 에 있 는 경 우 도 있 습 니다 . 우 리 는 변 수 가 다 른 컨 텍 스 트 ( context ) 에 있을 때 만 복 사 를 수 행하 기 원 합 니다 . 이 경 우 , as _ in _ context ( ) 를 이 용 하 면 됩 니다 . 먄 약 변 수 가 지 정 된 컨 텍 스 트 ( context ) 에 있 는 경 우 리 면 , 아 무 일이 일 어 나 지 않 습 니다 . 진짜 로 데 이 터 의 복 제 본 을 만 드 는 경 우 가 아 니 라 면 , as _ in _ context ( ) 를 사 용 하 세 요 . [ 9 ] : z = x . as _ in _ context ( mx . gpu ( 1 ) ) z [ 9 ] : [ [ 1 . 1 . 1 . ] [ 1 . 1 . 1 . ] ] < NDArray 2x3 @ gpu ( 1 ) > 소 스 와 타 겟 변 수 의 context 가 동 일 하 다 면 , as _ in _ context 함 수 는 타 겟 변 수 와 소 스 변 수 가 소 스 변 수 의 메모 리를 공 유 한 다는 사 실 을 기 억 해 두 는 게 중 요 합 니다 . [ 10 ] : y . as _ in _ context ( mx . gpu ( 1 ) ) is y [ 10 ] : True 반 면 , copyto 함 수 는 타 겟 변 수 를 위 해 서 항 상 새 로 운 메모 리를 만 듭 니다 . [ 11 ] : y . copyto ( mx . gpu ( ) ) is y [ 11 ] : False 조 심 하 세 요 사 람 들 은 빠 른 속 도 를 기 대 하 면 서 머 신 러 닝 을 수 행할 때 GPU 들 을 사 용 합 니다 . 컨 텍 스 트 ( context ) 들 사 이 에 변 수 를 이 동 하 는 것 은 느 립 니다 . 우 리 가 그 렇 게 하 라 고 하 기 전 에 이 미 많 은 경 우 사 람 들 은 느 린 무 엇 인 가 를 수 행합 니다 . 예 를 들 면 , MXNet 이 복 사 를 문 제 를 발 생 하 지 않 고 자 동 으 로 수 행했 다 면 , 느 리 게 동 작 하 는 코 드 를 작 성 했 다는 것 을 눈 치 채 지 못 할 것 입 니다 . 디 바 이 스 간 ( CPU , GPU , 다 른 머 신 ) 에 데 이 터 를 옮 기 는 것 은 연 산 보 다 훨 씬 느 립 니다 . 더 군 다 나 병 렬 화 ( parallelization ) 를 더 어 렵 게 만 듭 니다 . 연 산 을 계 속 수 행하 기 전 에 데 이 터 가 보 내 지 거 나 받 아 지 는 것 이 끝날 때 까 지 대 기 해 야 하 기 때 문 입 니다 . 그 렇 게 때 문 에 복 사 연 산 은 아 주 조 심 해 서 수 행해 야 합 니다 . 경 험 적 인 법 칙 으 로 작은 연 산 을 많 이 하 는 것 은 큰 연 산 보 다 훨 씬 나 쁘 고 , 여 러 연 산 을 동 시 에 수 행하 는 것 은 하 나 의 연 산 을 여 러 개 를 수 행하 는 것 보 다 나 쁩 니다 . 이 런 경 우 들 은 다 른 무 언 가 를 하 기 전 에 한 개 의 디 바 이 스 가 다 른 디 바 이 스 를 기 다 려 야 하 는 예 들 입 니다 . 스 마 트 폰 으 로 미 리 주 문 한 후 도 착 하 면 커 피 가 준 비 되 어 있 는 것 이 아 닌 줄 을 서서 커 피 를 주 문 하 는 것과 유 사 합 니다 . 274 6 . 딥 러 닝 계 산 마 지 막 으 로 는 메 인 메모 리 에 데 이 터 가 있 는 경 우 가 아 닐 때 , NDArray 데 이 터 를 출 력 하 거 나 NDArray 를 NumPy 형 태 로 바 꾸 는 경 우 에 MXNet 은 먼 저 데 이 터 를 메 인 메모 리 에 복 사 합 니다 . 즉 , 전 송 오 버 헤 드 가 발 생 합 니다 . 더 나 쁜 사 실 은 모 든 것 이 Python 이 완 료 되 기 를 기 다 리 는 글 로 벌 인 터 프 린 터 락 에 종 속 된 다는 것 입 니다 . 6 . 6 . 3 Gluon 과 GPU NDArray 와 비 슷 하 게 Gluon 의 모 델도 초 기 화 과 정 중 에 ctx 파 라 미 터 를 통 해 서 context 를 지 정 할 수 있 습 니다 . 아 래 코 드 는 모 델 파 라 미 터 를 GPU 에 서 초 기 화 합 니다 . [ 12 ] : net = nn . Sequential ( ) net . add ( nn . Dense ( 1 ) ) net . initialize ( ctx = mx . gpu ( ) ) 입 력 이 GPU 에 있 는 NDArray 객 체 라 면 , Gluon 은 같 은 GPU 에 서 연 산 을 수 행합 니다 . [ 13 ] : net ( x ) [ 13 ] : [ [ 0 . 04995865 ] [ 0 . 04995865 ] ] < NDArray 2x1 @ gpu ( 0 ) > 모 델 파 라 미 터 들 이 같 은 GPU 에 저 장 되 어 있 는 지 확 인 해 보 겠 습 니다 . [ 14 ] : net [ 0 ] . weight . data ( ) [ 14 ] : [ [ 0 . 0068339 0 . 01299825 0 . 0301265 ] ] < NDArray 1x3 @ gpu ( 0 ) > 요 약 하 면 , 모 든 데 이 터 와 파 라 미 터 들 이 같 은 디 바 이 스 에 있 어야 모 델 을 효 과 적 으 로 학 습시 킬 수 있 습 니다 . 앞 으 로 그 런 예 제 들 을 여 러 개 보 게 될 것 입 니다 . 6 . 6 . 4 요 약 • MXNet 은 저 장 과 연 산 을 수 행할 디 바 이 스 ( GPU , GPU ) 를 지 정 할 수 있 습 니다 . 기 본 설 정 으 로 MXNet 은 메 인 메모 리 에 데 이 터 를 생 성 하 고 , CPU 를 사 용 해 서 연 산 을 수 행합 니다 . • MXNet 은 모 든 입 력 데 이 터 가 동 일 한 디 바 이 스 ( CPU 또 는 같 은 GPU ) 에 있 어야 연 산 을 수 행할 수 있 습 니다 . 6 . 6 . GPU 275 • 데 이 터 를 조 심 하 게 옮 기 지 않 을 경 우 상 단 한 성 능 손 실 이 발 생 합 니다 . 전 형 적 인 실수 는 다 음 과 같 습 니다 . GPU 를 이 용 해 서 미 니 배 치 의 손 실 ( loss ) 을 계 산 하 고 , 매 번 화 면 에 출 력 ( 또 는 NumPy 배 열에 추 가 ) 하 는 경 우 . 이 경 우 , 글 로 벌 인 터 프 린 터 락 이 필 요 하 기 때 문 에 모 든 GPU 가 멈 추 어 야 합 니다 . 권 장 하 는 방법 은 GPU 에 로 깅 을 위 한 메모 리를 할 당 하 고 , 큰 로 그 를 옮 기 는 것 입 니다 . 6 . 6 . 5 문 제 1 . 큰 행 렬 의 곱같 은 큰 연 산 을 수 행하 면 서 CPU 와 GPU 의 속 도 차 이 를 관 찰 해 보 세 요 . 작은 크 기 의 연 산 은 어 떤 가 요 ? 2 . GPU 에 파 라 미 터 를 읽 고 쓰 기 를 어 떻 게 하 나 요 ? 3 . 100 × 100 행 렬 들 의 행 렬 곱 1000 개 를 수 행하 고 , 행 렬 놈 ( norm ) tr 𝑀𝑀 ⊤ ﬀ 매 번 출 력 하 는 것과 GPU 에 로 그 를 저 장 한 후 마 지 막 에 최 종 결과 만 옮 길 때 각 수 행 시 간 을 측 정 해 보 세 요 4 . 두 개 의 GPU 에 서 두 행 렬 곱 을 동 시 에 수 행하 는 것과 , 하 나 의 GPU 에 서 순 서 대 로 수 행하 면 서 수 행 시 간 을 측 정 해 보 세 요 . 힌 트 - 선 형 적 인 성 능 수 치 를 볼 것 입 니다 . 6 . 6 . 6 참 고 자 료 [ 1 ] CUDA download address . https : / / developer . nvidia . com / cuda - downloads 6 . 6 . 7 Scan the QR Code to Discuss 276 6 . 딥 러 닝 계 산 7 Appendix 7 . 1 이 책 에 기 여 하 는 방법 이 오 픈 - 소 스 책 의 기 여 자 목 록 을 [ 1 ] 에 서 볼 수 있 습 니다 . 기 여 를 하 고 싶 다 면 , Git 을 설 치 하 고 이 책 의 GitHub 코 드 리 포 지 토 리 에 pull request 을 제 출 해 주 세 요 . 저 자 가 여 러 분 의 pull request 를 코 드 리 포 지 토 리 에 머 지 하 면 , 여 러 분 은 기 여 자 가 됩 니다 . 이 절 에 서 는 이 책 에 기 여 를 하 는 데 사 용 되 는 기 본 적 인 Git 절 차 를 설 명 하 겠 습 니다 . Git 사 용 법 에 친 숙 하 다 면 , 이 절 을 넘 어 가 도 됩 니다 . 아 래 절 차 를 수 행할 때 , 기 여 자의 GitHub ID 가 “astonzhang” 이 라 고 가 정 하 겠 습 니다 . 1 단 계 : Git 을 설 치 하 세 요 . Git 오 픈 소 스 책 [ 3 ] 은 Git 을 어 떻 게 설 치 하 는 지 상 세 하 게 알 려 줍 니다 . 만 약 아 직 GitHub 계 정 이 없 으 면 , 지 금 가 입 하 세 요 . [ 4 ] 2 단 계 : GitHub 에 로 그 인 합 니다 . 웹 브 라 우 저 에 이 책 의 코 드 리 포 지 토 리 주 소 를 입 력 합 니다 . [ 2 ] 그 림 11 . 20 의 우 측 상 단 에 빨 간 색 으 로 표 시 된 “Fork” 버 튼 을 클 릭 해 서 이 책 의 코 드 리 포 지 토 리를 복 제 합 니다 . 277 자 그 럼 그 림 11 . 21 의 왼 쪽 위 에 보 이 는 “Your GitHub ID / d2l - ko” 와 같 이 여 러 분 의 username 에 이 책 의 코 드 리 포 지 토 리 가 복 사 해 졌 을 것 입 니다 . 278 7 . Appendix 3 단 계 : 그 림 11 . 21 의 오 론 쪽 에 보 이 는 초 록 색 “Clone or download” 버 튼 을 클 릭 하 고 , 빨 간 색 박 스 로 표 시 한 버 튼 을 눌 러 서 여 러 분 의 username 아 래 있 는 코 드 리 포 지 토 리 주 소 를 복 사 합 니다 . “Acquiring and Running Codes in This Book” 를 참 고 해 서 명 령 행 모 드 로 들 어 가 세 요 . 여 기 서 는 로 컬 디 스 크 의 “ ~ / repo” 경 로 에 코 드 리 포 지 토 리를 복 사 한 다 고 가 정 하 겠 습 니다 . 그 럼 이 경 로로 이 동 하 고 , git clone 을 적 고 , 그 뒤 에 여 러 분 의 username 이 포 함 된 코 드 리 포 지 토 리 의 주 소 를 붙 여 놓 습 니다 . 이 제 명 령 을 수 행하 세 요 . # Replace your _ Github _ ID with your GitHub username git clone https : / / github . com / your _ Github _ ID / d2l - ko . git 이 제 이 책 의 코 드 리 포 지 토 리 의 모 든 파 일이 로 컬 디 스 크 의 ‘ ~ / repo / d2l - ko’ 경 로 에 복 사 되 었 을 것 입 니다 . 4 단 계 : 로 컬 경 로 에 있 는 코 드 리 포 지 토 리를 수 정 하 세 요 . ~ / repo / d2l - ko / chapter _ deep - learning - basics / linear - regression . md 파 일의 오 타 를 수 정 했 다 고 가 정 해 보 겠 습 니다 . 명 령 행 모 드 에 서 , ~ / repo / d2l - ko 경 로로 이 동 한 후 아 래 명 령 을 수 행하 세 요 . 7 . 1 . 이 책 에 기 여 하 는 방법 279 git status 그 러 면 그 림 11 . 22 에 서 처 럼 Git 이 “chapter _ deep - learning - basics / linear - regression . md” 파 일이 수 정 되 었 다 고 알 려 줍 니다 . 변 경 을 제 출 할 파 일이 정 확 하 면 , 아 래 명 령 을 수 행하 세 요 . git add chapter _ deep - learning - basics / linear - regression . md git commit - m ' fix typo in linear - regression . md ' git push 여 기 서 ' fix typo in linear - regression . md ' 는 제 출 하 는 변 경 에 대 한 설 명 입 니다 . 여 러 분 이 제 출 하 는 변 경 내 용 에 따 라 서 바 꿔 주 세 요 . 5 단 계 : 다 시 웹 브 라 우 저 에 서 이 책 의 코 드 리 포 지 토 리 주 소 [ 2 ] 를 입 력 하 세 요 . 그 림 12 . 20 의 좌 측 하 단 의 빨 간 색 상 자 로 표 시 된 “New pull request” 버 튼 을 클 릭 하 고 , 이 후 에 나 오 는 페 이 지 에 서 그 럼 11 . 12 의 오 른 쪽 에 빨 간 색 박 스 로 표 시 된 “compare across forks” 링 크 를 클 릭 하 세 요 . 다 음으 로 그 아 래 빤 간 박 스 로 표 시 된 “head fork : d2l - ai / d2l - ko” 를 클 릭 합 니다 . 그 럼 11 . 23 에 서 보 이 는 것 처 럼 , 팝 업 텍 스 트 상 자 에 여 러 분 의 GitHub ID 를 입 력 하 고 , 드 롭 다 운 메 뉴 에 서 ’YourGitHub - ID / d2l - ko’ 를 선 택 합 니다 . 280 7 . Appendix 6 단 계 : 그 림 11 . 24 처 럼 , 제 목 과 본 문 텍 스 트 상 자 에 여 러 분 이 제 출 하 는 pull request 에 대 한 설 명 을 적 어 주 세 요 . 그 리 고 , 녹 색 “Create pull request” 버 튼 을 눌 러 서 pull request 를 제 출 합 니다 . 7 . 1 . 이 책 에 기 여 하 는 방법 281 요 청 이 제 출 되 면 , 그 림 11 . 25 과 같 이 제 출 된 모 든 pull request 를 보 여 주 는 페 이 지 를 볼 수 있 습 니다 . 282 7 . Appendix 7 . 1 . 1 요 약 • 여 러 분 은 GitHub 를 이 용 해 서 이 책 에 기 여 를 할 수 있 습 니다 . 7 . 1 . 2 문 제 • 이 책 의 어 떤 부분 이 개 선 되 어야 한 다 고 느 낀 다 면 , pull request 를 제 출 해 보 세 요 . 7 . 1 . 3 참 고 자 료 [ 1 ] 이 책 ( 한 글 판 ) 의 기 여 자 목 록 . https : / / github . com / d2l - ai / d2l - ko / graphs / contributors [ 2 ] 이 책 의 코 드 리 포 지 토 리 주 소 . https : / / github . com / d2l - ai / d2l - ko [ 3 ] Git 설 치 하 기 . https : / / git - scm . com / book / zh / v2 [ 4 ] GitHub URL . https : / / github . com / 7 . 1 . 이 책 에 기 여 하 는 방법 283 7 . 1 . 4 Scan the QR Code to Discuss 7 . 2 d2l 패 키 지 색 인 함 수 또 는 클 래 스 이 름 Relevant Chapter bbox _ to _ rect Object Detection and Bounding Boxes Benchmark Asynchronous Computation corr2d Two - dimensional Convolutional Layer count _ tokens Sentiment Classiﬁcation : Using Recurrent Neural Networks data _ iter Linear Regression Implementation from Scratch data _ iter _ consecutive Language Model Data Set ( Jay Chou album lyrics ) data _ iter _ random Language Model Data Set ( Jay Chou album lyrics ) download _ imdb Sentiment Classiﬁcation : Using Recurrent Neural Networks download _ voc _ pascal Semantic Segmentation and Data Sets evaluate _ accuracy Image Augmentation get _ data _ ch7 Mini - batch Stochastic Gradient Descent get _ fashion _ mnist _ labels Image Classiﬁcation Data Set ( Fashion - MNIST ) get _ tokenized _ imdb Sentiment Classiﬁcation : Using Recurrent Neural Networks get _ vocab _ imdb Sentiment Classiﬁcation : Using Recurrent Neural Networks grad _ clipping Recurrent Neural Network Implementation from Scratch linreg Linear Regression Implementation from Scratch load _ data _ fashion _ mnist Deep Convolutional Neural Networks ( AlexNet ) load _ data _ jay _ lyrics Language Model Data Set ( Jay Chou album lyrics ) load _ data _ pikachu Object Detection Data Set ( Pikachu ) plt Linear Regression Implementation from Scratch predict _ rnn Recurrent Neural Network Implementation from Scratch predict _ rnn _ gluon Concise Implementation of Recurrent Neural Networks 일 반 색 인 284 7 . Appendix Table 7 . 1 – 이 전 페 이 지 에 서 계 속 함 수 또 는 클 래 스 이 름 Relevant Chapter predict _ sentiment Sentiment Classiﬁcation : Using Recurrent Neural Networks preprocess _ imdb Sentiment Classiﬁcation : Using Recurrent Neural Networks read _ imdb Sentiment Classiﬁcation : Using Recurrent Neural Networks read _ voc _ images Semantic Segmentation and Data Sets Residual Residual Networks ( ResNet ) resnet18 Concise Implementation of Multi - GPU Computing RNNModel Concise Implementation of Recurrent Neural Networks semilogy Model Selection set _ figsize Linear Regression Implementation from Scratch sgd Linear Regression Implementation from Scratch show _ bboxes Anchor Boxes show _ fashion _ mnist Image Classiﬁcation Data Set ( Fashion - MNIST ) show _ images Image Augmentation show _ trace _ 2d Gradient Descent and Stochastic Gradient Descent squared _ loss Linear Regression Implementation from Scratch to _ onehot Recurrent Neural Network Implementation from Scratch train Image Augmentation train _ 2d Gradient Descent and Stochastic Gradient Descent train _ and _ predict _ rnn Recurrent Neural Network Implementation from Scratch train _ and _ predict _ rnn _ gluon Concise Implementation of Recurrent Neural Networks train _ ch3 Softmax Regression Implementation from Scratch train _ ch5 Convolutional Neural Networks ( LeNet ) train _ ch7 Mini - batch Stochastic Gradient Descent train _ gluon _ ch7 Mini - batch Stochastic Gradient Descent try _ all _ gpus Image Augmentation try _ gpu Convolutional Neural Networks ( LeNet ) use _ svg _ display Linear Regression Implementation from Scratch VOC _ CLASSES Semantic Segmentation and Data Sets VOC _ COLORMAP Semantic Segmentation and Data Sets voc _ label _ indices Semantic Segmentation and Data Sets voc _ rand _ crop Semantic Segmentation and Data Sets VOCSegDataset Semantic Segmentation and Data Sets 7 . 2 . d2l 패 키 지 색 인 285