Expected Value of Matrix Quadratic Forms with Wishart distributed Random Matrices Melinda Hagedorn , Heinrich Heine University , D¨usseldorf , Germany Dec . 13 , 2022 All data generated or analysed in this article are available in [ 4 ] . Abstract To explore the limits of a stochastic gradient method , it may be useful to consider an example consisting of an inﬁnite number of quadratic functions . In this context , it is appropriate to determine the expected value and the covariance matrix of the stochastic noise , i . e . the diﬀerence of the true gradient and the approximated gradient generated from a ﬁnite sample . When specifying the covariance matrix , the expected value of a quadratic form QBQ is needed , where Q is a Wishart distributed random matrix and B is an arbitrary ﬁxed symmetric matrix . After deriving an expression for E ( QBQ ) and considering some special cases , a numerical example is used to show how these results can support the comparison of two stochastic methods . Key words : Wishart distribution , quadratic form , expected value , second momentum , stochastic gradient method , averaging 1 . Outline The Wishart distribution is a generalization of the χ 2 distribution . According to [ 6 ] and [ 7 ] the Wishart distribution plays a prominent role in estimating the covariance matrix in context of multivariate statistics . Therefore , it is not surprising that this important distribution is subject of current research . For instance , [ 10 ] considers quadratic forms Y T CY with non - negative deﬁnite matrix C and normally distributed random matrix Y and investigates what are the necessary and suﬃcient conditions for Y T CY to be Wishart distributed . Based on this , [ 9 ] examines in the special case of Y with expected value zero under which conditions Y T CY is central Wishart distributed . Furthermore , in [ 11 ] the dispersion matrix of vec ( Y T AY ) is derived , where A is an arbitrary nonrandom matrix . In this paper we are interested in a diﬀerent kind of quadratic form : For a Wishart distributed Q and a symmetric matrix B we derive an expression for the expected value of QBQ . For B = I n this is the second momentum of the Wishart distribution and thus part of the 1 a r X i v : 2212 . 01412v3 [ m a t h . O C ] 1 3 D ec 2022 examination of the momenta of the Wishart distribution in [ 1 ] . In [ 8 ] a diﬀerent and more general formula for the expected value of XAX T ⊗ XBX T was already derived , where X ∼ N n , k ( µ , Σ , Ψ ) with symmetric positive deﬁnite matrices Σ and Ψ . While the formulation in [ 8 ] is mathematically equivalent to the one derived in this paper for the special case considered here , the actual computation of the expected value is quite diﬀerent - using a linear system based on Kronecker products in [ 8 ] and a lower dimensional variant below . This paper is structured as follows . First , in chapter 3 . , we motivate in the context of the stochastic gradient method why an expression for the expected value E ( QBQ ) is needed . In the 4 . chapter we recall two important , well - known properties of Wishart distributed random matrices . With this preparation , we are then able to present a theorem with a general expression for E ( QBQ ) in the 5 . chapter , prove the assertion , and derive more compact expressions under stronger assumptions . Also the connection to the result of [ 8 ] is worked out in more detail . Finally , we show in chapter 6 . that the approximated value for increasing sample size approaches the theoretical value from the previous chapter , which illustrates the statement of the theorem , and use the theorem to compare the ordinary stochastic gradient method with a variant that uses averaging . An application of the expected value E ( QBQ ) derived here is the minimization of a random convex quadratic function . While convex quadratic problems in some form are the simplest nontrivial problems , they are complex enough to reproduce local dynamics of more diﬃcult smooth problems . They arise in practical applications in the form of large scale systems of linear equations and least squares problems . Studying the performance of a method on convex quadratic problems is a fundamental preparation to extend the method to more general problems ( see [ 3 ] and [ 2 ] ) . 2 . Notation In this paper the all - one and the all - zero vectors and matrices are denoted by 0 n : = ( 0 , . . . , 0 ) T ∈ R n , 0 n × n : = 0 n 0 T n ∈ R n × n , 1 n : = ( 1 , . . . , 1 ) T ∈ R n , 1 n × n : = 1 n 1 T n ∈ R n × n and the identity matrix by I n with dimension n ∈ N . The Hadamard product of two matrices X and Y of the same dimension is deﬁned componentwise as ( X ◦ Y ) ij : = X ij Y ij . Let X ⊗ Y be the Kronecker product of two arbitrary matrices X and Y . A matrix M has rank rk ( M ) , determinant det ( M ) and trace tr ( M ) : = (cid:80) ni = 1 M i , i . If a matrix M is positive deﬁnite , we write M (cid:31) 0 . The vector with the diagonal elements of a quadratic matrix M is denoted by diag ( M ) and for a vector x ∈ R n the expression Diag ( x ) symbolizes the n × n diagonal matrix with the entries of x on its diagonal . The vector vec ( M ) is obtained by stacking the columns 2 of M on top of one another . The inverse function of vec is mat : = vec − 1 . Furthermore , S n + denotes the set of all symmetric , positive deﬁnite matrices , i . e . S n + : = { M ∈ R n × n | M = M T , M (cid:31) 0 } . ( 1 ) The expected value and the covariance matrix of a random vector X are denoted by E ( X ) and Cov ( X ) whenever they exist . 3 . Motivation 3 . 1 Stochastic Gradient Method In order to ﬁnd the minimum of a function f : R n → R , f ( x ) : = 1 m (cid:80) mi = 1 f i ( x ) , i . e . the root of ∇ f , the gradient descent can be used whose iterates x k + 1 = x k − γ k ∇ f ( x k ) are generated with step length γ k ≥ 0 starting at a point x 0 . If m is very large , the calculation of the exact gradient ∇ f ( x k ) is computationally expensive . To avoid computing the full gradient ∇ f ( x k ) at each iteration , it can be approximated . Assuming an i . i . d . chosen batch S k from the uniform distribution of { 1 , . . . , m } , the expected value of ∇ S k f ( x k ) : = 1 | S k | (cid:80) i ∈ S k ∇ f i ( x k ) is E ( ∇ S k f ( x k ) ) = E (cid:32) 1 | S k | (cid:88) i ∈ S k ∇ f i ( x k ) (cid:33) = 1 | S k | (cid:88) i ∈ S k E ( ∇ f i ( x k ) ) = 1 | S k | (cid:88) i ∈ S k m (cid:88) j = 1 ∇ f j ( x k ) · P ( ∇ f j ( x k ) = ∇ f i ( x k ) ) = 1 | S k | (cid:88) i ∈ S k 1 m m (cid:88) j = 1 ∇ f j ( x k ) = 1 | S k | · | S k | · ∇ f ( x k ) = ∇ f ( x k ) . This is the motivation for using the approximation ∇ S k f ( x k ) instead of ∇ f ( x k ) , i . e . the stochastic gradient ( descent ) method ( SGD ) is given by the iterates x k + 1 = x k − γ k ∇ S k f ( x k ) . ( 2 ) There exist numerous modiﬁcations of the stochastic gradient method . The following ex - ample can be useful to examine the limits of a SGD method or to compare two variants of SGD . 3 . 2 Random quadratic functions As in [ 5 ] , we assume a ﬁxed matrix A ∈ R n × n with det ( A ) (cid:54) = 0 and n ∈ N . At each iteration (cid:96) ∈ { 1 , . . . , m } we draw random vectors r (cid:96) and b (cid:96) independently from the n - variate normal distribution with expected value 0 n and covariance matrix Σ = Σ T (cid:31) 0 . Brieﬂy , this can be written as r (cid:96) , b (cid:96) ∼ N n ( 0 n , Σ ) . With a (cid:96) : = Ar (cid:96) we are able to deﬁne the functions f (cid:96) : R n → R , f (cid:96) ( x ) : = 12 ( ( a (cid:96) ) T x ) 2 + ( b (cid:96) ) T x . ( 3 ) 3 Since a (cid:96) has the expected value E ( a (cid:96) ) = E ( Ar (cid:96) ) = A E ( r (cid:96) ) = 0 n and the covariance matrix Cov ( a (cid:96) ) = Cov ( Ar (cid:96) ) = A Cov ( r (cid:96) ) A T = A Σ A T , it holds a (cid:96) ∼ N n ( 0 n , A Σ A T ) and the second momentum is given by E ( a (cid:96) ( a (cid:96) ) T ) = Cov ( a (cid:96) ) + E ( a (cid:96) ) E ( a (cid:96) ) T = Cov ( a (cid:96) ) = A Σ A T . Because f (cid:96) are quadratic functions of r (cid:96) and b (cid:96) with expected value E ( f (cid:96) ( x ) ) = 12 E ( x T a (cid:96) ( a (cid:96) ) T x ) + x T E ( b (cid:96) ) = 12 x T E ( a (cid:96) ( a (cid:96) ) T ) x = 12 x T A Σ A T x = : f ( x ) and due to the existence of the fourth momenta of r (cid:96) and b (cid:96) , for a given x the variances of f (cid:96) ( x ) are bounded and almost surely it exists lim m →∞ m (cid:88) (cid:96) = 1 f (cid:96) ( x ) = f ( x ) . ( 4 ) The stochastic gradient method uses the approximation ∇ f (cid:96) ( x ) instead of the full gradient ∇ f ( x ) . Therefore it is reasonable to examine the noise ξ (cid:96) deﬁned by ξ (cid:96) : = ∇ f (cid:96) ( x ) − ∇ f ( x ) = a (cid:96) ( a (cid:96) ) T x + b (cid:96) − A Σ A T x ( 5 ) with expected value E ( ξ (cid:96) ) = E ( a (cid:96) ( a (cid:96) ) T ) x + E ( b (cid:96) ) − A Σ A T x = A Σ A T x + 0 n − A Σ A T x = 0 n . Using the independence of a (cid:96) and b (cid:96) and deﬁning B : = A T xx T A , the covariance matrix of ξ (cid:96) can be written as Cov ( ξ (cid:96) ) = E ( ξ (cid:96) ( ξ (cid:96) ) T ) − E ( ξ (cid:96) ) E ( ξ (cid:96) ) T = E ( ξ (cid:96) ( ξ (cid:96) ) T ) = E ( ( a (cid:96) ( a (cid:96) ) T x + b (cid:96) − A Σ A T x ) ( a (cid:96) ( a (cid:96) ) T x + b (cid:96) − A Σ A T x ) T ) = E ( a (cid:96) ( a (cid:96) ) T xx T a (cid:96) ( a (cid:96) ) T ) + E ( a (cid:96) ( a (cid:96) ) T x ( b (cid:96) ) T ) − E ( a (cid:96) ( a (cid:96) ) T xx T A Σ A T ) + E ( b (cid:96) x T a (cid:96) ( a (cid:96) ) T ) + E ( b (cid:96) ( b (cid:96) ) T ) − E ( b (cid:96) x T A Σ A T ) − E ( A Σ A T xx T a (cid:96) ( a (cid:96) ) T ) − E ( A Σ A T x ( b (cid:96) ) T ) + E ( A Σ A T xx T A Σ A T ) = E ( Ar (cid:96) ( r (cid:96) ) T A T xx T Ar (cid:96) ( r (cid:96) ) T A T ) + E ( a (cid:96) ( a (cid:96) ) T x ) E ( b (cid:96) ) T − E ( a (cid:96) ( a (cid:96) ) T ) xx T A Σ A T + E ( b (cid:96) ) E ( x T a (cid:96) ( a (cid:96) ) T ) + (cid:2) Cov ( b (cid:96) ) + E ( b (cid:96) ) E ( b (cid:96) ) T (cid:3) − E ( b (cid:96) ) x T A Σ A T − A Σ A T xx T E ( a (cid:96) ( a (cid:96) ) T ) − A Σ A T x E ( b (cid:96) ) T + A Σ A T xx T A Σ A T = A E ( r (cid:96) ( r (cid:96) ) T Br (cid:96) ( r (cid:96) ) T ) A T + 0 n × n − A Σ A T xx T A Σ A T + 0 n × n + Σ − 0 n × n − A Σ A T xx T A Σ A T − 0 n × n + A Σ A T xx T A Σ A T = A E ( r (cid:96) ( r (cid:96) ) T Br (cid:96) ( r (cid:96) ) T ) A T + Σ − A Σ B Σ A T . ( 6 ) This motivates us to determine expected values of the form E ( r (cid:96) ( r (cid:96) ) T Br (cid:96) ( r (cid:96) ) T ) with random vectors r (cid:96) ∼ N n ( 0 n , Σ ) and a symmetric matrix B . 4 4 . Introduction In the context of multivariate statistics , the following deﬁnition is of great importance : Deﬁnition 1 For k ∈ N independent and identically distributed ( i . i . d . ) random vectors r (cid:96) ∼ N n ( 0 n , Σ ) with covariance matrix Σ = Σ T (cid:31) 0 the random matrix Q : = (cid:80) k(cid:96) = 1 r (cid:96) ( r (cid:96) ) T is called n - variate Wishart distributed with scale matrix Σ and k degrees of freedom . We write Q ∼ W n ( Σ , k ) . Remark 1 Alternatively Q ∼ W n ( Σ , k ) can be deﬁned by Q : = RR T , where R is a random n × k matrix , which columns r (cid:96) are independent and identically N n ( 0 n , Σ ) distributed . This coincides exactly with Deﬁnition 1 . The following result is well known : Lemma 1 The expected value of Q ∼ W n ( Σ , k ) is E ( Q ) = k Σ . In the proof of the main result , the following important property of Wishart distributed random matrices will also be needed . For the sake of completeness it is proved here . Lemma 2 Consider Q ∼ W n ( Σ , k ) with Σ ∈ S n + , C ∈ R n × m and rk ( C ) = m ∈ N . Then C T QC ∼ W m ( C T Σ C , k ) . Proof . Since we assume Q ∼ W n ( Σ , k ) , there exist k independent and identically dis - tributed r (cid:96) ∼ N n ( 0 n , Σ ) so that Q = (cid:80) k(cid:96) = 1 r (cid:96) ( r (cid:96) ) T . Consider C T r (cid:96) with the expected value E ( C T r (cid:96) ) = C T E ( r (cid:96) ) = 0 m and positive deﬁnite Cov ( C T r (cid:96) ) = C T Cov ( r (cid:96) ) C = C T Σ C due to the full rank of C , thus C T r (cid:96) ∼ N m ( 0 m , C T Σ C ) . Then we are able to conclude that C T QC = C T (cid:16)(cid:80) k(cid:96) = 1 r (cid:96) ( r (cid:96) ) T (cid:17) C = (cid:80) k(cid:96) = 1 ( C T r (cid:96) ) ( C T r (cid:96) ) T ∼ W m ( C T Σ C , k ) . 5 . Determination of E ( QBQ ) The following theorem is the main result of this paper : Theorem 1 For n ∈ N we consider the n - variate Wishart distributed random matrix Q with the symmetric , positive deﬁnite scale matrix Σ and k ∈ N degrees of freedom , i . e . Q ∼ W n ( Σ , k ) . Furthermore , we deal with a ﬁxed matrix B = B T ∈ R n × n . The expected value of the quadratic form QBQ is E ( QBQ ) = k · tr ( B Σ ) Σ + ( k 2 + k ) Σ B Σ . 5 Proof . Let the matrix Σ = Σ T (cid:31) 0 be factorized as Σ = UDU T with diagonal matrix D and orthogonal matrix U , i . e . UU T = I n . Since Q ∼ W n ( Σ , k ) , there exist i . i . d . random vectors r (cid:96) ∼ N n ( 0 n , Σ ) so that Q = (cid:80) k(cid:96) = 1 r (cid:96) ( r (cid:96) ) T = (cid:80) k(cid:96) = 1 Q (cid:96) with Q (cid:96) : = r (cid:96) ( r (cid:96) ) T ∼ W n ( Σ , 1 ) . We note that Lemma 1 gives us E ( Q (cid:96) ) = 1 · Σ = Σ . Now we deﬁne the unitary transformations ˜ B : = U T BU , ˜ r (cid:96) : = U T r (cid:96) and ˜ Q (cid:96) : = U T Q (cid:96) U . Since ˜ r (cid:96) ∼ N n ( 0 n , D ) , its components ˜ r (cid:96)i ∼ N ( 0 , D i , i ) have the momenta E ( ˜ r (cid:96)i ) = 0 , E ( ( ˜ r (cid:96)i ) 2 ) = D i , i and E ( ( ˜ r (cid:96)i ) 4 ) = 3 D 2 i , i , and due to Lemma 2 we ﬁnd ˜ Q (cid:96) = U T Q (cid:96) U = U T r (cid:96) ( r (cid:96) ) T U = U T r (cid:96) ( U T r (cid:96) ) T = ˜ r (cid:96) ( ˜ r (cid:96) ) T ∼ W n ( D , 1 ) . The expected value of ˜ Q (cid:96) ˜ B ˜ Q (cid:96) is componentwise given by E ( ˜ Q (cid:96) ˜ B ˜ Q (cid:96) ) i , j = E ( e T i ˜ r (cid:96) ( ˜ r (cid:96) ) T ˜ B ˜ r (cid:96) ( ˜ r (cid:96) ) T e j ) = E ( ˜ r (cid:96)i ˜ r (cid:96)j ( ˜ r (cid:96) ) T ˜ B ˜ r (cid:96) ) = E (cid:32) ˜ r (cid:96)i ˜ r (cid:96)j n (cid:88) p , q = 1 ˜ B p , q ˜ r (cid:96)p ˜ r (cid:96)q (cid:33) = n (cid:88) p , q = 1 ˜ B p , q E ( ˜ r (cid:96)i ˜ r (cid:96)j ˜ r (cid:96)p ˜ r (cid:96)q ) = n (cid:88) p , q = 1 ˜ B p , q   3 D 2 i , i if i = j = p = q D i , i D p , p if i = j (cid:54) = p = q D i , i D j , j if i (cid:54) = j and ( ( i = p , j = q ) or ( i = q , j = p ) ) 0 else = (cid:26) 3 D 2 i , i ˜ B i , i + D i , i (cid:80) np = 1 , p (cid:54) = i ˜ B p , p D p , p if i = j 2 D i , i D j , j ˜ B i , j if i (cid:54) = j = (cid:26) 2 D 2 i , i ˜ B i , i + D i , i tr ( ˜ BD ) if i = j 2 D i , i D j , j ˜ B i , j if i (cid:54) = j = (cid:104) 2 ( diag ( D ) diag ( D ) T ) ◦ ˜ B + tr ( ˜ BD ) D (cid:105) i , j . All in all , we obtain E ( QBQ ) = E (cid:32)(cid:32) k (cid:88) (cid:96) = 1 Q (cid:96) (cid:33) B (cid:32) k (cid:88) h = 1 Q h (cid:33)(cid:33) = k (cid:88) (cid:96) , h = 1 E ( Q (cid:96) BQ h ) = k (cid:88) (cid:96) = 1 E ( Q (cid:96) BQ (cid:96) ) + k (cid:88) (cid:96) , h = 1 , (cid:96) (cid:54) = h E ( Q (cid:96) BQ h ) = k (cid:88) (cid:96) = 1 E ( UU T Q (cid:96) UU T BUU T Q (cid:96) UU T ) + k (cid:88) (cid:96) , h = 1 , (cid:96) (cid:54) = h E ( Q (cid:96) ) B E ( Q h ) = k (cid:88) (cid:96) = 1 U E ( ˜ Q (cid:96) ˜ B ˜ Q (cid:96) ) U T + k (cid:88) (cid:96) , h = 1 , (cid:96) (cid:54) = h Σ B Σ = kU (cid:104) 2 ( diag ( D ) diag ( D ) T ) ◦ ˜ B + tr ( ˜ BD ) D (cid:105) U T + ( k 2 − k ) Σ B Σ . 6 With ˜ B = U T BU as deﬁned above one gets E ( QBQ ) = 2 kU ( D ˜ BD ) U T + k · tr ( ˜ BD ) UDU T + ( k 2 − k ) Σ B Σ = 2 k ( UDU T ) B ( UDU T ) + k · tr ( U T BUD ) Σ + ( k 2 − k ) Σ B Σ = 2 k Σ B Σ + k · tr ( BUDU T ) Σ + ( k 2 − k ) Σ B Σ = k · tr ( B Σ ) Σ + ( k 2 + k ) Σ B Σ , which corresponds exactly to the assertion . In practice , the following special cases might be of interest . Additionally to the assumptions of Theorem 1 we demand k = 1 . Then the result of the theorem simpliﬁes to E ( QBQ ) = tr ( B Σ ) Σ + 2Σ B Σ ( 7 ) If we assume in addition that Σ = σ 2 I n , we get E ( QBQ ) = σ 4 [ 2 B + tr ( B ) I n ] . ( 8 ) Finally , the assumption Σ = I n leads to the special case analyzed in paper [ 5 ] . With B = I n it follows immediately from Theorem 1 : Corollary 1 The second momentum of a W n ( Σ , k ) distributed random matrix Q with scale matrix Σ ∈ S n + is given by E ( Q 2 ) = ( k 2 + k ) Σ 2 + tr ( Σ ) k Σ . Now we are able to present an expression for the covariance matrix of the noise ( 6 ) in which the random variables r (cid:96) have been eliminated . Since r (cid:96) ∼ N n ( 0 n , Σ ) we can deﬁne the random matrix Q (cid:96) : = r (cid:96) ( r (cid:96) ) T ∼ W n ( Σ , 1 ) and using ( 7 ) we conclude Cov ( ξ (cid:96) ) = A E ( r (cid:96) ( r (cid:96) ) T Br (cid:96) ( r (cid:96) ) T ) A T + Σ − A Σ B Σ A T = A E ( Q (cid:96) BQ (cid:96) ) A T + Σ − A Σ B Σ A T = AU (cid:2) 2 (cid:0) diag ( D ) ( diag ( D ) ) T (cid:1) ◦ ( U T BU ) + tr ( U T BUD ) D (cid:3) U T A T + Σ − A Σ B Σ A T , where D and U satisfy Σ = UDU T and B = A T xx T A . In case of Σ = I n this expression can be simpliﬁed to Cov ( ξ (cid:96) ) = A [ 2 B + tr ( B ) I n ] A T + I n − ABA T = 2 ABA T + tr ( B ) AA T + I n − ABA T = AA T xx T AA T + tr ( A T xx T A ) AA T + I n = AA T xx T AA T + (cid:107) A T x (cid:107) 22 AA T + I n . As mentioned before , an alternative expression for E ( QBQ ) can be derived . If one chooses A = I k , B = I k , Ψ = I k and µ = 0 k · n in Theorem 2 . 2 . 9 ( ii ) from [ 8 ] , one obtains for Q : = XX T E ( Q ⊗ Q ) = k 2 · Σ ⊗ Σ + k · vec ( Σ ) vec ( Σ ) T + k · K n , n · Σ ⊗ Σ , ( 9 ) 7 where K n , n is the commutation matrix consisting of n × n blocks with n × n entries each . In the ( i , j ) - th block the only non - zero element is a “1” in position ( j , i ) . It is a permutation matrix that can be used to describe the relationship between the vectorized forms of a square matrix A and its transpose , since vec ( A T ) = K n , n vec ( A ) . Using the calculation rule ( A ⊗ B ) vec ( V ) = vec ( BV A T ) for Kronecker products we get E ( QBQ ) = mat ( E ( vec ( QBQ ) ) ) = mat ( E ( ( Q ⊗ Q ) vec ( B ) ) ) = mat ( E ( Q ⊗ Q ) vec ( B ) ) = mat ( ( k 2 · Σ ⊗ Σ + k · vec ( Σ ) vec ( Σ ) T + k · K n , n · Σ ⊗ Σ ) vec ( B ) ) = mat ( k 2 · vec ( Σ B Σ ) + k · vec ( Σ ) vec ( Σ ) T vec ( B ) + k · K n , n · vec ( Σ B Σ ) ) = mat ( vec ( k 2 Σ B Σ ) + k · vec ( Σ ) sum ( Σ ◦ B ) + vec ( k Σ B Σ ) ) = k 2 Σ B Σ + k Σ · sum ( Σ ◦ B ) + k Σ B Σ = k · tr ( B Σ ) Σ + ( k 2 + k ) Σ B Σ , where sum ( M ) denotes the sum over all entries of a matrix M . Thus , we get the same expression as in Theorem 1 . 6 . Numerical examples 6 . 1 Illustrative example For dimension n = 10 we generate randomly the matrices B and Σ with i . i . d . standard normally distributed entries and ensure , that B is symmetric and Σ ∈ S n + . Let k = 3 and Q ∼ W n ( Σ , k ) . The aim is to determine E ( QBQ ) . With the diagonal matrix D and the orthogonal matrix U from an eigendecomposition Σ = UDU T and with Theorem 1 we get on the one hand E exact : = kU (cid:2) 2 (cid:0) diag ( D ) ( diag ( D ) ) T (cid:1) ◦ ( U T BU ) + tr ( U T BUD ) D (cid:3) U T + ( k 2 − k ) Σ B Σ . On the other Hand , the expected value can be approximated with m realizations Q i as E empiric : = 1 m m (cid:88) i = 1 Q i BQ i , ( 10 ) because the law of large numbers provides lim m →∞ 1 m m (cid:88) i = 1 Q i BQ i = E ( QBQ ) almost surely . To get an impression of how fast E empiric approaches the theoretical value E exact for in - creasing sample size m , we plot the relative error (cid:107) E exact − E empiric (cid:107) 2 / (cid:107) E exact (cid:107) 2 against m . Due to the randomness during the generation of B and Σ and in the realizations of Q , ten independent runs are made . At each run the relative error is calculated for m ∈ { 1 , 10 , 100 , 10 3 , 10 4 , 10 5 , 10 6 } . Thus , to be more precise , the logarithmic plot in ﬁg - ure 1 shows the arithmetic means of the relative errors in dependence of m . 8 The standard deviation is represented by error bars : Figure 1 : Relative error (cid:107) E exact − E empiric (cid:107) 2 / (cid:107) E exact (cid:107) 2 with respect to the number of samples m used for the approximation E empiric . For m = 1 the mean distance between E exact and E empiric is about 4 · 10 4 which leads to a relative error of 2 . Using 10 6 samples this distance reduces to approximately 90 and the relative error to 3 · 10 − 3 . The curve in the logarithmic plot is roughly linear decreasing . Two interesting observations arise : For large m the approximation E empiric tends to the result of Theorem 1 and in order to approximate E ( QBQ ) adequately by ( 10 ) many samples and a lot of time is needed . Thus , the main result is not only theoretically fascinating but also of practical relevance . 6 . 2 Comparison of two SGD methods Our actual goal , as mentioned before , is to compare two algorithms that approximate so - lutions for the problem min x f ( x ) with f ( x ) = lim m →∞ (cid:80) m(cid:96) = 1 f (cid:96) ( x ) from ( 4 ) . For n = 10 dimensions we randomly generate the entries of the matrices Σ and A i . i . d . from the N ( 0 , 1 ) distribution and ensure that Σ = Σ T is positive deﬁnite and that A = A T is positive semideﬁnite with norm (cid:107) A (cid:107) 2 = 1 and condition number cond ( A ) = 5 . As initial value x 0 ∈ R n we choose the entries randomly from N ( 0 , 1 ) and normalize the vector . Set the number of iterations to k max = 10 7 and let the step length be given by γ ≡ γ k = 10 − 3 . Let { x 1 , . . . , x k max } be the iterates generated by the SGD method ( 2 ) . A variation of the SGD method described above is the averaged SGD as analyzed in [ 12 ] . Starting with ¯ x 0 : = x 0 the iterates of the ASGD can be deﬁned as ¯ x k : = 1 k k (cid:88) (cid:96) = 1 x (cid:96) , ( 11 ) 9 where x (cid:96) are the iterates of the ordinary SGD method and k ∈ { 1 , . . . , k max } . In each iteration we randomly draw r k and b k from N n ( 0 n , Σ ) , calculate a k = Ar k , the gradient of f k ( x k ) deﬁned in ( 3 ) , the iterates x k and ¯ x k and the noise ξ k from ( 5 ) . The necessary condition for a minimum of f at x ∗ is that the gradient has to vanish , i . e . ∇ f ( x ∗ ) = 0 . By construction the global optimal solution is x opt = 0 . Below the two algorithms are compared by creating graphs of (cid:107)∇ f ( x k ) (cid:107) 2 and (cid:107) x k − x opt (cid:107) 2 = (cid:107) x k (cid:107) 2 in dependence of the number of iterations k , respectively . Additionally , we are interested in using our insights about the noise ξ k for this comparison . Since E exact ≡ E ( ξ k ) = 0 is valid independently of k , the approximation E empiric : = 1 k (cid:80) k(cid:96) = 1 ξ (cid:96) should tend to E exact . This motivates plotting (cid:107) E exact − E empiric (cid:107) 2 with respect to the number of samples to estimate E ( ξ k ) . On the other hand , Cov ( ξ k ) depends on k . At the optimal solution x opt the covariance matrix of the noise is just the scale matrix Σ . We can use this to investigate , how the covariance matrices of the iterates , that can be calculated exactly using Theorem 1 , approach to Σ with increasing number of iterations . Thus we plot (cid:107) Cov ( ξ k ) − Σ (cid:107) 2 in dependence of the number of iterations k . This way we obtain the following four graphs : Figure 2 : In all four graphs the iterates of the SGD method ( blue , solid line ) is compared to the iterates of the ASGD method ( red , dashed line ) . On the left hand side the norm of the gradient and the distance to the optimal solution are shown with respect to the number of iterations k . The ASGD method reaches lower values in both cases . In addition to that statistical ﬂuctuations are much smaller . At the top on the right hand side there is a plot of the distance of the empiric estimate of the expected value of the noise to the exact expected value in dependence of the number of samples . Both algorithms perform comparably well . Bottom right we have a plot of the distance of the covariance matrix of the noise in the k th iteration to the covariance matrix 10 at the optimal solution with respect to k . Again , the ASGD method performs better in both counts : by reaching lower values and by ﬂuctuating less . The advantages of the ASGD are not surprising and consistent with the results of [ 12 ] . This serves as a simple example of how two algorithms can be compared using Theorem 1 . 7 . Conclusion In Theorem 1 it was proven that the expected value of the quadratic form QBQ with Q ∼ W n ( Σ , k ) and B = B T can be expressed using k , B , Σ and an eigendecomposition Σ = UDU T . Moreover , special cases for certain k , Σ and B were derived from this general formula , for instance the second momentum of a Wishart distributed random matrix Q , i . e . E ( Q 2 ) . A ﬁrst example demonstrates the validity of the theorem . Beyond that the result is used to compare two stochastic methods . Acknowledgment I would like to express my sincere thanks to Florian Jarre , Holger Schwender and Dietrich von Rosen for their support and valuable comments . References [ 1 ] Bishop , Adrian N . ; Del Moral , Pierre ; Niclas , Ang ` ele u . a . : An introduction to Wishart matrix moments . In : Foundations and Trends ® in Machine Learning 11 ( 2018 ) , Nr . 2 , S . 97 – 218 [ 2 ] Goh , Gabriel : Why Momentum Really Works . In : Distill ( 2017 ) . http : / / dx . doi . org / 10 . 23915 / distill . 00006 . – DOI 10 . 23915 / distill . 00006 [ 3 ] Gonzaga , Cl´ovis C ; Schneider , Ruana M . : On the steepest descent algorithm for quadratic functions . In : Computational Optimization and Applications 63 ( 2016 ) , Nr . 2 , S . 523 – 542 [ 4 ] Hagedorn , Melinda : GitHub repository . In : https : / / github . com / MHagedorn / wishart ( 2022 ) [ 5 ] Hagedorn , Melinda ; Jarre , Florian : Optimized convergence of stochastic gradient descent by weighted averaging . In : Preprint , https : / / optimization - online . org / 2022 / 09 / optimized - convergence - of - stochastic - gradient - descent - by - weighted - averaging / ( 2022 ) [ 6 ] H¨ardle , Wolfgang K . ; Hl´avka , Zdenˇek : Multivariate statistics : exercises and solu - tions . Springer , 2015 [ 7 ] Kanti , Mardia ; Kent , JT ; Bibby , John : Multivariate analysis . 1979 11 [ 8 ] Kollo , Tonu ; Von Rosen , Dietrich : Advanced multivariate statistics with matrices . Springer , 2005 [ 9 ] Masaro , Joe ; Wong , Chi S . : Wishart distributions associated with matrix quadratic forms . In : Journal of multivariate analysis 85 ( 2003 ) , Nr . 1 , S . 1 – 9 [ 10 ] Mathew , Thomas ; Nordstr¨om , Kenneth : Wishart and chi - square distributions associated with matrix quadratic forms . In : journal of multivariate analysis 61 ( 1997 ) , Nr . 1 , S . 129 – 143 [ 11 ] Neudecker , H : On the dispersion matrix of a matrix quadratic form connected with the noncentral Wishart distribution . In : Linear algebra and its applications 70 ( 1985 ) , S . 257 – 261 [ 12 ] Polyak , Boris T . ; Juditsky , Anatoli B . : Acceleration of stochastic approximation by averaging . In : SIAM journal on control and optimization 30 ( 1992 ) , Nr . 4 , S . 838 – 855 12