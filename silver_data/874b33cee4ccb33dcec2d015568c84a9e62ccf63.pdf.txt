ilIultivariate Behavioral Research , 25 ( 4 ) , 467 - 478 Copyright O 1990 , Lawrence Erlbaum Associates , Inc . The Detection and Interpretation of Interaction Effects Between Gontinuous Variables in Multiple Regression James Jaccard , Choi K . Wan , and Robert Turrisi University at Albany State University of New York , Albany Issues in the detection and interpretation of interaction effects between quantitative variables in multiple regression analysis are discussed . Recent articles by Cronbach ( 1987 ) amtd Dunlap and Kemery ( 1987 ) suggested the use of two transformations to reduce " problem " of multicollinearity . These transformations are discussed in the context of the conditional nature of multiple regression with product terms . It is argued that although additive transformations do not affect Ihe overall test of statistical interaction , they do affect the interpretationalvalue of regression coefficients . Factors other than multicollinevlrity that may account for failures to observe interaction effects are noted . Morris , Sherman , and Mansfield ( 1986 ) have noted a persistent failure of psychologists to detect interaction effects between continuous variables in multiple regressioin analysis . They suggested that multicollinearity between product terms and their constituent parts may be the source of the problem and proposed a form of principal components analysis as a remedy . In two later articles , Cronbach ( 1987 ) and Dunlap and Kemery ( 1987 ) point out limitations with the approach of Morris et al . and specify alternative methods for coping with multicollinearity . Cronbach suggests that interaction effects 1be evaluated by the traditional hierarchical regression approach advocated by Cohen and Cohen ( 1983 ) . However , he suggests that an additive transformation be performed on the predictor variables prior to the formation of the product term , and that the dependent variable ( Y ) be regressed onto the transformed variablt : ~ and their product for purposes of evaluating the interaction effect . The transformation for a given predictor ilnvolves subtracting the mean of the predictor variable from each individual ' s raw score on that predictor , thus forming deviitltion scores . Cronbach suggests that such a transformation will yield low correlations between the product term and the component parts of the term . This is desired , he asserts , because it decreases the probability of computational errors and Preparation of this article was supported by MCHD grant HD2215201 ancl MAAA grant AA0687501 . We would like to thank George Alliger , Jacob Cohen , William Dilllon , and Scott Maxwell for their helpful comments on a previous draft of the manuscript . Requests for reprints should be sent to James Jaccard , Department of Psychology , State University of New York , Albany , New York 12 ; ! 22 . OCTOBER 1990 467 J . Jaccard , C . Wan , and R . Turrisi because " the bs are closely related to the simplexand Yeffects and the interaction " ( p . 416 ) . Dunlap and Kemery examined the same issues as Cronbach , but suggested a transformation that involved standardizing the predictor variables . The results of the hierarchical test of the interaction effect are identical for both the Cronbach and Dunlap and Kemery transformations . Based on these articles , a researcher might decide to use either transformation when testing for statistical interaction . The implication is that the choice of transformation is somewhat arbitrary , because the overall test of the interaction effect will yield identical results . Although this is substantively correct , we argue that the choice of a transformation has interpretative implications for the analysis and should not be arbitrary . Cronbach ( 1987 ) and Dunlap and Kemery ( 1987 ) also fail to provide insights into the problem motivating the original Morris et al . ( 1986 ) article , namely the persistent failure by psychologists using moderated regression analysis to observe significant interaction effects . The implication of the Cronbach article is that multicollinearity will only be a problem when it leads to computational errors within current computer algorithms . It is unlikely that the high degree of multicollinearity required for this to occur has been present in the majority of empirical evaluations of moderated multiple regression . Thus , other factors probably are operating that make it difficult to correctly detect moderated relationships . In this article , we will suggest what some of these others factors might be . Additive Transformations and Interpretive Issues Consider the case of three continuous variables , where the investigator is interested in the effects of two independent variables ( XI andX2 ) on adependent variable ( Y ) . The test of an additive ( or main effects ) model for predicting Yfrom X1 and X2 typically takes the form of a least squares regression approach such that where a = the least squares estimate of the intercept , and bl ' and b2 ' = the least squares estimates of the population regression coefficients for X1 and X2 , respectively , and e is a residual term . The sample multiple correlation coefficient , R , is an index of overall model fit ( in the sample ) , and the regression coefficients represent estimates of the effects of anxvariable on Y , holding all otherXvariab1e . s constant . For the case of interaction effects , a multiplicative term is formed , X1X2 , which is said to encompass the interaction effect , yielding a three term equation : 468 MULTIVARIATE BEHAVIORAL RESEARCH J . Jaccard , C . Wan , and R . Turrisi If an interaction effect is present , then the difference between the ^ ^ in l ~ quations 1 and 2 should be statistically significant ( barring a Type I1 error ) . ' The F test in such a hierarchical regression strategy yields the same substantive result as a t - test of the b3 coe : fficient for the multiplicative term . The interpretation of regression coefficients in Equations 1 ancl2is distinct . In Equation 1 , a regression coefficient estimates the effects of the independent variable on the delpendent variable , across the levels of the other independent variable . That is , bl ' reflects the trends of change in Ywith changes inX1 at various levels ofX2 , and b2 ' reflects the trends of change in Ywith changes inX2 at various levels ofX1 . In contrast , in Equation 2 , the regression coefficients fblrX1 andX2 reflect conditional relationships : b l reflects the influence of X l on Y when X2 equals zero , and b2 reflects the influence ofX2 on YwhenX1 equals zer0 . l The coefficient b3 represents an interaction effect in that it estimates the change in the slope of Y onX1 given a one unit change inX2 ( or , alternatively , the change in the slope of Yon X2 given a one unit change in XI , depending , Ion how one conceptualizes the interacti ~ n ) . ~ The distinctions between the regression coefficients in Equations 1 and 2 hold with equal vigor for the standard errors associated with the coefficients : The standard errors for regression coefficients in Equation 1 reflect ( estimates of sampling error across levels of the independent variables . In contrast , the standard errors for regression coefficients in Equation 2 are conditional and reflect sampling error at particular levels of the independent variables . The standard error for b l in Equation 2 estimates sampling error for the regression coefficient whenX2 equals zero . Similarly , the standard error for b2 ir ~ Equation 2 estimates sampling error for the regression coefficient whenXl equads zero . The standard error for ( 73 estimates sampling error for the effect ofXl on Ywith a one unit change inX2 ( or , alternatively , the effect of X2 on Ywith a one unit change in XI ) . Both the bl r ~ egression coefficient and its associated standar ~ d error will differ as a function ofX2 , given the presence of statistical interactioin . Although the derivation of b l and b2 at selected values ofX2 andX1 , respectively , appear in several regression texts , the derivation of the corresponding standard errors is less well known . Focusing on Equation 2 , if we factorXl from the terms blXl and b3XlX2 , we obtain the value of bl for any given value of X2 : Technically , the models implied by Equations 1 and 2 are both conditional in nature . For Equation 1 , the focus is on the conditional expectation of E ( YIXl , XZ ) , and for Bquation 2 , the focus is on the conditional expectation of E ( YIX1 , XZ = 0 ) . For pedagogical reasons , in the remainder of this article , we will interpret interaction effects as if X2 is the rnodera ~ tor variable . OCTOBER 1990 J . Jaccard , C . Wan , and R . Turrisi Similarly , the value of b2 at any given value of Xl is The corresponding standard error for b l at any given value of X2 is and the standard error for b2 at a particular value of X l is Consider the following numerical example . For 125 hypothetical subjects , the intention to use birth control ( Y ) was predicted from the subjects7 attitude toward birth control ( XI ) and the perceived normative pressures to use birth control ( X2 ) . TheXl andX2 variables were measured on scales that ranged from 1 to 5 , and the Y variable was measured on a scale from 0 to 30 . The X1 and X2 scores can be conceptualized in terms of a 5 x 5 factorial design , and the mean scores for each cell of the design are reported in Table 1 . There are equal n in each cell ( n = 5 ) . A small degree of within cell variability was introduced by allowing four of the five scores in a given cell to deviate one unit from the cell mean . Inspection of Table 1 shows an orderly , monotonic trend of changes in the slope of YonXl across the levels ofX2 . WhenX2 equals 1 , the slope of YonXl is 1 . 00 , and with every one unit that X2 increases , the slope increases by 1 . 00 units . The multiple R for the two term additive model is 0 . 901 and the regression equation is The multiple R for the three term model is 0 . 968 and the regression equation is The coefficient bl ( . 00 ) reflects the number of units that Yis predicted to change given a one unit increase inXl whenX2 equals zero and the coefficient b2 reflects the number of units that Y is predicted to change given a one unit change inX2 when XI equals zero . Note that the coefficient b3 captures the changes in the slope of YonXl with changes in X2 ( i . e . , for every one unit thatX2 changes , the slope of Y on XI changes 1 . 00 units ) . 470 MULTIVARIATE BEHAVIORAL RESEARCH J . Jaccard , C . Wan , i ~ nd R . Turrisi Table 1 Cell Means as a Function of Xl and X2 Now consideir the effects of an additive transformation on alne of the X variables . Suppose we transform the normative pressure scores ( X2 ) bly subtracting the mean of X2 from each score ( in this case , the mean is 3 . IDO ) . Such a transformation is traditionally referred to as centering . This transformation will leave unchanged the values of 62 and 63 , but will alter the values ( and standard errors ) of 61 and the intercept . The pre - transformation regression equation is and the post - transformation regression equation is The change in 61 in the two equations ( from 0 . 00 to 3 . 00 ) occurs because the conditional relationship of the influence ofXl on Yis being evaluated at adifferent zero point than was originally the case . In the pre - transformation analysis , the zero point was based on a scale that was 3 . 00 units higher than the transformed X2 score , whereas after centering , the zero point occurs at Ithe mean . In fact , if in the original analysis we wanted to evaluate the impact ofXl on Y at the mean of X2 , we could do so by substituting the mean value forX2 in Equation 9 and then , by algebraic manipulation , calculate the slope of Yon XI : Note that the observed slope equals the slope for Xl in Equation 10 where X2 has been centered . Application of Equation 5 to the above would yield the OCTOBER 1990 471 J . Jaccard , C . Wan , and R . Turrisi identical standard error for bl as Equation 10 . Centering has no effect on the substantive evaluation of the effect ofXl on Y at a given value ofm . It only changes the value ofX2 being evaluated ( because with centering , a zero o n m corresponds to the mean , whereas without centering , this is not necessarily the case ) . It can be seen that Cronbach ' s ( 1987 ) contention that b l and b2 using centered data " more closely relates to the simplex and Y effects " is somewhat misleading . With centered data , b l reflects the influence ofXl on Y at the mean X2 score . Similarly , b2 reflects the influence of X2 on Y at the mean X1 score . Cronbach ' s transformation is desirable because it does indeed reduce potential problems with multicollinearity when testing for the presence of statistical interaction . However , one must go beyond concern with just this criterion and also consider the implications of a transformation for the meaningful interpretation of the nature of an interaction effect , not just the presence of that effect . In this regard , some transformations are more useful than others . In traditional analysis of variance ( ANOVA ) paradigms , two methods of interaction decomposition are used , simple main effects analysis and interaction comparisons ( see Keppel , 1982 ) . There is a direct analog to both procedures in multiple regression with product terms . Simple main effects analysis focuses on the statistical significance of the effects of an independent variable ( XI ) on a dependent variable ( Y ) at selected levels of a moderator variable ( X2 ) . This corresponds to the specification of the slope of Y on X1 at theoretically or empirically meaningful values ofX2 , and a corresponding test of significance of those effects . To conduct the analysis , one needs to specify values ofX2 where one wishes to evaluate the effects of Xl on Y . In the absence of theory to guide this choice , a reasonable strategy is to evaluate the effects of X1 on Y at low , medium , and high values of X2 , where low might be defined as one standard deviation below the mean , medium as at the mean , and high as one standard deviation above the mean . This can be accomplished by centering data and then calculating the appropriate three term regression equation . Equations 3 - 6 are then used to define the relevant coefficients and their associated standard errors for low , medium and high scores . With centered data , a score of zero on the moderator variable corresponds to a medium value , a score of ( + l ) ( sd ) corresponds to a high value and a score of ( - l ) ( sd ) corresponds to a low value , where sd = the standard deviation of the moderator variable . A t - ratio to test the significance of b l at a given value ofX2 is formed by dividing a given coefficient by its standard error . Formulas for computing the standard errors from traditional computer output are provided in the appendix . In contrast to simple main effects analysis , the approach of interaction comparisons focuses on the formal comparison of slope differences as one moves from one value of the moderator variable to another . This information is readily available in b3 ( and its associated test of significance ) : Again using 472 MULTIVARIATE BEHAVIORAL RESEARCH J . Jaccard , C . Wan , and R . Turrisi measures because the variables are measured on a common metric ( i . e . , with a mean of zero and a standard deviation of 1 ) . However , strong arguments have been presented against the use of such measures in certain situations ( see , for example , Kim & Ferree , 1981 ) , especially in the context of causal analysis via structural equation models . The major problem with standardized coefficients is that they lack the property of causal invariance . Causal relationships typically are conceptualized in terms of change . A variablex is said to be a cause of Y if changes in X produce changes in Y . Because regression coefficients also focus on change , it is natural to study them in the context of causal models . If a causal relationship is identical in each of a set of groups , then the coefficients on which an analysis is performed should reflect this invariance . Situations arise where unstandardized regression coefficients properly reflect causal invariance , whereas standardized regression coefficients do not . Consider the following example . Assume in apopulation that Yis completely determined by X l andX2 , in accord with the following linear model : The value of the intercept is zero and there is no residual term . Assume also that the standard deviations ofXl andX2 are both 1 . 0 . Now suppose that three subsets of scores are randomly selected from the population . Table 2 presents scores that might be observed . The scores in each group represent atypical , but nevertheless , plausible random samples . Note that each Y score is completely specified by Equation 11 . If one computes unstandardized regression coefficients in each sub - group , the result will be the generating equation ( i . e . , Y = 0 . 60X1 + 0 . 80X2 ) . The coefficients are invariant . This is not true of the standardized coefficients . Specifically , the standardized coefficients for the three sub - groups would be 0 . 80 , 0 . 56 and 0 . 94 , respectively . These coefficients are not invariant across sub - groups ( or over replications of experiments ) and do not adequately reflect the generating causal function . Given an invariant causal structure , unstandardized regression coefficients are capable of detecting that invariance , whereas standardized regression coefficients are not . This is true even when the structural coefficients in the population are assumed to be standardized in form ( see Kim & Ferree , 1981 , for a more elaborate discussion of the issues involved ) . Thus , unstandardized coefficients are generally preferred to standardized ones and the DK transformation is limited , accordingly . In sum , we recommend the centering approach advocated by Cronbach ( 1987 ) , coupled with a formal evaluation ( i . e . , statistical test ) of b3 and an evaluation of b l at selected levels ofX2 . The latter is analogous to simple main effects analysis in ANOVA frameworks and b3 provides information corresponding to interaction comparison analysis . If standardized solutions are 474 MULTIVARIATE BEHAVIORAL RESEARCH J . Jaccard , C . Wan , and R . Turrisi Table 2 Example of Three Subsets from a Population Sub - group 1 Sub - group 2 - - Sub - group 3 Individual Y X1 X2 - - - Y X1 X2 - - - Y x1 X2 - - - sought , the DK transformation can be used in a similar fashion . However , this transformation is problematic where causal invariance is of conce : rn , , Reasons for Failures to Detect Interactic ~ ns Additive transformations do not affect the overall test of statistical interaction , but they do affect the interpretability of regression coefficienb . Similarly , contrary to Morris et al . ( 1986 ) , multicollinearity probably is not the major culprit inpsychologists ' failure to detect interaction effects in multiple regression analysis ( barring computational errors ) . To the extent that multicollinearity is a problem , the transformations suggested in this article can mitigate its effects . However , we believe that such transformations will not be sufficient to adequately detect interaction effects . Rather , there are other reasons as to why interactions in multiple regression may be elusive . First , is the problem of nleasuaement error . It is well known that unreliable measures can yield biased estimates of regression coefficients in multiple regression or structural equation analysis ( e . g . , Bohrnstedt & Carter , 1971 ) . If measures are fallible , intera ~ ction effects may be difficult to detect ( see Busemeyer & Jones , 1983 , for elaboration ) . Second , is the issue of the functional form of the interaction . Traditional interaction analysis in multiple regression relies on a product term , X1X2 . This assesses interaction of alimited type , namely the case where the linear relationship between Y and X1 changes as a linear function of X2 . Thus , Equatiion 2 will be diagnostic of statistical interaction ( as it is traditionally thought of by psychologists ) only when the changes in the slope of Yon X1 are orderly and monotonic ( linear ) as one progresses across the levels of X2 . Given curvilinear relationships within levels of X2 , non - monotonic changes in slopes ; , or changes in slopes that are large when progressing between levels within one range of X2 , OCTOBER 1990 475 J . Jaccard , C . Wan , and R . Turrisi but small when progressing between levels within a different range of X2 , the reliance on the traditional multiplicative term can , in many cases , be uninformative , if not misleading . Third , is the problem of levels of measurement . Although the evaluation of product terms is appropriate for interval level data , use of the approach on ordinal level data ( as if the data had interval characteristics ) may be problematic . Busemeyer and Jones ( 1983 ) present a convincing case for the biasing effects of departures from interval level data for the analysis of interaction effects . Finally , is the problem of statistical power . To the extent that analyses of interaction effects lack statistical power , the presence of the interaction is more likely to go undetected . Although it is difficult to specify the extent to which these four factors have contributed to a failure to observe expected interaction effects , they certainly represent issues that an investigator must consider when designing studies to explore interaction effects . We conclude with the following recommendations : 1 . To reduce potential problems with multicollinearity , thexvariables should be centered prior to the formation of product terms . 2 . Given a statistically significant interaction effect that is bi - linear in form , perspectives on the nature of the interaction can be gained by application of the regression analogs of simple main effects analysis and interaction comparisons . These approaches explicitly recognize the conditional nature of slopes and standard errors . The centering transformation is readily amenable to execution of these analyses . 3 . When concern is with causal analysis , unstandardized coefficients in interaction analysis are preferred to standardized coefficients because of their causal invariance . In this situation , the DK transformation is not recommended . 4 . In the final analysis , the formulation and evaluation of interaction terms in multiple regression are best guided by a strong theory . The theory may suggest the optimal transformation for purposes of interpretation , the values of the moderator variable within which to pursue simple effects analysis , and / or the form of the interaction that is to be modeled . Wherever possible , theory should be brought to bear on statistical analyses of interactions . 5 . Researchers should consider the potential effects of measurement error , levels of measurement , and statistical power when designing interaction studies using multiple regression . References Bohmstedt , G . , & Carter , T . M . ( 1971 ) . Robustness in regression analysis . In H . L . Costner ( Ed . ) , Sociological methodology . San Francisco : Jossey - Bass . Busemeyer , J . R . , & Jones , L . ( 1983 ) . Analysis of multiplicative combination rules when the causal variables are measured with error . Psychological Bulletin , 93 , 549562 . 476 MULTIVARIATE BEHAVIORAL RESEARCH J . Jaccard , C . Wan , and R . Turrisi Cohen , J . , & Cohen , P . ( 1983 ) . Applied multiple regressionlcorrelation analysis for the behavioral sciences . Hillsdale , NJ : Lawrence Erlbaum Associates . Cronbach , L . ( 1987 ) . Statistical tests for moderator variables : Flaws in analysis recently proposed . Psychological Bulletin , 102 , 414 - 417 . Dunlap , W . P . , & Kemery , E . ( 1987 ) . Failure to detect moderating effects : Is multicollinearity the problem ? Psychological Bulletin , 102 , 418 - 420 . Keppel , G . ( 1982 ) . Experimental design : A researcher ' s handbook . Prentice Hall , KIJ : Englewood Cliffs . Kim , J . , & Ferree , G . ( 1981 ) . Standardization in causal analysis . SociologicalMetho , ds , 10 , 22 - 43 . Morris , J . H . , Sherman , J . , & Mansfield , E . R . ( 1986 ) . Failures to detect moderating effects with ordinary least squares - moderated regression : Some reasons and a remedy . psychological Bulletin , 99 , 282 - 288 . Appendix This appendix presents computational procedures for calculating var ( bl ) , var ( b2 ) , var ( b3 ) , eov ( b1 , b3 ) , and cov ( b2 , b3 ) from standard computer output . Each regression coefficient in Equation 2 , bl , b2 , and b3 will have an associated standard error . The var ( b ) for a given coefficient is the square of tlhe standard error for that coefficient yielded by standard computer output . Let bl ' and b2 " represent the regression coefficients for X l and X2 in Equation 1 , re : spectively , and bl , b2 , and b3 represent the regression coefficients forXl , X2 , a . ndXlX2 in Equation 2 , respectively . Let MSE represent the mean squared errlor from the overall analysis ( usually reported in a summary table ) for Equation 2 and MSE " represent the corresponding term for Equation 1 . For analyzing the relationship between Y andX1 at levels ofX2 , the value ofX2 at which the minimal standard error occurs is minx2 = ( bl ' - bl ) / b3 and ( MSE / MSE1 ) var ( bll ) - var ( b1 ) - ( m i ~ X 2 ~ ) var ( b3 ) cov ( b1 , b3 ) = 2 min X2 The parallel formulas for analyzing the relationship between Y andim at levels of Xl are min XI = ( b2 ' - b2 ) / b3 OCTOBER 1990 J . Jaccard , C . Wan , and R . Turrisi and ( MSE / MSE1 ) var ( b2 ' ) - var ( b2 ) - ( minX12 ) var ( b3 ) cov ( b2 , b3 ) = 2 min X1 All of the terms required to execute Equations 5 and 6 are thus defined . Some computer packages have options that permit the output of the above variances and covariances ( e . g . , the COVB option in SPSS - X REGRESSION ) . MULTIVARIATE BEHAVIORAL RESEARCH