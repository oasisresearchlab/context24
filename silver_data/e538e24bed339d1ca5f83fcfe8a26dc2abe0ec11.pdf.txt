GestureMap : Supporting Visual Analytics and Quantitative Analysis of Motion Elicitation Data by Learning 2D Embeddings Duong Hai Dang Daniel Buschek duong . dang @ uni - bayreuth . de daniel . buschek @ uni - bayreuth . de Research Group HCI + AI , Department of Computer Science , University of Bayreuth Bayreuth , Germany 1 6 3 2 4 5 a b c Figure 1 : Our visual analytics tool , GestureMap : A configuration view to initialize and run the interactive clustering algorithm ( a ) , an overview of the entire application window ( b ) , a density plot projected onto the gesture map ( c ) . The numbers mark different components further explained in Section 5 . ABSTRACT This paper presents GestureMap , a visual analytics tool for gesture elicitation which directly visualises the space of gestures . Con - cretely , a Variational Autoencoder embeds gestures recorded as 3D skeletons on an interactive 2D map . GestureMap further integrates three computational capabilities to connect exploration to quan - titative measures : Leveraging DTW Barycenter Averaging ( DBA ) , we compute average gestures to 1 ) represent gesture groups at a glance ; 2 ) compute a new consensus measure ( variance around average gesture ) ; and 3 ) cluster gestures with k - means . We evaluate GestureMap and its concepts with eight experts and an in - depth analysis of published data . Our findings show how GestureMap facilitates exploring large datasets and helps researchers to gain a Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , topostonserversortoredistributetolists , requirespriorspecificpermission and / or a fee . Request permissions from permissions @ acm . org . CHI ’21 , May 8 – 13 , 2021 , Yokohama , Japan © 2021 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . ACM ISBN 978 - 1 - 4503 - 8096 - 6 / 21 / 05 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3411764 . 3445765 visual understanding of elicited gesture spaces . It further opens new directions , such as comparing elicitations across studies . We discuss implications for elicitation studies and research , and opportunities to extend our approach to additional tasks in gesture elicitation . CCS CONCEPTS • Human - centered computing → Visualization systems and tools ; HCI design and evaluation methods . KEYWORDS Gesture elicitation , dimensionality reduction , deep learning , visual analytics ACM Reference Format : Duong Hai Dang and Daniel Buschek . 2021 . GestureMap : Supporting Visual Analytics and Quantitative Analysis of Motion Elicitation Data by Learning 2D Embeddings . In CHI Conference on Human Factors in Computing Systems ( CHI ’21 ) , May 8 – 13 , 2021 , Yokohama , Japan . ACM , New York , NY , USA , 12 pages . https : / / doi . org / 10 . 1145 / 3411764 . 3445765 1 INTRODUCTION Designing effective interactions and user interfaces often involves exploring two potentially high - dimensional spaces [ 65 ] : 1 ) The a r X i v : 2103 . 00912v1 [ c s . H C ] 1 M a r 2021 CHI ’21 , May 8 – 13 , 2021 , Yokohama , Japan Dang and Buschek space of human behaviour ( e . g . comfortable motion ranges of arm and hand ) , and 2 ) the space of senseable input in a system or context ( e . g . tracking of up to X body joints in 3D ) . Although central to HCI , the field has developed few dedicated methods and tools for supporting the ( joint ) exploration of such user - sensor spaces ( cf . [ 65 ] ) . One successful method that has seen widespread use is the elicitation study paradigm [ 66 ] , which helps HCI researchers and practitioners to explore the space of possible and “intuitive” or “guessable” ( gesture ) commands : Participants are shown a “referent” ( often a system action , e . g . volume up ) and are asked to propose and perform a gesture they would use for it ( e . g . turn wrist right ) . This is repeated for several referents . Researchers then analyse these gesture proposals , compute mea - sures to identify common proposals ( e . g . [ 59 , 66 ] ) , and decide on a set of gestures to be used in an interactive system , typically com - posed of the gestures with high agreement among participants ( e . g . [ 56 , 67 ] ) . In this way , elicitation studies inform gestural interaction with user - driven exploration : Most studies focus on the human behaviour space and thus do not rely on a specific sensor ; they typically video - record participants for manual gesture analysis ( e . g . [ 14 , 28 ] ) . Some additionally employ a sensor in elicitation ( e . g . Leap [ 62 ] , Kinect [ 56 ] ) , thus also potentially considering the senseable space . While elicitation studies have become a widely used staple in the HCI toolbox , they still present challenges ( cf . [ 51 , 63 ] ) , including the need for manual data analysis . This limits elicitation studies , as well as the general endeavor of systematically exploring behaviour - sensor spaces in HCI , as characterised in the following paragraphs : • Workload : Watching videos to manually classify gestures ( e . g . [ 14 , 28 , 56 , 62 , 67 ] ) is tedious work [ 51 ] . It may thus also hinder the use of elicitation in user - centred processes that require repeating such work ( e . g . iteration ) . • Subjectivity : As critically pointed out [ 51 ] , manual interpre - tation at best requires further efforts ( e . g . multiple coders ) ; at worst , it leads to subjectively biased results . • Limited scale : Without quantitative analysis tools , large - scale elicitation remains scarce ( survey : mean = 25 partici - pants [ 63 ] ) . This stands in contrast to motivations for di - verse samples and for training recognisers on elicited ges - tures [ 14 , 25 , 49 , 63 ] . • Isolated results : The lack of quantitative data analysis meth - ods and tools hinders replication , reuse , extensions , and com - parisons across elicitations , even if the same sensor was used ( e . g . Kinect ) . Most work collects and analyses new data [ 63 ] , isolated from previously published datasets . These challenges motivate our work on new quantitative meth - ods and tools for analyzing elicitation data . Fittingly , recent related work highlighted the need and feasibility of more objective , compu - tational measures [ 59 ] , and called for further computational models and measures , based on a survey of 216 elicitation studies [ 63 ] . Addressing this , we extend the computational toolbox for ana - lyzing gesture elicitation data with these contributions : • GestureMap , a method and tool for visualizing and explor - ing motion data from elicitation studies on an interactive , learned 2D map , inspired by concepts from visual analytics . • New computational capabilities for gesture representation , consensus and clustering , based on average gestures com - puted with DTW Barycenter Averaging ( DBA ) [ 43 ] , to con - nect exploration to quantitative measures , extending the computational approach motivated in recent work [ 59 , 63 ] . • Insights from using GestureMap in a detailed case study on datasets from the literature , plus a qualitative expert evalua - tion with eight researchers . 2 RELATED WORK The analysis concepts introduced in this work are built on previous work spanning HCI , machine learning and visual analytics . This section briefly describes gesture elicitation studies , followed by an overview of tools that support researchers across different tasks involved in analyzing elicitation data . In particular , we outline existing analysis concepts for high - dimensional data . 2 . 1 Gesture Elicitation Studies The gesture elicitation paradigm was first introduced by Wobbrock et al . [ 66 ] to elicit users’ interaction preferences for new systems . This method was then specifically adapted to include gesture pro - posals to control surface tabletop computers [ 67 ] . In a subsequent study , Morris et al . [ 68 ] confirmed that new users do prefer the user - defined gesture set over the one created by experts . Since then , this method has become a standard tool for the design of gesture input mappings for new interactive systems , for example to control a swarm of robots [ 28 ] , smart - home appliances [ 26 , 56 ] , or AR / VR applications [ 44 ] . Central to gesture elicitation studies is an in - depth analysis of the proposed data to find common behavior . Researchers have therefore developed various measures to formalize the consensus among participants [ 56 , 57 , 61 , 62 , 67 ] . However , these measures rely on subjectively assessing the sim - ilarity of the observed gestures : They require researchers to group proposals into subgroups that they consider identical , which is usually done by manual annotation based on watching videos of the participants in the study [ 28 , 39 ] . Thus , while these measures set standards on how to compute consensus from gesture proposal , they cannot avoid subjectivity per se . To address this , Vatavu [ 59 ] has recently proposed a new , data - driven approach : It employs a distance measure as an objective basis for assessing consensus in elicitation studies . Our work builds on this idea , extends its data - driven perspective with a visual analytics tool , and introduces a new measure fitting this visualization . 2 . 2 Gesture Analysis Tools Several tools have been created for more effective and objective analyses . Video analysis has been the preferred evaluation method , but the annotation of individual video sequences can be time - consuming [ 51 ] . An efficient analysis becomes even more important as large - scale gesture data sets can be collected online , for exam - ple , through cloud elicitation tools [ 2 ] . Thus , researchers devised different ways to distribute the work among people [ 1 , 36 ] . While the concepts introduced in this paper also enable re - searchers to better annotate sequences , our focus lies in particular on the exploration of elicited gesture data . GestureMap : Supporting Visual Analytics and Quantitative Analysis of Motion Elicitation Data CHI ’21 , May 8 – 13 , 2021 , Yokohama , Japan Nebeling et al . [ 41 ] created a tool to analyze recordings created by a Kinect camera sensor . They included three visualizations . First , they used a 3D animation of a Kinect skeleton . Second , they pro - vided a visualization where only the moving joint is drawn on the canvas . The third visualization is similar to the second , but additionally employs a heat - map to emphasize the time domain . The most similar work to ours is GestureAnalyzer by Jang et al . [ 24 ] which also focuses on the analysis of gesture elicitation studies . To find behavioral patterns , it employs a variation of the small - multiples plot [ 52 ] and an interactive hierarchical clustering in - terface visualized in a tree layout . Their calculations and analyses are based on hand - engineered features . The gesture map which we propose in this work facilitates a richer exploration of the be - havior space using machine learned features for the gesture poses . It provides an overview of the gesture data and introduces a new continuous traceable 2D paths which represent gesture sequences . For further discussions on the comparison of these two systems we refer to section 8 . 2 . 2 . 2 . 3 Visualization of High Dimensional Data A key challenge in visual analytics is the effective visualization of high - dimensional data . This typically involves two steps : 1 ) Project - ing the data to 2D for display on a screen . 2 ) Suitably visualising the projected data , considering the analysts’ tasks and goals . While there exist many dimension reduction techniques [ 20 , 32 , 38 , 54 , 55 ] , we use a Variational Autoencoder [ 30 ] to reduce the dimensions of the raw sensor data . To visualize temporal data , a common repre - sentation is a line plot , horizon plot [ 16 ] , or a small - multiples plot [ 52 ] . However , these highly abstract visualizations may occlude the nature of the underlying data . For example , these plots may hide the structure of a 3D skeleton recording . We therefore combine an abstract 2D mapping with a grid of representative 3D skeletons to give analysts a visual overview of the proposed gestures . Also related to our work are tools to analyze and visualize machine learned representations of complex data : Deep learning models are capable of learning human - understandable features of high - dimensional data : For example , Kingma and Welling [ 30 ] and Lawrence [ 33 ] sample multiple points from the learned space and visualize them to demonstrate that the learned space is continu - ous and smooth , but without providing interaction functionalities . Smilkov et al . [ 48 ] filled this gap by providing a generic tool to visualize these embeddings . Some researchers created specific visualizations to facilitate in - terpretation of the axes of a ( 2D ) projection , to judge the variation of the data [ 27 , 60 ] or the relative importance of the data attributes along an axis [ 31 ] . Liu et al . [ 35 ] used a cartographic approach to compare and analyze learned embedding spaces . In this work , we adapt similar visualization concepts with the goal to create an interpretable gesture space . To the best of our knowledge , GestureMap is the first tool to use a latent variable model to analyze sensor - based motion data in the context of gesture elicitation studies . We combine interactive k - means clustering , automatic metric computation , a new visual - ization , and analysis concepts to provide an integrated platform . Challenge / Motivation in the Literature Visual Analytics Actions Feature in GestureMap Call for more computational support [ 51 , 61 , 63 ] Model Building , Model Usage Average Gesture Sequence ; Statistical Plot Overlay ; Variance Computation Multiple representation for gesture sequences [ 24 , 63 ] Visual Mapping 2D Path ; 3D Skeleton Comparisons across participants , sessions and trials [ 24 , 61 ] VisualizationManipulation Selective Filtering ; Gesture Highlighting Visual support for temporal dimension [ 24 , 41 ] Visual Mapping 2D / 3D Animation Unfamiliarity with Gesture Design Space [ 9 , 12 ] Visual Mapping , Model usage , Model - vis Interactive Gesture Map Processing large data sets [ 1 , 2 , 24 , 41 ] Model Building , Model Usage Interactive Clustering ; Cluster Reassignment Share and Save Analysis [ 24 , 36 , 41 ] N / A Export Analysis Table 1 : Main analysis components in GestureMap with the challenge and related work that motivated this feature and a reference to the supported action within the Knowledge Gen - eration Model for Visual Analytics [ 45 ] . 3 GESTUREMAP CONCEPT We introduce a structured analysis approach based on a learned 2D gesture map , as realised in GestureMap . We motivate the conceptual features via related work as summarized in Table 1 and elaborate on them in the following sections . 3 . 1 Feature Requirements and Overview The features in GestureMap were informed by close examination of the literature on gesture elicitation and related concepts and tools : We collected features 1 ) proposed in related work , 2 ) motivated in calls for further improvements , and 3 ) explicitly requested from future work . In addition , we included further ideas . Table 1 shows an overview of the relation to related work . The following paragraphs further introduce and motivate the features . 3 . 1 . 1 3D Skeleton View ( Figure 1b 2 ○ ) . Related tools [ 24 , 41 ] show a 3D skeleton view with animation . GestureMap also offers this , to afford easy examination of a recorded gesture . 3 . 1 . 2 2D Map View ( Figure 1b 1 ○ ) . GestureMap is fundamentally motivated by providing researchers with a visual overview of the elicited gesture space . Furthermore , some researchers indicated that participants may struggle to propose gestures , if they are unfamiliar with the gesture design space [ 9 , 12 , 46 ] . They therefore modified elicitation such that people could choose from a predefined list of gesture proposals . GestureMap addresses these needs as its 2D map shows observed gesture proposals and gives an idea of past behavior . While we focus on researchers as users of this map in this paper , it could also be shown to participants as we described in Section 8 . 3 . CHI ’21 , May 8 – 13 , 2021 , Yokohama , Japan Dang and Buschek 3 . 1 . 3 2D Map Overlays ( Figure 1b 1 ○ , Figure 1c ) . Prior work has extensively used scatter plots to analyze machine learned represen - tations [ 35 , 48 ] . Our map view affords different plots on top of it , such as : • Scatter plots ( point = body posture ; Figure 1b 1 ○ ) • Drawing paths ( path = gesture ; Figure 4 ) • Densities ( e . g . where in the space are postures and gestures located ? Figure 1c ) 3 . 1 . 4 Linked Views of Postures . Villarreal - Narvaez et al . [ 63 ] called for future work to include multiple representations of gestures . GestureMap realises this by linking the 2D map and the 3D skeleton . Concretely , the 3D skeleton view updates while the user moves the cursor over the 2D map to present the posture at that point in the gesture space . 3 . 1 . 5 Linked Animations of Gestures . Complementary to the fea - ture for postures , GestureMap accounts for the temporal nature of gesture data [ 24 , 41 ] by offering linked animations of gesture paths ( point moving on the path ) and 3D skeletons ( skeleton moving ) . 3 . 1 . 6 Gesture Clustering . As larger data sets are expected in the fu - ture [ 1 , 24 , 41 ] , we also provide an interactive clustering method to reduce manual workload for identifying similar gesture ( sub ) groups . 3 . 1 . 7 Sharing Results . Motivatedbysuchinterestsinrelated work [ 24 , 36 , 41 ] , we include an export functionality to easily share analyses with other researchers . 3 . 2 The Learned 2D Gesture Map Here , we describe the map concept in more detail . 3 . 2 . 1 Core Visualization Concept . Following a cartographic ap - proach [ 47 ] , and in line with 2D projections in visual analytics ( e . g . [ 27 , 64 ] ) , we use a map metaphor to visually guide analysts through the elicited gesture space . This gesture map is a 2D plot with a grid of representative body poses shown as small human skeletons . These “pose landmarks” give an overview of the poses in the corresponding rectangular map region ( Figure 1b 1 ○ ) . The map itself is continuous , that is , each 2D point represents a pose . Thus , since gestures are sequences of poses , they are paths con - necting multiple points on the map . In this way , the gesture map combines a line plot’s simplicity with the structural expressiveness of a small - multiples visualization [ 24 ] . 3 . 2 . 2 Learning a Gesture Map . The two dimensions of the map do not have a direct predefined meaning yet emerge from elicited data . Formally , let the set of all 𝑁 individual gesture poses in the dataset be denoted by G = { 𝑔 𝑖 | 𝑖 = [ 1 , . . . , 𝑁 ] } , 𝑔 𝑖 ∈ R 𝐷 where 𝐷 is the dimensionality of the raw sensor data ( in our case D = 20 ) . A gesture sequence which consists of 𝑇 gesture poses can be viewed as an ordered tuple of size 𝑇 i . e . , g = ( 𝑔 1 , . . . , 𝑔 𝑇 ) . ( 1 ) To reduce the dimensions of the raw sensor data , we use an encoder 𝑓 𝐸𝑛𝑐𝑜𝑑𝑒𝑟 : R 𝐷 → R 2 to embed every gesture pose 𝑔 𝑖 ∈ G into a latent space code 𝑧 𝑖 ∈ R 2 . These latent codes represent a pose using only two learned features . ( 2 ) The raw and high - dimensional gesture sequence g is then embedded as a two - dimensional path z = ( 𝑧 1 , . . . , 𝑧 𝑇 ) in the latent space . ( 3 ) To create the grid of gesture poses in the background , we compute an evenly spaced grid M ∈ R 𝑚 × 𝑚 × 2 of 𝑚 rows and columns over a visible region in the latent space . For example , if the embedded gesture poses ( latent codes ) range from - 4 to 4 in both x and y dimension , we would linearly sample a number of points within this square region . ( 4 ) Using the decoder model we can decode arbitrary 2D map points into a full pose , i . e . f 𝐷𝑒𝑐𝑜𝑑𝑒𝑟 : R 2 → R 𝐷 In this paper we use a Variational Autoencoder ( VAE ) . In general , layout and quality of the space ( e . g . smoothness ) , and of pose decod - ing , depend on the model , and we reflect on this in our discussion . 3 . 3 Map Interaction Concepts Here we describe how users can interact with the map . 3 . 3 . 1 Pan and Zoom . The map supports pan and zoom and ac - cordingly recomputes the grid of landmarks ( small skeletons ) . This feature helps to adjust the viewport to support exploration of data - dense areas , and deal with the fact that landmark representations are discrete indicators for the continuous space . 3 . 3 . 2 Examining Poses . Scatter or density plots can be projected onto the map ( e . g . Figure 1b 1 ○ and Figure 1c ) . Using “details on demand” , users can hover over points to see the corresponding pose skeleton ( Figure1b 2 ○ ) , and referent , participant and trial number in the detail view ( Figure1b 6 ○ ) . The scatterplot may help researchers to detect outlier body poses , while the density plot reveals regions with recorded data . 3 . 3 . 3 Examining Gestures . For further inspection , one or more gestures can be selected ( e . g . Figure 4 ) from a referent’s list of gesture proposals ( Figure 1 3 ○ ) . This allows researchers to view details on - demand e . g . to reduce the risk of information overload . 3 . 3 . 4 Examining Unseen Poses . A fundamentally new capability of GestureMap is that unseen poses or gestures ( i . e . not proposed by participants ) can be simulated by decoding arbitrary 2D points in the learned space . In our prototype users can thus hover over the map to visualize 3D skeletons for any cursor location . Analysts can examine if empty regions are anatomically not feasible ( cf . 8 . 3 . 3 ) or if people did not show such behaviour . This might be useful to adjust elicitation setup / instructions , for example to prompt people to also cover a previously empty part of the map . 3 . 4 Analysis Concepts Using the Gesture Map Exploratory analysis seeks to uncover structural patterns in the dataset , identify anomalies , and single - out outliers [ 53 ] . We thus conceptualized the gesture map to enable researchers to seamlessly cycle between the detection of new observations and the assess - ment of supporting evidence . The analysis concept is structured further by differentiating between global observations and local observations . The former targets questions that may span multiple referents or the entire dataset , while the latter focuses on a few gestures to identify specific behavioral idiosyncrasies . 3 . 4 . 1 Global Observations . The first of many analysis steps often involves developing an overview of the data to understand its un - derlying properties : Researchers here often use statistical plots to summarize the data and to identify broad patterns . GestureMap : Supporting Visual Analytics and Quantitative Analysis of Motion Elicitation Data CHI ’21 , May 8 – 13 , 2021 , Yokohama , Japan Developing an Overview of the Gesture Space . GestureMap sup - ports this as well : For instance , Figure 3 depicts a scatter plot pro - jected on the gesture map . Scatter points on top of the pose grid enable researchers to quickly identify which general poses were observed in the data . Each scatter point corresponds to a pose from the dataset , whereas empty patches in the gesture map may indi - cate behavior that has not been observed ( e . g . poses / gestures not proposed by participants during elicitation ) . Spotting Clusters and Outliers . Scatter points may visually cluster near gesture poses that are characteristic for a particular referent . These clusters can help researchers to form a mental model of the main poses that are characteristic for a group of gesture sequences . It might also be interesting to analyze outlier behavior which can be detected by examining scatter points that lie far from these clusters . Comparing Referents and Regions . Additionally , color codes fa - cilitate the comparison of behavior across different referents . For example , it might be interesting to identify which referents share behavior and which are distinctive . Regions in the gesture map that contain multiple embedded data points from different referents may indicate that this region encodes shared generic behavior . Judging Densities and Overlap of Referents . Scatter plots may con - tain too much detail and clutter the visualization . Density plots then offer a visualization of the most frequent gesture poses . Researchers can use it to detect overlapping or distinctive behavior across dif - ferent referents . For example , these observations can inform re - searchers interested in building gesture recognizers in judging the difficulty of separating gestures for the various referents . 3 . 4 . 2 Local Observations . The key local observation in elicitation data is to examine individual gesture proposals . GestureMap also supports such analysis , as outlined here : After the initial data exploration it is often necessary to find con - crete example for detected patterns . For example , in the elicitation context , we might be interested in comparing the behavior across different participants and experimental trials . The gesture map can serve as a common visual basis for such investigations : By projecting multiple gestures onto the map , re - searchers can evaluate each participant’s behavior individually . The trajectory of the embedded gesture paths can inform them on specific behavioral characteristics . For example , a participant’s movement can be subtle , in which case the embedded gesture path is simple in shape and typically spans a small region in the ges - ture map . In contrast , a complex gesture may be represented as an intricate path that may meander across the map . Thus , bycomparingmultipleembeddedgesturepathsresearchers can visually assess gestures as similar or not . Considering research interests in the elicitation context from the literature , for example , this might support researchers to examine if a participant can re - member and repeat the same gesture proposal across multiple trials [ 40 ] , or if behavior was influenced by a priming effect [ 6 ] . 4 CONSENSUS AND CLUSTERING WITH DBA We introduce the concept of an average gesture sequence as a new computational capability in the context of gesture elicitation . This has three practical values , which complement our tool : ( 1 ) Descriptive : The average gesture can serve as a single , visual proxy for a group of gestures , which opens up new visual - ization opportunities ( e . g . showing and comparing referents as average paths on our gesture map ) . ( 2 ) Evaluative : It enables a new measure of consensus / variability among gesture proposals for a referent . This measure aligns well with other statistical notions of variability : Consensus is assessed via the variance of actual gestures around the average gesture . ( 3 ) Explorative : The average gesture enables clustering methods that require averaging ( e . g . here : k - means ) , supporting the automated detection of groups of gestures in a dataset ( e . g . in “open elicitation” without referents , cf . [ 63 ] ) . We next describe the technical approach in more detail . 4 . 1 Computing an Average Gesture with DBA We employ the DTW Barycenter Averaging ( DBA ) algorithm by Petitjean et al . [ 43 ] to compute an average gesture : Intuitively , this algorithm first aligns an initial sequence with every sequence in the set of gesture proposals , before computing a centroid ( barycenter ) for each aligned coordinate . For further technical details we refer the reader to the related work [ 43 ] . 4 . 2 Consensus as Variation Around Barycenter Vatavu [ 59 ] were the first to propose a data - driven consensus mea - sure that does not rely on human judgement of gesture similarity . To achieve this , they employed Dynamic Time Warping ( DTW ) distance computations to define a consensus measure : They con - sidered two gesture sequences g 𝐴 and g 𝐵 as similar if the DTW distance was below a threshold Δ 𝐷𝑇𝑊 ( g 𝐴 , g 𝐵 ) ≤ 𝜏 . To determine consensus for a referent they calculated the pairwise distances across all gesture proposals for this referent . Finally , to report a measure independent of the threshold value 𝜏 , they used a logistic regres - sion model to determine the consensus for a range of normalized threshold values and reported the growth rate as an indication of the overall consensus . This work motivates us to further explore data - driven measures of consensus : We follow a similar approach , but instead of regress - ing on the DTW distance values , and relying on pairwise compar - isons , we directly compute an average sequence from all gesture proposals in a referent group , using DBA . We then measure the DTW distance of every gesture proposal 𝑔 ∗ for a referent 𝑅 to the computed average gesture ( i . e . barycenter ) g 𝐷𝐵𝐴 for 𝑅 . Finally , we report the variance of these DTW distances as a measure of consensus . Formally , this is noted as : VAR 𝑅 = (cid:205) G 𝑅 g ∗ ( Δ 𝐷𝑇𝑊 ( g ∗ , g 𝐷𝐵𝐴 ) ) 2 | G 𝑅 | ( 1 ) G 𝑅 denotes the set of all gestures elicited for referent 𝑅 . Intuitively , for example , a high value VAR 𝑅 may inform an analyst that referent 𝑅 contains quite varied gesture proposals ( i . e . low consensus ) . The gesture variance integrates well with GestureMap ’s visu - alization concept because this already displays the involved aver - age gestures as visual elements . Moreover , this approach yields a one - number summary without a logistic regression model on top . Overall , we see this approach as an additional measure , not CHI ’21 , May 8 – 13 , 2021 , Yokohama , Japan Dang and Buschek a replacement of others : As a flexible tool , GestureMap can be ex - tended to additionally display further such measures ( e . g . the one by Vatavu [ 59 ] ) to support researchers with the analysis . 4 . 3 Clustering Gestures with DBA & K - Means Being able to compute an average gesture enables the use of clus - tering methods that require average computations . Here , we use k - means in particular . The idea of clustering gesture elicitation data is motivated by two aspects : ( 1 ) Exploration : For example , in “open elicitation” [ 63 ] or settings where referents are not predefined , such as in the work by Williamson and Murray - Smith [ 65 ] , a clustering may proved a valuable reference point to identify novel behavior . ( 2 ) Annotation : Clustering may also be used to help kickstart ( manual ) annotation in cases where explicit groupings of proposals are desired ( e . g . for agreement measures [ 67 ] ) . Considering the literature , Jang et al . [ 24 ] used an interactive hierarchical clustering approach with complete - linkage . In contrast , we experimented with the k - means algorithm , using DBA to calcu - late the centroids . We motivate this choice by interpretability of the resulting centroids , versus the abstract representations in the hierar - chical approach : In particular , the centroids ( i . e . average / barycenter gestures ) are more compatible with our 2D gesture map , on which they could be displayed as paths . In contrast , a hierarchical treemap does not directly fit the map metaphor well . 5 IMPLEMENTATION OF GESTUREMAP We implemented GestureMap as an analysis tool that integrates the described concepts of both the interactive gesture map ( Section 3 ) and the DBA - based computations ( Section 4 ) . Here we describe the key implementation aspects . 5 . 1 User Interface and Functionality Figure 1 shows the UI ; the following sections refer to the numbers in the figure . Overall , we implemented all UI views and interactions conceptually described in Section 3 . 5 . 1 . 1 Gesture Map Figure 1b 1 ○ . Researchers can zoom , pan , and hover over the gesture map , and overlay a scatter plot or a density plot ( e . g . Figure 1c ) to explore individual or multiple gesture poses . 5 . 1 . 2 Experiment View Figure 1b 3 ○ . This view lists all referents and gesture proposals in a compact way as numbers for quick reference and selection . When hovering over an element , the corre - sponding gesture path is shown on the map for a moment . 5 . 1 . 3 3D Skeleton View Figure 1b 2 ○ , Figure 1b 4 ○ . This view either shows the raw skeleton recording or a reconstructed skeleton . If researchers animate a gesture , it is simultaneously animated in this view and on the map . The progress of the animation can be controlled via a play / pause button and slider . 5 . 1 . 4 Statistics View Figure 1b 5 ○ . This view shows different met - rics , namely variances around the average gesture sequence per selected referent ( Section 4 . 2 ) , the distributions of DTW distances of proposals to their average gesture sequence , and nearest neighbor distances for a selected gesture . 5 . 1 . 5 Cluster View Figure 1b 6 ○ . This dialog is unfolded with a button in Figure 1b 3 ○ and lets users interactively cluster gesture proposals for a referent . Centroids can be animated and once the clusters have been computed , users can toggle all gesture proposals that were assigned to a centroid . 5 . 2 Architecture We used a server - client architecture . The frontend and backend modules communicate through a REST API through which the data is transmitted as a JSON formatted string . The frontend was im - plemented with NodeJS [ 18 ] and React [ 15 ] . For plotting , we use the PlotlyJs library [ 22 ] . For the backend we used the Flask frame - work [ 10 ] and Pandas [ 11 ] to handle the data transformations and queries . We cached expensive computations such as the computed average sequences and distances matrices on MongoDB [ 21 ] to . PyTorch [ 42 ] was used to develop the embedding model . 6 EXPERIMENTS Ledo et al . [ 34 ] identified four evaluation strategies for toolkit con - tributions . We follow their perspective to evaluate GestureMap , combining two such strategies : First , here we follow the Demon - stration strategy and provide a detailed analysis of examples on elicitation data from related work . Second , Section 7 follows the Usage strategy and reports on a user study with HCI researchers . 6 . 1 Datasets We consider four existing datasets : One explicit gesture elicitiation study by Vatavu [ 59 ] , plus three datasets collected for gesture recog - nition systems [ 3 , 7 , 17 ] . We first focus on the dataset by Vatavu [ 59 ] that consists of 1312 full body gestures elicited from children aged 3 - 6 , recorded with a Kinect sensor . For preprocessing , we followed the original authors [ 59 ] but left out the resampling step . 6 . 2 Model Training We used a Variational Autoencoder ( VAE ) [ 13 ] to embed the data as a 2D gesture map . The VAE here serves as an exemplar of a model with both powerful ( non - linear ) encoding and decoding capabilities . We reflect on other possible choices in our discussion . We trained the VAE on the poses ( frames ) of the mentioned dataset [ 59 ] which has 60 dimensions ( 20 body joints × 𝑥 , 𝑦 , 𝑧 ) . We adapted the architecture from Spurr et al . [ 50 ] ( i . e . 4 hidden layers for both encoder and decoder ) and used a 2D bottleneck layer . In line with Fu et al . [ 19 ] , we used a weight term to modulate the mix of KL - loss and reconstruction loss in early training . We trained for 2000 epochs with Adam [ 29 ] ( lr = 3 𝑒 − 5 ) . We experimented with different numbers of hidden neurons ℎ : Overall , reconstruction loss decreases for larger models , regularized by the KL - loss , leading to diminishing returns and a decision for ℎ = 512 here . For full details , we provide the training scripts and model comparisons on the project website . 6 . 3 Global Observations Here demonstrate the use of GestureMap in a walkthrough of an explorative analysis : Examining the gesture map , the center ( Fig - ure 2C ) reveals start / end poses ( standing upright , arms at rest ) . We further see , for example , sitting ( Figure 2B ) , clapping ( Figure 2D ) , GestureMap : Supporting Visual Analytics and Quantitative Analysis of Motion Elicitation Data CHI ’21 , May 8 – 13 , 2021 , Yokohama , Japan Figure 2 : Gesture map for the dataset by Vatavu [ 59 ] . Pose landmarks represent poses in that part of the learned ges - ture space . Marked areas are referenced in Section 6 . 3 . and raising an arm ( Figure 2A ) . Thus , the map reveals the space of poses elicited by Vatavu [ 59 ] at a glance : For example , their refer - ents included crouch , draw a flower , draw a circle , draw a square , applaud or raise your hands , which all match the poses in our map . Using overlays in GestureMap , we can identify similarity and differences between gestures across referents : For example , Figure 3 ( left ) shows that crouch , draw circle , draw flower , draw square share common behavior ; their scatter points largely overlap in the region that encodes “raised arm” behavior . In contrast , for instance , gestures proposed for crouch cover a different region ( pink ) . The variance plot in GestureMap ( Figure 3 right ) indicates that proposals for crouch and draw flower vary more than for draw circle and draw square . Potentially , for the children the basic shapes afforded less flexible interpretation than a flower or crouching . We defined a consensus measure on this variability ( Section 4 . 2 ) : Comparing this variability between all referents , our results largely agree with Vatavu [ 59 ] : In particular , applaud , fly like a bird and hands up show high consensus while climb ladder , crouch , turn around have low consensus . 6 . 4 Local Observations Proposals for crouch form two main clusters ( pink points in Figure 3 left ) , one in the region of starting poses , another in sitting / crouching regions . Thus , GestureMap visually reveals that people interpreted crouch in different ways , matching the high variance ( Figure 3 d r a w fl o w e r c r ou c h d r a w c i r c l e d r a w s qua r e Variance around Barycenter Scatterplot 50 0 40 30 20 10 Figure 3 : Left : Scatter plots show gesture poses for four refer - ents elicitet by Vatavu [ 59 ] ( crouch , draw circle , draw flower , draw square ) . Right : Variances of the gestures’ DTW dis - tances to their average gesture sequence . Figure 4 : Gesture proposals for throw ball from four peo - ple ( different colors ) . Trials per person are not discernible ( same color ) , yet the colored paths distinctly cover different regions , revealing high consistency per person . right ) . Examining the map locally , in combination with gesture animations , reveals that some children sat on the floor , some on their heels , some crawled on hands / knees , and others stood with a stooped body posture . Some additionally jumped at the end of their gesture proposals to get back onto their feet . As another such example , for throw ball , behavior can be catego - rized into four clusters : Most children used their right hand , others used two hands , and some kicked the ball . Only a few used the left hand . As Figure 4 shows , the children mostly stuck to their inter - pretation across multiple repeated trials for that referent , revealing consistency ( cf . [ 5 ] ) . This is an example for using GestureMap ’s spatial visualisation of gestures as paths for visual comparison via shape . CHI ’21 , May 8 – 13 , 2021 , Yokohama , Japan Dang and Buschek 6 . 5 Interactive Clustering For a typical elicitation study , such as this one by Vatavu [ 59 ] , it is reasonable to expect clusters induced by the referents . Therefore , to demonstrate our proposed clustering analysis we removed the referent labels and then evaluated if k - means finds clusters that match the original referents . Concretely , we ran the clustering with 15 sequences chosen randomly . We then inspected the mix of original referents present in the gestures assigned to each found cluster . We repeated this ten times and made these observations : • Our k - means clustering identified those referents with high agreement ( e . g . hands up , crouch , applaud , fly like a bird ) . • Gestures for referents with much common behavior ap - peared as one cluster ( e . g . draw circle , square , flower ) . Note that this is not necessarily “wrong” , since a behaviour “draw something” would also have been a plausible referent . • The resting pose was detected as a separate cluster . • Other referents were ( clearly ) present only in some of the clustering repetitions . Overall , this indicates the potential of automated clustering , for example , when examining data from open elicitation with no given referents . We return to ideas for improvements in our discussion . In another experiment , we applied clustering to look for patterns within a referent : As mentioned , referents such as throw ball and crouch contained distinct patterns , revealed on the map . Indeed , running k - means revealed some of them : For example , for throw ball k - means also detected throwing with the right hand vs using both hands . In contrast , it did not separately find left hand and kicking , presumably since those were proposed only a few times . 6 . 6 Comparison Between Datasets Other researchers noted that elicitation findings are spread across multiple venues and need to be consolidated [ 63 ] . GestureMap sup - ports this as it offers a platform to visualize and analyze multiple studies . We demonstrate this by creating a gesture map using four datasets [ 3 , 8 , 17 , 59 ] . To motivate a concrete example , citetJain2016 showed that ob - servers can distinguish behavior of children and adults . Figure 5 shows all 20 proposals for jump from the data by Aloba et al . [ 3 ] , next to the children’s proposals from Vatavu [ 59 ] . The gesture paths visit roughly similar main parts of the gesture space , yet the chil - dren do not find consensus . Our variance measure also reflects this ( Aloba - adults 23 . 70 , children 44 . 89 ; Vatavu - children 54 . 94 ) . As a second example , we compared behavior diversity across datasets . Without knowing anything about the referents , Figure 6 already reveals that one dataset [ 3 ] ( blue ) covers a larger region than the other [ 59 ] ( orange ) . Thus , it seems to contain a more diverse set of body poses . Indeed , this observation can be explained by the longer referent list ( 58 referents in [ 3 ] vs 15 in [ 59 ] ) . 7 USER STUDY To further evaluate GestureMap , we recruited eight HCI researchers ( 7 male , 1 female ) from three universities via e - mail for remote think - alound and interview sessions . Six were familiar with ges - ture elicitation studies , the other two were interested in analysing gesture sensor data . Five were familiar with machine learning . Aloba - Adults Aloba - Children Vatavu - Children Figure 5 : Gesture paths for adults and children for “jumping” referents from two studies [ 3 , 23 ] . Figure 6 : Combined gesture space from [ 3 , 8 , 17 , 59 ] . The den - sity plots projected on this gesture map refer to [ 3 ] ( blue ) and [ 59 ] ( orange ) . 7 . 1 Procedure The interviews lasted 80 minutes and were conducted via screen - sharing using Skype / Zoom , with GestureMap hosted online such that people could use it on their own computer . We again used the dataset by Vatavu [ 59 ] . With people’s consent we recorded the interviews . We encouraged them to think out loud and occasionally asked questions to better understand actions . We took notes and compiled a report from this material . Given the exploratory nature of the interactions and the diversity in people’s approaches this was done in an inductive approach , leading to the themes in Section 7 . 2 . The interviews had four parts : 1 ) We introduced GestureMap ( 20 minutes ) , with a concept presentation , a guided walk through the tool and UI , and opportunities for questions . 2 ) In an exploratory , manual analysis task people were prompted to use GestureMap to identify groups of behaviors in the gesture proposals for two refer - ents . In real use , researchers would conduct such analyses to better understand elicited data . 3 ) In a more confirmatory , automatic analy - sis task we asked them to build on their gained insights to initialize the clustering algorithm and refine the automatic clustering results . In real use , researchers might export this result , for example , for a report , calculations of agreement , etc . 4 ) The session concluded with a semi - structured interview of at least ten minutes . Here , we GestureMap : Supporting Visual Analytics and Quantitative Analysis of Motion Elicitation Data CHI ’21 , May 8 – 13 , 2021 , Yokohama , Japan inquired into what people liked / disliked about GestureMap , and asked for ideas for improvements and additional features . 7 . 2 Findings 7 . 2 . 1 Initial Use . Upon first use , most people immediately ani - mated a few gestures , saying that this was the most natural and familiar way to view the data Since the map visualization was unfamiliar to them , some had initial difficulties to understand the distinction of single poses ( points ) and entire gestures ( paths ) . These people found the animation particularly important : Seeing the 3D skeleton and the 2D path animated in sync highlighted that a ges - ture was a path on the map and thus helped them to get familiar with the map concept . Summarising their initial experience , one person said : “Although , the learning curve [ . . . ] is steep , once you understand the core concepts , this tool offers a great overview of the entire behavior that is captured in the dataset . ” 7 . 2 . 2 Statistical Plot Overlays . We asked the researchers to analyze the proposals for crouch and throw ball . Throughout the interview we noticed that all participants preferred the scatter plot over the density plot . When asked why they keep returning to the scatter plot , they said that it provided more detail and that density can also be estimated from scatter points . They also said that points were visually closer to the data ( point = pose ) . 7 . 2 . 3 Details of the Gesture Map View . When study participants paused their exploration for a longer period , we inquired why that was the case . Some people noted that they struggled to find a specific pose on the map . They suggested to increase the visibility of the poses by showing fewer and larger landmarks . Another researcher felt that the map should show more detail so it would be easier to judge differences and transitions of poses . Together , this feedback motivates a changeable grid size ( our zoom was implemented to always keep an 11 × 11 grid ) . Some found similar poses encoded in different map regions and noted that these should ideally reside in one area . This is an artefact of dimensionality reduction , as we discuss further in Section 8 . 2 7 . 2 . 4 Exploration Strategies . When we asked the participants what the main aspect was that they used to determine interesting be - havioral patterns , we observed diverse analysis strategies , but we broadly highlight two main ones : 1 ) Shape driven analysis : Some started by skimming through gestures to get an overview of their different path shapes on the map . They stopped to examine gestures in more detail that differed largely from the shapes seen so far . In a sense , they searched for outlier behavior based on the path shapes . These participants noted that the 2D gesture path visualization offers a quick way to spot irregular behavior and that their analysis becomes an active search versus passively watching every gesture individually . 2 ) Position driven analysis : In contrast , other participants focused entirely on the scatter points as template poses . Using expectations about possible behavior for a gesture proposal ( e . g . left vs right hand throwing ) , they examined scatter points in those map regions that based on the landmark skeletons encoded related poses . 7 . 2 . 5 Manually Forming Clusters . Regardless of their initial analy - sis strategy , when asked which feature they would use to group the gestures , people agreed on the path shapes as primary discerning feature ( strategy 1 ) . For the crouch referent , everyone distinguished two to three groups of behaviors . For throw ball , everyone found at least three ( left / right / both handed throwing ) . Some also found the kicking behavior as described in Section 6 . Overall , the researchers felt comfortable with grouping the proposals based on the path shapes . However , there were some complex paths ( e . g . crossing over many poses on the map ) that people were unable to assign to a group . One person suggested to create an outlier group for these . 7 . 2 . 6 Interactive Clustering . We asked people to use the interactive clustering tool based on their observations in the first task . Next , they were asked to initialize the clustering algorithm us - ing their knowledge from the previous task . Now , all participants specifically searched for individual gesture proposals as templates ( strategy 2 ) and used those to initialize the algorithm . However , the resulting computed centroids often deviated from people’s expectations , and thus did not immediately make sense to them . One user noted that one still has to inspect all gesture proposals in order to choose suitable initialisations for the k - means algorithm . On the positive side , the researchers liked the refinement step , where they could reassign proposals to another cluster . These reassignments , however , were not yet considered when rerunning the clustering algorithm in the current implementation . Overall , after being asked to give a final verdict over the inter - active clustering feature , all deemed it important . However , they noted that it should be more accurate and manually refined as - signments need to be respected when rerunning the clustering algorithm , thus enabling iterative , interactive use . Technically , this can be readily implemented by initialising k - means with the current ( refined ) assignments . 8 DISCUSSION 8 . 1 Extending the Gesture Elicitation Toolbox GestureMap builds on and extends functionalities of previous tools for gesture elicitation : It combines 1 ) gesture modeling and visu - alization , 2 ) automatic computation of elicitation metrics , and 3 ) interactive clustering to provide an integrated analysis platform . Seeing this and related work as a “toolbox” , researchers may now consider various options : For example , AGATE 2 . 0 [ 61 ] is a highly specialized tool to compute agreement , which assumes a given labeled dataset . GestureMap could be used to label data and export it for analysis in tools like this . Alternatively , Ali et al . [ 1 ] proposed a crowd platform for anno - tation , yet without computational support for the workers , such as alternative gesture representations or similarity measures . Such support as shown in GestureMap could be combined with a crowd approach in the future . GestureMap is already implemented as a web - based tool , rendering it flexible and open to such integration . Looking ahead , new cloud elicitation tools [ 2 , 36 ] yield large datasets . GestureMap ’s concepts support handling large data , visu - ally summarised and explored via our map view . Finally , the “toolbox” in the literature includes several formal - ized agreement measures [ 56 , 67 ] . These could be used also with our interactive clustering , for example , by plugging in the cluster cardinalities instead of subjective gesture group counts . CHI ’21 , May 8 – 13 , 2021 , Yokohama , Japan Dang and Buschek 8 . 2 Reflection on Model & Clustering Choices Here , we highlight model and clustering aspects to consider . 8 . 2 . 1 Smoothness of the Latent Space . A smooth latent space facil - itates suitable visualization by reducing “jumps” in gesture paths . These occur due to recording issues ( e . g . sensor occlusion in some frames ) or when subsequent poses are embedded far apart in the 2D space . While some models address this ( e . g . we used a VAE instead of AE ) , there is no universal “natural” 2D layout of body poses and some artifacts are likely to exist for most models and datasets . Be - sides technical model improvements , visualization concepts could be explored to address this as well ( e . g . visually mark “jumps” along the gesture path ) . 8 . 2 . 2 Cluster Approaches . A difficulty with k - means is setting the number of clusters . As an example strategy , to detect the subgroup behavior for the throw ball referent , we quickly skimmed through the gestures using the map and visually identified rough patterns . We then chose 𝑘 correspondingly . We chose k - means , because it readily integrates with the gesture map and the " variance around mean gesture " that we introduced in section 4 . 2 . Color coding the cluster results can be done quickly . Jang et al . [ 24 ] proposed to use interactive hierarchical clustering . Integrating such a tree - like layout into the gesture map adds complexity and might be material for future endeavours . We can imagine that average gestures calcu - lated with the DBA - algorithm can be used to visualize the non - leaf nodes in the hierarchical tree . In addition , interactive hierarchical clustering would eliminate the need for choosing the number of clusters beforehand . 8 . 2 . 3 Feature Representation . Hand - engineered features [ 4 , 24 , 58 ] may help with the interpretation , however , they may be specific to a sensor and interaction setup . As an exploratory tool , GestureMap’s learned space is applicable to new and changing setups , without de - veloping hand - engineered features first . Furthermore , our learned representation supports gesture simulation useful to examine re - gions of the behavior space that were not covered by participants . 8 . 3 Opportunities for Research & Applications Here we outline further ideas enabled or supported by GestureMap . 8 . 3 . 1 Supporting Meta - Analysis and Consolidation . GestureMap empowers researchers to compare data across studies ( cf . Section 6 ) . As a community , we could consolidate our findings in a meta - map of many studies , as a sensor data - driven complement to literature surveys [ 63 ] . For instance , such a map might reveal which gestures and poses are most common or intensely studied . Separate maps could also compare gesture spaces for different contexts , devices , etc . , for example , to better understand the influences of such factors . 8 . 3 . 2 Enabling Map - based Gesture Authoring . GestureMap could be extended to define new gestures : For example , users could draw a gesture as a path on the map . Since the underlying latent vari - able model can simulate new behavior ( decoding ) , such a drawn path implicitly defines a pose sequence that could be exported as a template - based gesture recogniser . As an alternative to drawing , users could demonstrate the gesture in front of the sensor , with a “cursor” moving on the map live . Users could also select recorded gestures on the map , labelled manually or with help from our clus - tering tool , to train a classifier . Such a recognizer then also could be used in other tools to support sensor feed annotation ( e . g . [ 41 ] ) . 8 . 3 . 3 Enabling Analysis of Unseen Behavior . So far , elicitation has focused on observed gestures , yet it might also be relevant to ex - amine why behavior was not observed . GestureMap enables this : Researchers can explore map areas without data , which may reveal unlikely behavior , or indicate issues with interaction ( e . g . anatomi - cally difficult or tiring gestures ) or the sensor ( e . g . gestures leading to self - occlusion of body parts ) . In this way , GestureMap supports the diagnosis of challenges and limitations in the joint user - sensor space of an interactive system ( cf . [ 65 ] ) . 8 . 3 . 4 Supporting Live Exploration and Monitoring . GestureMap could be extended to more than post - hoc analysis : For example , we could embed live sensor data and continuously update the under - lying mode . This live embedding provides a monitoring tool , for example , for participants to see their currently performed gesture ( e . g . shown as a “cursor” / point on the map ) , possibly to nudge them towards exploring new regions of the behavior space ( cf . [ 65 ] ) . One could also predefine a gesture path to monitor live performances and to judge deviation from this “template” , possibly to learn / teach a movement sequence . Related , gesture sets are mostly presented as drawings and videos today [ 37 ] . Instead , GestureMap could be used to show gestures to users , allowing them to reenact and explore them with live monitoring via the map . 9 CONCLUSION As our key contribution , we presented a set of visualization and analysis concepts for gesture elicitation data and a tool that im - plements them : GestureMap is the first visual analytics tool for gesture elicitation which directly visualises the space of gestures , using a learned 2D embedding . It further leverages the computation of average gestures to enable researchers to 1 ) represent gesture groups with one gesture ; 2 ) assess consensus as variance around this average gesture ; and 3 ) cluster gestures automatically . Expert users especially liked the visual expressiveness of Ges - tureMap , as it quickly summarizes the underlying dataset . The extensibility of GestureMap further encourages future work to em - ploy machine learning as a tool for analysis of human behavior . With this work , we contribute to the vision of more widespread use of applicable computational methods in HCI , also to support more extensive and cost - efficient large - scale , data - driven HCI work . Given the proliferation of crowd platforms to collect large datasets , we expect computational methods and visual analytics as proposed here to become indispensable tools for many future HCI studies . GestureMap and further materials are available on the project website : https : / / osf . io / dzn5g / ACKNOWLEDGMENTS This project is funded by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation ( bidt ) . REFERENCES [ 1 ] Abdullah X . Ali , Meredith Ringel Morris , and Jacob O . Wobbrock . 2018 . Crowd - sourcing Similarity Judgments for Agreement Analysis in End - User Elicitation GestureMap : Supporting Visual Analytics and Quantitative Analysis of Motion Elicitation Data CHI ’21 , May 8 – 13 , 2021 , Yokohama , Japan Studies . In Proceedings of the 31st Annual ACM Symposium on User Interface Soft - wareandTechnology ( Berlin , Germany ) ( UIST’18 ) . AssociationforComputingMa - chinery , New York , NY , USA , 177 – 188 . https : / / doi . org / 10 . 1145 / 3242587 . 3242621 [ 2 ] AbdullahX . Ali , MeredithRingelMorris , andJacobO . Wobbrock . 2019 . Crowdlicit : A System for Conducting Distributed End - User Elicitation and Identification Studies . In Proceedingsofthe2019CHIConferenceonHumanFactorsinComputing Systems ( Glasgow , Scotland Uk ) ( CHI ’19 ) . Association for Computing Machinery , New York , NY , USA , 1 – 12 . https : / / doi . org / 10 . 1145 / 3290605 . 3300485 [ 3 ] Aishat Aloba , Gianne Flores , Julia Woodward , Alex Shaw , Amanda Castonguay , Isabella Cuba , Yuzhu Dong , Eakta Jain , and Lisa Anthony . 2018 . Kinder - Gator : The UF Kinect Database of Child and Adult Motion . In Proceedings of the 39th Annual European Association for Computer Graphics Conference : Short Papers ( Delft , The Netherlands ) ( EG ) . Eurographics Association , Goslar , DEU , 13 – 16 . [ 4 ] Aishat Aloba , Julia Woodward , and Lisa Anthony . 2020 . FilterJoint : Toward an Understanding of Whole - Body Gesture Articulation . In Proceedings of the 2020 International Conference on Multimodal Interaction ( Virtual Event , Netherlands ) ( ICMI ’20 ) . Association for Computing Machinery , New York , NY , USA , 213 – 221 . https : / / doi . org / 10 . 1145 / 3382507 . 3418822 [ 5 ] Lisa Anthony , Radu - Daniel Vatavu , and Jacob O . Wobbrock . 2013 . Understand - ing the Consistency of Users’ Pen and Finger Stroke Gesture Articulation . In Proceedings of Graphics Interface 2013 ( Regina , Sascatchewan , Canada ) ( GI ’13 ) . Canadian Information Processing Society , CAN , 87 – 94 . [ 6 ] Francesco Cafaro , Leilah Lyons , and Alissa N . Antle . 2018 . Framed Guessability : Improving the Discoverability of Gestures and Body Movements for Full - Body Interaction . In Proceedings of the 2018 CHI Conference on Human Factors in Com - puting Systems ( Montreal QC , Canada ) ( CHI ’18 ) . Association for Computing Machinery , New York , NY , USA , 1 – 12 . https : / / doi . org / 10 . 1145 / 3173574 . 3174167 [ 7 ] Chen Chen , Roozbeh Jafari , and Nasser Kehtarnavaz . 2015 . UTD - MHAD : A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor . In 2015 IEEE International conference on image processing ( ICIP ) . IEEE , 168 – 172 . [ 8 ] Chen Chen , Roozbeh Jafari , and Nasser Kehtarnavaz . 2015 . UTD - MHAD : A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor . In 2015 IEEE International conference on image processing ( ICIP ) . IEEE , 168 – 172 . [ 9 ] Zhen Chen , Xiaochi Ma , Zeya Peng , Ying Zhou , Mengge Yao , Zheng Ma , Ci Wang , Zaifeng Gao , and Mowei Shen . 2018 . User - defined gestures for gestural interaction : extending from hands to other body parts . International Journal of Human – Computer Interaction 34 , 3 ( 2018 ) , 238 – 250 . [ 10 ] Community . 2020 . Flask . Accessed : 2020 - 09 - 15 . [ 11 ] Community . 2020 . Pandas . https : / / pandas . pydata . org / Accessed : 2020 - 09 - 15 . [ 12 ] Nem Khan Dim , Chaklam Silpasuwanchai , Sayan Sarcar , and Xiangshi Ren . 2016 . Designing Mid - Air TV Gestures for Blind People Using User - and Choice - Based Elicitation Approaches . In Proceedings of the 2016 ACM Conference on Designing Interactive Systems ( Brisbane , QLD , Australia ) ( DIS ’16 ) . Association for Computing Machinery , New York , NY , USA , 204 – 214 . https : / / doi . org / 10 . 1145 / 2901790 . 2901834 [ 13 ] Carl Doersch . 2016 . Tutorial on Variational Autoencoders . arXiv : 1606 . 05908 [ stat . ML ] [ 14 ] Jane L . E , Ilene L . E , James A . Landay , and Jessica R . Cauchard . 2017 . Drone & Wo : Cultural Influences on Human - Drone Interaction Techniques . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems ( Denver , Colorado , USA ) ( CHI ’17 ) . Association for Computing Machinery , New York , NY , USA , 6794 – 6799 . https : / / doi . org / 10 . 1145 / 3025453 . 3025755 [ 15 ] Facebook . 2020 . React . https : / / reactjs . org / Accessed : 2020 - 09 - 15 . [ 16 ] Stephen Few and Perceptual Edge . 2008 . What ordinary people need most from information visualization today . ( 2008 ) . [ 17 ] Simon Fothergill , Helena Mentis , Pushmeet Kohli , and Sebastian Nowozin . 2012 . InstructingPeopleforTrainingGesturalInteractiveSystems . In Proceedingsofthe SIGCHI Conference on Human Factors in Computing Systems ( Austin , Texas , USA ) ( CHI ’12 ) . Association for Computing Machinery , New York , NY , USA , 1737 – 1746 . https : / / doi . org / 10 . 1145 / 2207676 . 2208303 [ 18 ] OpenJS Foundation . 2020 . NodeJs . https : / / nodejs . org / en / Accessed : 2020 - 09 - 15 . [ 19 ] Hao Fu , Chunyuan Li , Xiaodong Liu , Jianfeng Gao , Asli Celikyilmaz , and Lawrence Carin . 2019 . Cyclical Annealing Schedule : A Simple Approach to Mitigating KL Vanishing . arXiv : 1903 . 10145 [ cs . LG ] [ 20 ] Ian J . Goodfellow , Jean Pouget - Abadie , Mehdi Mirza , Bing Xu , David Warde - Farley , Sherjil Ozair , Aaron Courville , and Yoshua Bengio . 2014 . Generative Adversarial Networks . arXiv : 1406 . 2661 [ stat . ML ] [ 21 ] MongoDB Inc . 2020 . MongoDB . https : / / www . mongodb . com / Accessed : 2020 - 09 - 15 . [ 22 ] Plotly Technologies Inc . 2020 . Plotly . https : / / plotly . com / Accessed : 2020 - 09 - 15 . [ 23 ] Eakta Jain , Lisa Anthony , Aishat Aloba , Amanda Castonguay , Isabella Cuba , Alex Shaw , and Julia Woodward . 2016 . Is the Motion of a Child Perceivably Different from the Motion of an Adult ? ACM Trans . Appl . Percept . 13 , 4 , Article 22 ( July 2016 ) , 17 pages . https : / / doi . org / 10 . 1145 / 2947616 [ 24 ] Sujin Jang , Niklas Elmqvist , and Karthik Ramani . 2014 . GestureAnalyzer : Visual Analytics for Pattern Analysis of Mid - Air Hand Gestures . In Proceedings of the 2nd ACM Symposium on Spatial User Interaction ( Honolulu , Hawaii , USA ) ( SUI ’14 ) . Association for Computing Machinery , New York , NY , USA , 30 – 39 . https : / / doi . org / 10 . 1145 / 2659766 . 2659772 [ 25 ] Shaun K . Kane , Jacob O . Wobbrock , and Richard E . Ladner . 2011 . Usable Gestures for Blind People : Understanding Preference and Performance . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Vancouver , BC , Canada ) ( CHI ’11 ) . Association for Computing Machinery , New York , NY , USA , 413 – 422 . https : / / doi . org / 10 . 1145 / 1978942 . 1979001 [ 26 ] Christine Kühnel , Tilo Westermann , Fabian Hemmert , Sven Kratz , Alexander Müller , and Sebastian Möller . 2011 . I’m home : Defining and evaluating a gesture set for smart - home control . International Journal of Human - Computer Studies 69 , 11 ( 2011 ) , 693 – 704 . https : / / doi . org / 10 . 1016 / j . ijhcs . 2011 . 04 . 005 [ 27 ] H . Kim , J . Choo , H . Park , and A . Endert . 2016 . InterAxis : Steering Scatterplot Axes via Observation - Level Interaction . IEEE Transactions on Visualization and Computer Graphics 22 , 1 ( 2016 ) , 131 – 140 . [ 28 ] Lawrence H . Kim , Daniel S . Drew , Veronika Domova , and Sean Follmer . 2020 . User - Defined Swarm Robot Control . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems ( Honolulu , HI , USA ) ( CHI ’20 ) . Association for Computing Machinery , New York , NY , USA , 1 – 13 . https : / / doi . org / 10 . 1145 / 3313831 . 3376814 [ 29 ] Diederik P . Kingma and Jimmy Ba . 2014 . Adam : A Method for Stochastic Opti - mization . arXiv : 1412 . 6980 [ cs . LG ] [ 30 ] Diederik P Kingma and Max Welling . 2013 . Auto - Encoding Variational Bayes . arXiv : 1312 . 6114 [ stat . ML ] [ 31 ] Bum Chul Kwon , Hannah Kim , Emily Wall , Jaegul Choo , Haesun Park , and Alex Endert . 2016 . Axisketcher : Interactive nonlinear axis mapping of visualizations through user drawings . IEEE transactions on visualization and computer graphics 23 , 1 ( 2016 ) , 221 – 230 . [ 32 ] NeilD . Lawrence . 2003 . GaussianProcessLatentVariableModelsforVisualisation of High Dimensional Data . In Proceedings of the 16th International Conference on Neural Information Processing Systems ( Whistler , British Columbia , Canada ) ( NIPS’03 ) . MIT Press , Cambridge , MA , USA , 329 – 336 . [ 33 ] Neil D Lawrence . 2004 . Gaussian process latent variable models for visualisation of high dimensional data . In Advances in neural information processing systems . 329 – 336 . [ 34 ] David Ledo , Steven Houben , Jo Vermeulen , Nicolai Marquardt , Lora Oehlberg , and Saul Greenberg . 2018 . Evaluation Strategies for HCI Toolkit Research . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI ’18 ) . Association for Computing Machinery , New York , NY , USA , 1 – 17 . https : / / doi . org / 10 . 1145 / 3173574 . 3173610 [ 35 ] YangLiu , EuniceJun , QishengLi , andJeffreyHeer . 2019 . Latentspacecartography : Visual analysis of vector space embeddings . In Computer Graphics Forum , Vol . 38 . Wiley Online Library , 67 – 78 . [ 36 ] Nathan Magrofuoco and Jean Vanderdonckt . 2019 . Gelicit : A Cloud Platform for Distributed Gesture Elicitation Studies . Proc . ACM Hum . - Comput . Interact . 3 , EICS , Article 6 ( June 2019 ) , 41 pages . https : / / doi . org / 10 . 1145 / 3331148 [ 37 ] Erin McAweeney , Haihua Zhang , and Michael Nebeling . 2018 . User - Driven Design Principles for Gesture Representations . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI ’18 ) . Association for Computing Machinery , New York , NY , USA , 1 – 13 . https : / / doi . org / 10 . 1145 / 3173574 . 3174121 [ 38 ] Leland McInnes , John Healy , and James Melville . 2018 . UMAP : Uni - form Manifold Approximation and Projection for Dimension Reduction . arXiv : 1802 . 03426 [ stat . ML ] [ 39 ] Meredith Ringel Morris . 2012 . Web on the Wall : Insights from a Multimodal Interaction Elicitation Study . In Proceedings of the 2012 ACM International Con - ference on Interactive Tabletops and Surfaces ( Cambridge , Massachusetts , USA ) ( ITS ’12 ) . Association for Computing Machinery , New York , NY , USA , 95 – 104 . https : / / doi . org / 10 . 1145 / 2396636 . 2396651 [ 40 ] MiguelA . Nacenta , YemlihaKamber , YizhouQiang , andPerOlaKristensson . 2013 . Memorability of Pre - Designed and User - Defined Gesture Sets . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Paris , France ) ( CHI ’13 ) . Association for Computing Machinery , New York , NY , USA , 1099 – 1108 . https : / / doi . org / 10 . 1145 / 2470654 . 2466142 [ 41 ] Michael Nebeling , David Ott , and Moira C . Norrie . 2015 . Kinect Analysis : A System for Recording , Analysing and Sharing Multimodal Interaction Elicita - tion Studies . In Proceedings of the 7th ACM SIGCHI Symposium on Engineering Interactive Computing Systems ( Duisburg , Germany ) ( EICS ’15 ) . Association for Computing Machinery , New York , NY , USA , 142 – 151 . https : / / doi . org / 10 . 1145 / 2774225 . 2774846 [ 42 ] Adam Paszke , Sam Gross , Soumith Chintala , Gregory Chanan , Edward Yang , Zachary DeVito , Zeming Lin , Alban Desmaison , Luca Antiga , and Adam Lerer . 2017 . AutomaticdifferentiationinPyTorch . ( 2017 ) . https : / / pytorch . org / Accessed : 2020 - 09 - 15 . [ 43 ] FrançoisPetitjean , AlainKetterlin , andPierreGançarski . 2011 . Aglobalaveraging method for dynamic time warping , with applications to clustering . Pattern Recognition 44 , 3 ( 2011 ) , 678 – 693 . CHI ’21 , May 8 – 13 , 2021 , Yokohama , Japan Dang and Buschek [ 44 ] Thammathip Piumsomboon , Adrian Clark , Mark Billinghurst , and Andy Cock - burn . 2013 . User - Defined Gestures for Augmented Reality . In CHI ’13 Ex - tended Abstracts on Human Factors in Computing Systems ( Paris , France ) ( CHI EA ’13 ) . Association for Computing Machinery , New York , NY , USA , 955 – 960 . https : / / doi . org / 10 . 1145 / 2468356 . 2468527 [ 45 ] Dominik Sacha , Andreas Stoffel , Florian Stoffel , Bum Chul Kwon , Geoffrey Ellis , and Daniel A Keim . 2014 . Knowledge generation model for visual analytics . IEEE transactions on visualization and computer graphics 20 , 12 ( 2014 ) , 1604 – 1613 . [ 46 ] Chaklam Silpasuwanchai and Xiangshi Ren . 2015 . Designing concurrent full - body gestures for intense gameplay . International Journal of Human - Computer Studies 80 ( 2015 ) , 1 – 13 . [ 47 ] A . Skupin . 2002 . A Cartographic Approach to Visualizing Conference Abstracts . IEEE Comput . Graph . Appl . 22 , 1 ( Jan . 2002 ) , 50 – 58 . https : / / doi . org / 10 . 1109 / 38 . 974518 [ 48 ] DanielSmilkov , NikhilThorat , CharlesNicholson , EmilyReif , FernandaB . Viégas , and Martin Wattenberg . 2016 . Embedding Projector : Interactive Visualization and Interpretation of Embeddings . arXiv : 1611 . 05469 [ stat . ML ] [ 49 ] Tiffanie R . Smith and Juan E . Gilbert . 2018 . Dancing to Design : A Gesture Elicita - tion Study . In Proceedings of the 17th ACM Conference on Interaction Design and Children ( Trondheim , Norway ) ( IDC ’18 ) . Association for Computing Machinery , New York , NY , USA , 638 – 643 . https : / / doi . org / 10 . 1145 / 3202185 . 3210790 [ 50 ] Adrian Spurr , Jie Song , Seonwook Park , and Otmar Hilliges . 2018 . Cross - modal Deep Variational Hand Pose Estimation . arXiv : 1803 . 11404 [ cs . CV ] [ 51 ] Theophanis Tsandilas . 2018 . Fallacies of Agreement : A Critical Review of Con - sensus Assessment Methods for Gesture Elicitation . ACM Trans . Comput . - Hum . Interact . 25 , 3 , Article 18 ( June 2018 ) , 49 pages . https : / / doi . org / 10 . 1145 / 3182168 [ 52 ] Edward R . Tufte . 1986 . The Visual Display of Quantitative Information . Graphics Press , USA . [ 53 ] John W Tukey . 1977 . Exploratory data analysis . Vol . 2 . Reading , MA . [ 54 ] Laurens Van Der Maaten , Eric Postma , and Jaap Van den Herik . 2009 . Dimen - sionality reduction : a comparative . ( 2009 ) . [ 55 ] Marc M Van Hulle . 2012 . Self - organizing Maps . [ 56 ] Radu - Daniel Vatavu . 2012 . User - Defined Gestures for Free - Hand TV Control . In Proceedings of the 10th European Conference on Interactive TV and Video ( Berlin , Germany ) ( EuroITV ’12 ) . Association for Computing Machinery , New York , NY , USA , 45 – 48 . https : / / doi . org / 10 . 1145 / 2325616 . 2325626 [ 57 ] Radu - Daniel Vatavu . 2013 . The Impact of Motion Dimensionality and Bit Cardi - nality on the Design of 3D Gesture Recognizers . Int . J . Hum . - Comput . Stud . 71 , 4 ( April 2013 ) , 387 – 409 . https : / / doi . org / 10 . 1016 / j . ijhcs . 2012 . 11 . 005 [ 58 ] Radu - Daniel Vatavu . 2017 . Beyond Features for Recognition : Human - Readable Measures to Understand Users’ Whole - Body Gesture Performance . International Journal of Human – Computer Interaction 33 , 9 ( 2017 ) , 713 – 730 . https : / / doi . org / 10 . 1080 / 10447318 . 2017 . 1278897 arXiv : https : / / doi . org / 10 . 1080 / 10447318 . 2017 . 1278897 [ 59 ] Radu - Daniel Vatavu . 2019 . The Dissimilarity - Consensus Approach to Agree - ment Analysis in Gesture Elicitation Studies . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( Glasgow , Scotland Uk ) ( CHI ’19 ) . Association for Computing Machinery , New York , NY , USA , 1 – 13 . https : / / doi . org / 10 . 1145 / 3290605 . 3300454 [ 60 ] Radu - Daniel Vatavu , Lisa Anthony , and Jacob O . Wobbrock . 2014 . Gesture Heatmaps : Understanding Gesture Performance with Colorful Visualizations . In Proceedings of the 16th International Conference on Multimodal Interaction ( Istanbul , Turkey ) ( ICMI ’14 ) . Association for Computing Machinery , New York , NY , USA , 172 – 179 . https : / / doi . org / 10 . 1145 / 2663204 . 2663256 [ 61 ] Radu - Daniel Vatavu and Jacob O . Wobbrock . 2016 . Between - Subjects Elicitation Studies : FormalizationandToolSupport . In Proceedingsofthe2016CHIConference on Human Factors in Computing Systems ( San Jose , California , USA ) ( CHI ’16 ) . Association for Computing Machinery , New York , NY , USA , 3390 – 3402 . https : / / doi . org / 10 . 1145 / 2858036 . 2858228 [ 62 ] Radu - Daniel Vatavu and Ionut - Alexandru Zaiti . 2014 . Leap Gestures for TV : Insights from an Elicitation Study . In Proceedings of the ACM International Con - ference on Interactive Experiences for TV and Online Video ( Newcastle Upon Tyne , United Kingdom ) ( TVX ’14 ) . Association for Computing Machinery , New York , NY , USA , 131 – 138 . https : / / doi . org / 10 . 1145 / 2602299 . 2602316 [ 63 ] Santiago Villarreal - Narvaez , Jean Vanderdonckt , Radu - Daniel Vatavu , and Ja - cob O . Wobbrock . 2020 . A Systematic Review of Gesture Elicitation Studies : What Can We Learn from 216 Studies ? . In Proceedings of the 2020 ACM De - signing Interactive Systems Conference ( Eindhoven , Netherlands ) ( DIS ’20 ) . As - sociation for Computing Machinery , New York , NY , USA , 855 – 872 . https : / / doi . org / 10 . 1145 / 3357236 . 3395511 [ 64 ] John Wenskovitch , Michelle Dowling , and Chris North . 2020 . With Respect to What ? Simultaneous Interaction with Dimension Reduction and Clustering Projections . In Proceedings of the 25th International Conference on Intelligent User Interfaces ( Cagliari , Italy ) ( IUI ’20 ) . Association for Computing Machinery , New York , NY , USA , 177 – 188 . https : / / doi . org / 10 . 1145 / 3377325 . 3377516 [ 65 ] John Williamson and Roderick Murray - Smith . 2012 . Rewarding the Original : Explorations in Joint User - Sensor Motion Spaces . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Austin , Texas , USA ) ( CHI ’12 ) . Association for Computing Machinery , New York , NY , USA , 1717 – 1726 . https : / / doi . org / 10 . 1145 / 2207676 . 2208301 [ 66 ] Jacob O . Wobbrock , Htet Htet Aung , Brandon Rothrock , and Brad A . Myers . 2005 . Maximizing the Guessability of Symbolic Input . In CHI ’05 Extended Ab - stracts on Human Factors in Computing Systems ( Portland , OR , USA ) ( CHI EA ’05 ) . Association for Computing Machinery , New York , NY , USA , 1869 – 1872 . https : / / doi . org / 10 . 1145 / 1056808 . 1057043 [ 67 ] Jacob O . Wobbrock , Meredith Ringel Morris , and Andrew D . Wilson . 2009 . User - Defined Gestures for Surface Computing . In Proceedings of the SIGCHI Conference onHumanFactorsinComputingSystems ( Boston , MA , USA ) ( CHI’09 ) . Association for Computing Machinery , New York , NY , USA , 1083 – 1092 . https : / / doi . org / 10 . 1145 / 1518701 . 1518866 [ 68 ] Jacob O . Wobbrock , Meredith Ringel Morris , and Andrew D . Wilson . 2009 . User - Defined Gestures for Surface Computing . In Proceedings of the SIGCHI Conference onHumanFactorsinComputingSystems ( Boston , MA , USA ) ( CHI’09 ) . Association for Computing Machinery , New York , NY , USA , 1083 – 1092 . https : / / doi . org / 10 . 1145 / 1518701 . 1518866