VisiBlends : A Flexible Workflow for Visual Blends Lydia B . Chilton Columbia University chilton @ cs . columbia . edu Savvas Petridis Columbia University sdp2137 @ columbia . edu Maneesh Agrawala Stanford University maneesh @ cs . stanford . edu ABSTRACT Visual blends are an advanced graphic design technique to draw attention to a message . They combine two objects in a way that is novel and useful in conveying a message sym - bolically . This paper presents VisiBlends , a flexible workflow for creating visual blends that follows the iterative design process . We introduce a design pattern for blending sym - bols based on principles of human visual object recognition . Our workflow decomposes the process into both computa - tional techniques and human microtasks . It allows users to collaboratively generate visual blends with steps involving brainstorming , synthesis , and iteration . An evaluation of the workflow shows that decentralized groups can generate blends in independent microtasks , co - located groups can col - laboratively make visual blends for their own messages , and VisiBlends improves novices’ ability to make visual blends . CCS CONCEPTS • Human - centered computing → Interactive systems and tools ; Collaborative and social computing systems and tools ; KEYWORDS Design ; collaboration ; workflow ; visual blends ACM Reference Format : Lydia B . Chilton , Savvas Petridis , and Maneesh Agrawala . 2019 . Vis - iBlends : A Flexible Workflow for Visual Blends . In CHI Conference on Human Factors in Computing Systems Proceedings ( CHI 2019 ) , May 4 – 9 , 2019 , Glasgow , Scotland UK . ACM , New York , NY , USA , 14 pages . https : / / doi . org / 10 . 1145 / 3290605 . 3300402 1 INTRODUCTION Visual blends are an advanced graphic design technique used in journalism , advertising and public service announcements Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland UK © 2019 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . ACM ISBN 978 - 1 - 4503 - 5970 - 2 / 19 / 05 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3290605 . 3300402 Figure 1 : An illustration of the VisiBlends workflow to cre - ate a visual blend for the concepts Starbucks and summer . to draw users’ attention to a message . They blend two ob - jects together in a way that is novel and useful in conveying a message symbolically . For example , in Figure 1 , the Star - bucks logo is blended with a sun to convey that “Starbucks is here for summer . ” Visual blends are widely considered to be creative [ 5 ] , and many of the top image search results for “creative ads” are visual blends . Novices have never heard of visual blends and thus do not consider them as a way to attract attention to their news and announcements . Even when introduced to the concept of visual blends and seeing professional examples like those in Figure 2 , it is still difficult to make one because there are two opposing goals : combining two objects into one while ensuring both objects are still recognizable . There are no obvious characteristics that visual blends share . They all use different objects and combine them in unique ways . It seems that every one requires creative inspiration and that there is no exact procedure . To enable novices to make visual blends , we decompose the process of constructing them . Although there is no obvi - ous surface - level structure to visual blends , there is a common abstract structure to many visual blends : they combine two CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 1 objects with a similar shape . For example , in Figure 1 the Starbucks logo and the sun are both circles and in Figure 2 , the Tabasco Bottle and fire extinguisher are both cylinders . In the design literature , abstract structure used to guide the design process is referred to as a design pattern [ 4 ] . We present a system that decomposes the process of mak - ing visual blends into a workflow that uses a combination of human microtasks and computational techniques . The workflow follows the iterative design process with steps in - volving brainstorming , synthesis , and iteration . The input is two concepts such as Starbucks and summer . First , people brainstorm related concepts and find images of the concepts to generate many possible symbols . Next , we automatically match images and synthesize them into blends based on the design pattern . A human then evaluates each blend . If there are no blends , iteration is needed to refocus the brainstorm - ing to find more symbols . In iteration , new constraints often emerge . The workflow is flexible in that it allows users to move between tasks and see others’ work in order to adapt . The VisiBlends system makes it possible for groups of peo - ple to collaborate towards creating a visual blend to convey their messages of news , events , and public service announce - ments . This paper makes the following contributions : • Introducing and defining the problem of visual blends - a creative visual technique for conveying messages . • Decomposing the process of creating visual blends into microtasks that follow the iterative design process including brainstorming , synthesis , and iteration . • Introducing a design pattern which specifies the ab - stract structure for synthesizing images into visual blends . • VisiBlends : a system for users to collaboratively gen - erate visual blends for their own messages . • Three studies showing decentralized groups can gen - erate blends in independent microtasks , groups can collaboratively make visual blends for their own mes - sages , and VisiBlends improves novices’ ability to make visual blends . The discussion addresses lessons learned about decompos - ing the design process and how to generalize this approach to other creative design problems . 2 RELATED WORK Linguistics and Visual Metaphors Visual metaphors are a media technique studied in psychol - ogy [ 12 ] and linguistics because they are related to the field of pragmatics - the study of how context and implicature are used to create meaning . Charles Forceville , a prominent researcher in visual metaphors , proposes a theory of the me - chanics underlying visual metaphors [ 13 ] . He presents case studies of ads similar to those in Figure 2 and theorizes that when viewers encounter a visual metaphor , they recognize an object in an image , but also notice something is odd about it . This deviates from the viewers’ expectations and causes them to seek a meaning . Often the meaning is not entirely clear from the image alone . Supporting text on the image is needed to indicate that the image is an advertisement for a product . Then the user understands that one of the objects is meant to be interpreted literally ( like the Tabasco bottle ) and one of the objects is meant to be interpreted figuratively ( like the fire extinguisher , implying a meaning about the literal object ( “Tabasco is hot . ” ) . In his studies of the impact of visual metaphors on viewers across cultures , he identifies three types of visual metaphors : Similes where objects are “visually separate” , Hybrids where objects are “fused together” and Contextual Metaphors where one of the objects is not visible but inferred from context or environment . He finds that Hybrids that fuse together objects have the highest positive impact on viewer appreciation . Additionally , he finds that highly complex visual metaphors are negatively correlated with appreciation . Simpler blends with fewer objects are easier to perceive and interpret [ 28 ] . In this paper we create the Hybrid blends Forceville describes : images that fuse two simple objects into one . Design Patterns Design patterns are high - level solutions to recurring engi - neering and design problems . This includes architectural patterns [ 4 ] , software engineering patterns [ 14 ] , and web design patterns [ 11 ] . Design patterns are reusable solutions , but because they are abstract , effort must be put into under - standing when to apply them and how to adapt them to a new problem . Design patterns can be used to automatically solve some design challenges such as laying out furniture in a room [ 21 ] , generating usable maps [ 3 ] , illustrating furniture assembly instructions [ 2 ] , or sequencing cuts in film [ 18 ] . However , when the whole problem can’t be computed automatically , human intelligence can also be used to complete design pat - terns . Motif [ 17 ] used design patterns to help novice film mak - ers structure their videos . Human - robot interaction program - ming can be facilitated by design patterns [ 16 , 24 ] . Crowd innovation techniques [ 30 , 31 ] used schemas ( which can be viewed as design patterns ) to propose innovative products through analogies . In general , design patterns are abstract solutions that can provide high - level structure to solving problems . Decomposing Design Many web - based systems have made progress toward scaf - folding the design process in online environments that en - able collaboration . Yu and Nickerson [ 32 ] crowdsourced the CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 2 Figure 2 : Examples of professional visual blends in print media . One magazine cover , two ads , and one PSA for global warming design of chairs by mixing ideas across users to spur in - novation . Yu and Kittur [ 30 , 31 ] used the crowd in a two - stage , analogy - based product idea generation mentioned previously . BlueSky [ 15 ] and IdeaHound [ 25 ] both use brain - storming and crowd ideation to solicit a diverse set of ideas that span a design space . IdeaHound then uses a hybrid of human intelligence and computational techniques to clus - ter the ideas into an idea map to make sense of the design space . Voyant [ 29 ] and CrowdCrit [ 20 ] allow creators to solicit feedback and structured evaluation from crowds to enable them to get multiple perspectives on their work and iterate . In general , scaffolding the design process leads to better outcomes [ 7 , 10 , 23 ] . Recent systems have had success decomposing the design process into large tasks which have experts [ 23 ] or dedicated students [ 27 , 33 ] do the tasks with the benefit of a manager who coordinates the workers . There is still an open problem of how to decompose the design pro - cess into independent microtasks so that people can work in a decentralized manner . Microtask workflows are a common way to structure crowdwork [ 6 , 9 , 19 ] . However , open - ended tasks like plan - ning conferences or vacations [ 8 , 34 , 35 ] often require the collaborators to work together towards a common goal . In these systems the workflow is not static . The system pro - vides feedback on progress as users iterate towards the goal . Creating flexible workflows with feedback and iteration are a promising approach to coordinating online collaboration to achieve the goal of a high - quality output that meets the constraints of the problem . VisiBlends takes a flexible work - flow approach towards solving a creative design problem and explicitly leverages the iterative design process . 3 DEFINITION OF VISUAL BLENDS Visual blends associate two concepts by blending objects related to each of the concepts . We define a visual blend as having the following properties : ( 1 ) Two concepts . The input to a visual blend is two con - cepts . The concepts can be concrete , such as “RedBull” or “Brazil” or abstract like “energy” or “taking off . ” ( 2 ) Two objects . For each of the two concepts , the vi - sual blend has an object that is a visually recognizable symbol of that concept . ( 3 ) Two objects are integrated into one object . In or - der for the two objects to appear blended , they must be integrated into one object . They cannot simply be next to each other or in the same scene as each other . ( 4 ) Both objects are recognizable . Both objects must be individually recognizable to the viewer so they can see what concepts they symbolize , and infer the associa - tion between the concepts . Figure 3 shows examples of what is and is not a visual blend . All three images contain two objects , one of which is symbolic of Starbucks , and one which is symbolic of summer . The first image places the two objects near each other . This is not a visual blend . Although both objects are identifiable , they are not integrated . The middle image places a Starbucks logo on a beach scene and fully covers the sun . This is also not a visual blend . Since the Starbucks logo fully covers the sun , the two objects are not integrated into one object . Instead , the logo looks integrated into a beach scene . The image on the right is a visual blend . It integrates the Starbucks logo into the sun , and both objects are individually recognizable . CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 3 Figure 3 : Three ways of combining symbols for Starbucks and summer . The first 2 do not meet the definition of a visual blend , but the third does by integrating the two symbols into one object . 4 BLENDING DESIGN PATTERN The main challenge in creating a visual blend is finding a way to blend two objects into one object and yet have both objects be recognizable . Our approach to satisfy these two requirements is based on theories of the human visual perceptual system and on an analysis of hundreds of visual blends . The human visual system uses many different visual fea - tures at different stages to recognize an object including the object’s simple 3D shape , silhouette , depth , color and de - tails [ 26 ] . Based on this cognitive model , our approach to creating a visual blend is to make one object that borrows visual features from two objects . In the blend , some of the features will indicate one object , other features will indicate another object , and some features may indicate both objects . The approach we explore here are blends of two objects that share a simple 3D shape but the silhouette indicates one object and the colors and details indicate the other object . For example , Figure 4 shows the Earth + ice cream cone blend . The Earth and ice cream cone share a simple spherical shape . The ice cream cone is identifiable by the silhouette created by cone and the Earth is identifiable by the blue and green color and pattern details inside its main sphere . We call this design pattern for blending objects Single Shape Mapping because it matches two objects based on a single shared shape , then blends them by mapping all of one object into part of the other object . 5 VISIBLENDS SYSTEM The VisiBlends system is a flexible workflow that uses a hybrid of human intelligence and computational techniques to create visual blends . The workflow scaffolds the design process with separate interfaces for each type of microtask : brainstorming , annotation , and evaluation . The workflow is implemented in a website where each type of microtask is contained on its own page . Figure 4 : An illustration and examples of the Single Shape Mapping design pattern . To enable collaboration , users’ contributions are synchro - nized to all other users in real time . To be flexible , users are allowed to move between the tasks and see others’ work . Traditional workflows are more rigid — they assign tasks to workers and do not allow access to all the data . By be - ing more flexible , VisiBlends allows users to better adapt to emerging constraints . The website is implemented in the Meteor web framework , using a Fabric . js drawing canvas to annotate shapes and mock up blends . The target users are groups of 2 or 3 people with a mes - sage they want to convey . The group could be a company , student organization , news outlet , or any other community . These messages can be news headlines , advertisements or public service announcements . To begin , the users must find two important concepts from the message that they want to associate in the blend . For the headline “Football Dangerous to Youth Development , ” the users could pick football + dan - gerous as the two concepts to blend . The concepts must be broad enough so that there is enough variety in the symbols to find matches . If the concepts are not broad enough , the users may need to brainstorm to broaden them . The workflow has six steps inspired by the iterative design process . Before users can participate in the workflow , they must complete a 15 - minute training session to learn what visual blends are and the steps to make them . For both of the two concepts , ( 1 ) Users brainstorm associations with the concept . ( 2 ) Users find images of objects that visually represent the concept in simple , iconic ways . ( 3 ) Users annotate images for shape and coverage . With the collection of annotated images for both concepts , ( 4 ) The system automatically detects which images to blend . ( 5 ) The system automatically synthesizes the blends . ( 6 ) Users evaluate each blend . CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 4 Figure 5 : The VisiBlends workflow . A ) is the interface for collaborative brainstorming , B ) is the interface for collaboratively finding and annotating images . C ) is pseudocode for the matching algorithm . D ) is the interface to see automatically synthe - sized blends and evaluate them . If the first iteration does not yield a satisfactory blend , then users can repeat the process : brainstorm more concepts , find more images , and evaluate the new blends . This produces a blend mock - up of reasonable quality . If the user wants to improve the aesthetics of the blend they can edit the image or hire an artist . Workflow Once users have received training on the entire workflow , they are ready to participate in any of the microtasks . The input to the workflow are two concepts such as Starbucks and summer . Figure 5 contains an overview of the interfaces for each step . 1 . Brainstorming . For each concept , we want to find multiple objects that represent it . Although most people can easily think of a few objects that represent a concept , we need many options to increase the likelihood of finding a blend . Thus , we broaden their brainstorm by having them think of associ - ated people , activities and settings . One such brainstorm for summer includes the following : • Objects : beach , pool , sunglasses , watering can , lawn - mower • People and actions : lifeguard : saving people from drowning , students : taking vacation from school , base - ball manager : coaching players • Activities : tanning at the beach , playing tennis , Cele - brating 4th of July • Settings : baseball stadium , backyard barbecue , pool , beach The objects brainstormed for summer did not include “barbecue , ” but one of the settings brainstormed was “back - yard barbecue , ” which led to the object “barbecue . ” Brain - storming people , activities and settings indirectly helps users think of more objects . This technique is adapted from the an ethnographic observation technique called the AEIOU framework [ 1 ] . 2 . Finding Images . For a concept such as summer , a Google Image search will return many images that depict summer . However , very few of these images are useful for making blends . In the training session , we emphasize that the images users find must be simple , iconic objects with a single main shape . They cannot be people , animals , complicated scenes , or special versions of objects . To represent Starbucks , we do not want this year’s Christmas cup , we want the plain , iconic Starbucks cup because it is easier recognize . The top panel of Figure 6 shows good and bad examples of simple , iconic objects . Using the brainstorm results from the previous step , users find and input ten image URLS from Google Image Search . These are saved by the system and later used to produce visual blends . 3 . Annotate Images for Shape and Coverage . The Single Shape Mapping design pattern blends two objects based on their shape and how much of the object is covered by the shape . Thus we need users to annotate each image’s main shape and whether the shape covers the whole object in the image or only part of it . In the interface , each image is presented in an HTML5 canvas where users can move and scale shapes to cover the main part of the object . They can then input what 3D shape best represents the object : circle ( 2D ) , sphere ( 3D ) , rectangle CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 5 Figure 6 : Guidelines for 3 microtasks : rules for finding im - ages , shape categories , and shape coverage ( 2D ) , box ( 3D ) , or cylinder ( 3D ) and whether that shape covers all of the object or part of the object ( Figure 5 ) . In the tutorial , users saw examples the shape category and shape coverage annotations and practiced annotating them ( see Figure 6 ) . 4 . Matching Algorithm . Once users have found and anno - tated images for both concepts , the system can automatically detect which pairs of images should be blended according to the Single Shape Mapping design pattern . The algorithm takes in two sets of annotated image objects and finds all possible pairs across the two sets that meet all the following criteria : • Shape match : Both objects have the same main shape . • All - to - part match : in one object the shape covers all of the object and in the other object , the shape covers part of the object . • Similar aspect ratio : The height - to - width ratio of one object’s shape is within 50 % of the other object’s aspect ratio . 5 . Automatic Blend Synthesis . Once the matching algorithm has found a pair of annotated images to blend , the synthesis tool automatically synthesizes a mock - up of the blend based on their shape masks . We define Object A as the object with the shape mask covering all of the object and Object B as the object with the shape mask covering part of the object . The goal is to replace the masked part of Object B with all of Object A , as seen in Figure 4 . There are four steps to automatic blend synthesis . The algorithm first crops Object A based on its shape mask . It then scales the image to match the aspect ratio of the shape mask of Object B . It finds the centers of both shape masks and layers Object A on top of Object B so that their centers overlap . Lastly , it rotates Object A to match the rotation of Object B . This makes Object A fit inside the shape mask of Object B . Examples of this can be seen in the last panel of Figure 5 where users see and evaluate the automatically synthesized blends . 6 . Evaluation . The blends produced by the algorithm are presented to users in a list that show Object A , Object B and the automated blend . See Figure 5 . Users evaluate the blend by judging that it meets both criteria : ( 1 ) both objects are blended ( 2 ) both objects are individually recognizable . If so , they save the blend . They can make small adjustment to the masks to correct for small errors in annotations from previous steps . If the masks were drawn imprecisely , it can be tweaked in this interface before saving . Iteration . If no blends are found , or the user wants to im - prove blends they can iterate . If no blends are found , a naive way to iterate would be to simply find and annotate more im - ages , and hope for more blends . However , users can also use their knowledge of the design pattern to see in what specific ways they should refine their search in order to maximize the possibility of finding good blends . If the user wants to improve the blend aesthetics , they can refine the search . For example , the Starbucks + sun blend in Figure 1 , the user did a second iteration to search for a yellow Starbucks logo , to better blend with the yellow sun . Output and Refinement . The system will often output many visual blends for a given concept pair . If the user wants to improve the aesthetics of the blends , they can either edit it themselves , or hire an artist from an online labor market to produce a professional - quality image based on the output . For example , the Starbucks + sun blend in Figure 1 has been cropped to a square . Moreover , if a yellow logo wasn’t found , they could have done a color replacement . 6 EVALUATION Decomposing a creative task into independent microtasks is notoriously difficult [ 22 ] . To validate the task decomposition in the VisiBlends workflow , we test it on three populations : decentralized workers , co - located groups , and individuals . We address the following research questions : 1 . Decentralized collaboration : Can users create blends in independent steps with no central planner or coordinator ? When the workflow does not find a blend on the first pass , CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 6 Figure 7 : Examples of 4 of the 11 blends produced in the first iteration of Study 1 , with aesthetic editing by an artist . can new workers effectively iterate on top of previous work , or do they have to start over ? 2 . Group collaboration for messages : Can co - located teams effectively collaborate to create blends for their own messages ? With the additional challenge of making blends that conveys a message , is the workflow sufficiently flexible to allow groups to meet additional constraints ? 3 . Individual novices : Does VisiBlends help novices cre - ate more successful blends than their innate process ? Does scaffolding the process and reusing previous work help indi - viduals make better blends ? Study 1 : Decentralized collaboration We often think of design has a process that needs centralized control - one person to who either does all the work , or is in control over all work done by others . However , if we can decompose the design process , we can enable contributors to work independently without the oversight of a central planner . To test whether the VisiBlends workflow enables decentralized collaboration , we ran a study that made blends for 16 concept pairs where each step of the workflow was done by a different person . We recruited 7 university students ( 5 male , 2 female ) who were paid $ 20 for a 1 - hour study . They spent 30 - minutes completing an interactive tutorial on all stages on the work - flow . Afterwards , they spent 30 - minutes doing microtasks . To ensure independence , each user participated in only one stage of the workflow for any given concept pair . Although each person only worked on one step of each blend , it is important to note that everyone was trained on the entire workflow . In early versions we found that people were only successful at completing microtasks when they knew where each microtask fit into the entire workflow . We expand on this in the discussion . We tested the workflow on 16 concept pairs made from 8 concepts : 4 brand or product concepts : New York City , bicy - cles , Lego Toys , and McDonald’s and 4 nouns or adjectives to associate with a brand or product : fall ( season ) , healthy , en - ergy , and fashion . Some of these combinations make sensible messages like “Riding a bike is healthy” and some are less sensible like , “Legos are fashionable . ” The goal at this stage is to test the feasibility of the making blends for many random concepts pairs . The concept pairs we picked had never been run before and there was no guarantee it was possible to find a blend for any of the inputs . For each concept , the workflow first collected a brainstorm of 40 ideas from 5 MTurk workers ( they were paid $ 0 . 25 for 8 ideas ) . Next , the in - person participants independently completed the steps of workflow : one person found 10 images of the first concepts , a different person found 10 images of the second concept , a third and fourth person annotated the two sets of images , and the blending algorithm automatically produced visual blends for another person to evaluate . Results . Across the 16 concept pairs there were a total of 320 brainstormed items , 80 images , and 80 image annotations . The first iterative pass of the workflow produced at least one visual blend for 11 of the 16 concept pairs . Figure 7 shows 4 successful blends found on the first pass with aesthetic refinement by an artist . This demonstrates that it is possible to decompose the process of finding visual blends for some concepts pairs , but indicates that the process does not always work in one pass and that iteration is sometimes necessary . To find blends for the 5 remaining concept pairs , we gave a new user access to the system and asked them to iterate . We did not give users any guidelines how to iterate . The naive way to iterate would be to find more images and simply hope it results in a new blend . Instead , users did something much better – they looked at the annotated images to see why no blends were found , and used their knowledge of the Single Shape Mapping design pattern helped them guide a new search . For example , there were no matches found for Lego + healthy . They realized this was because all the Lego symbols were box - shaped and all the healthy symbols were spheres . To find a blend , they searched for a healthy CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 7 symbol that was box - shaped and found sushi ( Figure 10 ) . Their knowledge of the Single Shape Mapping design pattern helped them direct their new search . We expand on this idea in the Discussion . Study 2 : Group collaboration on blends for messages After establishing that decentralized workers can use the workflow , we ran 5 case studies where groups of 2 or 3 peo - ple familiar with the workflow collaborated to make visual blends for a message . During the study the group members worked in one room , each with their own laptop , but still able to discuss their work aloud . Each message was either a headline for a news article , a public service announcement or an advertisement inspired by events or concerns on campus . The groups were given messages and their concept pairs : ( 1 ) News : “Football linked to brain damage” Concept pair : football + dangerous ( 2 ) PSA : “Wash your hands . It’s the smart move . ’ Concept pair : Hand - washing + smart ( 3 ) Ad : “Joe’s Coffee : Open late” Concept pair : Joe’s Coffee + Night ( 4 ) Ad : “Panel Discussion : Women in CS” Concept pair : Women + Computer Science ( 5 ) Ad : “Join the Philosophy Dept’s Holiday Celebration” Concept pair : Philosophy + Christmas In these case studies , we want to know if the system enables leaderless collaboration and whether it is flexible enough to allow users to adapt when new constraints emerge . Results . On average , groups brainstormed more than 20 items and found more than 20 images for each concept . They found at least 3 good blends for each concept pair . Figure 8 contains posters made featuring these blends . Participants got many benefits from collaboration . They built off each others’ brainstormed ideas and images . They annotated the same image in multiple ways based on how they modeled the object . They were surprised and delighted to see images they found being unexpectedly blended with an image another user found . They enjoyed looking at the blends together and seeing the reaction of other users in person rather than evaluating blends in isolation . They also made minor edits like correcting each others’ annotations . In all cases , the users did not need a central planner to assign them tasks ; were able to select their own tasks based on feedback provided by the system . When using VisiBlends for real messages , we discovered that many emergent constraints were discovered and users had to adapt . First , they discovered that it is easier to find images for some concepts than others . Joe’s Coffee is a local chain and although users could brainstorm many symbols related to the concept , they could not find many images online . To compensate for the lack of images , they focused their search on finding more images of the other concept night to increase the chances of finding a blend . Based on availability of symbols and images , users could choose which tasks were worth spending time on . The system’s flexibility was useful in allowing them to decide how many images to find in response to discovering that some concepts are harder to symbolize than others . Based on the symbols and images found , users sometimes had to relax constraints to meet the goal . The best symbols for Philosophy were images of thought - experiments like the trolley problem and Searle’s Chinese room . However , these are scenes , not single objects , as required for visual blends . Still , they were able to replace key elements of the scene with a Christmas symbol and still satisfy the definition of a visual blend . Although these images are more complicated than desired , they convey the idea of Philosophy + Christmas very well . Women + CS both have many symbols , but the group wanted to find symbols for women that were not gender stereotypes . Despite having two women and one man in the group , they struggled to find non - stereotypical symbols for women . They ended up blending a perfume bottle with a QR code . They liked the visual appearance of the blend , but the symbol was not ideal . In any design problem , it’s hard to meet all the constraints , and users have to decide where they want to compromise . The workflow gives them the flexibility to add or relax constraints so they can convey their message . In all 5 case studies , iteration was used to improve existing blends by finding slightly different versions of the same objects . For example , in the football + dangerous blend , the first iteration blended a side - view of a red football helmet with the skull and crossbones . Although this qualifies as a visual blend , users iterated on it by finding a white , cartoon helmet that fit the aesthetic of the skull and crossbones better . The first iteration of the visual blending workflow is great at finding shape matches , but iteration was always beneficial in refining the secondary visual aspects of the blend , such as color and style . Allowing user to improve the aesthetics of the blend was not a design goal of the system , but the flexibility of when and how to iterate made this type of iteration possible . Study 3 : Novices with and without VisiBlends VisiBlends aims to scaffold the design process so that novices can create blends . Additionally , because the task is broken down into microtasks , there is an opportunity to reuse im - ages across blends . A key question is whether these features actually help novice designers make visual blends compared to their innate process . To test this , we ran a controlled study of individual novice users and compared how many success - ful blends they could made with and without VisiBlends . CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 8 Figure 8 : Blends produced for the 5 messages in Study 2 , with aesthetic improvements done by the users . We recruited 13 undergraduates ( 11 female , 2 male ) with no formal training in graphic design . Each person was intro - duced to the concept of visual blends with a definition , three annotated examples showing how two objects were blended into one object , and four exercises for them to annotate and check their answer . Each person was then asked to make six visual blends . Half the participants were assigned to the control - first conditions where they made visual blends with - out VisiBlends for the first set of three concept pairs , then with VisiBlends for the second set of three concept pairs . The other half were assigned to the VisiBlends - first condi - tion , where they made visual blends first with VisiBlends , then without it . After making blends , we interviewed partic - ipants about their experience and collected demographics . This setup allowed us to compare visual blends made with and without the system . The study took a maximum of 1 hour and participants were paid $ 20 . In the control condition , people were asked to make visual blend mock - ups in a common application for novices to edit images – Google Presentations . Before starting the tasks , they were given a warm - up to learn Google Presentations and ask any questions about the task . They were given 5 minutes for the warm - up to create mock - ups for the concept pair apple ( fruit ) + energy . All participants found the time suf - ficient to become familiar with the task and how to perform operations such as image search , copy and paste , bring to front , crop , and transparency adjustment . After the warm - up , they were given fifteen minutes to create as many blends as they could for the first set of concept pairs : McDonald’s + dangerous , Bicycle + smart , Football + autumn . They had 5 minutes per concept pair . Next , they were given a tutorial on the design pattern and the VisiBlends tool and asked to spend another 15 minutes creating visual blends for the sec - ond set of concept pairs : Joe’s Coffee + morning , New York City + night , Columbia University + computer science . In the VisiBlends condition , participants got the same amounts of time , but used VisiBlends first to make blends for the first set of concept pairs , then used Google Presentations to make blends for the second set of concept pairs . In both conditions , when participants used VisiBlends they had brainstorms , images , and annotations reused from previous users . However , all the concept pairs in the study had never been made before . For example , the concept pair McDonald’s + dangerous had never been produced before but McDonald’s symbols were taken from when users made blends for McDonald’s + healthy and symbols for dangerous were reused from symbols found while creating blends for football + dangerous . One of the potential strengths of Visi - Blends is that it allows brainstorms and symbols to be reused in new combinations . However , it is not guaranteed that there will be any matches for the new combination . Thus , users must evaluate and improve the new blends . Results . During the study , participants created 332 attempts at visual blends of which 243 were successful . Blends were considered successful if they met all of the objective qualifi - cations of a visual blend – there are two objects blended into one such that each object is individually identifiable . The quality of the symbols was not a part of the evaluation ; we assumed that the symbols conveyed their intended mean - ing . The evaluation ( performed by the authors ) was done by counting the number of attempted and successful blends for each person and each concept pair in both conditions . Results show that using the tool dramatically increased the number of successful visual blends . Using a two - tailed t - test assuming unequal variance , we compared the number of successful blends made by users in the VisiBlends - first condition to the control - first condition . On average , partic - ipants made 5 . 67 successful blends per concept pair in the VisiBlends condition and 0 . 55 successful blends in the control condition ( t ( 22 ) = 5 . 91 , p < 0 . 001 ) . In the VisiBlends condition , users made more attempts at blends and had a higher success rate . On average , people made 6 attempts per concept pair with VisiBlends and 2 . 72 in the control ( t ( 22 ) = 3 . 46 , p = 0 . 002 ) . Additionally , in the VisiBlends condition , users had a 96 % CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 9 Figure 9 : Examples of images made with and without Visi - Blends for bicycle + smart and New York City + night . The images made without the system are not visual blends . The images made with the system are blends and have no addi - tional editing . success rate , as opposed to a 21 % success rate without it ( t ( 18 ) = 9 . 28 , p < 0 . 001 ) . For the participants in the control condition who started without VisiBlends , we measured the performance after intro - ducing VisiBlends and found there is a significance increase in number of successful blends after they were introduced to VisiBlends . Their average number of successful blends jumped from 0 . 56 to 5 . 56 ( t ( 18 ) = 4 . 88 , p < 0 . 001 ) Conversely , for participants who saw VisiBlends first we measured per - formance after removing VisiBlends , and found their perfor - mance was much worse . With VisiBlends , they averaged 5 . 67 successful blends but after removing the tool , they produced an average of only 0 . 67 blends ( t ( 21 ) = 5 . 84 , p < 0 . 001 ) . Once VisiBlends was removed , those participants had a very simi - lar performance to those in the control group ( averaging 0 . 67 blends , as opposed to 0 . 56 blends ) . Thus , using VisiBlends has a large and significant effect on performance in both conditions , demonstrating the utility of the system beyond just having knowledge of the process . After making blends in both conditions , participants were interviewed about their experience . In general , people ex - pressed that using VisiBlends was a lot easier than making it themselves . P10 from the control - first group said “ [ The task is ] harder than I thought . Using the tool was a lot easier . You already know what shapes to use . ” From interviews and observations , we seek to understand the role of the tool and why it helps users . Finding symbols is hard and reusing previous work helps . 11 of the 13 participants mentioned that in the con - trol condition , finding images was difficult , especially for abstract concepts . P6 said : “Smart is such an abstract word . How do you picture smart ? What is the symbol for smart ? ” People often assume that Google Image Search will provide good symbols , but they are surprised to find that for abstract terms like “smart” , “autumn” or “computer science” it returns beautiful images of scenes but very few images of objects . This indicates that the brainstorming techniques in step 1 are helping in expanding abstract concepts to find more concrete symbols . Additionally , reusing brainstorming or images from other blends saves time and effort . The automated blend synthesis saves time and makes evaluation easier . Making mock - ups is not intellectually difficult , but it does take a lot of time . P8 said “This is taking a lot longer than I thought” . 8 of 13 people mentioned that the tool produced higher quality mock - ups . Additionally , two people said that better mock - ups made evaluation easier . P5 said : “It was easier seeing [ system mock - ups ] to know if they were good or bad . When you’re making it yourself it’s hard to know if it’s good . ” . Being able to evaluate your own work is crucial to success . When the system synthesizes blends automatically , users can focus on the high - level task of eval - uating the blends rather than the low - level tasks like moving , cropping , and resizing images . By scaffolding the design process , VisiBlends helps users meet all the constraints . When asked about their strategies for making blends without VisiBlends , participants expressed focusing on only one constraint of the problem . Two people focused on finding images : “I thought of two ob - jects then see if I could find a way to make them come together . I thought of them separately before the actual blend . ” ( P3 ) Five people mentioned looking for images with similar shapes that they could replace . “ [ I ] think of words , [ and ] just Google them . [ Then ] try to find similar shapes . ” One other focused on the technical considerations first : “I wanted to find things with transparent backgrounds so I could superimpose them . ” Only one person had a strategy involving flare - and - focus . She col - lected multiple images of each concept on the desktop before choosing a pair to combine . Almost everyone else followed a greedy strategy by focusing on one constraint , and then “look [ ing ] for coincidences” ( P11 ) . By scaffolding the design process , VisiBlends helps people meet all of the constraints and does not have to rely as much on coincidence . Figure 9 illustrates some of the common mistakes made without the tool . For New York City + night the user without the system focused on finding two good symbols : the New York skyline and the moon . They then tried to put them together , but it did not result in a blend because the two CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 10 things do not share a shape . In contrast , the image made using VisiBlends is a blend because the rectangular shape of the New York Metro Card to blend with the big dipper . For bicycle + smart , the user again finds good symbols : a bicycle to represent bicycle and Einstein and “A + ” to represent smart . However , Einstein is not blended with anything . His face is simply placed where a rider’s body could be . Additionally , the “A + ” is replacing the basket but , the basket is not fully a part of the bicycle – it a different object add - on to the bicycle . Thus , when it is replaced with the “A + ” , most viewers cannot tell what it is replacing . In contrast , the image using VisiBlends replaces the circle in the graph with the bicycle wheel , which meets all the constraints of a visual blend . When a blend is not successful , users often added elements like the “A + ” rather than starting over . VisiBlends helps users explore multiple designs early on , so that users avoid this tendency . 7 DISCUSSION Decomposing the design process Decomposing creative design problems is notoriously diffi - cult . How can we allow people to work independently with - out having full context of the problem ? In early versions we found that users can only perform tasks independently if they have general training on the entire workflow . Users who were not familiar with the entire workflow performed poorly on their microtasks and asked many questions like “how different should the images be ? ” This is a hard question to answer with an explicit rule . Instead , when users were given training on the entire workflow , they saw the purpose of each microtask and could implicitly answer those ques - tions by reasoning about later stages in the workflow : “The images should be different enough so that there is enough variety for the workflow to find blends . Variety in shape matters , but variety in color does not . ” Design tasks are too complex to state all the rules explicitly , however , a high - level understanding of the process can provide tacit knowledge useful for collaborators to make good judgments . When de - composing a problem , we learned that it is important to train workers on the entire workflow , even if they will only be doing one subtask . This provides the tacit knowledge needed to make good decisions when the rules are not enough . Three types of iteration in the design process When following the iterative design process , it is generally good advice to design , prototype and iterate . However , no explicit guidance is given on how to iterate . In the visual blending task , we observed three common strategies for iteration . See Figure 10 . 1 . Improving second - order features . When the system finds blends , the shape fit is usually good , but its secondary Figure 10 : Three cases of iteration . visual features like color , texture , or scaling can be improved . To make the objects appear better integrated , users can find different versions of the object that match the style of the blended image as seen in the football + dangerous blend . 2 . No matches are found . In the blend between Lego + healthy , users only found rectangular objects for Lego and round objects like apples and lettuce for healthy . The algo - rithm found no matches . However , instead of starting over , users can iterate by searching more precisely for the shape of items that they need to make more blends . Here , the user went back to the brainstorm for healthy and saw “fish” and realized that sushi was a rectangular object that could sym - bolizes healthy and blend with Lego . 3 . Emergent constraints . It is possible to satisfy the the Single Shape Mapping pattern and not get a good blend . For oranges + healthy , apples are an iconic representation of healthy in the abstract , but when blended with an orange , the orange is not identifiable . This is a emergent constraint : something particular to this blend that would be hard to anticipate or create a rule for . One way to iterate is to brain - storm for symbols of healthy that aren’t food . In Figure 10 , orange slices are blended with a piece of exercise equipment . CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 11 In general , when users iterate they use the definition of a visual blend and their knowledge of the design pattern to understand what went wrong and to refocus their search for the images that will improve the results . Decomposing other design problems VisiBlends takes the general design process and tailors it to one specific problem , based on one design pattern . However , the design process and the idea of design patterns is very gen - eral , so there is hope that flexible workflows can be created for other problems . To do so we would need to know what components go into the solution and what abstract design pattern can describe how those components fit together . Although we often assume creative design problems can’t be reduced to an easy formula , many creative tasks do have patterns : stories have the Hero’s Journey , music has chord progressions , proofs have proof techniques , software has design patterns and even academic papers have an abstract structure that advisors pass on to students . We could create flexible workflows that take this abstract knowledge and help users apply it by defining elements that need to go into the patterns and encourage them to iterate until the design pattern is satisfied . There was no existing design pattern for visual blends , so we had to find the pattern by looking at examples and testing theories . To find design patterns it is important to ignore the surface level details and focus on the elements that are more fundamental to human cognition . For visual blends , shape was important to a blend . For a domain like persuasive writing , psychological principles of emotional states may be the key elements of a design pattern . 8 LIMITATIONS AND FUTURE WORK There were three major limitations users experienced that we can address in the future . One of the biggest frustrations for users was the amount of time it took to search for images . Although most users intuit that finding images will be easy , it is surprisingly difficult to meet all the constraint - a simple , iconic object with a single main shape . Moreover , search engines return many unhelpful images like scenes of summer or images of the word summer . We believe that specialized image filtering techniques can be developed to quickly identify many distinct images of simple , iconic objects with a main shape . Although the automatic blend synthesize produces good mock - ups , it would be better if there were tools to assist users in quickly refining the blends within the system . After creating dozens of visual blends , we believe there are a small set of operations users could perform to greatly enhance the quality of the blend . Color replacement is one : the ability to select the green color of the Starbucks logo and replace it with the yellow of the sun . Replacing details is another . In the professional , blend of Tabasco + hot ( Figure 2 ) , the Tabasco bottle replaces the fire extinguisher , but it keeps a small detail - the fire extinguisher belt . The detail is small , but helps further identify the extinguisher and make the objects appear more blended . However , some techniques will always be out of reach . The professional earth + ice cream cone melting would be very hard to produce automatically . Lastly , it would be ideal to support more types of blends . All of the visual blends we produce have objects that look static . However , two of the professional examples in Figure 2 have the appearance of motion : the ice cream cone in melting and the Brazilian statue is taking off . In the future , we would like to expand the tool to make blends that look like they are in action , or even animate the action to produce a GIF rather than an image . 9 CONCLUSION Visual blends are an advanced graphic design technique to draw users’ attention to a message . They combine two ob - jects in a way that conveys a symbolic connection between them . Achieving this effect is challenging because there are two opposing goals : blending two objects into one while ensuring both objects are still recognizable . The VisiBlends systems help novices collaboratively create visual blends by decomposing the process for creating them into computa - tional techniques and human microtasks The process of creating visual blends has no obvious surface - level pattern . However , we discovered a deeper abstract struc - ture : blend two objects that have the same basic shape but other identifying visual features . In the design literature , abstract structure used for solving problems is called a de - sign pattern . This design pattern gives us the structure to decomposing the problem into a workflow that follows the design process with steps involving brainstorming , annota - tion , evaluation , and iteration . VisiBlends is a website that allows groups of people to collaborate in real - time to make blends for their own mes - sages . Rigid workflows often fail when tasks have emergent constraints , however , the VisiBlends workflow is flexible - it allows workers to move between types of tasks and iterate and adapt when new constraints are discovered . This task has implications for decomposing a broad range of design problems so that novices can collaboratively complete them . 10 ACKNOWLEDGMENTS This work was supported in part by the Brown Institute . REFERENCES [ 1 ] 2017 . AEIOU Framework . https : / / help . ethnohub . com / guide / aeiou - framework . ( 19 September 2017 ) . [ 2 ] Maneesh Agrawala , Wilmot Li , and Floraine Berthouzoz . 2011 . Design Principles for Visual Communication . Commun . ACM 54 , 4 ( April CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 12 2011 ) , 60 – 69 . https : / / doi . org / 10 . 1145 / 1924421 . 1924439 [ 3 ] Maneesh Agrawala and Chris Stolte . 2001 . Rendering Effective Route Maps : Improving Usability Through Generalization . In Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques ( SIGGRAPH ’01 ) . ACM , New York , NY , USA , 241 – 249 . https : / / doi . org / 10 . 1145 / 383259 . 383286 [ 4 ] Christopher Alexander , Sara Ishikawa , Murray Silverstein , Max Ja - cobson , Ingrid Fiksdahl - King , and Shlomo Angel . 1977 . A Pattern Language - Towns , Buildings , Construction . Oxford University Press . [ 5 ] Pete Barry . 2016 . The Advertising Concept Book : Think Now , Design Later ( Third ) . Thames & Hudson , London , UK . 296 pages . [ 6 ] Michael S . Bernstein , Greg Little , Robert C . Miller , Björn Hartmann , Mark S . Ackerman , David R . Karger , David Crowell , and Katrina Panovich . 2010 . Soylent : A Word Processor with a Crowd Inside . In Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology ( UIST ’10 ) . ACM , New York , NY , USA , 313 – 322 . https : / / doi . org / 10 . 1145 / 1866029 . 1866078 [ 7 ] Joel Chan , Steven Dang , and Steven P . Dow . 2016 . Improving Crowd Innovation with Expert Facilitation . In Proceedings of the 19th ACM Conference on Computer - Supported Cooperative Work & Social Com - puting ( CSCW ’16 ) . ACM , New York , NY , USA , 1223 – 1235 . https : / / doi . org / 10 . 1145 / 2818048 . 2820023 [ 8 ] Lydia B . Chilton , Juho Kim , Paul André , Felicia Cordeiro , James A . Landay , Daniel S . Weld , Steven P . Dow , Robert C . Miller , and Haoqi Zhang . 2014 . Frenzy : Collaborative Data Organization for Creating Conference Sessions . In Proceedings of the 32Nd Annual ACM Confer - ence on Human Factors in Computing Systems ( CHI ’14 ) . ACM , New York , NY , USA , 1255 – 1264 . https : / / doi . org / 10 . 1145 / 2556288 . 2557375 [ 9 ] Lydia B . Chilton , Greg Little , Darren Edge , Daniel S . Weld , and James A . Landay . 2013 . Cascade : Crowdsourcing Taxonomy Cre - ation . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’13 ) . ACM , New York , NY , USA , 1999 – 2008 . https : / / doi . org / 10 . 1145 / 2470654 . 2466265 [ 10 ] Steven P . Dow , Alana Glassco , Jonathan Kass , Melissa Schwarz , Daniel L . Schwartz , and Scott R . Klemmer . 2010 . Parallel Prototyp - ing Leads to Better Design Results , More Divergence , and Increased Self - efficacy . ACM Trans . Comput . - Hum . Interact . 17 , 4 , Article 18 ( Dec . 2010 ) , 24 pages . https : / / doi . org / 10 . 1145 / 1879831 . 1879836 [ 11 ] Douglas K . Van Duyne , James Landay , and Jason I . Hong . 2002 . The De - sign of Sites : Patterns , Principles , and Processes for Crafting a Customer - Centered Web Experience . Addison - Wesley Longman Publishing Co . , Inc . , Boston , MA , USA . [ 12 ] G . Fauconnier and M . Turner . 2002 . The Way We Think : Conceptual Blending and the Mind’s Hidden Complexities . Basic Books . [ 13 ] Charles Forceville . 1994 . Pictorial Metaphor in Ad - vertisements . Metaphor and Symbolic Activity 9 , 1 ( 1994 ) , 1 – 29 . https : / / doi . org / 10 . 1207 / s15327868ms0901 _ 1 arXiv : http : / / dx . doi . org / 10 . 1207 / s15327868ms0901 1 [ 14 ] Erich Gamma , Richard Helm , Ralph E . Johnson , and John Vlissides . 1995 . Design patterns : elements of reusable object - oriented software . Vol . 206 . 395 pages . https : / / doi . org / 10 . 1093 / carcin / bgs084 [ 15 ] Gaoping Huang and Alexander J . Quinn . 2017 . BlueSky : Crowd - Powered Uniform Sampling of Idea Spaces . In Creativity & Cognition . [ 16 ] Peter H . Kahn , Nathan G . Freier , Takayuki Kanda , Hiroshi Ishiguro , Jolina H . Ruckert , Rachel L . Severson , and Shaun K . Kane . 2008 . Design Patterns for Sociality in Human - robot Interaction . In Proceedings of the 3rd ACM / IEEE International Conference on Human Robot Interaction ( HRI ’08 ) . ACM , New York , NY , USA , 97 – 104 . https : / / doi . org / 10 . 1145 / 1349822 . 1349836 [ 17 ] JoyKim , MiraDontcheva , WilmotLi , MichaelS . Bernstein , andDaniela Steinsapir . 2015 . Motif : Supporting Novice Creativity Through Expert Patterns . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems ( CHI ’15 ) . ACM , New York , NY , USA , 1211 – 1220 . https : / / doi . org / 10 . 1145 / 2702123 . 2702507 [ 18 ] Mackenzie Leake , Abe Davis , Anh Truong , and Maneesh Agrawala . 2017 . Computational Video Editing for Dialogue - driven Scenes . ACM Trans . Graph . 36 , 4 , Article 130 ( July 2017 ) , 14 pages . https : / / doi . org / 10 . 1145 / 3072959 . 3073653 [ 19 ] Greg Little , Lydia B . Chilton , Max Goldman , and Robert C . Miller . 2010 . TurKit : Human Computation Algorithms on Mechanical Turk . In Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology ( UIST ’10 ) . ACM , New York , NY , USA , 57 – 66 . https : / / doi . org / 10 . 1145 / 1866029 . 1866040 [ 20 ] Kurt Luther , Amy Pavel , Wei Wu , Jari - lee Tolentino , Maneesh Agrawala , Björn Hartmann , and Steven P . Dow . 2014 . CrowdCrit : CrowdsourcingandAggregatingVisualDesignCritique . In Proceedings of the Companion Publication of the 17th ACM Conference on Computer Supported Cooperative Work & # 38 ; Social Computing ( CSCW Compan - ion ’14 ) . ACM , New York , NY , USA , 21 – 24 . https : / / doi . org / 10 . 1145 / 2556420 . 2556788 [ 21 ] Paul Merrell , Eric Schkufza , Zeyang Li , Maneesh Agrawala , and Vladlen Koltun . 2011 . Interactive Furniture Layout Using Interior Design Guidelines . In ACM SIGGRAPH 2011 Papers ( SIGGRAPH ’11 ) . ACM , New York , NY , USA , Article 87 , 10 pages . https : / / doi . org / 10 . 1145 / 1964921 . 1964982 [ 22 ] Daniela Retelny , Michael S . Bernstein , and Melissa A . Valentine . 2017 . No Workflow Can Ever Be Enough : How Crowdsourcing Workflows Constrain Complex Work . Proc . ACM Hum . - Comput . Interact . 1 , CSCW , Article 89 ( Dec . 2017 ) , 23 pages . https : / / doi . org / 10 . 1145 / 3134724 [ 23 ] Daniela Retelny , Sébastien Robaszkiewicz , Alexandra To , Walter S . Lasecki , Jay Patel , Negar Rahmati , Tulsee Doshi , Melissa Valentine , and Michael S . Bernstein . 2014 . Expert Crowdsourcing with Flash Teams . In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology ( UIST ’14 ) . ACM , New York , NY , USA , 75 – 85 . https : / / doi . org / 10 . 1145 / 2642918 . 2647409 [ 24 ] Allison Sauppé and Bilge Mutlu . 2014 . Design Patterns for Explor - ing and Prototyping Human - robot Interactions . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’14 ) . ACM , New York , NY , USA , 1439 – 1448 . https : / / doi . org / 10 . 1145 / 2556288 . 2557057 [ 25 ] Pao Siangliulue , Joel Chan , Steven P . Dow , and Krzysztof Z . Gajos . 2016 . IdeaHound : Improving Large - scale Collaborative Ideation with Crowd - Powered Real - time Semantic Modeling . In Proceedings of the 29th Annual Symposium on User Interface Software and Technology ( UIST ’16 ) . ACM , New York , NY , USA , 609 – 624 . https : / / doi . org / 10 . 1145 / 2984511 . 2984578 [ 26 ] Robert J Sternberg . 2011 . Cognitive Psychology . [ 27 ] Rajan Vaish , Snehalkumar ( Neil ) S . Gaikwad , Geza Kovacs , Andreas Veit , Ranjay Krishna , Imanol Arrieta Ibarra , Camelia Simoiu , Michael Wilber , Serge Belongie , Sharad Goel , James Davis , and Michael S . Bern - stein . 2017 . Crowd Research : Open and Scalable University Labo - ratories . In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology ( UIST ’17 ) . ACM , New York , NY , USA , 829 – 843 . https : / / doi . org / 10 . 1145 / 3126594 . 3126648 [ 28 ] Margot van Mulken , Rob le Pair , and Charles Forceville . 2010 . The impact of perceived complexity , deviation and comprehension on the appreciation of visual metaphor in advertising across three European countries . Journal of Pragmatics 42 , 12 ( 2010 ) , 3418 – 3430 . https : / / doi . org / 10 . 1016 / j . pragma . 2010 . 04 . 030 [ 29 ] Anbang Xu , Shih - Wen Huang , and Brian Bailey . 2014 . Voyant : Gen - erating Structured Feedback on Visual Designs Using a Crowd of Non - experts . In Proceedings of the 17th ACM Conference on Com - puter Supported Cooperative Work & # 38 ; Social Computing ( CSCW ’14 ) . ACM , New York , NY , USA , 1433 – 1444 . https : / / doi . org / 10 . 1145 / 2531602 . CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 13 2531604 [ 30 ] Lixiu Yu , Aniket Kittur , and Robert E . Kraut . 2014 . Distributed Ana - logical Idea Generation : Inventing with Crowds . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’14 ) . ACM , New York , NY , USA , 1245 – 1254 . https : / / doi . org / 10 . 1145 / 2556288 . 2557371 [ 31 ] Lixiu Yu , Aniket Kittur , and Robert E . Kraut . 2014 . Searching for Ana - logical Ideas with Crowds . In Proceedings of the 32Nd Annual ACM Con - ference on Human Factors in Computing Systems ( CHI ’14 ) . ACM , New York , NY , USA , 1225 – 1234 . https : / / doi . org / 10 . 1145 / 2556288 . 2557378 [ 32 ] Lixiu Yu and Jeffrey V . Nickerson . 2011 . Cooks or Cobblers ? : Crowd Creativity Through Combination . In Proceedings of the SIGCHI Con - ference on Human Factors in Computing Systems ( CHI ’11 ) . ACM , New York , NY , USA , 1393 – 1402 . https : / / doi . org / 10 . 1145 / 1978942 . 1979147 [ 33 ] Haoqi Zhang , Matthew W . Easterday , Elizabeth M . Gerber , Daniel Rees Lewis , and Leesha Maliakal . 2017 . Agile Research Studios : Or - chestrating Communities of Practice to Advance Research Training . In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing ( CSCW ’17 ) . ACM , New York , NY , USA , 220 – 232 . https : / / doi . org / 10 . 1145 / 2998181 . 2998199 [ 34 ] Haoqi Zhang , Edith Law , Rob Miller , Krzysztof Gajos , David Parkes , and Eric Horvitz . 2012 . Human Computation Tasks with Global Con - straints . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’12 ) . ACM , New York , NY , USA , 217 – 226 . https : / / doi . org / 10 . 1145 / 2207676 . 2207708 [ 35 ] Haoqi Zhang , Edith Law , Rob Miller , Krzysztof Gajos , David Parkes , and Eric Horvitz . 2012 . Human Computation Tasks with Global Con - straints . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’12 ) . ACM , New York , NY , USA , 217 – 226 . https : / / doi . org / 10 . 1145 / 2207676 . 2207708 CHI 2019 Paper CHI 2019 , May 4 – 9 , 2019 , Glasgow , Scotland , UK Paper 172 Page 14