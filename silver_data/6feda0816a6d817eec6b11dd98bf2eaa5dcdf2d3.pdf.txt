A Methodological Adaptation for Heuristic Evaluation of HRI Astrid Weiss , Daniela Wurhofer , Regina Bernhaupt , Martin Altmaninger , and Manfred Tscheligi * ( * ) Member , IEEE Abstract —Usability inspection methods like cognitive walk - throughs and guideline - based evaluations of robotic systems ﬁnd more and more usage in the design of human - robot interaction . However , due to the fact that robots can be used in a variety of different scenarios , applying these methods to robots is rather difﬁcult . Expert evaluation methods like heuristic eval - uations thus need a methodological adaptation to be reasonably applied for human - robot interaction scenarios . We propose a methodological adaptation of remote heuristic evaluation based on video scenarios . To investigate this methodological adaptation we conducted a heuristic evaluation case study based on six different video scenarios with three different robots . Traditional heuristic evaluation data was gathered ( number of found usability problems and severity rankings of the problems ) and additionally a feedback questionnaire on the method itself was distributed to the participating expert evaluators . We could show that video - based remote heuristic evaluations conducted by interdisciplinary expert evaluators are a valuable approach to detect usability problems in human - robot interaction design in an early development phase . I . INTRODUCTION An increased usage of Human - Computer Interaction ( HCI ) expert evaluation methods ( also called usability inspection methods ) can be observed in the research ﬁeld of Human - Robot Interaction ( HRI ) . Expert evaluation methods com - prise techniques like for example cognitive walkthroughs [ 1 ] , pluralistic walkthroughs [ 2 ] , and heuristic evaluations [ 3 ] . However , as there are some fundamental differences be - tween HRI and HCI , the adaptation and validation of expert evaluation methods for the HRI domain is necessary . Suitable methodological approaches for expert evaluations of robots are still an open issue and present a number of challenges . Firstly , the embodiment of robots can offer a broad variety of usage scenarios . A robot does not always offer physical cues for the user , which indicate its functionalities . Secondly robots are hard to transport , meaning that they have to be evaluated on - site in the research laboratory . Finally , direct interaction with a robot in an early development stage is not always feasible , as prototypes only exist virtually ( in simulations ) or because of spatial distance between the re - quired expert evaluator and the robotic system . Additionally , the question arises , who can be seen as expert for the assessment of human - robot interaction scenarios with robots , Manuscript received March 11 , 2009 . This work was supported in part by the ROBOT @ CWE project ( funded by FP6 - 2005 - IST - 5 ) . A . Weiss , Daniela Wurhofer , Martin Altmaninger , and M . Tscheligi are with the ICT & S Center , University of Salzburg , Sigmund - Haffner - Gasse 18 , 5020 Salzburg , Austria firstname . lastname @ sbg . ac . at R . Bernhaupt is with the IHCS - IRIT , University Paul Sabtier , 118 Route de Narbonne , 31062 Toulouse Cedex 9 regina . bernhaupt @ irit . fr as many researchers from various ﬁelds are integrated in the implementation process ( sometimes also distributed over different research institutes and countries ) . In the following , we will present a methodological adap - tation of the heuristic evaluation method for human - robot interaction . This approach is based on video scenarios allow - ing a remote expert evaluation by an interdisciplinary eval - uator team ( engineers , programmers , interaction designers , psychologists etc . ) . As human - robot interaction is heavily dependent on the interaction context , a video scenario should offer the required contextual information to enable a remote usability inspection . Based on a case study with six different scenarios and a feedback questionnaire on the methodological adaptation ﬁlled in by the expert evaluators , the following research question was investigated : Is remote heuristic evaluation , based on the usage of video scenarios , a valuable approach to detect usability problems in human - robot interaction ? The outline of our paper is the following : First we present related work in the area of usability inspection methods and video - based methods in HRI . Next we describe the methodological set - up and the results of our video - based heuristic evaluation . Then comes a discussion of the fea - sibility of the methodological adaptation . The article ends with a conclusion on our methodological adaptation and an outlook for future work . II . RELATED WORK Due to an increasing number of HRI systems and the necessity to evaluate these systems , ﬁrst methodological variations of classical HCI usability inspection methods for the ﬁeld of HRI have been made . Traditional usability inspection methods are characterized by the aim to ﬁnd usability problems of a system , mostly at an early stage in the usability engineering life cycle [ 4 ] . Concerning the adoption of classical HCI usability inspec - tion methods for the area of HRI , one problem is that the focus was on the evaluation of graphical user interfaces . However , there are ﬁrst reports and investigations on the applicability and feasibility of usability inspection methods and heuristic evaluations in particular for the area of HRI [ 5 ] , [ 6 ] . In contrast to Messina [ 7 ] , the evaluation guidelines of Scholtz [ 8 ] explicitly consider humans ( for example , one guideline asks : “Is the necessary information present for the human to be able to determine that an intervention is needed ? ” ) . 19th IEEE International Symposium on Robot and Human Interactive Communication Principe di Piemonte - Viareggio , Italy , Sept . 12 - 15 , 2010 978 - 1 - 4244 - 7989 - 4 / 10 / $ 26 . 00 ©2010 IEEE 1 978 - 1 - 4244 - 7990 - 0 / 10 / $ 26 . 00 ©2010 IEEE A direct adaptation of HCI heuristics to heuristics for evaluating human - robot interaction was done by Mayora et al . [ 9 ] . In their work , they adapted Nielsen’s heuristic eval - uation method to robotic interfaces , supporting the design of naturally interactive robotic systems . Another attempt for adapting traditional usability inspection methods to the ﬁeld of HRI was made by Yanco et al . [ 10 ] . By applying CSCW and HCI techniques to the domain of HRI , Yanco et al . [ 11 ] developed a set of guidelines for evaluating human - robot interaction . Their approach supports our assumption that the usability inspection approach is feasible for HRI . Pointing out a lack of discount methods for the HRI domain , Clarkson et al . [ 5 ] also developed heuristics for evaluating human - robot interaction . For human - humanoid robot interaction in particular , heuristics for the domain of humanoid soccer robotics were developed by Elara et al . [ 6 ] . However , when having a detailed look at the heuristics of Elara et al . , there is almost no difference between their heuristics and those of Clarkson and Arkin . Both collections have the same number of heuristics ( eight heuristics ) , and except of one heuristic they provide the same information . Three of the heuristics even have the same title . Next to the use of heuristics for evaluating human - robot interaction , our approach is characterized by indirect as - sessment of human - robot interaction , i . e . the observation of the interaction presented on video . In general , user in - teraction scenarios help to make a system’s use explicit [ 12 ] . According to Rosson and Carroll , user interaction scenarios represent an effective design tool for inspiring and systematizing the iterative development of interactive systems . Videos can be used as instantiations of scenarios in order to represent prototypes , thus helping to bridge the gap between abstraction and detail in the design process [ 13 ] , [ 14 ] . Up to now , video based methods in HRI are not very common . However , as running a live human - robot interaction trial under controlled conditions with statistically valid results requires relatively high investment of time , resources and personnel [ 15 ] , video trials could represent an alternative to live trials . A fundamental question when using video based methods in HRI is , if people’s perceptions from live and video trials are comparable . Several studies in the ﬁeld of HRI research support this assumption ( see e . g . [ 15 ] [ 16 ] , [ ? ] ) . Similar in the ﬁeld of mobile interfaces video scenarios proved their feasibility as basis for evaluation studies [ 17 ] . All theses studies let us assume that videos are a promising instrument for investigating human - robot interactions in an early stage of development also by expert evaluators . In the following we will describe our methodological set - up based on the related research ﬁndings . III . METHODOLOGICAL SET - UP The heuristic evaluation is a traditional usability inspection method proposed by Nielsen and Molich in 1990 [ 3 ] . The idea was to develop an evaluation technique for usability engineering that is lower in costs and time effort than user testing . The method is intended to ﬁnd and describe usability problems of a system on the basis of fundamental principles , so called heuristics . Heuristics describe essential attributes that a system should feature to ensure that the user is able to perform a task within a speciﬁed context in an effective , efﬁcient and satisfying way . Heuristic evaluations are per - formed by a small group of interface experts inspecting the system and comparing to what extend the principles have been adopted . All detected divergences and incongruities are deﬁned as so called usability problems . All usability problems found are ﬁnally compiled into one list and ranked by the experts regarding their severity . According to Nielsen , only a small number of expert evaluators are required ( three to ﬁve ) to ﬁnd about 75 % of the existing usability problems [ 18 ] . A . The Video - based Heuristic Evaluation Assessing the usability of robotic systems by means of a heuristic evaluation poses several challenges : ( 1 ) the embod - iment of the robots often gives no cues of its functionalities , meaning that there is a broad variety of usage scenarios ; ( 2 ) robots are hard to transport , meaning that they mostly have to be evaluated on - site in the research laboratory ; ( 3 ) direct interaction with a robot in an early development stage is not always feasible , as prototypes only exist virtually ( in simulations ) . These three aspects would make on - site heuristic evaluations of robots time and cost intensive . Thus , we intended to offer a methodological approach which addresses these challenges , decreasing costs and expenditure of time . By adapting the heuristic evaluation method through the usage of videos , experts from various disciplines are allowed to assess human - robot interaction scenarios at the same time , taking into account the interaction context . In order to prove the feasibility of a video - based approach for remote usability inspection , we used this methodological adaptation to detect usability problems in the interaction with one of three robots in different interaction scenarios ( see section III - D for details ) . For the heuristic evaluation we used the validated heuristics provided by Clarkson and Arkin [ 5 ] . These heuristics , however , only proved validity for direct human - robot interaction where the expert evaluator and the robotic system are co - located and the evaluator has the possibility to directly interact with the system . The usage of these heuristics in our evaluation should thus help to ﬁnd out , if these heuristics are also valuable for remote usability evaluation . In a ﬁrst step the expert evaluators were asked to provide a video mock - up of a human - robot interaction scenario . Then an heuristic evaluation ( using the heuristics of Clarkson and Arkin [ 5 ] ) was conducted on the basis of the presented interaction scenario . B . Evaluation Materials 1 ) Video Scenarios : As the video scenarios build the basis for the evaluation approach , a uniﬁed structure of the interaction scenarios was needed to guarantee comparable evaluation results from the different experts . This structure 2 TABLE I S CENARIO S TRUCTURE Scenario Name Please write down here the collaboration scenario in which the human - robot interaction happens in a narrative short - story , in - cluding the points listed belowRoles : Which roles are in - volved in the scenario e . g . a human worker and a robot that should raise an object in cooperation Tasks : What is the main task and the subtasks ( action sequences ) which should be carried out e . g . task : raise an object together ; sub - tasks : Identify the object ; giving feed - back to the human , grab the object etc . Setting : How can the set - up for the human - robot in - teraction be described e . g . the robot is positioned in front of the human , the robot moves au - tonomously , the human needs to wear sensors was adopted from the scenario approach for robot assisted play proposed by Robins et al . [ 19 ] and is described in Table I . 2 ) The Evaluation Manual : In order to reach common ground between the expert evaluators and to avoid open questions due to the absence of a test leader , a manual explaining the method and offering templates for the problem documentation , were sent to the expert evaluators . The manual consisted of a general introduction to the method ( goal and purpose ) , the heuristics [ 5 ] , the explanation of the severity ranking [ 18 ] , and a detailed plan of the procedure of the evaluation ( schedule , responsibilities , and deliverables ) . Furthermore , templates for the problem lists were provided as well as hints and literature references . All these materials can be found on the following website : http : / / www . robot - at - cwe . eu . 3 ) The Heuristics : The eight heuristics by Clarkson and Arkin [ 5 ] were used : 1 ) Visibility of system status 2 ) Appropriate information presentation 3 ) Use natural cues 4 ) Synthesis of system and interface 5 ) Help users recognize , diagnose , and recover from errors 6 ) Flexibility of interaction architecture 7 ) Aesthetic and minimalistic design 4 ) The Feedback Questionnaire : To investigate if the video scenario - based heuristic evaluation was perceived as a reasonable method for assessing the usability of robotic systems by an interdisciplinary expert evaluator team , a feedback questionnaire was developed . The feedback ques - tionnaire was sent out to all experts participating in the heuristic evaluation after the case study was ﬁnished . The questionnaire consisted of three closed questions : ( 1 ) How clear were the instructions in the manual for you ? ; ( 2 ) How easy did you experience the conduction of the heuristic evaluation ? ; ( 3 ) Which problems did you encounter during the evaluation ? ; and one open ended question : ( 4 ) Do you have any points of criticism ? C . The Evaluation Procedure In a ﬁrst step the manual was sent out via email to the expert evaluators , who were all part of the EU - funded FP6 Project “ROBOT @ CWE : Advanced robotic systems in future collaborative working environments” . Within the man - ual , the following procedure was suggested to the evaluators : 1 ) Assemble a small group of evaluators ( three to ﬁve ) to perform the heuristic evaluation . 2 ) Describe at least one collaboration scenario in which the human - robot interaction happens in a narrative short story , according to the scenario structure ( see Table I ) . 3 ) Videotape the scenario as basis for the documentation of the usability problems . 4 ) Each evaluator independently assesses the human - robot interaction and judges its compliance with a set of usability guidelines ( the heuristics ) and documents them in the provided template ( see Figure 1 ) . Fig . 1 . Template : Usability Problem After assessing the usability of the human - robot interac - tion scenarios , based on the heuristics of Clarkson and Arkin [ 5 ] , the expert evaluators sent their individual results back to us . We compiled a total problem list of the usability problems found for each scenario and sent it back to the evaluators with the following requests : 1 ) Reassemble ( if possible ) all the evaluators who already performed the heuristic evaluation . 2 ) Read through the complete problem list and watch the video sequences according to the problem to get an understanding of the described problem . 3 ) Each evaluator independently ranks all usability prob - lems from 0 ( no usability problem ) to 4 ( usability catastrophe ) and adds a reason why he or she rates the problem like that . 4 ) Document your ranking in the document template we sent you with this manual ( see Figure 2 ) D . The Robots The following three different robots were evaluated by the experts evaluators ( see ﬁgure 3 ) . 3 Fig . 2 . Template : Severity Ranking Fig . 3 . The Three Robots ( a ) RH - 1 ( b ) HRP - 2 ( c ) HOAP - 3 • RH - 1 : This robot is able to perform tasks autonomously , like dynamic walking , object transportation , moving to the sound or the source of an image , etc , whereas at some tasks , the interaction with the environment or humans will be needed . • HRP - 2 : This robot is the ﬁnal version of the Human Robotics Project headed by the Manufacturing Science and Technology Center . • HOAP - 3 : This robot has been designed in order to provide portability . The height of the robot is 60 cm and weights only 8 . 8 kg . The focus of the development of this robot was put on cooperation functions , like communication and image recognition . These different robots were used in six different human - robot interaction scenarios ( see Table II ) . For the case of the RH - 1 robot , the video mock - up was a simulation , i . e . the interaction with the RH - 1 robot was represented virtually . IV . R ESULTS According to the two phases of the heuristic evaluation ( phase 1 : usability problem identiﬁcation ; phase 2 : severity ranking of identiﬁed problems ) , Table III gives an overview of the number of evaluators involved . Nine expert evaluators with different backgrounds took part in the usability problem identiﬁcation phase of the case study and ten experts in the severity ranking . Overall , 39 different usability problems were found in six different collaboration scenarios ( see Table II ) . The expert TABLE II T HE I NTERACTION S CENARIOS Scenario Robot Problems found Move an object together ( autonomous ) HRP - 2 12 Move an object together I ( tele - operated ) HRP - 2 3 Move an object together II ( tele - operated ) HRP - 2 3 Move an object together III ( tele - operated ) HRP - 2 2 Learning by demonstra - tion HOAP - 3 5 Transporting a ladder to - gether with a robot RH - 1 14 TABLE III D ISCIPLINES OF THE E XPERT E VALUATORS Discipline Number of evalu - ators involved in problem ﬁnding Number of evalu - ators involved in severity ranking Haptic Engineering ( Telepresence ) 4 2 Electronic Engineering 1 4 Human - Robot Interaction 2 2 Computer Science 2 2 evaluators could identify usability problems , violating seven out of eight heuristics , as one usability problem can violate several heuristics ( see Table IV ) . All in all , 59 times heuristics were violated . Heuristic 2 , 3 , and 7 covered over 50 % of the found usability problems ( see Table IV ) . Brieﬂy said , the problems of the investigated robotic systems lie in the lack of the provided feedback , the communication , and the availability of the information coming from the robot . Heuristic 2 and 3 together cover 22 of the 59 heuristics violated . Heuristic 8 ( “Aesthetic and minimalist design” ) was not taken into account by any of the evaluators . We assume that this could be due to the fact that none of the expert evaluators had a background in interaction design . In the second evaluation phase , the evaluators had to rate the severity of the problems identiﬁed in phase 1 . 21 problems were identiﬁed as severe usability problems ( 3 or 4 on severity rating scale ) , and 18 problems were rated as TABLE IV V IOLATED H EURISTICS Heuristic Frequency MeanSeverity 1 Sufﬁcient information design 6 2 . 40 2 Visibility of system status 13 2 . 42 3 Appropriate information presenta - tion 9 2 . 61 4 Use natural cues 7 2 . 31 5 Synthesis of system and interface 7 2 . 44 6 Help users recognize , diagnose , and recover from errors 7 2 . 67 7 Flexibility of interaction architec - ture 10 2 . 55 8 Aesthetic and minimalistic design 0 0 . 00 4 minor usability problems ( 2 on rating scale ) . No problem was rated as unproblematic ( 0 on the rating scale ) . Aspects concerning visual feedback , task conduction ( of the robot ) , and feedback on errors were all rated as severe usability problems , thus reﬂecting the main issues to be improved . The average severity rating ranges from a “minor” to a “major” usability problem ( mean = 2 . 49 ) , which is rather high on a scale between 0 and 4 . Looking at the average of each heuristic in detail ( see Table IV ) , no signiﬁcant deviations could be observed ( SD ranges from 0 . 39 to 1 . 48 ) . These results can be interpreted in the following way : Although there is no signiﬁcant difference between the severity of the heuristic violations , the frequency of the violations reﬂects deﬁciencies , particularly concerning infor - mation representation and feedback . A . Results of Feedback Questionnaire The feedback questionnaire turned out to be a valuable instrument for getting information about problems and chal - lenges / experiences when conducting the video - based heuris - tic evaluation . Furthermore , suggestions for improvement of the method were revealed . Concerning the clearness of instructions in the evaluation manual , the respondents of the feedback questionnaire indi - cated to ﬁnd them clear ( 4 out of 6 ) or rather clear ( 2 out of 6 ) ; nobody rated the instructions as incomprehensible . These results indicate that the precondition of a successful “remote” evaluation was fulﬁlled . As such a “remote evaluation” is characterized by the absence of a test leader , it is especially important to have clear instructions . The second question was about how easy the conduction of the heuristic evaluation was experienced by the evaluators . Half of the respondents experienced the conduction as “rather easy” ( 3 out of 6 ) and one as “easy” ( 1 out of 6 ) . One participant characterized the conduction as “rather difﬁcult” ( 1 out of 6 ) and one as “difﬁcult” ( 1 out of 6 ) . These results point out that an autonomous conduction of the heuristic evaluation was experienced as difﬁcult for some of the participants . Therefore , the evaluation should be made easier for the next time , starting with the problems mentioned most often ( see below ) . Concerning problems of the heuristic evaluation , the du - ration and the length of the instruction could be improved . The duration of the heuristic evaluation was the problem mentioned most often by the respondents . All respondents indicated that the duration of the heuristic evaluation was too long ( 6 out of 6 ) . This problem could be solved by proposing short term video scenarios , where for instance only one task has to be assessed . Regarding the length of the manual , most of the respon - dents found the instruction too short ( 4 out of 6 ) . This is a problem which can be easily solved by giving more detailed instructions . However , a good balance between detail and length must be found , as instructions which are too long are often not read carefully . Thus , we suggest to provide a basic manual with supplemented “further reading sections” for participants who are interested in more details . The example which was given in the manual was clear to almost all of the respondents ( 5 out of 6 ) , except one who considered the example as unclear ( 1 out of 6 ) . This result indicates that the presentation of the example was adequate but could be perhaps revised in its details to make it more comprehensible . Points for improvement mentioned by the participants were to describe the scenarios more deeply and to make the rating scale more comprehensible . Another interesting suggestion was to integrate the evaluation within the video materials . V . F EASIBILITY OF THE M ETHODOLOGICAL A DAPTATION The analysis of the six case studies revealed that the heuristics from Clarkson and Arkin [ 5 ] are feasible for a remote expert evaluation on human - robot interaction based on video scenarios . Video scenarios of direct human - robot interaction are suitable for expert evaluations . However , the material should be produced accordingly to the requirements of the method , meaning a detailed visualization of all action sequences from the robot’s and the human’s perspective . For instance the videos of the tele - operation scenarios ( scenarios 2 to 4 ) were only recorded from one camera perspective which could be the reason why only little usability problems were detected by the experts . Similarly , the adaptation that the evaluation is conducted by an interdisciplinary team proved its value as the usability problems were described in detail from different perspec - tives . However , a missing methodology understanding of the expert evaluators could be observed during the conduction of the case studies , which could not be supported by the manual , but had to be explained by individual email answers . This fact leads to the assumption of less valid results , due to missing methodological pre - experience . This could be reduced by an on - site moderator who introduces the method in a short tutorial and a FAQ list . From the methodological point of view , an explicitly written manual is a key element for a successful application of the proposed adapted heuristic evaluation approach . A re - view on important human - computer interaction and usability literature could additionally increase the method knowledge and thus enhance the results of the evaluation . Regarding the number of required evaluators at least ﬁve or more evaluators should conduct the heuristic evaluation , as possible low return rates can be absorbed and the evaluator teams can be composed with a broader variety of research backgrounds . VI . C ONCLUSION AND F UTURE W ORK This article reported about an adaptation of the heuristic evaluation approach to allow remote assessment of human - robot interaction by means of video scenarios . Six case studies and a feedback questionnaire for the expert evalua - tors could show that the heuristics of Clarkson and Arkin [ 5 ] , originally intended to be used for direct assessment of human - robot interaction , can be reasonably applied for remote assessment of human - robot interaction . 5 By presenting scenarios of human - robot interaction on video , an interdisciplinary evaluation team could successfully identify usability problems in the interaction scenarios . As numerous severe problems could be found at an early stage in the development phase , our approach can be considered as effective . Even a virtual interaction scenarios was evaluated effectively concerning usability aspects . One of the biggest advantages of evaluating video - based human - robot interaction scenarios is the fact that it is more economical than live interactions . The evaluation of video - taped interactions requires less effort compared to live inter - actions , as the scenario only has to be conducted once for the recording of the video material . Moreover , conducting an evaluation remotely and being independent from a location ( i . e . evaluation is not limited to the research laboratory where the robot is developed ) does not only save resources , but also allows a variable sample size of expert evaluators who can assess the scenario in parallel . Generally , greater levels of control and standardization over the human - robot interaction set - up ( compared to live interactions ) is possible by using video - based interaction sce - narios . Thus , suggestions for improvement of the robot can be gained and implemented at an early stage of development , which in turn saves costs and time . We do not claim that our approach can replace live trials of human - robot interaction . However , we think that our approach can be used as a pre - stage of life trials , supporting the conduction of life trials with more advanced human - robot interaction scenarios with less usability problems . We learned from the case studies that the goal - oriented production of the video material is a key - element for compa - rable and informative evaluation results , as the video scenario needs to show in detail all interaction possibilities with the robot and has to reﬂect the interaction context as good as possible . Thus , a video - based heuristic evaluation is only reasonable if the video material is especially produced for the method , as the action sequences of the robot and the human need to be traceable in detail for the evaluator . For the future we plan to conduct a comparative study with the traditional heuristic evaluation ( on - site ) and our video - based heuristic evaluation ( remote ) , in order to investigate , if both methods produce similar results . Moreover , we want to gain more insights on the impact of a robot’s physical presence on the evaluation . A methodological drawback of the presented case studies using our video - based heuristic evaluation could be that these studies were very different , i . e . are characterized by different tasks done with different robots . Our results would be more reliable if we used the same tasks for all robots . We want to address this issue in future studies . We conclude that our methodological adaptation repre - sents a ﬁrst step towards a beneﬁcial approach to detect usability problems in human - robot interaction especially at an early stage in the development phase and thus is a valuable complement to traditional heuristic evaluation . VII . ACKNOWLEDGMENTS The authors gratefully acknowledge the work of the ex - pert evaluators . This work was supported by the European Commission through the EU Project ROBOT @ CWE ( FP6 - 034002 ) . R EFERENCES [ 1 ] P . G . Polson , C . Lewis , J . Rieman , and C . Wharton , “Cognitive walk - throughs : A method for theory - based evaluation of user interfaces , ” International Journal of Man - Machine Studies , vol . 36 , no . 5 , pp . 741 – 773 , 1992 . [ 2 ] R . G . Bias , “The pluralistic usability walkthrough : coordinated em - pathies , ” pp . 63 – 76 , 1994 . [ 3 ] J . Nielsen and R . Molich , “Heuristic evaluation of user interfaces , ” in CHI ’90 : Proceedings of the SIGCHI conference on Human factors in computing systems . New York , NY , USA : ACM , 1990 , pp . 249 – 256 . [ 4 ] J . Nielsen , “Usability inspection methods , ” in CHI ’95 : Conference companion on Human factors in computing systems . New York , NY , USA : ACM , 1995 , pp . 377 – 378 . [ 5 ] E . Clarkson and R . C . Arkin , “Applying heuristic evaluation to human - robot interaction systems , ” in FLAIRS Conference , 2007 , pp . 44 – 49 . [ 6 ] R . E . Mohan , C . A . A . Calderon , C . Zho , P . K . Yue , and L . Hu , “Using heuristic evaluation for human - humanoid robot interaction in the soccer robotics domain , ” in Second Workshop on Humanoid Soccer Robots @ 2007 IEEE - RAS International Conference on Humanoid Robots Pittsburgh ( USA ) , 2007 . [ 7 ] Messina , “Measuring performance and intelligence of intelligent sys - tems , ” Proceedings of the Performance Metrics for Intelligent Systems ( PerMIS ) Workshop , 2010 . [ 8 ] J . Scholtz , “Evaluation methods for human - system performance of intelligent systems , ” in Proceedings of the 2002 Performance Metrics for Intelligent Systems ( PerMIS ) Workshop , 2002 . [ 9 ] O . Mayora - Ibarra , E . Sucar , S . Aviles , and C . Miranda - Palma , “From HCI to HRI - usability inspection in multimodal human robot inter - actions , ” in Proceedings RO - MAN 2003 : The 12th IEEE International Workshop on Robot and Human Interactive Communication , 2003 , 2003 , pp . 37 – 41 . [ 10 ] H . Yanco , J . L . Drury , and J . Scholtz , “Beyond usability evaluation : Analysis of humanrobot interaction at a major robotics competition , ” Journal of Human - Computer Interaction , vol . 19 , pp . 117 – 149 , 2006 . [ 11 ] J . L . Drury , J . Scholtz , and H . Yanco , “Applying CSCW and HCI techniques to human - robot interaction , ” in CHI 2004 Workshop on Shaping Human - Robot Interaction . New York , NY , USA : ACM , 2006 , pp . 13 – 16 . [ 12 ] M . B . Rosson and J . M . Carroll , “Scenario - based usability engineer - ing , ” in DIS ’02 : Proceedings of the 4th conference on Designing interactive systems . New York , NY , USA : ACM , 2002 , pp . 413 – 413 . [ 13 ] W . E . Mackay , A . V . Ratzer , and P . Janecek , “Video artifacts for design : Bridging the gap between abstraction and detail , ” in In Proc . ACM Conference on Designing Interactive Systems , DIS 2000 . ACM Press , 2000 , pp . 72 – 82 . [ 14 ] H . Martin and B . Gaver , “Beyond the snapshot from speculation to prototypes in audiophotography , ” in DIS ’00 : Proceedings of the 3rd conference on Designing interactive systems . New York , NY , USA : ACM , 2000 , pp . 55 – 65 . [ 15 ] S . Woods , M . Walters , K . Koay , and K . Dautenhahn , “Comparing human robot interaction scenarios using live and video based methods : towards a novel methodological approach , ” in Proceedings of 9th IEEE International Workshop on Advanced Motion Control , 2006 . , 2006 , pp . 750 – 755 . [ 16 ] C . Kidd , Sociable robots : The role of presence and task in human - robot interaction , MSc Thesis . hskip 1em plus 0 . 5em minus 0 . 4em - Massachusetts Institute of Technology , 2003 . [ 17 ] J . Buur , T . Binder , and E . Brandt , “Taking video beyond ’hard data’ in user centered design , ” in Participatory Design Conference - PDC’00 , New York , NY , USA , 2000 . [ 18 ] J . Nielsen , Usability Engineering . San Francisco , USA : Morgan Kaufman , 1993 . [ 19 ] B . Robins , E . Ferrari , and K . Dautenhahn , “Developing scenarios for robot assisted play , ” in IEEE RO - MAN 2008 : The 17th IEEE Interna - tional Symposium on Robot and Human Interactive Communication , 2008 . , 2008 , pp . 180 – 186 . 6