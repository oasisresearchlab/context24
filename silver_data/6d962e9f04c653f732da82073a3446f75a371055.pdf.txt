COMMUNICATIONS OF THE ACM November 1996 / Vol . 39 , No . 11 27 A S WE MARCH INTO THE AGE of digital information , the problem of data overload looms ominously ahead . Our ability to analyze and understand massive datasets lags far behind our ability to gather and store the data . A new gen - eration of computational techniques and tools is required to support the extraction of useful knowledge from the rapidly growing volumes of data . These techniques and tools are the subject of the emerging field of knowl - edge discovery in databases ( KDD ) and data mining . Large databases of digital informa - tion are ubiquitous . Data from the neighborhood store’s checkout regis - ter , your bank’s credit card authoriza - tion device , records in your doctor’s office , patterns in your telephone calls , and many more applications generate streams of digital records archived in huge databases , sometimes in so - called data warehouses . Current hardware and database tech - nology allow efficient and inexpensive reliable data storage and access . Howev - er , whether the context is business , medicine , science , or government , the datasets themselves ( in raw form ) are of little direct value . What is of value is the knowledge that can be inferred from the data and put to use . For example , the marketing database of a consumer U s a m a F a y y a d , G r e g o r y P i a t e t s k y - S h a p i r o , a n d P a d h r a i c S m y t h T E RR Y W I D E N E R The KDD Process for Extracting Useful Knowledge from Volumes of Data Knowledge Discovery in Databases creates the context for developing the tools needed to control the flood of data facing organizations that depend on ever - growing databases of business , manufacturing , scientific , and personal information . 28 November 1996 / Vol . 39 , No . 11 COMMUNICATIONS OF THE ACM goods company may yield knowledge of correlations between sales of certain items and certain demographic group - ings . This knowledge can be used to introduce new targeted marketing campaigns with predictable financial return relative to unfocused cam - paigns . Databases are often a dormant potential resource that , tapped , can yield substantial benefits . This article gives an overview of the emerging field of KDD and data min - ing , including links with related fields , a definition of the knowledge discov - ery process , dissection of basic data mining algorithms , and an analysis of the challenges facing practitioners . Impractical Manual Data Analysis The traditional method of turning data into knowledge relies on manual analysis and interpretation . For exam - ple , in the health - care industry , it is common for specialists to analyze cur - rent trends and changes in health - care data on a quarterly basis . The special - ists then provide a report detailing the analysis to the sponsoring health - care organization ; the report is then used as the basis for future decision making and planning for health - care manage - ment . In a totally different type of application , planetary geologists sift through remotely sensed images of planets and asteroids , carefully locat - ing and cataloging geologic objects of interest , such as impact craters . For these ( and many other ) appli - cations , such manual probing of a dataset is slow , expensive , and highly subjective . In fact , such manual data analysis is becom - ing impractical in many domains as data volumes grow exponentially . Databases are increasing in size in two ways : the number N of records , or objects , in the data - base , and the number d of fields , or attributes , per object . Databases containing on the order of N = 10 9 objects are increasingly common in , for example , the astronomical sciences . The number d of fields can easi - ly be on the order of 10 2 or even 10 3 in medical diag - nostic applications . Who could be expected to digest billions of records , each with tens or hundreds of fields ? Yet the true value of such data lies in the users ' ability to extract use - ful reports , spot interesting events and trends , support decisions and policy based on statistical analysis and inference , and exploit the data to achieve business , opera - tional , or scientific goals . When the scale of data manip - ulation , exploration , and infer - ence grows beyond human capacities , people look to comput - er technology to automate the bookkeeping . The problem of knowledge extraction from large databases involves many steps , ranging from data manipulation and retrieval to fundamental mathematical and statistical infer - ence , search , and reasoning . Researchers and practitioners interested in these problems have been meeting since the first KDD Workshop in 1989 . Although the problem of extracting knowledge from data ( or observations ) is not new , automation in the context of large databases opens up many new unsolved problems . Definitions Finding useful patterns in data is known by different names ( includ - ing data mining ) in different com - munities ( e . g . , knowledge extraction , information discovery , information harvesting , data archeology , and data pattern pro - cessing ) . The term “data mining” is used most by statisticians , data - base researchers , and more recently by the MIS and business communities . Here we use the term “KDD” to refer to the overall process of discovering useful knowl - edge from data . Data mining is a particular step in this process—application of specific algorithms for extract - ing patterns ( models ) from data . The additional steps in the KDD process , such as data preparation , data selec - tion , data cleaning , incorporation of appropriate prior knowledge , and proper interpretation of the results of mining ensure that useful knowledge is derived from the data . Blind application of data mining methods The value of storing volumes of data depends on our ability to extract useful reports , spot interesting events and trends , support decisions and policy based on statistical analysis and inference , and exploit the data to achieve business , operational , or scientific goals . COMMUNICATIONS OF THE ACM November 1996 / Vol . 39 , No . 11 29 ( rightly criticized as data dredging in the statistical liter - ature ) can be a dangerous activity leading to discovery of meaningless patterns . KDD has evolved , and continues to evolve , from the intersection of research in such fields as databases , machine learning , pattern recognition , statistics , artifi - cial intelligence and reasoning with uncertainty , knowl - edge acquisition for expert systems , data visualization , machine discovery [ 7 ] , scientific discovery , information retrieval , and high - performance computing . KDD soft - ware systems incorporate theories , algorithms , and methods from all of these fields . Database theories and tools provide the necessary infrastructure to store , access , and manipulate data . Data warehousing , a recently popularized term , refers to the current business trend of collecting and cleaning transactional data to make them available for online analysis and decision support . A popular approach for analysis of data warehouses is called online analytical processing ( OLAP ) . 1 OLAP tools focus on providing multidimensional data analysis , which is superior to SQL ( a standard data manipulation language ) in com - puting summaries and breakdowns along many dimen - sions . While current OLAP tools target interactive data analysis , we expect they will also include more auto - mated discovery components in the near future . Fields concerned with inferring models from data— including statistical pattern recognition , applied statis - tics , machine learning , and neural networks—were the impetus for much early KDD work . KDD largely relies on methods from these fields to find patterns from data in the data mining step of the KDD process . A natural question is : How is KDD different from these other fields ? KDD focuses on the overall process of knowl - edge discovery from data , including how the data is stored and accessed , how algorithms can be scaled to massive datasets and still run efficiently , how results can be interpreted and visualized , and how the overall human - machine interaction can be modeled and sup - ported . KDD places a special empha - sis on finding understandable pat - terns that can be interpreted as useful or interesting knowledge . Scaling and robustness properties of modeling algorithms for large noisy datasets are also of fundamental interest . Statistics has much in common with KDD . Inference of knowledge from data has a fundamental statistical component ( see [ 2 ] and the article by Gly - mour on statistical inference in this special section for more detailed discussions of the relationship between KDD and statistics ) . Statistics provides a language and framework for quantifying the uncertainty resulting when one tries to infer general patterns from a partic - ular sample of an overall population . As mentioned earlier , the term data mining has had negative conno - tations in statistics since the 1960s , when computer - based data analysis techniques were first introduced . The concern arose over the fact that if one searches long enough in any dataset ( even randomly generated data ) , one can find patterns that appear to be statisti - cally significant but in fact are not . This issue is of fun - damental importance to KDD . There has been substantial progress in understanding such issues in sta - tistics in recent years , much directly relevant to KDD . Thus , data mining is a legitimate activity as long as one understands how to do it correctly . KDD can also be viewed as encompassing a broader view of modeling than statistics , aiming to provide tools to automate ( to the degree possible ) the entire process of data analysis , including the statistician’s art of hypothesis selection . The KDD Process Here we present our ( necessarily subjective ) perspec - tive of a unifying process - centric framework for KDD . The goal is to provide an overview of the variety of activ - Selection Pre - processing Trans - formation DataMining Interpretation / Evaluation Knowledge TargetData PreprocessedData TransformedData Patterns Data Figure 1 . Overview of the steps constituting the KDD process 1 See Providing OLAP to User Analysts : An IT Mandate by E . F . Codd and Associates ( 1993 ) . ities in this multidisciplinary field and how they fit together . We define the KDD process [ 4 ] as : The nontrivial process of identifying valid , novel , potentially useful , and ultimately understandable patterns in data . Throughout this article , the term pattern goes beyond its traditional sense to include models or structure in data . In this definition , data comprises a set of facts ( e . g . , cases in a database ) , and pattern is an expression in some language describing a subset of the data ( or a model applicable to that subset ) . The term process implies there are many steps involving data prepara - tion , search for patterns , knowledge evaluation , and refinement—all repeated in multiple iterations . The process is assumed to be nontrivial in that it goes beyond computing closed - form quantities ; that is , it must involve search for structure , models , patterns , or parameters . The discovered patterns should be valid for new data with some degree of cer - tainty . We also want patterns to be novel ( at least to the system , and preferably to the user ) and potentially useful for the user or task . Finally , the patterns should be understandable —if not immediately , then after some postprocessing . This definition implies we can define quantitative measures for eval - uating extracted patterns . In many cases , it is possible to define measures of certainty ( e . g . , estimated classifica - tion accuracy ) or utility ( e . g . , gain , perhaps in dollars saved due to better predictions or speed - up in a system’s response time ) . Such notions as nov - elty and understandability are much more subjective . In certain contexts , understandability can be estimated through simplicity ( e . g . , number of bits needed to describe a pattern ) . An important notion , called interesting - ness , is usually taken as an overall mea - sure of pattern value , combining validity , novelty , usefulness , and sim - plicity . Interestingness functions can be explicitly defined or can be mani - fested implicitly through an ordering placed by the KDD system on the dis - covered patterns or models . Data mining is a step in the KDD process consisting of an enumeration of patterns ( or models ) over the data , subject to some acceptable computational - efficiency limitations . Since the patterns enumerable over any finite dataset are potentially infinite , and because the enumeration of patterns involves some form of search in a large space , computational constraints place severe limits on the subspace that can be explored by a data mining algo - rithm . The KDD process is outlined in Figure 1 . ( We did not show all the possible arrows to indicate that loops can , and do , occur between any two steps in the process ; also not shown is the system ' s performance ele - ment , which uses knowledge to make decisions or take actions . ) The KDD process is interactive and iterative ( with many decisions made by the user ) , involving numerous steps , summarized as : 1 . Learning the application domain : includes relevant prior knowledge and the goals of the application 2 . Creating a target dataset : includes selecting a dataset or focusing on a subset of vari - ables or data samples on which discovery is to be performed 3 . Data cleaning and preprocessing : includes basic operations , such as removing noise or outliers if appropriate , collecting the neces - sary information to model or account for noise , deciding on strategies for handling missing data fields , and accounting for time sequence information and known changes , as well as decid - ing DBMS issues , such as data types , schema , and mapping of missing and unknown values 4 . Data reduction and projection : includes finding useful features to represent the data , depend - ing on the goal of the task , and using dimensionality reduction or transformation methods to reduce the effective number of variables under consideration or to find invariant representa - tions for the data 5 . Choosing the function of data mining : includes deciding the purpose of the model derived by the data mining algorithm ( e . g . , summarization , classifica 30 November 1996 / Vol . 39 , No . 11 COMMUNICATIONS OF THE ACM In practice , a large portion of the applications effort can go into properly formulating the problem ( asking the right question ) rather than optimizing the algorithmic details of a particular data mining method . COMMUNICATIONS OF THE ACM November 1996 / Vol . 39 , No . 11 31 tion , regression , and clustering ) 6 . Choosing the data mining algorithm ( s ) : includes selecting method ( s ) to be used for searching for patterns in the data , such as deciding which models and parameters may be appropriate ( e . g . , models for categorical data are different from models on vectors over reals ) and matching a particular data mining method with the overall criteria of the KDD process ( e . g . , the user may be more interested in understanding the model than in its predictive capabilities ) 7 . Data mining : includes searching for patterns of interest in a particular representational form or a set of such representations , including classification rules or trees , regression , clustering , sequence mod - eling , dependency , and line analysis 8 . Interpretation : includes interpreting the discovered patterns and possibly returning to any of the previ - ous steps , as well as possible visualization of the extracted patterns , removing redundant or irrele - vant patterns , and translating the useful ones into terms understandable by users 9 . Using discovered knowledge : includes incorporat - ing this knowledge into the performance system , taking actions based on the knowledge , or simply documenting it and reporting it to interested par - ties , as well as checking for and resolving potential conflicts with previously believed ( or extracted ) knowledge . Most previous work on KDD focused primarily on the data mining step . However , the other steps are equally if not more important for the successful appli - cation of KDD in practice . We now focus on the data mining component , which has received by far the most attention in the literature . Data Mining Data mining involves fitting models to or determining patterns from observed data . The fitted models play the role of inferred knowledge . Deciding whether or not the models reflect useful knowledge is a part of the overall interactive KDD process for which subjec - tive human judgment is usually required . A wide vari - ety and number of data mining algorithms are described in the literature—from the fields of statis - tics , pattern recognition , machine learning , and data - bases . Thus , an overview discussion can often consist of long lists of seemingly unrelated , and highly spe - cific algorithms . Here we take a somewhat reduction - ist viewpoint . Most data mining algorithms can be viewed as compositions of a few basic techniques and principles . In particular , data min - ing algorithms consist largely of some specific mix of three com - ponents : • The model . There are two rele - vant factors : the function of the model ( e . g . , classification and clustering ) and the representational form of the model ( e . g . , a linear function of multiple variables and a Gaussian probability den - sity function ) . A model contains parameters that are to be determined from the data . • The preference criterion . A basis for preference of one model or set of parameters over another , depending on the given data . The criterion is usual - ly some form of goodness - of - fit function of the model to the data , perhaps tempered by a smooth - ing term to avoid overfitting , or generating a model with too many degrees of freedom to be constrained by the given data . • The search algorithm . The specification of an algo - rithm for finding particular models and parameters , given data , a model ( or family of models ) , and a preference criterion . A particular data mining algorithm is usually an instantiation of the model / preference / search compo - nents ( e . g . , a classification model based on a decision - tree representation , model preference based on data likelihood , determined by greedy search using a partic - ular heuristic . Algorithms often differ largely in terms of the model representation ( e . g . , linear and hierarchi - cal ) , and model preference or search methods are often similar across different algorithms . The literature on learning algorithms frequently does not state clear - ly the model representation , preference criterion , or search method used ; these are often mixed up in a description of a particular algorithm . The reductionist view clarifies the independent contributions of each component . Model Functions The more common model functions in current data mining practice include : • Classification : maps ( or classifies ) a data item into one of several predefined categorical classes . • Regression : maps a data item to a real - value predic - tion variable . • Clustering : maps a data item into one of several cate - gorical classes ( or clusters ) in which the classes must be determined from the data—unlike classification in which the classes are predefined . Clusters are defined by finding natural groupings of data items based on similarity metrics or probability density models . • Summarization : provides a compact description for a subset of data . A simple example would be the mean and standard deviations for all fields . More sophisti - cated functions involve summary rules , multivariate visualization techniques , and functional relation - ships between variables . Summarization functions are often used in interactive exploratory data analy - sis and automated report generation . • Dependency modeling : describes significant depen - dencies among variables . Dependency models exist at two levels : structured and quantitative . The struc - tural level of the model specifies ( often in graphical form ) which variables are locally dependent ; the quantitative level specifies the strengths of the dependencies using some numerical scale . • Link analysis : determines relations between fields in the database ( e . g . , association rules [ 1 ] to describe which items are commonly purchased with other items in grocery stores ) . The focus is on deriving multi - field correlations satisfying sup - port and confidence thresholds . • Sequence analysis : models sequential patterns ( e . g . , in data with time dependence , such as time - series analysis ) . The goal is to model the states of the process generating the sequence or to extract and report deviation and trends over time . Model Representation Popular model representations include decision trees and rules , lin - ear models , nonlinear models ( e . g . , neural networks ) , example - based methods ( e . g . , nearest - neighbor and case - based reasoning meth - ods ) , probabilistic graphical depen - dency models ( e . g . , Bayesian networks [ 6 ] ) , and relational attribute models . Model representa - tion determines both the flexibility of the model in representing the data and the interpretability of the model in human terms . Typically , the more complex models may fit the data better but may also be more difficult to understand and to fit reli - ably . While researchers tend to advocate complex mod - els , practitioners involved in successful applications often use simpler models due to their robustness and interpretability [ 3 , 5 ] . Model preference criteria determine how well a par - ticular model and its parameters meet the criteria of the KDD process . Typically , there is an explicit quanti - tative criterion embedded in the search algorithm ( e . g . , the maximum likelihood criterion of finding the para - meters that maximize the probability of the observed data ) . Also , an implicit criterion ( reflecting the subjec - tive bias of the analyst in terms of which models are ini - tially chosen for consideration ) is often used in the outer loops of the KDD process . Search algorithms are of two types : parameter search , given a model , and model search over model space . Finding the best parameters is often reduced to an optimization problem ( e . g . , finding the global max - imum of a nonlinear function in parameter space ) . Data mining algorithms tend to rely on relatively simple optimiza - tion techniques ( e . g . , gradient descent ) , although in principle more sophisticated optimization techniques are also used . Problems with local minima are common and dealt with in the usual manner ( e . g . , multiple random restarts and searching for multiple models ) . Search over model space is usually carried out in a greedy fashion . A brief review of specific popu - lar data mining algorithms can be found in [ 4 , 5 ] . An important point is that each technique typically suits some problems better than others . For example , decision - tree classi - fiers can be very useful for finding structure in high - dimensional spaces and are also useful in prob - lems with mixed continuous and categorical data ( since tree meth - ods do not require distance met - rics ) . However , classification trees with univariate threshold decision boundaries may not be suitable for problems where the true decision boundaries are nonlinear multi - variate functions . Thus , there is no universally best data mining 32 November 1996 / Vol . 39 , No . 11 COMMUNICATIONS OF THE ACM Researchers and practitioners should ensure that the potential contributions of KDD are not overstated and that users understand the true nature of those contributions along with their limitations . COMMUNICATIONS OF THE ACM November 1996 / Vol . 39 , No . 11 33 method ; choosing a particular algorithm for a particu - lar application is something of an art . In practice , a large portion of the applications effort can go into properly formulating the problem ( asking the right question ) rather than into optimizing the algorithmic details of a particular data mining method . The high - level goals of data mining tend to be pre - dictive , descriptive , or a combination of predictive and descriptive . A purely predictive goal focuses on accura - cy in predictive ability . A purely descriptive goal focuses on understanding the underlying data - generating process—a subtle but important distinction . In predic - tion , a user may not care whether the model reflects reality as long as it has predictive power ( e . g . , a model combining current financial indicators in some nonlin - ear manner to predict future dollar - to - deutsche - mark exchange rates ) . A descriptive model , on the other hand , is interpreted as a reflection of reality ( e . g . , a model relating economic and demographic variables to educational achievements used as the basis for social policy recommendations to cause change ) . In practice , most KDD applications demand some degree of both predictive and descriptive modeling . Research Issues and Challenges Current primary research and application challenges for KDD [ 4 , 5 ] include : • Massive datasets and high dimensionality . Multigiga - byte databases with millions of records and large numbers of fields ( attributes and variables ) are com - monplace . These datasets create combinatorially explosive search spaces for model induction and increase the chances that a data mining algorithm will find spurious patterns that are not generally valid . Possible solutions include very efficient algo - rithms , sampling , approximation methods , massively parallel processing , dimensionality reduction tech - niques , and incorporation of prior knowledge . • User interaction and prior knowledge . An analyst is usually not a KDD expert but a person responsible for making sense of the data using available KDD techniques . Since the KDD process is by definition interactive and iterative , it is a challenge to provide a high - performance , rapid - response environment that also assists users in the proper selection and match - ing of appropriate tools and techniques to achieve their goals . There needs to be more emphasis on human - computer interaction and less emphasis on total automation—with the aim of supporting both expert and novice users . Many current KDD methods and tools are not truly interactive and do not easily incorporate prior knowledge about a problem except in simple ways . Use of domain knowledge is impor - tant in all steps of the KDD process . For example , Bayesian approaches use prior probabilities over data and distributions as one way of encoding prior knowledge ( see [ 6 ] and Glymour ' s article on statisti - cal inference in this special section ) . Others employ deductive database capabilities to discover knowledge that is then used to guide the data mining search . • Overfitting and assessing statistical significance . When an algorithm searches for the best parameters for one particular model using a limited set of data , it may overfit the data , resulting in poor perfor - mance of the model on test data . Possible solutions include cross - validation , regularization , and other sophisticated statistical strategies . Proper assessment of statistical significance is often missed when the system searches many possible models . Simple meth - ods to handle this problem include adjusting the test statistic as a function of the search ( e . g . , Bonfer - roni adjustments for independent tests ) and ran - domization testing , although this area is largely unexplored . • Missing data . This problem is especially acute in business databases . Important attributes may be missing if the database was not designed with discov - ery in mind . Missing data can result from operator error , actual system and measurement failures , or from a revision of the data collection process over time ( e . g . , new variables are measured , but they were considered unimportant a few months before ) . Possible solutions include more sophisticated statisti - cal strategies to identify hidden variables and depen - dencies . • Understandability of patterns . In many applications , it is important to make the discoveries more under - standable by humans . Possible solutions include graphical representations , rule structuring , natural language generation , and techniques for visualiza - tion of data and knowledge . Rule refinement strate - gies can also help address a related problem : Discovered knowledge may be implicitly or explicitly redundant . • Managing changing data and knowledge . Rapidly changing ( nonstationary ) data may make previously discovered patterns invalid . In addition , the variables measured in a given application database may be modified , deleted , or augmented with new measure - ments over time . Possible solutions include incre - mental methods for updating the patterns and treating change as an opportunity for discovery by using it to cue the search for patterns of change . • Integration . A standalone discovery system may not be very useful . Typical integration issues include integration with a DBMS ( e . g . , via a query interface ) , integration with spreadsheets and visualization tools , and accommodation of real - time sensor readings . Highly interactive human - computer environments as outlined by the KDD process permit both human - assisted computer discovery and computer - assisted human discovery . Development of tools for visualiza - tion , interpretation , and analysis of discovered pat - terns is of paramount importance . Such interactive environments can enable practical solutions to many real - world problems far more rapidly than humans or computers operating independently . There are a potential opportunity and a challenge to developing techniques to integrate the OLAP tools of the data - base community and the data mining tools of the machine learning and statistical communities . • Nonstandard , multimedia , and object - oriented data . A significant trend is that databases contain not just numeric data but large quantities of nonstandard and multimedia data . Nonstandard data types include nonnumeric , nontextual , geometric , and graphical data , as well as nonstationary , temporal , spatial , and relational data , and a mixture of cate - gorical and numeric fields in the data . Multimedia data include free - form multilingual text as well as digitized images , video , and speech and audio data . These data types are largely beyond the scope of cur - rent KDD technology . Conclusions Despite its rapid growth , the KDD field is still in its infancy . There are many challenges to overcome , but some successes have been achieved ( see the articles by Brachman on business applications and by Fayyad on science applications in this special section ) . Because the potential payoffs of KDD applications are high , there has been a rush to offer products and services in the market . A great challenge facing the field is how to avoid the kind of false expectations plaguing other nascent ( and related ) technologies ( e . g . , artificial intel - ligence and neural networks ) . It is the responsibility of researchers and practitioners in this field to ensure that the potential contributions of KDD are not overstated and that users understand the true nature of the con - tributions along with their limitations . Fundamental problems at the heart of the field remain unsolved . For example , the basic problems of statistical inference and discovery remain as difficult and challenging as they always have been . Capturing the art of analysis and the ability of the human brain to synthesize new knowledge from data is still unsurpassed by any machine . However , the volumes of data to be analyzed make machines a necessity . This niche for using machines as an aid to analysis and the hope that the massive datasets contain nuggets of valuable knowl - edge drive interest and research in the field . Bringing together a set of varied fields , KDD creates fertile ground for the growth of new tools for managing , ana - lyzing , and eventually gaining the upper hand over the flood of data facing modern society . The fact that the field is driven by strong social and economic needs is the impetus to its continued growth . The reality check of real applications will act as a filter to sift the good the - ories and techniques from those less useful . References 1 . Agrawal , R . , Mannila , H . , Srikant , R . , Toivonen , H . , and Verkamo , I . Fast discovery of association rules . In Advances in Knowledge Discovery and Data Mining , U . Fayyad , G . Piatetsky - Shapiro , P . Smyth , and R . Uthurusamy , Eds . AAAI / MIT Press , Cambridge , Mass . , 1996 . 2 . Elder , J . , and Pregibon , D . A statistical perspective on KDD . In Advances in Knowledge Discovery and Data Mining , U . Fayyad , G . Piatetsky - Shapiro , P . Smyth , and R . Uthurusamy , Eds . AAAI / MIT Press , Cambridge , Mass . , 1996 . 3 . Fayyad , U . , and Uthurusamy , R . , Eds . Proceedings of KDD - 95 : The First International Conference on Knowledge Discovery and Data Min - ing . AAAI Press , Menlo Park , Calif . , 1995 . 4 . Fayyad , U . , Piatetsky - Shapiro , G . , and Smyth , P . From data mining to knowledge discovery : An overview . In Advances in Knowledge Dis - covery and Data Mining , U . Fayyad , G . Piatetsky - Shapiro , P . Smyth , and R . Uthurusamy , Eds . AAAI / MIT Press , Cambridge , Mass . , 1996 . 5 . Fayyad , U . , Piatetsky - Shapiro , G . , Smyth , P . , and Uthurusamy , R . , Eds . Advances in Knowledge Discovery and Data Mining , AAAI / MIT Press , Cambridge , Mass . , 1996 . 6 . Heckerman , D . Bayesian networks for knowledge discovery . Advances in Knowledge Discovery and Data Mining , U . Fayyad , G . Piatetsky - Shapiro , P . Smyth , and R . Uthurusamy , Eds . AAAI / MIT Press , Cambridge , Mass . , 1996 . 7 . Langley , P . , and Simon , H . A . Applications of machine learning and rule induction . Commun . ACM 38 , 11 ( Nov . 1995 ) , 55 – 64 . Additional references for this article can be found at http : / / www . research . microsoft . com / research / datamine / CACM - DM - refs / . USAMA FAYYAD is a senior researcher at Microsoft Research and a distinguished visiting scientist at the Jet Propulsion Laboratory of the California Institute of Technology . He can be reached at fayyad @ microsoft . com . GREGORY PIATETSKY - SHAPIRO is a principal member of the technical staff at GTE Laboratories . He can be reached at gps @ gte . com . PADHRAIC SMYTH is an assistant professor of computer science at the University of California , Irvine , and technical group leader at the Jet Propulsion Laboratory of the California Institute of Technology . He can be reached at smyth @ ics . uci . edu . Permission to make digital / hard copy of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage , the copyright notice , the title of the publication and its date appear , and notice is given that copying is by permission of ACM , Inc . To copy otherwise , to republish , to post on servers , or to redistribute to lists requires prior specific permission and / or a fee . © ACM 0002 - 0782 / 96 / 1100 $ 3 . 50 C 34 November 1996 / Vol . 39 , No . 11 COMMUNICATIONS OF THE ACM