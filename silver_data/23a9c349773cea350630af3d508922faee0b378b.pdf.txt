EmoG : Supporting the Sketching of Emotional Expressions for Storyboarding Yang Shi Nan Cao Xiaojuan Ma iDVX Lab , Tongji University iDVX Lab , Tongji University Hong Kong University of yangshi . idvx @ tongji . edu . cn nan . cao @ gmail . com Science and Technology Siji Chen iDVX Lab , Tongji University sijichen . idvx @ gmail . com ABSTRACT Storyboarding is an important ideation technique that uses sequential art to depict important scenarios of user experience . Existing data - driven support for storyboarding focuses on con­ structing user stories , but fail to address its beneﬁt as a graphic narrative device . Instead , we propose to develop a data - driven design support tool that increases the expressiveness of user stories by facilitating sketching storyboards . To explore this , we focus on supporting the sketching of emotional expressions of characters in storyboards . In this paper , we present EmoG , an interactive system that generates sketches of characters with emotional expressions based on input strokes from the user . We evaluated EmoG with 21 participants in a controlled user study . The results showed that our tool has signiﬁcantly better performance in usefulness , ease of use , and quality of results than the baseline system . Author Keywords Storyboarding ; Creativity Support Tools ; Data - Driven Design ; Emotional Expression Generation CCS Concepts • Human - centered computing → Interactive systems and tools ; • Applied computing → Arts and humanities ; • Computing methodologies → Neural networks ; INTRODUCTION Storyboarding , as an ideation technique , uses sequential art to depict essential moments of the user experience [ 29 ] . It is widely used to facilitate the explanation and exploration of a yet - to - be - designed product / service , aiming to deliver its intended beneﬁts to users [ 36 ] . While the recent advances in deep learning have opened a ﬂoodgate of creativity support Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior speciﬁc permission and / or a fee . Request permissions from permissions @ acm . org . CHI ’20 , April 25 – 30 , 2020 , Honolulu , HI , USA . Copyright is held by the owner / author ( s ) . Publication rights licensed to ACM . ACM ISBN 978 - 1 - 4503 - 6708 - 0 / 20 / 04 . . . $ 15 . 00 . http : / / dx . doi . org / 10 . 1145 / 3313831 . 3376520 mxj @ cse . ust . hk Pei Liu iDVX Lab , Tongji University peiliu . idvx @ gmail . com tools for design ideation , most of that interest has transferred to brainstorming to support the collection , curation , and pre­ sentation of ideas [ 14 , 17 , 30 ] . Storyboarding [ 6 , 12 , 13 , 20 ] , on the other hand , has received limited attention in the Human Computer Interaction ( HCI ) community . Prior research in data - driven support for storyboarding has focused on constructing user stories , such as mining user com­ ments to identify pain points [ 34 ] or generating storyboards for a mobile app to present its activity transition graph [ 4 ] . How­ ever , few attempts have been made to facilitate sketching user stories for storyboarding . For designers who use storyboards as a graphic narrative device , sketching plays an important role in increasing the expressiveness of user stories . Thus , we can improve storyboarding tools by support sketching the essential visual elements of storyboards , whose expressive­ ness is the primary concern for designers [ 24 ] . We focus on the characters’ emotional experience in storyboards , which conveys envisioned user attitudes , expectations , and motiva­ tions towards the proposed design in addition to characters’ physical activities [ 33 ] . Well - drawn expressions particularly allow readers to better empathize with the characters [ 35 ] . Rendering emotions in an aesthetically pleasing and expres­ sive manner is challenging , especially for designers who are not skilled at drawing [ 22 ] . In this case , Artiﬁcial Intelligence ( AI ) can be leveraged to augment human creative capabilities by suggesting visual materials as inspiration or even generat­ ing desired results [ 1 , 14 , 17 , 26 ] . In this paper , we present EmoG , a data - driven design support tool that facilitates sketch­ ing Emo tional expressions for storyboarding using a deep­ learning - based expression G eneration approach . As a user draws a neutral face of an intended character , EmoG can sug­ gest potential character designs based on input strokes from the user and generate new sketches of the character with six basic expressions . The system also allows the user to interac­ tively specify the type and intensity of expression , the gender of the character , and viewing angle . Our approach to gener­ ating these sketches requires a stroke - based sketch dataset as the training set . To this end , we collected the ﬁrst large - scale dataset consisting of over 200K original face sketches drawn by experienced designers . This dataset allows us to adopt a CHI 2020 Paper CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Paper 393 Page 1 deep learning model , AI - Skecther [ 3 ] , to learn the sequences of strokes and generate high - quality sketches of emotional expressions . To evaluate the effectiveness of EmoG , we conducted a con­ trolled user study with 21 designers . EmoG is compared to a baseline system , Freehand , which uses the default mode of EmoG without any AI - related function . The designers leveraged these systems to sketch emotional expressions for storyboarding based on the storylines we provided to them . To further compare the quality of the results supported by Free­ hand and EmoG , we recruited 20 external judges and asked them to rate the results . By analyzing quantitative and qualita­ tive results collected from the user study , we discuss design implications on developing data - driven design support tools to provide better means of facilitating ideation activities . The main contributions of this work include : • We designed EmoG , an interactive data - driven design sup­ port tool for sketching the character’s emotional expressions in a storyboard . • We collected the ﬁrst large - scale dataset of face sketches with expressions , which is used as the training set for our EmoG system . • We conducted a user study with 21 designers . The results showed that EmoG has signiﬁcantly better performance in usefulness , ease of use , quality of results than Freehand . RELATED WORK Our work builds on prior research on data - driven creativity support tools for design , face sketch datasets , and emotional expression generation algorithms . Data - Driven Creativity Support Tools for Design The recent advances in data - driven technologies facilitate the development of creativity support tools for design such as fashion design [ 19 ] and user interface design [ 14 ] . These data­ driven creativity support tools focus on different parts of the creative process such as design ideation or the realization of the creative outcome [ 8 ] . To support design ideation , IdeaWall [ 30 ] extracts essential information from human discussion and introduces relevant web - search material to trigger more ideas . Koch et al . [ 17 ] designed an interactive digital mood board that collects and cu­ rates visual inspirational materials . Data - driven technologies also shed light on the application of novel interaction modal­ ities to design ideation . Swire [ 14 ] enables users to retrieve design examples from user interface datasets using sketches . Most aforementioned work supports brainstorming in design ideation , helping collect , curate , and present ideas . Our work focus on storyboarding in ideation by providing suggestions for character design and generating emotional expressions for the character . In the creative process of realization , many data - driven inter­ active systems have been introduced for sketching and draw­ ing . For example , DrawFromDrawings [ 23 ] assists users in 2D drawings by referring to a sketch image in its dataset . DualDraw [ 26 ] assists in human - AI co - creative drawing by duplicating , mimicking , and supplementing users’ sketches . Smart Inker [ 32 ] provides an interactive method for users to transform a rough sketch into a clean line drawing . Based on convolutional neural networks ( CNN ) , it allows users to con­ nect lines , erase shading , and adjust the line drawing . Zhang et al . [ 41 ] introduced a semi - automatic two - stage framework to help users colorize sketches with color , texture , and gradient . The drafting stage creates a color draft while the reﬁnement stage corrects color mistakes and artifacts to achieve the ﬁnal result . Our work is inﬂuenced by data - driven design tools de­ scribed above . Speciﬁcally , we focus on assisting designers in drawing sketches of emotional expressions for storyboarding . To do this , our work applies techniques used in the realization that assist drawing to design ideation . Face Sketch Datasets With the increasing popularity of deep learning and crowd­ sourcing , a number of large - scale face sketch datasets have been constructed to facilitate data - driven design applications . The TU - Berlin sketch dataset contains 20 , 000 crowdsourced sketches in 250 categories , while each category contains 80 sketches . Sketches in this dataset were collected through Ama­ zon Mechanical Turk ( AMT ) . Categories related to human face include ear , eye , eyeglasses , head , mouth , and nose [ 7 ] . The Quick , Draw ! dataset [ 16 ] consists of 50 million sketches across 345 categories . Sketches in this dataset were collected through an online game requiring participants to draw an indi­ cated object in 20 seconds . As a result , in the face category , which contains 148 , 436 drawings , a majority of them present oversimpliﬁed facial features with limited expressions . The CUFS dataset collects pencil - drawing sketches created by artists based on a photo taken in a frontal view and with a neutral expression . It contains 606 pixel - based sketches that depict levels of details such as shading [ 38 ] . While the afore­ mentioned datasets have been shown to be helpful in many data - driven design tools , to the best of our knowledge , no large - scale datasets of high - quality face sketches are currently available , especially in a vector format . To ensure the qual­ ity of the generative capabilities of our system , we created a stroke - based dataset by recruiting a group of professional designers to draw face sketches . The sketches in our dataset show different attributes including genders , facial features , viewing angles , and emotions . Emotional Expression Generation Algorithms Previous research in emotional expression generation primar­ ily uses two approaches , including computer graphics - based and deep - learning - based generation . Computer graphics­ based approaches adopt image manipulations and geomet­ ric manipulations to generate facial expression images [ 18 ] . For example , Zhang et al . [ 42 ] proposed a facial expression generation algorithm based on geometric warping . In their approach , a muscle - distribution based model is used to clone facial expressions from a neutral face . As for deep - learning - based approaches , most of them exploit generative models such as generative adversarial network ( GAN ) [ 9 ] . For example , ExprGAN [ 5 ] is introduced for CHI 2020 Paper CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Paper 393 Page 2 realistic facial expression editing . It allows users to synthe­ size different emotional expressions and adjust expression intensities . D - GAN [ 10 ] , a differential generative adversar­ ial network , can generate face images with conditional labels using dual discriminators . Wang et al . [ 37 ] proposed U - net Conditional GAN for facial expression transferring . It imposes a U - net structure and an identity - preserving constraint to main­ tain the property of input faces . Pumarola et al . [ 28 ] presented GANimation , an action unit ( AU ) based GAN , which con­ tinuously controls the generation of various expressions by automatically tuning the activation value of AU . Compared with the aforementioned pixel image modeling approaches , we exploit AI - Sketcher [ 3 ] which extends Sketch - RNN [ 11 ] and uses a CNN - based autoencoder to capture the positional information of each stroke , an inﬂuence layer to guide the generation of each stroke , and a conditional vector to facilitate multi - class sketch generation . In our work , we apply AI - Sketcher to emotional expression generation and integrate it into our interactive system . The reason is that AI - Sketcher is a state - of - the - art deep learning model that is capable of learning the sequences of strokes and generating high - quality multi - class sketches . NEEDFINDING STUDY To understand designers’ current practice of drawing charac­ ters’ expressions for storyboarding , we conducted a needﬁnd­ ing study with four designers ( two product designers , an in­ teraction designer , and a service designer ) . We ﬁrst asked the designers to present the storyboards they had worked on . We then conducted a series of interviews with the designers . In each interview , we asked them about ( 1 ) what workﬂows they would use for depicting characters’ emotional experience , ( 2 ) what challenges they would want to address when rendering emotions , and ( 3 ) what potential functions they would add to the current tools to facilitate sketching expressions . Each presentation and interview session lasted for 1 . 5 - 2 hours . Challenges and Design Requirements Our study reveals some challenges designers face when using current vector / raster graphics editor tools such as Adobe Illus­ trator and Procreate for storyboarding . Once the storytelling is decided , designers start with designing the character of the story . They collect design inspirations by exploring online materials . However , it is not an easy task to create a satisfying and original character within a limited time frame . For exam­ ple , one designer said , “ I usually spend a lot of time searching for design examples , but even with these examples I might get stuck when I’m drawing . The reason is that facial features are with different anatomy and complexity , it’s difﬁcult to make everything look great . ” Existing tools do not address such user requirements of providing inspirations and suggestions relevant to the designers’ preference . After creating the character , designers render his or her emo­ tions in each frame of the storyboard . We found that they often use a limited number of salient emotional features for depicting expressions . For example , one designer perceives lip corners as the only feature to differentiate between happy and sad . However , prior research suggests that other facial fea­ tures such as eyebrows and eyes can also be included to better communicate emotions [ 22 ] . “ Guidelines for illustrating emo­ tions in storyboards are desirable . It’ll be even better if future systems can automatically sketch the character with various expressions for me ” . Overall , our study identiﬁed a need for including salient features to generate sketches of expressions that effectively communicate intended emotions . We observed that the designers drew emotional expressions with different viewing angles to accommodate speciﬁc sce­ narios . They reported that existing tools “ lack the capability to intelligently add more proﬁle views when users draw an expression in a frontal view ” . Also , when asked about poten­ tial functions of automated systems , three of the designers suggested that generating emotional expressions of varying intensity ( e . g . , mild , intense ) could be an interesting feature . One designer explained , “ different intensities can offer more convincing representations of the user experience . ” By extracting the challenges derived from the study , we identi­ ﬁed the following design requirements : DR1 Suggesting face sketches based on input strokes from the designer to provide inspirations for character creation . DR2 Generating face sketches of the character with emotional expressions to help communicate the intended emotions . DR3 Allowing the designer to specify the attributes of gener­ ation to meet different design requirements . EMOTIONAL EXPRESSION SKETCH DATASET Our approach to generating sketches satisfying the design re­ quirements ( DR1 - DR3 ) requires a stroke - based sketch dataset as the training set . To the best of our knowledge , no large - scale datasets of high - quality face sketches are currently available , especially in a vector format . To create such dataset , we re­ cruited a group of designers to draw face sketches following speciﬁc design criteria . The sketch dataset is of more than 200K original sketches and is released to support the develop­ ment of future data - driven design support tools . The dataset is available at https : / / facex . idvxlab . com . Designer Recruitment We recruited ﬁve designers via freelance websites . Our recruit­ ment material indicated that we were looking for designers who received degrees in design and have drawing and sketch­ ing experience . We also checked their sketching portfolio to Figure 1 : Sketch examples of eyebrows , eyes , noses , and mouths drawn by the designers . CHI 2020 Paper CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Paper 393 Page 3 ensure that they have diverse drawing styles . Each designer worked for 480 - 560 hours over three months and was compen­ sated 12 . 5 USD per hour . Data Collection To create the face sketch dataset , we ﬁrst identiﬁed that sketches in our dataset should have various genders , facial features , viewing angles , and emotions after analyzing the 16 storyboards and feedback collected from the needﬁnding study . Due to limited resources , we did not attempt to cover all examples exhibited in these storyboards . Based on a litera­ ture review [ 21 , 22 , 31 ] , we summarized essential attributes that our dataset should include , including two genders ( male , female ) , four facial features ( eyebrows , eyes , noses , mouths ) , three viewing angles ( frontal , mid - proﬁle left , mid - proﬁle right views ) , and seven emotions ( neutral , happy , sad , angry , fearful , surprised , disgusted ) . Note that our categorization is not meant to be exclusive but to provide an initial framework that encourages future contributions from more practitioners and researchers . Next , we asked the ﬁve designers to draw sketches of four facial features with the identiﬁed attributes ( gender , viewing angle , emotion ) . The designers started with sketching one fa­ cial feature with a neutral expression in a frontal view . During the process , two researchers with design - related background independently examined these sketches to ensure their aes­ thetics and uniqueness . Following the guidelines of drawing eyebrows , eyes , noses , and mouths [ 2 ] , the two researchers independently checked if each facial feature drawn by the designers accord with the basic shape , scale , and proportion to ensure aesthetics . In terms of uniqueness , two researchers independently compared a newly ﬁnished design with the sketches in the dataset to ensure that it has a unique appear­ ance . If both of the two researchers think the new one looks different from the existing ones , it will be added to the dataset . If one of them disagrees , the sketch will be returned to the designer for revision . The whole process terminated when the designers could not create any new unique design . We collected 105 pairs of eyebrows designs ( male : 52 , female : 53 ) , 96 pairs of eyes designs ( male : 48 , female : 48 ) , 86 noses ( male : 43 , female : 43 ) , and 98 mouths ( male : 49 , female : 49 ) , as shown in Figure 1 . Then , the designers sketched more expressions and viewing angles of each feature design , result­ ing in 21 sketches in total for one design ( 7 emotions and 3 viewing angles ) . To ensure the expressiveness of sketches gen­ erated by our approach , we summarized the salient features for depicting six basic emotions based on a literature survey [ 22 , 31 ] ( Figure 2 ) . Our ﬁve designers referred to these guidelines when sketching different facial features to communicate spe­ ciﬁc emotions . For example , a wrinkled nose is often used by designers to convey the emotion of disgust . Each of these facial features was created with stylus and tablet in Adobe Illustrator in 32 . 6 minutes on average . Finally , our program created all possible combinations [ eye­ brows , eyes , nose , mouth ] that form face sketches and placed the four facial features according to the golden ratio of the human head [ 21 ] ( Figure 3 ) . We followed this strategy of placement because the human heads conforming to the golden ratio are perceived as aesthetically pleasing [ 27 ] . For example , the head is three and a half units high . The eyes are approx­ imately halfway down the head . We also let the designers identify faces of which the four features are of compatible drawing styles , resulting in 249 , 528 original designs . In total , the dataset contains 5 , 240 , 088 sketches with 7 emotions and 3 viewing angles . EMOG SYSTEM DESCRIPTION In this section , we introduce EmoG , a data - driven storyboard­ ing tool that suggests potential character designs based on input strokes from the user and generates new sketches of the character with expressions . We describe how each of EmoG’s components addresses the design requirements ( DR1 - DR3 ) presented in Section 3 , along with details on user interface , interaction , and algorithm . Figure 2 : Facial expressions of six basic emotions . The text Figure 3 : ( a ) The golden ratio of the human face . The blocks with a speciﬁc background color is related to one facial feature with light gray background indicate the approximate areas of ( light green : eyebrows , light blue : eye , dark blue : nose , light four facial features , eyebrows , eyes , nose , and mouth , from gray : mouth ) . top to bottom . ( b ) Examples of ﬁnal results from our dataset . CHI 2020 Paper CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Paper 393 Page 4 Figure 4 : The user interface of EmoG contains two pages , ( a ) Character Creation Page and ( b ) Storyboard Creation Page . The Character Creation Page has ( 1 ) Tools Panel , ( 2 ) Drawing Canvas , and ( 3 ) Suggestion Panel . The Storyboard Creation Page has ( 1 ) Tools Panel , ( 2 ) Drawing Canvas , ( 4 ) Options Panel , ( 5 ) Control Panel , ( 6 ) Script Panel , and ( 7 ) Navigation Panel . User Interface The user interface of EmoG is composed of two pages , the Character Creation Page ( Figure 4 ( a ) ) and the Storyboard Creation Page ( Figure 4 ( b ) ) . The designer ﬁrst draws a neutral face of an intended character in the Character Creation Page . Then he or she draws a storyboard using a sequence of frames in the Storyboard Creation Page . In both pages , EmoG provides the Tools Panel ( Figure 4 ( a ) : 1 , ( b ) : 1 ) and the Drawing Canvas ( Figure 4 ( a ) : 2 , ( b ) : 2 ) . The Tools Panel ( Figure 4 ( b ) : 1 ) at the top contains brush , eraser , move , text box , and trash ( from left to right ) , supporting creating and editing sketches in the Drawing Canvas . In the Character Creation Page , the Suggestion Panel ( Figure 4 ( a ) : 3 ) displays sketches suggested by the system . In the Story­ board Creation Page , the Options Panel ( Figure 4 ( b ) : 4 ) lists options for specifying the character’s expressions ( viewing angle , expression type ) . The Control Panel ( Figure 4 ( b ) : 5 ) provides controls ( intensity , rotation , scale ) for the selected expression . The Script Panel ( Figure 4 ( b ) : 6 ) allows the user to type in the title and script of each frame . The Navigation Panel ( Figure 4 ( b ) : 7 ) at the bottom offers a synthetic view of the storyboard and supports creating and deleting frames . EmoG in Action We present how EmoG works in action through a usage sce­ nario . This scenario highlights the key interactions and func­ tions of EmoG by illustrating how a designer sketches emo­ tional expressions of a character in a storyboard . Suggestions ( DR1 ) Imagine that the user experience ( UX ) designer Alex is going to pitch a new product , a GPS pet tracker , to his team . He uses EmoG to draw a storyboard for the purpose of effectively connecting the team to customer pain points . To start with , Alex creates a character and sketches a neutral face of the character in the Drawing Canvas ( Figure 4 ( a ) : 2 ) . EmoG provides two default templates of character head ( male , fe­ male ) to regulate the position of facial expression . He can draw and erase strokes , or clear the canvas if necessary . In this process , EmoG suggests face sketches in real - time based on the strokes Alex has drawn . The suggestions are displayed in the Suggestion Panel to provide inspirations ( Figure 4 ( a ) : 3 ) . Alex can also click the “Show More” button in this panel to update and then explore another set of suggestions . If Alex observes a favored design , he can use it as the character of the user story . Otherwise , he continues drawing until a satisfactory result is reached . Once it is completed , he can click the “ Go to Scenario” button and jumps to the next page . Automatic Generation ( DR2 ) In the Storyboard Creation Page , Alex can write scripts in the Script Panel ( Figure 4 ( b ) : 6 ) and draws a sequence of frames accordingly . The character he has created is displayed at the top of the Options Panel for his reference ( Figure 4 ( b ) : 4 ) . The bottom half of the Options Panel shows a list of the character’s expressions generated by the system , with options of three viewing angles ( frontal , mid - proﬁle left , and mid - proﬁle right view ) and six basic emotions ( happy , sad , angry , fearful , surprised , and disgusted ) . Alex can browse these expressions and click to preview them on the canvas . Speciﬁcation ( DR3 ) Based on the script , Alex selects the sad expression in a frontal view and inserts it into the canvas . Next , he ﬁne - tunes the expression using the Control Panel ( Figure 4 ( b ) : 5 ) . He ﬁrst scales it up to tightly frame the character and tweaks the expression intensity . He ﬁnds that the expression of intense sadness can better convey the character’s emotional experience at the moment . By using the text box tool , he adds a thought bubble to reveal what is going on in the character’s mind . To better align with the script , Alex uses the brush to draw more CHI 2020 Paper CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Paper 393 Page 5 Figure 5 : Schematic diagram of the expression generation algorithm . tears rolling down the character’s cheek . After ﬁnishing the ﬁrst frame , Alex right - clicks the Navigation Panel ( Figure 4 ( b ) : 7 ) ) to create a new frame . Emotional Expression Generation Algorithm Given a few strokes of a neutral face , the objective of the algorithm is to synthesize high - quality sketches of the same face with multiple emotional expressions . To this end , we use AI - Sketcher [ 3 ] , a state - of - the - art deep learning model that is capable of learning the sequences of strokes , as the backend of EmoG . It consists of two components : ( 1 ) an enhanced conditional variation autoencoder ( VAE ) framework that uses semantic and sequential information of input strokes to support multi - class expression generation ( Figure 5 ( 1 ) ) and ( 2 ) a CNN - based autoencoder module that captures the positional information of input strokes to improve the quality in hand - drawn sketches ( Figure 5 ( 2 ) ) . To generate multi - class expressions of an input sketch illus­ trating a neutral face , we use three types of information as the input of the algorithm , including the semantic ( expression type ) , positional ( relative positions of strokes ) , and sequential ( drawing sequences of strokes ) information of input strokes . The semantic information is labeled by the conditional vector vvv c 1 . vvv c 1 is a l - dimensional one - hot vector representing the input expression type and l indicates the number of expres­ sion types that can be generated . To obtain sequential and positional information , the input sketch is converted into two formats : a sequence of stroke vectors S S S v and a raster image matrix S S S r , respectively . Then , the stroke vectors S S S v are fed into a bidirectional RNN­ based encoder and projected into hidden nodes containing the features of the input strokes ( Figure 5 ( 1 ) ) . Through an inﬂuence layer , all the hidden nodes are used to enhance the inﬂuence of the input sketch on the decoding process and thus better guide the generation of each stroke . The hidden nodes are transformed into a normal distribution , from which a stroke sequence vector vvv f is sampled for decoding . Similarly , the last node vector of the hidden nodes is used and concatenated with the conditional vector vvv c 1 to produce the stroke feature vector vvv s for decoding . Figure 5 ( 2 ) shows that the raster image matrix SSS r is fed into the CNN - based autoencoder module and projected into the image feature vector vvv r , which captures relative positions of strokes in sketches . To control the expression type of the output , the framework uses another conditional vector vvv c 2 . vvv c 2 is a l - dimensional one - hot vector indicating the expression type to be generated . These four vectors , vvv c 2 , vvv f , vvv s , and vvv r , are concatenated into a latent vector vvv . vvv and the last stroke point ppp i is fed into the decoder at each decoding step i . The output of the decoder is fed into a fully - connected ( FC ) layer and is further transformed into the parameters P of a Gaussian mixture model ( GMM ) to predict the next stroke point ppp i + 1 . Here , the GMM is trained using our sketch dataset . Overall , the output sketch contains features that are similar to both the input sketch and sample data from the training set . Expression Intensity To support adjusting the intensity of expressions , we modi­ ﬁed the algorithm by tuning the conditional vector vvv c 2 . Since each ﬁeld in vvv c 2 represents an expression type , we can tune the value of the corresponding ﬁeld to change the intensity of the expression . Speciﬁcally , if the value is close to zero , the generated expression will be mild . Otherwise , the gener­ ated expression will be intense . Thus , EmoG can generate sketches of expressions with two different intensities ( mild , intense ) without requiring the training samples of both inten­ sities . Note that in our training set , only face sketches with intense expressions are provided . USER STUDY To evaluate the effectiveness of EmoG , we conducted a within­ subject user study . In this study , we compare EmoG ( with - AI ) to a baseline with the same functionality ( without - AI ) . To this end , we designed the baseline system , Freehand . In Freehand , participants draw a face of an intended character and his / her emotional expressions only using the Tools Panel without the assistance provided by AI ( Figure 4 ( a ) : 3 - 5 ) . Hypotheses EmoG and Freehand assist in sketching emotional expressions for storyboarding with different levels of automation . Previous work suggests that AI can generate images at a professional level of quality [ 1 ] . Users will ﬁnd systems with AI more CHI 2020 Paper CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Paper 393 Page 6 useful and easier to use . They will also feel some degree of control over the process when the systems request user input [ 15 ] . Thus , we form hypotheses as follows : H1 EmoG is more useful when compared to the baseline system . Users give signiﬁcantly higher ratings for helpful­ ness , desirability , satisfaction , and effectiveness for EmoG than Freehand . H2 EmoG is easier to use when compared to the baseline system . ( H2a ) Users spend signiﬁcantly shorter time us­ ing EmoG than Freehand . ( H2b ) Users give signiﬁcantly higher ratings for ease of use , ease of learning for EmoG than Freehand . H3 EmoG helps create better emotional expressions when compared to the baseline system . Users give signiﬁcantly higher ratings for ( H3a ) aesthetics , ( H3b ) expressiveness , and ( H3c ) viewing angle in expressions for EmoG than Freehand . ( H3d ) Users give signiﬁcantly higher ratings for intensity in expressions for EmoG than Freehand . H4 Emog offers less ﬂexibility when compared to Freehand . Users give signiﬁcantly lower ratings for sense of own­ ership , creativity , and degree of freedom for EmoG than Freehand . Participants We recruited participants via mobile event apps . Our recruit­ ment material indicated that we were looking for designers who have storyboarding experience . We recruited 21 designers ( 12 females ) with an average age of 22 . 10 ( SD = 1 . 64 ) , includ­ ing UX designers , industrial designers , service designers , and digital media designers . All of the participants reported that they have experience in storyboarding and their drawing skills vary ( very good : 14 . 29 % , good : 28 . 57 % , fair : 28 . 57 % , poor : 14 . 29 % , very poor : 14 . 29 % ) . Procedure and Tasks The user study consisted of two tasks , each of which involved one of the two tools : Freehand and EmoG . In each task , the participants were provided with a partially ﬁnished storyboard , which intentionally left the face of the character blank . They were ﬁrst asked to draw a face of the character in the Charac­ ter Creation Page and then sketch the character’s emotional expressions in each frame in the Storyboard Creation Page . We limited the topics , scripts , and the number of frames in each storyboard . The details are included in Supplementary Materials . We also ensured an equal complexity across the two tasks in terms of expression type , expression intensity , and viewing angle . To avoid learning effects , we counterbalanced the orders of the two tools as well as their assignments to the two different tasks . After obtaining consent from the participants , we asked them to ﬁll out a brief demographic survey . The participants were given a tutorial introduction to each task before they started . At the end of each task , the participants were asked to complete a questionnaire using a 7 - point Likert scale . After all the tasks were completed , we conducted a semi - structured interview . The study lasted about 1 - 1 . 5 hours for each participant . Our user study was conducted using a Lenovo MIIX 520 - 12IKB touch - screen tablet PC with a stylus . The recorded logs of completion time were captured . The interaction process and the semi - structured interview were recorded and manually transcribed for thematic analysis . At the end of the study , we had collected 63 storyboards drawn by the 21 participants . To evaluate the quality of emotional expressions in these storyboards from another perspective , we recruited 20 external judges ( 12 females ) with an average age of 26 . 67 ( SD = 2 . 08 ) . We randomized the order of the storyboards and displayed them to the judges one at a time . In the questionnaire presented to the judges , expressiveness and intensity were evaluated by accuracy , the judges selected their answers from multiple choices including a “none of the above” option . Aesthetics and viewing angle were rated on a 7 - point Likert scale . RESULTS In this section , we report the statistical analyses and interview results from the user study . Quantitative Results Hypotheses of involvement items ( H1 , H2b , H4 ) have good internal consistency ( Cronbach’s Alpha > . 08 ) . A paired t - test was applied to examine if there is a signiﬁcant difference , as shown in Figure 6 . Inclusion of EmoG suggestions In the Character Creation Page , most of the participants ( 16 out of 21 ) found a satisfying face sketch suggested by EmoG before ﬁnishing sketching a character’s face on the canvas . More than half ( 13 out of 21 ) of the participants used the “Show More” button to update and explore more suggestions . EmoG is Useful Signiﬁcant difference is found in helpfulness ( t ( 20 ) = - 8 . 80 , p < . 01 , η 2 = . 99 ) ( Figure 6 ( a ) ) . The analysis result indicates that EmoG ( M = 6 . 33 , SD = . 64 ) produces signiﬁcantly higher scores than Freehand ( M = 3 . 86 , SD = 1 . 39 ) . Signiﬁcant difference is also found in desirability ( t ( 20 ) = - 7 . 16 , p < . 01 , η 2 = . 97 ) ( Figure 6 ( b ) ) . The participants would like to use EmoG ( M = 5 . 86 , SD = 1 . 12 ) signiﬁcantly more than Freehand ( M = 3 . 52 , SD = 1 . 14 ) . There is signiﬁcant difference in satisfaction ( t ( 20 ) = - 5 . 26 , p < . 01 , η 2 = . 99 ) ( Figure 6 ( c ) ) . EmoG performs signiﬁcantly better in satisfaction ( M = 5 . 67 , SD = . 71 ) than Freehand ( M = 3 . 76 , SD = 1 . 52 ) . We observed signiﬁcant difference in effectiveness ( t ( 20 ) = - 6 . 03 , p < . 01 , η 2 = . 98 ) ( Figure 6 ( d ) ) . The participants felt EmoG ( M = 6 . 00 , SD = . 87 ) is signiﬁcantly more effective than Freehand ( M = 3 . 71 , SD = 1 . 52 ) ( H1 accepted ) . EmoG is Easy to Use The results show signiﬁcant difference in complete time ( t ( 20 ) = 2 . 38 , p < . 05 , η 2 = . 95 ) . Figure 6 ( e ) suggests that the participants spent signiﬁcantly shorter time with EmoG ( M = 271 . 38 , SD = 64 . 63 ) than with Freehand ( M = 367 . 98 , SD = 214 . 96 ) ( H2a accepted ) . The analysis of ease of use delivers signiﬁcant difference ( t ( 20 ) = - 5 . 28 , p < . 01 , η 2 = . 99 ) ( Fig­ ure 6 ( f ) ) . EmoG ( M = 6 . 29 , SD = . 76 ) is placed signiﬁcantly CHI 2020 Paper CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Paper 393 Page 7 Figure 6 : Means and standard errors of each item in Freehand and EmoG conditions ( + : . 05 < p < . 1 , ∗ : p < . 05 , ∗∗ : p < . 01 ) organized by hypotheses H1 - H4 . Complete Time is measured by the time participants spent in each task . Expressiveness Accuracy ( peer ) and intensity Accuracy ( peer ) are measured by accuracy . Other items are measured by user ratings on a 7 - point Likert scale . higher than Freehand ( M = 4 . 57 , SD = 1 . 22 ) . Even though no signiﬁcant difference is found in ease of learning ( t ( 20 ) = - 1 . 77 , p < . 1 , η 2 = 1 . 00 ) , the trend is similar to ease of use ( Figure 6 ( g ) ) ( H2b partially accepted ) . EmoG generates high - quality results We evaluated the quality of the results generated by EmoG using both self - ratings and peer - ratings . The Fleiss’s kappa of aesthetics , expressiveness , viewing angle , and intensity in peer­ ratings are over . 80 , indicating good inter - rater agreement . In terms of aesthetics , signiﬁcant difference is detected for both self - ratings ( t ( 20 ) = - 2 . 68 , p < . 05 , η 2 = . 97 ) ( Figure 6 ( h ) ) and peer - ratings ( t ( 19 ) = - 8 . 02 , p < . 01 , η 2 = . 96 ) ( Figure 6 ( i ) ) . From the perspective of the judges , the expressions generated by EmoG ( M = 4 . 41 , SD = . 96 ) are signiﬁcantly more aesthetic than those by Freehand ( M = 3 . 70 , SD = 1 . 06 ) ( H3a accepted ) . We observed signiﬁcant differences in expressiveness both for self - ratings ( t ( 20 ) = - 3 . 18 , p < . 01 , η 2 = . 99 ) ( Figure 6 ( j ) ) and peer - ratings ( t ( 19 ) = - 14 . 12 , p < . 01 , η 2 = . 99 ) ( Figure 6 ( k ) ) . EmoG ( M = 5 . 95 , SD = . 84 ) shows a signiﬁcantly higher rating than Freehand ( M = 5 . 14 , SD = . 77 ) . The judges could signiﬁcantly better recognize the expressions created by EmoG ( M = . 70 , SD = . 09 ) than those by Freehand ( M = . 41 , SD = . 06 ) ( H3b accepted ) . For viewing angle , signiﬁcant difference is only found for peer - ratings ( t ( 19 ) = - 6 . 00 , p < . 01 , η 2 = . 54 ) ( Figure 6 ( i ) , ( m ) ) . The judges thought that EmoG ( M = 4 . 55 , SD = . 88 ) performs signiﬁcantly better than Freehand ( M = 3 . 99 , SD = . 92 ) ( H3c partially accepted ) . We found signiﬁcant difference in intensity for both self - ratings ( t ( 19 ) = - 2 . 10 , p < . 05 , η 2 = . 97 ) ( Figure 6 ( n ) ) and peer - ratings ( t ( 19 ) = - 2 . 13 , p < . 05 , η 2 = . 96 ) ( Figure 6 ( o ) ) . Both of the participants and judges thought EmoG produces signiﬁcantly more accurate intensity than Freehand ( H3d accepted ) . EmoG offers limited ﬂexibility Signiﬁcant difference is found in sense of ownership ( t ( 20 ) = 2 . 83 , p < . 05 , η 2 = . 99 ) ( Figure 6 ( p ) ) . EmoG ( M = 5 . 00 , SD = 1 . 02 ) produces a signiﬁcantly lower score than Freehand ( M = 5 . 86 , SD = . 89 ) . Similarly , signiﬁcant difference is detected in creativity ( t ( 20 ) = 2 . 53 , p < . 05 , η 2 = . 99 ) ( Figure 6 ( q ) ) . EmoG ( M = 4 . 86 , SD = 1 . 04 ) is rated signiﬁcantly lower than Freehand ( M = 5 . 76 , SD = 1 . 02 ) . Figure 6 ( r ) shows that signiﬁcant difference in degree of freedom is found between EmoG and Freehand ( t ( 20 ) = 2 . 21 , p < . 05 , η 2 = . 99 ) . EmoG ( M = 4 . 90 , SD = . 92 ) is rated signiﬁcantly lower than Freehand ( M = 5 . 67 , SD = 1 . 04 ) ( H4 accepted ) . Qualitative Results In the qualitative analysis , the goal was to understand the participants’ thoughts , comments , and suggestions beyond the quantitative results . Examples of emotional expressions created in the study are shown in Figure 7 . Perceived AI Helpfulness and Effectiveness Most participants believed that EmoG can deﬁnitely support their work in storyboarding . P3 noted : “EmoG is fun to use . . . It’s like I’m co - creating with a partner , a super talented part­ ner” . P11 said : “I got good results with EmoG within a few CHI 2020 Paper CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Paper 393 Page 8 seconds” . Six participants suggested that due to its capability of generating high - quality results , EmoG is helpful in cases when presenting storyboards to customers is necessary . One stated , “ I’m not skilled at drawing , but now I can conﬁdently present my storyboard to my team after a few tweaks ” ( P9 ) ; another said , “ the most important beneﬁt of storyboarding is to help you present your idea effectively , I’d like to use EmoG in my work as it facilitates ideation without requiring much effort on drawing ” ( P21 ) . On the other hand , ﬁve participants expressed their concern with such data - driven design support tools , “ Although it performs well , I’m a bit worried that I rely too much on it in my future work and lose the opportunity to practice my skills as a designer ” ( P18 ) . Quality of the Results During the interview , the participants were allowed to refer to the two storyboards they drew using EmoG and Freehand , respectively . More than half of the participants ( 15 out of 21 ) thought that EmoG helps deliver better results . P8 said , “ sketching expressions in a mid - proﬁle view without the help of AI can easily produce an unnatural look . I can see that by comparing my two storyboards ” ( Figure 7 ) ( a ) , ( e ) ) . P17 noted , “I thought it’d be easy to express emotions if I draw simpliﬁed facial features , but it turns out that I’m wrong” . To echo this comment , the results of peer - ratings suggest that more Figure 7 : Sketches from the user study . Each group from ( a ) to ( d ) shows two snapshots of the character’s face with the same expression ; one expresses mild emotion in a mid - proﬁle left view ( left ) while the other conveys intense emotion in a frontal view ( right ) . ( e ) shows a storyboard drew by a participant using EmoG . than one - third of the judges misinterpreted the happy face in the group ( b ) as a disgusted face ( left ) or a surprised face ( right ) . Two participants observed that EmoG can generate more vivid expressions of different intensities . For example , Figure 7 ( c ) shows that the intense surprised face ( right ) has “ more enlarged pupils ” ( P12 ) than the mild one ( left ) . In group ( d ) , the intense happy face ( right ) has “ more raised lip corners ” ( P18 ) than the mild one ( left ) . Creativity and Agency The participants reported being surprised by the fact that AI can augment human creative capabilities and user experience . One reﬂected , “ it’s interesting AI can help me create charac­ ters , and with various expressions ” ( P14 ) . P9 noted , “ I really like the idea that AI automatically draws different expressions of the character . It makes storyboarding much easier and faster . I will deﬁnitely recommend this tool to my colleagues ” . When asked about agency , some ( P2 , P5 , P16 , P21 ) noted that EmoG keeps a good balance between creativity and agency , “ although AI helped , I still felt a sense of ownership . I drew a few strokes and AI kept that in its suggestions . The ﬁnal result was originated from my idea and I regard it as my creative work ” ( P2 ) . “ it’s not like AI starts from zero but lets me ex­ press my ideas ﬁrst . . . I don’t have to worry about details such as how to sketch vivid expressions for the character , AI can take over it . . . I , as a user , was co - creating with AI . ” ( P16 ) . However , three designers suggested that automatic generation might conﬂict with the process of ideation , “ sketching a sto­ ryboard also helps ideation , as EmoG automates the process , I’m not quite sure it’ll bring positive or negative effects ” ( P17 ) . DISCUSSION In this section , we discuss the design implications from our study and limitations in our current work . Depict Expressions with More Diversity In our deployment of EmoG , the current algorithm generates expressions of six basic emotions , each with two different intensities . The participants suggested that more emotional expressions should be provided by our system to achieve more variety . Instead of having designers draw more expressions for the training set , we will extend the algorithm to modify and mix the basic expressions to create more expressions . For example , varying the intensity of happiness produces satisfac­ tion , amusement , and laughter . Mixing happy and surprise creates amazement [ 25 ] . As expressions change non - linearly , a possible solution can be the interpolation and vector arith­ metic in the latent vector . This approach has been successfully applied in the manipulation of pixel - based expressions [ 40 ] . The participants also noted that EmoG focuses on generating expressions of young adults and should create expressions of more age groups in storyboards . For example , aging adults tend to display more crow’s feet and nasolabial folds than young adults when smiling . These facial features should be added to our training samples to depict expressions of more age groups and thus increase the utility of the system . Support Storyboarding with More Intelligence We observed that our EmoG users are mostly positive toward the design supported by AI . In our user study , most of the CHI 2020 Paper CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Paper 393 Page 9 participants found a satisfying result suggested by EmoG in real - time as they drawing a face sketch in the Character Cre­ ation Page . The participants also requested more intelligence in EmoG to generate gestures and poses of characters in addi­ tion to emotional expressions . They suggested that data - driven design can provide a good starting point and relieve the work­ load of design . Laborious and tedious work in storyboarding such as adding backgrounds and colorizing can be done auto­ matically . For example , they implied that the system can ﬁnd an empty space on the canvas and suggest objects ( e . g . , laptop , desk , coffee mug ) based on the settings in previous frames . Moreover , the participants expected that EmoG progressively learns their editing behaviors and preferences to effectively support co - creation . For example , if a user ﬁne - tunes the lip corners of a happy face , the system will record the user’s modiﬁcations and use the information to guide the subsequent generation . A possible solution can be the interpolation [ 39 ] between the latent vector of the generated image and that of the user - modiﬁed image . Generalize to More Design Scenarios When asked about the most surprising beneﬁt that EmoG pro­ vides , four participants noted that it helps create a well - drawn storyboard that can be presented to the team or even clients after a few reﬁnements . This beneﬁt results from EmoG’s capability to effectively render emotions in an aesthetically appealing and expressive manner . At their core , suggestions and automatic generation supported by EmoG have analogies in other creative ﬁelds . For example , for a web designer who is tasked with sketching UIs , AI can suggest design examples of a homepage in a vector form based on the designer’s input and automatically generate a set of relevant pages such as a con­ tact page . The designer can then modify the generated results to create more polished web designs . More importantly , we found that it is a potential usage scenario for AI to bridge the gap between ideation and realization in the creative process . That is , designers collaboratively form ideas with AI , while the output of this co - ideation activity can be directly used as an input of design realization . In this process , AI helps provide suggestions relevant to the designers’ ideas and then generate the ﬁnal results based on the idea . Human - AI Co - Creation As we have seen in both the quantitative and qualitative feed­ back , the participants are satisﬁed with the expressiveness of the results generated by EmoG . However , some of the participants expressed concerns with human - AI co - creation experience in terms of sense of ownership and degree of free­ dom . For example , one participant suggested that having AI complete the sketch lowers users’ perceived controllability and thus decreases a sense of ownership . To augment human - AI co - creation experience , AI should allow users to provide continuous feedback and update accordingly . In this case , users are involved in each step of co - creation and are able to observe how their inputs shape the ﬁnal result . In this respect , combining AI with human interaction prompts a sense of shared control and raise collective potential . Also , AI should provide users with substantial freedom to design and create . That is , users can decide when and how AI interferes . For example , when sketching a storyboard , a user can notify AI that he or she requires assistance in constructing user stories or ﬁnding design materials . In each step , the user can decide in which way AI helps . For example , AI can ﬁnd design materials with similar features or dissimilar features , as well as color palettes to provide inspiration . Limitations and Future Work Despite EmoG’s success in supporting the sketching of emo­ tional expressions , the participants reported that the identity across different expressions is not consistently preserved . The reason is that the algorithm generates emotional expressions based on the latent vector , which is non - deterministic . Further study should better balance between the identity and diver­ sity of the generation , that is , the facial identity should be preserved across different expression types , intensities , and viewing angles . To address the issue , we will explore adding a constraint that preserves identity [ 37 ] , which sets the distance between the expressions of the same character generated by EmoG to be less than the distance between the different face sketches in the training set . We also identiﬁed two limitations in our user study . First , the expression type , expression intensity , and viewing angle of the character in each storyboard were pre - deﬁned . Without creating the script , character , and scenario by themselves , the participants had less ﬂexibility and might not be aware of these details such as viewing angle until the proctors pointed out . Second , the study solicited users’ ﬁrst impressions with EmoG . Some of the qualitative feedback implies that users were impressed with the technology , partly due to its nov­ elty . A longer - term evaluation would reveal which features of EmoG are “interesting” at ﬁrst glance , and which ones are useful over time . CONCLUSION This paper presents EmoG , an interactive system that learns the strokes from the user , suggests potential designs of an intended character , and generates new sketches of the character with six basic expressions . Our user study with 21 participants showed that EmoG is useful , easy to use , and effective for drawing emotional expressions in storyboards when compared to the baseline system . Future work could include augmenting human creative capabilities in EmoG , deploying EmoG in real design work , and exploring design guidelines for human - AI co - creative systems . We hope this work can inspire more researchers and practitioners to explore and design diverse data - driven approaches for storyboarding and other design ideation processes . ACKNOWLEDGEMENTS Nan Cao is the corresponding author . We thank Xin Yan and Chaoran Chen from the College of Design and Innovation , Tongji University for their assistance . This work was sup­ ported in part by the National Natural Science Foundation of China ( 61802283 and 61602306 ) and Shanghai Summit Discipline in Design under Grant DA19103 . CHI 2020 Paper CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Paper 393 Page 10 REFERENCES [ 1 ] 2018 . MakeGirlsMoe . ( 2018 ) . https : / / make . girls . moe [ 2 ] Tom Bancroft . 2012 . Character Mentor : Learn by Example to Use Expressions , Poses , and Staging to Bring Your Characters to Life . Routledge . [ 3 ] Nan Cao , Xin Yan , Yang Shi , and Chaoran Chen . 2019 . AI - Sketcher : A Deep Generative Model for Producing High - Quality Sketches . The Thirty - Third AAAI Conference on Artiﬁcial Intelligence ( 2019 ) , 2547 – 2554 . [ 4 ] Sen Chen , Lingling Fan , Chunyang Chen , Ting Su , Wenhe Li , Yang Liu , and Lihua Xu . 2019 . Storydroid : Automated generation of storyboard for Android apps . In Proceedings of the 41st International Conference on Software Engineering . IEEE Press , 596 – 607 . [ 5 ] Hui Ding , Kumar Sricharan , and Rama Chellappa . 2018 . ExprGan : Facial Expression Editing with Controllable Expression Intensity . In Thirty - Second AAAI Conference on Artiﬁcial Intelligence . 6781 – 6788 . [ 6 ] Steven Dow , T Scott Saponas , Yang Li , and James A Landay . 2006 . External Representations in Ubiquitous Computing Design and the Implications for Design Tools . In Proceedings of the 6th conference on Designing Interactive systems . ACM , 241 – 250 . [ 7 ] Mathias Eitz , James Hays , and Marc Alexa . 2012 . How do humans sketch objects ? ACM Transactions on Graphics 31 , 4 ( 2012 ) , 44 : 1 – 44 : 10 . [ 8 ] Jonas Frich , Lindsay MacDonald Vermeulen , Christian Remy , Michael Mose Biskjaer , and Peter Dalsgaard . 2019 . Mapping the Landscape of Creativity Support Tools in HCI . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . ACM , 389 : 1 – 389 : 18 . [ 9 ] Ian Goodfellow , Jean Pouget - Abadie , Mehdi Mirza , Bing Xu , David Warde - Farley , Sherjil Ozair , Aaron Courville , and Yoshua Bengio . 2014 . Generative Adversarial Nets . In Advances in Neural Information Processing Systems . Curran Associates , Inc . , 2672 – 2680 . [ 10 ] Geonmo Gu , Seong Tae Kim , Kihyun Kim , Wissam J Baddar , and Yong Man Ro . 2017 . Differential generative adversarial networks : Synthesizing non - linear facial variations with limited number of training data . arXiv preprint arXiv : 1711 . 10267 ( 2017 ) . [ 11 ] David Ha and Douglas Eck . 2017 . A Neural Representation of Sketch Drawings . CoRR abs / 1704 . 03477 ( 2017 ) . http : / / arxiv . org / abs / 1704 . 03477 [ 12 ] Mieke Haesen , Jan Meskens , Kris Luyten , and Karin Coninx . 2009 . Supporting Multidisciplinary Teams and Early Design Stages Using Storyboards . In International Conference on Human - Computer Interaction . Springer , 616 – 623 . [ 13 ] Rorik Henrikson , Bruno De Araujo , Fanny Chevalier , Karan Singh , and Ravin Balakrishnan . 2016 . Storeoboard : Sketching Stereoscopic Storyboards . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . ACM , 4587 – 4598 . [ 14 ] Forrest Huang , John F Canny , and Jeffrey Nichols . 2019 . Swire : Sketch - based User Interface Retrieval . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . ACM , 104 : 1 – 104 : 10 . [ 15 ] Anthony David Jameson . 2009 . Understanding and dealing with usability side effects of intelligent processing . AI Magazine 30 , 4 ( 2009 ) , 23 – 40 . [ 16 ] Jonas Jongejan , Henry Rowley , Takashi Kawashima , Jongmin Kim , and Nick Fox - Gieg . 2019 . Quick , Draw ! ( 2019 ) . https : / / quickdraw . withgoogle . com [ 17 ] Janin Koch , Andrés Lucero , Lena Hegemann , and Antti Oulasvirta . 2019 . May AI ? : Design Ideation with Cooperative Contextual Bandits . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . ACM , 633 : 1 – 633 : 12 . [ 18 ] Stelios Krinidis , Ioan Buciu , and Ioannis Pitas . 2003 . Facial expression analysis and synthesis : A survey . In 10th International Conference on Human - Computer Interaction ( HCI 2003 ) . Citeseer , 22 – 27 . [ 19 ] Ranjitha Kumar and Kristen Vaccaro . 2017 . An Experimentation Engine for Data - driven Fashion Systems . In 2017 AAAI Spring Symposium Series . 389 – 394 . [ 20 ] Yang Li and James A Landay . 2008 . Activity - based Prototyping of Ubicomp Applications for Long - lived , Everyday Human Activities . In Proceedings of the 2008 CHI Conference on Human Factors in Computing Systems . ACM , 1303 – 1312 . [ 21 ] Andrew Loomis . Drawing the Head and Hands ( illustrated , reprint ed . ) . Viking Press , 1956 . [ 22 ] Xiaojuan Ma , Jodi Forlizzi , and Steven Dow . 2012 . Guidelines for depicting emotions in storyboard scenarios . In 8th International Design And Emotion Conference . [ 23 ] Yusuke Matsui , Takaaki Shiratori , and Kiyoharu Aizawa . 2016 . DrawFromDrawings : 2D drawing assistance via stroke interpolation with a sketch database . IEEE Transactions on Visualization and Computer Graphics 23 , 7 ( 2016 ) , 1852 – 1862 . [ 24 ] Scott McCloud . 1993 . Understanding comics : The invisible art . Northampton , Mass ( 1993 ) . [ 25 ] Scott McCloud . 2006 . Making Comics : Storytelling Secrets of Comics , Manga and Graphic Novels . William Morrow and Company . [ 26 ] Changhoon Oh , Jungwoo Song , Jinhan Choi , Seonghyeon Kim , Sungwoo Lee , and Bongwon Suh . 2018 . I Lead , You Help but Only with Enough Details : Understanding User Experience of Co - Creation with Artiﬁcial Intelligence . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . ACM , 649 : 1 – 649 : 13 . CHI 2020 Paper CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Paper 393 Page 11 [ 27 ] Hans Pancherz , Verena Knapp , Christina Erbe , and Anja Melina Heiss . 2010 . Divine Proportions in Attractive and Nonattractive Faces . World Journal of Orthodontics 11 , 1 ( 2010 ) , 27 – 36 . [ 28 ] Albert Pumarola , Antonio Agudo , Aleix M Martinez , Alberto Sanfeliu , and Francesc Moreno - Noguer . 2018 . Ganimation : Anatomically - aware facial animation from a single image . In Proceedings of the European Conference on Computer Vision ( ECCV ) . 818 – 833 . [ 29 ] Mary Beth Rosson and John M Carroll . 2009 . Scenario - based Design . In International Conference on Human - Computer Interaction . CRC Press , 161 – 180 . [ 30 ] Yang Shi , Yang Wang , Ye Qi , John Chen , Xiaoyao Xu , and Kwan - Liu Ma . 2017 . IdeaWall : improving creative collaboration through combinatorial visual stimuli . In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing . ACM , 594 – 603 . [ 31 ] Stephen Silver . 2017 . The Silver Way : Techniques , Tips , and Tutorials for Effective Character Design . Design Studio Press . [ 32 ] Edgar Simo - Serra , Satoshi Iizuka , and Hiroshi Ishikawa . 2018 . Real - time data - driven interactive rough sketch inking . ACM Transactions on Graphics 37 , 4 ( 2018 ) , 98 : 1 – 98 : 14 . [ 33 ] Frank Spillers . 2004 . Emotion as a cognitive artifact and the design implications for products that are perceived as pleasurable . Experience Dynamics ( 2004 ) . [ 34 ] Wan Tao , Qiang Zhang , Mengyao Zhang , and Yuewei Li . 2019 . Mining Pain Points from Hotel Online Comments based on Sentiment Analysis . In 2019 IEEE 8th Joint International Information Technology and Artiﬁcial Intelligence Conference ( ITAIC ) . IEEE , 1672 – 1677 . [ 35 ] Khai N Truong , Gillian R Hayes , and Gregory D Abowd . 2006 . Storyboarding : an empirical determination of best practices and effective guidelines . In Proceedings of the 6th Conference on Designing Interactive Systems . ACM , 12 – 21 . [ 36 ] Corrie Van der Lelie . 2006 . The Value of Storyboards in the Product Design Process . Personal and Ubiquitous Computing 10 , 2 - 3 ( 2006 ) , 159 – 162 . [ 37 ] Xueping Wang , Weixin Li , Guodong Mu , Di Huang , and Yunhong Wang . 2018 . Facial Expression Synthesis by U - Net Conditional Generative Adversarial Networks . In Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval . ACM , 283 – 290 . [ 38 ] Xiaogang Wang and Xiaoou Tang . 2009 . Face photo - sketch synthesis and recognition . IEEE Transactions on Pattern Analysis and Machine Intelligence 31 , 11 ( 2009 ) , 1955 – 1967 . [ 39 ] Tom White . 2016 . Sampling Generative Networks . CoRR abs / 1609 . 04468 ( 2016 ) . http : / / arxiv . org / abs / 1609 . 04468 [ 40 ] Raymond Yeh , Ziwei Liu , Dan B Goldman , and Aseem Agarwala . 2016 . Semantic Facial Expression Editing Using Autoencoded Flow . CoRR abs / 1611 . 09961 ( 2016 ) . http : / / arxiv . org / abs / 1611 . 09961 [ 41 ] Lvmin Zhang , Chengze Li , Tien - Tsin Wong , Yi Ji , and Chunping Liu . 2018 . Two - stage Sketch Colorization . In SIGGRAPH Asia 2018 Technical Papers . ACM , 261 : 1 – 261 : 14 . [ 42 ] Yihao Zhang , Weiyao Lin , Bing Zhou , Zhenzhong Chen , Bin Sheng , and Jianxin Wu . 2014 . Facial expression cloning with elastic and muscle models . Journal of Visual Communication and Image Representation 25 , 5 ( 2014 ) , 916 – 927 . CHI 2020 Paper CHI 2020 , April 25 – 30 , 2020 , Honolulu , HI , USA Paper 393 Page 12