CrowdIDEA : Blending Crowd Intelligence and Data Analytics to Empower Causal Reasoning Chi - Hsien ( Eric ) Yen cyen4 @ illinois . edu Computer Science , University of Illinois at Urbana - Champaign Urbana , Illinois , USA Yilin Xia yilinx2 @ illinois . edu Information Sciences , University of Illinois at Urbana - Champaign Champaign , Illinois , USA Haocong Cheng haocong2 @ illinois . edu Information Sciences , University of Illinois at Urbana - Champaign Champaign , Illinois , USA Yun Huang yunhuang @ illinois . edu Information Sciences , University of Illinois at Urbana - Champaign Champaign , Illinois , USA ABSTRACT Causal reasoning is crucial for people to understand data , make decisions , or take action . However , individuals often have blind spots and overlook alternative hypotheses , and using only data is insufcient for causal reasoning . We designed and implemented CrowdIDEA , a novel tool consisting of a three - panel integration incorporating the crowd’s beliefs ( Crowd Panel with two designs ) , data analytics ( Data Panel ) , and user’s causal diagram ( Diagram Panel ) to stimulate causal reasoning . Through an experiment with 54 participants , we showed the signifcant efects of the Crowd Panel designs on the outcomes of causal reasoning , such as an increased number of causal beliefs generated . Participants also devised new strategies for bootstrapping , strengthening , deepening , and explaining their causal beliefs , as well as taking advantage of the unique characteristics of both qualitative and quantitative data sources to reduce potential biases in reasoning . Our work makes theoretical and design implications for exploratory causal reasoning . CCS CONCEPTS • Human - centered computing → Empirical studies in collab - orative and social computing ; Empirical studies in HCI . KEYWORDS Crowd Intelligence , Crowd - informed Reasoning Tools , Causal Rea - soning , Causal Diagrams , Visualization ACM Reference Format : Chi - Hsien ( Eric ) Yen , Haocong Cheng , Yilin Xia , and Yun Huang . 2023 . CrowdIDEA : Blending Crowd Intelligence and Data Analytics to Empower Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specifc permission and / or a fee . Request permissions from permissions @ acm . org . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany © 2023 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . ACM ISBN 978 - 1 - 4503 - 9421 - 5 / 23 / 04 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3544548 . 3581021 Causal Reasoning . In Proceedings of the 2023 CHI Conference on Human Fac - tors in Computing Systems ( CHI ’23 ) , April 23 – 28 , 2023 , Hamburg , Germany . ACM , New York , NY , USA , 17 pages . https : / / doi . org / 10 . 1145 / 3544548 . 3581021 1 INTRODUCTION Causal reasoning is the process of understanding and explaining the relationships between diferent events , examining the context and circumstances in which they occurred , and drawing conclusions about what could have led to a particular outcome [ 84 ] . People conduct causal reasoning to explain past events , predict future trends , and make important decisions more reliably [ 13 , 31 , 76 ] . With the increasing amount of digital data and modern causal analysis frameworks [ 63 , 64 , 77 ] , causal reasoning has recently gained great popularity in various felds , such as risk analysis , policy making , etc . [ 14 , 48 , 69 , 74 ] . Albeit the progress , several challenges remain when people con - duct causal reasoning . First , using only quantitative data poses sev - eral challenges to causal reasoning . For example , it is often difcult to determine which variable is causing the other to change without providing insights into the underlying mechanisms or causes of the relationships [ 83 ] . Neglecting a confounding factor can result in wrong conclusions due to mistaking spurious correlations as causation [ 49 ] . Also , people tend to choose the frst hypothesis that appears to be good enough without considering all other possible hypotheses , a sub - optimal strategy called “satisfcing strategy” [ 35 ] . Therefore , prior works suggested incorporating qualitative data into the causal reasoning process [ 15 , 23 , 40 , 56 ] , e . g . , by bringing a group of people together to brainstorm all possible hypotheses to overcome an individual’s blind spots and biases [ 35 ] . In other words , crowd intelligence ( e . g . , the knowledge or beliefs from a group of people ) has been suggested to be benefcial in complex reasoning performance . To support crowd - based causal reasoning , researchers in the HCI community have extensively researched tool design and evaluation [ 3 , 6 , 7 , 24 , 25 , 65 ] , e . g . , studies showed that allowing tags or links in a comment in a visual analytics system could help people establish common ground [ 88 ] . A recent work also shows that when people construct a causal diagram , looking at a peer’s causal diagram and associated explanations can help them identify blind spots [ 94 ] . Indeed , since a crowd is able to provide diverse and high - quality explanations for social data trends [ 87 ] , integrating CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Chi - Hsien ( Eric ) Yen , Haocong Cheng , Yilin Xia , and Yun Huang diagrams and narratives can facilitate causal belief externalization and sharing [ 94 ] . However , little is known about how people conduct causal rea - soning when they are exposed to both crowd’s causal beliefs and data analytics , nor has the literature evaluated efective designs for delivering aggregated crowd’s beliefs to inform causal reasoning . To fll this research gap , we designed and evaluated CrowdIDEA , a Crowd - Informed Data rEAsoning system , which blends crowd in - telligence , in the form of crowd’s causal beliefs ( referred as crowd’s beliefs in the remainder of this paper ) , and data analytics for causal reasoning . Our work makes the following contributions to the HCI commu - nity : First , we introduced CrowdIDEA , a novel tool consisting of : 1 ) a three - panel integration of the crowd’s beliefs ( via Crowd Panel ) , data analytics ( via Data Panel ) , and users’ own causal diagram draw - ing ( via Diagram Panel ) to promote causal reasoning ; and 2 ) two interaction designs of the Crowd Panel for delivering the aggregated crowd’s beliefs . For example , one design is called Overview , guided by the “overview - then - details” design principle [ 75 ] ; it visualizes the crowd’s beliefs through a weighted and directional diagram and links them with corresponding causal narratives . Second , we provided empirical evidence showing the efect of CrowdIDEA on the outcomes and processes of causal reasoning . Specifcally , we conducted a between - subject experiment with 54 participants who possessed a data analytics background . The par - ticipants were randomly assigned to three groups : two were treated with the Crowd Panel design options , respectively , and one control group was not provided with the crowd’s beliefs . Our results showed that the Overview design not only changed the causal reasoning out - comes ( e . g . , resulting in signifcantly more causal relationships in their fnal diagrams than the control group ) , but also impacted the causal reasoning processes ( e . g . , reduced use of the Data Panel ) . The three - panel integration enabled participants to develop diferent strategies for bootstrapping , deepening , explaining , and evaluating causal beliefs , as well as leveraging the complementary features of the qualitative and quantitative data sources to mitigate potential biases of causal reasoning . Third , our fndings made theoretical implications , i . e . , extending a knowledge generation model with a new component of crowd intelligence to promote the processes of causal inspiration and internalization . Lastly , we discussed design implications about how to leverage crowd intelligence to mitigate potential credibility and potential biases involved in exploratory data analysis and how to turn reasoning - process data into causal reasoning input to gain deeper insights . 2 RELATED WORK In this section , we frst present the limitations of an individual’s causal reasoning when using only quantitative data . We then show the benefts and challenges of incorporating qualitative ( subjective ) data to support the reasoning process . We further briefy discuss the systems and theoretical models proposed for people to conduct reasoning and show that little is known how to support the integra - tion of the crowd’s qualitative input and data analytics for causal reasoning . 2 . 1 Limitations of Causal Reasoning When Using Only Quantitative Data Using quantitative data to infer causal relationships has been stud - ied for decades in related felds such as mathematics and statis - tics . Many advanced mathematical frameworks have been devel - oped , e . g . , Mediation analysis [ 5 ] , path analysis [ 90 ] , Bayesian net - works [ 63 ] , and structural equation modeling [ 37 ] . Building on these frameworks , prior works proposed interactive tools for causal rea - soning with data analytics . For example , Wang and Mueller [ 85 , 86 ] designed an interactive system that allows users to visualize and modify algorithm - generated causal models based on statistical test - ing results . Dang et al . [ 12 ] proposed ReactionFlow , an interactive causal analysis tool on biological pathways . However , literature has also pointed out several limitations of using data analytics in causal reasoning tools . First , correlation does not necessarily imply causation , which means that even if two variables appear to be strongly correlated , it is still possible that there is no causal relationship between them . While p - values of the regression results may aid decisions in causal reasoning , they cannot be used alone to directly infer any causal relationship [ 20 ] and may cause type I error [ 38 ] . Second , though data analytics can be used to identify relationships between variables , it is often difcult to determine the direction of the relationship [ 83 ] or which variable is causing the other to change . Insights into underlying mechanisms or causes of the relationships between variables may not be provided by the data . Third , relying too much on statistical results may also lead to undesirable conclusions , as humans possess cognitive biases while searching for and interpreting data , such as confrmation bias [ 57 ] , anchoring efect [ 35 ] , framing efect [ 46 ] , and the illusion of causality in visualized data [ 91 ] . For example , confrmation bias refers to the tendency to look for and give higher weights to data analytics that confrms one’s beliefs . In addition , people may adopt a “satisfcing strategy , ” where they stop searching for alternative explanations after fnding the initial hypothesis good enough [ 35 ] . This might lead a person to neglect possible mediating factors or confounding factors and jump to false conclusions . These limitations of using only quantitative data adversely afect the quality of reasoning outcomes . 2 . 2 Benefts and Challenges of Incorporating Qualitative Data Existing literature has demonstrated the distinct advantages of using qualitative data to investigate causal relationships [ 15 , 23 , 40 , 56 ] , Specifcally , close observations and recordings of unfolding chains of events can provide valuable insights into causal pathways . This approach can yield a much more nuanced understanding of the underlying causal mechanisms than one could gain using only quantitative data . As a result , many researchers have also designed systems for collaborative reasoning using qualitative data , which allow people to share hypotheses , rank evidence , organize insights , and make decisions ( e . g . , [ 7 , 8 , 24 , 25 , 66 , 98 ] ) . Several approaches are proposed to collect qualitative causal beliefs , such as crowdsourcing narratives [ 9 , 10 , 51 ] , making concept maps [ 52 ] , and generating storytelling narratives [ 50 ] to support causal relationships . A more recent study [ 94 ] suggested that sharing peers’ causal beliefs could help users discover their blind spots . CrowdIDEA : Blending Crowd Intelligence and Data Analytics to Empower Causal Reasoning CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany However , these studies did not propose ways of presenting aggre - gated crowd’s beliefs with dataset to support the causal reasoning . When other people’s outcomes are shared to a person , social biases may occur , such as teammate inaccuracy blindness [ 44 ] , example exposure efect [ 82 ] , conformity efect [ 4 ] , or backfre efect [ 60 ] . For example , conformity efect refers to the lack of making inde - pendent decisions under group pressure [ 4 ] . Incorrect information shared by teammates may also lead to unwanted performance in the reasoning process [ 32 ] , but mixing correct and incorrect infor - mation could improve the performance [ 44 ] . Little is known about how providing data analytics could help people mitigate potential biases introduced by the crowd’s beliefs . 2 . 3 Designing Visual Analytics Systems for Causal Reasoning To support causal reasoning with both crowded intelligence and data analytics efectively , new visual analytics systems need to be developed and evaluated . Extensive HCI theories of information vi - sualization [ 75 , 81 ] could be potentially applied to guide the system design , and prior research has shown the signifcant efects of inter - action design on people’s reasoning and sense - making processes . For example , the amount of information and the way it is presented can infuence people’s performance on conditional probability eval - uation tasks [ 61 ] ; and a slight change in the interface design could make people consider more alternative hypotheses during causal reasoning [ 41 ] . However , there is a lack of empirical understanding about how a system , with the integration of the crowd’s beliefs and data analytics , afects people’s causal reasoning outcomes . The acquisition of a deeper empirical understanding can advance the development of theoretical models for knowledge generation , which describe the interactions between a user and a computer during sensemaking and reasoning . For example , Sacha et al . [ 70 ] proposed the Knowledge Generation Model ( KGM ) , where the rea - soning processes of a person are modeled as three loops : exploration , verifcation , and knowledge generation loops . These loops connect to each other as a person explores a dataset , verifes their hypothe - ses , and updates their knowledge in the dataset . Other cognitive models include Pirolli and Card’s sensemaking model [ 67 ] , Green’s Human Cognition Model [ 26 , 27 ] , and Norman’s seven stages of action [ 59 ] . In addition , Xiong et al . [ 92 ] found that the way people estimate the relationship between variables is infuenced by their existing beliefs or hypothesis . However , these models do not cap - ture how the crowd’s causal beliefs could be incorporated into the reasoning processes . Given the above literature , this paper presents a novel system design and its evaluation study to address the following research questions ( RQs ) : • RQ1 : What’s the efect of integrating crowd’s beliefs and data analytics on people’s causal reasoning outcomes ? • RQ2 : How do people leverage crowd intelligence and data analytics in their causal reasoning processes ? 3 SYSTEM DESIGN 3 . 1 CrowdIDEA System Model The goal of the CrowdIDEA system is to support users to leverage both crowd intelligence ( e . g . , aggregated crowd’s causal beliefs ) and data analytics ( e . g . , data visualization and regression analysis result ) to perform causal reasoning on a dataset . It is for anyone with a basic statistics background to conduct causal reasoning , as it is increasingly popular for people to use public data visualization and analytics tools to draw causal inferences even when they may not have expertise in causal analysis . As shown in Figure 1 , the interface consists of three main panels : ( 1 ) Diagram Panel in the middle , which allows the user to con - struct a causal diagram while interacting with the other panels , ( 2 ) Crowd Panel on the left , to present the causal beliefs of the crowd , and ( 3 ) Data Panel on the right , to present data visualization and statistics . 3 . 2 Diagram Panel : the Central and Bridging Point The Diagram Panel , shown in Figure 1 , provides an intuitive graph - ical interface for a user to draw a directed causal diagram . Each node in the diagram represents a variable in a dataset , and each arrow represents a causal relationship from one variable to another . After the dataset is loaded , the user can select a variable from a drop - down menu to add it into the drawing canvas and construct a causal diagram by connecting nodes with arrows ( as shown in Figure 1 B area ) . To be specifc , the interface allows users to draw bi - directional arrows , drag nodes to organize the diagram , and save or load diagrams . In addition , each arrow is associated with a box area ( as shown in Figure 1 C area ) , where a user can write a nar - rative for the causal relationship and use a slider to specify how strong they believe the efect strength should be . The associated narratives allow users to externalize details of their causal beliefs by providing hypotheses or context , which have been shown to be benefcial for causal beliefs externalizing and sharing [ 94 ] . The Diagram Panel serves as a central and bridging point be - tween the Crowd Panel and the Data Panel . With reference to large visualization tools , i . e . , Gephi and Dundas [ 1 ] , we enable several visualization interaction techniques , including flter , drag , and se - lection in the Diagram Panel , to allow users to easily explore their interested variables in the panels . Whenever a node or an arrow is selected in the diagram ( highlighted in yellow ) , the relevant in - formation for those selected nodes or arrows will be provided in the Crowd Panel , and the data visualization and regression analysis results will be automatically updated in the Data Panel . This co - ordinated interaction allows a user to seamlessly retrieve relevant and important information while they are constructing their causal diagram . 3 . 3 Crowd Panel : Two Design Options The Crowd Panel provides a collection of causal beliefs gathered from a crowd with the goal of inspiring users during causal rea - soning . Prior research has shown that people are poor at thinking CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Chi - Hsien ( Eric ) Yen , Haocong Cheng , Yilin Xia , and Yun Huang Crowd Panel Diagram Panel Data Panel TWO DESIGN OPTIONS C B A D ComparewithDATAMODEL DATA STATS E Overview ( Fig . 2 ) Focus ( Fig . 3 ) ( Fig . 5 ) ( Fig . 4 ) Figure 1 : CrowdIDEA has three panels : Crowd Panel ( left ) , Diagram Panel ( middle ) , and Data Panel ( right ) . The Crowd Panel has two design options : overview design in Figure 2 and focus design in Figure 3 . More interaction designs of the Data Panel are shown in Figure 4 and Figure 5 . of all alternative hypotheses [ 21 ] and may have cognitive biases or blind spots when performing causal reasoning [ 35 ] . A recent research [ 94 ] found that reviewing a few peers’ causal diagrams and narratives did help people discover blind spots ; however , the repre - sentation interface for a larger group of people has to be designed carefully to avoid confusion and information overload . Therefore , we aggregate a group of people’s causal beliefs by counting the same causal relationship ( from the same starting variable to the same destination variable ) suggested by the crowds , which is ca - pable of accommodating a large number of causal models from crowds . Note that relationships between two variables in diferent directions are counted separately . To present the aggregated results , we proposed two design options , overview design and focus design , as described below . 3 . 3 . 1 Option 1 : Overview Design ( “Overview - then - details” ) . The frst design is inspired by Ben Shneiderman’s visual information - seeking mantra : “Overview frst , zoom and flter , then details - on - demand” [ 75 ] , which is widely applied in data navigation systems [ 29 , 45 ] . The design principle argues that information visualizations should give consumers a broad perspective of the entire collection and allow them to zoom in on specifc things of interest . Users should also be able to flter out uninteresting stuf and go further into certain items if necessary . For instance , geographic maps usu - ally shows an overall area and enable users to zoom into the region of interests [ 34 ] . The design goal is to give users a quick overview of the crowd’s beliefs , such as what causal relationships are the most popular among a crowd . Then , the user can flter the arrows they are interested in and obtain details . The overview design supports three main actions as follows . “Overview” frst through an aggregated crowd diagram . In the overview design , users will be provided with an aggregated crowd’s causal diagram ( we refer as “crowd diagram” ) , as shown in Figure 2 . The crowd diagram is constructed from a set of causal diagrams independently created by each member of a crowd , re - gardless of how large the crowd is . For each arrow that was drawn by at least one person in the crowd , we frst count how many people had drawn it and collect their narratives for that arrow . We then plot an aggregated diagram , where the thickness of an arrow is proportional to the number of people who had drawn it . Notice - ably , if arrows with opposite directions between two nodes were drawn by the crowd , both arrows will be plotted in the aggregated diagram . Among these two arrows , the one drawn by more people is longer and thicker than the other , which can occupy up to 70 % of the distance between the two nodes to make sure that both arrows can be seen . In this way , it is visually comparable for a user to determine which arrows are the most and least popular ; therefore , this design conveys a quick overview of the crowd’s beliefs . “Details” on demand . Users can demand corresponded details by clicking an arrow on either the crowd diagram or their own causal diagram . After clicking , the system will highlight the selected arrow as yellow and auto - scroll to bring the associated narrative box into the viewport . The narrative box includes information CrowdIDEA : Blending Crowd Intelligence and Data Analytics to Empower Causal Reasoning CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Arrows not drawn in user’s diagram Arrows opposite to the direction in user’s diagram Arrows that are the same in both diagrams O v e r v i e w D e s i g n OPTION 1 Crowd Panel Click to Compare Fig . 1 A Figure 2 : The overview design of the Crowd Panel in CrowdIDEA is composed of three main parts : ( 1 ) an aggregated crowd - based causal diagram that represents all causal relationships added by crowds ; ( 2 ) detailed narratives from the crowd to explain why they draw the arrow ; and ( 3 ) a “Show Diagram Diference” button that allows users to identify the arrows not included in their own causal diagram compared with the crowd diagram . When a user clicks the “Show Diagram Diferences” button at the top , the system highlights arrows on the crowd diagram with diferent colors to display the diferences between the user’s diagram and the crowd diagram . regarding the number of people ( with percentage ) in the crowd who also drew this arrow , the narratives for this arrow , and the strength ratings provided by the crowd . To help users identify arrows that are not included in their diagram but may be worth further investigation , we designed a “Show Diagram Diferences” function in Figure 2 . Specifcally , a blue arrow indicates that the arrow exists in the crowd diagram but not in the user’s diagram ; and a red arrow further indicates that the arrow does not exist in the user’s diagram and is going in the opposite direction of the corresponding arrow the user has drawn . 3 . 3 . 2 Option 2 : Focus Design ( “Focus - then - expand” ) . The second design is adapted from Frank van Ham and Adam Perer’s “Search , show Context , expand on demand” design principle [ 81 ] . They claimed that while the overview gives a big picture of the entire collection , such overview is sometimes too complex or not relevant to users’ goals in practice , especially for graph - based visualization . Instead , they adopted their design principle when designing a graph - based intelligence analysis system by utilizing a degree of interest measurement to diferentiate the signifcance of nodes based on the target of the user . Therefore , users can conduct a search and receive only the information that has the highest relevance . Instead of showing a crowd diagram , this interaction model allows users to browse immediate context around a specifc point of interest frst , then expand to other relevant information . This approach is particularly efective in the scenarios where an overview network visualization becomes too complex and difcult to comprehend due to the large number of arrows . “Focus” on a specifc relationship with relevant crowd’s beliefs . As shown in Figure 3 , whenever a user creates an arrow or selects one , the Crowd Panel in the focus design will provide the number of people who drew the same arrow or the reversed arrow . To help users compare the crowd’s beliefs visually , a green progress bar is used to indicate the percentage of the crowd that drew the arrow with the same direction . On the other hand , a red progress bar is used for the reversed arrow , warning that the user should consider the alternative hypotheses . Moreover , users can click “Review Their Narratives + ” to unzip the narrative box , which includes narratives and strength ratings from the crowd . This design permits users to receive immediate context after drawing or clicking an arrow , which becomes increasingly benef - cial as the number of people in the crowd increases . “Expand” the user diagram with potential relationships . In addition to providing immediate context , the focus design shows ideas for potential expansions of the current diagram that the user is constructing . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Chi - Hsien ( Eric ) Yen , Haocong Cheng , Yilin Xia , and Yun Huang Click to Expand Crowd Panel OPTION 2 F o c u s D e s i g n Fig . 1 A Figure 3 : The focus design of the Crowd Panel in CrowdIDEA shows the percentage of people in crowds who added the same arrow ( in green ) or the opposite arrow ( in red ) . Users can click on “Review Their Narratives” to see the narratives of the corresponding arrow . In addition , the system will suggest additional arrows using the selected node as the source node , and users can click " Add Arrow " to easily add arrows that are not currently included in their causal diagrams . As shown in Figure 3 , the Crowd Panel in the focus design will suggest a list of arrows that start from the current reasoning focus ( i . e . , the selected node , or the destination node of the selected arrow ) . These suggested arrows are drawn by at least one person from the crowd and are not existing in the user’s diagram yet . Users can click the “Add Arrow” button to quickly add a suggested arrow into their own diagram with the provided information . Therefore , this feature provides a convenient way for users to expand their reasoning scope from the crowd’s beliefs . 3 . 4 Data Panel : Automatically Responding to the Changes of the Diagram Panel The Data Panel is identical in all Crowd Panel designs , as shown in Figure 1 . It provides data visualizations and regression results to help users understand data patterns . The goal of this panel is not enabling users to formally prove or disprove a causal hypothesis with data analytics , instead , our study focuses on the efects of integrating crowd intelligence ( Crowd Panel ) and data analytics ( Data Panel ) on people’s causal reasoning . In other words , the ro - bustness of the Data Panel is not the focus , and we expect users to confrm or modify the results derived from the Data Panel with the Crowd Panel . We design the data panel with the commonly used data analytics techniques as described below . Data visualizations respond to the selected variables in the user diagram . Whenever a user selects nodes ( variables ) in their diagram , a data visualization will be generated ( as shown in Figure 1 D area ) , using the similar automated visualization method in [ 96 ] . To simplify the experiments , we provide users the same data visualizations when they select the same set of variables . For example , when one numerical variable and one categorical variable are selected , a bar chart is plotted ; when two or more numerical variables are selected , a scatterplot is used . Noticeably , users may select up to three variables of their interest on their diagram to generate the data visualization , because a visualization with four or more variables is generally difcult to interpret . is more likely to be causal if the correlation coefcient is large and statistically signifcant , ” correlation results are helpful for users to the statistical signifcance of relationship between variables within a causal relationship . To further assist interpreting statistical results , we design a feature that visualizes the signifcant correlation results directly on the user’s diagram . When the button “Show Signifcance on My Diagram” is clicked , the system will use the second part Show signifcant correlation results on the user diagram . of the regression analysis result ( “Regressions for All Variables in Based on the Hill’s Criteria of Causation [ 36 , 53 ] , “ A relationship Diagram” in Figure 1 E area ) and color the corresponding arrows in Figure 4 : Clicking the “Show Signifcance on My Diagram” button will update the user’s diagram in Figure 1 B area with colored arrows , where green ones indicate signifcant correlations and gray dashed arrows suggest insignifcant correlations . CrowdIDEA : Blending Crowd Intelligence and Data Analytics to Empower Causal Reasoning CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Figure 5 : Upon clicking the “stats + ” button , users can review the full regression results in Figure 1 E area for more details about the statistics . the diagram based on the signifcance level , as shown in Figure 4 . To show the signifcant correlation , we use the p - value [ 2 ] . If the p - value of two variables ( from source node to destination node ) is less than  = 0 . 05 , the arrow will be colored green to indicate signifcant correlation ; otherwise , the arrow is colored gray and dashed - out . Clicking the “Reset Color” button on the top of the Diagram Panel would return all colored arrows back to black . Regression analysis results respond to the selected arrows in the user diagram . The system automatically runs multiple regressions as users remove or add arrows in the Diagram Panel ( Figure 1 E area ) . The regression analysis results are generated with Python package statsmodel [ 72 ] and include regression results for both selected variables and all variables in the user created diagram . When a coefcient is less than 0 . 05 , the corresponding IV will be bold with an asterisk next to it . Users can also review a full statistics report of the regression by clicking the “stats” button , as shown in Figure 5 , where users can fnd the standard output of the statsmodel package , including coefcients , standard error , p - values , R square , etc . 4 USER STUDY DESIGN To evaluate the interface designs , we conducted experiments with a total of 54 user interviews with the system , involving 18 participants for each interface design including a overview design ( Figure 2 ) , a focus design ( Figure 3 ) , and a control design where no Crowd Panel was provided . The study plan was reviewed and approved by the university IRB before running the study , and we followed all IRB protocols throughout the study . 4 . 1 Study Material : Dataset The dataset we used for the user study was adapted from a dataset about a safety reporting app , which enables a member of a univer - sity or an organization to submit a safety - related report , to which the security agent can reply directly through chat . We chose this context because safety is a common yet critical aspect of everyone’s daily life , and people can reason causal relationships in this context without specifc domain knowledge . In addition , many causal rela - tionships may occur within the dataset , such as the safety report’s category might infuence the report’s length or the reply rate of the security teams . As a result , this dataset is suitable for a causal reasoning task . Rather than directly using the real - world data from the safety reporting app , we generated a synthesized dataset which embedded a set of realistic causal relationships that were found in the real - world data [ 55 ] . Since data visualizations using real - world data frequently contain noises , and regression results commonly present visually small deviations as statistically signifcant , individuals may spend a great deal of time investigating these visualizations and regression results in the Data Panel . To facilitate the process and allow users to fnish the task within a reasonable time , we decided to use a synthetic dataset to ensure that the data analytics in the Data Panel are generally easy to perceive . For ethical reasons , we also informed our participants that the dataset was adapted from a real system’s log that was generated by a sample of users from certain safety organizations , but not the original real - world raw data . Note that we utilized the same dataset for all condition groups , and the comparisons across groups in our study are valid and should be attributable to the availability or designs of the crowd’s beliefs . To create the synthetic dataset , seven variables from the real - world data were chosen for the study : replied by police , report category , is anonymous , report month , report hour , provide GPS , and report length ( as shown in Table 1 ) . We frst analyzed the real - world dataset to identify possible relationships between the variables . Then , we constructed a data - generating model with nine causal relationships and designed a realistic causal pattern for each causal relationship . For example , a prior study found that anonymous reports are more likely to be replied to than non - anonymous reports [ 55 ] ; therefore , we included the causal relation - ship is anonymous → replied by police in the causal model . We subsequently used this causal model to generate the dataset used in our study with the common generative data model approach , which has been widely used in evaluating visual analytics and visualiza - tion techniques [ 71 ] . Specifcally , the generation process followed the causal relationships in the causal model , i . e . , a cause variable was generated frst , then the efect variable was generated based on the values of the cause variables and the predefned probability distributions . Finally , we ran a regression analysis to make sure that the generated dataset contained statistically signifcant relation - ships described by the causal model . The dataset and the script that generated this dataset can be found in the supplementary material , and the predefned causal relationships were documented in the script . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Chi - Hsien ( Eric ) Yen , Haocong Cheng , Yilin Xia , and Yun Huang Name Type Possible values Description replied by police binary True / False Whether the police replied to the safety report report category categorical “Drugs / Alcohol” , “Noise Disturbance” The event type of the safety report report month categorical 1 , 2 , . . . , 12 The month when the safety report was submitted report hour categorical 0 , 1 , . . . , 23 The hour in a day when the safety report was submitted provide GPS binary True / False Whether the reporter shared the GPS location to the police report length numerical positive number The safety report text length written by the reporter , measured in # of characters is anonymous binary True / False Whether the reporter chose to remain anonymous to the police Table 1 : Study Material : descriptions of variables chosen from the safety reporting app dataset . 4 . 2 Study Material : Collecting Crowd’s Beliefs To gather a collection of causal beliefs for this study , we ran a pilot study with 20 people and asked them to create a causal diagram independently . In the pilot study , the crowd was given only the Diagram Panel as shown in Figure 1 . That is , we only provided a list of variable names , their descriptions , and the context of the dataset . Our intention was to collect people’s causal beliefs from their real - world experiences and allow them to brainstorm all pos - sible causal relationships . Providing data when collecting causal beliefs would potentially limit the crowd to conform to the data , which may constrain their explanation of causal beliefs . Therefore , it is recommended to collect as many hypotheses as possible from a group of people before using data to validate them [ 35 ] . Prior studies also showed that it is efective to collect causal beliefs from people without providing data . For example , Aminpour et al . [ 3 ] collected potential social - ecological causal relationships from a group of fshers by giving them a set of cards with variable names . In addition , previous research [ 58 ] has demonstrated that in the context of safety , people’s subjective beliefs may not correlate with objective measures , which implies that both are important when people are reasoning about safety . Collecting subjective crowd be - liefs allows us to observe how people perform reasoning not solely based on objective data and how they would react if the two are contradicting . The crowd was informed that their causal beliefs would be shared with future users and all of them gave consent to use their causal diagrams and narratives in subsequent studies . The identifable in - formation related to these participants was removed or anonymized before using in this study . Note that in the actual user study with CrowdIDEA , the participants were informed that the causal be - liefs were collected from people who did not have access to data visualizations or statistics . 4 . 3 Participants We recruited 54 participants ( 28 female , 26 male ) through fyers , social networks , email lists , and online forums . To qualify for the study , a participant must : 1 ) be age 18 or older ; 2 ) have received training in Data Science , Economics , or other STEM ( Science , Tech - nology , Engineering , or Mathematics ) felds ; and 3 ) have applied regressions in data analysis projects . In addition , they had to an - swer three basic questions about regression analysis correctly to be qualifed for our study , which ensured that they knew how to inter - pret regressions’ coefcients , p - values , and signifcance levels , as expected for our target users . Most participants ( 49 out of 54 ) were between 18 and 29 years old , while the other 5 participants were between 30 and 39 years old . Participants either held a Bachelor’s degree or above or were pursuing a Bachelor’s degree . Twenty - seven participants held or were majoring in statistics , whereas the other participants were in other majors such as computer science , economy , and information sciences . Thirteen participants reported that they had used a safety reporting app before , with 3 of them having used it for a moderate amount of time . For easier reference , we numbered our participants based on their condition groups in our paper : C01 to C18 refer to the participants receiving the control design , O01 to O18 refer to the participants receiving the overview design , and F01 to F18 refer to the participants receiving the focus design . 4 . 4 Procedure Participants were randomly assigned into one of the three study groups ( 18 participants per group ) : the overview group used the overview design , the focus group used the focus design , and the control group received a design without the Crowd Panel , but they can use the Diagram Panel and the Data Panel as in other designs , whose interface designs could be found in section 3 . Moreover , participants were not told which group they were in nor that there were diferent condition groups in our study . The study was conducted individually in - person or through online Zoom meetings , which took 1 to 1 . 5 hours on average . The participants could use a provided computer ( in - person meeting ) or their own computers ( online meeting ) to join our study via our system’s website . In either case , participants were asked to share their screens and give us permission to record their screens throughout the task , allowing us to observe their usage of the system . At the end of the interview , participants were compensated with 10 dollars . Each participant needs to complete a tutorial , including a practice task , which aims to get participants familiar with the interface of CrowdIDEA they would use for the task . During the tutorial , we made it clear to the participants that the crowds shared their causal beliefs only based on their experience without having access to the Data Panel . We also informed the participants that they should decide for themselves if the patterns presented by the data made CrowdIDEA : Blending Crowd Intelligence and Data Analytics to Empower Causal Reasoning CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany sense to them according to their own experiences and / or Crowd Panel . At the end of the tutorial , participants were given a practice task , which used a dataset diferent from the data used in the main task . After the practice session , the safety reporting app context was introduced , and the main task was given . In the main task , participants were asked to create a complete causal diagram in 20 minutes using the information presented on their interface . Before the task , we explained the task goal “Draw a complete causal diagram that you believe best explains the dataset , ” which was displayed at the top of the screen . We also remind partic - ipants of the time limit , which helps make their reasoning outcomes fairly comparable and observe how participants prioritize using diferent parts of the system . Due to time constraints , participants are allowed to skip writing their explanations for a relationship . During the task , participants were asked to “think aloud” , that is , speak out what they were thinking when using the system . We did not comment on the validity of their causal hypothesis or rea - soning throughout the task but only answered questions about the interface or study logistics . For every seven minutes ( i . e . , twice in the task ) , the interface would remind participants of all available features in the system to guarantee that participants are well in - formed . In other words , if a participant did not utilize a feature in the task , it was on purpose . At the end of the task , participants were told that the causal diagram in the Diagram Panel would be their fnal diagram , and they could review it one more time and remove arrows that they may forget to do during the task . After the task , we ofered participants a brief questionnaire re - garding their system usage experience , followed by a semi - structured interview for us to learn more about their experiences and thoughts on the tool . The interview questions covered participants’ strate - gies for constructing causal diagrams using diferent components on the interface and the benefts or drawbacks of those features . If participants only provided binary answers , follow - up questions were asked to get deeper insights . To reduce the efect of response biases , we created an open , relaxing interview environment and reminded participants that any criticism was always welcome . 4 . 5 Data Analysis The data collected in our study include the causal diagrams , the narratives of causal relationships , the audio and screen recordings taken during the tasks and post - study interviews , and low - level action logs stored in the system . To perform quantitative analysis on the causal diagrams , we converted them into matrices , where the rows and columns rep - resent nodes , and the cells contain values of 0 ( the arrow is not drawn ) or 1 ( the arrow is drawn ) . We used the matrices to count and compare the number of arrows each participant drew across three condition groups . The participants’ diagrams were then compared to the crowd causal diagram and data - generating model . In the end , we discussed the results to answer RQ1 . It is worth noting that we did not measure whether the arrows added by participants were correct or not based on the data - generating model , but only focused on what arrows they added or explored during the task . For the post - study interview , we transcribed the interviews into text transcriptions , then conducted a thematic analysis using an inductive coding approach [ 78 ] . Two of the authors independently did open coding on three participants’ interviews , one from each group , to reach an agreed coding book . One author continued to code the remaining transcripts and regularly discussed with the other author to refne the coding book . If there were conficts be - tween two authors during the coding process , a third author would be involved . After all interviews were coded , the authors further discussed and grouped the codes into themes that could answer RQ2 . 5 RESULTS 5 . 1 Outcomes of Participants’ Causal Reasoning ( RQ1 ) In this section , we analyzed the diagrams created by 54 participants ( 18 diagrams per condition group ) to understand how the designs of integrating the crowd’s beliefs and data analytics afect an individ - ual’s causal reasoning . We compared the number of relationships presented in the diagrams and examined the system log of partic - ipants’ interactions to understand their usage of the features ( in section 5 . 1 . 1 ) . We also compared the graph structure of the causal diagrams between the condition groups ( in section 5 . 1 . 2 ) . 5 . 1 . 1 Inspiring causal reasoning with crowd’s beliefs . Participants in the study inserted or removed arrows to show whether there are causal relationships between two variables . To better understand how the crowd’s beliefs afected participants’ decisions on arrow numbers in general , we used statistical methods to measure both the number of arrows included in the fnal causal diagram at the end of the study and the number of arrows they added but then removed during the task . Overview design yielded more relationships in the causal diagrams . We counted the number of arrows in the causal diagrams submitted by the participants . On average , the control participants drew 7 . 0 arrows ( SD = 3 . 6 ) , whereas the overview participants drew 11 . 3 arrows ( SD = 4 . 7 ) , and the focus participants drew 9 . 4 arrows ( SD = 3 . 4 ) . To test whether there is a statistical diference among the three condition groups , we ran pairwise Mann – Whitney U tests (  = 0 . 05 ) . The results demonstrate that the number of the fnalized arrows by the overview participants is signifcantly higher than that in the control participants ( U = 81 . 0 , p = . 005 ) . Further , we conducted a power analysis that discloses the mag - nitude of the diference that a Mann - Whitney U test could have discovered . The G * Power [ 18 ] was utilized for power analysis , and Cohen’s d efect size [ 11 ] ( d = 1 . 03 ) is calculated , which indicates large efect size [ 47 ] . The result found adequate power to have detected large signifcant diference ( U = 81 . 0 , power = 0 . 83 , d = 1 . 03 , n = 18 , p = . 005 ) [ 16 , 93 ] , indicating that the overview design led partic - ipants to include signifcantly more arrows in their causal diagrams compared to the control design . Despite that the p - value between the control participants and the focus participants is p = . 04 < 0 . 05 , the power analysis ( U = 107 . 0 , p = . 04 , d = 0 . 70 , power = 0 . 50 ) indicates that there is insufcient statistical power to detect the signifcance . Therefore , we conclude that there is no large signifcant diference between the control participants and the focus participants . Lastly , the Mann - Whitney U test shows that there is no signifcant difer - ence between the focus participants and the overview participants ( U = 132 . 5 , p = . 178 ) . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Chi - Hsien ( Eric ) Yen , Haocong Cheng , Yilin Xia , and Yun Huang Overview participants removed fewer arrows during causal reasoning . Before fnalizing their diagrams , participants could delete arrows at any time . To further understand their reasoning behavior , our system logged the number of arrows they drew but eventually removed . We calculated the total of removed arrows , indi - cating the relationships they explored and rejected . Overall , the con - trol participants eliminated 6 . 7 arrows on average ( SD = 3 . 6 ) , while on average , the focus participants removed 5 . 0 arrows ( SD = 3 . 7 ) , and the overview participants deleted 3 . 6 arrows ( SD = 3 . 1 ) . Pairwise Mann - Whitney U tests show that the number of removed arrows in the control and overview participants are signifcantly diferent ( U = 71 . 0 , p = . 002 , d = 0 . 92 , power = 0 . 8 ) . That is , compared to the con - trol participants , the Overview participants removed signifcantly fewer arrows than the control participants . Moreover , there is no large signifcant diference between the focus and control partici - pants ( U = 97 . 0 , p = . 019 , d = 0 . 47 , power = 0 . 26 ) and between the focus participants and the overview participants ( U = 115 . 0 , p = . 178 ) . Overview design reduced the use of the Data Panel . We com - pared the use of the Data Panel among the three condition groups . Specifcally , we calculated the clicks of the “Show Signifcance on My Diagram” button ( as shown in Figure 4 ) . The control partici - pants clicked an average of 8 . 5 times ( SD = 8 . 1 ) , whereas the focus participants clicked 5 . 5 times ( SD = 7 . 2 ) and the overview participants clicked 2 . 6 times ( SD = 2 . 5 ) . The pairwise Mann – Whitney U tests and power analysis results showed that the overview participants used this feature signifcantly less than the control participants ( U = 93 . 5 , p = . 03 , d = 0 . 98 , power = 0 . 80 ) , though there were no signifcant dif - ferences between overview and focus participants ( U = 130 . 5 , p = . 33 ) , nor between focus and control participants ( U = 126 , p = . 26 ) . 5 . 1 . 2 Without Changing the Similarities of Causal Relationships . The above results showed that the overview design signifcantly impacted the causal reasoning outcomes ( e . g . , increased number of relationships generated ) and processes ( e . g . , reduced use of the Data Panel ) . We would expect that the structure of the generated re - lationships would be signifcantly changed . However , the following results showed an unexpected result . Specifcally , to examine whether the new design impacted the structures of the generated relationships , we applied a distance mea - surement called Graph Edit Distance ( GED ) [ 42 ] , which has been widely used to compare the similarity of causal models [ 17 , 22 , 79 ] . GEDs were calculated between each participant’s diagram and the crowd diagram ( provided by the Crowd Panel ) , as well as between the participant’s diagram and the data - generating model ( provided by the Data Panel ) . Then , the GEDs were used to compare the distances across the three conditions . All directed acyclic graphs ( DAGs ) [ 28 ] were frst quantifed before analyzing , then the sim - ilarity of diagrams was measured by the GEDs . For instance , if a participant’s diagram had three more arrows that were not in the crowd diagram and did not include two arrows that were in the crowd diagram , the GED of this participant is fve . Note that the data - generating model had nine arrows ( as illus - trated in Figure S1 ) , and the crowd diagram had 35 arrows ( as illustrated in Figure 2 ) . No signifcant efects on the similarity between the crowd’s and participants’ causal relationships . In addition to the data - generating model , we were interested in how the crowd’s causal diagram changed participants’ perceptions of causal relationships as a whole . In other words , we would like to know if the overview de - sign , as opposed to the other two designs , would drive participants to generate causal diagrams that were more similar to the crowd’s causal diagrams . We computed the GED between the participants’ diagrams to the crowd’s causal diagram : the control participants have an average GED of 27 . 9 ( SD = 4 . 4 ) , whereas the focus partici - pants have an average GED of 25 . 8 ( SD = 4 . 0 ) , and the overview par - ticipants have an average GED of 23 . 9 ( SD = 4 . 9 ) . We also conducted the Mann - Whitney U tests across the three condition groups to com - pare the GEDs among three condition groups , followed by a power analysis . The results showed that the diference between the con - trol and overview conditions ( U = 82 . 50 , p = . 006 , d = 0 . 86 , power = 0 . 70 ) was not large enough ( power < 0 . 8 ) , nor between the control and focus conditions ( U = 101 . 0 p = . 027 , d = 0 . 50 , power = 0 . 30 ) . There was no signifcant diference between focus and overview conditions ( U = 138 . 0 , p = . 227 ) . No signifcant efects on the similarity between the data - generating model and participants’ causal relationships . By comparing the participants’ diagrams with the data - generating model , we investigated how our system designs afect the similarity between the data - generating model and the participants’ perception of casual relationships . We computed GED across three condition groups . The average GED for the control participants is 6 . 4 ( SD = 2 . 1 ) , while the average GED for the overview participants is 6 . 7 ( SD = 2 . 7 ) , and the average GED for the focus participants is 6 . 3 ( SD = 1 . 6 ) . The Mann - Whitney U tests yielded no signifcant results , as p - values for three paired comparisons are above 0 . 05 . As a result , the participants provided with the crowd’s beliefs did not create causal diagrams that were signifcantly more similar to the data - generating model than the participants who only used the Data Panel for causal reasoning . 5 . 2 Diferent Strategies of Causal Reasoning Enabled by CrowdIDEA ( RQ2 ) The above RQ1 results showed that the Crowd Panel with the overview design impacted the outcomes of causal reasoning by increasing the number of relationships and reducing the use of the comparison with data - generating model ; however , there were no signifcant efects on the similarity between the crowd’s diagram and participants’ causal relationships . The results suggested that participants received inspiration from the crowd’s beliefs but did not blindly draw diagrams by following the crowd’s diagram . To understand how participants used the Crowd Panel and Data Panel ( RQ2 ) , we analyzed the system logs of participants’ behav - ioral data and triangulated it with participants’ interview data . We were able to identify several strategies participants adopted when interacting with CrowdIDEA at diferent times of their causal rea - soning , e . g . , bootstrapping , revising , evaluating , acknowledging , rejecting , and interpreting their causal beliefs . 5 . 2 . 1 Strategies of Using the Crowd Panel Features . We identifed two major ways that participants developed in using the Crowd Panel . Bootstrapping causal relationships by browsing the crowd diagram and narratives . Many participants used the Crowd Panel to jump - start with causal reasoning . For example , before creating CrowdIDEA : Blending Crowd Intelligence and Data Analytics to Empower Causal Reasoning CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany causal diagrams using the Diagram Panel , eight participants looked at the thicker arrows to draw some ideas . O10 shared that , “ I think if there is an arrow that is highly popular , it is likely that it is [ statis - tically ] signifcant , because there are a lot of people thinking the same way . ” F02 and F11 also added arrows that were frequently shared by the crowds , as F02 shared : “ Even if you know nothing [ about the data ] , you could still construct a rough causal diagram based on the crowd’s beliefs . . . During the task , I would add the arrows with higher than 70 % or 60 % [ of crowds who also added the arrow ] . . . and I can check it out later and modify it based on the data panel on the right . ” Besides connecting the variables , 26 ( out of 36 ) participants in the treatment groups referred to the narratives of the crowd’s beliefs for considering the directions of the arrows . As F07 explained , “ reading through the narratives for both directions , you can see which ones made more sense . ” The combination of the percentage of crowds added to the arrow and narratives gave participants a clearer picture of whether a causal relationship existed between the variables . As F08 said , “ I checked if anything had any efect on the response variable , and then I checked if a lot of the crowd had also done the same , and if their reasoning was logical enough , then I went with it . ” Deepening causal reasoning by comparing with the crowd’s beliefs . After creating an initial version of the diagrams , either by drawing insights from the crowd’s or starting with one’s own opinions , 25 participants mentioned that they compared their draft diagram with the crowd’s beliefs before fnalizing their diagrams . When participants found that the majority of crowds had the same arrows as they did , or the crowd’s narratives shared a similar ratio - nale , they would be more confdent in their causal diagrams . For example , F01 remembered doing so : “ after I drew each arrow , I would look at the [ crowd’s beliefs ] , and if the majority of people agreed with me , it made me feel a little more confdent . ” Meanwhile , diferent perspectives from the crowd made the par - ticipants refect on their rationales , which might lead to adding new arrows , changing arrow directions , or deleting arrows . For example , participants would read the narratives to understand why a relationship was in the crowd’s diagram . If they thought the narra - tives to be convincing , they might follow the crowd’s beliefs . As an example , O05 added an arrow that he missed earlier after checking the crowd’s beliefs , as he further elaborated , “ [ crowd’s beliefs ] could remind me to notice arrows that I did not add but strongly agreed by the crowds , which were worth checking and rethink why I did not think the relationship exist earlier . ” O11 changed the arrow report length → is anonymous to is anonymous → report length when reviewing his causal diagram because “ I noticed when check - ing my diagram that the majority of the crowd added the arrow in the other direction as I did , so I checked how the majority of crowd explained in narratives , which turned out to be making sense to me . ” On the contrary , O07 removed an arrow provide GPS → report hour , as he explained “ only one person in the crowd added this arrow , and the strength level [ specifed by the person ] was not very high . I further thought about it and then believed maybe GPS afects other variables , which in turn afect report hour . ” 5 . 2 . 2 Usage of the Data Panel Features . Regarding the usage of the Data Panel , there were two camps of participants . Some participants tended to follow their own beliefs , e . g . , 12 participants mentioned that they did not rely on the data to infer causal relationships . For example , C10 kept all gray arrows ( the ones that were not signifcant based on the Data Panel ) since “ if you remove them , it changes the p values , and you can make something more signifcant than it really is . ” O08 kept a gray arrow , report hour → provide GPS , based on her own experience that “ if it’s later at night , people are less likely to provide GPS . I mean , obviously , it’s a safety app . It’s there for you . But I don’t think providing your location to some random person in the night [ is a good idea ] . ” The remaining participants made use of the Data Panel for dif - ferent purposes to support their causal reasoning . There were three major ways of using the Data Panel . Driving causal reasoning by data visualization . Participants mainly used the data visualization panel to look for obvious patterns between two variables and to decide whether the two variables were causally related . For example , by reviewing the data visualization between is anonymous and replied by police , O15 noticed that these two variables “ are at least causally related by some degree even without reading the [ regression results ] . ” Some participants would only draw arrows when they identifed patterns on data visualization , e . g . , O04 said , “ I would draw an arrow only if data visualization showed me a trend or seemed to be related . ” Validating causal relationships by comparing with data correlations . Most participants used the regression results to iden - tify signifcant and / or insignifcant relationships when constructing their causal diagrams . They either checked the p - values in the re - gression results or used the “Show Signifcance” button to quickly highlight the signifcant relationships on their causal diagram . Some participants used the “Show Signifcance” button through - out the task to refne their diagrams . For example , F07 described her strategy , “ So what I would do is once I add an arrow , I would just check the signifcance . . . when it’s signifcant , I would likely leave the arrow , and if it isn’t , I’ll remove it . ” Validating causal relationships by checking details of the statistics . The full regression results were utilized by some par - ticipants who mentioned they were familiar with outputs from statistical programs . Besides the p - values that most of these partici - pants checked , some participants checked other statistics , including correlation coefcient ( six participants ) and R square values ( four participants ) . C07 preferred more statistics than p - values since “ the signifcance level could be diferent in diferent situations . Sometimes it would be 0 . 05 , while sometimes it would be 0 . 01 . ” She also checked correlation coefcients and standard errors . 5 . 2 . 3 Leveraging the Complementary Support from the Crowd and Data Panels . The Crowd Panel allows people to interact with subjective causal beliefs , and the Data Panel presents objective views of the data . They provide complementary support for participants to improve their causal reasoning processes . Understanding the causation behind unexpected data pat - terns through the crowd’s beliefs . When using the integrated system with both the Crowd Panel and the Data Panel , many partici - pants used the crowd’s narratives to explain the causal relationships between correlated variables . This strategy was particularly useful when they were unable to understand data patterns . For example , F09 found the data pattern for report hour → report category diferent from expected , as he initially thought “ there would be like , CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Chi - Hsien ( Eric ) Yen , Haocong Cheng , Yilin Xia , and Yun Huang more noise complaints in nighttime , I guess , but it doesn’t seem like that , because it looks like it’s just picks up some random times but doesn’t really say that it’s all nighttime . ” However , after reading the narratives from the crowd that helped him understand this causal relationship , he then added the arrow . Meanwhile , the crowd’s beliefs were also used to identify vari - ables that could be causally related but not correlated . For instance , the relationship report length → replied by police was added by 80 % of people in crowds , but it became statistically insignif - cant when report hour → replied by police was added to the causal diagram . By reading narratives , O07 decided to keep the arrow report length → replied by police because “ I further read through some narratives . They shared the views with me ; there - fore , [ I kept this arrow ] . ”F18 also kept the arrow report length → replied by police because he believed that the insignifcant regression results could be caused by “ an issue of the sample of data . If the data was using a diferent sample , I and crowds could be correct . ” Tending to follow data when having credibility issues with the crowd’s beliefs . Among the participants who used the Crowd Panel , more than half ( 19 out of 36 ) mentioned that they had con - cerns with the credibility of the crowd’s beliefs . Some perceived misleading or wrong information when zooming into the narra - tives for specifc arrows . For example , F12 found several narratives explaining the relationship report month → report category misplaced as narratives for the relationship report hour → report category . She also found that some narratives for one arrow could support a relationship in the other direction . The lack of knowledge about the background of crowds can also raise doubts about their credibility . Before the study , the partici - pants were informed that crowds were not exposed to data when building their causal diagrams ; therefore , they did not blindly take the crowd’s beliefs . For example , F17 also mentioned , “ You know , it [ the crowd’s beliefs ] is not necessarily based on the data . So just because it’s common may or may not be a real efect . ” F04 doubted whether the crowds actually existed : “ this [ crowd’s beliefs ] probably might be something that’s created on purpose to throw people of , like , you never know , do these participants actually exist . ” When they had credibility issues with the crowd’s beliefs , they tended to fol - low data . For example , 25 participants noticed that some common crowd’s beliefs were not statistically signifcant based on the re - gression results . Eighteen participants followed the results from the Data Panel and removed these arrows from their causal diagram . F03 explained her rationale , “ it could come from a generalization from people’s experiences , but they can’t be 100 % sure that they were correct . . . it is similar to making a hypothesis test , and the test was rejected , so it cannot be concluded that the causal relationship exists . ” Using both panels to mitigate potential biases . Out of the 36 participants in the treatment groups , 19 reported that they had concerns that the crowd’s beliefs could lead to potential biases . Therefore , 11 of these 19 participants checked the crowd’s beliefs only to deepen the causal reasoning process after they had created an initial diagram based on their own beliefs and data analytics to avoid being preconceived . For example , O11 shared , “ If you saw others’ perspectives frst , that could afect how you think about the relationship , but the bias could be avoided if you think about it your - self frst . You can check the crowds later and maybe compare your perspective with others’ perspectives to achieve a relatively objec - tive viewpoint and decision . ” Alternatively , O08 , who checked the crowd’s beliefs for bootstrapping , considered her strategy to be not ideal , “ I think maybe I should look at the data before looking at crowd’s beliefs . . . I think [ crowd’s beliefs infuenced ] what variables I focused on , and what variables I was more likely to flter out . ” Eventually , participants who shared such concerns leveraged between the three panels of CrowdIDEA to mitigate their concerns , as O02 reviewed the process : “ at frst , I’ll draw my own diagram , and then I will see what other people have drawn . And then before including them in my diagram , I would do some tests with signifcance , see the visualization , and then add them . ” 6 DISCUSSION In this section , we refect on the signifcant efects of CrowdIDEA on causal reasoning , showing new empirical understandings about the strategies people can apply to conduct causal reasoning by leveraging both quantitative and qualitative data . We then propose an expanded model for knowledge generation and discuss opportu - nities for future designs for exploratory data analysis and enriched data input for causal reasoning . 6 . 1 CrowdIDEA : a Novel System Manifesting New Strategies for Causal Inspiration and Internalization Inspiration . Prior work has suggested individuals are generally poor at thinking of all hypotheses [ 21 ] and a group of users should gather and brainstorm hypotheses in the early stage [ 35 ] . Our fnd - ings for RQ1 in section 5 . 1 showed the signifcant efect of the overview design and the integration of three panels on participants being able to generate more causal hypotheses than only informed by data statistics . Our fndings for RQ2 in section 5 . 2 provided detailed accounts of diferent strategies participants applied , e . g . , bootstrapping and deepening their causal reasoning , which helped explain the signifcant results in RQ1 . Specifcally , when partici - pants ran out of ideas from their own knowledge , many would start to explore the crowd’s beliefs and identify the ones that they would have agreed with . According to the Prepared Mind theory , people are more motivated to utilize new information when their reason - ing reaches an impasse [ 73 ] . Thus , the crowd’s beliefs were a good stimulus for inspiring people to discover their blind spots , which is one of the major challenges in causal reasoning [ 21 , 35 , 94 ] . Internalization . Previous studies have demonstrated that sta - tistical data can be used to infer causality [ 54 ] and that the same data can be interpreted to generate diferent causal diagrams [ 62 ] . Therefore , only relying on the Data Panel to draw causal inferences has major limitations . The CrowdIDEA usage helped mitigate the limitations . Specifcally , our results in section 5 . 2 . 3 showed that some participants were able to leverage the complementary support from the panels to gain a deeper understanding of the data and the crowd’s beliefs . The process of forming insights is regarded as internalization . For example , participants mentioned that they would add or look deeper into the arrows that were drawn by the majority of the crowd . Such behavior may be contributed by herd mentality [ 4 , 68 ] , which explains that people tend to follow what most people do . While this may be an efcient strategy to prioritize CrowdIDEA : Blending Crowd Intelligence and Data Analytics to Empower Causal Reasoning CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany reasoning focus , it may lead to a groupthink efect [ 39 ] , where peo - ple converge to cohesive thinking and therefore neglect ideas from the minority that might be worth investigating . Our investigation into RQ2 revealed that providing data analytics while participants explored relationships encouraged them to reduce bias that may have been introduced by the crowd . 6 . 2 Theoretical Implications : Extending a Knowledge Generation Model with Crowd Intelligence Many cognitive models have been proposed in the literature to illustrate the interaction between an analyst and a visual analytics system , such as Sacha et al . ’s Knowledge Generation Model [ 70 ] , Pirolli and Card’s Sensemaking Model [ 67 ] , and other reasoning models [ 26 , 27 , 59 ] . However , all these models focus on the rea - soning processes based on quantitative ( objective ) data and do not consider the roles and efects of qualitative ( subjective ) data . In this work , we proposed novel system designs that enable users to gain insights from other people’s beliefs together with data statistics . The empirical fndings expand the existing theoretical models for knowledge generation . Our work makes signifcant contributions with a novel system , empirical fndings , theoretical implications , and design suggestions for crowd - informed causal reasoning . For example , as shown in Figure 6 , a recent Knowledge Genera - tion Model proposed by Sacha et al . provides a generic framework by unifying many other cognitive models [ 70 ] . Our results demon - strate a new component , Crowd Intelligence , and two associated reasoning processes can be added to the Knowledge Generation Model . As a simplifed description of the original model , it decom - poses an analyst’s reasoning processes as follows : one uses their Knowledge to generate a specifc Hypothesis about certain data pat - terns , then performs Actions on a visual analytics tool to model or visualize data . By interpreting the data results , one discovers new Findings from the data , distills Insights relevant to their hypothesis , and updates their Knowledge accordingly . CrowdIDEA can be used by researchers to test this new model by generating causal beliefs with diverse datasets . 6 . 3 Design Implications 6 . 3 . 1 Informing Exploratory Data Analysis ( EDA ) by Crowd Intelli - gence . In this paper , we evaluated two designs of the Crowd Panel . The overview design implements the “overview - then - details” princi - ple [ 75 ] , and the focus design supports the “focus - then - expand” principle [ 81 ] . The overview design led to signifcantly more causal hypotheses generated . This design is particularly useful for ex - ploratory data analysis ( EDA ) when the goal is to understand data characteristics , explore interesting data patterns , and brainstorm possible hypotheses that may lead to new data collection and ex - periments [ 80 ] . In EDA , rigorous statistical testing and conclusion drawing are not the focus ; thus overview design is recommended to help people discover new possible hypotheses and collect a rich set of potential causal relationships more efciently . Meanwhile , our results suggest two major issues that need ef - fective solutions . One is about the credibility issues of the crowd’s beliefs . When people explain their causal beliefs , their narratives may not be accurate or precise , and may unavoidably be wrong or misleading [ 94 ] . Prior works in collaborative reasoning have also proposed methods that may be considered . For example , Furtado et al . [ 19 ] developed a reputation - rating system for collaborators in a crowd - powered crime reporting system to balance information reliability . Kang et al . [ 44 ] suggested that systems should identify less - skilled users and capture low - quality input . The system may allow users to rate each other and would suppress the displaying of lower - rated insights to reduce errors . User - rated causal narratives could be used as training data to create models for automatically identifying low - quality causal reasoning . The second issue is about the potential biases introduced by the crowd’s beliefs . Our fndings in section 5 . 2 revealed that partici - pants could have herd biases ( e . g . , following the majority when seeking inspiration ) and confrmation biases ( e . g . , interpreting or favoring the crowd’s beliefs that supported the participants’ views ) . Even though many participants were aware of these biases and triangulated data between the panels to fnalize their own views , future systems may design features that could support them to mitigate these biases more efciently . For example , our focus design made recommendations for new arrows based on what arrows the user had added , and future design may indicate whether the arrows were supported by data or not in the Crowd Panel . According to our participants’ feedback , future designs could also ofer users the option to customize when the crowd’s beliefs were delivered throughout the process . For example , users could choose to receive the crowd’s beliefs at the beginning , during , or at the conclusion of the bootstrapping and validating phases . This would allow par - ticipants to review the crowd’s beliefs after generating their own beliefs , should they prefer to do so . 6 . 3 . 2 Turning Reasoning - Process Data into Causal Reasoning Input . The CrowdIDEA’s Data Panel is designed to provide data visu - alization , correlation results , and regression statistics . On the one hand , many participants used the features to validate their causal diagram , quickly understand data trends , or determine whether a causal relationship was signifcant . On the other hand , participants could also be biased or lead to incorrect conclusions due to the po - tential misuse of statistics . Below , we provide the situations where users’ reasoning - process data could be recorded by the Data Panel to improve the processes of causal reasoning . First , some participants tended to use only the p - values in the regression results to identify and keep signifcant relationships in their causal models . While p - values of the regression results may aid decisions in causal reasoning , they cannot be used alone to directly infer causal relationships [ 20 ] . To help users distinguish be - tween correlation relationship and causal relationship , the system could implement more features such as detecting potential con - founders [ 30 ] or pairing the user with data experts [ 43 ] . In addition , we observed that some participants ran many regressions on one variable with diferent variables to get signifcant p - values . To help people avoid p - hacking strategies [ 33 ] and the infated Type I error from multiple tests , we suggest that future systems should keep track of other reasoning - process data , such that the alpha level can be adjusted accordingly [ 97 ] . We also suggest future systems improve this panel by refecting more statistics , e . g . , efect sizes , to help users make a more informed decision . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Chi - Hsien ( Eric ) Yen , Haocong Cheng , Yilin Xia , and Yun Huang B . Internalization A . Inspiration Crowd Intelligence Figure 6 : Extending Sacha et al . ’s Knowledge Generation Model [ 70 ] by adding Crowd Intelligence and two associated reasoning processes : ( A ) Inspiration , the process of generating hypotheses based on Crowd Intelligence , and 2 ) Internalization , the process of forming insights using Crowd Intelligence . Second , we observed that participants sometimes got confused when a relationship became insignifcant after they edited their causal diagram . This is a common yet confusing phenomenon that frequently occurs during the causal reasoning process . For example , in our dataset , adding a confounding variable may change the signifcance level of another relationship . With these seemingly conficting results , some participants were not sure whether adding the confounding variable would be better or worse . The system could allow users to mark or comment on a relationship that is not included in the fnal diagram . Users’ thoughts about how such thinking occurs and how such thinking should occur , called meta - thinking [ 89 ] , could also be used to explain the data . Future systems may allow users to log their thought processes as intelligent cues to assist interpretation [ 95 ] . 6 . 4 Limitations and Future Work We acknowledge several limitations in our work . First , given the time restriction of the task , participants only had a limited time ( 20 minutes ) for the main task , and their behavior may be diferent if more time had been allowed . Second , our research only mea - sured the causal relationships investigated by participants without considering whether their hypotheses were true or not in real life . Future work could investigate how crowd intelligence afects the correctness of the causal diagrams made by the users . Third , our Data Panel did not allow participants to customize the type of data visualization . Literature has shown that people perceive causal strength diferently when the same trend is plotted in difer - ent chart types [ 91 ] . Thus , future work could investigate whether more fexibility in presenting data analytics afects how people use crowd - informed causal reasoning tools . Fourth , in our study , the crowd’s causal beliefs were collected without them reviewing any data . However , the crowd’s causal models may be diferent if the data is provided , and participants’ behavior may be diferent if such crowd’s models are collected . Future work could investigate whether providing the same data analytics to the contributors of crowd’s causal model would afect the perceived trustworthiness and how people use the crowd’s causal beliefs . In addition , as the dataset we used for our study is relatively small with only seven attributes , we did not include features that could help participants explore a larger dataset with more attributes ( e . g . , more than 10 nodes ) . However , both designs of our system can be easily expanded to accommodate more attributes . For the overview design , users can zoom into part of the diagram or use flters to only keep interested attributes on the diagram . For the focus design , users may flter the attributes of interest for the system to recommend on . Future work may explore other ways to present both crowd intelligence and data analytics with more attributes more efciently . 7 CONCLUSION In this paper , we present a crowd - informed causal reasoning sys - tem , CrowdIDEA . The novel system allows users to conduct causal reasoning with crowd intelligence , a collection of causal beliefs pro - vided by a crowd , and data analytics , including data visualization and regression results . We designed two representation designs of crowd intelligence : the overview design , which shows a crowd diagram aggregated from the crowd’s beliefs , and the focus design , which provides the crowd’s beliefs relevant to the analyst’s reason - ing focus . We evaluated the system through a user study with 54 participants and found that the overview design led to signifcantly more causal relationships included in the participants’ causal dia - grams . Additionally , participants were able to discover blind spots , avoid potential biases , and make sense of unexpected data results by devising strategies that incorporating both crowd beliefs and data statistics . ACKNOWLEDGMENTS The authors would like to thank the anonymous reviewers for their eforts and valuable comments . CrowdIDEA : Blending Crowd Intelligence and Data Analytics to Empower Causal Reasoning CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany REFERENCES [ 1 ] Rajeev Agrawal , Anirudh Kadadi , Xiangfeng Dai , and Frederic Andres . 2015 . Challenges and opportunities with big data visualization . In Proceedings of the 7th International Conference on Management of computational and collective intEl - ligence in Digital EcoSystems . 169 – 173 . [ 2 ] Haldun Akoglu . 2018 . User’s guide to correlation coefcients . Turkish journal of emergency medicine 18 , 3 ( 2018 ) , 91 – 93 . [ 3 ] Payam Aminpour , Steven A Gray , Antonie J Jetter , Joshua E Introne , Alison Singer , and Robert Arlinghaus . 2020 . Wisdom of stakeholder crowds in complex social – ecological systems . Nature Sustainability 3 , 3 ( 2020 ) , 191 – 199 . [ 4 ] Solomon E Asch . 1956 . Studies of independence and conformity : I . A minority of one against a unanimous majority . Psychological monographs : General and applied 70 , 9 ( 1956 ) , 1 . [ 5 ] Reuben M Baron and David A Kenny . 1986 . The moderator – mediator variable distinction in social psychological research : Conceptual , strategic , and statistical considerations . Journal of personality and social psychology 51 , 6 ( 1986 ) , 1173 . [ 6 ] Daniel Berenberg and James P Bagrow . 2018 . Efcient crowd exploration of large networks : The case of causal attribution . Proceedings of the ACM on Human - Computer Interaction 2 , CSCW ( 2018 ) , 1 – 25 . [ 7 ] Dorrit Billman , Gregorio Convertino , Jef Shrager , P Pirolli , and J Massar . 2006 . Collaborative intelligence analysis with CACHE and its efects on information gathering and cognitive bias . In Human Computer Interaction Consortium Work - shop . [ 8 ] Yang Chen , Jamal Alsakran , Scott Barlowe , Jing Yang , and Ye Zhao . 2011 . Support - ing efective common ground construction in asynchronous collaborative visual analytics . In 2011 IEEE Conference on Visual Analytics Science and Technology ( VAST ) . IEEE , 101 – 110 . [ 9 ] Haeyong Chung , Sai Prashanth Dasari , Santhosh Nandhakumar , and Christo - pher Andrews . 2017 . Cricto : Supporting sensemaking through crowdsourced information schematization . In 2017 IEEE Conference on Visual Analytics Science and Technology ( VAST ) . IEEE , 139 – 150 . [ 10 ] Haeyong Chung , Seungwon Yang , Naveed Massjouni , Christopher Andrews , Rahul Kanna , and Chris North . 2010 . Vizcept : Supporting synchronous col - laboration for constructing visualizations in intelligence analysis . In 2010 IEEE Symposium on Visual Analytics Science and Technology . IEEE , 107 – 114 . [ 11 ] Jacob Cohen . 1992 . Statistical power analysis . Current directions in psychological science 1 , 3 ( 1992 ) , 98 – 101 . [ 12 ] Tuan Nhon Dang , Paul Murray , Jillian Aurisano , and Angus Graeme Forbes . 2015 . ReactionFlow : an interactive visualization tool for causality analysis in biological pathways . In BMC proceedings , Vol . 9 . Springer , S6 . [ 13 ] David Danks . 2014 . Unifying the mind : Cognitive representations as graphical models . Mit Press . [ 14 ] Denver Dash , Mark Voortman , and Martijn De Jongh . 2013 . Sequences of mecha - nisms for causal reasoning in artifcial intelligence . In Twenty - Third International Joint Conference on Artifcial Intelligence . [ 15 ] Li Du , Xiao Ding , Kai Xiong , Ting Liu , and Bing Qin . 2021 . ExCAR : Event Graph Knowledge Enhanced Explainable Causal Reasoning . In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) . 2354 – 2363 . [ 16 ] Lisa A Elkin , Jean - Baptiste Beau , Géry Casiez , and Daniel Vogel . 2020 . Manipula - tion , learning , and recall with tangible pen - like input . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1 – 12 . [ 17 ] Nicolas Espinosa Dice , Megan Kaye , Hana Ahmed , and George Montañez . 2021 . A Probabilistic Theory of Abductive Reasoning . In Proceedings of the 13th Inter - national Conference on Agents and Artifcial Intelligence , Vol . 2 . [ 18 ] Franz Faul , Edgar Erdfelder , Albert - Georg Lang , and Axel Buchner . 2007 . G * Power 3 : A fexible statistical power analysis program for the social , behavioral , and biomedical sciences . Behavior research methods 39 , 2 ( 2007 ) , 175 – 191 . [ 19 ] Vasco Furtado , Leonardo Ayres , Marcos De Oliveira , Eurico Vasconcelos , Carlos Caminha , Johnatas D’Orleans , and Mairon Belchior . 2010 . Collective intelligence in law enforcement – The WikiCrimes system . Information Sciences 180 , 1 ( 2010 ) , 4 – 17 . [ 20 ] Robert C Gardner . 2000 . Correlation , causation , motivation , and second language acquisition . Canadian Psychology / Psychologie Canadienne 41 , 1 ( 2000 ) , 10 . [ 21 ] Charles F Gettys , Carol Manning , Tom Mehle , and Stanley D Fisher . 1980 . Hypoth - esis generation : A fnal report of three years of research . Technical Report . Technical report . University of Oklahoma , Decision Processes Laboratory . Norman , OK . [ 22 ] Philippe J Giabbanelli , Andrew A Tawfk , and Vishrant K Gupta . 2019 . Learning analytics to support teachers’ assessment of problem solving : A novel application for machine learning and graph algorithms . Utilizing learning analytics to support study success ( 2019 ) , 175 – 199 . [ 23 ] Andrew Gordon , Zornitsa Kozareva , and Melissa Roemmele . 2012 . Semeval - 2012 task 7 : Choice of plausible alternatives : An evaluation of commonsense causal reasoning . In * SEM 2012 : The First Joint Conference on Lexical and Computational Semantics – Volume 1 : Proceedings of the main conference and the shared task , and Volume 2 : Proceedings of the Sixth International Workshop on Semantic Evaluation ( SemEval 2012 ) . 394 – 398 . [ 24 ] Nitesh Goyal and Susan R Fussell . 2016 . Efects of sensemaking translucence on distributed collaborative analysis . In Proceedings of the 19th ACM Conference on Computer - Supported Cooperative Work & Social Computing . 288 – 302 . [ 25 ] Nitesh Goyal , Gilly Leshed , Dan Cosley , and Susan R Fussell . 2014 . Efects of implicit sharing in collaborative analysis . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 129 – 138 . [ 26 ] Tera Marie Green , William Ribarsky , and Brian Fisher . 2008 . Visual analytics for complex concepts using a human cognition model . In 2008 IEEE Symposium on Visual Analytics Science and Technology . IEEE , 91 – 98 . [ 27 ] Tera Marie Green , William Ribarsky , and Brian Fisher . 2009 . Building and applying a human cognition model for visual analytics . Information Visualization 8 , 1 ( 2009 ) , 1 – 13 . [ 28 ] Sander Greenland , Judea Pearl , and James M Robins . 1999 . Causal diagrams for epidemiologic research . Epidemiology ( 1999 ) , 37 – 48 . [ 29 ] Thomas Robert Gruber , Adam John Cheyer , Dag Kittlaus , Didier Rene Guzzoni , Christopher Dean Brigham , Richard Donald Giuli , Marcello Bastea - Forte , and Harry Joseph Saddler . 2016 . Intelligent automated assistant . US Patent 9 , 318 , 108 . [ 30 ] Yue Guo , Carsten Binnig , and Tim Kraska . 2017 . What you see is not what you get ! Detecting Simpson’s Paradoxes during Data Exploration . In Proceedings of the 2nd Workshop on Human - In - the - Loop Data Analytics . 1 – 5 . [ 31 ] York Hagmayer and Cilia Witteman . 2017 . Causal knowledge and reasoning in decision making . In Psychology of Learning and Motivation . Vol . 67 . Elsevier , 95 – 134 . [ 32 ] Derek L Hansen , Patrick J Schone , Douglas Corey , Matthew Reid , and Jake Gehring . 2013 . Quality control mechanisms for crowdsourcing : peer review , arbitration , & expertise at familysearch indexing . In Proceedings of the 2013 conference on Computer supported cooperative work . 649 – 660 . [ 33 ] Megan L Head , Luke Holman , Rob Lanfear , Andrew T Kahn , and Michael D Jennions . 2015 . The extent and consequences of p - hacking in science . PLoS Biology 13 , 3 ( 2015 ) , e1002106 . [ 34 ] Jefrey Heer and Ben Shneiderman . 2012 . Interactive dynamics for visual analysis : A taxonomy of tools that support the fuent and fexible use of visualizations . Queue 10 , 2 ( 2012 ) , 30 – 55 . [ 35 ] Richards J Heuer . 1999 . Psychology of intelligence analysis . Center for the Study of Intelligence . [ 36 ] Austin Bradford Hill . 1965 . The environment and disease : association or causa - tion ? [ 37 ] Rick H Hoyle . 1995 . Structural equation modeling : Concepts , issues , and applications . Sage . [ 38 ] Jessica Hullman and Andrew Gelman . 2021 . Designing for interactive exploratory data analysis requires theories of graphical inference . Harvard Data Science Review 3 , 3 ( 2021 ) . [ 39 ] Irving L Janis . 2008 . Groupthink . IEEE Engineering Management Review 36 , 1 ( 2008 ) , 36 . [ 40 ] Ruth Jensen . 2022 . Exploring causal relationships qualitatively : An empirical illus - tration of how causal relationships become visible across episodes and contexts . Journal of Educational Change 23 , 2 ( 2022 ) , 179 – 196 . [ 41 ] Radu Jianu and David Laidlaw . 2012 . An evaluation of how small user interface changes can improve scientists’ analytic strategies . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . ACM , 2953 – 2962 . [ 42 ] Jing Jin . 2006 . Similarity of weighted directed acyclic graphs . ( 2006 ) . [ 43 ] Linda T Kaastra and Brian Fisher . 2014 . Field experiment methodology for pair analytics . In Proceedings of the Fifth Workshop on Beyond Time and Errors : Novel Evaluation Methods for Visualization . 152 – 159 . [ 44 ] Ruogu Kang , Aimee Kane , and Sara Kiesler . 2014 . Teammate inaccuracy blindness : when information sharing tools hinder collaborative analysis . In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing . 797 – 806 . [ 45 ] Daniel Keim , Jörn Kohlhammer , Geofrey Ellis , and Florian Mansmann . 2010 . Mastering the information age : solving problems with visual analytics . ( 2010 ) . [ 46 ] Ha - Kyung Kong , Zhicheng Liu , and Karrie Karahalios . 2018 . Frames and slants in titles of visualizations on controversial topics . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . 1 – 12 . [ 47 ] Daniël Lakens . 2013 . Calculating and reporting efect sizes to facilitate cumulative science : a practical primer for t - tests and ANOVAs . Frontiers in psychology 4 ( 2013 ) , 863 . [ 48 ] Christopher M Layne , Jesse R Steinberg , and Alan M Steinberg . 2014 . Causal reasoning skills training for mental health practitioners : Promoting sound clinical judgment in evidence - based practice . Training and Education in Professional Psychology 8 , 4 ( 2014 ) , 292 . [ 49 ] Félicien Le Louedec and Etienne Chatelut . 2020 . Correlation Between Beva - cizumab Exposure and Survival Does Not Necessarily Imply Causality . Oncologist 25 , 12 ( 2020 ) . [ 50 ] Boyang Li , Stephen Lee - Urban , Darren Scott Appling , and Mark O Riedl . 2012 . Crowdsourcing narrative intelligence . Advances in Cognitive systems 2 , 1 ( 2012 ) . [ 51 ] Tianyi Li , Kurt Luther , and Chris North . 2018 . CrowdIA : Solving Mysteries with Crowdsourced Sensemaking . Proceedings of the ACM on Human - Computer CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Chi - Hsien ( Eric ) Yen , Haocong Cheng , Yilin Xia , and Yun Huang Interaction 2 , CSCW ( 2018 ) , 1 – 29 . [ 52 ] Ching Liu , Juho Kim , and Hao - Chuan Wang . 2018 . ConceptScape : Collaborative concept mapping for video learning . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . 1 – 12 . [ 53 ] Gill Livingston , Andrew Sommerlad , Vasiliki Orgeta , Sergi G Costafreda , Jonathan Huntley , David Ames , Clive Ballard , Sube Banerjee , Alistair Burns , Jiska Cohen - Mansfeld , et al . 2017 . Dementia prevention , intervention , and care . The Lancet 390 , 10113 ( 2017 ) , 2673 – 2734 . [ 54 ] Luciano Lopez and Sylvain Weber . 2017 . Testing for Granger causality in panel data . The Stata Journal 17 , 4 ( 2017 ) , 972 – 984 . [ 55 ] Shufan Ming , Ryan DW Mayfeld , Haocong Cheng , Ke - Rou Wang , and Yun Huang . 2021 . Examining Interactions Between Community Members and University Safety Organizations through Community - Sourced Risk Systems . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW1 ( 2021 ) , 1 – 23 . [ 56 ] Paramita Mirza and Sara Tonelli . 2016 . Catena : Causal and temporal relation extraction from natural language texts . In The 26th international conference on computational linguistics . ACL , 64 – 75 . [ 57 ] Raymond S Nickerson . 1998 . Confrmation bias : A ubiquitous phenomenon in many guises . Review of general psychology 2 , 2 ( 1998 ) , 175 – 220 . [ 58 ] Per Nilsen , DS Hudson , Agneta Kullberg , Toomas Timpka , Robert Ekman , and Kent Lindqvist . 2004 . Making sense of safety . Injury Prevention 10 , 2 ( 2004 ) , 71 – 73 . [ 59 ] Don Norman . 2013 . The design of everyday things : Revised and expanded edition . Basic books . [ 60 ] Brendan Nyhan and Jason Reifer . 2010 . When corrections fail : The persistence of political misperceptions . Political Behavior 32 , 2 ( 2010 ) , 303 – 330 . [ 61 ] Alvitta Ottley , Evan M Peck , Lane T Harrison , Daniel Afergan , Caroline Ziemkiewicz , Holly A Taylor , Paul KJ Han , and Remco Chang . 2016 . Improving Bayesian reasoning : The efects of phrasing , visualization , and spatial ability . IEEE transactions on visualization and computer graphics 22 , 1 ( 2016 ) , 529 – 538 . [ 62 ] Judea Pearl . 1998 . Graphs , causality , and structural equation models . Sociological Methods & Research 27 , 2 ( 1998 ) , 226 – 284 . [ 63 ] Judea Pearl . 2009 . Causality . Cambridge university press . [ 64 ] Judea Pearl and Dana Mackenzie . 2018 . The book of why : the new science of cause and efect . Basic Books . [ 65 ] Mark S Pfaf , Jill L Drury , and Garly L Klein . 2015 . Crowdsourcing mental models using DESIM ( Descriptive to Executable Simulation Modeling ) . In International Conference on Natualistic Decision Making , McLean , VA . [ 66 ] William A Pike , Richard May , Bob Baddeley , Roderick Riensche , Joe Bruce , and Katarina Younkin . 2007 . Scalable visual reasoning : Supporting collaboration through distributed analysis . In 2007 International Symposium on Collaborative Technologies and Systems . IEEE , 24 – 32 . [ 67 ] Peter Pirolli and Stuart Card . 2005 . The sensemaking process and leverage points for analyst technology as identifed through cognitive task analysis . In Proceedings of International Conference on Intelligence Analysis , Vol . 5 . McLean , VA , USA , 2 – 4 . [ 68 ] Ramsey M Raafat , Nick Chater , and Chris Frith . 2009 . Herding in humans . Trends in Cognitive Sciences 13 , 10 ( 2009 ) , 420 – 428 . [ 69 ] Martin Rein and Christopher Winship . 2018 . The dangers of strong causal reasoning : Root causes , social science , and poverty policy . In Experiencing poverty . Routledge , 26 – 54 . [ 70 ] Dominik Sacha , Andreas Stofel , Florian Stofel , Bum Chul Kwon , Geofrey Ellis , and Daniel A Keim . 2014 . Knowledge generation model for visual analytics . IEEE Transactions on Visualization and Computer Graphics 20 , 12 ( 2014 ) , 1604 – 1613 . [ 71 ] Christoph Schulz , Arlind Nocaj , Mennatallah El - Assady , Stefen Frey , Marcel Hlawatsch , Michael Hund , Grzegorz Karch , Rudolf Netzel , Christin Schätzle , Miriam Butt , et al . 2016 . Generative data models for validation and evaluation of visualization techniques . In Proceedings of the Sixth Workshop on Beyond Time and Errors on Novel Evaluation Methods for Visualization . ACM , 112 – 124 . [ 72 ] Skipper Seabold and Josef Perktold . 2010 . Statsmodels : Econometric and statistical modeling with python . In Proceedings of the 9th Python in Science Conference , Vol . 57 . Austin , TX , 10 – 25080 . [ 73 ] Colleen M Seifert , David E Meyer , Natalie Davidson , Andrea L Patalano , and Ilan Yaniv . 1994 . Demystifcation of cognitive insight : Opportunistic assimilation and the prepared - mind hypothesis . In The Nature of Insight . MIT Press , Cambridge , MA . [ 74 ] Hamidreza Seiti , Ahmad Makui , Ashkan Hafezalkotob , Mehran Khalaj , and Ibrahim A Hameed . 2022 . R . Graph : A new risk - based causal reasoning and its application to COVID - 19 risk analysis . Process Safety and Environmental Protection 159 ( 2022 ) , 585 – 604 . [ 75 ] Ben Shneiderman . 2003 . The eyes have it : A task by data type taxonomy for information visualizations . In The craft of information visualization . Elsevier , 364 – 371 . [ 76 ] Steven Sloman . 2005 . Causal models : How people think about the world and its alternatives . Oxford University Press . [ 77 ] Peter Spirtes , Clark N Glymour , Richard Scheines , and David Heckerman . 2000 . Causation , prediction , and search . MIT press . [ 78 ] David R Thomas . 2006 . A general inductive approach for analyzing qualitative evaluation data . American journal of evaluation 27 , 2 ( 2006 ) , 237 – 246 . [ 79 ] Christian Toth , Lars Lorch , Christian Knoll , Andreas Krause , Franz Pernkopf , Robert Peharz , and Julius von Kügelgen . 2022 . Active Bayesian Causal Inference . arXiv preprint arXiv : 2206 . 02063 ( 2022 ) . [ 80 ] John W Tukey et al . 1977 . Exploratory data analysis . Vol . 2 . Reading , Mass . [ 81 ] Frank Van Ham and Adam Perer . 2009 . “Search , show context , expand on demand” : Supporting large graph exploration with degree - of - interest . IEEE Transactions on Visualization and Computer Graphics 15 , 6 ( 2009 ) , 953 – 960 . [ 82 ] Luis A Vasconcelos and Nathan Crilly . 2016 . Inspiration and fxation : Questions , methods , fndings , and challenges . Design Studies 42 ( 2016 ) , 1 – 32 . [ 83 ] Brad Verhulst , Lindon J Eaves , and Peter K Hatemi . 2012 . Correlation not causa - tion : The relationship between personality traits and political ideologies . Ameri - can journal of political science 56 , 1 ( 2012 ) , 34 – 51 . [ 84 ] Michael R Waldmann and York Hagmayer . 2013 . Causal reasoning . ( 2013 ) . [ 85 ] Jun Wang and Klaus Mueller . 2016 . The visual causality analyst : An interactive interface for causal reasoning . IEEE transactions on visualization and computer graphics 22 , 1 ( 2016 ) , 230 – 239 . [ 86 ] Jun Wang and Klaus Mueller . 2017 . Visual causality analysis made practical . In 2017 IEEE Conference on Visual Analytics Science and Technology ( VAST ) . IEEE , 151 – 161 . [ 87 ] Wesley Willett , Jefrey Heer , and Maneesh Agrawala . 2012 . Strategies for crowd - sourcing social data analysis . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 227 – 236 . [ 88 ] Wesley Willett , Jefrey Heer , Joseph Hellerstein , and Maneesh Agrawala . 2011 . CommentSpace : structured support for collaborative visual analysis . In Proceed - ings of the SIGCHI conference on Human Factors in Computing Systems . 3131 – 3140 . [ 89 ] Elizabeth J Wilson and Arch G Woodside . 2001 . Executive and consumer deci - sion processes : increasing useful sensemaking by identifying similarities and departures . Journal of Business & Industrial Marketing ( 2001 ) . [ 90 ] Sewall Wright . 1921 . Correlation and causation . Journal of Agricultural Research 20 , 7 ( 1921 ) , 557 – 585 . [ 91 ] Cindy Xiong , Joel Shapiro , Jessica Hullman , and Steven Franconeri . 2019 . Illusion of Causality in Visualized Data . IEEE Transactions on Visualization and Computer Graphics 26 , 1 ( 2019 ) , 853 – 862 . [ 92 ] Cindy Xiong , Chase Stokes , Yea - Seul Kim , and Steven Franconeri . 2022 . Seeing what you believe or believing what you see ? belief biases correlation estimation . IEEE Transactions on Visualization and Computer Graphics ( 2022 ) . [ 93 ] Koji Yatani . 2016 . Efect sizes and power analysis in hci . In Modern statistical methods for hci . Springer , 87 – 110 . [ 94 ] Chi - Hsien Yen , Haocong Cheng , Grace Yu - Chun Yen , Brian P Bailey , and Yun Huang . 2021 . Narratives + Diagrams : An Integrated Approach for Externalizing and Sharing People’s Causal Beliefs . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW2 ( 2021 ) , 1 – 27 . [ 95 ] Chi - Hsien Yen , Yu - Chun Yen , and Wai - Tat Fu . 2019 . An intelligent assistant for mediation analysis in visual analytics . In Proceedings of the 24th International Conference on Intelligent User Interfaces . 432 – 436 . [ 96 ] Chi - Hsien Eric Yen , Aditya Parameswaran , and Wai - Tat Fu . 2019 . An Exploratory User Study of Visual Causality Analysis . In Computer Graphics Forum , Vol . 38 . Wiley Online Library , 173 – 184 . [ 97 ] Emanuel Zgraggen , Zheguang Zhao , Robert Zeleznik , and Tim Kraska . 2018 . Investigating the efect of the multiple comparisons problem in visual analysis . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . ACM , 479 . [ 98 ] Jian Zhao , Michael Glueck , Petra Isenberg , Fanny Chevalier , and Azam Khan . 2017 . Supporting handof in asynchronous collaborative sensemaking using knowledge - transfer graphs . IEEE Transactions on Visualization and Computer Graphics 24 , 1 ( 2017 ) , 340 – 350 . CrowdIDEA : Blending Crowd Intelligence and Data Analytics to Empower Causal Reasoning CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany SUPPLEMENTARY MATERIALS 1 THE SYNTHETIC DATASET USED IN OUR STUDY Figure S1 illustrates the data - generating model we used in our study . There are nine causal links embedded in our dataset , each represents a realistic causal relationship that could be found in real - world data [ 55 ] . We adopted the common generative data model approach , which has been widely used in evaluating visual ana - lytics and visualization techniques [ 71 ] . Specifcally , the genera - tion process followed the causal links in the causal model , i . e . , a cause variable was generated frst , then the efect variable was generated based on the values of the cause variables and the pre - defned probability distributions . The script we used to generate the dataset , “gen _ safety _ data . py” , and the dataset in CSV format , “gen _ safety _ data _ model1 _ 1 . csv” , can be found along with this sup - Figure S1 : The data - generating model used in our study . plementary material .