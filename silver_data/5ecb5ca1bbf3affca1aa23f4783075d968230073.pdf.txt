Abstract —Correlation clustering problem is a NP hard problem and technologies for the solving of correlation clustering problem can be used to cluster given data set with relation matrix for data in the given data set . In this paper , an approach based on genetic algorithm for correlation clustering problem , named as GeneticCC , is presented . To estimate the performance of a clustering division , data correlation based clustering precision is defined and features of clustering precision are discussed in this paper . Experimental results show that the performance of clustering division for UCI document data set constructed by GeneticCC is better than clustering performance of other clustering divisions constructed by SOM neural network with clustering precision as criterion . I . I NTRODUCTION enerally , clustering analysis [ 1 ] - [ 14 ] can be described as follow . For a given data set V , the purpose of clustering analysis is to divide V into some clustering partitions and data in same partition are similar as possible while data in different partitions are not similar as possible [ 1 ] . A clustering algorithm can divide a given data set into some clustering partitions and those clustering partitions are elements of a clustering division of the given data set generated by the clustering algorithm . For D , a text data set with n document objects as elements , technologies on clustering those documents according to their topics are focused by lots of researchers . And at this case , conditions such as topics of documents in same clustering partition should be similar as possible and topics of documents in different clustering partitions should be not similar as possible should be satisfied . Usually , because there is no prior knowledge about topic of document objects , it is necessary that each document object in D should be represented Manuscript received December 15 , 2007 . This work was supported by National Basic Research Program of China ( 973 Program ) under 2007CB814901 , the Science Research Fund of MOE - Microsoft Key Laboratory of Multimedia Computing and Communication under Grant No . 05071807 , Post - doc’s research fund of Anhui Institute of Architecture & Industry , Nature Science Key Fund of Anhui Education Department under Grant No . KJ2007A110ZC , Anhui Provincial Natural Science Foundation under Grant No . 070414176 . Zhenya Zhang is with the Shool of Electrical and Information Engineering , Anhui Institute of Architecture & Industry ( AIAI ) , Hefei 230022 , China ( phone / fax : + 86 - 0551 - 3521400 ; e - mail : zzychm @ ustc . edu . cn ) . Hongmei Cheng is with the Management Engineering Department of AIAI , Hefei 230022 , China ( e - mail : zyzhang @ aiai . edu . cn ) . Wanli Chen is with the Electrical and Information Engineering School of AIAI , Hefei 230022 , China ( e - mail : cwl @ aiai . edu . cn ) . Shuguang Zhang is with the Department of Statistics & Finance , University of Science and Technology of China ( e - mail : sgzhang @ ustc . edu . cn ) . Qiansheng Fang is with the Electrical and Information Engineering School of AIAI , Hefei 230022 , China ( e - mail : fqs @ aiai . edu . cn ) . appropriately according its’ grammatical structure such as words , sentences and paragraphs . For document clustering analysis , it is feasible and commonly that a document object is represented with a TF or TF - IDF vector [ 16 ] , [ 17 ] . At this case , because each document is represented as a high - dimensional vector , the process of clustering analysis is running slowly which make it difficult to cope with the time requirements of online clustering analysis . If each document objects in data set D is represented suitably , the correlation of any two documents can be calculated with certain measurement such as cosine similarity and the judgment for the correlativity of any two documents can be ascertain . If the correlativity of any two documents in D is ascertain , the correlativity of documents in D can be represented with 0 / 1 n×n matrix M and M is called as the correlation matrix of D . If documents u , v in D are correlativity , M ( u , v ) = 1 . Otherwise M ( u , v ) = 0 . D can be clustered according to M and the problem that clustering D according to M is the problem named as correlation clustering . Formally , correlation clustering problem [ 1 ] - [ 7 ] can be describe as follow . Given a fully - connected graph G with edges labeled “ + ” ( similar ) or “ (cid:2) ” ( different ) , find a partition of the vertices into clusters that agrees as much as possible with the edge labels . In particular , we can look at this in terms of maximizing agreements ( the number of + edges inside clusters plus the number of (cid:2) edges between clusters ) or in terms of minimizing disagreements ( the number of (cid:2) edges inside clusters plus the number of + edges between clusters ) . Problem of correlation clustering is a NP hard problem . MaxCut [ 15 ] and PTAS [ 1 ] are two representing approximate algorithm for solutions of that problem . The time complexity of MaxCut is 1 1 ( ( ) ) ( ) O O ne ε ε and the time complexity of PTAS is 1 ( ) 2 ( ) O O n e ε . In this paper , a genetic algorithm [ 18 ] , [ 19 ] based algorithm named as GeneticCC for correlation clustering is presented . And the clustering performance of clustering division constructed by GeneticCC is high and diversity is the important characteristic of clustering divisions of a data set constructed by GeneticCC . With that characteristic , it is possible that technologies on clustering ensemble [ 8 ] - [ 14 ] can be used to constructed new clustering division for better clustering performance based on some clustering divisions constructed by GeneticCC . This paper is organized as follow . The evaluation criteria for performance of clustering division are discussed at session 2 . Genetic algorithm based correlation clustering ( GeneticCC ) is given at session 3 . Correlation Clustering Based on Genetic Algorithm for Documents Clustering Zhenya Zhang , Hongmei Cheng , Wanli Chen , Shuguang Zhang and Qiansheng Fang G 3193 978 - 1 - 4244 - 1823 - 7 / 08 / $ 25 . 00 c (cid:2) 2008 IEEE Authorized licensed use limited to : National Chung Cheng University . Downloaded on October 21 , 2009 at 02 : 52 from IEEE Xplore . Restrictions apply . Experimental results and analysis are given at session 4 . In session 5 , conclusion about GeneticCC and more future work on correlation clustering are shown . II . P ERFORMANCE E VALUATION OF C LUSTERING D IVISION B ASED ON C ORRELATIVITY Clustering analysis is an unsupervised learning process . Usually , there is no label information for data in the data set which should be clustered and it is impossible to use clustering precision as the evaluation criteria for the clustering performance of clustering division . To evaluate the performance of clustering division and evaluate the performance of a clustering algorithm via statistical features , data set with packet labeled information can be clustered with clustering algorithm . On the one hand , because there are packets labeled information in data of the data set , the correlation among data is certain . On the other hand , a clustering division for a data determines a kind of correlation among data in the data set ( If u and v are in same clustering partition , they are correlative otherwise they are not correlative . ) . The clustering precision of a clustering division can be calculated by comparing the correlation determined by packet labeled information with correlation determined by the clustering division . As to correlation clustering , because the correlativity of data is ascertain , it is suitable for measuring clustering performance of a clustering division or a clustering algorithm with clustering precision as criteria . Definition1 : Let V be a data set and n V = . If F is a clustering division of V , F can be represented with F M , which is a 0 / 1 n n × matrix and for each , u v V ∈ , 1 , or u , v in same clustering partition of F ( , ) 0 , otherwise F u v M u v = = (cid:2)(cid:3)(cid:4) . F M is called as the representing matrix of F . It is obvious that F M is a kind of formula description of clustering division F and F M is a symmetric matrix . Because main diagonal elements of F M are 1 and F M is a symmetric matrix , there are ( 1 ) 2 n n − independent assertion about data’s mutuality for clustering . Because Clustering analysis is an unsupervised learning process , it is impossible to judge whether those independent assertions are valid in a clustering division . On the other hand , Because the correlativity of ant two datum in V is ascertain for correlation clustering and those correlativity can be represented with a n n × correlation matrix V M ( if the i th element and j th element of V is correlativity , ( , ) 1 V M i j = . Otherwise ( , ) 0 V M i j = ) , it is possible to judge whether those independent ascertains are valid in a clustering division based correlation on clustering . Because of ( , ) V M i j can not equal ( , ) V M j i and ( , ) V M i i can not valued with 1 , to sum the number of valid ascertains in F M of clustering division F , it is necessary to compare whether elements in each location in F M and V M are same . Definition2 : Let V M be the correlation matrix of data set V , n V = and F is a clustering division for V constructed by a clustering algorithm . F M is the representing matrix of F . For , u v V ∀ ∈ , if ( , ) ( , ) F V M u v M u v = , the assertion for the correlativity between u and v in F is valid , otherwise the assertion is invalid . Let k be the number of valid assertion in F . 2 k r n = is the clustering precision of F . Proposition1 : Let V M be the correlation matrix of data set V , n V = , F is a clustering division for V constructed by a clustering algorithm , r is the clustering precision of F . 2 ( ) 1 V F abs M M r n − = − , here abs ( ) is function for the absolute value of each element in a matrix and • is the operator for the sum of value of each element in a matrix . Proof : Let k be the number of valid independent assertion of F . According to definition1 , for , u v V ∀ ∈ , 1 , ( , ) 0 , ( , ) 1 ( , ) ( , ) 0 , ( , ) ( , ) 1 , ( , ) 1 , ( , ) 0 V F V F V F V F M u v M u v M u v M u v M u v M u v M u v M u v − = = − = = = = (cid:2)(cid:5)(cid:3)(cid:5)(cid:4) . Th at is to say 1 1 1 0 , ( , ) ( , ) ( , ) ( , ) 1 , ( , ) ( , ) F F F F F F M u v M u v M u v M u v M u v M u v = − = ≠ (cid:2)(cid:5)(cid:3)(cid:5)(cid:4) . Thus 1 2 ( ) F F k n abs M M = − − . According to definition2 , 2 ( ) 1 V F abs M M r n − = − . # Definition3 : Let F be a clustering division of data set V , n V = , 2 m n = and k be the number of valid assertion of F . If / 2 k m > , F is a valid clustering division of V . Furthermore , for ( 0 , 1 ] η∈ , if 0 . 5 k m η < < , F is a η - weak clustering division of V . Proposition2 : Let F be a clustering division of data set V , n V = , 2 m n = . For , u v V ∀ ∈ , the right probability of the assertion of F about ( u , v ) is p and error probability is q = 1 - p . If F is a valid clustering division of V and F r is the clustering 3194 2008 IEEE Congress on Evolutionary Computation ( CEC 2008 ) Authorized licensed use limited to : National Chung Cheng University . Downloaded on October 21 , 2009 at 02 : 52 from IEEE Xplore . Restrictions apply . precision of F , (cid:6) (cid:7) / 2 m k k m k F m k m r C p q − = > (cid:8) . Proof : Let k be the number of valid assertion of F . According to definition3 , if / 2 k m > , F is a valid clustering division of V . That is to say if / 2 k m ≥ (cid:6) (cid:7) (cid:9) (cid:10) , F is a valid clustering division of V . because for , u v V ∀ ∈ , the right probability of the assertion of F about ( u , v ) is p and error probability is q = 1 - p , the probability of k valid assertion in F is k k m k m C p q − . Because F r is the clustering precision of F , (cid:6) (cid:7) / 2 m k k m k F m k m r C p q − = > (cid:8) if F is valid . # Usually , if F is a random clustering division of data set V , for , u v V ∀ ∈ , the probability that the assertion of F about u , v is right is 0 . 5 . That is to say p = 0 . 5 and q = 0 . 5 in proposition 2 . At this case , (cid:6) (cid:7) (cid:6) (cid:7) / 2 / 2 ( ) / 2 m m k k m k k m F m m k m k m r C p q C − = = > = (cid:8) (cid:8) . Thus if 0 . 5 F r > , F is valid . III . G ENETIC A LGORITHM B ASED C ORRELATION C LUSTERING Usually , according to definition1 , F M , the representing matrix of clustering division F of data set V prescribes the correlativity of any two elements in V . Because the correlation of data in data set V is prior in correlation clustering problem , clustering precision can be calculated in correlation clustering problem and correlation clustering can be described as an optimization problem based on clustering precision according to definition2 . Let V M be the correlation matrix of data set V , n V = , F be a clustering division of V and F M be the representing matrix of F . If k is the number of valid assertion in F M and r is the clustering precision of F , it is obvious that 2 k r n = and [ 0 , 1 ] r ∈ . Because clustering performance of F is better if r is greater , correlation clustering problem can be described as an optimization problem defined at equation 2 where the definition of fitness function is given at equation 1 . 2 ( ) 1 1 k fitness F r n = − = − ( 1 ) min ( ) . . is a clustering division of data set V fitness F s t F (cid:2)(cid:3)(cid:4) ( 2 ) The optimization problem in equation 2 is a NP hard problem and that problem can be solved quickly with genetic algorithm . The fitness of each individual in genetic algorithm can be calculated according to equation 1 where the performance of a individual is better if its fitness value is smaller . When genetic algorithm is used to solve problem defined as equation 2 , a clustering division of data set V is an individual and it is necessary to code each individual suitably . Suppose there are at least n ( n > 1 ) clustering partitions in any clustering division of data set V . If those n possible clustering partitions are identified with an integer in { 0…n - 1 } as id , each id can be represented with a 0 / 1 string and the length of the bit string is 2 log n (cid:6) (cid:7) (cid:9) (cid:10) . Because each data in a clustering division is in and only in a clustering partition , each data can be coded with the bit string for the clustering partition which it is in . The conjunction of code of each data in V with some order is the code of a clustering division . The description of GeneticCC ( generic algorithm based correlation clustering ) is give at algorithm 1 ~ 3 . The method for the constructing of representing matrix for a clustering division is shown in algorithm 1 . The calculating method for fitness of individual F is given by algorithm 2 and the framework for GeneticCC is shown in algorithm 3 . Algorithm1 : M F ( representing Matrix of Clustering Division F ) constructing Input : clustering division F ; n ( the number of data in data set V ) Output : M F ( representing Matrix of F ) 1 ) Generate M which is a n n × 0 / 1 matrix . 2 ) value each element in M with 1 . 3 ) for i = 1 to n 4 ) for j = i + 1 to n 5 ) if F ( i ) < > F ( j ) 6 ) begin 7 ) M ( i , j ) = 0 8 ) M ( j , i ) = 0 9 ) End 10 ) M F = M Algorithm2 : calculating the fitness of each individual Input : P , the code of clustering division F of data set V ; n , the number of data in V ; M V , the correlation matrix of V . Output : The fitness of individual P 1 ) Construct the representing matrix M F of F according P ; 2 ) ( ) V F k abs M M = − 3 ) 2 k fitness n = Algorithm3 : GeneticCC : Generic Algorithm Based Correlation Clustering Input : data set V ; M V , the correlation matrix of V Output : F , the clustering division of V 1 ) Initial the population size and generate each individual randomly 2 ) Calculating the fitness of each individual 3 ) If termination conditions are satisfied , go to 5 ) 4 ) Generate new individuals for the next generation , go to 2 ) 2008 IEEE Congress on Evolutionary Computation ( CEC 2008 ) 3195 Authorized licensed use limited to : National Chung Cheng University . Downloaded on October 21 , 2009 at 02 : 52 from IEEE Xplore . Restrictions apply . 5 ) Construct clustering division F of data set V according to the individual with best fitness . IV . E XPERIMENTAL R ESULTS In our experiments , clustering performances of GeneticCC and SOM neural network based clustering algorithm are tested with document data set download from http : / / kdd . ics . uci . edu . Documents in the data set are divided into 20 catalogs and there are 1000 articles in each catalog . Articles in same catalog are downloaded from same newsgroup in Internet . In our experiments , each article is represented with a TF vector . Words in the dictionary for the TF vector constructing are words in sample article which are normal sampling with 1 % as sampling rate . Each word is stemmed before it is added into the dictionary and noise words are omitted . Because there are 2446 words in that dictionary , each article is represented with a vector in 2446 dimensional real space and those vectors are normalized with 1 as modular length . Let M V , be the correlation matrix of the document data set used in GeneticCC . M V ( u , v ) = 1 if document u and v are in same news group otherwise M V ( u , v ) = 0 . Fig1 . clustering precision of SOM network ( 4 catalog ) Fig2 . clustering precision ofGeneticCC ( 4 catalog ) Fig3 . clustering precision of SOM network ( 8 catalog ) Fig4 . clustering precision ofGeneticCC ( 8catalog ) In our experiments , clustering performance of clustering division constructed by SOM and GeneticCC are tested firstly . Four / eight news groups with 50 articles in each news group are selected randomly . 100 clustering division are constructed for those 200 / 400 articles by SOM and GeneticCC separately and clustering precisions of those clustering divisions are calculated . In our experiment , the number of individual in GeneticCC is 120 and 30 ( 5 6 × in a plane ) competitive neurons in the completive layer of SOM . The histogram for the distribution of those clustering precision are given at fig1 ~ fig4 . The distribution of clustering precision for those 100 clustering division of 200 articles in 4 news groups constructed by SOM is shown at fig1 . In fig1 , the minimum value of clustering precision is 0 . 7822 meanwhile the maximum value of clustering precision is 0 . 8100 . The mean value clustering precision is 0 . 7957 meanwhile the median value of clustering precision is 0 . 7953 with 0 . 0052 as standard deviation . The distribution of clustering precision for those 100 clustering division of 200 articles in 4 news groups constructed by GeneticCC is shown at fig2 . In fig2 , the minimum value of clustering precision is 0 . 8438 meanwhile the maximum value of clustering precision is 1 . The mean value clustering precision is 0 . 9621 meanwhile the median value of clustering precision is 0 . 9762 with 0 . 0379 as standard deviation . The distribution of 3196 2008 IEEE Congress on Evolutionary Computation ( CEC 2008 ) Authorized licensed use limited to : National Chung Cheng University . Downloaded on October 21 , 2009 at 02 : 52 from IEEE Xplore . Restrictions apply . clustering precision for those 100 clustering division of 400 articles in 8 news groups constructed by SOM is shown at fig3 . In fig3 , the minimum value of clustering precision is 0 . 8887 meanwhile the maximum value of clustering precision is 0 . 9182 . The mean value clustering precision is 0 . 9056 meanwhile the median value of clustering precision is 0 . 9059 with 0 . 0062 as standard deviation . The distribution of clustering precision for those 100 clustering division of 400 articles in 8 news groups constructed by GeneticCC is shown at fig4 . In fig4 , the minimum value of clustering precision is 0 . 8791 meanwhile the maximum value of clustering precision is 0 . 9765 . The mean value clustering precision is 0 . 9175 meanwhile the median value of clustering precision is 0 . 9191 with 0 . 0190 as standard deviation . Because the standard deviation of clustering precision of clustering divisions generated by GeneticCC is larger than the standard deviation of clustering precision of clustering divisions generated by SOM , it is obvious that clustering performance of clustering divisions constructed by GeneticCC is more fluctuant than clustering performance of clustering divisions constructed by SOM . But in fact , because the standard deviation of clustering precision of clustering divisions generated by Genetic is very small , the clustering performance of clustering divisions constructed by GeneticCC is much better than clustering performance of clustering divisions constructed by SOM according to other evidences such as minimum value , maximum value , mean vale and median value of clustering precision in fig1 ~ fig4 . Time consumed by clustering division constructing by GeneticCC is tested in our experiments for the clustering performance of GeneticCC . In that experiment , four / eight news groups are selected randomly . The number of articles in each news group is varied from 5 to 50 with 5 as step . 20 clustering divisions are constructed by GeneticCC at each step and the mean time for a clustering division constructing by Genetic is calculated . Curves of time consumed by GeneticCC for clustering division constructing are shown at fig5 and fig6 . In fig5 and fig6 , the x axis is for the number of articles clustered and y axis is for mean time consumed by GeneticCC for a clustering division constructing . The time unit in fig5 and fig6 is second Fig5 . Time consumed by GeneticCC ( 4 catalog ) Fig6 . Time consumed by GeneticCC ( 8 catalog ) To evaluate the time complexity of GeneticCC , let’s suppose that there are n articles to be clustered by GeneticCC and in the process of clustering division constructing by GeneticCC , the number of individuals is k and the number of iterativeness is T when the process is converged . It is obvious that each individual can be coded with a bit string with 2 log n n (cid:6) (cid:7) (cid:9) (cid:10) as it’s’ length . Because it isnecessary to calculate the fitness of each individual with its’ representing matrix of in each iterative process and the time complexity of the representing matrix constructing for an individual is 2 2 ( ) ( log ) O n O n n + , the time complexity for the fitness calculating in one in one iterativeness for all individuals is 2 2 ( ) ( log ) O kn O kn n + . Because the length of the bit string for the code of each individual is 2 log n n (cid:6) (cid:7) (cid:9) (cid:10) , the time complexity of genetic operation such as selection , crossover and mutation in genetic algorithm are 2 ( log ) O kn n . Thus the time complexity of one iterativeness is 2 ( ) O kn + 2 ( log ) O kn n + 2 3 ( log ) O kn n × = 2 ( ) O kn + 2 ( log ) O kn n . Because the process for clustering division constructing by GeneticCC will be converged with T iterativeness , the time complexity of GeneticCC is not greater than 2 2 ( ) ( ) O kTn O n = which is matched in the main with experiment results shown in fig5 and fig6 . In fig6 , it is seemed that the times consumed by GeneticCC are decreased remarkably and the reason for the phenomena is that T is smaller with those articles than others . To test clustering performance of GeneticCC further , all articles in twenty news groups with 100 articles in each news group are clustered by GeneticCC and the evolvement of best fitness and mean fitness of individual during the converging process of GeneticCC are given in fig7 . In fig7 , x axis is for the number of generation in genetic algorithm and y axis is for fitness value . It is obvious that the best fitness and mean fitness values of individual are decreased step by step and the clustering process is converged when genetic operations are 2008 IEEE Congress on Evolutionary Computation ( CEC 2008 ) 3197 Authorized licensed use limited to : National Chung Cheng University . Downloaded on October 21 , 2009 at 02 : 52 from IEEE Xplore . Restrictions apply . iterative with 214 times . The clustering precision of clustering division constructed by GeneticCC is 0 . 9077 . Fig7 . Correlation Clustering based on GeneticCC ( 20 catalog ) All programs in our experiments for this paper is running under Matlab 2007a and the implementation of SOM neural network and genetic algorithm are based on function provided by neural network toolbox and genetic and direct search toolbox in Matlab 2007a . V . C ONCLUSION AND F UTURE W ORK Correlation clustering problem is a NP - hard optimum problem and can be solved quickly with genetic algorithm . The key of GeneticCC is the coding of a clustering division where a clustering division is a chromosome in genetic algorithm . Good scheme for the coding of a clustering division can make the implementation of operations such as selection , crossover and mutation in genetic algorithm easily and effectively . In this paper , to verify our idea on GeneticCC quickly and expediently under our finished work on genetic algorithm , a clustering division is coded as a bit string . Integer coding for individual and the implementation of operations such as selection , crossover and mutation based on integer coding are our further research . It is an unavoidable question that the convergence time of genetic algorithm in GeneticCC is more . GeneticCC is only applied to the offline processing for large - scale data set such as automatic cataloging and the application on online process for large - scale data set is beyond its ability . Fast solutions of correlation clustering for online processing with large - scale data set are our future focuses . In particular , the design and implementation of fast fitness function are key issues about that work . One of our ongoing researches is chance discovery . Because the snapshot of each participant for the chance discovery process can be different , the construction of public snapshot of participants for the process of chance discovery can be modeled as a clustering aggregation problem . Because clustering aggregation problem can be solved with technologies for correlation clustering problem , the application of GeneticCC on the public snapshot for the process of chance discovery will be our future work on correlation clustering problem too . R EFERENCES [ 1 ] BansalN . , Blum A . , Chawla S . , Correlation clustering , Machine Learn , 2004 , Vol56 , pp89 - 113 [ 2 ] Kanani Pallika , McCallum Andrew , Resource - bounded information gathering for correlation clustering , Proceedings of 20th Annual Conference on Learning Theory , San Diego , CA , Jun 13 - 15 , 2007 , pp625 - 627 [ 3 ] Demaine E . D . , Emanuel D . , Fiat A , Immorlica N , Correlation clustering in general weighted graphs , Theoretical Computer Science , Vol . 361 ( 2 - 3 ) , 2006 . 9 , pp172 - 187 [ 4 ] Emanuel D . , Fiat A . , Correlation clustering—Minimizing disagreements on arbitrary weighted graphs , Proceedings ofESA2003 , pp208 - 220 [ 5 ] Christodoulakis , George A . , Common volatility and correlation clustering in asset returns , European Journal of Operational research , 2007 . 11 , Vol . 182 ( 3 ) , pp1263 - 1284 [ 6 ] Ioannis Giotis , Venkatesan Guruswami , Correlation clustering with a fixed number of clusters , Proceedings of the Seventeenth Annual ACM - SIAM Symposium on Discrete Algorithms , 2006 , pp1167 - 1176 [ 7 ] Demaine E . D . , Immorlica N . , Correlation clustering with partial information , Proceedings of APPROX , 2003 , pp1 - 13 . [ 8 ] Ailon N . , Charikar M . , Newman A . , Aggregating inconsistent information : Ranking and clustering , Proceedings of the ACM Symposium on Theory ofComputing 2005 , pp684 - 693 . [ 9 ] Andritsos P . , Tsaparas P . , Milter R . J . , Sevcik K . C . , Limbo : Scalable clustering of categorical data , Proceedings of the International Conference on Extending Database Technology 2004 , pp123 - 146 . [ 10 ] Boulis C . , Ostendorf M . , Combining multiple clustering systems , Proceedings of the European Conference on Principles and Practice of Knowledge Discovery in Databases 2004 , pp63 - 74 . [ 11 ] Charikar M . , Guruswami V . , Wirth A . , Clustering with qualitative information , Proceedings of the IEEE Symposium on Foundations of Computer Science 2003 , pp524 - 533 . [ 12 ] TandWei , Zhou Zhihua , Bagging - Based SelectiveClustererEnsemble , Journal of Software , 2005 , Vol . 16 ( 4 ) , pp496 - 502 [ 13 ] Yang Y , Kamel M . S . , An aggregated clustering approach using multi - ant colonies algorithms , Pattern Recognition , 2006 , vol39 , pp1278 - 1289 [ 14 ] Topchy A . P . , Law M . H . C . , Jain A . K . , Analysis of consensus partition in cluster ensemble , Proceedings of Fourth IEEE International Conference on Data Mining , 2004 . 11 , pp225 - 232 [ 15 ] W . Fernandez dela Vega , MAX - CUT has a randomizedapproximation scheme in dense graphs , Random Structures and Algorithms , 1996 , Vol8 ( 3 ) , pp187 – 198 . [ 16 ] Ponte Jay M . , Croft W . Bruce , Language modeling approach to information retrieval , SIGIR Forum ( ACM Special Interest Group on Information Retrieval ) , Melbourne , Vic . , Aust , 1998 , pp275 - 281 [ 17 ] Tata Sandeep , Patel , Jignesh M . , Estimating the selectivity of tf - idf based cosine similarity predicates , SIGMOD Record , 2007 , v36 ( 2 ) , pp7 - 12 [ 18 ] Guoliang Chen , Xufa Wang , Zhenquan Zhuang , Dongsheng Wang , Genetic Algorithm and its Application , Beijing : Posts & Telecom Press , 1996 , pp3 - 10 [ 19 ] Rowe Jonathan E . , Genetic algorithm theory , Proceedings of Genetic and Evolutionary Computation Conference , 2007 , pp3585 - 3608 3198 2008 IEEE Congress on Evolutionary Computation ( CEC 2008 ) Authorized licensed use limited to : National Chung Cheng University . Downloaded on October 21 , 2009 at 02 : 52 from IEEE Xplore . Restrictions apply .