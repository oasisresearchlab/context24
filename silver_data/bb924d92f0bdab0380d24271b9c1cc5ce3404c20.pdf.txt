Linear Coupling : An Ultimate Uniﬁcation of Gradient and Mirror Descent ∗ Zeyuan Allen - Zhu 1 and Lorenzo Orecchia 2 1 Institute for Advanced Study , Princeton , USA zeyuan @ csail . mit . edu 2 Boston University , USA orecchia @ bu . edu Abstract First - order methods play a central role in large - scale machine learning . Even though many variations exist , each suited to a particular problem , almost all such methods fundamentally rely on two types of algorithmic steps : gradient descent , which yields primal progress , and mirror descent , which yields dual progress . We observe that the performances of gradient and mirror descent are complementary , so that faster algorithms can be designed by linearly coupling the two . We show how to reconstruct Nes - terov’s accelerated gradient methods using linear coupling , which gives a cleaner interpretation than Nesterov’s original proofs . We also discuss the power of linear coupling by extending it to many other settings that Nesterov’s methods cannot apply to . 1998 ACM Subject Classiﬁcation G . 1 . 6 Optimization , F . 2 Analysis of Algorithms and Problem Complexity Keywords and phrases linear coupling , gradient descent , mirror descent , acceleration Digital Object Identiﬁer 10 . 4230 / LIPIcs . ITCS . 2017 . 3 1 Introduction The study of fast iterative methods for approximately solving convex problems is a central research focus in Machine Learning , Combinatorial Optimizations and many other areas of Computer Science and Mathematics . For large - scale programs , ﬁrst - order iterative methods are usually the methods of choice due to their cheap and often highly parallelizable iterations . First - order methods access the target optimization problem min x ∈ Q f ( x ) in a black - box fashion : the algorithm queries a point y ∈ Q at every iteration and receives the pair (cid:0) f ( y ) , ∇ f ( y ) (cid:1) . 1 The complexity of a ﬁrst - order method is usually measured in the number of queries necessary to produce an additive ε - approximate minimizer . First - order methods have recently experienced a renaissance in the design of fast algorithms for fundamental computer science problems , varying from discrete ones such as maximum ﬂow problems [ 20 ] , to continuous ones such as empirical risk minimization [ 39 ] . Despite the myriad of applications , ﬁrst - order methods with provable convergence guaran - tees can be mostly classiﬁed as instantiations of two fundamental algorithmic ideas : gradient ∗ The authors would like to thank Silvio Micali for listening to our work and suggesting the name “linear coupling” . The full version of this paper can be found on arXiv https : / / arxiv . org / abs / 1407 . 1537 . 1 Here , variable x is constrained to lie in a convex set Q ⊆ R n , which is known as the constraint set of the problem . © Zeyuan Allen - Zhu and Lorenzo Orecchia ; licensed under Creative Commons License CC - BY 8th Innovations in Theoretical Computer Science Conference ( ITCS 2017 ) . Editor : Christos H . Papadimitrou ; Article No . 3 ; pp . 3 : 1 – 3 : 22 Leibniz International Proceedings in Informatics Schloss Dagstuhl – Leibniz - Zentrum für Informatik , Dagstuhl Publishing , Germany 3 : 2 Linear Coupling : An Ultimate Uniﬁcation of Gradient and Mirror Descent descent and the mirror descent . 2 We argue that gradient descent takes a fundamentally primal approach , while mirror descent follows a complementary dual approach . In our main result , we show how these two approaches blend in a natural manner to yield a new and simple accelerated gradient method for smooth convex optimization problems , as well as lead to other applications where the classical accelerated gradient methods do not apply . 1 . 1 Understanding First - Order Methods : Gradient Descent and Mirror Descent We now provide high - level descriptions of gradient and mirror descent . While this material is classical , our intuitive presentation of these ideas forms the basis for our main result in the subsequent sections . For a more detailed survey , we recommend the textbooks [ 9 , 26 ] . Consider for simplicity the unconstrained minimization ( i . e . Q = R n ) , but , as we will see in Section 2 , the same intuition and a similar analysis extend to the constrained or even the proximal case . We use generic norms k · k and their duals k · k ∗ . At a ﬁrst reading , they can be both replaced with the Euclidean norm k · k 2 . 1 . 1 . 1 Primal Approach : Gradient Descent A natural approach to iterative optimization is to decrease the objective function as much as possible at every iteration . To formalize the eﬀectiveness of this idea , one usually introduces a smoothness assumption on the objective f ( x ) . Speciﬁcally , recall that an L - smooth function f satisﬁes k∇ f ( x ) − ∇ f ( y ) k ∗ ≤ L k x − y k for every x , y . Such a smoothness condition yields a global quadratic upper bound on the function around a query point x : ∀ y , f ( y ) ≤ f ( x ) + h∇ f ( x ) , y − x i + L 2 k y − x k 2 . ( 1 . 1 ) Gradient - descent algorithms exploit this bound by taking a step that maximizes the guaran - teed objective decrease ( i . e . , the primal progress ) f ( x k ) − f ( x k + 1 ) at every iteration k . More precisely , x k + 1 ← arg min y n L 2 k y − x k k 2 + h∇ f ( x ) , y − x k i o . Notice that here k · k is a generic norm . When this is the Euclidean ‘ 2 - norm , the step takes the familiar additive form x k + 1 = x k − 1 L ∇ f ( x k ) . However , in other cases , e . g . , for the non - Euclidean ‘ 1 or ‘ ∞ norms , the update step will not follow the direction of the gradient ∇ f ( x k ) ( see for instance [ 18 , 27 ] ) . Under the smoothness assumption above , the magnitude of this primal progress is at least f ( x k ) − f ( x k + 1 ) ≥ 1 2 L k∇ f ( x k ) k 2 ∗ . ( 1 . 2 ) In general , this quantity will be larger when the gradient ∇ f ( x k ) has large norm . Classical convergence analysis of gradient descent usually combines ( 1 . 2 ) with a basic convexity argument to relate f ( x k ) − f ( x ∗ ) and k∇ f ( x k ) k ∗ : that is , f ( x k ) − f ( x ∗ ) ≤ k∇ f ( x k ) k ∗ k x k − x ∗ k . 2 We emphasize here that these two terms are sometimes used ambiguosly in the literature ; in this paper , we attempt to stick as close as possible to the conventions of the optimization community and in particular in the textbooks [ 9 , 26 ] with one exception : we extend the deﬁnition of gradient descent to non - Euclidean norms in a natural way , following [ 18 ] . Z . Allen - Zhu and L . Orecchia 3 : 3 For L - smooth objectives , the ﬁnal bound shows that gradient descent converges in O (cid:0) Lε (cid:1) iterations [ 26 ] . The limitation of gradient descent is that it does not make any attempt to construct a good lower bound to the optimum value f ( x ∗ ) . It essentially ignores the dual problem . In the next subsection , we review mirror descent , a method that focuses completely on the dual side . 1 . 1 . 2 Dual Approach : Mirror Descent Mirror - descent methods ( see for instance [ 9 , 12 , 24 , 28 , 44 ] ) tackle the dual problem by constructing lower bounds to the optimum . Recall that each queried gradient ∇ f ( x ) can be viewed as a hyperplane lower bounding the objective f : that is , f ( u ) ≥ f ( x ) + h∇ f ( x ) , u − x i for all u . Mirror - descent methods attempt to carefully construct a convex combination of these hyperplanes in order to yield even a stronger lower bound . Formally , suppose one has queried points x 0 , . . . , x k − 1 , then we form a linear combination of the k hyperplanes and obtain 3 ∀ u , f ( u ) ≥ 1 k k − 1 X t = 0 f ( x t ) + 1 k k − 1 X t = 0 h∇ f ( x t ) , u − x t i . ( 1 . 3 ) On the upper bound side , we consider a simple choice x = 1 k P k − 1 t = 0 x t , i . e . , the mean of the queried points . By straightforward convexity argument , we have f ( x ) ≤ 1 k P k − 1 t = 0 f ( x t ) . As a result , the distance between f ( x ) and f ( u ) for any arbitrary u can be upper bounded using ( 1 . 3 ) : ∀ u , f ( x ) − f ( u ) ≤ 1 k k − 1 X t = 0 h∇ f ( x t ) , x t − u i def = R k ( u ) . ( 1 . 4 ) Borrowing terminology from online learning , the right hand side R k ( u ) is known as the regret of the sequence ( x t ) k − 1 t = 0 with respect to point u . Now , consider a regularized version e R k ( u ) of the regret e R k ( u ) def = 1 k · (cid:16) − w ( u ) α + k − 1 X t = 0 h∇ f ( x t ) , x t − u i (cid:17) , where α > 0 is a trade - oﬀ parameter and w ( · ) is some regularizer that is usually strongly convex . Then , mirror - descent methods choose the next iterate x k by minimizing the maximum regularized regret at the next iteration : that is , choose x k ← arg max u e R k ( u ) . This update rule can be shown to successfully drive max u e R k ( u ) down as k increases , and thus the right hand side of ( 1 . 4 ) decreases as k increases . This can be made into a rigorous analysis and show that mirror descent converges in T = O ( ρ 2 / ε 2 ) iterations . Here , ρ 2 is the average value of k∇ f ( x k ) k 2 ∗ across the iterations . To sum up , the smaller the queried gradients are ( i . e . the smaller k∇ f ( x k ) k ∗ is ) , the tighter the lower bound ( 1 . 3 ) becomes , and therefore the fewer iterations are needed for mirror descent to converge . ( Note that the above mirror - descent analysis can also be used to derive the 1 / ε convergence rate on smooth objectives similar to that in gradient descent [ 11 ] ; since this adaption is not needed in our paper , we omit the details . ) 3 For simplicity , we choose uniform weights here . For the purpose of proving convergence results , the weights of individual hyperplanes are typically uniform or only dependent on k . ITCS 2017 3 : 4 Linear Coupling : An Ultimate Uniﬁcation of Gradient and Mirror Descent Remarks . Mirror descent admits several diﬀerent algorithmic implementations , such as Nemirovski’s mirror descent [ 24 ] and Nesterov’s dual averaging [ 28 ] . 4 Results based on one implementation can usually be transformed into another with some eﬀorts . In this paper , we adopt Nemirovski’s mirror descent as our choice of mirror descent , see Section 2 . 2 . One may occasionally ﬁnd analyses that do not immediately fall into the above two categories . To name a few , solely using mirror descent and dual lower bounds , one can also obtain a convergence rate 1 / ε for smooth objectives similar to that in gradient descent [ 11 ] . Conversely , one can deduce the mirror - descent guarantee by applying gradient descent on a dual objective ( see Appendix A . 3 ) . Shamir and Zhang [ 40 ] obtained an algorithm that converges slightly slower than mirror descent , but has an error guarantee on the last iterate , rather than the average history . 1 . 2 Our Conceptual Question Following this high level description of gradient and mirror descent , it is useful to pause and observe the complementary nature of the two procedures . Gradient descent relies on primal progress , uses local steps and makes faster progress when the norms of the queried gradients k∇ f ( x k ) k are large . In contrast , mirror descent works by ensuring dual progress , uses global steps and converges faster when the norms of the queried gradients k∇ f ( x k ) k are small . This interpretation immediately leads to the question that inspires our work : Can Gradient Descent and Mirror Descent be combined to obtain faster ﬁrst - order methods ? In this paper , we initiate the formal study of this key conceptual question , and propose a linear coupling framework . To properly discuss our framework , we choose to mostly focus in the context of convex smooth minimization , and show how to reconstruct Nesterov’s accelerated gradient methods using linear coupling . We also discuss the power of our framework by extending it to many other settings beyond Nesterov’s original scope . 1 . 3 Accelerated Gradient Method Via Linear Coupling In the seminal work [ 25 , 26 ] , Nesterov designed an accelerated gradient method for L - smooth functions with respect to ‘ 2 norms , and it performs quadratically faster than gradient descent – requiring Ω ( L / ε ) 0 . 5 rather than Ω ( L / ε ) iterations . This is asymptotically tight [ 26 ] . Later in 2005 , Nesterov generalizes his method to allow non - Euclidean norms in the deﬁnition of smoothness [ 27 ] . All these versions of methods are referred to as accelerated gradient methods , or sometimes as Nesterov’s accelerated methods . Although accelerated gradient methods have been widely applied ( to mention a few , see [ 38 , 39 ] for regularized optimizations , [ 19 , 30 ] for composite optimization , [ 29 ] for cubic regularization , [ 31 ] for universal method , and [ 20 ] for an application on maxﬂow ) , they are often regarded as “analytical tricks” [ 17 ] because their convergence analyses are somewhat complicated and lack of intuitions . In this paper , we provide a simple , alternative , but complete version of the accelerated gradient method . Here , by “complete” we mean our method works for any norm , and for both 4 Other update rules can be viewed as specializations or generalizations of the mentioned implementations . For instance , the follow - the - regularized - leader ( FTRL ) step is a generalization of Nesterov’s dual averaging step where the regularizers are can be adaptively selected ( see [ 23 ] ) . Z . Allen - Zhu and L . Orecchia 3 : 5 the constrained and unconstrained case . 5 Our key observation is to construct two sequences of updates : one sequence of gradient - descent updates and one sequence of mirror - descent updates . Thought Experiment . Consider f ( x ) that is unconstrained and L - smooth . For sake of demonstrating the idea , suppose k∇ f ( x ) k 2 , the norm of the observed gradient , is either always ≥ K , or always ≤ K , where the cut - oﬀ value K is determined later . Under such “wishful assumption” , we propose the following algorithm : if k∇ f ( x ) k 2 is always ≥ K , we perform T gradient - descent steps ; otherwise we perform T mirror - descent steps . To analyze such an algorithm , suppose without loss of generality we start with some point x 0 whose objective distance f ( x 0 ) − f ( x ∗ ) is at most 2 ε , and we want to ﬁnd some x so that f ( x ) − f ( x ∗ ) ≤ ε . 6 If T gradient - descent steps are performed , the objective decreases by at least k∇ f ( · ) k 22 2 L ≥ K 2 2 L per step according to ( 1 . 2 ) , and we only need T ≥ Ω ( εLK 2 ) steps to achieve an ε accuracy . If T mirror - descent steps are performed , we need T ≥ Ω ( K 2 ε 2 ) steps according to the mirror - descent convergence . In sum , we need T ≥ Ω (cid:0) max (cid:8) εLK 2 , K 2 ε 2 (cid:9)(cid:1) steps to converge to an ε - minimizer . Setting K to be the “magic number” to balance the two terms , we only need T ≥ Ω (cid:0) Lε (cid:1) 1 / 2 iterations as desired . Towards an Actual Proof . To turn our thought experiment into an actual proof , we face the following obstacles . Although gradient - descent steps always decrease the objective , mirror - descent steps may sometimes increase the objective , cancelling the eﬀect of the gradient descent . On the other hand , the mirror - descent steps are only useful when a large number of iterations are performed in a row ; if any gradient - descent step stands in the middle , the convergence is destroyed . For this reason , it is natural to design an algorithm that , in every single iteration k , performs both a gradient and a mirror descent step , and somehow ensure that the two steps are coupled together . However , the following additional diﬃculty arises : if from some starting point x k , the gradient - descent step instructs us to go to y k , while the mirror - descent step instructs us to go to z k , then how do we continue ? Do we look at the gradient at ∇ f ( y k ) or ∇ f ( z k ) in the next iteration ? This problem is implicitly solved by Nesterov using the following simple idea 7 : in the k - th iteration , we choose a linear combination x k + 1 ← τz k + ( 1 − τ ) y k , and use this same gradient ∇ f ( x k + 1 ) to continue the gradient and mirror steps of the next iteration . Whenever τ is carefully chosen ( just like the “magic number” K ) , the two descent sequences provide a coupled bound on the error guarantee , and we recover the same convergence as [ 27 ] . Roadmap . We review the key lemmas of gradient and mirror descent in Section 2 . We propose a simple method with ﬁxed step length to recover Nesterov’s accelerated methods for the unconstrained case in Section 3 , and generalize it to the full - setting in Section 4 . We 5 Some authors have regarded the result in [ 26 ] as “momentum analysis” [ 32 , 41 ] or “ball method” [ 10 ] . These analyses only apply to Euclidean spaces . We point out the importance of allowing non - Euclidean norms in Appendix A . 1 . In addition , our proof in this paper extends naturally to the proximal version of ﬁrst - order methods , but for simplicity , we choose to include only the constrained version . 6 For all ﬁrst - order methods , the heaviest computation always happens in this 2 ε to ε process . 7 We wish to point out that Nesterov has phrased his method diﬀerently from ours , and little is known on why this linear combination is needed from his proof , except for being used as an algebraic trick to cancel speciﬁc terms . ITCS 2017 3 : 6 Linear Coupling : An Ultimate Uniﬁcation of Gradient and Mirror Descent discuss several important applications of linear coupling that Nesterov’s original methods do not solve in Section 5 . 2 Key Lemmas of Gradient and Mirror Descent 2 . 1 Review of Gradient Descent Consider a function f ( x ) that is convex and diﬀerentiable on a closed convex set Q ⊆ R n , and assume that f is L - smooth with respect to k · k , that is , for every x , y ∈ Q , it satisﬁes k∇ f ( x ) − ∇ f ( y ) k ∗ ≤ L k x − y k . Here , k · k ∗ is the dual norm of k · k . 8 (cid:73) Deﬁnition 2 . 1 . For any x ∈ Q , the gradient ( descent ) step ( with step length 1 L ) is e x = Grad ( x ) def = arg min y ∈ Q n L 2 k y − x k 2 + h∇ f ( x ) , y − x i o and we let Prog ( x ) def = − min y ∈ Q (cid:8) L 2 k y − x k 2 + h∇ f ( x ) , y − x i (cid:9) ≥ 0 . In particular , when k·k = k·k 2 is the ‘ 2 - norm and Q = R n is unconstrained , the gradient step can be simpliﬁed as Grad ( x ) = x − 1 L ∇ f ( x ) . Or , slightly more generally , when k · k = k · k 2 is the ‘ 2 - norm but Q may be constrained , we have Grad ( x ) = x − 1 L g Q ( x ) where g Q ( x ) is the gradient mapping of f at x ( see Chapter 2 . 2 . 3 of [ 26 ] ) . The classical theory on smooth convex programming gives rise to the following lower bound on the amount of objective decrease ( proved in Appendix B for completeness ) : f ( Grad ( x ) ) ≤ f ( x ) − Prog ( x ) ( 2 . 1 ) or in the special case when Q = R n f ( Grad ( x ) ) ≤ f ( x ) − 1 2 L k∇ f ( x ) k 2 ∗ . From the above descent guarantee , one can deduce the convergence rate of gradient descent . For instance , if k · k = k · k 2 is the Euclidean norm , and if gradient step x k + 1 = Grad ( x k ) is applied T times , we obtain the following convergence guarantee ( see [ 26 ] ) f ( x T ) − f ( x ∗ ) ≤ O (cid:16) L k x 0 − x ∗ k 22 T (cid:17) or equivalently T ≥ Ω (cid:16) L k x 0 − x ∗ k 22 ε (cid:17) ⇒ f ( x T ) − f ( x ∗ ) ≤ ε . Here , x ∗ is any minimizer of f ( x ) . If k·k is a general norm , but Q = R n is unconstrained , the above convergent rate becomes f ( x T ) − f ( x ∗ ) ≤ O (cid:0) LR 2 T (cid:1) , where R = max x : f ( x ) ≤ f ( x 0 ) k x − x ∗ k . We provide the proof of this later case in Appendix B because it is less known and we cannot ﬁnd it in the optimization literature . Note that , we are unaware of any universal convergence proof for both the general norm and the unconstrained case . As we shall see later in Section 4 , this convergence rate can be improved by accelerated gradient methods , even for the general norm k·k and the constrained case . 2 . 2 Review of Mirror Descent Consider some function f ( x ) that is convex on a closed convex set Q ⊆ R n , and assume that f is ρ - Lipschitz continuous with respect to norm k · k , that is , for every x , y ∈ Q , it satisﬁes | f ( x ) − f ( y ) | ≤ ρ k x − y k . This is equivalent to saying that f admits a subgradient 8 k ξ k ∗ def = max { h ξ , x i : k x k ≤ 1 } . For instance , ‘ p norm is dual to ‘ q norm if 1 p + 1 q = 1 . Z . Allen - Zhu and L . Orecchia 3 : 7 ∂f ( x ) at every point x ∈ Q , and satisﬁes k ∂f ( x ) k ∗ ≤ ρ . ( Recall that ∂f ( x ) = ∇ f ( x ) if f is diﬀerentiable . ) Mirror descent requires one to choose a regularizer ( also referred to as a distance generating function ) : (cid:73) Deﬁnition 2 . 2 . We say that w : Q → R is a distance generating function ( DGF ) , if w is 1 - strongly convex with respect to k · k , or in symbols , ∀ x ∈ Q \ ∂Q , ∀ y ∈ Q : w ( y ) ≥ w ( x ) + h∇ w ( x ) , y − x i + 12 k x − y k 2 . Accordingly , the Bregman divergence is given as V x ( y ) def = w ( y ) − h∇ w ( x ) , y − x i − w ( x ) ∀ x ∈ Q \ ∂Q , ∀ y ∈ Q . The property of DGF ensures that V x ( x ) = 0 and V x ( y ) ≥ 12 k x − y k 2 ≥ 0 . Common examples of DGFs include ( i ) w ( y ) = 12 k y k 22 , which is strongly convex with respect to the ‘ 2 - norm over every Q , and the corresponding V x ( y ) = 12 k x − y k 22 , and ( ii ) the entropy function w ( y ) = P i y i log y i , which is strongly convex with respect to the ‘ 1 - norm over any Q ⊆ ∆ def = { x ≥ 0 : 1 T x = 1 } . and the corresponding V x ( y ) = P i y i log ( y i / x i ) ≥ 12 k x − y k 21 . (cid:73) Deﬁnition 2 . 3 . The mirror ( descent ) step with step length α can be described as e x = Mirr x ( α · ∂f ( x ) ) where Mirr x ( ξ ) def = arg min y ∈ Q (cid:8) V x ( y ) + h ξ , y − x i (cid:9) . Mirror descent’s core lemma is the following inequality ( proved in Appendix B for complete - ness ) : If x k + 1 = Mirr x k (cid:0) α · ∂f ( x k ) (cid:1) , then ∀ u ∈ Q , α ( f ( x k ) − f ( u ) ) ≤ α h ∂f ( x k ) , x k − u i ≤ α 2 2 k ∂f ( x k ) k 2 ∗ + V x k ( u ) − V x k + 1 ( u ) . ( 2 . 2 ) The term h ∂f ( x k ) , x k − u i features prominently in online optimization , and is known as the regret at iteration k with respect to u ( see Appendix A . 2 for the folklore relationship between mirror descent and regret minimization ) . It is not hard to see that , telescoping ( 2 . 2 ) for k = 0 , . . . , T − 1 , setting x def = 1 T P T − 1 k = 0 x k to be the average of the x k ’s , and choosing u = x ∗ , we have αT ( f ( x ) − f ( x ∗ ) ) ≤ T − 1 X k = 0 α h ∂f ( x k ) , x k − x ∗ i ≤ α 2 2 T − 1 X k = 0 k ∂f ( x k ) k 2 ∗ + V x 0 ( x ∗ ) − V x T ( x ∗ ) . ( 2 . 3 ) Finally , letting Θ be any upper bound on V x 0 ( x ∗ ) ( recall Θ = 12 k x 0 − x ∗ k 22 when k · k is the Euclidean norm ) , and α = √ 2Θ ρ ·√ T be the step length , inequality ( 2 . 2 ) can be re - written as f ( x ) − f ( x ∗ ) ≤ √ 2Θ · ρ √ T or equivalently T ≥ 2Θ · ρ 2 ε 2 ⇒ f ( x ) − f ( x ∗ ) ≤ ε . ( 2 . 4 ) Remark . While their analyses share some similarities , mirror and gradient steps are often very diﬀerent . For example , if the optimization problem is over the simplex with ‘ 1 norm , then gradient step gives x 0 ← arg min y { 12 k y − x k 21 + α h∇ f ( x ) , y − x i } , while the mirror step with entropy regularizer gives x 0 ← arg min y { P i y i log ( y i / x i ) + α h∇ f ( x ) , y − x i } . We point out in Appendix A . 1 that non - Euclidean norms are very important for certain applications . In the special case of w ( x ) = 12 k x k 22 and k · k is ‘ 2 - norm , gradient and mirror steps are indistinguishable from each other . However , as we have discussed earlier , these two update rules are often equipped with very diﬀerent convergence analyses , even if they ‘look the same’ . ITCS 2017 3 : 8 Linear Coupling : An Ultimate Uniﬁcation of Gradient and Mirror Descent 3 Warm - Up Method with Fixed Step Length Consider the same setting as Section 2 . 1 : that is , f ( x ) is convex and diﬀerentiable on its domain Q , and is L - smooth with respect to some norm k · k . ( Note that f ( x ) may not have a good Lipschitz continuity parameter ρ , but we do not need such a property . ) In this section , we focus on the unconstrained case Q = R n , and combine gradient and mirror descent to produce a very simple accelerated method . We explain this method ﬁrst because it avoids the mysterious choices of step lengths as in the full setting , and carries our conceptual message in a very clean way . Design an algorithm that , in every step k , performs both a gradient and a mirror step , and ensures that the two steps are linearly coupled . More speciﬁcally , starting from x 0 = y 0 = z 0 , in each iteration k = 0 , 1 , . . . , T − 1 , we ﬁrst deﬁne x k + 1 ← τz k + ( 1 − τ ) y k and then perform a gradient step y k + 1 ← Grad ( x k + 1 ) , and perform a mirror step z k + 1 ← Mirr z k (cid:0) α ∇ f ( x k + 1 ) (cid:1) . 9 Above , α is the ( ﬁxed ) step length of the mirror step , while τ is the parameter controlling the coupling rate . The choices of α and τ will become clear at the end of this section , but from a high level , α will be determined from the mirror - descent analysis , similar to that in ( 2 . 3 ) , and τ will be determined as the best parameter to balance the gradient and mirror steps , similar to the “magic number” K in our thought experiment discussed in Section 1 . 3 . Classical gradient - descent and mirror - descent analyses immediately imply the following : (cid:73) Lemma 3 . 1 . For every u ∈ Q = R n , α h∇ f ( x k + 1 ) , z k − u i (cid:172) ≤ α 2 2 k∇ f ( x k + 1 ) k 2 ∗ + V z k ( u ) − V z k + 1 ( u ) (cid:173) ≤ α 2 L (cid:0) f ( x k + 1 ) − f ( y k + 1 ) (cid:1) + V z k ( u ) − V z k + 1 ( u ) . ( 3 . 1 ) Proof . To deduce (cid:172) , we note that our mirror step z k + 1 = Mirr z k ( α ∇ f ( x k + 1 ) ) is essentially identical to that of x k + 1 = Mirr x k ( α ∇ f ( x k ) ) in ( 2 . 2 ) , with only changes of variable names . Therefore , inequality (cid:172) is a simple copy - and - paste from ( 2 . 2 ) after changing the variable names ( see the proof of ( 2 . 2 ) for details ) . The second inequality (cid:173) is from the gradient step guarantee f ( x k + 1 ) − f ( y k + 1 ) ≥ 12 L k∇ f ( x k + 1 ) k 2 ∗ in ( 2 . 1 ) . (cid:74) One can immediately see from Lemma 3 . 1 that , although the mirror step introduces an error α 2 2 k∇ f ( x k + 1 ) k 2 ∗ , this error is proportional to the amount of the gradient - step progress f ( x k + 1 ) − f ( y k + 1 ) . This captures the observation we stated in the introduction : if k∇ f ( x k + 1 ) k ∗ is large , we can make a large gradient step , or if k∇ f ( x k + 1 ) k ∗ is small , the mirror step suﬀers from a small loss . If we choose τ = 1 or equivalently x k + 1 = z k , the left hand side of inequality ( 3 . 1 ) becomes h∇ f ( x k + 1 ) , x k + 1 − u i , the regret at iteration x k + 1 . In such a case we wish to telescope it for all iterations k in the spirit of mirror descent ( see ( 2 . 3 ) ) . However , we face the problem that the terms f ( x k + 1 ) − f ( y k + 1 ) do not telescope . 10 On the other hand , if we choose τ = 0 or 9 Here , the mirror step Mirr is deﬁned by specifying any DGF w ( · ) that is 1 - strongly convex over Q . 10 In other words , although a gradient step may decrease the objective from f ( x k + 1 ) to f ( y k + 1 ) , it may also get the objective increased from f ( y k ) to f ( x k + 1 ) . Z . Allen - Zhu and L . Orecchia 3 : 9 equivalently x k + 1 = y k , then the terms f ( x k + 1 ) − f ( y k + 1 ) = f ( y k ) − f ( y k + 1 ) telescope , but the left hand side of ( 3 . 1 ) is no longer the regret . 11 To overcome this issue , we use linear coupling . We compute and upper bound the diﬀerence between the left hand side of ( 3 . 1 ) and the actual “regret” : α h∇ f ( x k + 1 ) , x k + 1 − u i − α h∇ f ( x k + 1 ) , z k − u i = α h∇ f ( x k + 1 ) , x k + 1 − z k i = ( 1 − τ ) α τ h∇ f ( x k + 1 ) , y k − x k + 1 i ≤ ( 1 − τ ) α τ ( f ( y k ) − f ( x k + 1 ) ) . ( 3 . 2 ) Above , we used the fact that τ ( x k + 1 − z k ) = ( 1 − τ ) ( y k − x k + 1 ) , as well as the convexity of f ( · ) . It is now immediate that by choosing 1 − ττ = αL and combining ( 3 . 1 ) and ( 3 . 2 ) , we have (cid:73) Lemma 3 . 2 ( Coupling ) . Letting τ ∈ ( 0 , 1 ) satisfy that 1 − ττ = αL , we have that ∀ u ∈ Q = R n , α h∇ f ( x k + 1 ) , x k + 1 − u i ≤ α 2 L (cid:0) f ( y k ) − f ( y k + 1 ) (cid:1) + (cid:0) V z k ( u ) − V z k + 1 ( u ) (cid:1) . It is clear from the above proof that τ is introduced to precisely balance the objective decrease f ( x k + 1 ) − f ( y k + 1 ) , and the ( possible ) objective increase f ( y k ) − f ( x k + 1 ) . This is similar to the “magic number” K discussed in the introduction . Finally Convergence Rate . We telescope inequality Lemma 3 . 2 for k = 0 , 1 , . . . , T − 1 . Setting x def = 1 T P T − 1 k = 0 x k and u = x ∗ , we have αT ( f ( x ) − f ( x ∗ ) ) ≤ T − 1 X k = 0 α h ∂f ( x k ) , x k − x ∗ i ≤ α 2 L (cid:0) f ( y 0 ) − f ( y T ) (cid:1) + V x 0 ( x ∗ ) − V x T ( x ∗ ) . ( 3 . 3 ) Suppose our initial point is of error at most d , that is f ( y 0 ) − f ( x ∗ ) ≤ d , and suppose V x 0 ( x ∗ ) ≤ Θ , then ( 3 . 3 ) gives f ( x ) − f ( x ∗ ) ≤ 1 T (cid:0) αLd + Θ / α (cid:1) . Choosing α = p Θ / Ld to be the value that balances the above two terms , 12 we obtain that f ( x ) − f ( x ∗ ) ≤ 2 √ L Θ d T . In other words , in T = 4 p L Θ / d steps , we can obtain some x satisfying f ( x ) − f ( x ∗ ) ≤ d / 2 , halving the distance to the optimum . If we restart this entire procedure a few number of times , halving the distance for every run , then we obtain an ε - approximate solution in T = O (cid:0)p L Θ / ε + p L Θ / 2 ε + p L Θ / 4 ε + · · · (cid:1) = O (cid:0)p L Θ / ε (cid:1) iterations , matching the same running time of Nesterov’s accelerated methods [ 25 , 26 , 27 ] . It is important to note here that α = p Θ / Ld increases as time goes ( i . e . , as d goes down ) , and therefore τ = 1 αL + 1 decreases as time goes . This lesson instructs us that gradient steps should be given more weights than mirror steps , when it is closer to the optimum . 13 11 Indeed , our “thought experiment” in the introduction is conducted as if we both had x k + 1 = z k and x k + 1 = y k , and therefore we could arrive at the upcoming ( 3 . 3 ) directly . 12 This is essentially the same way to choose α in mirror descent , see ( 2 . 3 ) . 13 One may ﬁnd this counter - intuitive because when it is closer to the optimum , the observed gradients will become smaller , and therefore mirror steps should perform well due to our conceptual message in the introduction . This understanding is incorrect for two reasons . First , when it is closer to the optimum , the threshold between “large” and “small” gradients also become smaller , so one cannot rely only on mirror steps . Second , when it is closer to the optimum , mirror steps are more ‘unstable’ and may increase the objective more ( in comparison to the current distance to the optimum ) , and thus should be given less weight . ITCS 2017 3 : 10 Linear Coupling : An Ultimate Uniﬁcation of Gradient and Mirror Descent Algorithm 1 AGM ( f , w , x 0 , T ) Input : f a diﬀerentiable and convex function on Q that is L - smooth with respect to k · k ; w the DGF function that is 1 - strongly convex with respect to the same k · k over Q ; x 0 some initial point ; and T the number of iterations . Output : y T such that f ( y T ) − f ( x ∗ ) ≤ 4Θ L T 2 . 1 : V x ( y ) def = w ( y ) − h∇ w ( x ) , y − x i − w ( x ) . 2 : y 0 ← x 0 , z 0 ← x 0 . 3 : for k ← 0 to T − 1 do 4 : α k + 1 ← k + 2 2 L , and τ k ← 1 α k + 1 L = 2 k + 2 . 5 : x k + 1 ← τ k z k + ( 1 − τ k ) y k . 6 : y k + 1 ← Grad ( x k + 1 ) (cid:5) = arg min y ∈ Q n L 2 k y − x k + 1 k 2 + h∇ f ( x k + 1 ) , y − x k + 1 i o 7 : z k + 1 ← Mirr z k (cid:0) α k + 1 ∇ f ( x k + 1 ) (cid:1) (cid:5) = arg min z ∈ Q (cid:8) V z k ( z ) + h α k + 1 ∇ f ( x k + 1 ) , z − z k i (cid:9) 8 : end for 9 : return y T . Conclusion . Equipped with the basic knowledge of gradient descent and mirror descent , the above proof is quite straightforward and gives intuition on how the two “magic numbers” α and τ are selected . However , this simple algorithm has several caveats . First , the value α depends on the knowledge of Θ ; second , a good initial distance bound d has to be speciﬁed ; and third , the algorithm has to be restarted . In the next section , we let α and τ change gradually across iterations . This overcomes the mentioned caveats , and also extends the above analysis to allow Q to be constrained . 4 Final Method with Variable Step Lengths In this section , we recover the main result of [ 27 ] in the constrained case , that is (cid:73) Theorem 4 . 1 . If f ( x ) is L - smooth w . r . t . k · k on Q , and w ( x ) is 1 - strongly convex w . r . t . k · k on Q , then AGM outputs y T satisfying f (cid:0) y T (cid:1) − f ( x ∗ ) ≤ 4Θ L / T 2 , where Θ is any upper bound on V x 0 ( x ∗ ) . We remark here that it is very important to allow the norm k · k to be general , rather than focusing on the ‘ 2 - norm as in [ 26 ] . See our discussion in Appendix A . 1 . Our algorithm AGM ( see Algorithm 1 ) starts from x 0 = y 0 = z 0 . In each step k = 0 , 1 , . . . , T − 1 , it computes x k + 1 ← τ k z k + ( 1 − τ k ) y k and then performs gradient step y k + 1 ← Grad ( x k + 1 ) , and performs mirror step z k + 1 ← Mirr z k (cid:0) α k + 1 ∇ f ( x k + 1 ) (cid:1) . Here , α k + 1 is the step length of mirror descent and will be chosen at the end of this section . The value τ k is 1 α k + 1 L which is slightly diﬀerent from 1 αL + 1 used in the warm - up case . ( This is necessary to capture the constrained case . ) Our choice of α k + 1 will ensure that τ k ∈ ( 0 , 1 ] for each k . Convergence Analysis . We state the analogue of Lemma 3 . 1 whose proof is in Appendix C : (cid:73) Lemma 4 . 2 . If τ k = 1 α k + 1 L , then it satisﬁes that for every u ∈ Q , α k + 1 h∇ f ( x k + 1 ) , z k − u i ≤ α 2 k + 1 L Prog ( x k + 1 ) + V z k ( u ) − V z k + 1 ( u ) ≤ α 2 k + 1 L (cid:0) f ( x k + 1 ) − f ( y k + 1 ) (cid:1) + V z k ( u ) − V z k + 1 ( u ) . Z . Allen - Zhu and L . Orecchia 3 : 11 We state the analogue of Lemma 3 . 2 , whose proof is slightly diﬀerent and in Appendix C : (cid:73) Lemma 4 . 3 ( Coupling ) . For any u ∈ Q , (cid:0) α 2 k + 1 L (cid:1) f ( y k + 1 ) − (cid:0) α 2 k + 1 L − α k + 1 (cid:1) f ( y k ) + (cid:0) V z k + 1 ( u ) − V z k ( u ) (cid:1) ≤ α k + 1 f ( u ) . We are now ready to prove Theorem 4 . 1 : Proof of Theorem 4 . 1 . In order to telescope Lemma 4 . 3 , we only need to set the sequence of α k so that α 2 k L ≈ α 2 k + 1 L − α k + 1 as well as τ k = 1 / α k + 1 L ∈ ( 0 , 1 ] . In our AGM , we let α k = k + 1 2 L so that α 2 k L = α 2 k + 1 L − α k + 1 + 14 L . Summing up Lemma 4 . 3 for k = 0 , 1 , . . . , T − 1 , we obtain α 2 T Lf ( y T ) + T − 1 X k = 1 1 4 Lf ( y k ) + (cid:0) V z T ( u ) − V z 0 ( u ) (cid:1) ≤ T X k = 1 α k f ( u ) . By choosing u = x ∗ , we notice that P Tk = 1 α k = T ( T + 3 ) 4 L , f ( y k ) ≥ f ( x ∗ ) , V z T ( u ) ≥ 0 and V z 0 ( x ∗ ) ≤ Θ . Therefore , we obtain ( T + 1 ) 2 4 L 2 Lf ( y T ) ≤ (cid:16) T ( T + 3 ) 4 L − T − 1 4 L (cid:17) f ( x ∗ ) + Θ , which after simpliﬁcation implies f ( y T ) ≤ f ( x ∗ ) + 4Θ L ( T + 1 ) 2 . (cid:74) Let us make three remarks . AGM is slightly diﬀerent from [ 27 ] : ( 1 ) we use Nemirovski’s mirror steps instead of dual averaging steps , ( 2 ) we allow arbitrary starting points x 0 , and ( 3 ) we use τ k = 2 k + 2 rather than τ k = 2 k + 3 . AGM is very diﬀerent from the perhaps better - known version [ 26 ] , which is known by some authors as the “momentum method” [ 32 , 41 ] . Momentum methods do not apply to non - Euclidean settings . In Appendix D , we also recover the strong convexity version of accelerated gradient methods [ 26 ] , and thus linear coupling provides a complete proof of all existing accelerated gradient methods . 5 Beyond Accelerated Gradient Methods Providing an intuitive , yet complete interpretation of accelerated gradient methods is an open question in Optimization [ 17 ] . Our result in this paper is one important step towards this general goal . Linear coupling not only gives a reinterpretation of Nesterov’s accelerated methods , more importantly , it provides a framework for designing ﬁrst - order methods in a bigger agenda . Since the original version of this paper appeared online , our linear - coupling framework has led to breakthroughs for several problems in computer science . In all such problems , the original Nesterov’s accelerated methods do not apply . We illustrate a few examples in this line of research , in order to demonstrate the power and generality of linear coupling . Recall the key lemmas of gradient and mirror descent in linear coupling ( see ( 3 . 1 ) ) : gradient descent : f ( x k + 1 ) − f ( y k + 1 ) ≥ 1 2 L k∇ f ( x k + 1 ) k 2 ∗ ( 5 . 1 ) mirror descent : α h∇ f ( x k + 1 ) , z k − u i ≤ α 2 2 k∇ f ( x k + 1 ) k 2 ∗ + V z k ( u ) − V z k + 1 ( u ) ( 5 . 2 ) ITCS 2017 3 : 12 Linear Coupling : An Ultimate Uniﬁcation of Gradient and Mirror Descent Extension 1 : Strengthening ( 5 . 2 ) and ( 5 . 1 ) . If f satisﬁes good properties other than smoothness , one can also develop objective decrease lemma to replace ( 5 . 1 ) . In addition , if necessary , a non - strongly convex regularizer can be used in mirror descent to replace ( 5 . 2 ) . In either or both such cases , linear coupling can still be used to combine the two methods and obtain faster running times ; in contrast , Nesterov’s original accelerated methods do not apply . For example , recent breakthroughs on positive linear programming ( positive LP ) are all based on the above extension of linear coupling [ 3 , 4 , 5 , 22 , 42 , 43 ] . For such LPs , the corresponding objective f is intrinsically non - smooth . Some authors including Nesterov himself have applied simple smoothing to turn f into a smooth variant f 0 , and then minimized f 0 [ 27 ] ; however , even if Nesterov’s accelerated methods are used to minimize f 0 , the resulting running time scales with the problem’s width , a parameter that can be exponential in input size . 14 In contrast , if linear coupling is used , one can show that f ( x k + 1 ) − f ( y k + 1 ) is lower bounded by a constant times P j max { | ∇ j f ( x k + 1 ) | , 1 } 2 for the original objective f ( see [ 4 ] ) . This is a weaker version of ( 5 . 1 ) . However , after linear coupling , it leads to a faster algorithm than naively applying Nesterov’s accelerated methods on f 0 in all parameter regimes . Extension 2 : Three - Point Coupling . One may naturally consider linearly coupling for more than two vectors . While this is provably unnecessary for minimizing a smooth objective in the full - gradient setting ( because accelerated gradient methods are already optimal ) , it can be very helpful in the stochastic - gradient setting . More speciﬁcally , it was a known obstacle in Nesterov’s accelerated methods ( including our AGM ) that if the full gradient ∇ f ( x k + 1 ) is replaced with a random estimator e ∇ whose expectation E [ e ∇ ] = ∇ f ( x k + 1 ) , then acceleration disappears in the worst case . Using linear coupling , we can ﬁx this issue by providing the ﬁrst direct accelerated stochastic gradient method . In [ 1 ] , the author replaced the coupling step x k + 1 ← τz k + ( 1 − τ ) y k with x k + 1 ← τ 1 z k + τ 2 e x + ( 1 − τ 2 − τ 1 ) y k , where e x is a snapshot point whose full gradient is computed exactly but very infrequently . Such a “three - point” linear coupling provides an accelerated running time because one can combine ( 5 . 1 ) , ( 5 . 2 ) , together with a so - called variance - reduction inequality [ 16 ] all three at once . Extension 3 : Optimal Sampling Probability . Nesterov’s accelerated methods generalize to coordinate - descent settings , that is , to minimize f that is L i - smooth for each coordinate i . The best known coordinate - descent method [ 21 ] samples each coordinate i with probability proportional to L i , and is based on a randomized version of Nesterov’s original analysis . Using linear coupling , the authors of [ 6 ] discovered that one should select i with probability proportional to √ L i for an even faster running time . To illustrate the reasoning behind this , let us revisit ( 5 . 1 ) and ( 5 . 2 ) . In the coordinate - descent setting , if we abbreviate x k + 1 with x , the right hand side of ( 5 . 1 ) simply becomes 12 L i ( ∇ i f ( x ) ) 2 if coordinate i is selected . As for ( 5 . 2 ) , to ensure its left hand side stays the same in expectation , one should replace ∇ f ( x ) with 1 p i ∇ i f ( x ) , where p i is the probability to select i . As a result , the ﬁrst term on the right hand side of ( 5 . 2 ) becomes α 2 2 p 2 i ( ∇ i f ( x ) ) 2 . By comparing these two new terms 12 L i ( ∇ i f ( x ) ) 2 and α 2 2 p 2 i ( ∇ i f ( x ) ) 2 , we immediately notice 14 We recommend interested readers to ﬁnd detailed discussions in [ 4 ] regarding the importance of designing width - independent solvers for positive LP . As an illustrative example , in the problem of maximum matching ( which can be written as positive LP ) , the width of the problem is the number of edges in the graph . Z . Allen - Zhu and L . Orecchia 3 : 13 that p i had better be proportional to √ L i in order for the two terms to cancel . This simple idea , fully motivated from linear coupling , leads to the fastest accelerated coordinate - descent method [ 6 ] . Extension 4 : Supporting Non - Convexity . Consider objectives f that are not even convex but still smooth . For instance , neural network training objectives fall into this class if smoothed activation functions are used . In such a case , both ( 5 . 1 ) and ( 5 . 2 ) remain true . However , when coupling the two steps , we cannot claim h∇ f ( x k + 1 ) , x k + 1 − u i ≥ f ( x k + 1 ) − f ( u ) because there is no convexity . In [ 2 ] , the authors discovered that one can use the quadratic lower bound h∇ f ( x k + 1 ) , x k + 1 − u i ≥ f ( x k + 1 ) − f ( u ) − L 2 k x k + 1 − u k 2 to replace convexity arguments , and still perform a weaker version of linear coupling . This leads to a stochastic algorithm that converges to approximate saddle - points , 15 outperforming both gradient descent and stochastic gradient descent , the only two known ﬁrst - order methods with provably convergence guarantees . Acknowledgements . We thank Jon Kelner and Yin Tat Lee for helpful conversations , and Aaditya Ramdas for pointing out a typo in the previous version of this paper . This material is based upon work partly supported by the National Science Foundation under Grant CCF - 1319460 and by a Simons Graduate Student Award under grant no . 284059 . References 1 Zeyuan Allen - Zhu . Katyusha : Accelerated Variance Reduction for Faster SGD . ArXiv e - prints , abs / 1603 . 05953 , March 2016 . 2 Zeyuan Allen - Zhu and Elad Hazan . Variance Reduction for Faster Non - Convex Optimiza - tion . In ICML , 2016 . 3 Zeyuan Allen - Zhu , Yin Tat Lee , and Lorenzo Orecchia . Using optimization to obtain a width - independent , parallel , simpler , and faster positive SDP solver . In SODA , 2016 . 4 Zeyuan Allen - Zhu and Lorenzo Orecchia . Nearly - Linear Time Positive LP Solver with Faster Convergence Rate . In STOC , 2015 . 5 Zeyuan Allen - Zhu and Lorenzo Orecchia . Using optimization to break the epsilon barrier : A faster and simpler width - independent algorithm for solving positive linear programs in parallel . In SODA , 2015 . 6 Zeyuan Allen - Zhu , Peter Richtárik , Zheng Qu , and Yang Yuan . Even faster accelerated coordinate descent using non - uniform sampling . In ICML , 2016 . 7 Sanjeev Arora , Elad Hazan , and Satyen Kale . Fast Algorithms for Approximate Semidef - inite Programming using the Multiplicative Weights Update Method . In FOCS , pages 339 – 348 . IEEE , 2005 . doi : 10 . 1109 / SFCS . 2005 . 35 . 8 Sanjeev Arora , Elad Hazan , and Satyen Kale . The Multiplicative Weights Update Method : a Meta - Algorithm and Applications . Theory of Computing , 8 : 121 – 164 , 2012 . doi : 10 . 4086 / toc . 2012 . v008a006 . 9 Aharon Ben - Tal and Arkadi Nemirovski . Lectures on Modern Convex Optimization . Society for Industrial and Applied Mathematics , January 2013 . doi : 10 . 1137 / 1 . 9780898718829 . 10 Sébastien Bubeck , Yin Tat Lee , and Mohit Singh . A geometric alternative to Nesterov’s accelerated gradient descent . ArXiv e - prints , abs / 1506 . 08187 , June 2015 . arXiv : abs / 1506 . 08187 . 15 Recall that in general non - convex ﬁrst - order optimization one can only hope for converging to saddle - points . ITCS 2017 3 : 14 Linear Coupling : An Ultimate Uniﬁcation of Gradient and Mirror Descent 11 Ofer Dekel , Ran Gilad - Bachrach , Ohad Shamir , and Lin Xiao . Optimal distributed online prediction using mini - batches . The Journal of Machine Learning Research , 13 ( 1 ) : 165 – 202 , 2012 . arXiv : 1012 . 1367 . 12 John Duchi , Shai Shalev - Shwartz , Yoram Singer , and Ambuj Tewari . Composite Objective Mirror Descent . In COLT , 2010 . 13 Olivier Fercoq and Peter Richtárik . Accelerated , parallel , and proximal coordinate descent . SIAM Journal on Optimization , 25 ( 4 ) : 1997 – 2023 , 2015 . First appeared on ArXiv 1312 . 5799 in 2013 . 14 Yoav Freund and Robert E Schapire . A desicion - theoretic generalization of on - line learning and an application to boosting . In Computational learning theory , pages 23 – 37 . Springer , 1995 . 15 Elad Hazan , Amit Agarwal , and Satyen Kale . Logarithmic regret algorithms for online convex optimization . Machine Learning , 69 ( 2 - 3 ) : 169 – 192 , August 2007 . doi : 10 . 1007 / s10994 - 007 - 5016 - 8 . 16 Rie Johnson and Tong Zhang . Accelerating stochastic gradient descent using predictive variance reduction . In Advances in Neural Information Processing Systems , NIPS 2013 , pages 315 – 323 , 2013 . 17 Anatoli Juditsky . Convex optimization ii : Algorithms . Lecture notes , November 2013 . 18 Jonathan A . Kelner , Yin Tat Lee , Lorenzo Orecchia , and Aaron Sidford . An Almost - Linear - Time Algorithm for Approximate Max Flow in Undirected Graphs , and its Multicommodity Generalizations . In SODA , April 2014 . doi : 10 . 1137 / 1 . 9781611973402 . 16 . 19 Guanghui Lan . An optimal method for stochastic composite optimization . Mathematical Programming , 133 ( 1 - 2 ) : 365 – 397 , January 2011 . doi : 10 . 1007 / s10107 - 010 - 0434 - y . 20 Yin Tat Lee , Satish Rao , and Nikhil Srivastava . A new approach to computing maximum ﬂows using electrical ﬂows . In STOC , page 755 , New York , New York , USA , 2013 . 21 Yin Tat Lee and Aaron Sidford . Eﬃcient accelerated coordinate descent methods and faster algorithms for solving linear systems . In FOCS , pages 147 – 156 . IEEE , 2013 . 22 Michael W . Mahoney , Satish Rao , Di Wang , and Peng Zhang . Approximating the solution to mixed packing and covering lps in parallel e O ( (cid:15) − 3 ) time . In ICALP , 2016 . 23 H . Brendan McMahan and Matthew Streeter . Adaptive Bound Optimization for Online Convex Optimization . In COLT , 2010 . arXiv : 1002 . 4908 . 24 Arkadi Nemirovsky and David Yudin . Problem complexity and method eﬃciency in opti - mization . Nauka Publishers , Moscow ( in Russian ) , 1978 . John Wiley , New York ( in English ) 1983 . 25 Yurii Nesterov . A method of solving a convex programming problem with convergence rate O ( 1 / k 2 ) . In Doklady AN SSSR ( translated as Soviet Mathematics Doklady ) , volume 269 , pages 543 – 547 , 1983 . 26 Yurii Nesterov . Introductory Lectures on Convex Programming Volume : A Basic course , volume I . Kluwer Academic Publishers , 2004 . 27 Yurii Nesterov . Smooth minimization of non - smooth functions . Mathematical Programming , 103 ( 1 ) : 127 – 152 , December 2005 . doi : 10 . 1007 / s10107 - 004 - 0552 - 5 . 28 Yurii Nesterov . Primal - dual subgradient methods for convex problems . Mathematical Pro - gramming , 120 ( 1 ) : 221 – 259 , June 2007 . doi : 10 . 1007 / s10107 - 007 - 0149 - x . 29 Yurii Nesterov . Accelerating the cubic regularization of newton’s method on convex prob - lems . Mathematical Programming , 112 ( 1 ) : 159 – 181 , 2008 . 30 Yurii Nesterov . Gradient methods for minimizing composite functions . Mathematical Pro - gramming , 140 ( 1 ) : 125 – 161 , 2013 . doi : 10 . 1007 / s10107 - 012 - 0629 - 5 . 31 Yurii Nesterov . Universal gradient methods for convex optimization problems . Mathemat - ical Programming , May 2014 . doi : 10 . 1007 / s10107 - 014 - 0790 - 0 . Z . Allen - Zhu and L . Orecchia 3 : 15 32 Brendan O’Donoghue and Emmanuel Candès . Adaptive Restart for Accelerated Gra - dient Schemes . Foundations of Computational Mathematics , July 2013 . doi : 10 . 1007 / s10208 - 013 - 9150 - 3 . 33 Lorenzo Orecchia , Sushant Sachdeva , and Nisheeth K . Vishnoi . Approximating the expo - nential , the lanczos method and an e O ( m ) - time spectral algorithm for balanced separator . In STOC ’12 . ACM Press , November 2012 . 34 Serge A . Plotkin , David B . Shmoys , and Éva Tardos . Fast Approximation Algorithms for Fractional Packing and Covering Problems . Mathematics of Operations Research , 20 ( 2 ) : 257 – 301 , May 1995 . doi : 10 . 1287 / moor . 20 . 2 . 257 . 35 Ankan Saha , S . V . N . Vishwanathan , and Xinhua Zhang . New Approximation Algorithms for Minimum Enclosing Convex Shapes . In SODA , pages 1146 – 1160 , September 2011 . arXiv : 0909 . 1062 . 36 Shai Shalev - Shwartz . Online Learning and Online Convex Optimization . Foundations and Trends in Machine Learning , 4 ( 2 ) : 107 – 194 , 2012 . doi : 10 . 1561 / 2200000018 . 37 Shai Shalev - Shwartz and Yoram Singer . Logarithmic regret algorithms for strongly convex repeated games . Technical report , The Hebrew University , 2007 . 38 Shai Shalev - Shwartz and Tong Zhang . Accelerated Mini - Batch Stochastic Dual Coordinate Ascent . In NIPS , pages 1 – 17 , May 2013 . arXiv : 1305 . 2581 . 39 Shai Shalev - Shwartz and Tong Zhang . Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization . In ICML , pages 64 – 72 , 2014 . 40 Ohad Shamir and Tong Zhang . Stochastic Gradient Descent for Non - smooth Optimization : Convergence Results and Optimal Averaging Schemes . In Proceedings of the 30th Interna - tional Conference on Machine Learning - ICML ’13 , volume 28 , 2013 . arXiv : 1212 . 1824 . 41 Weijie Su , Stephen Boyd , and Emmanuel Candes . A diﬀerential equation for modeling nesterov’s accelerated gradient method : Theory and insights . In Advances in Neural Infor - mation Processing Systems , pages 2510 – 2518 , 2014 . 42 Di Wang , Michael W . Mahoney , Nishanth Mohan , and Satish Rao . Faster parallel solver for positive linear programs via dynamically - bucketed selective coordinate descent . ArXiv e - prints , abs / 1511 . 06468 , November 2015 . 43 Di Wang , Satish Rao , and Michael W . Mahoney . Uniﬁed acceleration method for packing and covering problems via diameter reduction . In ICALP , 2016 . 44 Lin Xiao . Dual averaging method for regularized stochastic learning and online optimization . The Journal of Machine Learning Research , 11 : 2543 – 2596 , 2010 . A Several Remarks on First - Order Methods A . 1 Importance of Non - Euclidean Norms Let us use a simple example to illustrate the importance of allowing arbitrary norms in studying ﬁrst - order methods . Consider the saddle point problem of min x ∈ ∆ n max y ∈ ∆ m y T Ax , where A is an m × n matrix , ∆ n = { x ∈ R n : x ≥ 0 ∧ 1 T x = 1 } is the unit simplex in R n , and ∆ m = { y ∈ R m : y ≥ 0 ∧ 1 T y = 1 } . This problem is important to study because it captures packing and covering linear programs that have wide applications in many areas of computer science ( see the survey of [ 8 ] ) . Letting µ = ε 2log m , Nesterov has shown that the following objective f µ ( x ) def = µ log (cid:0) 1 m m X j = 1 exp 1 µ ( Ax ) j (cid:1) , ITCS 2017 3 : 16 Linear Coupling : An Ultimate Uniﬁcation of Gradient and Mirror Descent when optimized over x ∈ ∆ n , can yield an additive ε / 2 solution to the original saddle point problem [ 27 ] . This f µ ( x ) is proven to be 1 µ - smooth with respect to the ‘ 1 - norm over ∆ n , if all the entries of A are between [ − 1 , 1 ] . Instead , f µ ( x ) is 1 µ - smooth with respect to the ‘ 2 - norm over ∆ n , only if the sum of squares of every row of A is at most 1 . This ‘ 2 condition is certainly stronger and less natural than the ‘ 1 condition , and the ‘ 1 condition one leads to the fastest ( approximate ) width - dependent positive LP solver [ 27 ] . Diﬀerent norm conditions also yield diﬀerent gradient and mirror descent steps . For instance , in the ‘ 1 - norm case , the gradient step is x 0 ← arg min x 0 ∈ ∆ n (cid:8) 12 k x 0 − x k 21 + α h∇ f µ ( x ) , x 0 − x i (cid:9) , and the mirror step is x 0 ← arg min x 0 ∈ ∆ n (cid:8)P i ∈ [ n ] x 0 i log x 0 i x i + α h∇ f µ ( x ) , x 0 − x i (cid:9) . In the ‘ 2 - norm case , gradient and mirror steps are both of the form x 0 ← arg min x 0 ∈ ∆ n (cid:8) 12 k x 0 − x k 22 + α h∇ f µ ( x ) , x 0 − x i (cid:9) . As another example , [ 35 ] has shown that the ‘ 1 norm , instead of the ‘ 2 one , is crucial when computing the minimum enclosing ball of points . One can ﬁnd other applications as well in [ 27 ] for the use of non - Euclidean norms , and an interesting example of ‘ ∞ - norm gradient descent for nearly - linear time maximum ﬂow in [ 18 ] . It is now important to note that , the methods in [ 25 , 26 ] work only for the ‘ 2 - norm case , and it is not clear how the proof can be generalized to other norms until [ 27 ] . Some other proofs ( such as [ 13 ] ) only work for the ‘ 2 - norm because the mirror steps are described as ( a scaled version of ) gradient steps . A . 2 Folklore Relationship Between Multiplicative Weight Updates and Mirror Descent The multiplicative weight update ( MWU ) method ( see the survey of Arora , Hazan and Kale [ 8 ] ) is a simple method that has been repeatedly discovered in theory of computation , machine learning , optimization , and game theory . The setting of this method is the following . Let ∆ n = { x ∈ R n : x ≥ 0 ∧ 1 T x = 1 } be the unit simplex in R n , and we call any vector in ∆ n an action . A player is going to play T actions x 0 , . . . , x T − 1 ∈ ∆ n in a row ; only after playing x k , the player observes a loss vector ‘ k ∈ R n that may depend on x k , and suﬀers from a loss value h ‘ k , x k i . The MWU method ensures that , if k ‘ k k ∞ ≤ ρ for all k ∈ [ T ] , then the player has an ( adaptive ) strategy to choose the actions such that the average regret is bounded : 1 T (cid:16) T − 1 X i = 0 h ‘ k , x k i − min u ∈ ∆ n T − 1 X i = 0 h ‘ k , u i (cid:17) ≤ O (cid:16) ρ √ log n √ T (cid:17) . ( A . 1 ) The left hand side is called the average regret because it is the ( average ) diﬀerence between the suﬀered loss P T − 1 i = 0 h ‘ k , x k i , and the loss P T − 1 i = 0 h ‘ k , u i of the best action u ∈ ∆ n in hindsight . Another way to interpret ( A . 1 ) is to state that we can obtain an average regret of ε using T = O ( ρ 2 log n ε 2 ) rounds . The above result can be proven directly using mirror descent . Letting w ( x ) def = P i x i log x i be the entropy DGF over the simplex Q = ∆ n , and its corresponding Bregman divergence V x ( x 0 ) def = P i ∈ [ n ] x 0 i log x 0 i x i , we consider the following update rule . Start from x 0 = ( 1 / n , . . . , 1 / n ) , and update x k + 1 = Mirr x k (cid:0) α‘ k (cid:1) , or equivalently , x k + 1 , i = x k , i · exp − α‘ k , i / Z k , where Z k > 0 is the normalization factor that equals to P ni = 1 x k , i · Z . Allen - Zhu and L . Orecchia 3 : 17 exp − α‘ k , i . 16 Then , the mirror - descent guarantee ( 2 . 2 ) implies that 17 ∀ u ∈ ∆ n , α h ‘ k , x k − u i ≤ α 2 2 k ‘ k k 2 ∞ + V x k ( u ) − V x k + 1 ( u ) . After telescoping the above inequality for all k = 0 , 1 , . . . , T − 1 , and using the upper bounds k ‘ ( x k ) k ∞ ≤ ρ and V x 0 ( u ) ≤ log n , we obtain that for all u ∈ ∆ n , 1 T T − 1 X k = 0 h ‘ k , x k − u i ≤ αρ 2 2 + log n αT . Setting α = √ log n ρ √ T we arrive at the desired average regret bound ( A . 1 ) . In sum , we have re - deduced the MWU method from mirror descent , and the above proof is quite diﬀerent from most of the classical analysis of MWU ( e . g . , [ 7 , 8 , 14 , 34 ] ) . It can be generalized to solve the matrix version of MWU [ 8 , 33 ] , as well as to incorporate the width - reduction technique [ 8 , 34 ] . We ignore such extensions here because they are outside the scope of this paper . A . 3 Deducing the Mirror - Descent Guarantee via Gradient Descent In this section , we re - deduce the convergence rate of mirror descent from gradient descent . In particular , we show that the dual averaging steps are equivalent to gradient steps on the Fenchel dual of the regularized regret , and deduce the same convergence bound as ( 2 . 4 ) . ( Similar proof can also be obtained for mirror steps but is notationally more involved . ) Given a sequence of points x 0 , . . . , x T − 1 ∈ Q , the ( scaled ) regret with respect to any point u ∈ Q is R ( x 0 , . . . , x T − 1 , u ) def = P T − 1 i = 0 α h ∂f ( x i ) , x i − u i . Since it satisﬁes that αT · ( f ( x ) − f ( u ) ) ≤ R ( x 0 , . . . , x T − 1 , u ) , the average regret ( after scaling ) upper bounds on the distance between any point f ( u ) and the average x = 1 T ( x 0 + · · · + x T − 1 ) . Consider now the regularized regret b R ( x 0 , . . . , x T − 1 ) def = max u ∈ Q n T − 1 X i = 0 α h ∂f ( x i ) , x i − u i − w ( u ) o , and we can rewrite it using the Fenchel dual w ∗ ( λ ) def = max u ∈ Q { h λ , u i − w ( u ) } of w ( · ) : b R ( x 0 , . . . , x T − 1 ) = w ∗ (cid:16) − α T − 1 X i = 0 ∂f ( x i ) (cid:17) + T − 1 X i = 0 α h ∂f ( x i ) , x i i . The classical theory of Fenchel duality tells us that w ∗ ( λ ) is 1 - smooth with respect to the dual norm k · k ∗ , because w ( · ) is 1 - strongly convex with respect to k · k . We also have ∇ w ∗ ( λ ) = arg max u ∈ Q { h λ , u i − w ( u ) } . ( See for instance [ 36 ] . ) With enough notations introduced , let us now minimize b R by intelligently selecting x 0 , . . . , x T − 1 . Perhaps a little counter - intuitively , we start from x 0 = · · · = x T − 1 = x ∗ and accordingly ∂f ( x ∗ ) = 0 ( if there are multiple subgradients at x ∗ , choose the zero one ) . 16 This version of the MWU is often known as the Hedge rule [ 14 ] . Another commonly used version is to choose x k + 1 , i = x k , i ( 1 − α‘ k , i ) Z k . Since e − t ≈ 1 − t whenever | t | is small and our choice of α will make sure that | α‘ k , i | (cid:28) 1 , this is essentially identical to the Hedge rule . 17 To be precise , we have replaced ∂f ( x k ) with ‘ k . It is easy to see from the proof of ( 2 . 2 ) that this loss vector ‘ k does not need to come from the subgradient of some objective f ( · ) . ITCS 2017 3 : 18 Linear Coupling : An Ultimate Uniﬁcation of Gradient and Mirror Descent This corresponds to a regret value of zero and a regularized regret b R ( x ∗ , . . . , x ∗ ) = w ∗ ( 0 ) = − min u ∈ Q { w ( u ) } . Next , we choose the values of x 0 , . . . , x T − 1 one by one . We choose x 0 = arg min u ∈ Q { w ( u ) } as the starting point . 18 Suppose that the values of x 0 , . . . , x k − 1 are already determined , and we are ready to pick x k ∈ Q . Let us compute the changes in the regularized regret as a function of x k : ∆ b R = b R ( x 0 , . . . , x k , x ∗ , . . . , x ∗ ) − b R ( x 0 , . . . , x k − 1 , x ∗ , . . . , x ∗ ) = w ∗ (cid:16) − α k X i = 0 ∂f ( x i ) (cid:17) − w ∗ (cid:16) − α k − 1 X i = 0 ∂f ( x i ) (cid:17) + α h ∂f ( x k ) , x k i ≤ D ∇ w ∗ (cid:16) − α k − 1 X i = 0 ∂f ( x i ) (cid:17) , − α∂f ( x k ) E + 1 2 (cid:13)(cid:13) α∂f ( x k ) (cid:13)(cid:13) 2 ∗ + α h ∂f ( x k ) , x k i . ( A . 2 ) Here , the last inequality is because w ∗ ( a ) − w ∗ ( b ) ≤ h∇ w ∗ ( b ) , a − b i + 12 k a − b k 2 ∗ , owing to the smoothness of w ∗ ( · ) . At this moment , it is clear to see that if one chooses x k = ∇ w ∗ (cid:16) − α k − 1 X i = 0 ∂f ( x i ) (cid:17) = arg min u ∈ Q n w ( u ) + k − 1 X i = 0 α h ∂f ( x i ) , u i o , the ﬁrst and third terms in ( A . 2 ) cancel out , and we obtain ∆ b R ≤ 12 (cid:13)(cid:13) α∂f ( x k ) (cid:13)(cid:13) 2 ∗ . In other words , the regularized regret increases by no more than 12 (cid:13)(cid:13) α∂f ( x k ) (cid:13)(cid:13) 2 ∗ ≤ α 2 ρ 2 / 2 in each step , so in the end we have b R ( x 0 , . . . , x T − 1 ) ≤ − w ( x 0 ) + α 2 ρ 2 T / 2 . In sum , by the deﬁnition of the regularized regret , we have αT · ( f ( x ) − f ( x ∗ ) ) − w ( x ∗ ) ≤ T − 1 X i = 0 α h ∂f ( x i ) , x i − x ∗ i − w ( x ∗ ) ≤ b R ( x 0 , . . . , x T − 1 ) ≤ − w ( x 0 ) + α 2 ρ 2 T 2 . This implies the following upper bound on the optimality of f ( x ) f ( x ) − f ( x ∗ ) ≤ αρ 2 2 + w ( x ∗ ) − w ( x 0 ) αT = αρ 2 2 + V x 0 ( x ∗ ) αT ≤ αρ 2 2 + Θ αT . Finally , choosing α = √ 2Θ ρ ·√ T to be the step length , we arrive at f ( x ) − f ( x ∗ ) ≤ √ 2Θ · ρ √ T , which is the same convergence rate as ( 2 . 4 ) . B Missing Proof of Section 2 For the sake of completeness , we provide self - contained proofs of the mirror descent and mirror descent guarantees in this section . 18 Dual averaging steps typically demand the ﬁrst point x 0 to be at the minimum of the regularizer w ( · ) , because that leads to the cleanest analysis . This can be relaxed to allow an arbitrary starting point . Z . Allen - Zhu and L . Orecchia 3 : 19 B . 1 Missing Proof for Gradient Descent Gradient Descent Guarantee f ( Grad ( x ) ) ≤ f ( x ) − Prog ( x ) ( 2 . 1 ) or in the special case when Q = R n f ( Grad ( x ) ) ≤ f ( x ) − 1 2 L k∇ f ( x ) k 2 ∗ . Proof . 19 Letting e x = Grad ( x ) , we prove the ﬁrst inequality by Prog ( x ) = − min y ∈ Q n L 2 k y − x k 2 + h∇ f ( x ) , y − x i o = − (cid:16) L 2 k e x − x k 2 + h∇ f ( x ) , e x − x i (cid:17) = f ( x ) − (cid:16) L 2 k e x − x k 2 + h∇ f ( x ) , e x − x i + f ( x ) (cid:17) ≤ f ( x ) − f ( e x ) . Here , the last inequality is a consequence of the smoothness assumption : for any x , y ∈ Q , f ( y ) − f ( x ) = Z 1 τ = 0 h∇ f ( x + τ ( y − x ) ) , y − x i dτ = h∇ f ( x ) , y − x ) + Z 1 τ = 0 h∇ f ( x + τ ( y − x ) ) − ∇ f ( x ) , y − x i dτ ≤ h∇ f ( x ) , y − x ) + Z 1 τ = 0 k∇ f ( x + τ ( y − x ) ) − ∇ f ( x ) k ∗ · k y − x k dτ ≤ h∇ f ( x ) , y − x ) + Z 1 τ = 0 τL k y − x k · k y − x k dτ = h∇ f ( x ) , y − x i + L 2 k y − x k 2 The second inequality follows because in the special case of Q = R n , we have Prog ( x ) = − min y ∈ Q n L 2 k y − x k 2 + h∇ f ( x ) , y − x i o = 1 2 L k∇ f ( x ) k 2 ∗ . (cid:74) (cid:73) Fact 2 . 1 ( Gradient Descent Convergence ) . Let f ( x ) be a convex , diﬀerentiable function that is L - smooth with respect to k · k on Q = R n , and x 0 any initial point in Q . Consider the sequence of T gradient steps x k + 1 ← Grad ( x k ) , then the last point x T satisﬁes that f ( x T ) − f ( x ∗ ) ≤ O (cid:0) LR 2 T (cid:1) , where R = max x : f ( x ) ≤ f ( x 0 ) k x − x ∗ k , and x ∗ is any minimizer of f . Proof . 20 Recall that we have f ( x k + 1 ) ≤ f ( x k ) − 12 L k∇ f ( x k ) k 2 ∗ from ( 2 . 1 ) . Furthermore , by the convexity of f and Cauchy - Schwarz we have f ( x k ) − f ( x ∗ ) ≤ h∇ f ( x k ) , x k − x ∗ i ≤ k∇ f ( x k ) k ∗ · k x k − x ∗ k ≤ R · k∇ f ( x k ) k ∗ . Letting D k = f ( x k ) − f ( x ∗ ) denote the distance to the optimum at iteration k , we now obtain two relationships D k − D k + 1 ≥ 12 L k∇ f ( x k ) k 2 ∗ as well as D k ≤ R · k∇ f ( x k ) k ∗ . Combining these two , we get D 2 k ≤ 2 LR 2 ( D k − D k + 1 ) = ⇒ D k D k + 1 ≤ 2 LR 2 (cid:16) 1 D k + 1 − 1 D k (cid:17) . 19 This proof can be found for instance in the textbook [ 26 ] . 20 Our proof follows almost directly from [ 26 ] , but he only uses the Euclidean ‘ 2 norm . ITCS 2017 3 : 20 Linear Coupling : An Ultimate Uniﬁcation of Gradient and Mirror Descent Noticing that D k ≥ D k + 1 because our objective only decreases at every round , we obtain that 1 D k + 1 − 1 D k ≥ 1 2 LR 2 . Finally , we conclude that at round T , we must have 1 D T ≥ T 2 LR 2 , ﬁnishing the proof that f ( x T ) − f ( x ∗ ) ≤ 2 LR 2 T . (cid:74) B . 2 Missing Proof for Mirror Descent Mirror Descent Guarantee If x k + 1 = Mirr x k (cid:0) α · ∂f ( x k ) (cid:1) , then ∀ u ∈ Q , α ( f ( x k ) − f ( u ) ) ≤ α h ∂f ( x k ) , x k − u i ≤ α 2 2 k ∂f ( x k ) k 2 ∗ + V x k ( u ) − V x k + 1 ( u ) . ( 2 . 2 ) Proof . 21 we compute that α h ∂f ( x k ) , x k − u i = h α∂f ( x k ) , x k − x k + 1 i + h α∂f ( x k ) , x k + 1 − u i (cid:172) ≤ h α∂f ( x k ) , x k − x k + 1 i + h−∇ V x k ( x k + 1 ) , x k + 1 − u i (cid:173) = h α∂f ( x k ) , x k − x k + 1 i + V x k ( u ) − V x k + 1 ( u ) − V x k ( x k + 1 ) (cid:174) ≤ (cid:16) h α∂f ( x k ) , x k − x k + 1 i − 1 2 k x k − x k + 1 k 2 (cid:17) + (cid:0) V x k ( u ) − V x k + 1 ( u ) (cid:1) (cid:175) ≤ α 2 2 k ∂f ( x k ) k 2 ∗ + (cid:0) V x k ( u ) − V x k + 1 ( u ) (cid:1) Here , (cid:172) is due to the minimality of x k + 1 = arg min x ∈ Q { V x k ( x ) + h α∂f ( x k ) , x i } , which implies that h∇ V x k ( x k + 1 ) + α∂f ( x k ) , u − x k + 1 i ≥ 0 for all u ∈ Q . (cid:173) is due to the triangle equality of Bregman divergence . 22 (cid:174) is because V x ( y ) ≥ 12 k x − y k 2 by the strong convexity of the DGF w ( · ) . (cid:175) is by Cauchy - Schwarz . (cid:74) C Missing Proofs of Section 4 (cid:73) Lemma 4 . 2 . If τ k = 1 α k + 1 L , then it satisﬁes that for every u ∈ Q , α k + 1 h∇ f ( x k + 1 ) , z k − u i (cid:172) ≤ α 2 k + 1 L Prog ( x k + 1 ) + V z k ( u ) − V z k + 1 ( u ) (cid:173) ≤ α 2 k + 1 L (cid:0) f ( x k + 1 ) − f ( y k + 1 ) (cid:1) + V z k ( u ) − V z k + 1 ( u ) . Proof . The second inequality (cid:173) is again from the gradient descent guarantee f ( x k + 1 ) − f ( y k + 1 ) ≥ Prog ( x k + 1 ) . To prove (cid:172) , we ﬁrst write down the key inequality of mirror - descent 21 This proof can be found for instance in the textbook [ 9 ] . 22 That is , ∀ x , y ≥ 0 , h−∇ V x ( y ) , y − u i = h∇ w ( x ) − ∇ w ( y ) , y − u i = ( w ( u ) − w ( x ) − h∇ w ( x ) , u − x i ) − ( w ( u ) − w ( y ) − h∇ w ( y ) , u − y ) i ) − ( w ( y ) − w ( x ) − h∇ w ( x ) , y − x i ) = V x ( u ) − V y ( u ) − V x ( y ) . Z . Allen - Zhu and L . Orecchia 3 : 21 analysis ( whose proof is identical to that of ( 2 . 2 ) ) α k + 1 h∇ f ( x k + 1 ) , z k − u i = h α k + 1 ∇ f ( x k + 1 ) , z k − z k + 1 i + h α k + 1 ∇ f ( x k + 1 ) , z k + 1 − u i (cid:172) ≤ h α k + 1 ∇ f ( x k + 1 ) , z k − z k + 1 i + h−∇ V z k ( z k + 1 ) , z k + 1 − u i (cid:173) = h α k + 1 ∇ f ( x k + 1 ) , z k − z k + 1 i + V z k ( u ) − V z k + 1 ( u ) − V z k ( z k + 1 ) (cid:174) ≤ (cid:16) h α k + 1 ∇ f ( x k + 1 ) , z k − z k + 1 i − 1 2 k z k − z k + 1 k 2 (cid:17) + (cid:0) V z k ( u ) − V z k + 1 ( u ) (cid:1) Here , (cid:172) is due to the minimality of z k + 1 = arg min z ∈ Q { V z k ( z ) + h α k + 1 ∇ f ( x k + 1 ) , z i } , which implies that h∇ V z k ( z k + 1 ) + α k + 1 ∇ f ( x k + 1 ) , u − z k + 1 i ≥ 0 for all u ∈ Q . (cid:173) is due to the triangle equality of Bregman divergence ( see Footnote 22 in Appendix B ) . (cid:174) is because V x ( y ) ≥ 12 k x − y k 2 by the strong convexity of the w ( · ) . If one stops here and uses Cauchy - Shwartz h α k + 1 ∇ f ( x k + 1 ) , z k − z k + 1 i− 12 k z k − z k + 1 k 2 ≤ α 2 k + 1 2 k∇ f ( x k + 1 ) k 2 ∗ , he will get the desired inequality in the special case of Q = R n , because Prog ( x k + 1 ) = 12 L k∇ f ( x k + 1 ) k 2 ∗ from ( 2 . 1 ) . For the general unconstrained case , we need to use the special choice of τ k = 1 / α k + 1 L follows . Letting v def = τ k z k + 1 + ( 1 − τ k ) y k ∈ Q so that x k + 1 − v = ( τ k z k + ( 1 − τ k ) y k ) − v = τ k ( z k − z k + 1 ) , we have h α k + 1 ∇ f ( x k + 1 ) , z k − z k + 1 i − 1 2 k z k − z k + 1 k 2 = h α k + 1 τ k ∇ f ( x k + 1 ) , x k + 1 − v i − 1 2 τ 2 k k x k + 1 − v k 2 = α 2 k + 1 L (cid:18) h∇ f ( x k + 1 ) , x k + 1 − v i − L 2 k x k + 1 − v k 2 (cid:19) ≤ α 2 k + 1 L Prog ( x k + 1 ) where the last inequality is from the deﬁnition of Prog ( x k + 1 ) . (cid:74) (cid:73) Lemma 4 . 3 ( Coupling ) . For any u ∈ Q , (cid:0) α 2 k + 1 L (cid:1) f ( y k + 1 ) − (cid:0) α 2 k + 1 L − α k + 1 (cid:1) f ( y k ) + (cid:0) V z k + 1 ( u ) − V z k ( u ) (cid:1) ≤ α k + 1 f ( u ) . Proof . We deduce the following sequence of inequalities α k + 1 (cid:0) f ( x k + 1 ) − f ( u ) (cid:1) ≤ α k + 1 h∇ f ( x k + 1 ) , x k + 1 − u i = α k + 1 h∇ f ( x k + 1 ) , x k + 1 − z k i + α k + 1 h∇ f ( x k + 1 ) , z k − u i (cid:172) = ( 1 − τ k ) α k + 1 τ k h∇ f ( x k + 1 ) , y k − x k + 1 i + α k + 1 h∇ f ( x k + 1 ) , z k − u i (cid:173) ≤ ( 1 − τ k ) α k + 1 τ k ( f ( y k ) − f ( x k + 1 ) ) + α k + 1 h∇ f ( x k + 1 ) , z k − u i (cid:174) ≤ ( 1 − τ k ) α k + 1 τ k ( f ( y k ) − f ( x k + 1 ) ) + α 2 k + 1 L (cid:0) f ( x k + 1 ) − f ( y k + 1 ) (cid:1) + V z k ( u ) − V z k + 1 ( u ) (cid:175) = (cid:0) α 2 k + 1 L − α k + 1 (cid:1) f ( y k ) − (cid:0) α 2 k + 1 L (cid:1) f ( y k + 1 ) + α k + 1 f ( x k + 1 ) + (cid:0) V z k ( u ) − V z k + 1 ( u ) (cid:1) Here , (cid:172) uses the choice of x k + 1 that satisﬁes τ k ( x k + 1 − z k ) = ( 1 − τ k ) ( y k − x k + 1 ) ; (cid:173) is by the convexity of f ( · ) and 1 − τ k ≥ 0 ; (cid:174) uses Lemma 4 . 2 ; and (cid:175) uses the choice of τ k = 1 / α k + 1 L . (cid:74) ITCS 2017 3 : 22 Linear Coupling : An Ultimate Uniﬁcation of Gradient and Mirror Descent D Strong Convexity Version of Accelerated Gradient Method When the objective f ( · ) is both σ - strongly convex and L - smooth with respect to the same norm k · k 2 , another version of accelerated gradient method exists and achieves a log ( 1 / ε ) convergence rate [ 26 ] . We show in this section that , our method AGM ( f , w , x 0 , T ) can be used to recover that strong - convexity accelerated method in one of the two ways . Therefore , the gradient - mirror coupling interpretation behind our paper still applies to the strong - convexity accelerated method . One way to recover the strong - convexity accelerated method is to replace the use of the mirror - descent analysis on the regret term by its strong - convexity counterpart ( also known as logarithmic - regret analysis , see for instance [ 15 , 37 ] ) . This would incur some diﬀerent parameter choices on α k and τ k , and results in an algorithm similar to that of [ 26 ] . Another , but simpler way is to recursively apply Theorem 4 . 1 . In light of the deﬁnition of strong convexity and Theorem 4 . 1 , we have σ 2 k y T − x ∗ k 22 ≤ f ( y T ) − f ( x ∗ ) ≤ 4 · 12 k x 0 − x ∗ k 22 · L T 2 . In particular , in every T = T 0 def = p 8 L / σ iterations , we can halve the distance k y T − x ∗ k 2 2 ≤ 12 k x 0 − x ∗ k 22 . If we repeatedly invoke AGM ( f , w , · , T 0 ) a sequence of ‘ times , each time feeding the initial vector x 0 with the previous output y T 0 , then in the last run of the T 0 iterations , we have f ( y T 0 ) − f ( x ∗ ) ≤ 4 · 12 ‘ k x 0 − x ∗ k 22 · L T 20 = 1 2 ‘ + 1 k x 0 − x ∗ k 22 · σ . By choosing ‘ = log (cid:0) k x 0 − x ∗ k 22 · σ ε (cid:1) , we conclude that (cid:73) Corollary 4 . 1 . If f ( · ) is both σ - strongly convex and L - smooth with respect to k · k 2 , in a total of T = O (cid:0)q Lσ · log (cid:0) k x 0 − x ∗ k 22 · σ ε (cid:1)(cid:1) iterations , we can obtain some x such that f ( x ) − f ( x ∗ ) ≤ ε . This is slightly better than the result O (cid:0)q Lσ · log (cid:0) k x 0 − x ∗ k 22 · L ε (cid:1)(cid:1) in Theorem 2 . 2 . 2 of [ 26 ] . We remark here that O’Donoghue and Candès [ 32 ] have studied some heuristic adaptive restarting techniques which suggest that the above ( and other ) restarting version of the accelerated method practically outperforms the original method of Nesterov .