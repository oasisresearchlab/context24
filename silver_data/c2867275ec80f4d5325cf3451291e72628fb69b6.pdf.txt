Which Algorithmic Choices Matter at Which Batch Sizes ? Insights From a Noisy Quadratic Model Guodong Zhang 1 , 2 , 3 ∗ , Lala Li 3 , Zachary Nado 3 , James Martens 4 , Sushant Sachdeva 1 , George E . Dahl 3 , Christopher J . Shallue 3 , Roger Grosse 1 , 2 1 University of Toronto , 2 Vector Institute , 3 Google Research , Brain Team , 4 DeepMind Abstract Increasing the batch size is a popular way to speed up neural network training , but beyond some critical batch size , larger batch sizes yield diminishing returns . In this work , we study how the critical batch size changes based on properties of the optimization algorithm , including acceleration and preconditioning , through two different lenses : large scale experiments , and analysis of a simple noisy quadratic model ( NQM ) . We experimentally demonstrate that optimization algorithms that employ preconditioning , speciﬁcally Adam and K - FAC , result in much larger critical batch sizes than stochastic gradient descent with momentum . We also demonstrate that the NQM captures many of the essential features of real neu - ral network training , despite being drastically simpler to work with . The NQM predicts our results with preconditioned optimizers , previous results with accel - erated gradient descent , and other results around optimal learning rates and large batch training , making it a useful tool to generate testable predictions about neural network optimization . 1 Introduction Increasing the batch size is one of the most appealing ways to accelerate neural network training on data parallel hardware . Larger batch sizes yield better gradient estimates and , up to a point , reduce the number of steps required for training , which reduces the training time . The importance of understanding the beneﬁts of modern parallel hardware has motivated a lot of recent work on training neural networks with larger batch sizes [ Goyal et al . , 2017 , Osawa et al . , 2018 , McCandlish et al . , 2018 , Shallue et al . , 2018 ] . To date , the most comprehensive empirical study of the effects of batch size on neural network training is Shallue et al . [ 2018 ] , who conﬁrmed that increasing the batch size initially achieves perfect scaling ( i . e . doubling the batch size halves the number of steps needed ) up to a problem - dependent critical batch size , beyond which it yields diminishing returns [ Balles et al . , 2017 , Goyal et al . , 2017 , Jastrz˛ebski et al . , 2018 , McCandlish et al . , 2018 ] . Shallue et al . [ 2018 ] also provided experimental evidence that the critical batch size depends on the optimization algorithm , the network architecture , and the data set . However , their experiments only covered plain SGD , SGD with ( heavy - ball ) momentum , and SGD with Nesterov momentum , leaving open the enticing possibility that other optimizers might extend perfect scaling to even larger batch sizes . Empirical scaling curves like those in Shallue et al . [ 2018 ] are essential for understanding the effects of batch size , but generating such curves , even for a single optimizer on a single task , can be very expensive . On the other hand , existing theoretical analyses that attempt to analytically derive critical batch sizes ( e . g . Ma et al . [ 2018 ] , Yin et al . [ 2018 ] , Jain et al . [ 2018 ] ) do not answer our questions about which optimizers scale the best with batch size . They tend to make strong assumptions , produce parameter - dependent results that are difﬁcult to apply , or are restricted to plain SGD . It would be ∗ Work done as part of the Google Student Researcher Program . Email : gdzhang @ cs . toronto . edu Preprint . Under review . a r X i v : 1907 . 04164v1 [ c s . L G ] 9 J u l 2019 ideal to ﬁnd a middle ground between a purely empirical investigation and theoretical analysis by building a model of neural network optimization problems that captures the essential behavior we see in real neural networks , while still being easy to understand . Additionally , we need to study optimizers beyond momentum SGD since they might provide us an approach to exploit speedups from the very largest batch sizes . In this work , we make the following contributions : 1 . We show that a simple noisy quadratic model ( NQM ) is remarkably consistent with the batch size effects observed in real neural networks , while allowing us to run experiments in seconds , making it a great tool to generate testable predictions about neural network optimization . 2 . We show that the NQM successfully predicts that momentum should speed up training relative to plain SGD at larger batch sizes , but do nothing at small batch sizes . 3 . Through large scale experiments with Adam [ Kingma and Ba , 2014 ] and K - FAC [ Martens and Grosse , 2015 ] , we conﬁrm that , as predicted by the NQM , preconditioning extends perfect batch size scaling to larger batch sizes than are possible with momentum SGD alone . Furthermore , unlike momentum , preconditioning can help at small batch sizes as well . 2 Related Work In a classic paper , Bottou and Bousquet [ 2008 ] studied the asymptotics of stochastic optimization algorithms and found SGD to be competitive with fancier approaches . They showed that stochastic optimization involves fundamentally different tradeoffs from full - batch optimization . More recently , several studies have investigated the relationship between batch size and training time for neural networks . Chen et al . [ 2018 ] studied the effect of network width on the critical batch size , and showed experimentally that it depends on both the data set and network architecture . Golmant et al . [ 2018 ] studied how various heuristics for adjusting the learning rate as a function of batch size affect the relationship between batch size and training time . Shallue et al . [ 2018 ] conducted a comprehensive empirical study on the relationship between batch size and training time with different neural network architectures and data sets using plain SGD , heavy - ball momentum , and Nesterov momentum . Finally , McCandlish et al . [ 2018 ] used the average gradient noise over training to predict the critical batch size . All of these studies described a basic relationship between batch size and training steps to a ﬁxed error goal , which is comprised of three regions : perfect scaling initially , then diminishing returns , and ﬁnally no beneﬁt for all batch sizes greater than the critical batch size . Other studies have attempted to characterize the critical batch size analytically in stochastic optimiza - tion . Under varying assumptions , Ma et al . [ 2018 ] , Yin et al . [ 2018 ] , Jain et al . [ 2018 ] all derived analytical notions of critical batch size , but to our knowledge , all for SGD . Finally , previous studies have shown that SGD and momentum SGD are equivalent for small learning rates ( after appropriate rescaling ) , both for the continuous limit [ Leen and Orr , 1994 ] and discrete settings Yuan et al . [ 2016 ] . However , they do not explain why momentum SGD ( including heavy - ball and Nesterov momentum ) sometimes outperforms plain SGD in mini - batch training ( as observed by Kidambi et al . [ 2018 ] and Shallue et al . [ 2018 ] ) . 3 Analysis of the Noisy Quadratic Model ( NQM ) In this section , we work with a noisy quadratic model ( NQM ) , a stochastic optimization problem whose dynamics can be simulated analytically , in order to reason about various phenomena encoun - tered in training neural networks . In this highly simpliﬁed model , we ﬁrst assume the loss function being optimized is a convex quadratic , with noisy observations of the gradient . For analytic tractabil - ity , we further assume the noise covariance is codiagonalizable with the Hessian ( an assumption we later test for neural networks ) . Because we are not interested in modeling overﬁtting effects , we focus on the online training setting , where the observations are drawn i . i . d . in every training iteration . Under these assumptions , we derive an analytic expression for the risk after any number of steps of SGD with a ﬁxed step size , as well as a dynamic programming method to compute the risk following a given step size schedule . Convex quadratics may appear an odd model for a complicated nonconvex optimization landscape . However , one obtains a convex quadratic objective by linearizing the network’s function around a given weight vector and taking the second - order Taylor approximation to the loss function ( assuming 2 it is smooth and convex ) . Indeed , recent theoretical works [ Jacot et al . , 2018 , Du et al . , 2019 , Zhang et al . , 2019a ] show that for wide enough networks , the weights stay close enough to the initialization for the linearized approximation to remain accurate . Empirically , linearized approximations closely match a variety of training phenomena for large but realistic networks [ Lee et al . , 2019 ] . 3 . 1 Problem Setup 0 50 100 150 200 250 300 350 400 Steps 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 R i s k high curvature ( 0 . 5 ) lower curvature ( 0 . 01 ) decay at step 100 decay at step 100 Figure 1 : Cartoon of the evolution of risk for different coordinates with and without learning rate decay . We now introduce the noisy quadratic model [ Schaul et al . , 2013 , Martens , 2014 , Wu et al . , 2018 ] , where the true function being optimized is a convex quadratic . Because we analyze rotation - invariant and translation - invariant optimizers such as SGD and heavy - ball momentum , we assume without loss of generality that the quadratic form is diagonal , and that the optimum is at the origin . Hence , our exact cost function decom - poses as a sum of scalar quadratic functions for each coordinate : L ( θ ) = 1 2 θ (cid:62) H θ = 1 2 d (cid:88) i = 1 h i θ 2 i (cid:44) d (cid:88) i = 1 (cid:96) ( θ i ) . ( 1 ) Without loss of generality , we assume h 1 ≥ h 2 ≥ . . . ≥ h d . We consider a single gradient query to have the form g ( θ ) = H θ + (cid:15) where E [ (cid:15) ] = 0 and Cov ( (cid:15) ) = C . To reduce the variance of gradient estimation , we can average over multiple independent queries , which corresponds to " mini - batch training " in neural network optimization . We denote the averaged gradient as g B ( θ ) and the covariance Cov ( g B ( θ ) ) = C / B , where B is the number of queries ( mini - batch size ) . For analytical tractability , we make the nontrivial assumption that H and C are codiagonalizable . ( Since H is diagonal , this implies that C = diag ( c 1 , . . . , c d ) . ) See Section 3 . 4 for justiﬁcation of this assumption . Under gradient descent with ﬁxed step size α , each dimension evolves independently as θ i ( t + 1 ) = ( 1 − αh i ) θ i ( t ) + α (cid:112) c i / B(cid:15) i , ( 2 ) where α is the learning rate and (cid:15) i is zero - mean unit variance iid noise . By treating θ i as a random variable , we immediately obtain the dynamics of its mean and variance . E [ θ i ( t + 1 ) ] = ( 1 − αh i ) E [ θ i ( t ) ] , V [ θ i ( t + 1 ) ] = ( 1 − αh i ) 2 V [ θ i ( t ) ] + α 2 c i B . ( 3 ) Based on eqn . ( 3 ) , the expected risk after t steps in a given dimension i is E [ (cid:96) ( θ i ( t ) ) ] = ( 1 − αh i ) 2 t (cid:124) (cid:123)(cid:122) (cid:125) convergence rate E [ (cid:96) ( θ i ( 0 ) ) ] + (cid:0) 1 − ( 1 − αh i ) 2 t (cid:1) αc i 2 B ( 2 − αh i ) (cid:124) (cid:123)(cid:122) (cid:125) steady state risk , ( 4 ) where we have assumed that αh i ≤ 2 . ( Note that this can be seen as a special case of the convergence result derived for convex quadratics in Martens [ 2014 ] . ) Remarkably , each dimension converges exponentially to a steady state risk . Unfortunately , there is a trade - off in the sense that higher learning rates ( up to 1 / h i ) give faster convergence to the steady state risk , but also produce higher values of the steady - state risk . The steady state risk also decreases proportionally to increases in batch size ; this is important to note because in the following subsections , we will show that traditional acceleration techniques ( e . g . , momentum and preconditioning ) help improve the convergence rate at the expense of increasing the steady state risk . Therefore , the NQM implies that momentum and preconditioning would beneﬁt more from large - batch training compared to plain SGD , as shown in later sections . 3 . 2 The Role of Momentum Applied to the same noisy quadratic model as before , the update equations for momentum SGD are : m i ( t + 1 ) = βm i ( t ) + h i θ i ( t ) + (cid:112) c i / B(cid:15) i , θ i ( t + 1 ) = θ i ( t ) − αm i ( t + 1 ) . ( 5 ) We show in the following theorem ( see Appendix C for proof ) that momentum SGD performs similarly to plain SGD in the regime of small batch sizes but helps in the large - batch regime , which can be viewed as a more " deterministically behaving " optimization problem . 3 Theorem 1 . Given a dimension index i , and 0 ≤ β < 1 with β (cid:54) = ( 1 − √ αh i ) 2 , the expected risk at time t associated with that dimension satisﬁes the upper bound E [ (cid:96) ( θ i ( t ) ) ] ≤ (cid:18) ( r t + 1 1 − r t + 1 2 ) − β ( r t 1 − r t 2 ) ) r 1 − r 2 (cid:19) 2 E [ (cid:96) ( θ i ( 0 ) ) ] + ( 1 + β ) αc i 2 B ( 2 β + 2 − αh i ) ( 1 − β ) , ( 6 ) where r 1 and r 2 ( with r 1 ≥ r 2 ) are the two roots of the quadratic equation x 2 − ( 1 − αh i + β ) x + β = 0 . As with plain SGD ( c . f . eqn . ( 4 ) ) , the loss associated with each dimension can be expressed as the sum of two terms , where the ﬁrst one decays exponentially and corresponds to the behavior of the deterministic version of the algorithm , and the second remains constant . Following the existing treatment of the deterministic version of the algorithm [ Chiang , 1974 , Qian , 1999 , Yang et al . , 2018 , Goh , 2017 ] , we divide our analysis two cases : overdamping and underdamp - ing . In the case of overdamping , where β < ( 1 − √ αh i ) 2 , both roots r 1 and r 2 are real and therefore the convergence rate is determined by the larger one ( i . e . r 1 ) , which has the value r 1 = 1 − αh i + β + (cid:112) ( 1 − β ) 2 − 2 ( 1 + β ) αh i + α 2 h 2 i 2 ( 7 ) With a ﬁxed learning rate , the steady state risk will be constant , and the best achievable expected risk will be lower bounded by it . Thus , to achieve a certain target loss we must either drive the learning rate down , or the batch size up . Assuming a small batch size and a low target risk , we are forced to pick a small learning rate , in which case one can show 2 that r 1 ≈ 1 − αh / 1 − β . In Figure 2 we plot the convergence rate as a function of β , and we indeed observe that the convergence rate closely matches 1 − αh / 1 − β , assuming a relative small learning rate . We further note that the convergence rate and steady state risk of eqn . ( 6 ) are the same as the ones in plain SGD ( eqn . ( 4 ) ) , except that they use an " effective learning rate " of α / 1 − β . To help validate these predictions , in Appendix D . 3 we provide a comparison of momentum SGD with plain SGD using the effective learning rate . 10 - 4 10 - 3 10 - 2 10 - 1 1 ¡ j r 1 j underdamping overdamping 1 ¡ ( 1 ¡ ®h 1 ¡ ¯ ) optimal 1 ¡ ¯ 10 - 3 10 - 2 10 - 1 10 0 1 ¡ ¯ 10 - 4 10 - 3 10 - 2 10 - 1 10 0 10 1 s t e a d y s t a t e r i s k ( SS K ) SSK of SGD with Momentum SSK of SGD using effective LR optimal 1 ¡ ¯ Figure 2 : Convergence rate and steady state risk ( SSK ) as a function of momentum for a single dimension with αh = 0 . 0005 and batch size B = 1 . In the case of underdamping where β > ( 1 − √ αh i ) 2 , both r 1 and r 2 will be complex and have norm √ β . We note that the optimal β should be equal to or smaller than ( 1 − √ αh d ) 2 , since otherwise all dimensions are under - damped , and we can easily improve the convergence rate and steady state risk by reducing β . Next we observe that the convergence of the total loss will eventually be dominated by the slowest converging dimension ( which corresponds to the smallest curvature h d ) , and this will be in the overdamping regime as argued above . By our analysis of the overdamping case , we can achieve the same convergence rate for this dimension by simply replacing the learning rate α in the bound for plain SGD ( eqn . ( 4 ) ) with the effective learning rate α / 1 − β . So while momentum gives no long - term training acceleration for very low ﬁxed learning rates ( which we are forced to use when the batch size is small ) , we note that it can help in large - batch training . With β > 0 , the steady state risk roughly ampliﬁes by a factor of 1 / 1 − β , and we note that steady state risk also decreases proportionally to increases in batch size . Therefore , we expect momentum SGD to exhibit perfect scaling up to larger batch sizes than plain SGD . 3 . 3 The Role of Preconditioning Many optimizers , such as Adam and K - FAC , can be viewed as preconditioned gradient descent methods . In each update , the gradient is rescaled by a PSD matrix P − 1 , called the preconditioner . θ ( t + 1 ) = θ ( t ) − α P − 1 [ H θ + (cid:15) ] . ( 8 ) In lieu of trying to construct noisy quadratic analogues of particular optimizers , we analyze precondi - tioners of the form P = H p with 0 ≤ p ≤ 1 . Note that P remains ﬁxed throughout training since the Hessian H is constant in the NQM . We can recover standard SGD by setting p = 0 . 2 To see this , note that the term in the square root of eqn . ( 7 ) for r 1 can be written as ( 1 − β − ( 1 + β ) αh i / 1 − β ) 2 + O ( α 2 h 2 i ) . Dropping the O ( α 2 h 2 i ) term and simplifying gives the claimed expression for r 1 . 4 Conveniently , for our NQM , the dynamics of preconditioned SGD are equivalent to the SGD dynamics in an NQM with Hessian ˜ H = P − 1 / 2 HP − 1 / 2 and gradient covariance ˜ C = P − 1 / 2 CP − 1 / 2 . Hence , the dynamics can be simulated using eqn . ( 4 ) , exactly like the non - preconditioned case . We immediately obtain the following bound on the risk : E [ L ( θ ( t ) ) ] ≤ d (cid:88) i = 1 ( 1 − αh ( 1 − p ) i ) 2 t E [ (cid:96) ( θ i ( 0 ) ) ] + d (cid:88) i = 1 αc i h − p i 2 B ( 2 − αh 1 − p i ) . ( 9 ) To qualitatively understand the effect of preconditioning , ﬁrst consider the ﬁrst term in eqn . ( 8 ) . The convergence of this term resembles that of gradient descent on a deterministic quadratic , which ( with optimal α ≈ 2 / ˜ h 1 ) converges exponentially at a rate of approximately 2 / ˜ κ , where ˜ κ = ˜ h 1 / ˜ h d is the condition number of the transformed problem . Since ˜ κ = κ 1 − p , this implies a factor of κ p improvement in the rate of convergence . Hence , for near - deterministic objectives where the ﬁrst term dominates , values of p closer to 1 correspond to better preconditioners , and result in much faster convergence . Unfortunately , there is no free lunch , as larger values of p will also increase the second term ( steady state risk ) . Assuming an ill - conditioned loss surface ( κ (cid:29) 1 ) , the steady state risk of each dimension becomes 1 2 B αc i h − p i 2 − αh ( 1 − p ) i ≈ c i 2 Bh 1 ( h i / h 1 ) − p 1 − ( h i / h 1 ) ( 1 − p ) , ( 10 ) which is a monotonically increasing function with respect to p . Even without this ampliﬁcation effect , the steady state risk will eventually become the limiting factor in the minimization of the expected risk . One way to reduce the steady state risk , apart from using Polyak averaging [ Polyak and Juditsky , 1992 ] or decreasing the learning rate ( which will harm the rate of convergence ) , is to increase the batch size . This suggests that the beneﬁts of using stronger preconditioners will be more clearly observed for larger batch sizes , which is an an effect that we empirically demonstrate in later sections . 3 . 4 Choice of H and C We’ve found that the qualitative behavior of optimizers in our NQM depends on the choices of H and C . Therefore , we choose matrices motivated by theoretical and empirical considerations about neural net training . First , we set the diagonal entries of H to be { 1 i } di = 1 for some integer d , giving a condition number of d . This closely matches the estimated eigenspectrum of the Hessian of a convolutional network ( see Figure 7 and Appendix D . 4 ) , and is also consistent with recent work ﬁnding heavy tailed eigenspectra of neural network Hessians [ Ubaru et al . , 2017 , Ghorbani et al . , 2019 ] . We choose d = 10 4 , which approximately matches the condition number of the K - FAC Hessian approximation for ResNet8 . ( Qualitative behaviors were consistent for a wide range of d . ) We also set C = H ( a nontrivial assumption ) . This was motivated by theoretical arguments that , under the assumption that the implicit conditional distribution over the network’s output is close to the conditional distribution of targets from the training distribution , the Hessian closely matches the gradient covariance in neural network training [ Martens , 2014 ] . Empirically , this relationship appears to hold tightly for a convolutional network and modestly well for a transformer ( see Appendix D . 2 ) . 3 . 5 Information Theoretic Lower Bound Since our NQM assumes the inﬁnite data ( online optimization ) setting , it’s instructive to compare the performance of optimizers against an information theoretic lower bound . Speciﬁcally , under the assumption that H = C , the NQM is equivalent to maximum likelihood estimation of the mean vector for a multivariate Gaussian distribution with covariance H − 1 . Hence , the risk obtained by any optimizer can be bounded below by the risk of the maximum likelihood estimator for the Gaussian , which is d / 2 N , where d is the dimension and N is the total number of training examples visited . We indicate this bound with a dashed black line in our plots . 3 . 6 Noisy Quadratic Experiments In this section , we simulate noisy quadratic optimization using the closed - form dynamics . Our aim is to formulate hypotheses for how different optimizers would behave for neural network optimization . Our main metric is the number of steps required to achieve a target risk . For efﬁciency , rather than 5 2 2 2 4 2 6 2 8 2 10 2 12 2 14 2 16 2 18 2 20 Batch size 2 - 2 2 0 2 2 2 4 2 6 2 8 2 10 2 12 2 14 2 16 2 18 2 20 S t e p s t o t h r e s h o l d pow 0 pow 0 . 25 pow 0 . 5 pow 0 . 75 lower bound ( a ) Scaling with Constant LR 2 2 2 4 2 6 2 8 2 10 2 12 2 14 2 16 2 18 2 20 Batch size 2 - 2 2 0 2 2 2 4 2 6 2 8 2 10 2 12 2 14 2 16 2 18 2 20 S t e p s t o t h r e s h o l d pow 0 pow 0 . 25 pow 0 . 5 pow 0 . 75 lower bound ( b ) Scaling with LR Schedules 0 10 20 30 40 50 Pieces 2 - 9 2 - 8 2 - 7 2 - 6 2 - 5 2 - 4 2 - 3 2 - 2 2 - 1 2 0 2 1 L e a r n i n g R a t e BS 16 BS 32 BS 64 BS 128 BS 256 BS 512 BS 1024 BS 2048 BS 4096 ( c ) Optimized LR Schedules Figure 3 : ( a ) Effects of momentum and preconditioning . Steps required to reach target loss as a function of batch size under different preconditioning power . Solid lines are momentum SGD while dashed lines are plain SGD . The black dashed line is the information theoretic lower bound . ( b ) Effect of learning rate decay . The solid lines use the optimized piecewise constant scheme , which are shown in ( c ) for power 0 . The dashed curves in ( b ) are plain SGD for comparison . We observe that learning rate schedules close most of the gap between the ﬁxed learning rate performance and the information theoretic lower bound . explicitly representing all the eigenvalues of H , we quantize them into 100 bins and count the number of eigenvalues in each bin . Unless otherwise speciﬁed , we initialize θ as N ( 0 , I ) and use a target risk of 0 . 01 . ( The results don’t seem to be sensitive to either the initial variance or the target risk ; some results with varying target risk thresholds are shown in Appendix D . 5 ) . 3 . 6 . 1 Effect of Momentum and Preconditioning We ﬁrst experiment with momentum and varying preconditioner powers on our NQM . We treat both the ( ﬁxed ) learning rate α and momentum decay parameter β as hyperparameters , which we tune using a ﬁne - grained grid search . Consistent with the empirical results of Shallue et al . [ 2018 ] , each optimizer shows two distinct regimes : a small - batch ( stochastic ) regime with perfect linear scaling , and a large - batch ( deterministic ) regime insensitive to batch size . We call the phase transition between these regimes the critical batch size . Consistent with the analysis of Section 3 . 2 and the observations of Smith et al . [ 2018 ] , Shallue et al . [ 2018 ] , Kidambi et al . [ 2018 ] , the performance of momentum - based optimizers matches that of the plain SGD methods in the small - batch regime , but momentum increases the critical batch size and gives substantial speedups in the large batch regime . Preconditioning also increases the critical batch size and gives substantial speedups in the large batch regime , but interestingly , also improves performance by a small constant factor even for very small batches . Combining momentum with preconditioning extends both of these trends . 3 . 6 . 2 Optimal Learning Rate and Decay Scheme In the NQM , we can calculate the optimal constant learning rate given a speciﬁc batch size . Figure 12 shows the optimal learning rate as a function of batch size for a target risk of 0 . 01 . Notably , the optimal learning rate of plain ( preconditioned ) SGD ( Figure 12a ) scales linearly with batch size before it hits the critical batch size , matching the scheme used in Goyal et al . [ 2017 ] . The linear scaling also holds for the effective learning rate of momentum SGD . In the small batch regime , the optimal effective learning rate for momentum SGD matches the optimal plain SGD learning rate , suggesting that the momentum and learning rate are interchangeable in the small batch regime . While a ﬁxed learning rate often works well for simple problems , good performance on the ImageNet benchmark [ Russakovsky et al . , 2015 ] requires a carefully tuned schedule . Here we explicitly optimize a piecewise constant learning rate schedule for SGD ( with 50 pieces ) , in terms of the number of steps to reach the loss threshold . 3 In Figure 3b , we show that optimized learning rate schedules help signiﬁcantly in the small batch regime , consistent with the analysis in Wu et al . [ 2018 ] . We observe the same linear scaling as with ﬁxed - learning - rate SGD , but with a better constant factor . In fact , optimized schedules nearly achieve the information theoretic optimum . However , learning rate schedules do not improve at all over ﬁxed learning rates in the large batch regime . Figure 3c shows optimized schedules for different batch sizes ; interestingly , they maintain a large learning rate throughout training followed by a roughly exponential decay , consistent with commonly used 3 For a given schedule and number of time steps , we obtain the exact risk using dynamic programming with eqn . ( 3 ) . For stability , the learning rates are constrained to be at most 2 / h 1 . For a ﬁxed number of time steps , we minimize this risk using BFGS . We determine the optimal number of time steps using binary search . 6 Data Set Size Model Remarks LR MNIST 55 , 000 Simple CNN Same as Shallue et al . [ 2018 ] except without dropout regularization . Constant FMNIST 55 , 000 CIFAR10 45 , 000 ResNet8 without BN Same as Shallue et al . [ 2018 ] . Constant ResNet32 with BN Ghost batch norm is used . Linear Decay VGG11 with BN Ghost batch norm is used . Linear Decay LM1B ∼ 30M Two - layer Transformer Shallow model in Shallue et al . [ 2018 ] Constant Table 1 : Data sets and models used in our experiments . See Appendix E . 2 for full details . 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 2 13 Batch size 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 2 13 2 14 S t e p s t o t a r g e t Target Accuracy : 0 . 992 ( a ) Simple CNN on MNIST 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 2 13 Batch size 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 2 13 2 14 S t e p s t o t a r g e t Target Accuracy : 0 . 920 ( b ) Simple CNN on Fashion MNIST 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 2 13 Batch size 2 8 2 9 2 10 2 11 2 12 2 13 2 14 2 15 2 16 2 17 S t e p s t o t a r g e t Target Accuracy : 0 . 800 sgdheavy ball adam w / o momentum adamkfac w / o momentum kfac ( c ) ResNet8 on CIFAR10 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 Batch size 2 10 2 11 2 12 2 13 2 14 2 15 2 16 2 17 2 18 S t e p s t o t a r g e t Target Accuracy : 0 . 910 ( d ) VGG11 on CIFAR10 2 6 2 7 2 8 2 9 2 10 2 11 2 12 2 13 Batch size 2 10 2 11 2 12 2 13 2 14 2 15 2 16 2 17 S t e p s t o t a r g e t Target Accuracy : 0 . 930 ( e ) ResNet32 on CIFAR10 2 6 2 8 2 10 2 12 2 14 2 16 2 18 Batch Size 2 9 2 11 2 13 2 15 2 17 2 19 2 21 S t e p s t o t a r g e t Target cross entropy : 3 . 90 ( f ) Transformer on LM1B Figure 4 : Empirical relationship between batch size and steps to result . Key observations : 1 ) momentum SGD has no beneﬁt over plain SGD at small batch sizes , but extends the perfect scaling to larger batch sizes ; 2 ) preconditioning also extends perfect scaling to larger batch sizes , i . e . K - FAC > Adam > momentum SGD . This is most noticeable in the Transformer model ; 3 ) preconditioning ( particularly K - FAC ) reduces the number of steps needed to reach the target even for small batch sizes . All of these agree with the predictions by NQM . neural network training schedules . Additionally , even though the different batch sizes start with the same learning rate , their ﬁnal learning rates at the end of training scale linearly with batch size ( see Figure 13 in Appendix D . 7 ) . 4 Neural Network Experiments We investigated whether the predictions made by the NQM hold in practice by running experiments with ﬁve neural network architectures across three image classiﬁcation tasks and one language modeling task ( see Table 1 ) . For each model and task , we compared a range of optimizers : SGD , momentum SGD , Adam ( with and without momentum ) , and K - FAC ( with and without momentum ) . See Appendix E for more details about our models and tasks . The primary quantity we measured is the number of steps required to reach a target accuracy ( for image classiﬁcation tasks ) or cross entropy ( for language modeling ) . Unless otherwise speciﬁed , we measured steps to target on the validation set . We chose the target metric values based on an initial set of experiments with practical computational budgets . For each model , task , optimizer , and batch size , we independently tuned the learning rate α , the parameters governing the learning rate schedule ( where applicable ) , and optimizer - speciﬁc metaparameters ( see Appendix E . 4 ) . We manually chose the search spaces based on our initial experiments , and we veriﬁed after each experiment that the optimal metaparameter values were far from the search space boundaries . We used quasi - random search [ Bousquet et al . , 2017 ] to tune the metaparameters with ﬁxed budgets of non - divergent 4 trials ( 100 for Simple CNN , ResNet8 , and Transformer , and 200 for ResNet32 and VGG11 ) . We chose the trial that reached the target metric value using the fewest number of steps . 4 We discarded trials with a divergent training loss , which occurred when the learning rate was too high . 7 4 . 1 Critical Batch Size Depends on the Optimizer Figure 4 shows the relationship between batch size and steps to target for each model , task , and optimizer . In each case , as the batch size grows , there is an initial period of perfect scaling where doubling the batch size halves the steps to target , but once the batch size exceeds a problem - dependent critical batch size , there are rapidly diminishing returns , matching the results of [ Goyal et al . , 2017 , McCandlish et al . , 2018 , Shallue et al . , 2018 ] . K - FAC has the largest critical batch size in all cases , highlighting the usefulness of preconditioning . Momentum SGD extends perfect scaling to larger batch sizes than plain SGD , but for batch sizes smaller than the plain SGD critical batch size , momentum SGD requires as many steps as plain SGD to reach the target . This is consistent with both the empirical results of Shallue et al . [ 2018 ] and our NQM simulations . By contrast , Adam and K - FAC can reduce the number of steps needed to reach the target compared to plain SGD even for the smallest batch sizes , although neither optimizer does so in all cases . Finally , we see some evidence that the beneﬁt of momentum diminishes with preconditioning ( Figures 4a and 4b ) , as predicted by our NQM simulations , although we do not see this in all cases ( e . g . Figure 4c and 4f ) . 4 . 2 Optimal Learning Rate 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 2 13 Batch size 2 - 4 2 - 3 2 - 2 2 - 1 2 0 2 1 2 2 2 3 2 4 2 5 2 6 O p t i m a l ( e ff e c t i v e ) L R 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 2 13 Batch size 2 - 6 2 - 5 2 - 4 2 - 3 2 - 2 2 - 1 2 0 2 1 2 2 2 3 2 4 O p t i m a l ( e ff e c t i v e ) L R sgdheavy ball Figure 5 : Optimal learning rates for plain SGD and momentum SGD . Left : Simple CNN on MNIST ; Right : ResNet8 on CIFAR10 The NQM predicts that the optimal constant learning rate for plain SGD ( or effective learning rate for momen - tum SGD ) scales linearly with batch size initially , and then levels off after a certain batch size . Figure 5 shows the empirical optimal ( effective ) learn - ing rate as a function of batch size for simple CNN on MNIST and ResNet8 on CIFAR10 . For small batch sizes , the optimal learning rate of plain SGD appears to match the optimal effective learning rate of momentum SGD . However , after a certain batch size , the optimal learning rate for plain SGD saturates while the optimal effective learning rate of momentum SGD keeps increasing . Interestingly , plain SGD and momentum SGD appear to deviate at the same batch size in the optimal effective learning rate and steps to target plots ( Figures 4 and 5 ) . 4 . 3 Steps to Target on the Training Set 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 2 13 Batch size 2 8 2 9 2 10 2 11 2 12 2 13 2 14 2 15 2 16 2 17 S t e p s t o t a r g e t Target Accuracy : 0 . 830 sgdheavy ball adam w / o momentum adamkfac w / o momentum kfac 2 6 2 7 2 8 2 9 2 10 2 11 2 12 2 13 Batch size 2 9 2 10 2 11 2 12 2 13 2 14 2 15 2 16 S t e p s t o t a r g e t Target Accuracy : 0 . 990 Figure 6 : Steps to training accuracy versus batch size on CIFAR10 . Left : ResNet8 ; Right : ResNet32 . Figure 6 shows the empirical rela - tionship between batch size and steps to target , measured on the training set , for ResNet8 and ResNet32 on CI - FAR10 . For ResNet8 , the curves are almost identical to those using vali - dation accuracy ( Figure 4c ) , but for ResNet32 , the gaps between different optimizers become much smaller than in Figure 4e and the effects of momen - tum and preconditioning appear to become less signiﬁcant . Nevertheless , the qualitative differences between optimizers are consistent with the validation set measurements . 5 Conclusion In this work , we analyzed the interactions between the batch size and the optimization algorithm from two perspectives : experiments with real neural networks , and a noisy quadratic model with parameters chosen based on empirical observations about neural networks . Despite its simplicity , the noisy quadratic model agrees remarkably well with a variety of neural network training phenomena , including learning rate scaling , critical batch sizes , and the effects of momentum and preconditioning . More importantly , the noisy quadratic model allows us to run experiments in seconds , while it can take weeks , or even months , to conduct careful large - scale experiments with real neural networks . Therefore , the noisy quadratic model is a convenient and powerful way to quickly formulate testable predictions about neural network optimization . 8 References Jimmy Ba , Roger Grosse , and James Martens . Distributed second - order optimization using Kronecker - factored approximations . In International Conference on Learning Representations , 2017 . Juhan Bae , Guodong Zhang , and Roger Grosse . Eigenvalue corrected noisy natural gradient . In Workshop of Bayesian Deep Learning , Advances in neural information processing systems , 2018 . Lukas Balles , Javier Romero , and Philipp Hennig . Coupling adaptive batch sizes with learning rates . In Conference on Uncertainty in Artiﬁcial Intelligence ( UAI ) 2017 . AUAI Press , 2017 . Léon Bottou and Olivier Bousquet . The tradeoffs of large scale learning . In Advances in neural information processing systems , pages 161 – 168 , 2008 . Olivier Bousquet , Sylvain Gelly , Karol Kurach , Olivier Teytaud , and Damien Vincent . Critical hyper - parameters : No random , no cry . arXiv preprint arXiv : 1706 . 03200 , 2017 . Lingjiao Chen , Hongyi Wang , Jinman Zhao , Dimitris Papailiopoulos , and Paraschos Koutris . The effect of network width on the performance of large - batch training . In Advances in Neural Information Processing Systems , pages 9302 – 9309 , 2018 . A . C . Chiang . Fundamental Methods of Mathematical Economics . International student edition . McGraw - Hill , 1974 . ISBN 9780070107809 . Simon S . Du , Xiyu Zhai , Barnabas Poczos , and Aarti Singh . Gradient descent provably optimizes over - parameterized neural networks . In International Conference on Learning Representations , 2019 . URL https : / / openreview . net / forum ? id = S1eK3i09YQ . Thomas George , César Laurent , Xavier Bouthillier , Nicolas Ballas , and Pascal Vincent . Fast approximate natural gradient descent in a Kronecker - factored eigenbasis . In Advances in Neural Information Processing Systems , pages 9550 – 9560 , 2018 . Behrooz Ghorbani , Shankar Krishnan , and Ying Xiao . An investigation into neural net optimization via hessian eigenvalue density . In Proceedings of the 36th International Conference on Machine Learning , pages 2232 – 2241 , 2019 . Gabriel Goh . Why momentum really works . Distill , 2 ( 4 ) : e6 , 2017 . Noah Golmant , Nikita Vemuri , Zhewei Yao , Vladimir Feinberg , Amir Gholami , Kai Rothauge , Michael W Mahoney , and Joseph Gonzalez . On the computational inefﬁciency of large batch sizes for stochastic gradient descent . arXiv preprint arXiv : 1811 . 12941 , 2018 . I . Goodfellow , Y . Bengio , and A . Courville . Deep Learning . MIT Press , 2016 . http : / / www . deeplearningbook . org . Priya Goyal , Piotr Dollár , Ross Girshick , Pieter Noordhuis , Lukasz Wesolowski , Aapo Kyrola , Andrew Tulloch , Yangqing Jia , and Kaiming He . Accurate , large minibatch SGD : Training Imagenet in 1 hour . arXiv preprint arXiv : 1706 . 02677 , 2017 . Roger Grosse and James Martens . A kronecker - factored approximate ﬁsher matrix for convolution layers . In International Conference on Machine Learning , pages 573 – 582 , 2016 . Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770 – 778 , 2016 . Elad Hoffer , Itay Hubara , and Daniel Soudry . Train longer , generalize better : Closing the generaliza - tion gap in large batch training of neural networks . In Advances in Neural Information Processing Systems , pages 1731 – 1741 , 2017 . Sergey Ioffe and Christian Szegedy . Batch normalization : Accelerating deep network training by reducing internal covariate shift . In International Conference on Machine Learning , pages 448 – 456 , 2015 . 9 Arthur Jacot , Franck Gabriel , and Clément Hongler . Neural tangent kernel : Convergence and generalization in neural networks . In Advances in neural information processing systems , pages 8571 – 8580 , 2018 . Prateek Jain , Sham M Kakade , Rahul Kidambi , Praneeth Netrapalli , and Aaron Sidford . Parallelizing stochastic gradient descent for least squares regression : mini - batching , averaging , and model misspeciﬁcation . Journal of Machine Learning Research , 18 ( 223 ) : 1 – 42 , 2018 . Stanisław Jastrz˛ebski , Zachary Kenton , Devansh Arpit , Nicolas Ballas , Asja Fischer , Yoshua Bengio , and Amos Storkey . Three factors inﬂuencing minima in SGD . In International Conference on Artiﬁcial Neural Networks , 2018 . Rahul Kidambi , Praneeth Netrapalli , Prateek Jain , and Sham Kakade . On the insufﬁciency of existing momentum schemes for stochastic optimization . In 2018 Information Theory and Applications Workshop ( ITA ) , pages 1 – 9 . IEEE , 2018 . Diederik P Kingma and Jimmy Ba . Adam : A method for stochastic optimization . In International Conference on Learning Representations , 2014 . Jaehoon Lee , Lechao Xiao , Samuel S Schoenholz , Yasaman Bahri , Jascha Sohl - Dickstein , and Jeffrey Pennington . Wide neural networks of any depth evolve as linear models under gradient descent . arXiv preprint arXiv : 1902 . 06720 , 2019 . Todd K . Leen and Genevieve B . Orr . Optimal stochastic search and adaptive momentum . In J . D . Cowan , G . Tesauro , and J . Alspector , editors , Advances in Neural Information Processing Systems 6 , pages 477 – 484 . Morgan - Kaufmann , 1994 . URL http : / / papers . nips . cc / paper / 772 - optimal - stochastic - search - and - adaptive - momentum . pdf . Siyuan Ma , Raef Bassily , and Mikhail Belkin . The power of interpolation : Understanding the effectiveness of SGD in modern over - parametrized learning . In International Conference on Machine Learning , pages 3331 – 3340 , 2018 . James Martens . New insights and perspectives on the natural gradient method . arXiv preprint arXiv : 1412 . 1193 , 2014 . James Martens and Roger Grosse . Optimizing neural networks with Kronecker - factored approximate curvature . In International Conference on Machine Learning , pages 2408 – 2417 , 2015 . Sam McCandlish , Jared Kaplan , Dario Amodei , and OpenAI Dota Team . An empirical model of large - batch training . arXiv preprint arXiv : 1812 . 06162 , 2018 . Kazuki Osawa , Yohei Tsuji , Yuichiro Ueno , Akira Naruse , Rio Yokota , and Satoshi Matsuoka . Second - order optimization method for large mini - batch : Training resnet - 50 on imagenet in 35 epochs . arXiv preprint arXiv : 1811 . 12019 , 2018 . Boris T Polyak and Anatoli B Juditsky . Acceleration of stochastic approximation by averaging . SIAM Journal on Control and Optimization , 30 ( 4 ) : 838 – 855 , 1992 . Ning Qian . On the momentum term in gradient descent learning algorithms . Neural networks , 12 ( 1 ) : 145 – 151 , 1999 . Olga Russakovsky , Jia Deng , Hao Su , Jonathan Krause , Sanjeev Satheesh , Sean Ma , Zhiheng Huang , Andrej Karpathy , Aditya Khosla , Michael Bernstein , et al . ImageNet large scale visual recognition challenge . International Journal of Computer Vision , 115 ( 3 ) : 211 – 252 , 2015 . Levent Sagun , Leon Bottou , and Yann LeCun . Eigenvalues of the hessian in deep learning : Singularity and beyond . arXiv preprint arXiv : 1611 . 07476 , 2016 . Tom Schaul , Sixin Zhang , and Yann LeCun . No more pesky learning rates . In International Conference on Machine Learning , pages 343 – 351 , 2013 . Nicol N Schraudolph . Fast curvature matrix - vector products for second - order gradient descent . Neural computation , 14 ( 7 ) : 1723 – 1738 , 2002 . 10 Christopher J Shallue , Jaehoon Lee , Joe Antognini , Jascha Sohl - Dickstein , Roy Frostig , and George E Dahl . Measuring the effects of data parallelism on neural network training . arXiv preprint arXiv : 1811 . 03600 , 2018 . Karen Simonyan and Andrew Zisserman . Very deep convolutional networks for large - scale image recognition . In International Conference on Learning Representations , 2015 . Samuel L . Smith , Pieter - Jan Kindermans , and Quoc V . Le . Don’t decay the learning rate , increase the batch size . In International Conference on Learning Representations , 2018 . URL https : / / openreview . net / forum ? id = B1Yy1BxCZ . Christian Szegedy , Vincent Vanhoucke , Sergey Ioffe , Jon Shlens , and Zbigniew Wojna . Rethinking the inception architecture for computer vision . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 2818 – 2826 , 2016 . Shashanka Ubaru , Jie Chen , and Yousef Saad . Fast estimation of tr ( f ( a ) ) via stochastic lanczos quadrature . SIAM Journal on Matrix Analysis and Applications , 38 ( 4 ) : 1075 – 1099 , 2017 . Twan van Laarhoven . L2 regularization versus batch and weight normalization . arXiv preprint arXiv : 1706 . 05350 , 2017 . Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Łukasz Kaiser , and Illia Polosukhin . Attention is all you need . In Advances in neural information processing systems , pages 5998 – 6008 , 2017 . Chaoqi Wang , Roger Grosse , Sanja Fidler , and Guodong Zhang . Eigendamage : Structured pruning in the Kronecker - factored eigenbasis . In Proceedings of the 36th International Conference on Machine Learning , pages 6566 – 6575 , 2019 . Yuhuai Wu , Mengye Ren , Renjie Liao , and Roger Grosse . Understanding short - horizon bias in stochastic meta - optimization . In International Conference on Learning Representations , 2018 . URL https : / / openreview . net / forum ? id = H1MczcgR - . Lin Yang , Raman Arora , Tuo Zhao , et al . The physical systems behind optimization algorithms . In Advances in Neural Information Processing Systems , pages 4372 – 4381 , 2018 . Dong Yin , Ashwin Pananjady , Max Lam , Dimitris Papailiopoulos , Kannan Ramchandran , and Peter Bartlett . Gradient diversity : a key ingredient for scalable distributed learning . In International Conference on Artiﬁcial Intelligence and Statistics , pages 1998 – 2007 , 2018 . Kun Yuan , Bicheng Ying , and Ali H . Sayed . On the inﬂuence of momentum acceleration on online learning . Journal of Machine Learning Research , 17 ( 192 ) : 1 – 66 , 2016 . URL http : / / jmlr . org / papers / v17 / 16 - 157 . html . Guodong Zhang , James Martens , and Roger Grosse . Fast convergence of natural gradient descent for overparameterized neural networks . arXiv preprint arXiv : 1905 . 10961 , 2019a . Guodong Zhang , Chaoqi Wang , Bowen Xu , and Roger Grosse . Three mechanisms of weight decay regularization . In International Conference on Learning Representations , 2019b . URL https : / / openreview . net / forum ? id = B1lz - 3Rct7 . 11 A Kronecker - factored Approximate Curvature ( K - FAC ) Kronecker - factored approximate curvature ( K - FAC ) [ Martens and Grosse , 2015 ] uses a Kronecker - factored approximation to the curvature matrix to perform efﬁcient approximate natural gradient updates . Considering the l - th layer in a neural network whose input activations are a ∈ R n , weight matrix W ∈ R n × m , and outputs s ∈ R m , we have s = W (cid:62) a . Therefore , the weight gradient is ∇ W L = a ( ∇ s L ) (cid:62) . With this formula , K - FAC decouples this layer’s Fisher matrix F using an independence assumption : F = E [ vec { ∇ W L } vec { ∇ W L } (cid:62) ] = E [ { ∇ s L } { ∇ s L } (cid:62) ⊗ aa (cid:62) ] ≈ E [ { ∇ s L } { ∇ s L } (cid:62) ] ⊗ E [ aa (cid:62) ] = S ⊗ A ( 11 ) where A = E [ aa (cid:62) ] and S = E [ { ∇ s L } { ∇ s L } (cid:62) ] . Decomposing F into A and S not only avoids the quadratic storage cost of the exact Fisher , but also enables tractable computation of the approximate natural gradient : F − 1 vec { ∇ W L } = ( S − 1 ⊗ A − 1 ) vec { ∇ W L } = vec [ A − 1 ∇ W L S − 1 ] ( 12 ) As shown by eqn . ( 12 ) , computing natural gradient using K - FAC only consists of matrix transforma - tions comparable to size of W , making it very efﬁcient . Later , Grosse and Martens [ 2016 ] further extended K - FAC to convolutional layers under additional assumptions of spatial homogeneity ( SH ) and spatially uncorrelated derivatives ( SUD ) . Suppose the input a ∈ R c in × h × w and the output s ∈ R c out × h × w , then the gradient of the reshaped weight W ∈ R c out × c in k 2 is ∇ W L = (cid:80) a i ∇ s i L (cid:62) , and the corresponding Fisher matrix is : F ≈ (cid:88) E (cid:2) { ∇ s i L } { ∇ s i (cid:48) L } (cid:62) (cid:3) ⊗ E (cid:2) a i a (cid:62) i (cid:48) (cid:3) ≈ (cid:18) 1 | I | (cid:88) E (cid:2) { ∇ s i L } { ∇ s i L } (cid:62) (cid:3)(cid:19) (cid:124) (cid:123)(cid:122) (cid:125) S , size = ( c out ) 2 ⊗ (cid:16)(cid:88) E (cid:2) a i a (cid:62) i (cid:3)(cid:17) (cid:124) (cid:123)(cid:122) (cid:125) A , size = ( c in × k 2 ) 2 ( 13 ) where I = [ h ] × [ w ] is the set of spatial locations , a i ∈ R c in k 2 is the patch extracted from a , ∇ s i L ∈ R c out is the gradient to each spatial location in s and i , i (cid:48) ∈ I . A . 1 K - FAC for Transformer K - FAC has been implemented on the autoencoder [ Martens and Grosse , 2015 ] and various convolu - tional networks [ Grosse and Martens , 2016 , Ba et al . , 2017 ] before . To our knowledge , this is the ﬁrst time K - FAC is implemented on the Transformer model . What is different from the previous models is the shared weight matrix between the embedding layer and the pre - softmax linear transfor - mation [ Vaswani et al . , 2017 ] . In particular , the weight matrix is transposed at the pre - softmax layer : s = Wa and ∇ W L = ( ∇ s L ) a (cid:62) . With the same assumptions as the non - transposed case , we get F ≈ E [ aa (cid:62) ⊗ { ∇ s L } { ∇ s L } (cid:62) ] = A ⊗ S ( 14 ) i . e . the positions of the two Kronecker factors are swapped . If we name the two Kronecker factors " input factor " and " output factor " respectively , i . e . F ≈ input _ factor ⊗ output _ factor , then for the weight matrix that is shared between the embedding layer and the pre - softmax layer , the input _ factor has contributions from both the embedding inputs and the gradients of pre - softmax layer outputs ; and the output _ factor has contributions from both the pre - softmax layer inputs and the gradients of the embedding outputs . In practice , when computing a Kronecker factor , we treat contribution from multiple sources as an equivalent situation as contribution from multiple training examples from a mini - batch . Also note that because of the high dimensionality of the embedding weight matrix ( with a vocabulary size of 32 , 768 ) , the dense input factor would have size [ 32768 , 32768 ] . In order to save memory , we use a diagonal matrix to estimate the input _ factor . The output _ factor is still estimated with a dense matrix . 12 B Dynamics of momentum SGD on noisy quadratic model Similar to plain SGD , by treating θ i as a random variable , we can explicitly write down the dynamics of its expectation and variance . But due to the use of momentum , we need to take into account m i and its correlation with θ i . Because each dimension evolves independently , we drop the the dimension subscripts . We ﬁrst calculate the expectation of the parameter and velocity : E [ θ ( t + 1 ) ] = ( 1 − αh ) E [ θ ( t ) ] − αβ E [ m ( t ) ] E [ m ( t + 1 ) ] = β E [ m ( t ) ] + h E [ θ ( t ) ] ( 15 ) We then calculate the variance : V [ θ ( t + 1 ) ] = ( 1 − αh ) 2 V [ θ ( t ) ] + ( αβ ) 2 E [ m ( t ) ] − 2 ( 1 − αh ) αβ Cov ( t ) + α 2 c B V [ m ( t + 1 ) ] = β 2 V [ m ( t ) ] + h 2 V [ θ ( t ) ] + 2 βh Cov ( t ) + c B ( 16 ) where Cov ( t ) = Cov ( θ ( t ) , m ( t ) ) evolves as Cov ( t + 1 ) = ( 1 − αh ) h V [ θ ( t ) ] − αβ 2 V [ m ( t ) ] + ( 1 − 2 αh ) β Cov ( t ) − αc B ( 17 ) Because the expected risk is totally decided by E [ θ ] 2 + V [ θ ] , we deﬁne A ( · ) = E [ · ] 2 + V [ · ] and C ( t ) = E [ θ ( t ) ] E [ m ( t ) ] + Cov ( θ ( t ) , m ( t ) ) . We can then simplify the dynamics as follows A ( θ ( t + 1 ) ) = ( 1 − αh ) 2 A ( θ ( t ) ) + ( αβ ) 2 A ( m ( t ) ) − 2 ( 1 − αh ) αβC ( t ) + α 2 c B A ( m ( t + 1 ) ) = β 2 A ( m ( t ) ) + h 2 A ( θ ( t ) ) + 2 βhC ( t ) + c B C ( t + 1 ) = ( 1 − αh ) hA ( θ ( t ) ) − αβ 2 A ( m ( t ) ) + ( 1 − 2 αh ) βC ( t ) − αc B ( 18 ) or equivalently   A ( θ ( t + 1 ) ) α 2 A ( m ( t + 1 ) ) − αC ( t + 1 )   =   ( 1 − αh ) 2 β 2 2 ( 1 − αh ) β ( αh ) 2 β 2 − 2 βαh − ( 1 − αh ) αh β 2 ( 1 − 2 αh ) β   (cid:124) (cid:123)(cid:122) (cid:125) transition matrix T   A ( θ ( t ) ) α 2 A ( m ( t ) ) − αC ( t )   +   α 2 cBα 2 cBα 2 cB   ( 19 ) The convergence rate is determined by the transition matrix T which has the characteristic polynomial | T − λ I | = − ( λ − β ) ( λ 2 − ( β 2 − 2 αhβ + ( 1 − αh ) 2 ) λ + β 2 ) ( 20 ) With the momentum value β = ( 1 − √ αh ) 2 , all eigenvalues of the transition matrix are equal to each other with the value β , giving the fastest convergence . C Proof of Theorem 1 To analyze the dynamics , we can perform a change of basis so that three different dimensions evolve independently . To achieve that , we ﬁrst take the eigendecomposition 5 of the transition matrix T = QDQ − 1 . Then the dynamics can be reformulated as follows : (cid:34) A ( θ ( t ) ) · · (cid:35) = QD t Q − 1 (cid:34) A ( θ ( 0 ) ) 0 0 (cid:35) (cid:124) (cid:123)(cid:122) (cid:125) deterministic term + Q ( I − D ) − 1 Q − 1   α 2 cBα 2 cB α 2 cB   (cid:124) (cid:123)(cid:122) (cid:125) stochastic term − QD t ( I − D ) − 1 Q − 1   α 2 cBα 2 cB α 2 cB   (cid:124) (cid:123)(cid:122) (cid:125) ≥ 0 ( 21 ) 5 Note that we implicitly assume β (cid:54) = ( 1 − √ αh ) 2 , otherwise the transition matrix T is not diagonalizable . 13 We ﬁrst analyze the stochastic term alone . By the identity ( I − D ) − 1 = (cid:80) ∞ p = 0 D p , we have Q ( I − D ) − 1 Q − 1   α 2 cBα 2 cBα 2 cB   = Q ∞ (cid:88) p = 0 D p Q − 1   α 2 cBα 2 cBα 2 cB   = ∞ (cid:88) p = 1 T p − 1   α 2 cBα 2 cBα 2 cB   (cid:44) ∞ (cid:88) p = 0 (cid:34) x p y p z p (cid:35) ( 22 ) In eqn . ( 22 ) , we append zero vector [ x 0 , y 0 , z 0 ] (cid:62) for convenience . To compute the inﬁnite sum , we ﬁrst focus on a single term . We have the following update : √ x p + 1 = ( 1 − αh ) √ x p + β √ y p √ y p + 1 = − αh √ x p + β √ y p ( 23 ) Since we only care x p which totally decide the loss , so we get rid of y p by merging two updates , which yields a second - order difference equation : √ x p + 1 = ( 1 − αh + β ) √ x p − β √ x p − 1 ( 24 ) with initial conditions √ x 0 = 0 and √ x 1 = (cid:113) α 2 cB . To solve the second - order difference equation , we leverage the Z - transform to get the analytical form . Based on basic manipulation of the Z - transform , we have the Z - domain function X ( Z ) = (cid:113) α 2 cB Z Z 2 − ( 1 − αh + β ) Z + β = (cid:113) α 2 cB r 1 − r 2 (cid:18) 1 1 − Z − 1 r 1 − 1 1 − Z − 1 r 2 (cid:19) ( 25 ) where r 1 and r 2 are two roots of equation z 2 − ( 1 − αh + β ) z + β . Then , we use the inverse Z - transform to get √ x p : √ x p = (cid:114) α 2 c B r p 1 − r p 2 r 1 − r 2 ( 26 ) and therefore x p = α 2 c B r 2 p 1 + r 2 p 2 − 2 ( r 1 r 2 ) p ( r 1 − r 2 ) 2 ( 27 ) Now , we are ready to compute the inﬁnite sum (cid:80) ∞ p = 0 x p : ∞ (cid:88) p = 0 x p = α 2 cB ( r 1 − r 2 ) 2 (cid:18) 1 1 − r 21 + 1 1 − r 22 − 2 1 − r 1 r 2 (cid:19) = α 2 c B 1 + r 1 r 2 ( 1 − r 21 ) ( 1 − r 22 ) ( 1 − r 1 r 2 ) ( 28 ) Because r 1 and r 2 are two roots with r 1 r 2 = β , r 1 + r 2 = 1 − αh + β , we have ∞ (cid:88) p = 0 x p = αc ( 1 + β ) Bh ( 2 β + 2 − αh ) ( 1 − β ) ( 29 ) Now , we analyze the deterministic term . We have QD t Q − 1 (cid:34) A ( θ ( 0 ) ) 00 (cid:35) = T t (cid:34) A ( θ ( 0 ) ) 00 (cid:35) ( 30 ) Similar to the analysis of stochastic term , we have the same second - order difference equation (cid:113) x (cid:48) p + 1 = ( 1 − αh + β ) (cid:113) x (cid:48) p − β (cid:113) x (cid:48) p − 1 ( 31 ) except the initial conditions become (cid:112) x (cid:48) 0 = (cid:112) x (cid:48) 1 = (cid:112) A ( θ ( 0 ) ) . According to Z - transform , we have x (cid:48) t = (cid:18) r t + 1 1 − r t + 1 2 − β ( r t 1 − r t 2 ) r 1 − r 2 (cid:19) 2 A ( θ ( 0 ) ) ( 32 ) Along with eqn . ( 29 ) , we have A ( θ ( t ) ) ≤ (cid:18) r t + 1 1 − r t + 1 2 − β ( r t 1 − r t 2 ) r 1 − r 2 (cid:19) 2 A ( θ ( 0 ) ) + αc ( 1 + β ) Bh ( 2 β + 2 − αh ) ( 1 − β ) ( 33 ) 14 D More results on the NQM D . 1 Eigenspectra of Neural Networks 0 1e4 2e4 3e4 4e4 5e4 6e4 7e4 8e4 Index 10 - 10 10 - 9 10 - 8 10 - 7 10 - 6 10 - 5 10 - 4 10 - 3 10 - 2 10 - 1 10 0 10 1 10 2 E i g e n v a l u e i n l o g - s c a l e iter 100 iter 200 iter 400 iter 600 iter 1000 Figure 7 : Eigenspectra of the K - FAC approx - imate Fisher matrix of ResNet8 at different training iterations . The model is trained on CIFAR - 10 with batch size 3000 . The main objective of this section is to examine the loss surface of modern neural networks in different stages of training in order to justify the assumptions made in NQM . Nevertheless , it is hard to visualize such a high dimen - sional space . Following recent work [ Sagun et al . , 2016 , Ghorbani et al . , 2019 ] , we instead focus on analyzing the eigenspectrum of the Hessian / Fisher matrices . The Hessian / Fisher of the training loss ( with respect to the parameters ) is crucial in determining many behaviors of neural networks . The eigenvalues of the Hessian / Fisher characterize the local curvature of the loss surface which determines many training behaviors , including ﬁrst - order methods optimization rates ( at least for convex problems . ) To construct the eigenspectrum of the true Fisher matrix , we ﬁrst leverage the Kronecker - factored approximation of the Fisher to get an estimation of the eigenspectrum , which may shed light upon the true eigenspectrum . Speciﬁcally , we train the network with K - FAC and then perform eigen - decomposition on saved Kronecker factors of the Fisher to calculate the eigenvalues . The eigenspectra are plotted in Figure 7 . One interesting observation is that there are only a few large eigenvalues and a few small eigenvalues in the approximate Fisher matrices ; the bulk of eigenvalues are in the middle of the spectrum . We also note that after 200 iterations of training the eigenspectrum remains mostly unchanged . 10 - 20 10 - 15 10 - 10 10 - 5 10 0 10 5 Gradient variance 10 - 20 10 - 15 10 - 10 10 - 5 10 0 10 5 C u r v a t u r e Step 100 10 - 20 10 - 15 10 - 10 10 - 5 10 0 10 5 Gradient variance Step 500 10 - 20 10 - 15 10 - 10 10 - 5 10 0 10 5 Gradient variance Step 1000 ( a ) ResNet8 10 - 20 10 - 15 10 - 10 10 - 5 10 0 10 5 Gradient variance 10 - 20 10 - 15 10 - 10 10 - 5 10 0 10 5 C u r v a t u r e Step 500 10 - 20 10 - 15 10 - 10 10 - 5 10 0 10 5 Gradient variance Step 2000 10 - 20 10 - 15 10 - 10 10 - 5 10 0 10 5 Gradient variance Step 10000 ( b ) Transformer Figure 8 : Scatter plots of second moment v . s . variance of gradients . The gradients are projected onto the Kronecker - factored eigenbasis , which approximates the eigenbasis of the true Fisher . Each point compares the gradient variance and the second moment of the gradient in the direction of an eigenvector of the K - FAC approximated Fisher . D . 2 Gradient Covariance in the Kronecker - Factored Eigenbasis To verify the assumption in Section 3 . 4 that H and C are codiagonalizable , we test it on practical neural networks by comparing the gradient variance to the curvature . This assumption is motivated by theoretical considerations that suggest H ≈ C for neural network training [ Martens , 2014 ] . Ideally , we would like to compare the gradient variance and the curvature of the Fisher in the directions of the eigenvectors of the true Fisher . However , it is typically infeasible to get all these eigenvectors , especially for low curvature directions . To resolve this we instead use the Kronecker - factored 15 eigenbasis [ George et al . , 2018 , Bae et al . , 2018 , Wang et al . , 2019 ] , which is obtained from the K - FAC approximation . For this experiment , we are not relying on this basis being an accurate approximation to the eigendecomposition of the true Fisher ; rather , we use the eigenbasis only as a way to obtain a diverse set of directions with both high and low curvature . For a given eigenvector v , we project the gradients g of each training example onto v and compute the gradient variance Cov ( v (cid:62) g ) , as well as the curvature v (cid:62) Fv . ( The latter quantity can be obtained using matrix - vector products [ Schraudolph , 2002 ] . ) As shown in Figure 8 , the gradient variances closely match the curvature ( especially for the ResNet8 model on CIFAR10 ) , validating our assumption that H ≈ C . D . 3 Plots for the Evolution of the First Term in Eqn . ( 6 ) In Section 3 . 2 , we claim that the convergence of momentum SGD for a single dimension is very close to that of plain SGD with an adjusted learning rate ( note that we already veriﬁed that the steady state risk of momentum SGD matches plain SGD using effective learning rate in Figure 2 ) . Here we verify this argument by comparing them in the NQM . The total risk consists of two terms ( eqn . ( 6 ) ) : the ﬁrst term determines convergence , while the second term ( steady state risk ) stays constant throughout training . Given that the second stays unchanged , we only plot the ﬁrst term of eqn . ( 6 ) in Figure 9 . Note that the values are normalized in the ﬁgures . We observe that the convergence dynamics of the two update rules closely match each other . For this experiment we set αh = 0 . 0005 , but the results are not sensitive to this value . 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 N o r m a li z e d f i r s t t e r m Step : 50 true dynamics dynamics with effective learning rate Step : 100 Step : 200 Step : 500 2 - 5 2 - 4 2 - 3 2 - 2 2 - 1 2 0 1 ¡ ¯ 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 N o r m a li z e d f i r s t t e r m Step : 1000 2 - 5 2 - 4 2 - 3 2 - 2 2 - 1 2 0 1 ¡ ¯ Step : 2000 2 - 5 2 - 4 2 - 3 2 - 2 2 - 1 2 0 1 ¡ ¯ Step : 4000 2 - 5 2 - 4 2 - 3 2 - 2 2 - 1 2 0 1 ¡ ¯ Step : 5000 Figure 9 : Comparison in convergence between momentum SGD and SGD with adjusted learning rate . This plot shows values for the ﬁrst term in eqn . ( 6 ) as a function of ( 1 − β ) , which is the scaling between the “effective learning rate” and the true learning rate for momentum SGD . The red curves show the ﬁrst term when using momentum , while the blue curves show the ﬁrst term when using plain SGD with the learning rate set to the effective learning rate of momentum . D . 4 Veriﬁcation of Eigenspectrum In Section 3 . 6 , we assume the diagonal entries of H are { 1 i } di = 1 . To justify this choice , we compare the K - FAC eigenspectra of ResNet8 to this distribution in Figure 10 . The distribution of eigenvalues we chose for H in the NQM very closely matches the eigenspectra of the real neural network , validating the assumption that the diagonal entries of H are { 1 i } di = 1 in Section 3 . 4 . 10 0 10 1 10 2 10 3 10 4 10 5 Log of Index 10 - 4 10 - 3 10 - 2 10 - 1 10 0 10 1 10 2 E i g e n v a l u e i n l o g - s c a l e Step 100 K - FAC Eigenspectrum Eigenspectrum in NQM 1 = i 10 0 10 1 10 2 10 3 10 4 10 5 Log of Index Step 500 10 0 10 1 10 2 10 3 10 4 10 5 Log of Index Step 1000 Figure 10 : Comparison between K - FAC Fisher eigenspectra and the 1 i distribution used in the NQM . D . 5 Effect of Loss Threshold 16 2 1 2 3 2 5 2 7 2 9 2 11 2 13 2 15 2 17 2 19 2 21 Batch size 2 11 2 12 2 13 2 14 2 15 2 16 2 17 2 18 2 19 2 20 2 21 2 22 2 23 2 24 2 25 S t e p s t o t h r e s h o l d 0 . 10 . 030 . 010 . 0030 . 001 Figure 11 : Number of training steps re - quired to reach a target loss as a function of batch size for different loss threshold values . Recall that a main objective of this work is to characterize the effects of increasing the batch size on training time , as measured in the number of steps necessary to reach a goal target error / loss . Here we experiment with different loss thresholds to study the relationship between batch size and number of training steps . To obtain the minimal training steps for a given batch size , we do grid search over constant learning rates . Figure 11 shows that increasing the batch size initially decreases the required number of training steps proportionally , but eventually there are diminishing returns , which matches the empirical ﬁndings [ Golmant et al . , 2018 , Shallue et al . , 2018 ] . The shape of the curves is characteris - tically the same for different loss thresholds , though the critical batch size seems to increase for more difﬁcult thresholds . D . 6 Results of Optimal Learning Rate on NQM 2 1 2 3 2 5 2 7 2 9 2 11 2 13 2 15 2 17 2 19 2 21 2 23 Batch size 2 - 14 2 - 12 2 - 10 2 - 8 2 - 6 2 - 4 2 - 2 2 0 2 2 2 4 2 6 O p t i m a l L e a r n i n g R a t e ( a ) Without Momentum 2 1 2 3 2 5 2 7 2 9 2 11 2 13 2 15 2 17 2 19 2 21 Batch size 2 - 14 2 - 12 2 - 10 2 - 8 2 - 6 2 - 4 2 - 2 2 0 2 2 2 4 2 6 O p t i m a l l e a r n i n g r a t e ( b ) Fixed Momentum 0 . 9 2 1 2 3 2 5 2 7 2 9 2 11 2 13 2 15 2 17 2 19 2 21 Batch size 2 - 14 2 - 12 2 - 10 2 - 8 2 - 6 2 - 4 2 - 2 2 0 2 2 2 4 2 6 E ff e c t i v e l e a r n i n g r a t e pow 0 pow 0 . 25 pow 0 . 5 pow 0 . 75 ( c ) Tuned Momentum Figure 12 : Optimal learning rate v . s . batch size for different preconditioning powers . ( a ) When momentum is not used , the learning rate increases with batch size until it is limited by the maximum stable learning rate . Larger preconditioning powers reduce the optimal learning rate for the same batch size , thus extending the batch size where the learning rate levels off . ( b , c ) Fixed ( 0 . 9 ) and tuned momentum values . In ( b ) and ( c ) , we plot the effective learning rate for momentum , deﬁned as α 1 − β . The dashed lines are the same plots from ( a ) for easier comparison . D . 7 Final Learning Rate of Different Batch Sizes for PWC Learning Rate Scheme 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 Batch size 2 - 8 2 - 7 2 - 6 2 - 5 2 - 4 2 - 3 2 - 2 2 - 1 2 0 2 1 F i n a l L e a r n i n g r a t e Figure 13 : Final learning rate of the piecewise - constant learning rate scheme v . s . batch size . In Section 3 . 6 . 2 , we study the piecewise constant learning rate scheme . The optimal scheme starts with a high learning rate which drops later in training ( Figure 3c ) . Recall that for ﬁxed learning rates , we observed that the optimal learning rate scaled linearly with the batch size for small batch sizes , but it is unclear whether there is a similar phenomenon for learning rate decay . In Figure 13 , we plot the ﬁnal learning rate as a function of batch size and show that it also scales linearly with batch size . E More Details for Experiments E . 1 Data Sets The data sets in Table 1 ( MNIST , Fashion MNIST , CIFAR10 , ImageNet and LM1B ) are identical to those of Shallue et al . [ 2018 ] ( described in their Appendix A . 1 ) . For CIFAR10 we used data augmentation ( including horizontal ﬂip and random crop ) , but they did not . E . 2 Model Details This section provides details of models in Table 1 . The models are very similar ( and some identical ) to those used in Shallue et al . [ 2018 ] ( described in their Appendix B ) . Any modiﬁcations from them are highlighted in this section . 17 Simple CNN consists of 2 convolutional layers with max - pooling followed by 1 fully connected hid - den layer . The convolutional layers use 5×5 ﬁlters with stride length 1 , “same” padding [ Goodfellow et al . , 2016 ] , and ReLU activation function . Max pooling uses 2×2 windows with stride length 2 . Unlike in Shallue et al . [ 2018 ] , we did not use any dropout regularization ( while they used dropout with probability 0 . 4 in the fully connected layer ) . We used 32 and 64 ﬁlters in the convolutional layers and 1 , 024 units in the fully connected layer . This corresponds to the “base” conﬁguration in Shallue et al . [ 2018 ] . ResNet8 [ He et al . , 2016 ] consists of 7 convolutional layers with residual connections followed by 1 fully connected hidden layer . We used the identical architecture as Shallue et al . [ 2018 ] . In particular , we did not use batch normalization . The only difference is that we used data augmentation in our experiments . ResNet32 [ He et al . , 2016 ] consists of 31 convolutional layers with residual connections followed by 1 fully connected hidden layer ( see Section 4 . 2 of He et al . [ 2016 ] ) . We replaced batch nor - malization [ Ioffe and Szegedy , 2015 ] with ghost batch normalization to keep the training objective ﬁxed between batch sizes and to avoid possible negative effects from computing batch normalization statistics over a large number of examples [ Hoffer et al . , 2017 ] . We used a ghost batch size of 32 for all experiments . We also applied label smoothing [ Szegedy et al . , 2016 ] to regularize the model at training time , which was helpful for larger batch sizes . We set the label smoothing parameter to 0 . 1 in all experiments . Instead of using weight decay , we applied channel - wise weight normalization by constraining the Frobenius norm of each convolutional channel to be exactly 1 , which controls the effective learning rate [ Zhang et al . , 2019b , van Laarhoven , 2017 ] . VGG11 [ Simonyan and Zisserman , 2015 ] consists of 8 convolutional layers followed by 1 fully connected hidden layers . as in ResNet32 , we used Ghost batch normalization , label smoothing , and channel - wise weight normalization . Transformer Vaswani et al . [ 2017 ] is a self - attention model . We chose the Transformer model identical to the “base” model described in Vaswani et al . [ 2017 ] , except with only two hidden layers instead of six . This is identical to the “Transformer Shallow” model in Shallue et al . [ 2018 ] . E . 3 Learning Rate Schedules This section describes two learning rate schedules mentioned in Table 1 : constant schedule and linear decay schedule . Constant schedule simply keeps a ﬁxed learning rate throughout training : α ( t ) = α 0 , where t is the training step index . Linear decay schedule is α ( t ) = α 0 − ( 1 − γ ) t T , where α 0 is the initial learning rate , γ is the rate of decay , and T is the number of steps taken to reach the ﬁnal learning rate . Shallue et al . [ 2018 ] experimented with various learning rate schedules and found that linear decay matched performance of the other schedules with fewer hyperparameters to tune . Therefore , we also chose the linear decay schedule , for which we tuned α 0 , γ and T . E . 4 Optimizer - Speciﬁc Hyperparamters For momentum SGD , we tuned the momentum β . For Adam , we tuned β 1 , β 2 , and (cid:15) ( see Kingma and Ba [ 2014 ] ) . For K - FAC , we tuned damping and the trust region constraint ( also known as the KL clipping term ) for Transformer , keeping momentum = 0 . 9 and the moving average parameter for damping = 0 . 99 ; for all other models , we tuned all four parameters ( see Martens and Grosse [ 2015 ] ) . 18