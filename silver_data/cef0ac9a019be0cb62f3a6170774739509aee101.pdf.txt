Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals Cecilia Ferrando , Shufan Wang , Daniel Sheldon College of Information and Computer Sciences University of Massachusetts Amherst { cferrando } , { shufanwang } , { sheldon } @ cs . umass . edu Abstract The goal of this paper is to develop a practi - cal and general - purpose approach to construct conﬁdence intervals for diﬀerentially private parametric estimation . We ﬁnd that the para - metric bootstrap is a simple and eﬀective so - lution . It cleanly reasons about variability of both the data sample and the randomized privacy mechanism and applies “out of the box” to a wide class of private estimation rou - tines . It can also help correct bias caused by clipping data to limit sensitivity . We prove that the parametric bootstrap gives consistent conﬁdence intervals in two broadly relevant settings , including a novel adaptation to linear regression that avoids accessing the covariate data multiple times . We demonstrate its eﬀec - tiveness for a variety of estimators , and ﬁnd that it provides conﬁdence intervals with good coverage even at modest sample sizes and per - forms better than alternative approaches . 1 Introduction Diﬀerential privacy provides a rubric for drawing infer - ences from data sets without compromising the privacy of individuals . This paper is about privately constructing conﬁdence in - tervals . In the non - private case , approximate methods based on asymptotic normality or the bootstrap ( Efron , 1979 ) apply to a wide range of models and are very widely used in practice . In the private case , such “swiss army knife” methods are hard to ﬁnd . The situation is complicated by the fact that private estimation pro - cedures are necessarily randomized , which leads to a Preprint . Under review . distinct source of randomness ( “privacy noise” ) in ad - dition to the usual random draw of a ﬁnite sample from a population ( “sampling noise” ) . We ﬁnd experi - mentally that asymptotic methods are signiﬁcantly less eﬀective in private settings , due to privacy noise that becomes negligible only for for very large sample sizes ( Section 7 ) . Bootstrap approaches face the challenge of incurring privacy costs by accessing the data many times ( Brawner and Honaker , 2018 ) . This paper advocates using the parametric bootstrap as a simple and eﬀective method to construct conﬁdence intervals for private statistical estimation . The para - metric bootstrap resamples data sets from an estimated parametric model to approximate the distribution of the estimator . It is algorithmically simple , can be used with essentially any private estimator , and cleanly reasons about both sampling noise and privacy noise . Unlike the traditional bootstrap , it is based on post - processing and avoids accessing the data many times , so it often has little or no privacy burden . By reason - ing about the distribution of a ﬁnite sample , it makes fewer assumptions than purely asymptotic methods and signiﬁcantly mitigates the problem of non - negligible privacy noise . The parametric bootstrap can also help correct bias in private estimation caused by artiﬁcially bounding data to limit sensitivity . We ﬁrst introduce the parametric bootstrap and dis - cuss its application to private estimation , including methods to construct conﬁdence intervals and correct bias . We then review parametric bootstrap theory , and apply the parametric bootstrap to obtain provably consistent conﬁdence intervals in two private estima - tion settings—exponential families and linear regression suﬃcient statistic perturbation ( SSP ) —as well as an empirical demonstration for the “one posterior sample” ( OPS ) method ( Wang et al . , 2015 ; Foulds et al . , 2016 ; Zhang et al . , 2016 ) . These demonstrate the broad appli - cability the parametric bootstrap to private estimation . One limitation of the parametric bootstrap is the re - striction to fully parametric estimation . For example , it doesn’t apply directly to regression problems that do a r X i v : 2006 . 07749v2 [ c s . L G ] 12 O c t 2021 Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals not have a parametric model for covariates , and may not be appropriate very complex data . In our linear regression application , we contribute a novel hybrid bootstrap approach to circumvent this limitation ; the resulting method is easy to use and simultaneously esti - mates regression coeﬃcients and constructs conﬁdence intervals with good coverage properties . A second limi - tation is computational cost , which scales with the data size . For small or medium data sets , the cost is likely manageable . For very large ones , cheap asymptotic methods will often be adequate ( see Section 7 ; for ex - ponential families and linear regression with suﬃcient statistic perturbation , the asymptotic distributions are a relatively simple byproduct of our bootstrap theory ) . However , it is unknown in general how large data must be for asymptotic methods to perform well . 2 Background Diﬀerential privacy is a formal deﬁnition to capture the notion that , to maintain privacy , the output of an algorithm should remain nearly unchanged if the data of one individual changes . Say that two data sets X and X (cid:48) of size n are neighbors if they diﬀer in exactly one data record . Deﬁnition 1 ( Diﬀerential privacy , Dwork et al . 2006 ) . A randomized algorithm A satisﬁes (cid:15) - diﬀerential pri - vacy ( (cid:15) - DP ) if , for neighboring data sets X and X (cid:48) , and any subset O ⊆ Range ( A ) , Pr [ A ( X ) ∈ O ] ≤ exp ( (cid:15) ) Pr [ A ( X (cid:48) ) ∈ O ] . One common way to achieve diﬀerential privacy is by injecting calibrated noise onto the statistics computed from the data . Let f be any function that maps data sets to R d . The magnitude of noise required to privatize the computation of f depends on its sensitivity . Deﬁnition 2 ( Sensitivity , Dwork et al . 2006 ) . The sensitivity of a function f is ∆ f = max X , X (cid:48) (cid:107) f ( X ) − f ( X (cid:48) ) (cid:107) 1 where X , X (cid:48) are any two neighboring data sets . When f is additive , it is straightforward to bound its sensitivity ( proof in Appendix A ) : Claim 1 . Suppose X = ( x 1 , . . . , x n ) and f ( X ) = (cid:80) ni = 1 g ( x i ) where g maps data points to R m . Let width ( g j ) = max x g j ( x ) − min x g j ( x ) where x ranges over the data domain . Then ∆ f ≤ (cid:80) mj = 1 width ( g j ) , which is a constant independent of n . Many algorithms satisfy diﬀerential privacy by using the Laplace mechanism . Deﬁnition 3 ( Laplace mechanism , Dwork et al . 2006 ) . Given a function f that maps data sets to R m , the Laplace mechanism outputs the ran - dom variable L ( X ) ∼ Lap ( f ( X ) , ∆ f / (cid:15) ) from the Laplace distribution , which has density Lap ( z ; u , b ) = ( 2 b ) − m exp ( − (cid:107) z − u (cid:107) 1 / b ) . This corresponds to adding zero - mean independent noise u i ∼ Lap ( 0 , ∆ f / (cid:15) ) to each component of f ( X ) . 3 Parametric Bootstrap Algorithm 1 Parametric Bootstrap 1 : Input : x 1 : n , B , estimator A 2 : ˆ θ , ˆ τ ← A ( x 1 : n ) 3 : for b from 1 to B do 4 : x ∗ 1 , . . . , x ∗ n ∼ P ˆ θ 5 : ˆ θ ∗ b , ˆ τ ∗ b ← A ( x ∗ 1 : n ) 6 : return ˆ τ , (cid:0) ˆ τ ∗ 1 , . . . , ˆ τ ∗ B (cid:1) We consider the standard setup of para - metric statis - tical inference , where a data sample x 1 : n = ( x 1 , . . . , x n ) is observed and each x i is assumed to be drawn independently from a distribution P θ in the family { P θ : θ ∈ Θ } with unknown θ . The goal is to estimate some population parameter τ = τ ( θ ) , the estimation target , via an estimator ˆ τ = ˆ τ ( x 1 : n ) . 1 We also seek a 1 − α conﬁdence interval for τ , that is , an interval [ ˆ a n , ˆ b n ] such that P θ ( ˆ a n ≤ τ ≤ ˆ b n ) ≈ 1 − α , where P θ is the probability measure over the full sample when the true parameter is θ . We will require ˆ τ and [ ˆ a , ˆ b ] to be diﬀerentially private . Our primary focus is not designing private estimators ˆ τ , but designing methods to construct private conﬁdence intervals [ ˆ a , ˆ b ] that can be used for many estimators and have little additional privacy burden . The parametric bootstrap is a simple way to approx - imate the distribution of ˆ τ for conﬁdence intervals and other purposes . It is a variant of Efron’s boot - strap ( Efron , 1979 , 1981a , b ; Efron and Tibshirani , 1986 ) , which runs an estimator many times on sim - ulated data sets whose distribution approximates the original data . In the parametric bootstrap , data sets are simulated from P ˆ θ , the parametric distribution with estimated parameter ˆ θ . 2 The procedure is shown in Algorithm 1 , where A is an algorithm that computes the estimates ˆ θ and ˆ τ from the data . A simple case is when ˆ τ ( x 1 : n ) = τ (cid:0) ˆ θ ( x 1 : n ) (cid:1) but in general these may be estimated separately . The parametric bootstrap is highly compatible with dif - ferential privacy . The data is only accessed in Line 2 , so 1 We use a hat on variables that are functions of the data and therefore random . 2 In the non - parametric bootstrap , data sets are simu - lated from the empirical distribution of x 1 : n . Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals the only requirement is that A be diﬀerentially private ( which necessitates it is randomized ) . The remaining steps are post - processing and incur no additional pri - vacy cost . The simulation cleanly handles reasoning about both data variability ( Line 4 ) and randomness in the estimator ( Line 5 ) . When the estimation target is θ , we have ˆ τ = ˆ θ , and the procedure incurs no privacy cost beyond that of the private estimator ˆ τ . In other cases , additional privacy budget is required to estimate the full vector θ including nuisance parameters ; an example is estimating the mean of a Gaussian with unknown variance ( Du et al . , 2020 ) . 3 . 1 Conﬁdence Intervals and Bias Correction Table 1 : Bootstrap conﬁdence intervals . ˆ ξ γ is the 1 − γ quantile of ˆ τ ∗ − ˆ τ or ( ˆ τ ∗ − ˆ τ ) / ˆ σ ∗ , and ˆ ζ γ is the 1 − γ quantile of ˆ τ ∗ . Interval Target Target interval τ interval Pivotal ˆ τ − τ [ ˆ ξ 1 − α 2 , ˆ ξ α 2 ] [ ˆ τ − ˆ ξ α 2 , ˆ τ − ˆ ξ 1 − α 2 ] Studentized pivotal ( ˆ τ − τ ) ˆ σ [ ˆ ξ 1 − α 2 , ˆ ξ α 2 ] [ ˆ τ − ˆ ξ α 2 ˆ σ , ˆ τ − ˆ ξ 1 − α 2 ˆ σ ] Efron’s percentile ˆ τ [ ˆ ζ 1 − α 2 , ˆ ζ α 2 ] [ ˆ ζ 1 − α 2 , ˆ ζ α 2 ] There are several well known methods to compute conﬁdence intervals from bootstrap replicates . Three are listed in Table 1 ; note that names are inconsistent in the literature . 3 The general principle is to treat the pair ( ˆ τ ∗ , ˆ τ ) analogously to ( ˆ τ , τ ) to approximate the distribution of the latter . The intervals diﬀer according to what function of ( ˆ τ , τ ) they target . A simple example is to approximate the “pivot” ˆ τ − τ by ˆ τ ∗ − ˆ τ , which leads to the pivotal interval . To construct it , we estimate the 1 − γ quantile of ˆ τ ∗ − ˆ τ as the 1 − γ quantile of the bootstrap replicates ( ˆ τ ∗ 1 − ˆ τ , . . . , ˆ τ ∗ B − ˆ τ ) . The number of replicates controls the error introduced by this step . This error is usually ignored theoretically because , in principle , it can be reduced arbitrarily with enough computation , and it can be controlled well in practice . The studentized pivotal interval targets ( ˆ τ − τ ) / ˆ σ instead , where ˆ σ is a standard error estimate of the main procedure ; it can converge faster than the pivotal interval ( Wasserman , 2006 ) . Efron’s percentile interval targets ˆ τ directly and , while simple , its logic is less obvious ; it can also be viewed as targeting the 3 Our mathematical presentation follows Van der Vaart ( 2000 ) , but names follow Wasserman ( 2006 ) . The names “pivotal” and “studentized pivotal” and are descriptive and avoid the confusion of “percentile interval” sometimes re - ferring to the pivotal interval and other times to Efron’s percentile interval . The possessive " Efron’s " ( Van der Vaart , 2000 ) clariﬁes that we use Efron’s deﬁnition of “percentile interval” ( e . g . , Efron and Hastie , 2016 ) . pivot ˆ τ − τ with a “reversed” interval , which is how theoretical properties are shown . By approximating ˆ τ − τ by ˆ τ ∗ − ˆ τ , we can also estimate the bias of ˆ τ . This leads to a simple bias corrected estimator ˆ τ bc : (cid:100) bias = E [ ˆ τ ∗ − ˆ τ ] , ˆ τ bc ← ˆ τ − (cid:100) bias . Similar to the quantiles above , E [ ˆ τ ∗ − ˆ τ ] is estimated as the sample mean over bootstrap replicates . 3 . 2 Signiﬁcance and Connection to Other Resampling Methods for Private Estimation The parametric bootstrap can be applied to any para - metric estimation problem , a wide range of private estimators , is very accurate in practice , and has little or no additional cost in terms of privacy budget or algo - rithm development . These make it an excellent default choice ( to our knowledge , the best ) for constructing pri - vate conﬁdence intervals for any parametric estimation problem with small to medium data sets . That such a simple and eﬀective choice is available is not articulated in the literature . Two prior works use methods that can be viewed as the parametric boot - strap , but do not discuss the classical procedure and its wide ranging applications , or diﬀerent facets of boot - strap methodology such as techniques for constructing conﬁdence intervals and bootstrap theory . Speciﬁcally , the simulation approach of Du et al . ( 2020 ) for Gaussian mean conﬁdence intervals is equivalent to the paramet - ric bootstrap with a non - standard variant of Efron’s percentile intervals , and performed very well empir - ically . In their application to independence testing , Gaboardi et al . ( 2016 ) approximate the distribution of a private test statistic by simulating data from a null model after privately estimating its parameters ; this can be viewed as an application of the parametric bootstrap to the null model . Several other works use resampling techniques that resemble the parametric bootstrap for a similar , but conceptually distinct , purpose ( D’Orazio et al . , 2015 ; Wang et al . , 2019 ; Evans et al . , 2019 ) . A typical setup is when ˆ τ = τ (cid:48) + η , where τ (cid:48) is a non - private estimator and η is noise added for privacy . Standard asymptotics are used to approximate √ n ( τ (cid:48) − τ ) as N ( 0 , ˆ σ ) , where ˆ σ is a ( private ) standard error estimate for τ (cid:48) . For the private estimator , this gives √ n ( ˆ τ − τ ) ≈ N ( 0 , ˆ σ ) + √ nη . Because η has known distribution , Monte Carlo sam - pling can be used to draw samples from N ( 0 , ˆ σ ) + √ nη for computing conﬁdence intervals or standard errors . The key distinction is that standard asymptotics are used to approximate the distribution of τ (cid:48) , which cap - tures all variability due to the data , and sampling is used only to combine that distribution with the pri - vacy noise distribution . In contrast , the key feature Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals of a bootstrap method is that it resamples data sets to reason about estimator variability due the random data , and thereby avoids standard asymptotics . This technique also does not apply when the privacy mecha - nism is more complicated than adding additive noise to a non - private estimate ( cf . the OPS example of Sec . 5 ) . 4 Bootstrap Theory This section gives general results we can use to argue correctness of bootstrap conﬁdence intervals in private settings . We give a general notion of “bootstrap” esti - mator that covers diﬀerent resampling methods . Let ( Ω , F , P θ ) be the probability space for x 1 , x 2 , . . . ∼ P θ and η 1 , η 2 , . . . where , for a given n , the data is x 1 : n and η n captures any other randomness used in the privacy mechanism or estimator ; we refer to this as the “outer” probability space . A bootstrap estimator is deﬁned in terms of a random experiment over an “inner” probabil - ity space conditional on ω ∈ Ω and n . Let P ∗ n ( · | ω ) be a Markov kernel deﬁning this space . The traditional boot - strap uses P ∗ n ( · | ω ) = ˆ P n with ˆ P ( dx ) = 1 n (cid:80) i δ x i ( dx ) ; the parametric bootstrap uses ˆ P = P ˆ θ instead . Our hybrid model in Section 5 uses a custom resampling method , which gives a custom measure P ∗ n ( · | ω ) . For our purposes , a bootstrap estimator of a parameter τ ( θ ) is a random variable ˆ τ ∗ n in the inner probability space that simulates the parameter estimate ˆ τ n of the “main” procedure . Typically , the bootstrap estimator arises from running the main procedure on resampled data . That is , if ˆ τ n = T n ( ω ) , then ˆ τ ∗ = T n ( ω ∗ ) with ω ∗ ∼ P ∗ n ( · | ω ) . Our hybrid OLS bootstrap will deviate slightly from this pattern . 4 . 1 Consistency Bootstrap “success” has to do with the asymptotic distributions of the ( approximate ) pivot √ n ( ˆ τ n − τ ) and its bootstrapped counterpart √ n ( ˆ τ ∗ n − ˆ τ n ) . For studentized intervals , the pivot ( ˆ τ n − τ ) / ˆ σ n is used instead , where ˆ σ n is a standard error estimate of the main procedure ; theory for this case is a straightforward extension if ˆ σ n → σ ( θ ) in P θ - probability ( Van der Vaart , 2000 ; Beran , 1997 ) . Deﬁnition 4 . The bootstrap estimator ˆ τ ∗ n is consistent if sup x (cid:12)(cid:12) (cid:12) P ∗ n (cid:16) √ n ( ˆ τ ∗ n − ˆ τ n ) ≤ t | ω (cid:17) − − P θ (cid:16) √ n ( ˆ τ n − ˆ τ ) ≤ t (cid:17)(cid:12)(cid:12)(cid:12) P → 0 ( 1 ) with convergence in P θ - probability . This says that the Kolmogorov - Smirnov distance be - tween the distribution of the pivot and the conditional distribution of the bootstrapped pivot converges to zero , in probability over ω . In most cases √ n ( ˆ τ n − τ ) (cid:32) T for a continuous ran - dom variable T . In this case it is enough for the bootstrapped pivot to converge to the correct limit distribution . Lemma 1 ( Van der Vaart 2000 , Eq . ( 23 . 2 ) ) . Suppose √ n ( ˆ τ n − τ ) (cid:32) T for a random variable T with contin - uous distribution function F . Then , ˆ τ ∗ n is consistent if and only if , for all t , P ∗ n (cid:16) √ n ( ˆ τ ∗ n − ˆ τ n ) ≤ t | ω (cid:17) P → F ( t ) . Consistency is also preserved under continuous map - pings : if ˆ τ ∗ n is consistent relative to √ n ( ˆ τ n − τ ) (cid:32) T and g is continuous , then g ( ˆ τ ∗ n ) is consistent relative to √ n (cid:0) g ( ˆ τ n ) − g ( τ ) (cid:1) ( Beran , 1997 ) . In our applications we will show consistency of a bootstrap estimator ˆ θ ∗ n for the full parameter vector θ , which implies consis - tency for continuous functions of θ ; a simple application is selecting one entry and constructing a conﬁdence interval . 4 . 2 Conﬁdence interval consistency Bootstrap consistency implies consistent conﬁdence intervals . The conﬁdence interval [ ˆ a n , ˆ b n ] for τ = τ ( θ ) is ( conservatively ) asymptotically consistent at level 1 − α if , for all θ , lim inf n →∞ P θ (cid:16) ˆ a n ≤ τ ≤ ˆ b n (cid:17) ≥ 1 − α . ( 2 ) Lemma 2 ( Van der Vaart 2000 , Lemma 23 . 3 ) . Sup - pose √ n ( τ n − τ ) (cid:32) T for a random variable T with continuous distribution function and τ ∗ n is consistent . Then the pivotal intervals are consistent , and , if T is symmetrically distributed around zero , then Efron’s per - centile intervals are consistent . When the analogous conditions hold for the studentized pivot ( ˆ τ n − τ ) / ˆ σ n , studentized intervals are consistent . 4 . 3 Parametric bootstrap consistency Beran ( 1997 ) showed that asymptotic equivariance of the main estimator guarantees consistency of the para - metric bootstrap . Let H n ( θ ) be the distribution of √ n ( ˆ τ n − τ ( θ ) ) under P θ . Deﬁnition 5 ( Asymptotic equivariance , Beran 1997 ) . The estimator ˆ τ n is asymptotically equivariant if H n ( θ + h n / √ n ) converges to a limiting distribution H ( θ ) for all convergent sequences h n and all θ . Theorem 1 ( Parametric bootstrap consistency ) . Sup - pose √ n ( ˆ θ n − θ ) (cid:32) J ( θ ) and ˆ τ n is asympotitcally equiv - ariant with continuous limiting distribution H ( θ ) . Then the parametric bootstrap estimator ˆ τ ∗ n is consistent . Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals All proofs are provided in the appendix . Furthermore , under reasonably general conditions , the reverse impli - cation is true , with bootstrap failures occurring pre - cisely at those parameter values θ 0 for which asymptotic equivariance does not hold ( Beran , 1997 ) . 5 Applications We apply the parametric bootstrap to three private esti - mation settings : ( i ) exponential families with suﬃcient statistic perturbation ( SSP ) , ( ii ) linear regression with SSP , ( iii ) the “one posterior sample” ( OPS ) estimator . Exponential Families A family of distributions is an exponential family if P θ has a density of the form : p ( x ; θ ) = h ( x ) exp ( θ T T ( x ) − A ( θ ) ) where h ( x ) is a base measure , θ is the natural parame - ter , T is the suﬃcient statistic function , and A is the log - partition function . Deﬁne the log - likelihood func - tion of an exponential family as (cid:96) ( θ ; x ) = log p ( x ; θ ) − log h ( x ) = θ T T ( x ) − A ( θ ) . The constant term log h ( x ) does not aﬀect parameter estimation and is subtracted for convenience . For a sample x 1 : n , let T ( x 1 : n ) = (cid:80) ni = 1 T ( x i ) . The log - likelihood of the sample is (cid:96) ( θ ; x 1 : n ) = θ T T ( x 1 : n ) − nA ( θ ) : = f (cid:0) θ ; T ( x 1 : n ) (cid:1) , which depends on the data only through the suﬃcient statistic T ( x 1 : n ) . The maximum - likelihood estimator ( MLE ) is ˆ θ = argmax θ f (cid:0) θ ; T ( x 1 : n ) (cid:1) . A simple way to create a private estimator is suﬃcient statistic perturbation ( SSP ) ; that is , to privatize the suﬃcient statistics using an elementary privacy mecha - nism such as the Laplace or Gaussian mechanism prior to solving the MLE problem . SSP is a natural choice because T ( x 1 : n ) is a compact summary of the data and has sensitivity that is easy to analyze , and it often works well in practice ( Bernstein and Sheldon , 2018 ; Foulds et al . , 2016 ) . Speciﬁcally , it means solving ˆ θ = argmax θ f (cid:0) θ , T ( x 1 : n ) + w (cid:1) ( SSP - MLE ) where w is a suitable noise vector . This problem has closed form solutions for many exponential families and standard numerical routines apply to others . For the Laplace mechanism , w j ∼ Lap ( ∆ (cid:15) ) for all j , where ∆ = (cid:80) j width ( T j ) is an upper bound on the L 1 sensitivity of T ( x 1 : n ) by Claim 1 . If width ( T j ) is not known or is unbounded , the analyst must supply bounds and guarantee they are met , e . g . , by discarding data points that don’t meet the bounds , or clamping them to the bounded interval . Theorem 2 . Let ˆ θ n be the solution to the ( SSP - MLE ) optimization problem for a sample x 1 : n from an expo - nential family model that satisﬁes the regularity con - ditions given in Davison ( 2003 , Section 4 . 4 . 2 ) . Then √ n ( ˆ θ n − θ ) is asymptotically equivariant with limiting distribution N ( 0 , I ( θ ) − 1 ) , where I ( θ ) = ∇ 2 A ( θ ) is the Fisher information . This implies consistency of the parametric bootstrap estimator ˆ θ ∗ n . Linear Regression We consider a linear regression model where we are given n pairs 4 ( x i , y i ) with x i ∈ R p and y i ∈ R assumed to be generated as y i = β T x i + u i , where the errors u i are i . i . d . , independent of x i , zero - mean , and have ﬁnite variance σ 2 , and the x i are i . i . d . with E [ xx T ] = Q . We wish to estimate the regression coeﬃcients β ∈ R p . Let X ∈ R n × p be the matrix with i th row equal to x Ti and y , u ∈ R N be the vectors with i th entries y i and u i , respectively . The ordinary least squares ( OLS ) estimator is : ˆ β = ( X T X ) − 1 X T y . ( 3 ) Like the MLE in exponential families , Eq . ( 3 ) depends on the data only through suﬃcient statistics X T X and X T y , and SSP is a simple way to privatize the estimator that works very well in practice ( Wang , 2018 ) . The privatized estimator is ˆ β = ( X T X + V ) − 1 ( X T y + w ) , ( SSP - OLS ) where V ∈ R p × p and w ∈ R p are additive noise vari - ables drawn from distributions P V and P w to ensure privacy . For the Laplace mechanism , we use V jk ∼ Lap ( 0 , ∆ V / (cid:15) 1 ) for j ≤ k , and V kj = V jk , ( 4 ) w j ∼ Lap ( 0 , ∆ w / (cid:15) 2 ) for all j , ( 5 ) where ∆ V and ∆ w bound the L 1 sensitivity of V and w , respectively . The result is ( (cid:15) 1 + (cid:15) 2 ) - DP . Because X T X = (cid:80) ni = 1 x i x Ti and X T y = (cid:80) ni = 1 x i y i are addi - tive , we can take ∆ V = (cid:80) j ≤ k width ( x j ) · width ( x k ) and ∆ w = (cid:80) j width ( x j ) · width ( y ) , where width ( x j ) and width ( y ) are widths of the j th feature and re - sponse variable , respectively , which are enforced by the modeler . For conﬁdence intervals , we will also need a private estimate of σ 2 : let ˆ σ 2 = ( n − p ) − 1 (cid:80) n i = 1 ( y i − ˆ β T x i ) 2 + Lap ( 0 , ∆ z / (cid:15) 3 ) where ∆ z = width ( ( y − ˆ β T x ) 2 ) . The released values for SSP - OLS are then ( X T X + V , X T y + w , ˆ β , ˆ σ 2 ) , which satisfy ( (cid:15) 1 + (cid:15) 2 + (cid:15) 3 ) - DP . 4 We use boldface for vectors as needed to distinguish from scalar quantities . Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals Limitations of parametric bootstrap for private regression The parametric bootstrap is more dif - ﬁcult to apply to regression problems in a private setting due to the covariates . It is typical to boot - strap conditioned on X , which means simulating new response variables y from a parametric distribution p ( y | X ; ˆ β , ˆ σ 2 ) , where ˆ β and ˆ σ 2 are ( privately ) estimated parameters , and a fully parametric distribution p ( u ; σ 2 ) is assumed for errors . A bootstrap replicate would look like ˆ β ∗ = ( X T X ) − 1 X T y ∗ with y ∗ = X ˆ β + u ∗ and u ∗ simulated form the error distribution . The challenge is that X is accessed to generate each replicate , so to make it diﬀerentially private would require additional randomization and consume privacy budget . An alter - native would be to posit a model p ( x ; θ ) and perform the parametric bootstrap with respect to a joint model p ( x , y ; θ , β , σ 2 ) , but the additional demand to model covariates is unappealing in a regression context . Hybrid parametric bootstrap for OLS We pro - pose a novel hybrid approach that avoids the need to repeatedly access covariate data or to model the covari - ate or error distributions explicitly . Conceptually , we use the part of the standard asymptotic analysis that “works well” to approximate the relevant statistics of the covariate data , and use the parametric bootstrap to deal with the noise added for privacy . Following stan - dard analysis for OLS , we can substitute y = Xβ + u in ( SSP - OLS ) and scale terms to get : √ n ˆ β n = √ n (cid:18) ˇ Q n + 1 nV (cid:19) − 1 ˇ Q n β + + (cid:18) ˇ Q n + 1 nV (cid:19) − 1 (cid:18) 1 √ nX T u + 1 √ nw (cid:19) , ˇ Q n = 1 nX T X . ( 6 ) This expression is instructive to see the diﬀerent sources of randomness that contribute to the variability of ˆ β n : the terms ˇ Q n = 1 n X T X and 1 √ n X T u are due to data variability , and 1 n V and 1 √ n w are due to privacy . We form a bootstrap estimator ˆ β ∗ n that treats ( ˆ β n , β ) anal - ogously to ( ˆ β ∗ n , ˆ β n ) and simulates the diﬀerent sources of variability using the best available information about their distributions : √ n ˆ β ∗ n = √ n (cid:18) ˆ Q n + 1 nV ∗ (cid:19) − 1 ˆ Q n ˆ β n + + (cid:18) ˆ Q n + 1 nV ∗ (cid:19) − 1 (cid:18) Z ∗ n + 1 √ nw ∗ (cid:19) , Z ∗ n ∼ N ( 0 , ˆ σ 2 n ˆ Q n ) , V ∗ ∼ P V , w ∗ ∼ P w , ˆ Q n = 1 nX T X + 1 nV . All privacy terms in Eq . ( 6 ) are simulated from their exact distributions in Eq . ( 7 ) . The variables ˇ Q n and ˆ Q n represent approximations of Q = E [ xx T ] available to the corresponding estimator . Both quantities con - verge in probability to Q . Our choice not to simulate variability in these estimates due to the covariates is analogous to the “ﬁxed X ” bootstrap strategy for regression problems ( Fox , 2002 ) ; we do simulate the variability due to privacy noise added to the estimates . The blue terms represent contributions to estimator variability due to interactions between covariates and unobserved noise variables . In a traditional bootstrap , we might simulate this term in Eq . ( 7 ) as 1 √ n X T u ∗ where u ∗ are simulated errors , but , as described above , we do not wish to access X within the bootstrap proce - dure . Instead , because we know 1 √ n X T u (cid:32) N ( 0 , σ 2 Q ) by the central limit theorem , 5 and because σ 2 and Q are estimable , we simulate this term directly from the normal distribution with estimated parameters . Theorem 3 . The private estimator satisﬁes √ n ( ˆ β n − β ) (cid:32) N ( 0 , σ 2 Q − 1 ) and the bootstrap estimator ˆ β ∗ n is consistent in the sense of Lemma 1 . OPS Dimitrakakis et al . ( 2014 ) , Wang et al . ( 2015 ) and Foulds et al . ( 2016 ) used the idea of sampling from a Bayesian posterior distribution to obtain a diﬀeren - tially private point estimate . One Posterior Sampling ( OPS ) , which releases one sample from the posterior , is a special case of the exponential mechanism , and the corresponding estimator is near - optimal for para - metric learning ( Wang et al . , 2015 ) . The parametric bootstrap applies easily to OPS estimators and pro - duces well calibrated intervals ( Figure 2 ) . We expect the asymptotic analysis of Wang et al . ( 2015 ) can be adapted to prove asymptotic equivariance , and hence parametric bootstrap consistency , for OPS , but do not give a formal proof . 6 Related work A number of prior works have studied private conﬁ - dence intervals for diﬀerent models ( D’Orazio et al . , 2015 ; Karwa and Vadhan , 2018 ; Sheﬀet , 2017 ; Barri - entos et al . , 2018 ; Gaboardi et al . , 2019 ; Brawner and Honaker , 2018 ; Du et al . , 2020 ) . Smith ( 2011 ) showed that a broad class private estimators based on subsam - ple & aggregate ( Nissim et al . , 2007 ) are asymptotically normal . D’Orazio et al . ( 2015 ) proposes an algorithm based on subsample & aggregate to approximate the variance of a private estimator ( see Section 3 . 2 ) . The topics of diﬀerentially private hypothesis testing ( Vu and Slavkovic , 2009 ; Solea , 2014 ; Gaboardi et al . , 2016 ; 5 This is a standard result of OLS asymptotics and is expected to be accurate for modest sample sizes . Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals Couch et al . , 2019 ) and Bayesian inference ( Williams and McSherry , 2010 ; Dimitrakakis et al . , 2014 ; Wang et al . , 2015 ; Foulds et al . , 2016 ; Zhang et al . , 2016 ; Heikkilä et al . , 2017 ; Bernstein and Sheldon , 2018 , 2019 ) are also related , but the speciﬁc considerations diﬀer somewhat from conﬁdence interval construction . Finding practical and general - purpose algorithms for diﬀerentially private conﬁdence intervals has been iden - tiﬁed as an important open problem ( King et al . , 2020 ) . The conﬁdence interval approach of Wang et al . ( 2019 ) applies to any model ﬁt by empirical risk minimization with objective or output perturbation and is similar to the asymptotic methods we compare to in Section 7 . Evans et al . ( 2019 ) also give a general - purpose proce - dure based on subsample & aggregate ( S & A ) ( Nissim et al . , 2007 ) with normal approximations . This method also uses S & A for the point estimates . We compare to a similar variant of S & A in Section 7 . Wang et al . ( 2018 ) study statistical approximating distributions for diﬀerentially private statistics in a general setting . Brawner and Honaker ( 2018 ) use the non - parametric bootstrap in a privacy context to estimate standard errors “for free” ( at no additional cost beyond mean estimation ) in some settings . Other methods most similar to the our work on the parametric bootstrap were discussed in more detail in Sec . 3 . 2 . Prior methods to construct conﬁdence intervals for pri - vate linear regression include ( Sheﬀet , 2017 ; Barrientos et al . , 2018 ) . 7 Experiments Figure 1 : Observed vs nominal coverage of 95 % CIs for diﬀerent distributions for diﬀerent n . We design synthetic experiments to demonstrate our proposed methods for diﬀerentially private conﬁdence interval estimation . First , we evaluate the performance of private paramet - ric bootstrap CIs vs a baseline method ( “Fisher CIs” ) based on asymptotic normality of the private estima - tor and described in more detail below . Performance is measured by how well the coverage of the private CIs matches the nominal coverage . For all models , we also include Fisher CIs of non - private estimators for comparison . Second , we demonstrate the bias - correction procedure in Sec . 3 . 1 in the case of Gaussian and Poisson distribu - tions with data points clamped to diﬀerent thresholds , which introduces estimation bias . These results show the eﬀectiveness of the parametric bootstrap at approx - imating and mitigating the bias of diﬀerentially private estimates when sensitivity is bounded by forcing data to take values within given bounds . Third , we compare parametric bootstrap CIs to another general purpose method to construct conﬁdence inter - vals based on subsample & aggregate ( Nissim et al . , 2007 ; Smith , 2011 ; D’Orazio et al . , 2015 ) . Finally , the appendix includes additional experiments exploring a broader range of settings and performance metrics . These include : multivariate distributions , the eﬀect of varying (cid:15) , and measurements of the upper - and lower - tail CI failures . We aim for private CIs to be as tight as possible while providing the correct coverage : in the appendix , we also compare the width of our intervals with that of intervals from existing methods for the speciﬁc case of Gaussian mean estimation of known variance . Baseline : “Fisher CIs” As a byproduct of our con - sistency analysis we also derive asymptotic normal distributions of the private estimators for both expo - nential families ( Theorem 2 ) and OLS ( Theorem 3 ) . In each case , we obtain a private , consistent estimate ˆ σ 2 j of the j th diagonal entry of the inverse Fisher information matrix of the private estimator ˆ θ n , and then construct the conﬁdence interval for θ j as C n = (cid:2) ˆ θ n , j − z α / 2 ˆ σ j , ˆ θ n , j + z α / 2 ˆ σ j (cid:3) , ( 7 ) where z γ is the 1 − γ - quantile of the standard nor - mal distribution . For exponential families , the Fisher information is estimated via plug - in estimation with the private estimator ˆ θ n . For OLS , it is estimated via plugging in private estimates ˆ Q n = 1 n X T X + 1 n V and ˆ σ 2 n , which are both released by the SSP mechanism . For non - private Fisher CIs , we follow similar ( and very standard ) procedures with non - private estimators . Exponential families We use synthetic data sets drawn from diﬀerent exponential family distributions . Given a family , true parameter θ , and data size n , a data set is drawn from P θ . We release private statistics via SSP with the Laplace mechanism . To simulate the modeler’s domain knowledge about the data bounds , we draw a separate surrogate data set of size 1000 drawn from the same distribution , compute the data range and use it to bound the width of each released statistic . For private estimation , sampled data is clamped to this range . Private ˆ θ is computed from the privately released statistics using SSP - MLE . For the parametric bootstrap CIs , we implement Algorithm 1 and compute Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals 46 days Figure 2 : Left pair . ( i ) observed vs . nominal coverage for diﬀerent coverage levels , for a Poisson , n = 100 , (cid:15) = 0 . 5 . ( ii ) same plot , comparing the OPS method ( Foulds et al . , 2016 ) and the parametric bootstrap for Bernoulli estimation . Center pair : average CI widths for diﬀerent n for ( i ) Gamma and ( ii ) OLS ( other distributions give qualitatively similar results ) . Width of the private bootstrap CIs approaches that of the public CIs as n → ∞ . Right pair . ( i ) private and bias - corrected private estimates for a Poisson clamped at varying right - tail thresholds . ( ii ) same for a Gaussian clamped at − 10 on the left tail and at varying thresholds on the right tail . Efron’s percentile intervals ( see Table 1 ) . The output coverage is computed over T = 1000 trials . Results are shown in Figures 1 and 2 . For the para - metric bootstrap , actual coverage closely matches the nominal coverage , even for very small n . Coverage of private Fisher CIs is too low until n becomes large , due to the fact that it ignores privacy noise . The boot - strap procedure correctly accounts for the increased uncertainty due to the privacy by enlarging the CIs . The width of the bootstrap intervals approaches the width of the baseline Fisher intervals as n → ∞ . In the appendix , we show that the coverage failures are balanced between left and right tails and examine the eﬀect of increasing (cid:15) ( which reduces privacy noise and has the same qualitative eﬀect as increasing n ) . Linear regression We follow a very similar proce - dure for OLS . Data is generated with x j ∼ Unif ( [ − 5 , 5 ] ) for all j and errors are u i ∼ Unif [ − 10 , 10 ] ; bounds on y are passed as inputs ( [ − 150 , 150 ] ) and assumed known . Observed values of y exceeding the given bounds are dropped . These bounds are also used to compute widths for the sensitivity . Private coeﬃcients are es - timated with SSP - OLS and bootstrap CIs are con - structed via Efron’s percentile method . The results are shown in Fig . 1 . Bias correction In the case of distributions with inﬁnite support , one option to bound the sensitivity is to clamp or truncate the data to given bounds . These procedures may induce estimation bias . As discussed in Sec 3 . 1 , the parametric bootstrap can be used to approximate this bias and mitigate it . We demonstrate bias correction on the private estimates and CIs of a Poisson and Gaussian distribution where data is clamped on the right tail at diﬀerent thresholds ( Fig . 2 ) . Comparison with subsample & aggregate We compare the parametric bootstrap CIs with the in - tervals obtained via a subsample & aggregate ( S & A ) algorithm . We adapted the S & A procedure of D’Orazio et al . ( 2015 ) for privately estimating standard errors to compute conﬁdence intervals ; see Algorithm 2 in the appendix . We compare the accuracy of point estimates and 95 % CIs for the mean of a Gaussian of known vari - ance . We found that the parametric bootstrap provides more accurate point estimates and better calibrated , tighter CIs than S & A ( Figure 3 ) . Figure 3 : Point estimates ( left ) , 95 % CI coverage ( center ) and average CI width ( right ) of the S & A method ( see Algorithm 2 in appendix ) vs parametric bootstrap for the mean of a Gaussian of known vari - ance . Settings : (cid:15) = 0 . 5 , θ = 0 , σ = 1 , ( x min , x max ) = ( − 20 , + 20 ) , ( L min , L max ) = ( − 10 , + 10 ) , var max = 50 . 8 Conclusion The parametric bootstrap is useful and eﬀective to construct consistent diﬀerentially private conﬁdence in - tervals for broad classes of private estimators , including private linear regression , for which we present a novel adaptation to avoid accessing the covariate data many times . The parametric bootstrap yields conﬁdence in - tervals with good coverage even at modest sample sizes , and tighter than the ones based on subsample & aggre - gate or other general methods . It can be used with any privacy mechanism , and can help mitigate diﬀerentially private estimation bias . Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals References Bradley Efron . Bootstrap methods : Another look at the jackknife . Annals of Statistics , 7 ( 1 ) : 1 – 26 , 01 1979 . Thomas Brawner and James Honaker . Bootstrap in - ference and diﬀerential privacy : Standard errors for free . Unpublished Manuscript , 2018 . Yu - Xiang Wang , Stephen Fienberg , and Alex Smola . Privacy for free : Posterior sampling and stochastic gradient monte carlo . In Proceedings of the 32nd In - ternational Conference on Machine Learning ( ICML - 15 ) , pages 2493 – 2502 , 2015 . James Foulds , Joseph Geumlek , Max Welling , and Ka - malika Chaudhuri . On the theory and practice of privacy - preserving bayesian data analysis . In Proceed - ings of the Thirty - Second Conference on Uncertainty in Artiﬁcial Intelligence , UAI’16 , page 192 – 201 , 2016 . Zuhe Zhang , Benjamin I . P . Rubinstein , and Christos Dimitrakakis . On the diﬀerential privacy of bayesian inference . In Thirtieth AAAI Conference on Artiﬁcial Intelligence , 2016 . Cynthia Dwork , Frank McSherry , Kobbi Nissim , and Adam Smith . Calibrating noise to sensitivity in private data analysis . In Theory of cryptography conference , pages 265 – 284 . Springer , 2006 . Bradley Efron . Nonparametric standard errors and conﬁdence intervals . Canadian Journal of Statistics , 9 ( 2 ) : 139 – 158 , 1981a . Bradley Efron . Nonparametric estimates of standard er - ror : the jackknife , the bootstrap and other methods . Biometrika , 68 ( 3 ) : 589 – 599 , 1981b . Bradley Efron and Robert Tibshirani . Bootstrap meth - ods for standard errors , conﬁdence intervals , and other measures of statistical accuracy . Statistical science , pages 54 – 75 , 1986 . Wenxin Du , Canyon Foot , Monica Moniot , Andrew Bray , and Adam Groce . Diﬀerentially private con - ﬁdence intervals . arXiv preprint arXiv : 2001 . 02285 , 2020 . URL https : / / arxiv . org / abs / 2001 . 02285 . Aad W . Van der Vaart . Asymptotic statistics , volume 3 . Cambridge university press , 2000 . Larry Wasserman . All of nonparametric statistics . Springer Science & Business Media , 2006 . Bradley Efron and Trevor Hastie . Computer age sta - tistical inference , volume 5 . Cambridge University Press , 2016 . Marco Gaboardi , Hyun Lim , Ryan Rogers , and Salil Vadhan . Diﬀerentially private chi - squared hypothesis testing : Goodness of ﬁt and independence testing . In Proceedings of the 33rd International Conference on Machine Learning , volume 48 , pages 2111 – 2120 , 2016 . Vito D’Orazio , James Honaker , and Gary King . Dif - ferential privacy for social science inference . Sloan Foundation Economics Research Paper , ( 2676160 ) , 2015 . Yue Wang , Daniel Kifer , and Jaewoo Lee . Diﬀerentially private conﬁdence intervals for empirical risk mini - mization . Journal of Privacy and Conﬁdentiality , 9 ( 1 ) , 2019 . Georgina Evans , Gary King , Margaret Schwenzfeier , and Abhradeep Thakurta . Statistically valid infer - ences from privacy protected data . Working paper , 2019 . URL https : / / gking . harvard . edu / files / gking / files / udpd . pdf . Rudolf Beran . Diagnosing bootstrap success . Annals of the Institute of Statistical Mathematics , 49 ( 1 ) : 1 – 24 , 1997 . Garrett Bernstein and Daniel R . Sheldon . Diﬀerentially private bayesian inference for exponential families . In Advances in Neural Information Processing Systems , pages 2919 – 2929 , 2018 . Anthony C . Davison . Statistical models , volume 11 . Cambridge University Press , 2003 . Yu - Xiang Wang . Revisiting diﬀerentially private lin - ear regression : optimal and adaptive prediction & estimation in unbounded domain , 2018 . John Fox . An R and S - Plus companion to applied regression . Sage , 2002 . Christos Dimitrakakis , Blaine Nelson , Aikaterini Mitrokotsa , and Benjamin I . P . Rubinstein . Robust and private bayesian inference . In International Con - ference on Algorithmic Learning Theory , pages 291 – 305 . Springer , 2014 . Vishesh Karwa and Salil Vadhan . Finite sample diﬀer - entially private conﬁdence intervals . 9th Innovations in Theoretical Computer Science Conference , 2018 . Or Sheﬀet . Diﬀerentially private ordinary least squares . In Proceedings of the 34th International Conference on Machine Learning , 2017 . Andrés F . Barrientos , Jerome P . Reiter , Ashwin Machanavajjhalab , and Yan Chen . Diﬀerentially private signiﬁcance tests for regression coeﬃcients . Journal of Computational and Graphical Statistics , 2018 . Marco Gaboardi , Ryan Rogers , and Or Sheﬀet . Lo - cally private mean estimation : z - test and tight conﬁ - dence intervals . In Kamalika Chaudhuri and Masashi Sugiyama , editors , Proceedings of Machine Learning Research , volume 89 , pages 2545 – 2554 , 2019 . Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals Adam Smith . Privacy - preserving statistical estimation with optimal convergence rates . In Proceedings of the forty - third annual ACM symposium on Theory of computing , pages 813 – 822 , 2011 . Kobbi Nissim , Sofya Raskhodnikova , and Adam Smith . Smooth sensitivity and sampling in private data anal - ysis . In Proceedings of the thirty - ninth annual ACM symposium on Theory of computing , pages 75 – 84 , 2007 . Duy Vu and Aleksandra Slavkovic . Diﬀerential privacy for clinical trial data : Preliminary evaluations . In 2009 IEEE International Conference on Data Mining Workshops , pages 138 – 143 . IEEE , 2009 . Eftychia Solea . Diﬀerentially private hypothesis testing for normal random variables . Master’s thesis , The Pennsylvania State University . 2014 . Simon Couch , Zeki Kazan , Kaiyan Shi , Andrew Bray , and Adam Groce . Diﬀerentially private nonparamet - ric hypothesis testing . In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Com - munications Security , pages 737 – 751 , 2019 . Oliver Williams and Frank McSherry . Probabilistic inference and diﬀerential privacy . In Advances in Neural Information Processing Systems , pages 2451 – 2459 , 2010 . Mikko Heikkilä , Eemil Lagerspetz , Samuel Kaski , Kana Shimizu , Sasu Tarkoma , and Antti Honkela . Diﬀer - entially private bayesian learning on distributed data . In Advances in neural information processing sys - tems , pages 3226 – 3235 , 2017 . Garrett Bernstein and Daniel R . Sheldon . Diﬀerentially private bayesian linear regression . In Advances in Neural Information Processing Systems , pages 523 – 533 , 2019 . Gary King et al . The OpenDP White Paper . Technical report , 2020 . Yue Wang , Daniel Kifer , Jaewoo Lee , and Vishesh Karwa . Statistical approximating distributions under diﬀerential privacy . Journal of Privacy and Conﬁ - dentiality , 8 ( 1 ) , 2018 . Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals A Proof of Claim 1 Claim 1 . Suppose X = ( x 1 , . . . , x n ) and f ( X ) = (cid:80) ni = 1 g ( x i ) where g maps data points to R m . Let width ( g j ) = max x g j ( x ) − min x g j ( x ) where x ranges over the data domain . Then ∆ f ≤ (cid:80) mj = 1 width ( g j ) , which is a constant independent of n . Proof . Since X and X (cid:48) diﬀer in exactly one element and f is additive , f ( X ) − f ( X (cid:48) ) = g ( x ) − g ( x (cid:48) ) for some elements x , x (cid:48) in the data domain . The absolute value of the j th output g j ( x ) − g j ( x (cid:48) ) is bounded by width ( g j ) = max x ∗ g j ( x ∗ ) − min x ∗ g j ( x ∗ ) . The L 1 sensitivity (cid:107) f ( X ) − f ( X (cid:48) ) (cid:107) 1 = (cid:107) g ( x ) − g ( x ) (cid:48) (cid:107) 1 is therefore at most the sum of the widths . B Proofs for Bootstrap Theory Theorem 1 ( Parametric bootstrap consistency ) . Suppose √ n ( ˆ θ n − θ ) (cid:32) J ( θ ) and ˆ τ n is asympotitcally equivariant with continuous limiting distribution H ( θ ) . Then the parametric bootstrap estimator ˆ τ ∗ n is consistent . This theorem is a simpliﬁed version of the result of Beran ( 1997 ) . We give a self - contained proof . See also ( Van der Vaart , 2000 , Problem 23 . 5 ) . Proof . The distribution of √ n ( ˆ τ n − τ ) under P θ is H n ( θ ) , which , by asymptotic equivariance , converges to H ( θ ) . In the parametric bootstrap , the distribution of √ n ( ˆ τ ∗ n − ˆ τ n ) conditional on ˆ θ n = θ + h n / √ n is H n ( θ + h n / √ n ) ) , and , by asymptotic equivariance , H n ( θ + h n / √ n ) ) (cid:32) H ( θ ) if h n is convergent . Since H ( θ ) is continuous this is equivalent to saying that , for all convergent sequences h n and all t P ∗ n (cid:16) √ n ( ˆ τ ∗ n − ˆ τ n ) ≤ t | ˆ θ n = θ + h n / √ n (cid:17) → F θ ( t ) . ( 8 ) where F θ is the CDF of H ( θ ) . Now , let ˆ h n = √ n ( ˆ θ n − θ ) so that ˆ θ n = θ + ˆ h n / √ n . By assumption , ˆ h n (cid:32) J ( θ ) and is therefore O P ( 1 ) . Therefore , by Lemma 3 , Eq . ( 8 ) implies P ∗ n (cid:16) √ n ( ˆ τ ∗ n − ˆ τ n ) ≤ t | ˆ θ n (cid:17) → F θ ( t ) in P θ - probability and the result is proved . Lemma 3 . Suppose g n is a sequence of functions such that g n ( h n ) → 0 for any ﬁxed sequence h n = O ( 1 ) . Then g n ( ˆ h n ) P → 0 for every random sequence ˆ h n = O P ( 1 ) . Proof . Fix (cid:15) , δ > 0 . We wish to show , for large enough n , that Pr (cid:2) | g n ( ˆ h n ) | > (cid:15) (cid:3) < δ . Since ˆ h n is O P ( 1 ) , there is some M such that , for all n , Pr (cid:2) (cid:107) h n (cid:107) > M (cid:3) < δ . By our assumption on g n , there is some N such that | g n ( h ) | < (cid:15) for all (cid:107) h (cid:107) ≤ M , n > N ( take the sequence h n ≡ h for each such h ) . Then , for n > N , Pr (cid:2) | g n ( ˆ h n ) | > (cid:15) (cid:3) ≤ Pr (cid:2) (cid:107) h n (cid:107) > M (cid:3) < δ . Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals C Proofs for Exponential Families Lemma 4 . Let w be any random variable with mean zero and ﬁnite variance . For any r > 0 , 1 N r w P → 0 . Proof . The variance of N − r w is equal to N − 2 r Var ( w ) , which goes to zero as N → ∞ . By Chebyshev’s inequality , this implies that N − r w P → 0 . Theorem 2 . Let ˆ θ n be the solution to the ( SSP - MLE ) optimization problem for a sample x 1 : n from an exponential family model that satisﬁes the regularity conditions given in Davison ( 2003 , Section 4 . 4 . 2 ) . Then √ n ( ˆ θ n − θ ) is asymptotically equivariant with limiting distribution N ( 0 , I ( θ ) − 1 ) , where I ( θ ) = ∇ 2 A ( θ ) is the Fisher information . This implies consistency of the parametric bootstrap estimator ˆ θ ∗ n . Following standard practice , we will prove this for the case when θ is scalar ; the generalization to vector θ is straightforward but cumbersome . We ﬁrst state the required ( standard ) regularity conditions . Let (cid:96) n ( θ ) = n (cid:88) i = 1 (cid:96) ( θ ; x i ) = n (cid:88) i = 1 (cid:0) log p ( x i ; θ ) − log h ( x ) (cid:1) = θ n (cid:88) i = 1 T ( x i ) − nA ( θ ) be the log - likelihood of a sample x 1 : n from the exponential family model using the deﬁnition of log - likelihood from Sec . 1 . Let (cid:96) ( θ ) = (cid:96) 1 ( θ ) be the log - likelihood of a single x ∼ p ( x ; θ ) . We assume the log - likelihood satisﬁes the conditions given in the book of Davison ( 2003 , Section 4 . 4 . 2 ) . If it does , then we have the following ( F1 ) E θ [ (cid:96) (cid:48) ( θ ) ] = 0 . ( F2 ) Var θ [ (cid:96) (cid:48) ( θ ) ] = − E θ [ (cid:96) (cid:48)(cid:48) ( θ ) ] = I ( θ ) . ( F3 ) Given a sequence of estimators ˆ θ n P → θ , for all ˜ θ n ∈ [ θ , ˆ θ n ] , 1 2 √ n (cid:96) (cid:48)(cid:48)(cid:48) n ( ˜ θ n ) ( ˆ θ n − θ ) 2 P → 0 Facts ( F1 ) and ( F2 ) are well known exponential family properties . Also recall that , for an exponential family , ( F4 ) I ( θ ) = A (cid:48)(cid:48) ( θ ) . ( F5 ) − (cid:96) (cid:48)(cid:48) ( θ ) is deterministic and equal to I ( θ ) . Proof . Let λ n ( θ ) = f (cid:0) θ , w + (cid:80) ni = 1 T ( x i ) (cid:1) be the objective of the SSP - MLE optimization problem . We have λ n ( θ ) = θ (cid:16) w + n (cid:88) i = 1 T ( x i ) (cid:17) − nA ( θ ) = θw + θ n (cid:88) i = 1 T ( x i ) − nA ( θ ) = θw + (cid:96) n ( θ ) where (cid:96) n ( θ ) is the log - likelihood of the true sample . That is , the original objective (cid:96) n ( θ ) is perturbed by the linear function θw to obtain λ n ( θ ) . The derivatives are therefore related as : λ (cid:48) n ( θ ) = w + (cid:96) (cid:48) n ( θ ) , ( 9 ) λ ( k ) n ( θ ) = (cid:96) ( k ) ( θ ) , k > 1 . ( 10 ) Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals At the optimum ˆ θ n , the ﬁrst derivative of λ n is equal to zero . (cid:96) (cid:48) ( θ ) is a sum of i . i . d . terms with mean 0 and variance I ( θ ) , more speciﬁcally : (cid:96) (cid:48) ( θ ) = n (cid:88) i = 1 (cid:0) T ( x i ) − A (cid:48) ( θ ) (cid:1) For asymptotic equivariance , we are interested in the sequence of estimators ˆ θ n when the “true parameter” follows the sequence θ + h n / √ n . We follow the standard approach of writing the Taylor expansion of the ﬁrst derivative about the true parameter θ + h n / √ n : 0 = w + (cid:96) (cid:48) n ( θ + h n / √ n ) + (cid:96) (cid:48)(cid:48) n ( θ + h n / √ n ) ( ˆ θ n − θ − h n / √ n ) + Z n ( 11 ) where we have used Eqs . ( 9 ) and ( 10 ) to replace the derivatives of λ on the right - hand side , and Z n = 12 (cid:96) (cid:48)(cid:48)(cid:48) n ( ˜ θ n ) ( ˆ θ n − θ − h n / √ n ) 2 is the second - order Taylor term , with ˜ θ n some point in the interval [ θ + h n / √ n , ˆ θ n ] . Multiply both sides of the equation by 1 √ n and rearrange to get √ n ( ˆ θ n − θ − h n / √ n ) = 1 √ n w + 1 √ n (cid:96) (cid:48) n ( θ + h n / √ n ) + 1 √ n Z n − 1 n (cid:96) (cid:48)(cid:48) n ( θ + h n / √ n ) = 1 √ n w + 1 √ n (cid:96) (cid:48) n ( θ + h n / √ n ) + 1 √ n Z n I ( θ + h n / √ n ) where in the second equality we used ( F5 ) . By Lemma 4 , 1 √ n w P → 0 and by ( F3 ) 1 √ n Z n P → 0 , so , by Slutsky , √ n ( ˆ θ n − θ − h n / √ n ) P → 1 √ n (cid:96) (cid:48) ( θ + h n √ n ) I ( θ + h n √ n ) = (cid:96) (cid:48) ( θ + h n √ n ) (cid:113) nI ( θ + h n √ n ) (cid:124) (cid:123)(cid:122) (cid:125) ( B ) 1 (cid:113) I ( θ + h n √ n ) ( 12 ) We know that under the regularity assumptions the Fisher information I ( · ) is a continuous function and so , since θ + h n √ n → θ , then by continuity ( I ( · ) is deterministic ) : (cid:32) I (cid:18) θ + h n √ n (cid:19)(cid:33) − 1 / 2 → (cid:0) I ( θ ) (cid:1) − 1 / 2 We now focus on the asymptotic behavior of ( B ) . We will use the fact that in exponential families , E θ T ( x ) = A (cid:48) ( θ ) and Var θ T ( x ) = A (cid:48)(cid:48) ( θ ) = I ( θ ) . For simplicity of notation , deﬁne : µ n = A (cid:48) (cid:18) θ + h n √ n (cid:19) . Deﬁne now the triangular array written in the following notation : T ( x 1 ) − µ 1 x 1 ∼ P θ + h 11 T ( x 1 ) − µ 2 , T ( x 2 ) − µ 2 x 1 : 2 i . i . d . ∼ P θ + h 2 √ 2 T ( x 1 ) − µ 3 , T ( x 2 ) − µ 3 , T ( x 3 ) − µ 3 x 1 : 3 i . i . d . ∼ P θ + h 3 √ 3 . . . . . . T ( x 1 ) − µ n , T ( x 2 ) − µ n , . . . , T ( x n ) − µ n x 1 : n i . i . d . ∼ P θ + hn √ n Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals Let’s focus on the n - th row . By construction the sum over the n - th row is S n = (cid:80) ni = 1 ( T ( x i ) − µ n ) = (cid:96) (cid:48) ( θ + h n √ n ) , so the numerator of ( A ) . Each term in the n - th row has mean zero and : σ 2 n = n (cid:88) i = 1 Var [ T ( x i ) − µ n ] = nI (cid:18) θ + h n √ n (cid:19) . If for every (cid:15) > 0 the following condition holds : lim n →∞ 1 σ 2 n n (cid:88) i = 1 E (cid:20) ( T ( x i ) − µ n ) 2 1 (cid:16)(cid:12)(cid:12) T ( x i ) − µ n (cid:12)(cid:12) ≥ (cid:15)σ n (cid:17)(cid:21) = 0 , then S n / σ n → N ( 0 , 1 ) by the Lindeberg - Feller Central Limit Theorem . By plugging in the terms in the condition above we have that : lim n →∞ 1 nI (cid:16) θ + h n √ n (cid:17) n (cid:88) i = 1 E   ( T ( x i ) − µ n ) 2 1  (cid:12)(cid:12) T ( x i ) − µ n (cid:12)(cid:12) ≥ (cid:15) √ n (cid:115) I (cid:18) θ + h n √ n (cid:19)     = lim n →∞ I (cid:18) θ + h n √ n (cid:19) − 1 E   ( T ( x 1 ) − µ n ) 2 1  (cid:12)(cid:12) T ( x 1 ) − µ n (cid:12)(cid:12) ≥ (cid:15) √ n (cid:115) I (cid:18) θ + h n √ n (cid:19) with the equality due to i . i . d . sampling within the row of the triangular array . Note that for any x 1 , lim n →∞ ( T ( x 1 ) − µ n ) 2 1  (cid:12)(cid:12) T ( x 1 ) − µ n (cid:12)(cid:12) ≥ (cid:15) √ n (cid:115) I (cid:18) θ + h n √ n (cid:19) = 0 , and that the integrand above is dominated by ( T ( x 1 ) − µ n ) 2 , which is integrable and ﬁnite , since E [ ( T ( x 1 ) − µ n ) 2 ] is ﬁnite . Hence by the dominated convergence theorem , the limit is zero and the condition is satisﬁed . Going back to equation ( 12 ) , we then have that 1 √ n (cid:96) (cid:48) ( θ + h n √ n ) I ( θ + h n √ n ) (cid:32) N ( 0 , 1 ) · (cid:0) I ( θ ) (cid:1) − 1 / 2 = N ( 0 , I ( θ ) − 1 ) , which proves that √ n ( ˆ θ n − θ − h n / √ n ) (cid:32) N ( 0 , I ( θ ) − 1 ) . Setting h n = 0 , it is straightforward to ﬁnd that √ n ( ˆ θ n − θ ) (cid:32) N ( 0 , I ( θ ) − 1 ) . This proves that SSP - MLE is asymptotically equivariant . D Proofs for OLS Theorem 3 . The private estimator satisﬁes √ n ( ˆ β n − β ) (cid:32) N ( 0 , σ 2 Q − 1 ) and the bootstrap estimator ˆ β ∗ n is consistent in the sense of Lemma 1 . Before proving the theorem , we give two lemmas . The ﬁrst is standard and describes the asymptotics of the dominant term . Lemma 5 . Under the assumptions of the OLS model in Section 5 , 1 √ n X T u (cid:32) N ( 0 , σ 2 Q ) . Proof . Observe that X T u = (cid:80) ni = 1 x i u i is a sum of iid terms , and , using the assumptions of the model in Section 1 , the mean and variance of the terms are E [ x i u i ] = E [ x i ] E [ u i ] = 0 and Var ( x i u i ) = Var ( x u ) = E [ x uu x T ] = E [ u 2 xx T ] = E [ u 2 ] E [ xx T ] = σ 2 Q . The result follows from the central limit theorem . Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals The theorem involves asymptotic statements about ˆ β n and ˆ β ∗ n . The following lemma is a general asymptotic result that will apply to both estimators using Eqs . ( 6 ) and ( 7 ) . Lemma 6 . Deﬁne the function B n { ˇ Q , ˇ β , ˇ Z , ˇ V , ˇ w } = (cid:18) ˇ Q + 1 n ˇ V (cid:19) − 1 ˇ Q ˇ β + (cid:18) ˇ Q + 1 n ˇ V (cid:19) − 1 (cid:18) 1 √ n ˇ Z + 1 n ˇ w (cid:19) and suppose the sequences Q n , β n , Z n , V n , w n are deﬁned on a common probability space and satisfy ( i ) Z n (cid:32) N ( 0 , σ 2 Q ) , ( ii ) Q n P → Q , ( iii ) β n , V n , w n are all O P ( 1 ) . Then √ n (cid:16) B n { Q n , β n , Z n , V n , w n } − β n (cid:17) (cid:32) N ( 0 , σ 2 Q − 1 ) . Proof . Substitute the sequences into B n and rearrange to get √ n ( B n − β n ) = √ n (cid:32)(cid:18) Q n + 1 nV n (cid:19) − 1 Q n − I (cid:33) β n + √ n (cid:18) Q n + 1 nV n (cid:19) − 1 (cid:18) 1 √ nZ n + 1 nw n (cid:19) ( 13 ) First , note that the sequences 1 n V n , 1 √ n V n and 1 √ n w n , which will appear below , are all o P ( 1 ) , since V n and w n are O P ( 1 ) . The ﬁrst term in Eq . ( 13 ) converges to zero in probability . Speciﬁcally , a manipulation shows : √ n (cid:32)(cid:18) Q n + 1 nV n (cid:19) − 1 Q n − I (cid:33) β n = (cid:18) Q n + 1 nV n (cid:19) − 1 (cid:18) − 1 √ nV n (cid:19) β n = O P ( 1 ) o P ( 1 ) O P ( 1 ) = o P ( 1 ) For the ﬁrst factor on the right side , ( Q n + 1 n V n ) − 1 P → Q − 1 ( by Slutsky’s theorem , since Q n P → Q and 1 n V n P → 0 ) , and is therefore O P ( 1 ) . For the second factor , we argued − 1 √ n V n = o P ( 1 ) . For the third factor , β n = O P ( 1 ) by assumption . The second term in Eq . ( 13 ) converges in distribution to N ( 0 , σ 2 Q ) . Rewrite it as (cid:18) Q n + 1 nV n (cid:19) − 1 (cid:18) Z n + 1 √ nw n (cid:19) . We already argued that ( Q n + 1 n V n ) − 1 P → Q − 1 and 1 √ n w n P → 0 . By assumption , Z n (cid:32) N ( 0 , σ 2 Q ) . Therefore , by Slutsky’s theorem , the entire term converges in distribution to Q − 1 N ( 0 , σ 2 Q ) = N ( 0 , σ 2 Q − 1 ) . We are ready to prove the Theorem 3 . Proof of Theorem 3 . We ﬁrst wish to show that √ n (cid:16) ˆ β n − β (cid:17) (cid:32) N ( 0 , σ 2 Q − 1 ) . To see this , write ˆ β n = B n (cid:110) 1 n X T X , β , 1 √ n X T u , V , w (cid:111) and apply Lemma 6 . It is easy to verify that the sequences satisfy the con - ditions of the lemma . Next , we wish to show that the bootstrap estimator is consistent . By Lemma 1 , it is enough to show that √ n ( ˆ β ∗ n − ˆ β n ) (cid:32) N ( 0 , σ 2 Q − 1 ) conditional on ω in P θ - probability , where θ = ( β , σ 2 , Q ) and P θ is the common Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals probability space of the data and privacy random variables , represented by ω . The bootstrap variables Z ∗ n , V ∗ , w ∗ correspond to the inner measure P ∗ n . Deﬁne ˆ Q n = 1 n X T X + 1 n V . Observe that Eq . ( 7 ) is equivalent to ˆ β ∗ n = B n { ˆ Q n , ˆ β n , Z ∗ n , V ∗ , w ∗ } under Z ∗ n ∼ N ( 0 , ˆ σ 2 n ˆ Q n ) , V ∗ ∼ F V , w ∗ ∼ F w , ( 14 ) and ( ˆ Q n , ˆ β n , ˆ σ 2 ) are consistent estimators and hence converge in P θ - probability to ( Q , β , σ 2 ) . We can’t apply Lemma 6 directly to Eq . ( 14 ) because this expression mixes random variables from the outer space ( ˆ Q n , ˆ β n , ˆ σ 2 n ) and inner space ( Z ∗ n , V ∗ , w ∗ ) . Instead , we temporarily reason about a deterministic sequence ( Q n , β n , σ 2 n ) → ( Q , β , σ 2 ) . Then , by Lemma 6 applied to the inner probability space , √ n (cid:16) B n { Q n , β n , Z ∗ n , V ∗ , w ∗ } − β n (cid:17) (cid:32) N ( 0 , σ 2 Q − 1 ) under Z ∗ n ∼ N ( 0 , σ 2 n Q n ) , V ∗ ∼ P V , w ∗ ∼ P w ( 15 ) The conditions of Lemma 6 can easily be checked . In particular , we have Z ∗ n (cid:32) N ( 0 , σ 2 Q ) . We can restate the result Eq . ( 15 ) as follows : for any ﬁxed sequence ( Q n , β n , σ 2 n ) → ( Q , β , σ 2 ) and all t , P ∗ n (cid:16) √ n ( ˆ β ∗ n − ˆ β n ) ≤ t | ˆ Q n = Q n , ˆ β n = β n , ˆ σ 2 n = σ n (cid:17) → F ( t ) where F is the CDF of N ( 0 , σ 2 Q − 1 ) . Lemma 7 below now implies that P ∗ n (cid:16) √ n ( ˆ β ∗ n − ˆ β n ) ≤ t | ˆ Q n , ˆ β n , ˆ σ 2 n (cid:17) → F ( t ) in P θ - probability , and the theorem is proved . Lemma 7 . Let g n : R k → R (cid:96) be a sequence of functions such that g n ( h n ) → c for any deterministic sequence h n → h . Then g n ( ˆ h n ) P → c for any random sequence ˆ h n P → h . Proof . Take c = 0 without loss of generality , let (cid:107) · (cid:107) be any norm and d ( x , y ) = (cid:107) x − y (cid:107) . Fix (cid:15) > 0 . It must be the case that ∃ δ > 0 , n 0 ∈ N such that : d ( h (cid:48) , h ) < δ = ⇒ (cid:107) g n ( h (cid:48) ) (cid:107) < (cid:15) , ∀ n ≥ n 0 . ( 16 ) Otherwise , we can construct a convergent sequence h n → h with lim sup n →∞ (cid:107) g n ( h n ) (cid:107) ≥ (cid:15) , which violates the conditions of the Lemma . 6 Now , suppose ˆ h n P → h . Then , for n ≥ n 0 , by Eq . ( 16 ) , Pr (cid:104) (cid:107) g n ( ˆ h n ) (cid:107) > (cid:15) (cid:105) ≤ Pr (cid:104) d ( ˆ h n , h ) > δ (cid:105) . Therefore lim n →∞ Pr (cid:104) (cid:107) g n ( ˆ h n ) (cid:107) > (cid:15) (cid:105) ≤ lim n →∞ Pr (cid:104) d ( ˆ h n , h ) > δ (cid:105) = 0 , which proves the result . 6 If Eq . ( 16 ) is not true , then for all δ > 0 and n 0 ∈ N , there exists h (cid:48) such that d ( h (cid:48) , h ) < δ and (cid:107) g n ( h (cid:48) ) (cid:107) ≥ (cid:15) for some n ≥ n 0 . Then we can construct a sequence h n → h as follows . Let δ k be any sequence such that δ k → 0 . Set n 0 = 0 , and , for k ≥ 1 , select h (cid:48) such that d ( h (cid:48) , h ) < δ k and (cid:107) g n (cid:48) ( h (cid:48) ) (cid:107) ≥ (cid:15) for some n (cid:48) ≥ n k − 1 + 1 . Set h n = h (cid:48) for all n ∈ { n k − 1 + 1 , . . . , n (cid:48) } and let n k = n (cid:48) . This sequence satisﬁes h n → h but g n k ( h n k ) ≥ (cid:15) for all k , so it is not true that g n ( h n ) → 0 . This contradicts the assumptions of the lemma , so Eq . ( 16 ) must be true . Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals E Subsample & Aggregate Algorithm 2 Subsample & Aggregate Input X , M , x min , x max , L min , L max , var max , (cid:15) , α 1 : procedure SubsampleAndAggregate 2 : X 1 , . . . , X M ← subsample ( X , M ) 3 : L ∗ min , L ∗ max ← L min √ N / M , L max √ N / M 4 : var ∗ max ← var max N / M 5 : for i = 1 , . . . , M do 6 : ˆ c i ← clamp ( A ( X i ) , L ∗ min , L ∗ max ) 7 : ∆ 1 ← | L ∗ max − L ∗ min | M 8 : ˆ θ DP ← 1 M (cid:80) Mi = 1 ˆ c i + Lap ( 0 , ∆ 1 (cid:15) / 2 ) 9 : for i = 1 , . . . , M do 10 : for b = 1 , . . . , B do 11 : X i , b ← resample ( X i , (cid:98) NM (cid:99) , replace = True ) 12 : ˆ c i , b ← clamp ( A ( X i , b ) , L ∗ min , L ∗ max ) 13 : ˆ var ˆ c i ← clamp ( Var ( ˆ c i , 1 : B ) , 10 − 6 , var ∗ max ) 14 : ∆ 2 ← var ∗ max / M 15 : ˆ var ˆ c ← 1 M (cid:80) Mi = 1 ˆ var ˆ c i + Lap ( 0 , ∆ 2 (cid:15) / 2 ) 16 : ˆ var DP ← 1 M ˆ var ˆ c + Var ( Lap ( 0 , ∆ 1 (cid:15) / 2 ) ) 17 : CI DP ← [ ˆ θ DP − z α 2 √ ˆ var DP , ˆ θ DP + z α 2 √ ˆ var DP ] return ˆ θ DP , CI DP Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals F Additional Experiments 0 0 . 5 1 P r i v a t e P B = 0 . 1 = 0 . 2 = 0 . 3 = 0 . 5 = 0 . 7 = 1 . 0 0 0 . 5 1 P r i v a t e F I 0 0 . 5 1 0 0 . 5 1 P u b li c F I 0 0 . 5 1 0 0 . 5 1 0 0 . 5 1 0 0 . 5 1 0 0 . 5 1 n = 100 Nominal coverage O b s e r v e d c o v e r a g e ( a ) = 0 . 1 = 0 . 2 = 0 . 3 = 0 . 5 = 0 . 7 = 1 . 0 1 2 3 4 5 6 7 8 95 % confidence intervals true parameter private PB private FI public FI ( b ) Figure 4 : Eﬀects of varying (cid:15) for a ﬁxed n = 100 . We selected a Gamma with inference on the scale parameter . The results are qualitatively equivalent for other distributions . ( a ) Observed coverage vs . nominal coverage of CIs . Coverage levels : { 0 . 5 , 0 . 6 , 0 . 7 , 0 . 8 , 0 . 9 , 0 . 95 , 0 . 99 } . From top to bottom : ( i ) diﬀerentially private parametric bootstrap ; ( ii ) diﬀerentially private Fisher intervals ; ( iii ) non - private Fisher CIs . Private methods use SSP via Laplace mechanism with varying values of (cid:15) . Note that the eﬀect of increasing (cid:15) with n ﬁxed is qualitatively similar to the eﬀect of increasing n holding (cid:15) ﬁxed . ( b ) Average CIs for the scale parameter for diﬀerent (cid:15) . The width of the private bootstrap CIs approaches that of the public CIs as (cid:15) increases . Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals 0 0 . 5 1 P r i v a t e P B n = 50 n = 100 n = 500 n = 1000 n = 5000 n = 10000 0 0 . 5 1 P r i v a t e F I 0 0 . 5 1 0 0 . 5 1 P u b li c F I 0 0 . 5 1 0 0 . 5 1 upper tail failure rate lower tail failure rate 0 0 . 5 1 0 0 . 5 1 0 0 . 5 1 Nominal coverage R a t e o f f a il u r e 0 0 . 5 1 P r i v a t e P B n = 50 n = 100 n = 500 n = 1000 n = 5000 n = 10000 0 0 . 5 1 P r i v a t e F I 0 0 . 5 1 0 0 . 5 1 P u b li c F I 0 0 . 5 1 0 0 . 5 1 upper tail failure rate lower tail failure rate 0 0 . 5 1 0 0 . 5 1 0 0 . 5 1 Nominal coverage R a t e o f f a il u r e Figure 5 : In this Figure , we look at the rate of failure of the conﬁdence intervals on the upper vs lower tail . For each of the two plots , the rows represent ( i ) diﬀerentially private parametric bootstrap ; ( ii ) diﬀerentially private Fisher intervals ; ( iii ) non - private Fisher intervals . Top : data range and sensitivity computed as described in Section 7 . Clamping the data to a range can introduce a bias if the range is not conservative enough . The bias becomes noticeable for large n , where the interval width is smaller . In our case , where the range is approximated from a data set of size 1000 , a small bias becomes noticeable for n ≥ 5000 , where upper - tail failures systematically outnumber lower - tail failures by a small margin . Bottom : same as top plot , with double the range . Increasing the range mitigates the bias . Parametric Bootstrap for Diﬀerentially Private Conﬁdence Intervals 0 0 . 5 1 P r i v a t e P B n = 50 n = 100 n = 500 n = 1000 n = 5000 n = 10000 0 0 . 5 1 P r i v a t e F I 0 0 . 5 1 0 0 . 5 1 P u b li c F I 0 0 . 5 1 0 0 . 5 1 0 0 . 5 1 0 0 . 5 1 0 0 . 5 1 Nominal coverage O b s e r v e d c o v e r a g e ( a ) N = 50 N = 100 N = 500 N = 1000 N = 5000 N = 10000 2 . 5 3 . 0 3 . 5 4 . 0 4 . 5 5 . 0 5 . 5 6 . 0 95 % confidence intervals true parameter private PB private FI public FI ( b ) Figure 6 : Observed vs nominal coverage ( left ) and average CI width ( right ) for a multivariate Gaussian in 5 dimensions , with (cid:15) = 0 . 5 . We compute CIs for each dimension separately and report results for the ﬁrst dimension as an example . 10 50 100 500 1000 5000 10000 n 10 1 10 0 10 1 10 2 10 3 10 4 w i d t h algorithm : public Karwa & Vadhan D ' Orazio & al . Brawner & Honaker NOISYMAD SYMQ PB Figure 7 : For diﬀerent algorithms , average width ( logscale ) of diﬀerentially private conﬁdence intervals for the mean of a standard normal , range [ − 8 , 8 ] , (cid:15) = 0 . 1 , for diﬀerent n levels . “public” is the conﬁdence interval computed without diﬀerential privacy ; “Karwa & Vadhan” refers to Karwa and Vadhan ( 2018 ) ; “D’Orazio & al . ” refers to D’Orazio et al . ( 2015 ) ; “Brawner & Honaker” refers to Brawner and Honaker ( 2018 ) ; “ NOISYMAD " and “ SYMQ ” are methods from Du et al . ( 2020 ) , and in particular “ NOISYMAD ” is very similar to our parametric bootstrap method ( “ PB ” ) . We used the publicly available implementation by Du et al . ( 2020 ) to reproduce their methods as well as the other prior methods .