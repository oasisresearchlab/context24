Robust analyses for longitudinal clinical trials with missing and non - normal continuous outcomes Siyi Liu 1 , Yilong Zhang 2 , Gregory T Golm 2 , Guanghan ( Frank ) Liu 2 , 3 , Shu Yang 1 1 Department of Statistics , North Carolina State University , Raleigh , NC , USA 2 Merck & Co . , Inc . , Kenilworth , NJ , USA 3 Posthumous Abstract Missing data is unavoidable in longitudinal clinical trials , and outcomes are not always nor - mally distributed . In the presence of outliers or heavy - tailed distributions , the conventional multiple imputation with the mixed model with repeated measures analysis of the average treat - ment eﬀect ( ATE ) based on the multivariate normal assumption may produce bias and power loss . Control - based imputation ( CBI ) is an approach for evaluating the treatment eﬀect under the assumption that participants in both the test and control groups with missing outcome data have a similar outcome proﬁle as those with an identical history in the control group . We develop a general robust framework to handle non - normal outcomes under CBI without imposing any parametric modeling assumptions . Under the proposed framework , sequential weighted robust regressions are applied to protect the constructed imputation model against non - normality in both the covariates and the response variables . Accompanied by the subsequent mean imputation and robust model analysis , the resulting ATE estimator has good theoretical properties in terms of consistency and asymptotic normality . Moreover , our proposed method guarantees the analysis model robustness of the ATE estimation , in the sense that its asymptotic results remain intact even when the analysis model is misspeciﬁed . The superiority of the proposed robust method is demonstrated by comprehensive simulation studies and an AIDS clinical trial data application . keywords : Longitudinal clinical trial ; missing data ; multiple imputation ; robust regression ; sensitivity analysis . 1 a r X i v : 2203 . 10561v1 [ s t a t . M E ] 20 M a r 2022 1 Introduction 1 . 1 Missing data in clinical trials Analysis of longitudinal clinical trials often presents diﬃculties as inevitably some participants do not complete the study , thereby creating missing outcome data . Additionally , some outcome data among participants who complete the study may not be of interest on account of intercurrent events such as initiation of rescue therapy prior to the analysis time point . With the primary interest focusing on evaluating the treatment eﬀect in longitudinal clinical trials , the approach to handling missingness plays an essential role and has gained substantial attention from the US Food and Drug Administration ( FDA ) and National Research Council ( Little et al . , 2012 ) . The ICH E9 ( R1 ) addendum provides a detailed framework of deﬁning estimands to target the major clinical question in a population - level summary with the consideration of intercurrent events that may cause additional missingness ( ICH , 2021 ) . The missing at random ( MAR ; Rubin , 1976 ) mechanism is often invoked in analyses that seek to evaluate the treatment eﬃcacy . However , MAR is unveriﬁable and may not be practical in some clinical trials . Further , if the response at the primary time point is of interest regardless of whether participants have complied with the test or comparator treatments through the primary time point ( corresponding to a ‘treatment policy’ intercurrent event strategy ) , an analysis based on the MAR assumption would not be appropriate , because such an analysis would assume that responses in those who drop out would follow the same trajectory as responses in those who remain in treatment . A more plausible assumption would be that the treatment eﬀect may quickly fade away , leading to a missing not at random ( MNAR ) assumption that responses among those who fail to complete treatments in both treatment groups behave similarly to the responses among those in the control group with identical historical covariates . Drawn on the idea of the zero - dose model in Little and Yau ( 1996 ) , Carpenter et al . ( 2013 ) refer to this scenario as the control - based imputation ( CBI ) . Since the CBI represents a deviation from MAR , it is widely used in sensitivity analyses to explore the robustness of the study results against the untestable MAR assumption ( e . g . , Carpenter et al . , 2013 ; Cro et al . , 2016 ) . Furthermore , an increasing number of clinical studies have applied this approach to primary analyses ( Tan et al . , 2021 ) . Throughout the paper , we focus on jump - to - reference ( J2R ) as one favorable scenario of the CBI used in the FDA statistical review and evaluation reports ( e . g . , US Food and Drug Administration , 2016 ) , which assumes that the missing outcomes in the treatment group will have the same outcome mean proﬁle as those with identical historical information in the 2 control group . Our goal is to assess the average treatment eﬀect ( ATE ) under J2R . 1 . 2 Multiple imputation Multiple imputation ( MI ; Rubin , 2004 ) followed by a mixed - model with repeated measures ( MMRM ) analysis acts as a standard approach to analyze longitudinal clinical trial data under J2R . The main idea of MI applied in longitudinal trials is to use MMRM to impute the missing components and then conduct full - data analysis on each imputed dataset . The simple implementation and high ﬂexibility of MI underlie the recommendation of this approach by the FDA and National Research Council ( Little et al . , 2012 ) . However , this approach relies heavily on the parametric modeling assumptions in the construction of both the imputation and the analysis model , where a normal distribution is typically assumed . In reality , the distribution of the outcomes may suﬀer from extreme outliers or a heavy tail , which contradicts the normality assumption . A motivating CD4 count dataset in Section 2 further addresses that a simple transformation such as the log transformation sometimes cannot ﬁx the non - normality issue ( Mehrotra et al . , 2012 ) . In the presence of outliers or heavy tails , applying the methods that rely on the normal distribution may produce bias and power loss . To tackle the issue in longitudinal clinical trials under MAR , Mogg and Mehrotra ( 2007 ) and Mehrotra et al . ( 2012 ) suggest substituting the conventional analysis of covariance model in the full - data analysis step of MI with the rank - based regression ( Jaeckel , 1972 ) or Huber robust regression ( Huber et al . , 1973 ) to down - weight the impact of non - normal response values . When the missingness mechanism is MNAR , a gap exists in the extension of the robust method to handle the MNAR - related scenarios . 1 . 3 Our contribution : a robust framework We develop a general robust framework to evaluate the ATE for non - normal longitudinal outcomes with missingness under the scenario where missing response data in both the test and reference groups are assumed to follow the same trajectory as the complete data in the reference group . We propose applying robust regression in conjunction with mean imputation to relax the parametric modeling assumption required by MI in both the imputation and analysis stages . Inspired by the sequential linear regression model involved in many longitudinal studies , where the current outcomes are regressed recursively on the historical information ( Tang , 2017 ) , we replace the least squares ( LS ) estimator with the estimator obtained by minimizing the robust loss function such as the Huber loss , the absolute loss ( Huber , 2004 ) , and the ε - insensitive loss ( Smola and Schölkopf , 2004 ) , to mitigate the impact of non - normality in the response variable . While the robust regression lacks 3 the protection against outliers in the covariates ( Chang et al . , 2018 ) , a weighted sequential robust regression model is put forward using the idea in Carroll and Pederson ( 1993 ) to down - weight the inﬂuential covariates by a robust Mahalanobis distance . Followed by mean imputation and a robust analysis step , the estimator from our proposed method has solid theoretical guarantees in terms of consistency and asymptotic normality . Rosenblum and Van Der Laan ( 2009 ) establish a test robustness result for randomized clinical trials with complete data ; i . e . , for a wide range of analysis models , testing the existence of the non - zero ATE has an asymptotically correct type - 1 error even under model misspeciﬁcation . However , they focus only on the LS model estimators when no ATE exists ; and the property remains unclear when the model is estimated via the robust loss function under any arbitrary ATE value . To uncover the ambiguity , we extend the test robustness property to our proposed method for ATE estimation in the context of missing data . We formally show that the ATE estimator obtained from the various non - LS loss functions , including the Huber loss , the absolute loss , and the (cid:15) - insensitive loss is analysis model - robust , in the sense that its asymptotic properties remain the same even when the analysis model is incorrectly speciﬁed . Although the paper mainly focuses on the J2R scenario , the established method and the desired theoretical properties are extendable to robust estimators under other MNAR - related conditions . The rest of the paper is organized as follows . Section 2 addresses a real - data example to motivate the demand for the robust method . Section 3 introduces notations , assumptions under J2R , and an overview of the existing methods to handle missingness along with their drawbacks in the presence of non - normal data . Section 4 presents our proposed robust method and its detailed implementation steps . Section 5 provides the asymptotic results of the ATE estimator and discusses the analysis model robustness property . Section 6 conducts comprehensive simulation studies to validate the proposed method . Section 7 returns to the motivating example to illustrate the performance of the robust method in practice . Section 8 draws the conclusion . 2 A motivating application Study 193A conducted by the AIDS Clinical Trial Group compares the eﬀects of dual or triple combinations of the HIV - 1 reverse transcriptase inhibitors ( Henry et al . , 1998 ) . The data consists of the longitudinal outcomes of the CD4 count data at baseline and during the ﬁrst 40 weeks of follow - up , with the fully - observed baseline covariates as age and gender . In the trial , the participants are randomly assigned among the four treatments regarding dual or triple therapies . We focus on 4 Figure 1 : Spaghetti plots of the log - transformed CD4 count data separated by the two treatments . Arm 1 Arm 2 1 2 3 4 5 1 2 3 4 5 4 2 0 2 Visit intervals Log ( CD 4 + 1 ) Line Type IndividualMean the treatment comparison between arm 1 ( zidovudine alternating monthly with 400 mg didanosine ) and arm 2 ( zidovudine plus 400mg of didanosine plus 400mg of nevirapine ) . As arm 1 involves fewer combinations of inhibitors than arm 2 , we view it as the reference group . Among individuals in these two arms , we delete the ones with missing baseline CD4 counts , partition the time into discrete intervals ( 0 , 12 ] , ( 12 , 20 ] , ( 20 , 28 ] , ( 28 , 36 ] and ( 36 , 40 ] , and create a dataset with a monotone missingness pattern . Since the original CD4 counts are highly skewed , we conduct a log transformation to get the transformed CD4 counts as log ( CD4 + 1 ) and use them as the outcomes of interest . Figure 1 presents the spaghetti plots of the transformed CD4 counts . Although there are no outstanding outliers , severe missingness is evident in the data , with only 34 of 320 participants in arm 1 and 46 of 330 participants in arm 2 completing the trial . The high dropout rates in this data reﬂect a typical missing data issue in longitudinal clinical trials , leading to the demand of conducting imputation for the missing components to prevent the substantial information loss if we focus on only the complete data . We check the normality of the data by ﬁtting sequential linear regressions on the current outcomes against all historical information and examining the conditional residuals at each visit point for model diagnosis . An assessment of the normality of the responses via the Shapiro - Wilk test and the normal QQ plots are presented in Figure 2 . Each normality test indicates a violation of the normal assumption , and the normal QQ plots reveal that the CD4 counts remain heavy - tailed even after 5 Figure 2 : Diagnosis of the conditional residuals at each visit . −3 −2 −1 0 1 2 3 − 3 − 2 − 1 0 1 2 Normal QQ plot at visit 1 norm quantiles C ond i t i ona l r e s i dua l 515 611 Normality : p−value = 6 . 58 * 10 - 8 Symmetry : p−value = 0 . 99 −2 −1 0 1 2 − 2 − 1 0 1 Normal QQ plot at visit 2 norm quantiles C ond i t i ona l r e s i dua l 1201 549 Normality : p−value = 3 . 97 * 10 - 3 Symmetry : p−value = 0 . 62 −2 −1 0 1 2 − 2 . 0 − 1 . 0 0 . 0 0 . 5 1 . 0 Normal QQ plot at visit 3 norm quantiles C ond i t i ona l r e s i dua l 474 926 Normality : p−value = 4 . 62 * 10 - 5 Symmetry : p−value = 0 . 81 −2 −1 0 1 2 − 3 − 2 − 1 0 1 Normal QQ plot at visit 4 norm quantiles C ond i t i ona l r e s i dua l 246 414 Normality : p−value = 8 . 50 * 10 - 5 Symmetry : p−value = 0 . 63 −2 −1 0 1 2 − 2 − 1 0 1 Normal QQ plot at visit 5 norm quantiles C ond i t i ona l r e s i dua l 1117 1050 Normality : p−value = 1 . 34 * 10 - 3 Symmetry : p−value = 0 . 48 the log transformation . Under this circumstance , potentially biased and ineﬃcient treatment eﬀect estimates may occur when applying the conventional MI along with the MMRM analysis . It motivates the development of a robust method to assess the treatment eﬀect precisely under non - normality . 3 Basic setup Consider a longitudinal clinical trial with n participants and t follow - up visits . Let A i be the bi - nary treatment without loss of generality , X i be the p - dimensional fully - observed baseline covariates including the intercept term with a full - column rank , Y is be the continuous outcome of interest at visit s , where i = 1 , · · · , n , and s = 1 , · · · , t . In longitudinal clinical trials , participants are randomly assigned to diﬀerent treatment groups with non - zero probabilities . When missingness is involved , de - note the observed indicator at visit s as R is , where R is = 1 if Y is is observed and R is = 0 otherwise . We assume a monotone missingness pattern throughout the paper , i . e . , if the missingness begins at visit s , we have R is (cid:48) = 1 for s (cid:48) < s and R is (cid:48) = 0 for s (cid:48) ≥ s . Denote H is = ( X T i , Y i 1 , · · · , Y is ) T 6 as the history up to visit s , with H i 0 = X i . Since the outliers in the baseline covariates can be identiﬁed and removed by data inspection before further analysis , throughout we assume that no outliers exist in the baseline covariates . However , outliers may exist in the longitudinal outcomes due to data - collection error in the long period of study . In most longitudinal clinical trials with continuous outcomes , the endpoint of interest is the mean diﬀerence of the outcomes at the last visit point between the two treatments . We utilize the pattern - mixture model ( PMM ; Little , 1993 ) framework to express the ATE as a weighted average over the missing patterns , i . e . , τ = E ( Y it | A i = 1 ) − E ( Y it | A i = 0 ) where E ( Y it | A = a ) = (cid:80) t + 1 s = 1 E ( Y it | R is − 1 = 1 , R is = 0 , A i = a ) P ( R is − 1 = 1 , R is = 0 | A i = a ) if we let R i 0 = 1 and R it + 1 = 0 for each individual . The assumed condition regarding the missing components is formed for the identiﬁcation of the pattern - speciﬁc expectation E ( Y it | R is − 1 = 1 , R is = 0 , A i = a ) . We describe one scenario based on the CBI model proposed by Carpenter et al . ( 2013 ) for illustration . 3 . 1 Jump - to - reference imputation model The CBI model ( Carpenter et al . , 2013 ) provides a scenario to model missingness in longitudinal clin - ical trials . We focus on one speciﬁc CBI model as J2R , whose plausibility reveals if the investigators believe that participants who discontinue the treatment have the same outcome mean performance as the ones in the control group with the same covariates . The following assumptions illustrate the J2R imputation model for the ATE identiﬁcation . Assumption 1 ( Partial ignorability of missingness ) R is | = Y is (cid:48) | ( H is − 1 , A i = 0 ) for s (cid:48) ≥ s . Assumption 1 characterizes the MNAR missing mechanism under J2R . The conventional MAR assumption is only required for the missing data in the control group . We do not impose any missing assumptions in the treatment group . Assumption 2 ( J2R outcome mean model ) For individuals who receive treatment a with his - torical information H is − 1 and drop out at visit s , E ( Y it | H is − 1 , R is − 1 = 1 , R is = 0 , A i = a ) = E ( Y it | H is − 1 , A i = 0 ) . Assumption 2 oﬀers a strategy to model the conditional mean of the missing component under J2R . Given the same historical information , the outcome mean will “jump” to the same conditional mean in the control group no matter the prior treatment . Combining with Assumption 1 , the con - ditional expectation E ( Y it | H is − 1 , A i = 0 ) = E (cid:8) · · · E ( Y it | H it − 1 , R it = 1 , A i = 0 ) · · · | H is − 1 , R is = 7 1 , A i = 0 (cid:9) is identiﬁed through a series of sequential regressions on the current outcome against the available historical information . Throughout the paper , we assume a linear relationship between the outcomes and the historical covariates in the J2R imputation model for simplicity . Extensions to nonlinear relationships are manageable , if the sequential regressions of the observed data are ﬁtted in backward order , i . e . , we start from the available data at the last visit point and use the predicted value as the outcome to regress on the previous history recursively to construct the imputation model . The elaboration of the sequential ﬁtting procedure is provided in Section S2 in the supplementary material . 3 . 2 Overview of the existing methods and the drawbacks MI proposed by Rubin ( 2004 ) provides a fully parametric approach to handle missingness under MNAR . Normality is often assumed for its simplicity and robustness against moderate model mis - speciﬁcation in the implementation of MI ( Mehrotra et al . , 2012 ) . One common MI procedure in longitudinal clinical trials under J2R is summarized in the following steps : Step 1 . For the control group , ﬁt the sequential regression for the observed data at each visit point against the available history . Denote the estimated model parameter as ˆ θ s − 1 for s = 1 , · · · , t . Step 2 . Impute missing data sequentially to form M imputed datasets : For individuals who have missing values at visit s , impute Y ( m ) is from the conditional distribution f ( Y is | H ∗ ( m ) is − 1 , A i = 0 ; ˆ θ s − 1 ) estimated in Step 1 , where H ∗ ( m ) is − 1 = ( X T i , Y ∗ ( m ) i 1 , · · · , Y ∗ ( m ) is − 1 ) T and Y ∗ ( m ) is = R is Y is + ( 1 − R is ) Y ( m ) is for m = 1 , · · · , M . Step 3 . For each imputed dataset , perform the complete data analysis by ﬁtting the imputed out - comes at the last visit point on a working analysis model . Denote ˆ τ ( m ) as the ATE estimator of the m th imputed dataset . Step 4 . Combine the estimation results from M imputed datasets and obtain the MI estimator as ˆ τ MI = M − 1 (cid:80) Mm = 1 ˆ τ ( m ) , with the variance estimator by Rubin’s rule as ˆ V ( ˆ τ MI ) = 1 M M (cid:88) m = 1 ˆ V ( ˆ τ ( m ) ) + ( 1 + 1 M ) B M , where B M = ( M − 1 ) − 1 (cid:80) Mm = 1 ( ˆ τ ( m ) − ˆ τ MI ) 2 is the between - imputation variance . Traditionally , the imputation model in Step 1 and the analysis model in Step 3 are obtained from the MMRM analysis , where we assume an underlying normal distribution for both the observed 8 and the imputed data . However , as illustrated in the motivating CD4 count dataset in Section 2 , normality may be violated , leading to a biased estimate of the target ATE parameter . With the consideration of non - normality , Mogg and Mehrotra ( 2007 ) and Mehrotra et al . ( 2012 ) modify the analysis model in Step 3 by replacing the LS estimator with the estimator obtained from the robust loss function . One drawback of MI is that it is fully parametric . The consistency of the MI estimator relies heavily on the correct speciﬁcation of the imputation distribution , i . e . , the conditional distribution given the observed data , which is often assumed to be normal . When a severe deviation from the assumed imputation distribution is detected in the data , the estimation may not be reliable . The possible misspeciﬁcation of the imputation distribution also exists in the “robust” approaches proposed by Mogg and Mehrotra ( 2007 ) and Mehrotra et al . ( 2012 ) , where the imputation model still depends on the normality assumption as required by MI . Moreover , the MI estimator is not eﬃcient in general . The ineﬃciency becomes more serious when it comes to interval estimation . The variance estimation using Rubin’s combining rule may produce an inconsistent variance estimate even when the imputation and analysis models are the same correctly speciﬁed models ( Wang and Robins , 1998 ; Robins and Wang , 2000 ) . Under the MNAR assumption , the overestimation issue raised from Rubin’s variance estimator is more pronounced ( e . g . , Lu , 2014 ; Liu and Pang , 2016 ; Yang and Kim , 2016 ; Guan and Yang , 2019 ; Yang et al . , 2021 ; Liu et al . , 2022 ) . One can resort to the bootstrap variance estimation to obtain a consistent variance estimator , which however exaggerates the computational cost . The unsatisfying performance of MI under non - normality motivates us to develop a robust ap - proach to accommodate the possible model misspeciﬁcation resulting from outliers or heavy - tailed errors without the reliance on parametric models . In the following sections , a weighted robust re - gression model in conjunction with mean imputation is proposed to overcome the issues in MI . 4 Proposed robust method We propose a mean imputation procedure based on robust regression in both the imputation and the analysis models to obtain valid inferences under J2R when the data suﬀers from a heavy tail or extreme outliers . To relax the strong parametric modeling assumption required by MI , mean imputation is preferred . Mehrotra et al . ( 2012 ) shed light on the possibility of incorporating the robust regression in the analysis step of MI to handle non - normality . Based on this idea , we further suggest using the sequential robust regression model in the imputation step to protect against deviations 9 from normality for the observed data . Throughout this section , we focus on the robust estimators obtained from minimizing the robust loss functions such as the Huber loss , the absolute loss ( Huber , 2004 ) , and the ε - insensitive loss ( Smola and Schölkopf , 2004 ) to account for the impact of outliers or heavy - tailed data . To obtain a valid mean - type estimator , a symmetric error distribution assumption is imposed whenever a robust regression is applied . Motivated by the sequential linear regression model under normality , where we regress the current outcomes on the historical information at each visit point to produce the sequential inferences , we develop a sequential robust regression procedure for the observed data to obtain valid inferences that are less likely to be inﬂuenced by non - normality . For the longitudinal data with a monotone miss - ingness pattern , a robust regression is ﬁtted on the observed data at each visit point , incorporating the observed historical information . Speciﬁcally , for the available data at visit s for s = 1 , · · · , t , the imputation model parameter estimate ˆ α s − 1 minimizes the loss function n (cid:88) i = 1 ( 1 − A i ) R is ρ ( Y is − H T is − 1 α s − 1 ) . Here , ρ ( x ) is the robust loss function . For example , the Huber loss function is deﬁned as ρ ( x ) = 0 . 5 x 2 I ( | x | < l ) + (cid:8) l | x | − 0 . 5 l 2 (cid:9) I ( | x | ≥ l ) , where the constant l > 0 controls the inﬂuence of the non - normal data points and I ( · ) is an indicator function ( Huber et al . , 1973 ) . When l → ∞ , the Huber - type robust estimator is equivalent to the conventional LS estimator . We also provide the deﬁnitions of the absolute loss and the ε - insensitive loss in Section S1 . 1 in the supplementary material . Remark 1 ( Tuning constant l in the Huber loss function ) The tuning constant l in the Hu - ber loss function mitigates the impact of extreme values and heavy - tailed errors in the data . Kelly ( 1992 ) argues the existence of trade - oﬀs between the bias and variance in the selection of the tuning constant . A small value of l provides more protections against non - normal values , yet suﬀers from the loss of eﬃciency if the data is indeed normal . In practice , a common recommendation of the tuning constant is l = 1 . 345 σ , where σ is the standard deviation of the errors ( Fox and Monette , 2002 ) . We use the Huber loss function with this tuning parameter to get the robust estimators throughout the simulation studies and real data application . While the estimator from the robust loss function provides protection against extreme outliers in the response variables , it is not robust against outliers in the covariates ( Chang et al . , 2018 ) , leading to an imprecise estimation and a loss of eﬃciency . In longitudinal data with the use of sequential 10 regressions , the issue becomes more profound , where the outcome is treated both as the response variable in the current regression and as the covariate in the subsequent regression . To deal with the outliers in the covariates , we utilize the idea in Carroll and Pederson ( 1993 ) to down - weight the high leverage point in the covariates via a robust Mahalanobis distance . Speciﬁcally , for a p - dimensional covariate X , we calculate the robust Mahalanobis distance as d = ( X − µ ) T V − 1 ( X − µ ) , where µ is a robust estimate of the center and V is a robust estimate of the covariance matrix . The trisquared redescending function is applied to form the assigned weights as w ( u ; ν ) = u (cid:8) 1 − ( u / ν ) 2 (cid:9) 3 I ( | u | ≤ ν ) , where u = ( d / ν ) 1 / 2 and ν is a tuning parameter to control the down - weight level . Therefore , in the sequential weighted robust regression model , the robust estimate ˆ α ws − 1 minimizes the weighted loss n (cid:88) i = 1 ( 1 − A i ) R is w ( H is − 1 ; ν s − 1 ) ρ ( Y is − H T is − 1 α s − 1 ) . ( 1 ) Remark 2 ( Tuning constants in the trisquared redescending function ) When selecting the tuning parameter , Carroll and Pederson ( 1993 ) used a ﬁxed constant ν = 8 to illustrate a speciﬁc down - weight behavior for cross - sectional studies . In longitudinal clinical trials involving multiple weighted sequential regressions , the tuning parameter ν s − 1 can be selected via cross - validation at each visit point , for s = 1 , · · · , t . The main idea is to conduct a K - fold cross - validation for the observed data at each visit point and determine the optimal tuning parameter ν s − 1 which minimizes the squared errors . Speciﬁcally , we ﬁrst partition the observed data at visit s into K parts denoted as P 1 , · · · , P K . The part P j is then left for the test , and the remaining ( K − 1 ) folds are utilized to learn the robust estimator ˆ α ws − 1 , − j , for j = 1 , · · · , K . The optimal ν s − 1 minimizes the cross - validation sum of the squared errors (cid:80) Kj = 1 (cid:80) i ∈ P j ( Y is − H T is − 1 ˆ α ws − 1 , − j ) 2 . After obtaining the robust estimates of the imputation model parameters , we impute the missing components by their conditional outcome means sequentially based on Assumptions 1 and 2 and construct the imputed data Y ∗ is = R is Y is + ( 1 − R is ) H ∗ T is − 1 ˆ α ws − 1 , where H ∗ is − 1 = ( X T i , Y ∗ i 1 , · · · , Y ∗ is − 1 ) T . Complete data analysis is then conducted on the imputed data , where we again minimize the robust loss function to mitigate the impact of outliers in the response variable . Note that since we assume that there are no outliers in the baseline covariates , assigning the weights to the loss function becomes unnecessary . Consider a general form of the working model in the analysis step as µ ( A , X | γ ) = Ag ( X ; γ ( 0 ) ) + h ( X ; γ ( 1 ) ) , ( 2 ) 11 where g ( X ; γ ( 0 ) ) and h ( X ; γ ( 1 ) ) are integrable functions bounded on compact sets , and γ = ( γ ( 0 ) T , γ ( 1 ) T ) T . The robust estimator ˆ γ = ( ˆ γ ( 0 ) T , ˆ γ ( 1 ) T ) T can be found by minimizing the loss n (cid:88) i = 1 ρ { Y ∗ it − µ ( A i , X i | γ ) } , ( 3 ) and the resulting ATE estimator ˆ τ is estimated by the mean diﬀerences between the two groups as ˆ τ = n − 1 (cid:80) ni = 1 g ( X i ; ˆ γ ( 0 ) ) . The modeling form ( 2 ) is commonly satisﬁed in randomized trials when constructing the working model for analysis . For example , the standard analysis model without the interaction term between the treatment and the baseline covariates as µ ( A , X | γ ) = γ ( 0 ) A + γ ( 1 ) T X gratiﬁes this form when g ( X ; γ ( 0 ) ) = γ ( 0 ) and h ( X ; γ ( 1 ) ) = γ ( 1 ) T X ; a similar logic applies to the interaction model µ ( A , X | γ ) = γ ( 0 ) T AX + γ ( 1 ) T X . As we will elaborate in the next section , the ATE estimator ˆ τ is analysis model - robust , in the sense that its asymptotic results stay intact regardless of the speciﬁcation of the analysis model . The implementation of the proposed mean imputation - based robust method is as follows . Step 1 . For the observed data in the control group , ﬁt the sequential weighted robust regression at each visit point and get the sequential model parameter estimates ˆ α ws − 1 by minimizing the weighted loss ( 1 ) for s = 1 , · · · , t . Step 2 . Impute missing data sequentially by the conditional outcome mean according to Assump - tions 1 and 2 and obtain the imputed data Y ∗ is = R is Y is + ( 1 − R is ) H ∗ T is − 1 ˆ α ws − 1 , where H ∗ is − 1 = ( X T i , Y ∗ i 1 , · · · , Y ∗ is − 1 ) T for s = 1 , · · · , t . Step 3 . Set up an appropriate working model µ ( A , X | γ ) in the form ( 2 ) , perform the complete data analysis and get the ATE estimator ˆ τ by minimizing the loss function ( 3 ) . The good theoretical properties of the ATE estimator along with a linearization - based variance estimator are provided in the next section . 5 Theoretical properties and analysis model robustness We present the asymptotic theory of the ATE estimator in terms of consistency and asymptotic normality along with a variance estimator based on three robust loss functions as the Huber loss , the absolute loss , and the ε - insensitive loss . To illustrate the theorems in a straightforward way , we intro - duce additional notations . Denote ϕ ( H is , α s − 1 ) = ( 1 − A i ) R is w ( H is − 1 ; ν s − 1 ) ψ ( Y is − H T is − 1 α s − 1 ) H is − 1 12 as the function derived from minimizing the weighted loss function ( 1 ) in the imputation model , where ψ ( x ) = ∂ρ ( x ) / ∂x is the derivative of the robust loss function , and α s − 1 , 0 as the true parameter such that E { ϕ ( H is , α s − 1 ) | H is − 1 } = 0 . Let ˆ α w = ( ˆ α w T 0 , · · · , ˆ α w T t − 1 ) T be the combination of the model estimators from t sequential regression models in the imputation , and α 0 = ( α T 0 , 0 , · · · , α T t − 1 , 0 ) T be the corresponding true model parameters . In terms of the components in the analysis model , denote ϕ a ( Z i , γ ) = ψ { Y ∗ it − µ ( A i , X i | γ ) } ∂µ ( A i , X i | γ ) / ∂γ T , where Z ∗ i = ( A i , X T i , Y ∗ it ) T represents the imputed data in the model , γ 0 is the true parameter such that E { ϕ a ( Z i , γ ) } = 0 , and τ 0 is the true ATE such that τ 0 = E ( Y it | A = 1 ) − E ( Y it | A = 0 ) . Suppose γ ( 0 ) is a d 0 - dimensional vector , and γ ( 1 ) is a d 1 - dimensional vector . Theorem 1 Under the regularity conditions listed in Section S1 . 1 in the supplementary material , the ATE estimator ˆ τ P −→ τ 0 as the sample size n → ∞ , for s = 1 , · · · , t . Theorem 2 Under the regularity conditions listed in Section S1 . 2 in the supplementary material , as the sample size n → ∞ , √ n ( ˆ τ − τ 0 ) d −→ N (cid:16) 0 , V { V τ , i ( α 0 , γ 0 ) } (cid:17) , where V τ , i ( α 0 , γ 0 ) = (cid:110) ∂g ( X i ; γ ( 0 ) 0 ) / ∂γ T (cid:111) c T V γ , i ( α 0 , γ 0 ) , V γ , i ( α 0 , γ 0 ) = D − 1 ϕ (cid:20) ϕ a { Z ∗ i ( β t ) , γ 0 } + t (cid:88) s = 1 E (cid:26) R s − 1 ( 1 − R s ) ∂µ ( A , X | γ 0 ) ∂γ T ∂ψ ( e ) ∂e H T s − 1 (cid:27) U t , s − 1 , i ( α 0 ) (cid:21) , U t , s − 1 , i ( α 0 ) = ( I p + s − 2 , α s − 1 , 0 ) U t , s , i ( α 0 ) + (cid:0) 0 T p + s − 2 , 1 (cid:1) β t , s q ( H is , α s − 1 , 0 ) for s < t , U t , t − 1 , i ( α 0 ) = q ( H it , α t − 1 , 0 ) , and q ( H is , α s − 1 , 0 ) = (cid:2) − ∂ E (cid:8) ϕ ( H is , α s − 1 , 0 ) H T is − 1 | H is − 1 (cid:9) / ∂α T s − 1 (cid:3) − 1 ϕ ( H is , α s − 1 , 0 ) . Here , c T = ( I d 0 , 0 d 0 × d 1 ) is a matrix where I d 0 is a ( d 0 × d 0 ) - dimensional identity matrix and 0 d 0 × d 1 is a ( d 0 × d 1 ) - dimensional zero matrix , 0 p + s − 2 is a ( p + s − 2 ) - dimensional zero vector , D ϕ = ∂ E [ ϕ a { Z ∗ i ( β t ) , γ 0 } ] / ∂γ T where Z ∗ i ( β t ) = (cid:0) A i , X T i , Y ∗ it ( β t ) (cid:1) T and Y ∗ it ( β t ) refers to the imputed value Y ∗ it based on the true imputation parameters β t = ( β T t , 0 , · · · β T t , t − 1 ) T which satisfy   β t , t − 1 = α t − 1 , 0 if s = t , β t , s − 1 = ( I p + s − 2 , α s − 1 , 0 ) ( I p + s − 1 , α s , 0 ) · · · ( I p + t − 3 , α t − 2 , 0 ) α t − 1 , 0 if s < t , and e i = Y ∗ it ( β t ) − µ ( A i , X i | γ 0 ) . The asymptotic variance in Theorem 2 motivates us to obtain a linearization - based variance 13 estimator by plugging in the estimated values as ˆ V ( ˆ τ ) = 1 n 2 n (cid:88) i = 1 (cid:8) V τ , i ( ˆ α w , ˆ γ ) − ¯ V τ ( ˆ α w , ˆ γ ) (cid:9) 2 , where ¯ V τ ( ˆ α w , ˆ γ ) = n − 1 (cid:80) ni = 1 V τ , i ( ˆ α w , ˆ γ ) , V τ , i ( ˆ α w , ˆ γ ) = (cid:8) ∂g ( X i ; ˆ γ ( 0 ) ) / ∂γ T (cid:9) c T V γ , i ( ˆ α w , ˆ γ ) , V γ , i ( ˆ α w , ˆ γ ) = D − 1 ϕ (cid:20) ϕ a ( Z ∗ i , ˆ γ ) + t (cid:88) s = 1 (cid:40) 1 n n (cid:88) i = 1 R is − 1 ( 1 − R is ) ∂µ ( A i , X i | ˆ γ ) ∂γ T ∂ψ ( ˆ e i ) ∂e i H T is − 1 (cid:41) U t , s − 1 , i ( ˆ α w ) (cid:21) , U t , s − 1 , i ( ˆ α w ) = (cid:0) I p + s − 2 , ˆ α ws − 1 (cid:1) U t , s , i ( ˆ α w ) + (cid:0) 0 T p + s − 2 , 1 (cid:1) ˆ β t , s ˆ q ( H is , ˆ α ws − 1 ) for s < t , U t , t − 1 , i ( ˆ α w ) = ˆ q ( H it , ˆ α wt − 1 ) , and ˆ q ( H is , ˆ α ws − 1 ) = (cid:2) − n − 1 (cid:80) ni = 1 (cid:8) ∂ϕ ( H is , ˆ α ws − 1 ) / ∂α T s − 1 (cid:9) H T is − 1 (cid:3) − 1 ϕ ( H is , ˆ α ws − 1 ) . Also , ˆ e i = Y ∗ it − µ ( A i , X i | ˆ γ ) , ˆ D ϕ = n − 1 (cid:80) ni = 1 ∂ϕ a ( Z ∗ i , ˆ γ ) / ∂γ T , and   ˆ β t , t − 1 = ˆ α wt − 1 if s = t , ˆ β t , s − 1 = ( I p + s − 2 , ˆ α ws − 1 ) ( I p + s − 1 , ˆ α ws ) · · · ( I p + t − 3 , ˆ α wt − 2 ) ˆ α wt − 1 if s < t , for s = 1 , · · · , t . Since the ATE estimator is asymptotically linear , we can also use bootstrap to obtain a replication - based variance estimator . We consider a speciﬁc working model as the interaction model for analysis and present the asymp - totic theories of the ATE estimator in Section S1 . 3 in the supplementary material . The interaction model is one of the most common models in the clinical trials suggested in ICH ( 2021 ) , which is also used in the simulation studies and real data application in the paper . Theorems 1 and 2 extend the test robustness ( Rosenblum and Van Der Laan , 2009 ) to the analysis model robustness in two aspects . First , the robustness expands its plausibility from the hypothesis test to the ATE estimation . Second , the robust estimator obtained from minimizing the loss function further broadens the types of the model estimator used in the analysis model . The resulting ATE estimator via the robust loss remains consistent and has the identical asymptotic normality even when the analysis model is misspeciﬁed . 6 Simulations We conduct simulation studies to validate the ﬁnite - sample performance of the proposed robust method . Consider a longitudinal clinical trial with two treatment groups and ﬁve visits . Set the sample size for each group as 500 and generate the data separately for each treatment . The baseline covariates X ∈ R 2 are a combination of a continuous variable generated from the standard normal 14 distribution and a binary variable generated from a Bernoulli distribution with the success probability of 0 . 3 . The longitudinal outcomes are generated in a sequential manner , regressing on the historical information separately for each group based on some speciﬁc distributions . The group - speciﬁc data generating parameters are given in Section S3 . 1 in the supplementary material . The missingness mechanism is set to be MAR with a monotone missingness pattern . For the visit point s , if R is (cid:48) − 1 = 0 , then R is (cid:48) = 0 for s (cid:48) = s , · · · , t ; otherwise , let R is | ( H is − 1 , A i = a ) ∼ Bernoulli { π s ( a , H is − 1 ) } . We model the observed probability π s ( a , H is − 1 ) at visit s > 1 as a function of the observed information as logit { π s ( a , H is − 1 ) } = φ 1 a + φ 2 a Y is − 1 , where φ 1 a and φ 2 a are the tuning parameters for the observed probabilities . The parameters are tuned to achieve the observed probability around 0 . 8 in each group . We select the Huber loss function to obtain robust estimators for its prevalence . Table 1 ( a ) summarizes the three methods we aim to compare in the simulation studies . We apply distinct estimation approaches for each method in the imputation and analysis models , along with diﬀerent imputation methods , where MI stands for the conventional method used in longitudinal clinical trials and Robust stands for our proposed method . LSE can be viewed as a transition from the conventional MI method to the proposed robust method . In terms of the variance estimation , Rubin’s and bootstrap methods are compared for the MI estimator while the linearization - based and bootstrap variance estimates are compared for the mean imputation estimators . The simulation results are based on 10 , 000 Monte Carlo ( MC ) simulations under H 0 : τ = 0 and 1000 MC simulations under one speciﬁc alternative hypothesis H 1 : τ = τ 0 , with the number of bootstrap replicates B = 100 and the imputation size M = 10 for MI . The tuning parameter of Huber robust regression is l = 1 . 345 σ , and the tuning parameters of the sequential weighted robust models are ν s − 1 = 10 for s = 1 , · · · , 5 . The imputation size M and the tuning parameters ν s − 1 do not have a strong impact on the inferences ( results are not shown ) . We assess the estimators using the point estimate ( Point est ) , the MC variance ( True var ) , the variance estimate ( Var est ) , the relative bias of the variance estimate computed by (cid:104) E (cid:8) ˆ V ( ˆ τ ) (cid:9) − V ( ˆ τ ) (cid:105) / V ( ˆ τ ) , the coverage rate of 95 % conﬁdence interval ( CI ) , the type - 1 error under H 0 , the power under H 1 , and the root mean squared error ( RMSE ) . We choose the 95 % Wald - type CI estimated by (cid:0) ˆ τ − 1 . 96ˆ V 1 / 2 ( ˆ τ ) , ˆ τ + 1 . 96ˆ V 1 / 2 ( ˆ τ ) (cid:1) . 6 . 1 Data with extreme outliers We ﬁrst focus on the settings when the outcomes are generated sequentially from the normal dis - tribution with or without severe outliers . To produce the outliers in the longitudinal outcomes , we randomly select 10 individuals from the top 30 completers with the highest outcomes at the last visit 15 point per group and multiply the original values by three for all post - baseline outcomes . We also consider adding extreme values only to one speciﬁc group and present the results in Section S3 . 2 in the supplementary material . Table 1 ( b ) and the ﬁrst two rows of Figure 3 illustrate the simulation results of the original data and the data with extreme outliers under the normal distribution . Without the presence of outliers , all methods produce unbiased point estimates . The robust method is slightly less eﬃcient compared to MI and LSE , as it has a larger MC variance and a smaller power . For MI , Rubin’s variance estimate is conservative and ineﬃcient , causing the coverage rate to be far away from the empirical value and the power to be smaller , which matches the observations detected in previous literature regarding J2R in longitudinal clinical trials ( e . g . , Liu and Pang , 2016 ; Liu et al . , 2022 ) . However , using bootstrap can ﬁx the overestimation issue and produce a reasonable coverage rate and power . When outliers exist , only the robust method produces an unbiased point estimate , a well - controlled type - 1 error under H 0 , and a satisfying coverage rate under H 1 with a smaller RMSE . 6 . 2 Data from a heavy - tailed distribution To assess the performance of the estimator from our proposed robust method in heavy - tailed distri - butions , we generate the longitudinal outcomes sequentially from a t - distribution with the degrees of freedom as 5 in time order . The detailed setup of the data - generating process is also given in Section S3 . 1 in the supplementary material . Table 1 ( c ) and the last row of Figure 3 show the simulation results . All the methods result in unbiased point estimates . The robust method produces the ATE estimator with the smallest MC variance , indicating the superiority of Huber robust regression under a heavy - tailed distribution . The linearization - based variance estimates behave similarly to the bootstrap variance estimates for the two mean imputation - based methods , with comparable coverage rates and powers . The overall simulation results indicate a recommendation of the proposed robust approach with the linearization - based variance estimation to obtain unbiased point estimates and save computation time . The advocated method works well in terms of consistency , well - controlled type - 1 errors , higher powers under H 1 , and smaller RMSEs . Even under the normality assumption , our proposed method has comparable performance as the conventional MI method , with only a slight loss in the power . When encountering a heavy - tailed distribution or extreme outliers , the proposed method outper - forms with more reasonable coverage rates and higher powers . Similar interpretations apply to the simulation results under H 0 given in Section S3 . 2 in the supplementary material . 16 Table 1 : Summary of the simulation methods and results . ( a ) Diﬀerent estimation and imputation approach in the four methods used for comparison . Method Imputation model Imputation method Analysis model MI LS MI LS LSE Weighted Huber regression Mean imputation LS Robust Weighted Huber regression Mean imputation Huber regression ( b ) Simulation results under the normal distributions without or with extreme outliers . Here the true value τ = 71 . 18 % . Point est True var Var est Relative bias Coverage rate Power RMSE Case Method ( × 10 − 2 ) ( × 10 − 2 ) ( × 10 − 2 ) ( % ) ( % ) ( % ) ( × 10 − 2 ) ˆ V 1 ˆ V Boot ˆ V 1 ˆ V Boot ˆ V 1 ˆ V Boot ˆ V 1 ˆ V Boot No outliers MI 70 . 89 3 . 02 5 . 35 3 . 22 77 . 00 6 . 39 98 . 90 95 . 00 92 . 80 98 . 00 17 . 38 LSE 70 . 93 3 . 03 3 . 25 3 . 15 7 . 14 4 . 04 95 . 40 94 . 80 97 . 70 97 . 80 17 . 40 Robust 70 . 09 3 . 26 3 . 41 3 . 38 4 . 75 3 . 73 95 . 00 94 . 20 96 . 70 96 . 70 18 . 07 Outliers in both groups MI 77 . 42 3 . 92 12 . 42 8 . 97 216 . 36 128 . 51 99 . 70 99 . 10 65 . 90 83 . 60 20 . 76 LSE 74 . 02 4 . 18 6 . 18 5 . 94 47 . 85 42 . 18 98 . 30 97 . 80 89 . 80 91 . 40 20 . 62 Robust 72 . 34 3 . 44 3 . 53 3 . 49 2 . 75 1 . 40 95 . 00 94 . 60 97 . 00 96 . 60 18 . 57 ( c ) Simulation results under the t - distribution . Here the true value τ = 68 . 09 % . Point est True var Var est Relative bias Coverage rate Power RMSE Method ( × 10 − 2 ) ( × 10 − 2 ) ( × 10 − 2 ) ( % ) ( % ) ( % ) ( × 10 − 2 ) ˆ V 1 ˆ V Boot ˆ V 1 ˆ V Boot ˆ V 1 ˆ V Boot ˆ V 1 ˆ V Boot MI 70 . 42 3 . 00 5 . 35 3 . 16 78 . 07 5 . 27 99 . 30 95 . 40 90 . 90 97 . 60 17 . 47 LSE 70 . 54 2 . 90 3 . 21 3 . 06 10 . 49 5 . 43 96 . 30 95 . 30 98 . 00 98 . 10 17 . 20 Robust 69 . 81 2 . 72 2 . 90 2 . 81 6 . 38 3 . 11 94 . 90 94 . 80 98 . 30 98 . 40 16 . 58 ˆ V 1 denotes the variance estimate obtained by Rubin’s rule in MI and linearization in mean imputation - based methods ; ˆ V Boot denotes the bootstrap variance estimates . 17 Figure 3 : Plot for the simulation results under diﬀerent distributions . 0 1 2 3 4 5 MI LSE Robust Method T y pe − 1 e rr o r ( % ) Type−1 error comparison under normal and H0 : tau = 0 0 25 50 75 100 MI LSE Robust Method P o w e r ( % ) Power comparison under normal and H1 : tau = 0 . 7118 0 2 4 MI LSE Robust Method T y pe − 1 e rr o r ( % ) Type−1 error comparison under normal with outliers and H0 : tau = 0 0 25 50 75 100 MI LSE Robust Method P o w e r ( % ) Power comparison under normal with outliers and H1 : tau = 0 . 7118 0 2 4 MI LSE Robust Method T y pe − 1 e rr o r ( % ) Type−1 error comparison under t−distribution and H0 : tau = 0 0 25 50 75 100 MI LSE Robust Method P o w e r ( % ) Power comparison under t−distribution and H1 : tau = 0 . 6809 Var type Rubin Linearization Bootstrap 18 7 Estimating eﬀects of HIV - 1 reverse transcriptase inhibitors We now apply our proposed robust method to the motivating example introduced in Section 2 . The primary goal is to assess the ATE between the two arms at the study endpoint under the J2R condition . The results of the normality test and the symmetry test proposed by Miao et al . ( 2006 ) in Figure 2 indicate that the data are symmetrically distributed without severe outliers , yet suﬀer from a heavy tail that deviates from normality . MI , mean imputation with LS estimators , and the proposed robust method using the Huber loss function are compared with respect to the point estimation , the variance estimation based on Rubin’s variance estimator or the linearization - based variance estimator , Wald - type 95 % CI and CI length . For MI , the imputation size is M = 100 . The tuning parameters for the weights in the robust method are selected via cross - validation , with the procedure described in Section S4 in the supplementary material . Table 2 shows the analysis results of the group means and the ATE under J2R . MI uses the sequential linear regressions estimated by the LS estimators for the imputation model , resulting in diﬀerent point estimates compared to other mean imputation - based methods , where the imputation model is obtained via robust regressions . Using LS or Huber loss in the analysis model also has a slight diﬀerence in the estimation because of the heavy tail . While the conventional MI method may contaminate the inference when the data deviates from the normal distribution , the proposed robust method preserves an unbiased estimate and a narrower CI , which coincides with the conclusions drawn from the simulation studies . All the implemented methods show a statistically signiﬁcant treatment eﬀect under J2R , uncovering the superiority of triple therapies . Table 2 : Analysis of the repeated CD4 count data under J2R . Variable Method Point est 95 % CI CI length Mean arm 1 MI - 0 . 68 ( - 0 . 84 , - 0 . 53 ) 0 . 31 LSE - 0 . 54 ( - 0 . 67 , - 0 . 42 ) 0 . 25 Robust - 0 . 53 ( - 0 . 64 , - 0 . 41 ) 0 . 23 Mean arm 2 MI - 0 . 39 ( - 0 . 55 , - 0 . 24 ) 0 . 31 LSE - 0 . 23 ( - 0 . 35 , - 0 . 11 ) 0 . 24 Robust - 0 . 27 ( - 0 . 38 , - 0 . 16 ) 0 . 22 Diﬀerence MI 0 . 29 ( 0 . 07 , 0 . 51 ) 0 . 44 LSE 0 . 31 ( 0 . 20 , 0 . 41 ) 0 . 21 Robust 0 . 26 ( 0 . 16 , 0 . 35 ) 0 . 19 19 8 Conclusion The non - normality issue frequently occurs in longitudinal clinical trials due to extreme outliers or heavy - tailed errors . With growing attention to evaluating the treatment eﬀect with an MNAR miss - ingness mechanism , we establish a robust method with the weighted robust regression and mean imputation under J2R for the longitudinal data , without the reliance on parametric models . The weighted robust regression provides double - layer protection against non - normality in both the co - variates and the response variable , therefore ensuring a valid imputation model estimator . Mean imputation and the subsequent robust analysis model further guarantee a valid ATE estimator with good theoretical properties . The proposed method also enjoys the analysis model robustness prop - erty , in the sense that the consistency and asymptotic normality of the ATE estimator are satisﬁed even when the analysis model is incorrectly speciﬁed . The symmetry error distribution , which is an essential assumption in the robust regression using the robust loss , must be satisﬁed in order to obtain a grounded inference for the ATE . It may not always be the case in practice . When encountering skewed distributions with asymmetric noises , biases and imprecisions may be detected in our proposed robust method . Takeuchi et al . ( 2002 ) provide a novel robust regression method motivated by data mining to handle asymmetric tails and obtain reasonable mean - type estimators . The extension of the proposed robust method may be plausible by replacing the robust regression with their proposed regression model . While we focus solely on a monotone missingness pattern throughout the development of the robust method , intermittent missingness is also ubiquitous in longitudinal clinical trials . To han - dle intermittent missing data with a non - ignorable missingness mechanism when the outcomes are non - normal , Elashoﬀ et al . ( 2012 ) suggest incorporating the Huber loss function in the pseudo - log - likelihood expression to obtain robust inferences . It is possible to extend our proposed robust method using their idea . We leave it as a future working direction . Acknowledgements Yang is partially supported by the NSF grant DMS 1811245 , NIA grant 1R01AG066883 , and NIEHS grant 1R01ES031651 . Supplementary material The supplementary material contains the proofs and more technical details . 20 References Boos , D . D . and L . A . Stefanski ( 2013 ) . Essential statistical inference : theory and methods , Volume 120 . Springer Science & Business Media . Carpenter , J . R . , J . H . Roger , and M . G . Kenward ( 2013 ) . Analysis of longitudinal trials with protocol deviation : a framework for relevant , accessible assumptions , and inference via multiple imputation . Journal of Biopharmaceutical Statistics 23 ( 6 ) , 1352 – 1371 . Carroll , R . J . and S . Pederson ( 1993 ) . On robustness in the logistic regression model . Journal of the Royal Statistical Society : Series B ( Methodological ) 55 ( 3 ) , 693 – 706 . Chang , L . , S . Roberts , and A . Welsh ( 2018 ) . Robust lasso regression using tukey’s biweight criterion . Technometrics 60 ( 1 ) , 36 – 47 . Cro , S . , T . P . Morris , M . G . Kenward , and J . R . Carpenter ( 2016 ) . Reference - based sensitivity analysis via multiple imputation for longitudinal trials with protocol deviation . The Stata Journal 16 ( 2 ) , 443 – 463 . Elashoﬀ , R . , G . Li , N . Li , and C . - H . Tseng ( 2012 ) . Robust inference for longitudinal data analysis with non - ignorable and non - monotonic missing values . Statistics and Its Interface 5 ( 4 ) , 479 – 490 . Fox , J . and G . Monette ( 2002 ) . An R and S - Plus companion to applied regression . Sage . Guan , Q . and S . Yang ( 2019 ) . A uniﬁed framework for causal inference with multiple imputation using martingale . arXiv preprint arXiv : 1911 . 04663 . Henry , K . , A . Erice , C . Tierney , H . H . Balfour , M . A . Fischl , A . Kmack , S . H . Liou , A . Kenton , M . S . Hirsch , J . Phair , et al . ( 1998 ) . A randomized , controlled , double - blind study comparing the survival beneﬁt of four diﬀerent reverse transcriptase inhibitor therapies ( three - drug , two - drug , and alternating drug ) for the treatment of advanced aids . Journal of acquired immune deﬁciency syndromes and human retrovirology 19 ( 4 ) , 339 – 349 . Huber , P . J . ( 2004 ) . Robust statistics , Volume 523 . John Wiley & Sons . Huber , P . J . et al . ( 1973 ) . Robust regression : asymptotics , conjectures and monte carlo . Annals of Statistics 1 ( 5 ) , 799 – 821 . ICH ( 2021 ) . E9 ( R1 ) statistical principles for clinical trials : Addendum : Estimands and sensitivity analysis in clinical trials . FDA Guidance Documents . 21 Jaeckel , L . A . ( 1972 ) . Estimating regression coeﬃcients by minimizing the dispersion of the residuals . The Annals of Mathematical Statistics , 1449 – 1458 . Jennrich , R . I . ( 1969 ) . Asymptotic properties of non - linear least squares estimators . The Annals of Mathematical Statistics 40 ( 2 ) , 633 – 643 . Kelly , G . E . ( 1992 ) . Robust regression estimators - the choice of tuning constants . Journal of the Royal Statistical Society : Series D ( The Statistician ) 41 ( 3 ) , 303 – 314 . Little , R . and L . Yau ( 1996 ) . Intent - to - treat analysis for longitudinal studies with drop - outs . Bio - metrics , 1324 – 1333 . Little , R . J . ( 1993 ) . Pattern - mixture models for multivariate incomplete data . Journal of the Amer - ican Statistical Association 88 ( 421 ) , 125 – 134 . Little , R . J . , R . D’Agostino , M . L . Cohen , K . Dickersin , S . S . Emerson , J . T . Farrar , C . Frangakis , J . W . Hogan , G . Molenberghs , S . A . Murphy , et al . ( 2012 ) . The prevention and treatment of missing data in clinical trials . New England Journal of Medicine 367 ( 14 ) , 1355 – 1360 . Liu , G . F . and L . Pang ( 2016 ) . On analysis of longitudinal clinical trials with missing data using reference - based imputation . Journal of Biopharmaceutical Statistics 26 ( 5 ) , 924 – 936 . Liu , S . , S . Yang , Y . Zhang , and G . F . Liu ( 2022 ) . Sensitivity analysis in longitudinal clinical trials via distributional imputation . arXiv preprint arXiv : 2203 . 09025 . Lu , K . ( 2014 ) . An analytic method for the placebo - based pattern - mixture model . Statistics in Medicine 33 ( 7 ) , 1134 – 1145 . Mehrotra , D . V . , X . Li , J . Liu , and K . Lu ( 2012 ) . Analysis of longitudinal clinical trials with missing data using multiple imputation in conjunction with robust regression . Biometrics 68 ( 4 ) , 1250 – 1259 . Miao , W . , Y . R . Gel , and J . L . Gastwirth ( 2006 ) . A new test of symmetry about an unknown median . In Random walk , sequential analysis and related topics : A festschrift in Honor of Yuan - Shih Chow , pp . 199 – 214 . World Scientiﬁc . Mogg , R . and D . V . Mehrotra ( 2007 ) . Analysis of antiretroviral immunotherapy trials with potentially non - normal and incomplete longitudinal data . Statistics in medicine 26 ( 3 ) , 484 – 497 . 22 Robins , J . M . and N . Wang ( 2000 ) . Inference for imputation estimators . Biometrika 87 ( 1 ) , 113 – 124 . Rosenblum , M . and M . J . Van Der Laan ( 2009 ) . Using regression models to analyze randomized trials : Asymptotically valid hypothesis tests despite incorrectly speciﬁed models . Biometrics 65 ( 3 ) , 937 – 945 . Rubin , D . B . ( 1976 ) . Inference and missing data . Biometrika 63 ( 3 ) , 581 – 592 . Rubin , D . B . ( 2004 ) . Multiple imputation for nonresponse in surveys , Volume 81 . John Wiley & Sons . Smola , A . J . and B . Schölkopf ( 2004 ) . A tutorial on support vector regression . Statistics and computing 14 ( 3 ) , 199 – 222 . Takeuchi , I . , Y . Bengio , and T . Kanamori ( 2002 ) . Robust regression with asymmetric heavy - tail noise distributions . Neural Computation 14 ( 10 ) , 2469 – 2496 . Tan , P . - T . , S . Cro , E . Van Vogt , M . Szigeti , and V . R . Cornelius ( 2021 ) . A review of the use of controlled multiple imputation in randomised controlled trials with missing outcome data . BMC Medical Research Methodology 21 ( 1 ) , 1 – 17 . Tang , Y . ( 2017 ) . An eﬃcient multiple imputation algorithm for control - based and delta - adjusted pattern mixture models using SAS . Statistics in Biopharmaceutical Research 9 ( 1 ) , 116 – 125 . US Food and Drug Administration ( 2016 ) . Statistical review and evaluation of tresiba and ryzodeg 70 / 30 . https : / / www . fda . gov / media / 102782 / download . Wang , N . and J . M . Robins ( 1998 ) . Large - sample theory for parametric multiple imputation proce - dures . Biometrika 85 ( 4 ) , 935 – 948 . Xiao , W . , H . H . Zhang , and W . Lu ( 2019 ) . Robust regression for optimal individualized treatment rules . Statistics in medicine 38 ( 11 ) , 2059 – 2073 . Yang , S . and J . K . Kim ( 2016 ) . A note on multiple imputation for method of moments estimation . Biometrika 103 ( 1 ) , 244 – 251 . Yang , S . , Y . Zhang , G . F . Liu , and Q . Guan ( 2021 ) . SMIM : a uniﬁed framework of survival sensitivity analysis using multiple imputation and martingale . Biometrics . doi : 10 . 1111 / biom . 13555 . 23 Supplementary material for " Robust analyses for longitudinal clinical trials with missing and non - normal continuous outcomes " by Liu et al . The supplementary material contains technical details , additional simulations , and real - data ap - plication results . Section S1 gives the regularity conditions and the proof of the model - robust ATE estimator obtained from the proposed robust method in terms of consistency and asymptotic normal - ity , and provides an example of the working model for illustration and extensions to other robust loss functions . Section S2 provides the sequential regression procedure . Section S3 shows additional sim - ulation results when the data is incorporated from outliers or diﬀerent data - generating distributions . Section S4 adds additional notes on the real data . S1 Asymptotic results for the ATE estimator In this section , we present the asymptotic properties of the ATE estimator ˆ τ obtained from the proposed robust method in terms of consistency and asymptotic normality . To begin with , we explore the asymptotic properties of ˆ α ws − 1 based on the observed data at visit s in the control group that minimizes the weighted loss function ( 1 ) in the main text . Since the Huber loss is strongly convex , minimizing the loss function is equivalent to ﬁnd the root of the ﬁrst derivative n (cid:88) i = 1 ( 1 − A i ) R is w ( H is − 1 ; ν s − 1 ) ψ ( Y is − H T is − 1 α s − 1 ) H is − 1 = 0 . We give the consistency result for the robust estimator ˆ α ws − 1 in the following lemma . Lemma S1 Assume the following regularity conditions : C1 . There exists a unique α s − 1 , 0 lying in the interior of the Euclidean parameter space Θ , such that the distribution of the observed regression errors ( Y s − H T s − 1 α s − 1 , 0 ) is symmetric around 0 . C2 . E (cid:8) ψ ( Y s − H T s − 1 α s − 1 ) | H s − 1 (cid:9) is dominated by an integrable function g ( H s − 1 ) for all H s − 1 ⊂ R p + s − 1 and α s − 1 with respect to the conditional distribution function f ( Y s | H s − 1 , α s − 1 ) . Then , the estimator ˆ α ws − 1 P −→ α s − 1 , 0 as the sample size n → ∞ , for s = 1 , · · · , t . Proof : Note that by the deﬁnition of Huber function , ψ ( Y s − H T s − 1 α s − 1 ) is a continuous function for α s − 1 and a measurable function for H s . By the regularity condition C2 , it satisﬁes the conditions for S1 Theorem 2 in Jennrich ( 1969 ) . Thus ψ ( Y s − H T s − 1 α s − 1 ) a . s . −−→ E (cid:8) ψ ( Y s − H T s − 1 α s − 1 ) | H k − 1 (cid:9) uniformly for ∀ α s − 1 ∈ Θ . In the weighted sequential robust regression , at s th visit point , the true value β s − 1 , 0 is the unique solution such that E { ( 1 − A ) R s w ( H s − 1 ; q s − 1 ) ψ ( Y s − H T s − 1 α s − 1 ) H s − 1 | H s − 1 } = 0 since it is a randomized trial and by Assumption 1 , E (cid:8) ( 1 − A ) R s w ( H s − 1 ; ν s − 1 ) ψ ( Y s − H T s − 1 α s − 1 ) H s − 1 | H s − 1 (cid:9) = E ( 1 − A ) E ( R s | H s − 1 ) E (cid:8) w ( H s − 1 ; ν s − 1 ) ψ ( Y s − H T s − 1 α s − 1 ) H s − 1 | H s − 1 (cid:9) = E ( 1 − A ) E ( R s | H s − 1 ) E (cid:8) ψ ( Y s − H T s − 1 α s − 1 ) | H s − 1 (cid:9) w ( H s − 1 ; ν s − 1 ) H s − 1 . By the regularity condition C1 , ( Y s − H T s − 1 α s − 1 , 0 ) is symmetric around 0 indicates that E (cid:8) ψ ( Y s − H T s − 1 α s − 1 ) | H s − 1 (cid:9) = 0 . Based on the regularity conditions C1 and C2 , we apply Theorem 7 . 1 in Boos and Stefanski ( 2013 ) and get ˆ α ws − 1 P −→ α s − 1 , 0 , for s = 1 , · · · , t . (cid:3) After obtaining the imputation model estimate ˆ α ws − 1 for each visit point , we conduct sequential mean imputation to the missing components and get Y ∗ s = R s Y s + ( 1 − R s ) H ∗ T s − 1 ˆ α ws − 1 , where H ∗ s − 1 = ( X T , Y ∗ 1 , · · · , Y ∗ s − 1 ) T for s = 1 , · · · , t . Denote the true imputation model parameter needed for imputing the outcome at visit t when the individual drops out at visit s as β t , s − 1 , such that E ( Y t | H s − 1 , R s − 1 = 1 , R s = 0 , A = a ) = H T s − 1 β t , s − 1 for t ≥ s . The following lemma characterizes the relationship between β t , s − 1 and the sequential imputation model parameters α s − 1 , · · · , α t − 1 . Lemma S2 Under the regularity conditions C1 and C2 , the parameter β t , s − 1 in formula ( S2 ) relates to the sequential imputation model parameters α s − 1 , 0 , · · · , α t − 1 , 0 in the following way :   β t , t − 1 = α t − 1 , 0 if s = t , β t , s − 1 = ( I p + s − 2 , α s − 1 , 0 ) ( I p + s − 1 , α s , 0 ) · · · ( I p + t − 3 , α t − 2 , 0 ) α t − 1 , 0 if s < t . ( S1 ) Proof : The regularity condition C1 implies that the distribution of the errors ( Y s − H T s − 1 α s − 1 , 0 ) is symmetric , we have E ( Y s | H s − 1 ) = H T s − 1 α s − 1 , 0 for s = 1 , · · · , t . If the individual in group a drops out at visit t , the missing value at visit t is imputed by H T t − 1 β t , t − 1 = E ( Y t | H t − 1 , R t − 1 = 1 , R t = 0 , A = a ) = E ( Y t | H t − 1 , A = 0 ; α t − 1 , 0 ) ( By Assumption 2 ) S2 = H T t − 1 α t − 1 , 0 ( By C1 ) . if using the true imputation model parameter . We then prove formula ( S1 ) by induction . Suppose the result holds for the individual who drops out at visit s , i . e . , we impute the value at the last visit point by E ( Y t | H s − 1 , A = 0 ) = H T s − 1 β t , s − 1 . Then for the one in group a who drops out at visit s − 1 , the missing outcome at visit s is imputed by H T s − 2 β t , s − 2 = E ( Y t | H s − 2 , R s − 2 = 1 , R s − 1 = 0 , A = a ) = E ( Y t | H s − 2 , A = 0 ) ( By Assumption 2 ) = E { E ( Y t | H s − 1 , A = 0 ) | H s − 2 , A = 0 } = E (cid:0) H T s − 1 β t , s − 1 | H s − 2 , A = 0 (cid:1) = (cid:0) H T s − 2 , E ( Y s − 1 | H s − 2 , A = 0 ) (cid:1) T β t , s − 1 = H T s − 2 ( I p + s − 2 , α s − 2 , 0 ) β t , s − 1 . Then we have β t , s − 2 = ( I p + s − 2 , α s − 2 , 0 ) β t , s − 1 = ( I p + s − 2 , α s − 2 , 0 ) ( I p + s − 2 , α s − 1 , 0 ) ( I p + s − 1 , α s , 0 ) · · · ( I p + t − 3 , α t − 2 , 0 ) α t − 1 , 0 , which completes the proof . (cid:3) Lemma S2 suggests an estimator of β t , s − 1 by plugging in the sequential imputation model param - eter estimates ˆ α w s − 1 , · · · , ˆ α w t − 1 in formula ( S1 ) . Set R 0 = 1 , we can rewrite the imputed value Y ∗ t at the last visit point based on the observed history , the dropout pattern , and the estimated imputation model parameters as Y ∗ t = R t Y t + t (cid:88) s = 1 R s − 1 ( 1 − R s ) H T s − 1 ˆ β t , s − 1 , ( S2 ) where ˆ β t , s − 1 is the estimate of β t , s − 1 . We proceed to prove the consistency of the ATE estimator ˆ τ . S1 . 1 Proof of Theorem 1 To illustrate the dependence of the imputed value Y ∗ t with the imputed parameter estimates ˆ β t : = ( ˆ β t , 0 , · · · , ˆ β t , t − 1 ) T , we rewrite Y ∗ t as Y ∗ t ( ˆ β t ) and Z ∗ as Z ∗ ( ˆ β t ) . Denote β t : = ( β t , 0 , · · · , β t , t − 1 ) T as the true value . S3 We do not assume the correct model form for the analysis model , instead , we give a wide range of models of the form ( 2 ) in the main text . When a symmetric error distribution is imposed , we write the model as Y ∗ t ( ˆ β t ) = µ ( A , X | γ 0 ) + ε , = ( A − π ) g ( X ; γ ( 0 ) 0 ) + ˜ h ( X ) + ε ( S3 ) where ε is the error term and is symmetric around 0 , and ˜ h ( X ) = πg ( X ; γ ( 0 ) 0 ) + h ( X ; γ ( 1 ) 0 ) . Under the symmetric error assumption , γ ( 0 ) 0 satisﬁes that E (cid:8) g ( X ; γ ( 0 ) ) (cid:9) = E ( Y t | A = 1 ) − E ( Y t | A = 0 ) = τ 0 . The robust estimator ˆ γ = ( ˆ γ ( 0 ) T , ˆ γ ( 1 ) T ) T is obtained from (cid:16) ˆ γ ( 0 ) , ˆ γ ( 1 ) (cid:17) = arg min 1 n n (cid:88) i = 1 ρ (cid:110) Y ∗ it ( ˆ β t ) − µ ( A , X | γ ) (cid:111) . We now want to show that ˆ γ ( 0 ) obtained from the robust loss function satisﬁes that ˆ τ = (cid:80) ni = 1 g ( X i ; ˆ γ ( 0 ) ) P −→ τ 0 . Before restating Theorem 1 in the main text with technical details , we ﬁrst give the deﬁnitions of the two robust loss functions as the absolute loss and the ε - insensitive loss . The absolute loss function is deﬁned as ρ a ( x ) = | x | . The ε - insensitive loss is deﬁned as L ε ( x ) = max { | x | − ε , 0 } , where the constant ε > 0 provides a tolerance margin where no penalties are given ( Smola and Schölkopf , 2004 ) . Theorem S1 Under the regularity conditions C1 and C2 , and assume the following regularity con - ditions holds for a = 0 , 1 : C3 . Given the baseline covariates , the error term is conditionally independent with the treatment variable , i . e . , ε | = A | X . C4 . For any ζ , the term K 1 : = ˜ h ( X ) + ε i − ˜ h ( X ; ζ ) has an expectation and a ﬁnite second moment , i . e . , E | K 1 | < ∞ and E ( K 21 ) < ∞ , where ˜ h ( X ) = πg ( X ; γ ( 0 ) 0 ) + h ( X ) , ˜ h ( X ; ζ ) is a parametric model of ˜ h ( X ) , and γ ( 0 ) 0 is the unique solution such that E (cid:8) g ( X ; γ ( 0 ) ) (cid:9) = τ 0 . C5 . For any γ ( 0 ) , the term K 2 : = ( A − π ) (cid:110) g ( X ; γ ( 0 ) ) − g ( X ; γ ( 0 ) 0 ) (cid:111) has an expectation and a ﬁnite second moment , i . e . , E | K 2 | < ∞ and E ( K 22 ) < ∞ . C6 . For any f j ( X ) , P (cid:110) X : g ( X ; γ ( 0 ) ) − g ( X ; γ ( 0 ) 0 ) (cid:54) = 0 (cid:111) > 0 , for ∀ γ ( 0 ) (cid:54) = 0 . C7 . The error term ε | X = x has a non - zero density function . S4 C8 . The function G 2 ( ζ ) : = E [ ρ ( K 1 ) ] has a unique global minimizer ζ ∗ . Then , the ATE estimator ˆ τ P −→ τ 0 as the sample size n → ∞ , for s = 1 , · · · , t . Proof : We begin the proof by rewriting the working model ( 2 ) . Note that µ ( A , X | γ ) = Ag ( X ; γ ( 0 ) ) + h ( X ; γ ( 1 ) ) = ( A − π ) g ( X ; γ ( 0 ) ) + πg ( X ; γ ( 0 ) ) + h ( X ; γ ( 1 ) ) = ( A − π ) g ( X ; γ ( 0 ) ) + ˜ h ( X ; ζ ) , ( S4 ) where ˜ h ( X ; ζ ) = πg ( X ; γ ( 0 ) ) + h ( X ; γ ( 1 ) ) and ζ combines the parameters γ ( 0 ) and γ ( 1 ) . We are interested in estimating g ( X ; γ ( 0 ) ) , as it is the only part that connects with the ATE estimation . We want to prove that ˆ γ ( 0 ) obtained from minimizing the Huber loss function satisﬁes that ˆ γ ( 0 ) P −→ γ ( 0 ) 0 , regardless of the model speciﬁcation . By Lemmas S1 and S2 , we have ˆ β t P −→ β t . Follow the similar proof in Lemma S1 , by continuous mapping theorem , we have Y ∗ t ( ˆ β t ) = Y ∗ t ( β t ) + o P ( 1 ) = ( A − π ) g ( X ; γ ( 0 ) 0 ) + ˜ h ( X ) + ε + o P ( 1 ) . Therefore , minimizing the loss function n − 1 (cid:80) ni = 1 ρ (cid:8) Y i ( ˆ β t ) − µ ( A i , X i | γ ) (cid:9) is asymptotically equivalent to minimizing n − 1 (cid:80) ni = 1 ρ (cid:8) Y i ( β t ) − µ ( A i , X i | γ ) (cid:9) . We then follow the proof in Xiao et al . ( 2019 ) to verify the consistency of the estimator under H 0 using the Huber loss function , the absolute loss function , or the ε - insensitive loss . ( i ) For the Huber loss , denote L n ( γ ( 0 ) , ζ ) = n − 1 (cid:80) ni = 1 ρ { Y it ( β t ) − µ ( A i , X i | γ ) } , where ρ ( x ) = 0 . 5 x 2 I ( | x | < l ) + (cid:8) l | x | − 0 . 5 l 2 (cid:9) I ( | x | ≥ l ) . Then the estimator based on the Huber loss is ( ˆ γ ( 0 ) , ˆ ζ ) = argmin L n ( γ ( 0 ) , ζ ) = argmin (cid:110) L n ( γ ( 0 ) , ζ ) − L n ( γ ( 0 ) 0 , ζ ) (cid:111) + (cid:110) L n ( γ ( 0 ) 0 , ζ ) − L n ( γ ( 0 ) 0 , ζ (cid:48) ) (cid:111) , ( S5 ) where ζ (cid:48) is a ﬁxed value . We examine the two terms in the objective function ( S5 ) separately . For the ﬁrst term in the function ( S5 ) , L n ( γ ( 0 ) , ζ ) − L n ( γ ( 0 ) 0 , ζ ) = 1 n n (cid:88) i = 1 ρ (cid:104) ˜ h ( X ) + ε i − ˜ h ( X ; ζ ) − ( A − π ) (cid:110) g ( X ; γ ( 0 ) ) − g ( X ; γ ( 0 ) 0 ) (cid:111)(cid:105) − ρ (cid:104) ˜ h ( X ) + ε i − ˜ h ( X ; ζ ) − ( A − π ) (cid:110) g ( X ; γ ( 0 ) ) − g ( X ; γ ( 0 ) 0 ) (cid:111)(cid:105) : = 1 n n (cid:88) i = 1 d i 1 . S5 The regularity conditions C4 and C5 allow us to apply the weak law of large number ( WLLN ) to L n ( γ ( 0 ) , ζ ) − L n ( γ ( 0 ) 0 , ζ ) , since | d i 1 | ≤ (cid:12)(cid:12) 1 2 ( K 1 − K 2 ) 2 − l | K 1 | + 1 2 l 2 (cid:12)(cid:12) = 1 2 ( K 1 − l ) 2 + | K 2 | | K 2 − 2 K 1 | has a ﬁnite expectation . Thus , by WLLN , L n ( γ ( 0 ) , ζ ) − L n ( γ ( 0 ) 0 , ζ ) P −→ E ( D 1 ) = G 1 ( γ ( 0 ) , ζ ) , where D 1 = ρ ( K 1 − K 2 ) − ρ ( K 1 ) . We claim that G 1 ( γ ( 0 ) , ζ ) ≥ 0 and reaches 0 if and only if γ ( 0 ) = γ ( 0 ) 0 . First , note that G 1 ( γ ( 0 ) 0 , ζ ) = ρ ( K 1 ) − ρ ( K 1 ) = 0 . We proceed to prove that G 1 ( γ ( 0 ) , ζ ) > 0 for ∀ γ ( 0 ) (cid:54) = γ ( 0 ) 0 and consider the following four cases : ( a ) If K 1 > l , we have D 1 = ρ ( K 1 − K 2 ) − ρ ( K 1 ) ≥ l ( K 1 − K 2 ) − 1 2 l 2 − (cid:18) lK 1 − 1 2 l 2 (cid:19) = − lK 2 . ( b ) If K 1 < − l , repeat the step in 1 and we can get D 1 ≥ lK 2 . ( c ) If K 1 ∈ [ − l , l ] and K 1 − K 2 ∈ [ − l , l ] , then D 1 = 1 2 ( K 1 − K 2 ) 2 − 1 2 K 21 = − K 1 K 2 + 1 2 K 22 . ( d ) If K 1 ∈ [ − l , l ] and K 1 − K 2 / ∈ [ − l , l ] , then D 1 = l | K 1 − K 2 | − 1 2 l 2 − 1 2 K 21 = 1 2 ( K 1 − K 2 ) 2 − 1 2 ( | K 1 − K 2 | − l ) 2 − 1 2 K 2 1 ≥ 1 2 ( K 1 − K 2 ) 2 − 1 2 K 22 − 1 2 K 21 = − K 1 K 2 . The inequality holds since by triangle inequality , 0 < | K 1 − K 2 | − l ≤ K 1 + | K 2 | − l ≤ | K 2 | . Incorporating the four cases together and taking expectations , we have G 1 ( γ ( 0 ) , ζ ) ≥ E { − lK 2 I ( K 1 > l ) } + E { lK 2 I ( K 1 < − l ) } + E (cid:26)(cid:18) − K 1 K 2 + 1 2 K 22 (cid:19) I ( K 1 ∈ [ − l , l ] ) I ( K 1 − K 2 ∈ [ − l , l ] ) (cid:27) + E { − K 1 K 2 I ( K 1 ∈ [ − l , l ] ) I ( K 1 − K 2 / ∈ [ − l , l ] ) } S6 ≥ E { − lK 2 I ( K 1 > l ) } + E { lK 2 I ( K 1 < − l ) } + E { − K 1 K 2 I ( K 1 ∈ [ − l , l ] ) } + E (cid:26) 1 2 K 22 I ( K 1 ∈ [ − l , l ] ) I ( K 1 − K 2 ∈ [ − l , l ] ) (cid:27) . Note that by the regularity condition C3 , P ( A ) = π , and A | = X ( randomized trial ) , we have E { − lK 2 I ( K 1 > l ) } = − l E [ E ( K 2 | X ) E { I ( K 1 > l ) | X } ] = − l E (cid:16) E (cid:104) ( A − π ) (cid:110) g ( X ; γ ( 0 ) ) − g ( X ; γ ( 0 ) 0 ) (cid:111) | X (cid:105) E { I ( K 1 > l ) | X } (cid:17) = − l E (cid:104) E ( A i − π ) (cid:110) g ( X ; γ ( 0 ) ) − g ( X ; γ ( 0 ) 0 ) (cid:111) E { I ( K 1 > l ) | X } (cid:105) = 0 . Similarly , we have E { lK 2 I ( K 1 < − l ) } = E { − K 1 K 2 I ( K 1 ∈ [ − l , l ] ) } = 0 . Therefore , G 1 ( γ ( 0 ) , ζ ) ≥ E (cid:26) 1 2 K 22 I ( K 1 ∈ [ − l , l ] ) I ( K 1 − K 2 ∈ [ − l , l ] ) (cid:27) . By the regularity conditions C6 and C7 , we know that G 1 ( γ ( 0 ) , ζ ) > 0 for ∀ γ ( 0 ) (cid:54) = γ ( 0 ) 0 . For the second term in the function ( S5 ) , denote L n ( γ ( 0 ) 0 , ζ ) − L n ( γ ( 0 ) 0 , ζ (cid:48) ) = n − 1 (cid:80) ni = 1 d i 2 . By the regularity condition C4 , WLLN is applied , and we have L n ( γ ( 0 ) 0 , ζ ) − L n ( γ ( 0 ) 0 , ζ (cid:48) ) P −→ E ( D 2 ) = G 2 ( ζ ) . The results for the ﬁrst term combined with the regularity condition C8 implies that ( γ ( 0 ) 0 , ζ ∗ ) is the unique minimizer of G 1 ( γ ( 0 ) , ζ ) + G 2 ( ζ ) . Since the Huber loss function is strongly convex , by the argmax continuous mapping theorem , we have ˆ γ ( 0 ) P −→ γ ( 0 ) 0 . By continuous mapping theorem , ˆ τ = n − 1 (cid:80) ni = 1 g ( X i ; ˆ γ ( 0 ) ) P −→ E (cid:8) g ( X i ; γ ( 0 ) ) (cid:9) = τ 0 . ( ii ) For the absolute loss , denote L a , n ( γ ( 0 ) , ζ ) = n − 1 (cid:80) ni = 1 ρ a { Y i − µ ( A , X | γ ) } , where ρ a ( x ) = | x | is the absolute loss . Then the estimator based on ρ a ( x ) is ( ˆ γ ( 0 ) , ˆ ζ ) = argmin L a , n ( γ ( 0 ) , ζ ) = argmin (cid:110) L a , n ( γ ( 0 ) , ζ ) − L a , n ( γ ( 0 ) 0 , ζ ) (cid:111) + (cid:110) L a , n ( γ ( 0 ) 0 , ζ ) − L a , n ( γ ( 0 ) 0 , ζ (cid:48) ) (cid:111) , ( S6 ) where ζ (cid:48) is a ﬁxed value . We again examine the two terms in the objective function ( S6 ) separately . For the ﬁrst term in the function ( S6 ) , L a , n ( γ ( 0 ) , ζ ) − L a , n ( γ ( 0 ) 0 , ζ ) = 1 n n (cid:88) i = 1 (cid:18) ρ a (cid:104) ˜ h ( X ) + ε i − ˜ h ( X ; ζ ) − ( A − π ) (cid:110) g ( X ; γ ( 0 ) ) − g ( X ; γ ( 0 ) 0 ) (cid:111)(cid:105) − ρ a (cid:104) ˜ h ( X ) + ε i − ˜ h ( X ; ζ ) − ( A − π ) (cid:110) g ( X ; γ ( 0 ) ) − g ( X ; γ ( 0 ) 0 ) (cid:111)(cid:105) (cid:19) S7 : = 1 n n (cid:88) i = 1 d i 1 . The regularity conditions C4 and C5 allow us to apply WLLN to L a , n ( γ ( 0 ) , ζ ) − L a , n ( γ ( 0 ) 0 , ζ ) , since | d i 1 | ≤ (cid:12)(cid:12) K 1 − K 2 | + | K 1 | ≤ 2 | K 1 | + | K 2 | has a ﬁnite expectation . Thus , by WLLN , L a , n ( γ ( 0 ) , ζ ) − L a , n ( γ ( 0 ) 0 , ζ ) P −→ E ( D 1 ) = G 1 ( γ ( 0 ) , ζ ) , where D 1 = ρ a ( K 1 − K 2 ) − ρ a ( K 1 ) . First , note that G 1 ( γ ( 0 ) 0 , ζ ) = ρ a ( K 1 ) − ρ a ( K 1 ) = 0 . We proceed to prove that G 1 ( γ ( 0 ) , ζ ) > 0 for ∀ γ ( 0 ) (cid:54) = γ ( 0 ) 0 and consider the following two cases : ( a ) If K 1 ≥ 0 , we have D 1 = | K 1 − K 2 | − | K 1 | ≥ K 1 − K 2 − K 1 = − K 2 . ( b ) If K 1 < 0 , we have D 1 = | K 1 − K 2 | − | K 1 | ≥ K 2 − K 1 + K 1 = K 2 . Incorporating the four cases together and taking expectations , we have G 1 ( γ ( 0 ) , ζ ) ≥ E (cid:8) − K 2 I ( K 1 ≥ 0 ) (cid:9) + E { K 2 I ( K 1 < 0 ) } . Follow the same proof , we have E { − K 2 I ( K 1 ≥ 0 ) } = E { K 2 I ( K 1 < 0 ) } = 0 . By the regularity conditions C6 and C7 , we know that G 1 ( γ ( 0 ) , η ) > 0 for ∀ γ ( 0 ) (cid:54) = γ ( 0 ) 0 . The remaining proof follows similar steps the proof for the Huber loss . ( iii ) For the ε - insensitive loss , denote L ε , n ( γ ( 0 ) , ζ ) = n − 1 (cid:80) ni = 1 L ε { Y i − µ ( A , X | γ ) } , where L ε ( x ) = max { | x | − ε , 0 } is the ε - insensitive loss . Then the estimator based on L ε ( x ) is ( ˆ γ ( 0 ) , ˆ ζ ) = argmin L ε , n ( γ ( 0 ) , ζ ) = argmin (cid:110) L ε , n ( γ ( 0 ) , ζ ) − L ε , n ( γ ( 0 ) 0 , ζ ) (cid:111) + (cid:110) L ε , n ( γ ( 0 ) 0 , ζ ) − L ε , n ( γ ( 0 ) 0 , ζ (cid:48) ) (cid:111) , ( S7 ) where ζ (cid:48) is a ﬁxed value . We again examine the two terms in the objective function ( S7 ) separately . For the ﬁrst term in the function ( S7 ) , L ε , n ( γ ( 0 ) , ζ ) − L ε , n ( γ ( 0 ) 0 , ζ ) = 1 n n (cid:88) i = 1 (cid:18) L ε (cid:104) ˜ h ( X ) + ε i − ˜ h ( X ; ζ ) − ( A − π ) (cid:110) g ( X ; γ ( 0 ) ) − g ( X ; γ ( 0 ) 0 ) (cid:111)(cid:105) − L ε (cid:104) ˜ h ( X ) + ε i − ˜ h ( X ; ζ ) − ( A − π ) (cid:110) g ( X ; γ ( 0 ) ) − g ( X ; γ ( 0 ) 0 ) (cid:111)(cid:105) (cid:19) : = 1 n n (cid:88) i = 1 d i 1 . The regularity conditions C4 and C5 allow us to apply WLLN to L ε , n ( γ ( 0 ) , ζ ) − L ε , n ( γ ( 0 ) 0 , ζ ) , since | d i 1 | ≤ (cid:12)(cid:12) K 1 − K 2 | + ε + | K 1 | + ε ≤ 2 | K 1 | + | K 2 | + 2 ε has a ﬁnite expectation . Thus , by WLLN , L ε , n ( γ ( 0 ) , ζ ) − L ε , n ( γ ( 0 ) 0 , ζ ) P −→ E ( D 1 ) = G 1 ( γ ( 0 ) , ζ ) , where D 1 = L ε ( K 1 − K 2 ) − L ε ( K 1 ) . S8 First , note that G 1 ( γ ( 0 ) 0 , ζ ) = L ε ( K 1 ) − L ε ( K 1 ) = 0 . We proceed to prove that G 1 ( γ ( 0 ) , ζ ) > 0 for ∀ γ ( 0 ) (cid:54) = γ ( 0 ) 0 and consider the following two cases : ( a ) If K 1 ≥ ε , we have D 1 = max ( | K 1 − K 2 | − ε , 0 ) − K 1 + ε ≥ | K 1 − K 2 | − | K 1 | ≥ K 1 − K 2 − K 1 = − K 2 . ( b ) If K 1 ≤ − ε , we have D 1 = max ( | K 1 − K 2 | − ε , 0 ) + K 1 + ε ≥ | K 1 − K 2 | − | K 1 | ≥ K 2 − K 1 + K 1 = K 2 . ( c ) If | K 1 | < ε , we have D 1 = max ( | K 1 − K 2 | − ε , 0 ) − 0 ≥ 0 . Incorporating the four cases together and taking expectations , we have G 1 ( γ ( 0 ) , ζ ) ≥ E (cid:8) − K 2 I ( K 1 ≥ ε ) (cid:9) + E { K 2 I ( K 1 ≤ − ε ) } . Follow the same proof , we have E { − K 2 I ( K 1 ≥ ε ) } = E { K 2 I ( K 1 ≤ ε ) } = 0 . By the regularity conditions C6 and C7 , we know that G 1 ( γ ( 0 ) , ζ ) > 0 for ∀ γ ( 0 ) (cid:54) = γ ( 0 ) 0 . The remaining proof follows similar steps the proof for the Huber loss . (cid:3) S1 . 2 Proof of Theorem 2 To explore the asymptotic normality of the estimator ˆ τ , we ﬁrst focus on the asymptotic normality of ˆ α ws − 1 for s = 1 , · · · , t . Denote ϕ ( H s , α s − 1 ) : = ( 1 − A ) R s w ( H s − 1 ; ρ s − 1 ) ψ ( Y s − H T s − 1 α s − 1 ) H s − 1 , where ψ ( x ) = ∂ρ ( x ) / ∂x is the derivative of the robust loss function . Therefore , ˆ α ws − 1 is the solution to the estimating equations (cid:80) ni = 1 ϕ ( H is , α s − 1 ) = 0 . Lemma S3 Assume the regularity conditions C1 and C2 and the following conditions : C9 . The partial derivative E (cid:8) ψ ( Y s − H T s − 1 α s − 1 ) | H s − 1 (cid:9) with respect to α s − 1 exists and is continu - ous around α s − 1 , 0 almost everywhere . The second derivative of E (cid:8) ψ ( Y s − H T s − 1 α s − 1 ) | H s − 1 (cid:9) with respect to α s − 1 is continuous and dominated by some integrable functions ; C10 . The partial derivative of E { ϕ ( H s , α s − 1 ) | H s − 1 } with respect to α s − 1 is nonsingular . C11 . The variance V { q ( H s , α s − 1 , 0 ) } is ﬁnite , where q ( H s , α s − 1 , 0 ) = (cid:34) − ∂ E (cid:8) ϕ ( H is , α s − 1 , 0 ) H T is − 1 | H is − 1 (cid:9) ∂α T s − 1 (cid:35) − 1 ϕ ( H s , α s − 1 , 0 ) . Then , for s = 1 , · · · , t , as the sample size n → ∞ , √ n ( ˆ α ws − 1 − α s − 1 , 0 ) d −→ N (cid:16) 0 , V { q ( H s , α s − 1 , 0 ) } (cid:17) . S9 Proof : Consider a Taylor expansion of the function R s ϕ ( H s , ˆ α ws − 1 ) with respect to ˆ α ws − 1 around α s − 1 , 0 for s = 1 , · · · , t , under the regularity conditions C1 , C9 and C10 , we have the linearization form of ˆ α ws − 1 as ˆ α ws − 1 − α s − 1 , 0 = 1 n n (cid:88) i = 1 (cid:34) − ∂ E (cid:8) ϕ ( H is , α s − 1 , 0 ) H T is − 1 | H is − 1 (cid:9) ∂α T s − 1 (cid:35) − 1 ϕ ( H is , α s − 1 , 0 ) + o P ( n − 1 / 2 ) = 1 n n (cid:88) i = 1 q ( H is , α s − 1 , 0 ) + o P ( n − 1 / 2 ) . Under the regularity condition C11 , we apply the central limit theorem and get the asymptotic distribution of ˆ α ws − 1 . (cid:3) For simplicity of the notations , denote α 0 = ( α T 0 , 0 , · · · , α T t − 1 , 0 ) T as the true model parameters from t sequential regression models . Based on Lemmas S2 and S3 , we can further obtain the asymptotic normality of ˆ β t , s for s = 1 , · · · , t in the following lemma . Lemma S4 Assume the regularity conditions C1 – C11 and the following conditions : C12 . The variance V { U t , s − 1 , i ( α 0 ) } is ﬁnite , where U t , s − 1 , i ( α ) is the linearization form produced by ˆ β t , s − 1 , i . e . , ˆ β t , s − 1 − β t , s − 1 = n − 1 (cid:80) ni = 1 U t , s − 1 , i ( α 0 ) + o P ( n − 1 / 2 ) . Speciﬁcally , U t , s , i ( α 0 ) = ( I p + s − 2 , α s − 1 , 0 ) U t , s + 1 , i ( α 0 ) + (cid:0) 0 T p + s − 2 , 1 (cid:1) β t , s q ( H is , α s − 1 , 0 ) and U t , t − 1 , i ( α 0 ) = q ( H it , α t − 1 , 0 ) . Then , as the sample size n → ∞ , for s = 1 , · · · , t , √ n ( ˆ β t , s − 1 − β t , s − 1 ) d −→ N (cid:16) 0 , V { U t , s − 1 , i ( α 0 ) } (cid:17) . Proof : Lemma S2 indicates that ˆ β t , t − 1 = ˆ α w t − 1 , thus ˆ β t , t − 1 shares the same linearization form as ˆ α w t − 1 , i . e . , ˆ β t , t − 1 − β t , t − 1 = 1 n n (cid:88) i = 1 q ( H it , α t − 1 , 0 ) + o P ( n − 1 / 2 ) = 1 n n (cid:88) i = 1 U t , t − 1 , i ( α 0 ) + o P ( n − 1 / 2 ) . For the individuals who drop out at visit t − 1 , the corresponding imputation parameter estimate ˆ β t , t − 2 can be expressed as ˆ β t , t − 2 = (cid:0) I p + t − 3 , ˆ α wt − 2 , 0 (cid:1) ˆ α wt − 1 S10 = (cid:0) I p + t − 3 , ˆ α wt − 2 , 0 (cid:1) ˆ β t , t − 1 = (cid:0) I p + t − 3 , 0 T p + t − 3 (cid:1) ˆ β t , t − 1 + ˆ α wt − 2 (cid:0) 0 T p + t − 3 , 1 (cid:1) ˆ β t , t − 1 . ( S8 ) The linearization form of the ﬁrst term in formula ( S8 ) can be obtained directly via delta - method as (cid:0) I p + t − 3 , 0 T p + t − 3 (cid:1) ˆ β t , t − 1 − (cid:0) I p + t − 3 , 0 T p + t − 3 (cid:1) β t , t − 1 = 1 n n (cid:88) i = 1 (cid:0) I p + t − 3 , 0 T p + t − 3 (cid:1) q ( H it , α t − 1 , 0 ) + o P ( n − 1 / 2 ) . For the second term , let g t − 2 ( α t − 2 , β t , t − 1 ) : = α t − 2 (cid:0) 0 T p + t − 3 , 1 (cid:1) β t , t − 1 . Then we have ∇ g t − 2 (cid:12)(cid:12)(cid:12) ( α t − 2 , 0 , β t , t − 1 ) = (cid:8)(cid:0) 0 T p + t − 3 , 1 (cid:1) β t , t − 1 , α t − 2 , 0 (cid:0) 0 T p + t − 3 , 1 (cid:1)(cid:9) T . Under the regularity condition C9 by Theorem 5 . 27 in Boos and Stefanski ( 2013 ) , we have g t − 2 ( ˆ α wt − 2 , ˆ β t , t − 1 ) − g t − 2 ( α t − 2 , 0 , β t , t − 1 ) = 1 n n (cid:88) i = 1 ∇ g T t − 2 (cid:12)(cid:12)(cid:12) ( α t − 2 , 0 , β t , t − 1 )   q ( H it − 1 , α t − 2 , 0 ) U t , t − 1 , i ( α 0 )   + o P ( n − 1 / 2 ) = 1 n n (cid:88) i = 1 (cid:0) 0 T p + t − 3 , 1 (cid:1) β t , t − 1 q ( H it − 1 , α t − 2 , 0 ) + α t − 2 , 0 (cid:0) 0 T p + t − 3 , 1 (cid:1) U t , t − 1 , i ( α 0 ) + o P ( n − 1 / 2 ) . Combine the two terms together , we have ˆ β t , t − 2 − β t , t − 2 = 1 n n (cid:88) i = 1 (cid:0) 0 T p + t − 3 , 1 (cid:1) β t , t − 1 q ( H it − 1 , α t − 2 , 0 ) + (cid:8)(cid:0) I p + t − 3 , 0 T p + t − 3 (cid:1) + α t − 2 , 0 (cid:0) 0 T p + t − 3 , 1 (cid:1)(cid:9) U t , t − 1 , i ( α 0 ) + o P ( n − 1 / 2 ) = 1 n n (cid:88) i = 1 ( I p + t − 3 , α t − 2 , 0 ) U t , t − 1 , i ( α 0 ) + (cid:0) 0 T p + t − 3 , 1 (cid:1) β t , t − 1 q ( H it − 1 , α t − 2 , 0 ) + o P ( n − 1 / 2 ) = 1 n n (cid:88) i = 1 U t , t − 2 , i ( α 0 ) + o P ( n − 1 / 2 ) , which matches the result in the lemma when s = t − 2 . We then prove the lemma by induction . Suppose the result holds for the individual who drops out at visit s + 1 , i . e . , ˆ β t , s − β t , s = 1 n n (cid:88) i = 1 U t , s , i ( α 0 ) + o P ( n − 1 / 2 ) = 1 n n (cid:88) i = 1 ( I p + s − 1 , α s , 0 ) U t , s , i ( α 0 ) + (cid:0) 0 T p + s − 1 , 1 (cid:1) β t , s + 1 q ( H is + 1 , α s , 0 ) + o P ( n − 1 / 2 ) . S11 Then for individuals in group a who drop out at visit s , the corresponding imputation parameter estimate ˆ β t , s − 1 can be expressed as ˆ β t , s − 1 = ( I p + s − 2 , ˆ α ws − 1 , 0 ) ˆ β t , s = (cid:0) I p + s − 2 , 0 T p + s − 2 (cid:1) ˆ β t , s + ˆ α ws − 1 , 0 (cid:0) 0 T p + s − 2 , 1 (cid:1) ˆ β t , s . ( S9 ) Similarly , the linearization form of the ﬁrst term in formula ( S9 ) can be obtained directly via delta - method as (cid:0) I p + s − 2 , 0 T p + s − 2 (cid:1) ˆ β t , s − (cid:0) I p + s − 2 , 0 T p + s − 2 (cid:1) β t , s = 1 n n (cid:88) i = 1 (cid:0) I p + s − 2 , 0 T p + s − 2 (cid:1) U t , s , i ( α 0 ) + o P ( n − 1 / 2 ) . For the second term , let g s − 1 ( α s − 1 , β t , s ) : = α s − 1 (cid:0) 0 T p + s − 2 , 1 (cid:1) β t , s . Then we have ∇ g s − 1 (cid:12)(cid:12)(cid:12) ( α s − 1 , 0 , β t , s ) = (cid:8)(cid:0) 0 T p + s − 2 , 1 (cid:1) β t , s , α s − 1 , 0 (cid:0) 0 T p + s − 2 , 1 (cid:1)(cid:9) T . Under the regularity condition C9 , we have g s − 1 ( ˆ α ws − 1 , ˆ β t , s ) − g t − 2 ( α s − 1 , 0 , β t , s ) = 1 n n (cid:88) i = 1 ∇ g T s − 1 (cid:12)(cid:12)(cid:12) ( α s − 1 , 0 , β t , s )   q ( H is , α s − 1 , 0 ) U t , s , i ( α 0 )   + o P ( n − 1 / 2 ) = 1 n n (cid:88) i = 1 (cid:0) 0 T p + s − 2 , 1 (cid:1) β t , s q ( H is , α s − 1 , 0 ) + α s − 1 , 0 (cid:0) 0 T p + s − 2 , 1 (cid:1) U t , s , i ( α 0 ) + o P ( n − 1 / 2 ) . Combine the two terms , the linearization form of ˆ β t , s − 1 is ˆ β t , s − 1 − β t , s − 1 = 1 n n (cid:88) i = 1 (cid:0) 0 T p + s − 2 , 1 (cid:1) β t , s q ( H is , α s − 1 , 0 ) + (cid:8)(cid:0) I p + s − 2 , 0 T p + s − 2 (cid:1) + α s − 1 , 0 (cid:0) 0 T p + s − 2 , 1 (cid:1)(cid:9) U t , s , i ( α 0 ) + o P ( n − 1 / 2 ) = 1 n n (cid:88) i = 1 ( I p + s − 2 , α s − 1 , 0 ) U t , s , i ( α 0 ) + (cid:0) 0 T p + s − 2 , 1 (cid:1) β t , s q ( H is , α s − 1 , 0 ) + o P ( n − 1 / 2 ) = 1 n n (cid:88) i = 1 U t , s − 1 , i ( α 0 ) + o P ( n − 1 / 2 ) . Apply the central limit theorem based on the regularity condition C12 and we complete the proof . (cid:3) We restate Theorem 2 in the main text below with technical details . Theorem S2 Under the regularity conditions C1 – C12 , and assume the following regularity condi - S12 tions : C13 . The partial derivatives ϕ a { Z ∗ ( β t ) , γ } with respect to γ and β t exist and are continuous around γ 0 and β t almost everywhere . The second derivatives of ϕ a { Z ∗ ( β t ) , γ } with respect to γ 0 and β t are continuous and dominated by some integrable functions . C14 . The partial derivative of E [ ϕ a { Z ∗ ( β t ) , γ } ] with respect to γ at γ = γ 0 , i . e . , D ϕ = ∂ E (cid:104) ϕ a (cid:8) Z ∗ ( β t ) , γ 0 (cid:9)(cid:105) / ∂γ T , is nonsingular . C15 . The partial derivative g ( X , γ ( 0 ) ) with respect to γ ( 0 ) exists and is continuous around γ ( 0 ) 0 almost everywhere . The second derivative of g ( X , γ ( 0 ) ) with respect to γ ( 0 ) 0 is continuous and dominated by some integrable functions . C16 . The variance V { V τ , i ( α 0 , γ 0 ) } is ﬁnite , where V τ , i ( α 0 , γ 0 ) = (cid:110) ∂g ( X i ; γ ( 0 ) 0 ) / ∂γ T (cid:111) c T V γ , i ( α 0 , γ 0 ) , V γ , i ( α 0 , γ 0 ) = D − 1 ϕ (cid:20) ϕ a { Z ∗ i ( β t ) , γ 0 } + t (cid:88) s = 1 E (cid:26) R s − 1 ( 1 − R s ) ∂µ ( A , X | γ 0 ) ∂γ T ∂ψ ( e ) ∂e H T s − 1 (cid:27) U t , s − 1 , i ( α 0 ) (cid:21) , e i = Y ∗ it ( β t ) − µ ( A , X | γ 0 ) , and c T = ( I d 0 , 0 d 0 × d 1 ) . Here , I d 0 is a ( d 0 × d 0 ) - dimensional identity matrix , 0 d 0 × d 1 is a ( d 0 × d 1 ) - dimensional zero matrix . Then , as the sample size n → ∞ , √ n ( ˆ τ − τ 0 ) d −→ N (cid:16) 0 , V { V τ , i ( α 0 , γ 0 ) } (cid:17) . Proof : Consider a Taylor expansion of the function (cid:80) ni = 1 ϕ a (cid:110) Z ∗ i ( ˆ β t ) , γ (cid:111) with respect to γ around γ 0 , under the regularity conditions C13 and C14 , we have the linearization form of ˆ γ as ˆ γ − γ 0 = 1 n n (cid:88) i = 1   − 1 n n (cid:88) i = 1 ∂ϕ a (cid:110) Z ∗ i ( ˆ β t ) , γ 0 (cid:111) ∂γ T   − 1 ϕ a (cid:110) Z ∗ i ( ˆ β t ) , γ 0 (cid:111) + o P ( n − 1 / 2 ) = (cid:18) − ∂ E [ ϕ a { Z ( β t ) , γ 0 } ] ∂γ T (cid:19) − 1 1 n n (cid:88) i = 1 ϕ a (cid:110) Z ∗ i ( ˆ β t ) , γ 0 (cid:111) + o P ( n − 1 / 2 ) = D − 1 ϕ 1 n n (cid:88) i = 1 ϕ a (cid:110) Z ∗ i ( ˆ β t ) , γ 0 (cid:111) + o P ( n − 1 / 2 ) . Therefore , √ n ( ˆ γ − γ 0 ) = D − 1 ϕ n − 1 / 2 n (cid:88) i = 1 ϕ a (cid:110) Z ∗ i ( ˆ β t ) , γ 0 (cid:111) + o P ( 1 ) S13 = D − 1 ϕ (cid:104) n − 1 / 2 n (cid:88) i = 1 ϕ a { Z ∗ i ( β t ) , γ 0 } + n − 1 / 2 n (cid:88) i = 1 ϕ a (cid:110) Z ∗ i ( ˆ β t ) , γ 0 (cid:111) − n − 1 / 2 n (cid:88) i = 1 ϕ a { Z ∗ i ( β t ) , γ 0 } (cid:105) + o P ( 1 ) . ( S10 ) The ﬁrst term in formula ( S10 ) is the sum of i . i . d . components with E [ ϕ a { Z ∗ i ( β t ) , γ 0 } ] = 0 . Then by the central limit theorem , the ﬁrst term converges to a normal distribution with the mean 0 and the variance V [ ϕ a { Z ∗ i ( β t ) , γ 0 } ] . For the term n − 1 / 2 (cid:80) ni = 1 ϕ a (cid:110) Z ∗ i ( ˆ β t ) , γ 0 (cid:111) − n − 1 / 2 (cid:80) ni = 1 ϕ a { Z ∗ i ( β t ) , γ 0 } in formula ( S10 ) , consider a Taylor expansion of n − 1 (cid:80) ni = 1 ϕ a (cid:110) Z ∗ i ( ˆ β t ) , γ 0 (cid:111) with respect to ˆ β t around β t , again by the regularity conditions C13 and C14 , we have 1 n n (cid:88) i = 1 ϕ a (cid:110) Z ∗ i ( ˆ β t ) , γ 0 (cid:111) = 1 n n (cid:88) i = 1 ϕ a { Z ∗ i ( β t ) , γ 0 } + 1 n n (cid:88) i = 1 ∂ϕ a { Z ∗ i ( β t ) , γ 0 } ∂β T t ( ˆ β t − β t ) + o P ( n − 1 / 2 ) = 1 n n (cid:88) i = 1 ϕ a { Z ∗ i ( β t ) , γ 0 } + ∂ E [ ϕ a { Z ∗ i ( β t ) , γ 0 } ] ∂β T t ( ˆ β t − β t ) + o P ( n − 1 / 2 ) . Note that ∂ E [ ϕ a { Z ∗ i ( β t ) , γ 0 } ] ∂β T t = (cid:32) ∂ E [ ϕ a { Z ∗ i ( β t ) , γ 0 } ] ∂β T t , 0 , · · · , ∂ E [ ϕ a { Z ∗ i ( β t ) , γ 0 } ] ∂β T t , t − 1 (cid:33) . From formula ( S2 ) , each component of the derivative ∂ E [ ϕ a { Z ∗ i ( β t ) , γ 0 } ] / ∂β T t can be obtained by the Chain Rule as ∂ E [ ϕ a { Z ∗ i ( β t ) , γ 0 } ] ∂β T t , s − 1 = E (cid:26) R s − 1 ( 1 − R s ) ∂µ ( A , X | γ 0 ) ∂γ T ∂ψ ( e ) ∂e H T s − 1 (cid:27) , for s = 1 , · · · , t . Then we can apply the linearization form stated in Lemma S4 , under the regularity condition C13 by Theorem 5 . 27 in Boos and Stefanski ( 2013 ) , we have n − 1 (cid:80) ni = 1 ϕ a (cid:110) Z ∗ i ( ˆ β t ) , γ 0 (cid:111) − n − 1 (cid:80) ni = 1 ϕ a { Z ∗ i ( β t ) , γ 0 } = 1 n n (cid:88) i = 1 t (cid:88) s = 1 E (cid:26) R s − 1 ( 1 − R s ) ∂µ ( A , X | γ 0 ) ∂γ T ∂ψ ( e ) ∂e H T s − 1 (cid:27) U t , s − 1 , i ( α 0 ) + o P ( n − 1 / 2 ) . Therefore , equation ( S10 ) can be further expressed as √ n ( ˆ γ − γ 0 ) = n − 1 / 2 n (cid:88) i = 1 D − 1 ϕ (cid:20) ϕ a { Z ∗ i ( β t ) , γ 0 } + t (cid:88) s = 1 E (cid:26) R s − 1 ( 1 − R s ) ∂µ ( A , X | γ 0 ) ∂γ T ∂ψ ( e ) ∂e H T s − 1 (cid:27) U t , s − 1 , i ( α 0 ) (cid:21) S14 + o P ( 1 ) = n − 1 / 2 n (cid:88) i = 1 V γ , i ( α 0 , γ 0 ) + o P ( 1 ) . By the regularity condition C15 , the ATE estimator ˆ τ = n − 1 (cid:80) ni = 1 g ( X i ; ˆ γ ( 0 ) ) can be linearized as ˆ τ − τ 0 = 1 n n (cid:88) i = 1 ∂g ( X i ; γ ( 0 ) 0 ) ∂γ T ( ˆ γ ( 0 ) − γ ( 0 ) 0 ) + o P ( 1 ) . Since ˆ γ ( 0 ) = c T ˆ γ = ( I d 0 , 0 d 0 × d 1 ) ˆ γ , by Theorem 1 and apply delta - method , we have the linearization form of ˆ τ as ˆ τ − τ 0 = 1 n n (cid:88) i = 1 ∂g ( X i ; γ ( 0 ) 0 ) ∂γ T c T V γ , i ( α 0 , γ 0 ) + o P ( n − 1 / 2 ) = 1 n n (cid:88) i = 1 V τ , i ( α 0 , γ 0 ) + o P ( n − 1 / 2 ) . Under the regularity condition C16 and apply the central limit theorem , we complete the proof . (cid:3) S1 . 3 An example : using the interaction model for the ATE estimation The working model in the form of ( 2 ) in the main text covers a wide range of analysis models in practice . We give an example of using the interaction model for analysis , i . e . , ﬁt the regression model with the interaction between the treatment variable and the baseline covariates for the imputed data , as it is one of the most common models in the clinical trials suggested in ICH ( 2021 ) . Example 1 When using an interaction model in the analysis step , the working model can be written as µ ( A , X | γ ) = AX T γ ( 0 ) − X T γ ( 1 ) , and the ATE estimator ˆ τ can then be obtained by solving the estimating equations n (cid:88) i = 1   ψ ( Y ∗ it − A i X T i γ ( 0 ) − X T i γ ( 1 ) ) ( A i X T i , X T i ) T X T i γ ( 0 ) − τ   = 0 . Denote V i = ( A i X T i , X T i ) T and γ 0 = ( γ ( 0 ) T 0 , γ ( 1 ) T 0 ) T such that E (cid:8) ψ ( Y ∗ it − A i X T i γ ( 0 ) − X T i γ ( 1 ) ) ( A i X T i , X T i ) T (cid:9) = 0 . Applying Theorems 1 and 2 , the estimator ˆ τ P −→ τ 0 and √ n ( ˆ τ − τ 0 ) d −→ S15 N (cid:16) 0 , V { V τ , i ( α 0 , γ 0 , µ X ) } (cid:17) , where V τ , i ( α 0 , γ 0 , µ X ) = ( X i − µ X ) T γ ( 0 ) 0 + µ T X V γ ( 0 ) , i ( α 0 , γ 0 ) , V γ ( 0 ) , i ( α 0 , γ 0 ) = c T D − 1 ϕ (cid:20) ψ ( e i ) V i + t (cid:88) s = 1 E (cid:26) R is − 1 ( 1 − R is ) V i ∂ψ ( e i ) ∂e i H T is − 1 (cid:27) U t , s − 1 , i ( α 0 ) (cid:21) , U t , s − 1 , i ( α 0 ) = ( I p + s − 2 , α s − 1 , 0 ) U t , s , i ( α 0 ) + (cid:0) 0 T p + s − 2 , 1 (cid:1) β t , s q ( H is , α s − 1 , 0 ) , U t , t − 1 , i ( α 0 ) = q ( H it , α t − 1 , 0 ) , and q ( H is , α s − 1 , 0 ) = (cid:34) − ∂ E (cid:8) ϕ ( H is , α s − 1 , 0 ) H T is − 1 | H is − 1 (cid:9) ∂α T s − 1 (cid:35) − 1 ϕ ( H is , α s − 1 , 0 ) . Here , c = ( I p , 0 p × p ) , where I p is a ( p × p ) - dimensional identity matrix , 0 p × p is a ( p × p ) - dimensional zero matrix , e i = Y ∗ it ( β t ) − V T i γ ∗ , D ϕ = ∂ E { ψ ( e i ) V i } / ∂γ T , and   β t , t − 1 = α t − 1 , 0 if s = t , β t , s − 1 = ( I p + s − 2 , α s − 1 , 0 ) ( I p + s − 1 , α s , 0 ) · · · ( I p + t − 3 , α t − 2 , 0 ) α t − 1 , 0 if s < t , for s = 1 , · · · , t . The asymptotic variance in Example 1 motivates us to obtain a linearization - based variance estimator by plugging in the estimated values as ˆ V ( ˆ τ ) = 1 n 2 n (cid:88) i = 1 (cid:8) V τ , i ( ˆ α w , ˆ γ , ˆ µ X ) − ¯ V τ ( ˆ α w , ˆ γ , ˆ µ X ) (cid:9) 2 , where ¯ V τ ( ˆ α w , ˆ γ , ˆ µ X ) = n − 1 (cid:80) ni = 1 V τ , i ( ˆ α w , ˆ γ , ˆ µ X ) , V τ , i ( ˆ α w , ˆ γ , ˆ µ X ) = ( X i − ˆ µ X ) T ˆ γ ( 0 ) + ˆ µ T X V γ ( 0 ) , i ( ˆ α w , ˆ γ ) , V γ ( 0 ) , i ( ˆ α w , ˆ γ ) = c T ˆ D − 1 ϕ ψ ( ˆ e i ) V i + t (cid:88) s = 1 (cid:40) 1 n n (cid:88) i = 1 R is − 1 ( 1 − R is ) V i ∂ψ ( ˆ e i ) ∂e i H T is − 1 (cid:41) U t , s − 1 , i ( ˆ α w ) , U t , s − 1 , i ( ˆ α w ) = (cid:0) I p + s − 2 , ˆ α ws − 1 (cid:1) U t , s , i ( ˆ α w ) + (cid:0) 0 T p + s − 2 , 1 (cid:1) ˆ β t , s ˆ q ( H is , ˆ α ws − 1 ) , U t , t − 1 , i ( ˆ α w ) = ˆ q ( H it , ˆ α wt − 1 ) , and ˆ q ( H is , ˆ α ws − 1 ) = (cid:18) − 1 n n (cid:88) i = 1 ∂ϕ ( H is , ˆ α ws − 1 ) ∂α T s − 1 H T is − 1 (cid:19) − 1 ϕ ( H is , ˆ α ws − 1 ) . Also , ˆ e i = Y ∗ it − V T i ˆ γ , ˆ D ϕ = n − 1 (cid:80) ni = 1 ∂ψ ( ˆ e i ) V i / ∂γ T , and    ˆ β t , t − 1 = ˆ α wt − 1 if s = t , ˆ β t , s − 1 = ( I p + s − 2 , ˆ α ws − 1 ) ( I p + s − 1 , ˆ α ws ) · · · ( I p + t − 3 , ˆ α wt − 2 ) ˆ α wt − 1 if s < t , S16 for s = 1 , · · · , t . In practice , ˆ µ X is estimated by the overall mean of the baseline covariates . We can also use the nonparametric bootstrap to obtain a replication - based variance estimator . In the simulation studies and real data application , we use the interaction model for analysis . S2 Illustration of the sequential regression procedure S2 . 1 Sequential linear regression In the main text , the sequential linear regression is mentioned multiple times in Sections 2 , 3 , and 4 , under the assumed scenario where the current outcomes and the the historical covariates have a linear relationship . Since the imputation model under J2R focuses on the control group , we ﬁt the current observed outcomes Y s in the control group against the historical information H s − 1 via a linear model to get the model parameter estimator ˆ α s − 1 by solving the estimating equations (cid:80) n i = 1 ( 1 − A i ) R is H is − 1 ( Y is − H T is − 1 α s − 1 ) = 0 . Our proposed weighted sequential robust regression model , whose robust loss function is of the form ( 1 ) , is motivated by this sequential linear regression model . S2 . 2 Extension to general sequential regression In Section 3 in the main text , we mentioned a possible extension to the nonlinear relationship between the current outcomes and the historical covariates for the ATE identiﬁcation . We now provide some insights into it . The key for the ATE identiﬁcation under the PMM framework is to form the assumption of the pattern - speciﬁc expectation E ( Y it | R is − 1 = 1 , R is = 0 , A i = a ) , which can be identiﬁed via the iterated expectations E ( Y it | H is − 1 , A i = 0 ) = E (cid:8) · · · E ( Y it | H it − 1 , R it = 1 , A i = 0 ) · · · | H is − 1 , R is = 1 , A i = 0 (cid:9) based on Assumptions 1 and 2 under J2R . If a nonlinear relationship is suspected , we can consider adding nonlinear terms in the parametric models or turn to ﬂexible models such as semiparametric models or machine learning models for model ﬁtting . One natural way to estimate the iterated expectation E ( Y it | H is − 1 , A i = 0 ) is to ﬁt the sequential regressions ( via ﬂexible models or parametric models with nonlinear terms ) in backward order . We again focus on the control group and give the detailed implementation steps as follows . Step 1 . For the participants who are fully observed , i . e . , with the observed indicator R it = 1 , ﬁt the regression model on Y it against the history H it − 1 . Use the ﬁtted model to predict the outcomes for those who are observed until ( t − 1 ) th visit time . Denote the predicted outcomes as ˆ E ( Y it | H it − 1 , R it = 1 , A i = 0 ) . Step 2 . For the participants who are observed until ( t − 1 ) th visit , ﬁt the regression model on the predicted outcomes ˆ E ( Y it | H it − 1 , R it = 1 , A i = 0 ) obtained in Step 1 against the history H it − 2 . S17 Use the ﬁtted model to predict the outcomes for those who are observed until ( t − 2 ) th visit time . Denote the predicted outcomes as ˆ E (cid:8) ˆ E ( Y it | H it − 1 , R it = 1 , A i = 0 ) | H it − 2 , R it − 1 = 1 , A i = 0 (cid:9) . Step 3 . Follow the similar procedure ( t − s − 2 ) times by ﬁtting the regression model in backward order . Obtain the predicted outcomes ˆ E ( Y it | H is − 1 , A i = 0 ) at last . S3 Additional notes on the simulation studies S3 . 1 Simulation setting In the simulation studies , the sample size is 500 for each group . The baseline covariates X = ( X 1 , X 2 ) T are generated independently by X 1 ∼ N ( 0 , 1 ) and X 2 ∼ Bernoulli ( 0 . 3 ) . The longitudinal outcomes are generated sequentially : ( a ) at t = 1 , generate Y 1 = 0 . 5 + X 1 − 0 . 2 X 2 + ε 1 for both groups ; ( b ) at t = 2 , generate   Y 2 = 0 . 4 + 0 . 14 X 1 + 0 . 52 X 2 + 0 . 01 Y 1 + ε 2 if A = 0 ; Y 2 = 1 . 79 + 0 . 35 X 1 − 0 . 05 X 2 + 0 . 33 Y 1 + ε 2 if A = 1 ; ( c ) at t = 3 , generate   Y 3 = 0 . 77 + 0 . 02 X 1 + 0 . 06 X 2 + 0 . 71 Y 1 + 0 . 84 Y 2 + ε 3 if A = 0 ; Y 3 = 2 . 52 + 1 . 16 X 1 − 0 . 51 X 2 − 1 . 53 Y 1 + 0 . 46 Y 2 + ε 3 if A = 1 ; ( d ) at t = 4 , generate   Y 4 = 1 . 44 − 0 . 45 X 1 − 0 . 24 X 2 − 0 . 50 Y 1 − 0 . 39 Y 2 + 0 . 53 Y 3 + ε 4 if A = 0 ; Y 4 = 2 . 72 − 0 . 46 X 1 − 0 . 06 X 2 + 0 . 91 Y 1 + 0 . 19 Y 2 + 0 . 70 Y 3 + ε 4 if A = 1 ; ( e ) at t = 5 , generate   Y 5 = 4 . 37 − 0 . 84 X 1 − 0 . 31 X 2 + 0 . 01 Y 1 + 0 . 35 Y 2 − 0 . 32 Y 3 + 0 . 81 Y 4 + ε 5 if A = 0 ; Y 5 = 4 . 21 − 0 . 02 X 1 − 1 . 26 X 2 + 0 . 24 Y 1 − 0 . 18 Y 2 + 0 . 65 Y 3 + 0 . 13 Y 4 + ε 5 if A = 1 ; S18 where ε k is from a distribution with mean 0 and standard deviation σ k , and σ = ( σ 1 , · · · , σ 5 ) T = ( 2 . 0 , 1 . 8 , 2 . 0 , 2 . 1 , 2 . 2 ) T . For the missing mechanisms , We set φ 11 = − 3 . 5 , φ 12 = − 3 . 6 , φ 21 = φ 22 = 0 . 2 . The tuning parameter in the weighted robust regression is set as 10 . We also try to use cross - validation to obtain the tuning parameters , which leads to very similar results . Therefore , to save computation time , the tuning parameter is ﬁxed in the MC simulation as q s − 1 = 10 for s = 1 , · · · , 5 . We consider two cases with the existence of extreme outliers or a heavy - tailed distribution as follows . ( a ) Data with / without extreme outliers : The error terms are generated by ε k ∼ N ( 0 , σ 2 k ) to form the multivariate normal distribution ( MVN ) . To create the outliers , we randomly select 10 individuals from the 30 completers with the maximum outcomes at the last visit point per group and multiply the original values by three for all post - baseline outcomes . ( b ) Data from a heavy - tailed distribution : We choose a common heavy - tailed distribution as t distribution . The error terms are generated by ε k ∼ ( 3 / 5 ) 1 / 2 σ k t 5 to get the same variation as the normal distribution , where t 5 is the standard t - distribution with the degrees of freedom as 5 . S3 . 2 Additional simulation results For the data with / without extreme outliers , apart from Table 1 ( b ) in the main text , we consider two more cases to incorporate the outliers only in one speciﬁc group , with the same approach to generate the outliers as presented in the main text . We again compare all the methods in terms of point and variance estimation , type - 1 error , power , and RMSE . Similar to the interpretation from Table 1 in the main text , Table S1 validates the superiority of the proposed robust method , as it shows unbiased point estimates , well - controlled type - 1 errors under H 0 , and high powers under H 1 . Table S1 : Simulation results under the normal distribution with extreme points at all post - baseline visit points . Here the true value τ = 71 . 18 % . Point est True var Var est Relative bias Coverage rate Power RMSE Case Method ( × 10 − 2 ) ( × 10 − 2 ) ( × 10 − 2 ) ( % ) ( % ) ( % ) ( × 10 − 2 ) ˆ V 1 ˆ V Boot ˆ V 1 ˆ V Boot ˆ V 1 ˆ V Boot ˆ V 1 ˆ V Boot Outliersonlyin control MI 43 . 07 4 . 29 13 . 10 6 . 23 205 . 40 45 . 37 98 . 00 84 . 80 8 . 80 40 . 80 34 . 90 LSE 51 . 44 3 . 66 4 . 71 4 . 39 28 . 68 19 . 84 88 . 70 86 . 90 68 . 60 70 . 70 27 . 48 Robust 74 . 86 3 . 44 3 . 54 3 . 48 2 . 77 1 . 03 94 . 80 93 . 90 98 . 10 97 . 80 18 . 91 Outliersonlyintreatment MI 116 . 55 3 . 43 8 . 29 6 . 40 141 . 36 86 . 31 74 . 40 58 . 80 100 . 00 100 . 00 49 . 00 LSE 94 . 11 3 . 63 4 . 73 4 . 69 30 . 24 28 . 98 84 . 90 86 . 20 99 . 70 99 . 70 29 . 82 Robust 67 . 71 3 . 28 3 . 41 3 . 38 3 . 94 3 . 04 94 . 50 93 . 70 95 . 40 96 . 00 18 . 44 S19 We also conduct the simulations under H 0 for each case . Under H 0 , we choose the same sequential regression coeﬃcients for both the control group and the treatment group . In addition , the tuning parameter in the missing mechanism model is set as φ 11 = φ 12 = − 3 . 5 and φ 21 = φ 22 = 0 . 2 . To achieve the accuracy of 0 . 01 , we choose the Monte Carlo sample size as 10 , 000 . Table S2 presents the simulation results under MVN and H 0 without or with extreme outliers . Although the point estimates seem to be unbiased when outliers exist ( since we generate the outliers in the same way for both groups , the bias for each group cancels oﬀ ) , the type - 1 error is extremely far away from the empirical value , suggesting huge variabilities for the MI and LSE methods . The proposed robust method outperforms as we observe a well - controlled type - 1 error , satisfying point and variance estimation results . The ﬁrst two rows of Figure 3 in the main text visualizes the simulation results . Table S2 : Simulation results under the normal distribution and H 0 without or with extreme outliers . Here the true value τ = 0 . Point est True var Var est Relative bias Type - 1 error RMSE Case Method ( × 10 − 2 ) ( × 10 − 2 ) ( × 10 − 2 ) ( % ) ( % ) ( × 10 − 2 ) ˆ V 1 ˆ V Boot ˆ V 1 ˆ V Boot ˆ V 1 ˆ V Boot No outliers MI 0 . 02 2 . 76 3 . 73 2 . 75 35 . 35 - 0 . 23 2 . 12 5 . 17 16 . 60 LSE 0 . 03 2 . 72 2 . 71 2 . 71 - 0 . 25 - 0 . 21 4 . 86 5 . 16 16 . 49 Robust - 0 . 94 2 . 94 2 . 89 2 . 96 - 1 . 50 0 . 73 4 . 96 5 . 06 17 . 17 Outliers in both groups MI 0 . 13 3 . 61 11 . 55 8 . 77 219 . 61 142 . 52 0 . 07 0 . 36 19 . 01 LSE 0 . 01 3 . 90 5 . 49 5 . 53 40 . 92 41 . 93 1 . 89 2 . 17 19 . 74 Robust - 1 . 05 3 . 03 2 . 90 3 . 00 - 4 . 36 - 1 . 05 5 . 26 5 . 29 17 . 45 Table S3 presents the simulation results under MVT and H 0 . Although all the methods have unbiased point estimates , the proposed robust method is more eﬃcient as the MC variance and RMSE are small . The last row of Figure 3 also visualizes the simulation results . Table S3 : Simulation results under the t - distribution and H 0 . Here the true value τ = 0 . Point est True var Var est Relative bias Type - 1 error RMSE Method ( × 10 − 2 ) ( × 10 − 2 ) ( × 10 − 2 ) ( % ) ( % ) ( × 10 − 2 ) ˆ V 1 ˆ V Boot ˆ V 1 ˆ V Boot ˆ V 1 ˆ V Boot MI - 0 . 14 2 . 78 3 . 75 2 . 76 34 . 87 - 0 . 77 2 . 33 5 . 35 16 . 68 LSE - 0 . 13 2 . 76 2 . 73 2 . 73 - 1 . 05 - 0 . 95 5 . 09 5 . 39 16 . 60 Robust - 1 . 05 2 . 46 2 . 41 2 . 47 - 2 . 18 0 . 52 5 . 38 5 . 47 15 . 72 S20 S4 Additional notes on the real - data application The repeated CD4 count data is available at https : / / content . sph . harvard . edu / fitzmaur / ala / cd4 . txt . It keeps track of the longitudinal CD4 counts during the ﬁrst 40 weeks of the clinical trial . Since the original CD4 counts are highly skewed , we conduct a log transformation to get the transformed CD4 count as log ( CD4 + 1 ) and use it as the outcome of interest . As the longitudinal outcomes are collected at 8 - week intervals , we factorize the continuous - time variable into the intervals ( 0 , 12 ] , ( 12 , 20 ] , ( 20 , 28 ] , ( 28 , 36 ] and ( 36 , 40 ] . To ensure that only one outcome is involved in a time interval for each individual , only the outcome that is nearest to week 8 k in the k th visit interval is preserved for k = 1 , · · · , 5 . Since our proposed method is only valid for a monotone missingness pattern , we delete the observations after the ﬁrst occurrence of missingness for each individual to create a monotone missingness dataset and use it for further analysis . The fully - observed baseline covariates consist of age , gender , and the baseline log CD4 counts . The created data suﬀers from severe missingness . In arm 1 , only 34 participants complete the study , while 94 drop out before week 12 , 52 drop out before week 20 , 47 drop out before week 28 , 17 drop out before week 36 , and 76 drop out before week 40 ; in arm 2 , only 46 participants complete the study , while 94 drop out before week 12 , 48 drop out before week 20 , 51 drop out before week 28 , 20 drop out before week 36 , and 71 drop out before week 40 . We ﬁrst conduct a scrutiny of the data to check the existence of extreme outliers and / or a violation of normality . Figure 1 in the main text presents the spaghetti plots of the repeated CD4 counts separated by each treatment . From the ﬁgure , there are no outstanding outliers in the data . Arm 2 has a higher average of the CD4 counts than arm 1 . Then we check for normality by ﬁtting sequential linear regressions on the current outcomes against all historical information in arm 1 and examining the conditional residuals at each visit point for model diagnosis . Figure 2 in the main text presents the QQ normal plots for the conditional residuals . Note that we only focus on the data in arm 1 since the imputation model under J2R relies solely on the data in the reference group . From the ﬁgure , heavier tails are detected at each visit point beyond the conﬁdence region . We further conduct the Shapiro - Wilk normality test for the conditional residuals . All the tests return p - values that are much smaller than 0 . 05 , therefore we reject the null hypothesis and conclude that the data does not follow a normal distribution . Moreover , we conduct a symmetry test proposed by Miao et al . ( 2006 ) on the conditional residuals . All the resulting p - values are larger than 0 . 05 and suggests that the residuals are symmetric around 0 , which allows us to obtain valid inferences of the ATE via the proposed robust methods . All the test results S21 are presented in Figure 2 at visit s for s = 1 , · · · , 5 . In the implementation of the weighted robust method , the tuning parameters in formula ( 1 ) are selected via cross - validation . Speciﬁcally , to mitigate the impact of outliers that are existed in the covariates in the imputation model , we ﬁrst conduct the cross - validation to select the tuning parameter at each visit point in the sequential robust regression that returns the smallest MSE , then insert the chosen tuning parameters in the imputation model and further select the tuning parameter for the analysis model in each group by cross - validation . The resulting tuning parameters for the imputation model are ( 20 , 19 . 5 , 17 . 5 , 15 , 8 ) . The choice of tuning parameters is not sensitive to the ﬁnal estimation . S22