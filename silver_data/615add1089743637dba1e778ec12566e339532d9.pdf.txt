Efficient Dataset Distillation through Alignment with Smooth and High - Quality Expert Trajectories Jiyuan Shen Wenzhuo Yang Kwok - Yan Lam Nanyang Technological University , Singapore jiyuan001 @ e . ntu . edu . sg { wenzhuo . yang , kwokyan . lam } @ ntu . edu . sg Abstract Training a large and state - of - the - art machine learning model typically necessitates the use of large - scale datasets , which , in turn , makes the training and parameter - tuning process expensive and time - consuming . Some researchers opt to distil information from real - world datasets into tiny and compact synthetic datasets while maintaining their ability to train a well - performing model , hence propos - ing a data - efficient method known as Dataset Distillation ( DD ) . Despite recent progress in this field , existing meth - ods still underperform and cannot effectively replace large datasets . In this paper , unlike previous methods that focus solely on improving the efficacy of student distillation , we are the first to recognize the important interplay between expert and student . We argue the significant impact of ex - pert smoothness when employing more potent expert tra - jectories in subsequent dataset distillation . Based on this , we introduce the integration of clipping loss and gradient penalty to regulate the rate of parameter changes in expert trajectories . Furthermore , in response to the sensitivity ex - hibited towards randomly initialized variables during distil - lation , we propose representative initialization for synthetic dataset and balanced inner - loop loss . Finally , we present two enhancement strategies , namely intermediate matching loss and weight perturbation , to mitigate the potential oc - currence of cumulative errors . We conduct extensive experi - ments on datasets of different scales , sizes , and resolutions . The results demonstrate that the proposed method signifi - cantly outperforms prior methods . 1 . Introduction Nowadays , large machine learning models have demon - strated unprecedented results in many fields such as com - puter vision , natural language processing , and speech recognition . Behind the usage of promising prospects of large models , large - scale dataset has gradually become a norm . At such scales , storing and preprocessing datasets becomes burdensome , and the need for specialized equip - ment and substantial financial investment exacerbates the difficulties of training large machine learning models on large - scale datasets . As a result , some researchers try to explore data - efficient methods , including coreset selection [ 1 , 2 , 38 ] , dataset pruning [ 10 , 15 , 23 ] and instance selec - tion [ 35 ] . They try to summarize the entire dataset by a small subset or by only selecting the ‘valuable’ data for model training . However , these approaches have resorted to greedy algorithms with heuristics [ 2 , 8 , 38 , 42 ] , resulting in significant performance drops , and there is no assurance of producing representative samples . Recently , a new data - efficient method called Dataset Distillation ( DD ) has emerged as a competitive alternative with promising results [ 9 , 11 , 37 , 46 ] . The goal of DD is to distil a large training set into a small synthetic set , upon which machine learning models are trained from scratch , and similar testing performance on the validation set is ex - pected to be preserved . DD has aroused significant interest from the machine learning community with various down - stream applications , such as federated learning [ 26 , 36 , 53 ] , neural architecture search [ 30 , 40 ] , and privacy - preserving tasks [ 7 , 28 , 29 ] , etc . As one of the state - of - the - art frameworks for dataset dis - tillation , Matching Training Trajectories ( MTT ) [ 5 ] distin - guishes itself by long - range matching characteristic . MTT encompasses two primary phases : expert trajectory gen - eration ( buffer ) and student parameter matching ( distilla - tion ) . Nevertheless , despite recent advancements , the re - search works [ 5 , 12 , 14 ] continue to employ the simplest optimizer for expert trajectory generation , accompanied by sole weight matching loss in the second phase . Prior works lack consideration regarding the suitability of experts for students , thus ignoring potential relationship that could lead to consistent improvement of both students and experts . We observe that the smoothness of expert trajectories plays an important role . For instance , when substituting the initial non - momentum Stochastic Gradient Descent ( SGD ) with a superior optimizer , it does enhance the accuracy of expert model . However , this replacement markedly intensifies the rate of parameter changes in expert 1 a r X i v : 2310 . 10541v1 [ c s . C V ] 16 O c t 2023 Figure 1 . Illustration of improved trajectory matching method : ( a ) Buffer : We generate the smooth and high - quality expert trajectory ( blue dash curve ) on the real dataset D real . The grey dash curve is a much steeper expert trajectory without applying clipping loss and gradient penalty . ( b ) Distillation : We first select representative initialization samples from the original training images . Then , the synthetic dataset D syn is optimized to match the segments of the expert trajectory through parameter matching . We apply a balanced inner - loop loss ˆ ℓ BIL to mitigate the influence of random initialized expert starting epoch t . The red solid curve denotes student trajectory . ( c ) Outer - loop in distillation : We show how student trajectories are matched with expert trajectories within a single iteration . We introduce weight perturbation d l , j to the well - trained expert model parameters . Subsequently , we calculate the intermediate matching loss L TM in the middle of the loop . These two methods can help mitigate the accumulated errors ε t . model , rendering it less smooth and moderate , ultimately resulting in difficulty of student parameter matching and a significant decline in the distillation outcomes . Besides , within the trajectory matching paradigm , the student parameter matching in the second stage exhibits a heightened susceptibility to two randomly initialized vari - ables , namely the initial samples and the initial starting epochs of expert trajectories . The above two factors to - gether result in optimization instability of the distillation process . Particularly when distilling datasets of larger scale and higher resolution , it may lead to the generation of im - ages akin to random noise and ‘black holes’ . Finally , we argue that not only discrepancy between dis - tillation and evaluation will lead to cumulative errors [ 14 ] . The long interval between inner and outer loop under the bi - level optimization structure will also generate errors in the second stage , which degrade the performance of distilled dataset . Building upon this , we have instituted improvements in both the expert trajectory generation and student parameter matching phases , as shown in Fig . 1 . We propose the amal - gamation of clipping loss and gradient penalty with a more proficient optimizer to regulate the rate of parameter update and concurrently sustain high expert performance through - out the training of expert models in the initial phase . Addi - tionally , we implement a strategy involving the use of repre - sentative sample initialization and inner loop loss balancing to mitigate the sensitivity to randomly initialized variables during dataset distillation . By leveraging smoother expert trajectories and the prescribed parameter matching strategy , we improve matching stability in the second stage and fa - cilitate the production of superior distilled datasets . More - over , we propose two enhancement strategies , namely in - termediate matching loss and weight perturbation of expert model , to ameliorate the propensity for cumulative errors . Our proposed methods have been extensively evaluated on datasets varying in scale , size , and resolution , and the re - sults substantiate their efficacy , exhibiting significant supe - riority over prior approaches . In summation , our main contributions can be summa - rized as follows : • We propose the need for integration of clipping loss and gradient penalty under a more potent optimizer to keep the expert trajectory smoothness . • We propose representative sample initialization and in - ner loop loss balancing strategies to alleviate the im - pact of stochasticity during the second - stage distilla - tion . • We propose two enhancement strategies , namely inter - mediate matching loss and the perturbation of expert model weights , to mitigate the accumulation of errors . 2 . Related Works 2 . 1 . Dataset Distillation The existing DD frameworks can be divided into four parts [ 37 ] , namely Meta - model Learning , Gradient Match - 2 ing , Distribution Matching and Trajectory Matching . Wang et al . [ 44 ] first introduces the dataset distillation problem and uses bi - level optimization similar to Model - Agnostic Meta - Learning ( MAML ) [ 16 ] . Following the Meta - Learning frameworks , recent works include adding soft labels [ 4 , 41 ] , developing closed - form approximation KIP [ 33 , 34 ] and FRePo [ 52 ] , and applying prior towards the synthetic dataest [ 6 , 27 ] . Gradient Matching conducts a single - step distance matching between the network trained on the original dataset and the identical network trained on synthetic data , methods including DC [ 51 ] , DSA [ 48 ] , DCC [ 25 ] and IDC [ 21 ] . To alleviate complex bi - level op - timization and second - order derivative computation , Distri - bution Matching directly matches the distribution of origi - nal data and synthetic data with a single - level optimization , methods include DM [ 50 ] , CAFE [ 43 ] , IT - GAN [ 49 ] . How - ever , the above - mentioned works are all short - range match - ing methods and may easily cause overfitting problem [ 46 ] . Trajectory Matching leverages the well - trained expert training trajectory as reliable references and aims to min - imize the distance between expert and student trajectories in the parameter matching process of the second stage , which improves the performance of distilled dataset . Re - cent works include MTT [ 5 ] , TESLA [ 12 ] that reparame - terizes the computation of matching loss and FTD [ 14 ] that minimizes accumulated errors between distillation and eval - uation . Nevertheless , Trajectory Matching framework still has its own issues , such as ignoring the interplay between smoothness and accuracy of expert trajectories , susceptibil - ity to random initial variables , and cumulative errors . 2 . 2 . Relationship between Teacher and Student Currently , there exist some methods in the field of Knowledge Distillation ( KD ) that explore the factors of what makes teachers conducive to students learning or how to transfer more valuable knowledge from highly proficient teachers . Huang et al . [ 20 ] has empirically pointed out that simply enhancing a teacher’s capabilities or employing a stronger teacher can , paradoxically , result in a decrease in student performance . In some cases , this decline can even be more pronounced than if students were to begin training from scratch with vanilla KD . Consequently , they have pro - posed a loose matching approach to replace the traditional Kullback - Leibler ( KL ) divergence . Similarly , Yuan et al . [ 47 ] argues that simplifying the teacher model’s outputs can offer greater advantages to the student’s learning process . Meanwhile , Shao et al . [ 39 ] pro - poses the principle that ‘teachers should teach what you should teach’ and introduces a data - based knowledge dis - tillation method . All of the methods mentioned above un - derscore the intimate relationship between teachers and stu - dents , necessitating a thorough exploration of how to en - hance the abilities of both teachers and students consis - Optimizer ( Expert ) Acc . ( Expert ) Acc . ( Distill ) Na¨ıve SGD 48 . 6 39 . 7 SGD w / Mom 57 . 1 18 . 8 ADAM 52 . 7 18 . 1 Table 1 . Expert model accuracy and distilled results on various expert trajectories using various optimizers on CIFAR - 100 under ipc = 10 . Initial Samples Starting Epoch Acc . ( Distill ) Random Random 39 . 7 Representative Random 40 . 3 Random Balanced Strategy 40 . 1 Table 2 . After using the stochasticity balancing strategy during parameter matching , there has been a consistent improvement in the performance of the distilled dataset . 3050 3055 3060 3065 3070 Distill Epochs on CIFAR100 ( ipc = 10 ) 0 5 10 15 20 25 30 S t a r t E p o c h ( a ) Randomly Start Epoch 3050 3055 3060 3065 3070 Distill Epochs on CIFAR100 ( ipc = 10 ) 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 3 . 5 I nn e r - l oo p L o ss NaiveBalanced ( b ) Inner - loop Loss Figure 2 . Because of the random variable of starting epoch for each iteration ( as depicted in the blue dashed plot in ( a ) ) , it leads to substantial fluctuations in the loss scale within the inner loop , as indicated by the solid grey plot in ( b ) . After employing a bal - anced inner - loop loss , the extent of loss variation is constrained to a lesser degree during the early starting epochs , while allocating more weight to the later starting epochs , as illustrated by the red plot in ( b ) . tently . Nonetheless , there remains a vacancy in this rela - tionship under the context of Dataset Distillation . 3 . Method 3 . 1 . Generate Smooth Expert Trajectories 3 . 1 . 1 Catastrophic Discrepancy with Stronger Expert Trajectory As depicted in Sec . 1 , the influence of a teacher on DD has not been adequately investigated , especially as the perfor - mance of the expert model becomes stronger , for instance , through the use of better optimizers . With this regard , as Tab . 1 , we demonstrate both results of the expert model and the distilled dataset achieved with various optimizers . We have observed that when employing SGD with mo - mentum or ADAM as optimizers , although they generate stronger expert trajectories , theoretically elevating the up - per limit of distillation dataset performance , this increment 3 does not directly connect with improved results of dis - tilled dataset . On the contrary , the accuracy of the distilled dataset on the validation set experiences a substantial de - cline , even falling behind the vanilla MTT method . This indicates that the knowledge encapsulated within the more potent expert trajectories is not effectively assimilated dur - ing following distillation process . The underlying reason is that the utilization of more potent optimizers often incor - porates the previously accumulated gradient means into the optimization process , which facilitates faster model conver - gence [ 18 ] . However , this causes the expert model’s param - eters to update too rapidly , surpassing the limitation of what can be distilled during the second stage . It is detrimental to the parameter matching process . For specific experimental details , please refer to Sec . 4 . 4 . 1 . Consequently , our proposed approach aims to enhance the performance of the expert model while simultaneously moderating the rate of parameter updates in the training pro - cess of expert models . Our strategy allows us to obtain a series of smooth and high - quality expert trajectories . 3 . 1 . 2 Better Expert Trajectory under Smoothness Constraint To constrain the changing speed of expert parameters , the straightforward method involves the utilization of value clipping on the cross - entropy loss , with a specific focus on the initial epochs . Gradually , the clipping coefficient is in - cremented until it reaches a value of 1 , which is then held constant . Our findings indicate that although loss clipping may impact the final expert performance to some extent , its key benefit lies in mitigating the rapid changes in model pa - rameters , particularly during the initial few epochs . Besides , to maintain the overall smoothness of the ex - pert trajectory , we introduce the concept of gradient penalty , building upon the insights from the Wasserstein GAN ( WGAN ) [ 3 , 19 ] . The goal of employing gradient penalty is to force the model to satisfy Lipschitz continuity , preventing abrupt and erratic changes in the model’s parameters . We incorporate gradient penalty by adding a regularization term to the loss function . This term is formulated as the squared norm of the gradient ∇ x W ( x ) of the model’s output with respect to its input . By penalizing large gradients , we incen - tivize the model to produce outputs that change gradually as inputs vary slightly . This has the effect of maintaining a smoother transition between data points and preventing sudden shifts that could lead to instability . We formulate the final loss equation as below : L SC = λ log exp ( x n , y n ) (cid:80) Cc = 1 exp ( x n , c ) (cid:124) (cid:123)(cid:122) (cid:125) Clipped CELoss + µ E x ∼ P x (cid:104) ( ∥∇ x W ( x ) ∥ 2 −K ) 2 (cid:105) (cid:124) (cid:123)(cid:122) (cid:125) Gradient Penalty ( 1 ) where ∥∇ x W ( x ) ∥ 2 represents a dual - sided penalty that aims to constrain the gradient norm to values below a pre - determined threshold denoted as K . Typically , K is set at 1 . The coefficient λ operates within a range of 0 . 5 to 1 during the initial few epochs , while the coefficient µ is responsible for scaling the gradient penalty effect . 3 . 2 . Balance Stochasticity during Parameter Matching 3 . 2 . 1 Representative Initialization for Synthetic Dataset Many existing dataset distillation algorithms initialize D syn either by employing random initialization with Gaussian noise or by randomly selecting a subset of images from the real dataset . However , this approach can inadvertently in - troduce outliers , lead to the inclusion of samples with simi - lar features , or overlook certain aspects of the feature space , which may introduce huge biases and stochasticity . More - over , as distilled datasets often end up with a limited num - ber of samples per class , it becomes crucial to efficiently encapsulate the inherent information of the original dataset within these limited samples . As a solution , we propose an alternative approach that facilitates more representative initialization . Leveraging the benefits of the MTT framework can sim - plify the process of selecting representative initialization samples for D syn . In the first stage , we have already ob - tained the training trajectory of the expert model . By load - ing the parameters of expert trajectory into the model , we can acquire a well - trained model . Next , we input all real data of the same class into the model , obtaining feature vectors before entering the fully connected layer . Subse - quently , we perform clustering on these feature maps , uti - lizing the K - Means algorithm to partition them into multiple sub - clusters . The value of K is chosen based on the desired number of distilled samples . These sub - cluster centroids are then selected as exemplary initialization samples . The primary objective of this approach is to enhance the initialization process by strategically selecting representa - tive samples that are better suited for distillation . 3 . 2 . 2 Balanced Inner - loop Loss During the second stage , student parameter matching ex - hibits sensitivity to the randomly initialized expert starting epochs t in each iteration . This sensitivity is specifically manifested in significant fluctuations of the inner - loop loss ℓ IL . As shown in Fig . 2b , the largest loss is nearly 60 times larger than the smallest one . The fluctuations in the inner - loop loss introduce a notable level of instability into the dis - tillation procedure , which impedes the model convergence and hinders the consistent acquisition of accurate distilled data . 4 Intuitively , our initial thought is to discard the practice of entirely random selection for the starting epoch and instead opt for a method that references the preceding starting point and selects the subsequent starting point within a reasonable range . However , experiments ( details in Sec . 4 . 4 . 2 ) have demonstrated that this dynamic selection approach fails to enhance the final performance . We speculate that employ - ing a range selection may introduce potential bias into the distillation process , which in turn causes the model to learn incorrect inductive bias . Hence , we opt to directly balance the loss within the inner loop . When the starting epoch t is much smaller , which means the network is under convergence , it will typically generate a much larger loss ℓ IL leading the gradient descent towards this direction . Thus , we add a penalty coefficient ν to nar - row the loss , clipping the loss to a small extent . Conversely , when the starting epoch t is large ( close to T + ) , the opti - mized steps become much smaller due to the decreased loss . We also add a coefficient ν to encourage a much bigger gra - dient . Under this circumstance , the balanced inner - loop loss ˆ ℓ BIL can be formulated as : ν = (cid:26) log ( | start − middle | + ϑ ) , start ≥ middle 1 / log ( | middle − start | + ϑ ) , start < middle ˆ ℓ BIL = ν × ℓ IL ( 2 ) where start represents the starting epoch , middle refers to half of the maximum starting epoch and ϑ is included to balance the log scale and prevent negative values . As shown in Fig . 2 , using balanced loss ˆ ℓ BIL can reduce the loss fluctuation in the inner loop . For ℓ IL , it is similar to cross - entropy loss : ℓ IL = 1 | D syn | (cid:88) ( s , y ) ∈D syn (cid:104) y log ( ˆ θ t ( A ( s ) ) ) (cid:105) ( 3 ) where A represents the differentiable siamese augmenta - tion [ 48 ] , while ˆ θ t denotes the student network parameter - ized using the expert’s t - th epoch . network , t is smaller than the defined maximum start epoch T + and D syn is the dis - tilled samples . 3 . 3 . Alleviate the Propensity for Accumulated Error 3 . 3 . 1 Intermediate Matching Loss Trajectory matching is a long - range framework , typically involving a larger number of steps denoted as N to match a smaller number of epochs denoted as M . Here , M signifies the interval between starting and target point of the expert trajectory , while N represents the training steps of the stu - dent model on the distilled dataset . N is also equivalent to the number of iterations in the inner loop . After every N steps , it will execute a matching interaction with the expert trajectory . However , extensive intervals of interaction can lead to the inner loop deviating from the correct direction prema - turely . It may result in the accumulation of errors without sufficient supervision guidance . Therefore , we propose the intermediate matching loss , that can optionally perform an additional parameter matching step within the inner loop . Specifically , we establish a set of intermediate match - ing points denoted as ξ , which consists of two parame - ters , M and N . Here , M signifies the interval between starting and target point of the expert trajectory , while N represents the training steps of the student model on the distilled dataset . Thus , it can be denoted as { ξ } = { ⌊N / M ⌋ , ⌊ 2 ×N / M ⌋ , . . . , ⌊ ( M− 1 ) ×N / M ⌋ } . When the inner loop n ( from 1 to N ) precisely aligns with ξ i ∈ { ξ } , the intermediate matching loss function is com - puted as : L TM = (cid:13)(cid:13) (cid:13) ˆ θ t + n − θ ∗ t + ξ i (cid:13)(cid:13) (cid:13) 2 2 (cid:13)(cid:13)(cid:13) θ ∗ t − θ ∗ t + ξ i (cid:13)(cid:13)(cid:13) 2 2 ( 4 ) in which θ ∗ t is the expert training trajectory at randomly starting epoch t , also known as the initial expert weights . Starting from θ ∗ t , θ ∗ t + ξ i denotes { ξ } steps trained by the expert network on real data and ˆ θ t + n stands for n steps ( n < N ) by student network trained on synthetic data corre - spondingly . The goal is to minimize the divergence between ˆ θ t + n and θ ∗ t + ξ i , and (cid:13)(cid:13)(cid:13) θ ∗ t − θ ∗ t + ξ i (cid:13)(cid:13)(cid:13) 2 2 is used to self - calibrate the magnitude across different iterations . L TM is then uti - lized to update the student parameters . 3 . 3 . 2 Weight Perturbation on Initial Expert Model Another potential source of cumulative error stems from the disparity between the distillation and evaluation processes , as pointed out in [ 14 ] . Due to the discrete nature of the distillation process versus the continuous nature of the eval - uation process , cumulative errors can thus arise . [ 14 ] intro - duce a regularization term during expert trajectory genera - tion , however , we adopt a more direct and simple approach , named weight perturbation . Weight perturbation is an orthogonal method to explore data diversity to boost the performance of the network , dif - ferent from the data operation like transformation [ 48 ] or perturbation [ 31 , 32 ] . We adopt a weight - perturbed initial expert model to formulate our dataset distillation , expressed as follows : d l , j = d l , j ∥ d l , j ∥ F ∥ θ l , j ∥ F θ ∗ t = θ t + α ∗ d l , j ( 5 ) where d l , j is sampled from a Gaussian distribution N ( 0 , 1 ) with dimensions same as θ l , j . d l , j is the j - th filter at the l - th layer of d and ∥·∥ F refers to the Frobenius norm . Finally , a coefficient α is added to obtain the final θ ∗ t . 5 Although it may increase the inner - loop loss , the per - turbed weight initialization during the distilled process sim - ulates the slight noise of weight parameters generated in the evaluation process . As the weight of the next epoch is ob - tained from the previous training epoch instead of getting from the totally correct expert network , weight perturbation on initial expert model during distillation process can effec - tively reduce the accumulated error . 3 . 4 . Training Algorithm We depict our proposed method in Algorithm 1 . 4 . Experiments 4 . 1 . Experiments Setup The majority of our experimental procedures closely fol - low the previous works [ 5 , 12 , 14 ] , which ensure a fair and equitable basis for comparison . Each of our experiments comprises three essential phases : buffer ( generating expert trajectories ) , distillation ( parameter matching ) , and evalu - ation phase . First , we generate 50 distinct expert training trajectories , with each trajectory encompassing 50 training epochs . Second , we synthesize a small synthetic set ( e . g . , 10 images per class ) from a given large real training set . Finally , we employ this learned synthetic dataset to train randomly initialized neural networks and assess the perfor - mance of these trained networks on the real test dataset . Datasets . We verify the effectiveness of our method on both low - and high - resolution datasets distillation bench - marks , including CIFAR - 10 & CIFAR - 100 [ 22 ] , Tiny Ima - geNet [ 24 ] and ImageNet [ 13 ] . Top - 1 accuracy is reported to show the performance . Baselines . We compare our method with various base - lines including Dataset Condensation ( DC ) [ 51 ] , Differ - entiable Siamese Augmentation ( DSA ) [ 48 ] , Distribution Matching ( DM ) [ 50 ] , Aligning Features ( CAFE ) [ 43 ] , Feature Regression with Pooling ( FRePo ) [ 52 ] , trajectory matching method ( MTT ) [ 5 ] , memory - efficient trajectory matching method ( TESLA ) [ 12 ] , and Flat Trajectory Dis - tillation ( FTD ) [ 14 ] . Implementation Details . In the buffer phase , we uti - lize SGD with momentum as the optimizer , setting λ as an array ranging from 0 . 5 to 1 . This range is applied individ - ually for the first 5 training epochs , while µ is set to 1 . We train each expert model for 50 epochs , with the learning rate reduced by half in the 25th epoch . During the distillation process , the value of K in the K - Means algorithm is deter - mined based on the ipc ( images per class ) value . However , when ipc equals 50 , we opt to cluster only 10 sub - clusters , each selecting extra 4 points approximating the sub - cluster centroid . For the balanced loss , we set θ to 8 . Concerning the intermediate matching loss , we introduce a hyperparam - eter β to control the scale of several losses , encompassing two strategies : equal scale β = 1 or varied scale β based on the value of { ξ } . In terms of weight perturbation , we set α to 0 . 1 , and we also conduct experiments to evaluate the performance with dropout added . Throughout the evalua - tion phase , the number of training iterations is set to 1000 , with a learning rate reduction by half at the 500 - th iteration . Furthermore , we employ the same Differentiable Siamese Augmentation ( DSA ) during both the distillation and eval - uation processes . 4 . 2 . Comparison with State - of - the - Art Methods Competitors : We categorize the current works into three main parts , shown in Tab . 3 . The first part focuses on core - set selection and includes non - learning methods such as random selection , herding methods [ 8 ] , and example for - getting [ 42 ] . These techniques aim to select ‘valuable’ in - stances from a large dataset . The second part comprises methods like DC , DM , DSA , CAFE , and FrePo , which pri - marily address short - range matching and truncated gradient backpropagation . They employ gradient matching or dis - tribution matching as the optimization objective . The third part including MTT , TESLA and FTD applies long - range matching characteristics , based on the framework of MTT , which are most relevant to our method . Results with Coreset & Short - range : Our proposed method outperforms the coreset selection baselines signif - icantly . The non - learning methods achieve inferior perfor - mance compared to our methods . When comparing with the second part works , our method demonstrates the same superiority over all other methods . Moreover , we improve one of the strong baseline DSA to nearly 15 % accuracy on both CIFAR - 10 and CIFAR - 100 under all ipc settings . Results with Long - range : Regarding the comparison of experimental results in the third part , we observe consis - tently positive outcomes with significant improvements in all settings . We specifically compare our method with the original MTT . For example , in CIFAR - 10 , at a compression rate of fifty images , we achieve a score of 74 . 6 % , which is 3 % higher than the accuracy score of MTT . Compared to the results obtained from the full dataset , we are only around 10 % behind . Similar improvements are observed in CIFAR - 100 , where we achieve a 4 % increase over MTT and our results are only 4 . 5 % lower than the results obtained from the full dataset . We visualize parts of the synthetic images in Fig . 3 and discover that our distilled samples Fig . 3c can focus more on the classified object itself , while diluting the background information . Compared to the original images , the distilled dataset is more informative and abstract . In the more intricate Tiny ImageNet dataset , our ap - proach demonstrates consistent and significant improve - ments , achieving 13 . 7 % and 25 . 7 % in ipc values of one and ten , respectively . These empirical findings substantiate the efficacy of our proposed method . 6 Algorithm 1 : Dataset Distillation in Parameter Matching Input : { τ i } : set of smoothed expert parameter trajectories trained on real dataset D real . Input : M : interval between starting and target expert trajectory . Input : N : distillation steps of student network . Input : A : differentiable siamese augmentation . Input : T + : maximum start epoch . Input : { ξ } = { ⌊N / M ⌋ , . . . , ⌊ ( M− 1 ) ×N / M ⌋ } : set of intermediate matching epoch . 1 Select representative initialization samples D syn ∼ D real ⇒ Method 3 . 2 . 1 ; 2 Initialize trainable learning rate α : = α 0 ; 3 for each distillation step do 4 Sample smooth expert trajectory τ ∼ { τ i } with τ : = { θ t } . ⇒ Method 3 . 1 . 2 ; 5 Choose random start epoch , t ⩽ T + ; 6 Perturb weight on initial expert model with τ ∗ : = { θ ∗ t } ⇒ Method 3 . 3 . 2 ; 7 Initialize student network with expert parameters ˆ θ t : = θ ∗ t ; 8 for n = 1 to N do 9 Sample a mini - batch of distilled images : b t + n ∼ D syn ; 10 Compute the cross - entropy loss based on Eq . ( 3 ) . ; 11 Get ν based on Eq . ( 2 ) and balance the inner - loop loss : ˆ ℓ BIL = ν × ℓ IL ⇒ Method 3 . 2 . 2 ; 12 Update student model : ˆ θ t + n + 1 = ˆ θ t + n − α ∇ ˆ ℓ BIL ; 13 if n in { ξ } then 14 Calculate intermediate matching loss L i = (cid:13)(cid:13)(cid:13) ˆ θ t + n − θ ∗ t + ξ i (cid:13)(cid:13)(cid:13) 2 2 / (cid:13)(cid:13)(cid:13) θ ∗ t − θ ∗ t + ξ i (cid:13)(cid:13)(cid:13) 2 2 ⇒ Method 3 . 3 . 1 ; 15 end 16 end 17 Get the final loss ˆ L = (cid:80) ξ i β i × L i ; 18 Update D syn and α with respect to ˆ L ; 19 endOutput : Distilled dataset D syn and learning rate α ( a ) Original CIFAR100 images . ( b ) The synthetic images of MTT . ( c ) The synthetic images of ous . Figure 3 . Visualizations of original images , and synthetic images generated by MTT and our proposed methods . Cross - Architecture Generalization : We evaluate gen - eralization capacity across various architectures . Initially , we distilled the synthetic dataset using ConvNet . Subse - quently , we trained several architectures , namely AlexNet , VGG11 , and ResNet18 , on the distilled dataset . Tab . 4 presents the results of our evaluations . Our method outper - forms MTT significantly in generalization performance . 7 IPC CIFAR - 10 CIFAR - 100 Tiny ImageNet 1 10 50 1 10 50 1 10 Full Dataset 84 . 8±0 . 1 56 . 2±0 . 3 39 . 5±0 . 4 Random 14 . 4±2 . 0 26 . 0±1 . 2 43 . 4±1 . 0 4 . 2±0 . 3 14 . 6±0 . 5 30 . 0±0 . 4 1 . 4±0 . 1 5 . 0±0 . 2 Herding [ 8 ] 21 . 5±1 . 2 31 . 6±0 . 7 40 . 4±0 . 6 8 . 4±0 . 3 17 . 3±0 . 3 33 . 7±0 . 5 2 . 8±0 . 2 6 . 3±0 . 2 Forgetting [ 42 ] 13 . 5±1 . 2 23 . 3±1 . 0 23 . 3±1 . 1 4 . 5±0 . 2 15 . 1±0 . 3 30 . 5±0 . 3 1 . 6±0 . 1 5 . 1±0 . 2 DC [ 51 ] 28 . 3±0 . 5 44 . 9±0 . 5 53 . 9±0 . 5 12 . 8±0 . 3 25 . 2±0 . 3 - - - DM [ 50 ] 26 . 0±0 . 8 48 . 9±0 . 6 63 . 0±0 . 4 11 . 4±0 . 3 29 . 7±0 . 3 43 . 6±0 . 4 3 . 9±0 . 2 12 . 9±0 . 4 DSA [ 48 ] 28 . 8±0 . 7 52 . 1±0 . 5 60 . 6±0 . 5 13 . 9±0 . 3 32 . 3±0 . 3 42 . 8±0 . 4 - - CAFE [ 43 ] 30 . 3±1 . 1 46 . 3±0 . 6 55 . 5±0 . 6 12 . 9±0 . 3 27 . 8±0 . 3 37 . 9±0 . 3 - - FRePo [ 52 ] 45 . 1±0 . 5 59 . 1±0 . 3 69 . 6±0 . 4 25 . 9±0 . 1 † 40 . 9±0 . 1 - 13 . 5±0 . 1 † 20 . 4±0 . 1 MTT [ 5 ] 46 . 2±0 . 8 65 . 4±0 . 7 71 . 6±0 . 2 24 . 3±0 . 3 39 . 7±0 . 4 47 . 7±0 . 2 8 . 8±0 . 3 23 . 2±0 . 2 TESLA [ 12 ] 48 . 5±0 . 8 † 66 . 4±0 . 8 72 . 6±0 . 7 24 . 8±0 . 4 41 . 7±0 . 3 47 . 9±0 . 3 9 . 8±0 . 4 24 . 4±0 . 6 FTD [ 14 ] 46 . 8±0 . 3 66 . 6±0 . 3 † 73 . 8±0 . 2 † 25 . 2±0 . 2 43 . 4±0 . 3 † 50 . 7±0 . 3 † 10 . 4±0 . 3 24 . 5±0 . 2 † Ours 48 . 8±0 . 9 67 . 1±0 . 4 74 . 6±0 . 5 26 . 6±0 . 4 44 . 4±0 . 6 51 . 7±0 . 7 13 . 7±1 . 4 25 . 7±1 . 1 Table 3 . Performance comparision trained with 128 width - ConvNet [ 17 ] to other state - of - the - art methods on the CIFAR and Tiny ImageNet . We cite the reported results from Sachdeva et al . [ 37 ] and Du et al . [ 14 ] . IPC : Images Per class . Bold digits represent the best results and † refers to the second - best results among all the methods . . Evaluation Model ConvNet ResNet VGG AlexNet M e t hod DC 53 . 9±0 . 5 20 . 8±1 . 0 38 . 8±1 . 1 28 . 7±0 . 7 CAFE 55 . 5±0 . 4 25 . 3±0 . 9 40 . 5±0 . 8 34 . 0±0 . 6 MTT 71 . 6±0 . 2 61 . 9±0 . 7 55 . 4±0 . 8 48 . 2±1 . 0 FTD 73 . 8±0 . 2 65 . 7±0 . 3 58 . 4±1 . 6 53 . 8±0 . 9 Ours 74 . 6±0 . 5 67 . 3±0 . 4 60 . 3±0 . 5 56 . 7±0 . 3 Table 4 . Generalization testing of different architectures on CIFAR - 10 dataset with IPC 50 . IPC ImageNette ImageWoof ImageFruit ImageMeow 1 10 1 10 1 10 1 10 Fulldataset 87 . 4±1 . 0 67 . 0±1 . 3 63 . 9±2 . 0 66 . 7±1 . 1 MTT [ 5 ] 47 . 7±0 . 9 63 . 0±1 . 3 28 . 6±0 . 8 35 . 8±1 . 8 26 . 6±0 . 8 40 . 3±1 . 3 30 . 7±1 . 6 40 . 4±2 . 2 FTD [ 14 ] 52 . 2±1 . 0 67 . 7±0 . 7 30 . 1±1 . 0 38 . 8±1 . 4 29 . 1±0 . 9 44 . 9±1 . 5 33 . 8±1 . 5 43 . 3±0 . 6 Ours 53 . 1±0 . 8 68 . 0±1 . 2 30 . 9±0 . 6 39 . 1±1 . 5 30 . 0±1 . 2 45 . 1±1 . 5 34 . 2±1 . 5 43 . 9±1 . 7 Table 5 . Applying our methods to 128 × 128 resolution ImageNet subsets . Bold digits represent the best results . 4 . 3 . Results on ImageNet Subsets ( 128 × 128 ) To further evaluate our method , we present results on a larger and more challenging dataset in Tab . 5 . The Ima - geNet subsets pose a greater difficulty compared to CIFAR - 10 / 100 and Tiny ImageNet , primarily due to their higher resolutions . The higher resolution makes it challenging for the distillation procedure to converge . The ImageNet sub - sets consist of 10 categories selected from ImageNet - 1k , following the setting of MTT . These subsets include Ima - geNette ( assorted objects ) , ImageWoof ( dog breeds ) , Im - ageFruits ( fruits ) , and ImageMeow ( cats ) . As shown in Tab . 5 , our method significantly improves MTT in every subset . For instance , we achieve a significant performance boost on the ImageNette subset with ipc = 1 and 10 , sur - passing MTT by more than 5 . 0 % . We also record the FTD results for fair comparison and our method achieves better results . 4 . 4 . Analysis 4 . 4 . 1 The Impact of Expert Trajectory Smoothness Fig . 4 and Tab . 6 elucidate why previous works have in - variably opted for naive SGD as the optimizer . This choice of SGD strikes an unavoidable trade - off between the per - formance of expert model and outcome of distilled dataset . For instance , as shown in the Tab . 6a , the expert model alone achieves a modest score of 48 . 6 % , whereas distilla - tion yields a respectable 39 . 7 % . However , despite adopting SGD with momentum or Adam enhancing expert perfor - mance ( shown in blue and yellow solid lines ) , it leads to the variation of parameters changing so fast ( shown in blue and yellow dash lines ) that surpasses the limitation for distilla - tion . Finally , it will cause significant declines in distillation results , even gradient explosions and training collapses , es - pecially for Adam . The essence of our proposed method for generating smooth expert trajectories lies in constraining the speed of parameter variation . The ideal expert trajectory exhibits slow parameter variations with consistent performance im - 8 0 10 20 30 40 50 Expert Trajectory Epoch on CIFAR100 0 2 4 6 8 10 12 14 | | t t + 1 | | 22 NO _ MOM MOM Flat _ MOM ADAM 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 A cc u r a c y ( a ) CIFAR - 100 0 10 20 30 40 50 Expert Trajectory Epoch on CIFAR100 0 1 2 3 4 5 6 7 8 | | t t + 1 | | 22 NO _ MOM MOM Flat _ MOM ADAM 0 . 05 0 . 10 0 . 15 0 . 20 0 . 25 0 . 30 0 . 35 0 . 40 A cc u r a c y ( b ) Tiny ImageNet Figure 4 . ∥ θ t − θ t + 1 ∥ 22 refers to the change of the model weights on two consecutive iterations , shown by dash curve . Correspondingly , the solid curve refers to the metric of evaluation accuracy . NO MOM refers to SGD without momentum . MOM means using SGD with momentum alone . Flat MOM denotes applying gradient penalty and clipped loss under the usage of SGD with momentum . ADAM means using ADAM as optimizer alone . We view the red curve as a much smoother and higher - quality expert trajectory . The ∥ θ t − θ t + 1 ∥ 22 of Adam is so huge that it cannot fully appear in both CIFAR - 100 and Tiny ImageNet . Momentum GradientPenalty ClippedLoss Avg Var ↓ Acc . ( Expert ) ↑ Acc . ( Distill ) ↑ × × × 0 . 1726 48 . 6 39 . 7 ✓ × × 7 . 9611 ( × 46 ) 57 . 1 ( + 8 . 5 ) 18 . 8 ( − 20 . 9 ) ✓ ✓ × 0 . 9331 ( × 5 . 4 ) 54 . 1 ( + 5 . 5 ) 41 . 7 ( + 2 . 0 ) ✓ ✓ ✓ 0 . 5899 ( × 3 . 4 ) 54 . 4 ( + 5 . 8 ) 42 . 0 ( + 2 . 3 ) ( a ) CIFAR - 100 Momentum GradientPenalty ClippedLoss Avg Var ↓ Acc . ( Expert ) ↑ Acc . ( Distill ) ↑ × × × 0 . 0705 25 . 8 8 . 8 ✓ × × 2 . 2950 ( × 33 ) 39 . 4 ( + 13 . 6 ) 1 . 8 ( − 7 . 0 ) ✓ ✓ × 1 . 7358 ( × 24 ) 39 . 0 ( + 13 . 2 ) 10 . 0 ( + 1 . 2 ) ✓ ✓ ✓ 1 . 3066 ( × 18 ) 39 . 5 ( + 13 . 7 ) 10 . 8 ( + 2 . 0 ) ( b ) Tiny ImageNet Table 6 . Comparison between different buffer generation methods using SGD as base optimizer . provements along the iterations . To quantify smoothness , we employ a metric called Avg V ar to measure the av - erage weight variation magnitude between two iterations along the whole training process : Avg V ar = E t (cid:104) ∥ θ t − θ t + 1 ∥ 22 (cid:105) Fig . 4 shows ∥ θ t − θ t + 1 ∥ 22 of each epoch and Tab . 6 demonstrates the average result . Through employing gra - dient penalty and loss clipping , we achieve significant im - provements in both expert performance ( an increase of 5 . 8 % and 13 . 7 % ) and distilled dataset performance ( an in - crease of 2 . 3 % and 2 . 0 % ) while only increasing Avg V ar by a factor of 3 . 4 and 18 ( compared to 46 × and 33 × for di - rect momentum addition , which is just a small increment ) . 4 . 4 . 2 The Impact of Balance Strategy As demonstrated in Tab . 7 , we conduct ablation experiments on each proposed module based on the usage of smooth ex - pert trajectory . In the ‘Balance Stochasticity’ part , the act of selecting representative samples for initialization resulted in an incremental gain of 0 . 5 % . In order to visualize the effec - tiveness of the selection strategy , We randomly choose five classes and apply PCA [ 45 ] to reduce the high - dimensional features to two dimensions . As can be seen from Fig . 6 , compared to random initialization , our proposed method avoids introducing huge bias coming from outliers . Addi - tionally , the distribution of distilled samples is more uni - form , and there is no over - concentration in a specific area . Moreover , the application of a balanced inner - loop loss leads to a further enhancement , yielding an improvement of 0 . 4 % . As mentioned in Sec . 3 . 2 . 2 , we compared another method : random initialization of t within a certain range . We set the range to be within 5 when selecting the next ex - pert starting point . From Fig . 5 , the experiments indicate that initialization within a range not only lacks a positive effect but , conversely , introduces a potential erroneous in - ductive bias that results in a decline in distillation outcomes . 4 . 4 . 3 The Impact of Accumulated Error To explore the contributions of the two error reduction methods , we conduct ablation experiments as depicted in Tab . 7 . In the ‘Alleviate Errors’ part , the experiments in - dicate that using weight perturbation leads to the most im - provement ( + 0 . 8 % ) in distilled dataset performance . How - 9 Methods Acc . Distill ( Gain ) Base 42 . 0 Balance Stochasticity : + Representative Initialization 42 . 5 ( + 0 . 5 ) + Balanced Inner - loop Loss 42 . 9 ( + 0 . 4 ) Alleviate Errors : + Intermediate Matching Loss 43 . 6 ( + 0 . 7 ) + Weight Perturbation 44 . 4 ( + 0 . 8 ) Table 7 . We use the distilled results obtained by applying smooth expert trajectory as the ‘Base’ . Following that , we conduct two parts ablation studies ( stochasticity and errors ) on the CIFAR - 100 dataset under IPC = 10 . Figure 5 . Results of ablation study on weight perturbation , inter - mediate matching loss and balanced strategy . ever , when applying Dropout after weight perturbation shown in Fig . 5 , which introduces another type of drop noise , we have not observed any improvement in the results . Besides , intermediate matching loss contributes to an in - crease of 0 . 7 % to the final outcome , which elucidates that extended - range interactions indeed may lead to a diver - gence in the optimization direction within the inner loop . Reducing this part of the accumulated error is equally sig - nificant . We also experiment with two different strate - gies for combining multiple intermediate matching losses . As illustrated in Fig . 5 , we observe that a straightforward approach , where the same scale of β coefficient is used , yielded superior results . 5 . Conclusion In this paper , we propose a novel dataset distillation strat - egy to address the problems of interplay between expert and student , sensitivity to stochastic variables , and accu - mulated errors . Building upon this , we introduce clipping loss and gradient penalty to smooth the expert trajectories under a more potent optimizer . To balance the impact of two random variables , we propose using representative ini - tialization for D syn and balanced inner - loop loss . Besides , we propose intermediate matching loss and weight pertur - bation to mitigate the errors arising from long - range inner steps and the discrepancies between distillation and evalu - ation . Furthermore , our methods are designed to be easily implemented and plugged in . Extensive experiments have confirmed the effectiveness of our approach . We hope our method can pave the way for future works on dataset distil - lation References [ 1 ] Pankaj K Agarwal , Sariel Har - Peled , and Kasturi R Varadarajan . Approximating extent measures of points . Journal of the ACM ( JACM ) , 51 ( 4 ) : 606 – 635 , 2004 . 1 [ 2 ] Rahaf Aljundi , Min Lin , Baptiste Goujaud , and Yoshua Ben - gio . Gradient based sample selection for online continual learning . Advances in Neural Information Processing Sys - tems , 32 , 2019 . 1 [ 3 ] Martin Arjovsky , Soumith Chintala , and L´eon Bottou . Wasserstein generative adversarial networks . In Interna - tional conference on machine learning , pages 214 – 223 . PMLR , 2017 . 4 [ 4 ] Ondrej Bohdal , Yongxin Yang , and Timothy Hospedales . Flexible dataset distillation : Learn labels instead of images . arXiv preprint arXiv : 2006 . 08572 , 2020 . 3 [ 5 ] George Cazenavette , Tongzhou Wang , Antonio Torralba , Alexei A Efros , and Jun - Yan Zhu . Dataset distillation by matching training trajectories . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , pages 4750 – 4759 , 2022 . 1 , 3 , 6 , 8 [ 6 ] George Cazenavette , Tongzhou Wang , Antonio Torralba , Alexei A Efros , and Jun - Yan Zhu . Generalizing dataset distillation via deep generative prior . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , pages 3739 – 3748 , 2023 . 3 [ 7 ] Dingfan Chen , Raouf Kerkouche , and Mario Fritz . Private set generation with discriminative information . Advances in Neural Information Processing Systems , 35 : 14678 – 14690 , 2022 . 1 [ 8 ] Yutian Chen , Max Welling , and Alex Smola . Super - samples from kernel herding . arXiv preprint arXiv : 1203 . 3472 , 2012 . 1 , 6 , 8 [ 9 ] Zongxiong Chen , Jiahui Geng , Herbert Woisetschlaeger , Sonja Schimmler , Ruben Mayer , and Chunming Rong . A comprehensive study on dataset distillation : Perfor - mance , privacy , robustness and fairness . arXiv preprint arXiv : 2305 . 03355 , 2023 . 1 [ 10 ] David A Cohn , Zoubin Ghahramani , and Michael I Jordan . Active learning with statistical models . Journal of artificial intelligence research , 4 : 129 – 145 , 1996 . 1 [ 11 ] Justin Cui , Ruochen Wang , Si Si , and Cho - Jui Hsieh . Dc - bench : Dataset condensation benchmark . Advances in Neu - ral Information Processing Systems , 35 : 810 – 822 , 2022 . 1 [ 12 ] Justin Cui , Ruochen Wang , Si Si , and Cho - Jui Hsieh . Scaling up dataset distillation to imagenet - 1k with constant memory . arXiv preprint arXiv : 2211 . 10586 , 2022 . 1 , 3 , 6 , 8 [ 13 ] Jia Deng , Wei Dong , Richard Socher , Li - Jia Li , Kai Li , and Li Fei - Fei . Imagenet : A large - scale hierarchical image 10 20 10 0 10 20 30 Class 0 . 20 15 10 5 0 5 10 15 20 20 10 0 10 20 Class 1 . 20 10 0 10 20 30 20 10 0 10 20 Class 2 . 20 10 0 10 20 20 10 0 10 20 30 Class 3 . 20 10 0 10 20 30 20 10 0 10 20 30 Class 4 . 20 15 10 5 0 5 10 15 20 Figure 6 . Example clustering and sub - cluster centre results . ⋆ denotes the representative initialization samples while × means the random initialization samples . database . In Proceedings of the IEEE conference on com - puter vision and pattern recognition , pages 248 – 255 , 2009 . 6 [ 14 ] Jiawei Du , Yidi Jiang , Vincent TF Tan , Joey Tianyi Zhou , and Haizhou Li . Minimizing the accumulated trajec - tory error to improve dataset distillation . arXiv preprint arXiv : 2211 . 11004 , 2022 . 1 , 2 , 3 , 5 , 6 , 8 [ 15 ] Pedro F Felzenszwalb , Ross B Girshick , David McAllester , and Deva Ramanan . Object detection with discriminatively trained part - based models . IEEE transactions on pattern analysis and machine intelligence , 32 ( 9 ) : 1627 – 1645 , 2010 . 1 [ 16 ] Chelsea Finn , Pieter Abbeel , and Sergey Levine . Model - agnostic meta - learning for fast adaptation of deep networks . In International conference on machine learning , pages 1126 – 1135 . PMLR , 2017 . 3 [ 17 ] Spyros Gidaris and Nikos Komodakis . Dynamic few - shot visual learning without forgetting . In Proceedings of the IEEE conference on computer vision and pattern recogni - tion , pages 4367 – 4375 , 2018 . 8 [ 18 ] Gabriel Goh . Why momentum really works . Distill , 2017 . 4 [ 19 ] Ishaan Gulrajani , Faruk Ahmed , Martin Arjovsky , Vincent Dumoulin , and Aaron C Courville . Improved training of wasserstein gans . Advances in Neural Information Process - ing Systems , 30 , 2017 . 4 [ 20 ] Tao Huang , Shan You , Fei Wang , Chen Qian , and Chang Xu . Knowledge distillation from a stronger teacher . Advances in Neural Information Processing Systems , 35 : 33716 – 33727 , 2022 . 3 [ 21 ] Jang - Hyun Kim , Jinuk Kim , Seong Joon Oh , Sangdoo Yun , Hwanjun Song , Joonhyun Jeong , Jung - Woo Ha , and Hyun Oh Song . Dataset condensation via efficient synthetic - data parameterization . In International Conference on Ma - chine Learning , pages 11102 – 11118 . PMLR , 2022 . 3 [ 22 ] Alex Krizhevsky , Geoffrey Hinton , et al . Learning multiple layers of features from tiny images . 2009 . 6 [ 23 ] Agata Lapedriza , Hamed Pirsiavash , Zoya Bylinskii , and Antonio Torralba . Are all training examples equally valu - able ? arXiv preprint arXiv : 1311 . 6510 , 2013 . 1 [ 24 ] Ya Le and Xuan Yang . Tiny imagenet visual recognition challenge . CS 231N , 7 ( 7 ) : 3 , 2015 . 6 [ 25 ] Saehyung Lee , Sanghyuk Chun , Sangwon Jung , Sangdoo Yun , and Sungroh Yoon . Dataset condensation with con - trastive signals . In International Conference on Machine Learning , pages 12352 – 12364 . PMLR , 2022 . 3 [ 26 ] Ping Liu , Xin Yu , and Joey Tianyi Zhou . Meta knowledge condensation for federated learning . In ICLR , 2023 . 1 [ 27 ] Songhua Liu , Kai Wang , Xingyi Yang , Jingwen Ye , and Xin - chao Wang . Dataset distillation via factorization . Advances in Neural Information Processing Systems , 2022 . 3 [ 28 ] Yugeng Liu , Zheng Li , Michael Backes , Yun Shen , and Yang Zhang . Backdoor attacks against dataset distillation . In Proceedings of the Network and Distributed System Security Symposium , 2023 . 1 [ 29 ] Noel Loo , Ramin Hasani , Mathias Lechner , and Daniela Rus . Dataset distillation fixes dataset reconstruction attacks . arXiv preprint arXiv : 2302 . 01428 , 2023 . 1 [ 30 ] Dmitry Medvedev and Alexander D’yakonov . Learning to generate synthetic training data using gradient matching and implicit differentiation . In Proceedings of the International Conference on Analysis of Images , Social Networks and Texts , pages 138 – 150 , 2021 . 1 [ 31 ] Giung Nam , Hyungi Lee , Byeongho Heo , and Juho Lee . Im - proving ensemble distillation with weight averaging and di - versifying perturbation . arXiv preprint arXiv : 2206 . 15047 , 2022 . 5 [ 32 ] Giung Nam , Jongmin Yoon , Yoonho Lee , and Juho Lee . Di - versity matters when learning from ensembles . Advances in Neural Information Processing Systems , 34 : 8367 – 8377 , 2021 . 5 [ 33 ] Timothy Nguyen , Zhourong Chen , and Jaehoon Lee . Dataset meta - learning from kernel ridge - regression . In ICLR , 2021 . 3 [ 34 ] Timothy Nguyen , Roman Novak , Lechao Xiao , and Jaehoon Lee . Dataset distillation with infinitely wide convolutional networks . Advances in Neural Information Processing Sys - tems , 34 : 5186 – 5198 , 2021 . 3 [ 35 ] J Arturo Olvera - L ´ opez , J Ariel Carrasco - Ochoa , J Mart ´ ınez - Trinidad , and Josef Kittler . A review of instance selec - tion methods . Artificial Intelligence Review , 34 ( 2 ) : 133 – 143 , 2010 . 1 [ 36 ] Renjie Pi , Weizhong Zhang , Yueqi Xie , Jiahui Gao , Xiaoyu Wang , Sunghun Kim , and Qifeng Chen . DYNAFED : Tack - ling client data heterogeneity with global dynamics . arXiv preprint arXiv : 2211 . 10878 , 2022 . 1 [ 37 ] Noveen Sachdeva and Julian McAuley . Data distillation : A survey . arXiv preprint arXiv : 2301 . 04272 , 2023 . 1 , 2 , 8 11 [ 38 ] Ozan Sener and Silvio Savarese . Active learning for convolu - tional neural networks : A core - set approach . arXiv preprint arXiv : 1708 . 00489 , 2017 . 1 [ 39 ] Shitong Shao , Huanran Chen , Zhen Huang , Linrui Gong , Shuai Wang , and Xinxiao Wu . Teaching what you should teach : A data - based distillation method . International Joint Conference on Artificial Intelligence , 2023 . 3 [ 40 ] Felipe Petroski Such , Aditya Rawal , Joel Lehman , Kenneth Stanley , and Jeffrey Clune . Generative teaching networks : Accelerating neural architecture search by learning to gener - ate synthetic training data . In International Conference on Machine Learning , pages 9206 – 9216 . PMLR , 2020 . 1 [ 41 ] Ilia Sucholutsky and Matthias Schonlau . Soft - label dataset distillation and text dataset distillation . In International Joint Conference on Neural Networks , pages 1 – 8 . IEEE , 2021 . 3 [ 42 ] Mariya Toneva , Alessandro Sordoni , Remi Tachet des Combes , Adam Trischler , Yoshua Bengio , and Geoffrey J Gordon . An empirical study of example forgetting during deep neural network learning . In ICLR , 2019 . 1 , 6 , 8 [ 43 ] Kai Wang , Bo Zhao , Xiangyu Peng , Zheng Zhu , Shuo Yang , Shuo Wang , Guan Huang , Hakan Bilen , Xinchao Wang , and Yang You . Cafe : Learning to condense dataset by align - ing features . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , pages 12196 – 12205 , 2022 . 3 , 6 , 8 [ 44 ] Tongzhou Wang , Jun - Yan Zhu , Antonio Torralba , and Alexei A Efros . Dataset distillation . arXiv preprint arXiv : 1811 . 10959 , 2018 . 3 [ 45 ] Svante Wold , Kim Esbensen , and Paul Geladi . Principal component analysis . Chemometrics and intelligent labora - tory systems , 2 ( 1 - 3 ) : 37 – 52 , 1987 . 9 [ 46 ] Ruonan Yu , Songhua Liu , and Xinchao Wang . Dataset distillation : A comprehensive review . arXiv preprint arXiv : 2301 . 07014 , 2023 . 1 , 3 [ 47 ] Mengyang Yuan , Bo Lang , and Fengnan Quan . Student - friendly knowledge distillation . arXiv preprint arXiv : 2305 . 10893 , 2023 . 3 [ 48 ] Bo Zhao and Hakan Bilen . Dataset condensation with differ - entiable siamese augmentation . In International Conference on Machine Learning , pages 12674 – 12685 . PMLR , 2021 . 3 , 5 , 6 , 8 [ 49 ] Bo Zhao and Hakan Bilen . Synthesizing informative train - ing samples with gan . NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research , 2022 . 3 [ 50 ] Bo Zhao and Hakan Bilen . Dataset condensation with dis - tribution matching . In Proceedings of the IEEE / CVF Winter Conference on Applications of Computer Vision , 2023 . 3 , 6 , 8 [ 51 ] Bo Zhao , Konda Reddy Mopuri , and Hakan Bilen . Dataset condensation with gradient matching . In ICLR , 2021 . 3 , 6 , 8 [ 52 ] Yongchao Zhou , Ehsan Nezhadarya , and Jimmy Ba . Dataset distillation using neural feature regression . arXiv preprint arXiv : 2206 . 00719 , 2022 . 3 , 6 , 8 [ 53 ] Yanlin Zhou , George Pu , Xiyao Ma , Xiaolin Li , and Dapeng Wu . Distilled one - shot federated learning . arXiv preprint arXiv : 2009 . 07999 , 2020 . 1 12 6 . Limitations and Future Work Although we have made significant improvements based on the currently best - performing MTT framework , focus - ing on enhancing the generation of expert trajectories and parameters matching , there are still two limitations to ad - dress . Firstly , the expert trajectories allocate a substantial amount of storage space . Storing just five out of the 50 tra - jectories , each comprising 50 weight iterations , already re - quires 800MB . Hence , future research should explore meth - ods to achieve similar results with fewer expert trajectories , perhaps even one or two . Secondly , there is still room for improvement in the per - formance of distilled images across various evaluation mod - els . Currently , ConvNet shows the best results . Ensuring that distilled datasets consistently exhibit competitive per - formance on different evaluation models is critical for de - velopment of dataset distillation . Future work might involve leveraging prior knowledge to enhance the distilled dataset generation . 7 . Models We employ the ConvNet architecture in the distillation process , in line with other methods mentioned in Sec . 4 . 1 , except for FrePo , which utilizes a different model that dou - bles the number of filters when the feature map size is halved . Our architecture comprises 128 filters in the convo - lutional layer with a 3 × 3 kernel , followed by instance nor - malization , ReLU activation , and an average pooling layer with a 2 × 2 kernel and stride 2 . For CIFAR - 10 and CIFAR - 100 , we adopt 3 - layer convolutional networks ( ConvNet - 3 ) . In the case of the Tiny ImageNet dataset with a resolution of 64 × 64 , we employ a depth - 4 ConvNet . For the ImageNet subsets with a resolution of 128 × 128 , a depth - 5 ConvNet is used . 8 . Training Burden While we have introduced additional plug - in modules into the distillation process , the training time for each image in our approach remains comparable to that of the original Dataset Image per class 1 Iter . ( sec ) CIFAR - 10 1 0 . 5 10 0 . 6 50 0 . 9 CIFAR - 100 1 0 . 6 10 0 . 85 50 1 . 97 Tiny ImageNet 1 1 . 15 10 2 . 42 Table 8 . Distillation time for each dataset and support ipc . MTT . It only requires a slight increase in time for each iter - ation , as illustrated in Tab . 8 . However , notably , our method exhibits faster conver - gence and superior results . As depicted in Fig . 7 , our ap - proach on CIFAR100 essentially attains the final perfor - mance of the original MTT after just 500 iterations , while the original MTT requires 5000 iterations to achieve the same level of performance . Moreover , our method consis - tently demonstrates improvement . For example on TinyIm - ageNet , our approach’s performance continues to ascend , in contrast to MTT , which essentially plateaus after 3000 iter - ations . 9 . Examples of Training Instability The sources of training instability can be attributed to two main factors . Firstly , the original MTT itself is prone to encountering sudden spikes in gradients , which can lead to training collapse . However , by incorporating the proposed methods , our training process becomes significantly more stable . As illustrated in Fig . 8 , our final loss is substantially lower than that of the original MTT , indicating a more ef - fective transfer of expert network knowledge into the target compressed dataset . The second source of instability stems from the param - eter variations in the expert model trajectories . We utilize simple momentum - based SGD as the optimizer for training the expert model . Subsequently , we compare a crucial out - put during each iteration : the learning rate . From Fig . 9 , it becomes evident that as the expert trajectories become less smooth , the learnable quantity experiences huge fluc - tuations . At around 1800 iterations , it even reaches Nan values due to sudden gradient explosions . This emphasizes the importance of generating smooth , slowly changing , and high - quality expert trajectories . 13 0 1000 2000 3000 4000 5000 Evaluation on CIFAR100 ( ipc = 10 ) 0 . 20 0 . 25 0 . 30 0 . 35 0 . 40 0 . 45 M a x A cc u r a c y 0 1000 2000 3000 4000 5000 Evaluation on CIFAR100 ( ipc = 50 ) 0 . 36 0 . 38 0 . 40 0 . 42 0 . 44 0 . 46 0 . 48 0 . 50 0 . 52 Ours MTT 0 1000 2000 3000 4000 5000 Evaluation on Tiny ImageNet ( ipc = 10 ) 0 . 075 0 . 100 0 . 125 0 . 150 0 . 175 0 . 200 0 . 225 0 . 250 Figure 7 . Applying our proposed methods brings stable performance and efficiency improvements . Figure 8 . Comparison between proposed methods and original MTT . Our methods can generate a much lower final loss . ( a ) Flat expert trajectory ( b ) Non - flat expert trajectory Figure 9 . Learning curve for student model learning rate . When applying a non - flat expert trajectory , the output of the learning rate may encounter Nan which will lead to the collapse of training pro - cess . 14 10 . Distilled Dataset Visualization We visualize parts of the distilled dataset of Tiny ImageNet in Fig . 10 . Figure 10 . Visualizations of synthetic images in Tiny ImageNet . 15