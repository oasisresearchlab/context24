Evaluation Methods for Creativity Support Environments Andruid Kerne Andrew M . Webb Interface Ecology Lab Texas A & M University College Station , TX , USA { andruid , awebb } @ cse . tamu . edu Celine Latulipe Erin Carroll HCILabUniversity of North Carolina at CharlotteCharlotte , NC , USA { clatulip , e . carroll } @ uncc . edu Steven M . Drucker Microsoft Research Redmond , WA , USA sdrucker @ microsoft . com Linda Candy Creativity and Cognition Studios University of Technology , Sydney Sydney , Australia linda @ lindacandy . com Kristina H¨o¨ok Swedish Institute of Computer ScienceKista , Sweden kia @ sics . se Copyright is held by the author / owner ( s ) . CHI 2013 , April 27 – May 2 , 2013 , Paris , France . ACM xxx - x - xxxx - xxxx - x / xx / xx . Abstract Creativity refers to the human processes that underpin sublime forms of expression and fuel innovation . Creativity support environments ( CSEs ) address diverse areas , including education , science , business , programming , design , art , performance , and everyday life . An environment may consist of a desktop application , or involve specialized hardware , networked topologies , and mobile devices . CSEs may address temporal - spatial aspects of collaborative work . This workshop will gather a community of researchers developing and evaluating creativity support environments . We will share approaches , engage in dialogue , and develop best practices . The outcome will not be a single prescription , but rather a landscape of routes , an ontology of methodologies with consideration to how they map to creative activities , and an emerging consensus on the range of expectations for rigorous evaluation to shape the ﬁeld of CSE research . The workshop will organize an open repository of CSE evaluation methods and test data . Keywords creativity , evaluation , innovation , art , design , user studies ACM Classiﬁcation Keywords H . 5 [ Information interfaces and presentation ( e . g . , HCI ) ] : . General Terms Human Factors , Design , Theory Introduction Creativity refers to the human processes that underpin sublime forms of expression and fuel innovation . Innovation is widely recognized as the key to information age economies [ 14 ] . This workshop will explore methodologies for assessing interactive tools , experiences , and environments designed to support , promote , and provoke creativity . A 2005 NSF workshop saw the development of creativity support tools , and their evaluation as a strikingly new ﬁeld [ 16 ] . Since a tool is designed to carry out a particular function – and valuable creative experiences may be more ﬂuid in their inception , intention , and outcomes – we expand the scope , using the nomenclature ‘environments , ’ a superset of ‘tools . ’ Rather than viewing the evaluation of creativity support environments ( CSEs ) as a strange , unknown territory , which can be dabbled in , we take the position that researchers are developing sophisticated methods , which have progressed well beyond infancy . Yet , these sophisticated methods are primarily only applied by the researchers who create them . CSE research will beneﬁt from increased utilization of evaluation methods by more researchers in diﬀerent contexts . A 2007 CHI workshop explored some of these methods by bringing together HCI researchers and artists to discuss evaluation methodology for creative engagement in respective ﬁelds [ 1 ] . We build upon these prior workshops , emphasizing methods to improve reuse of evaluation methods in future research . The goal of this workshop is to bring together the community involved in developing these methods and to inspire discussion and even debate about the value of particular methods in various types of situated contexts . The outcome of the workshop will not be a single prescription , but rather an ontology of methodologies with consideration to how they map to creative activities , and an emerging consensus on the range of expectations for rigorous evaluation to shape the ﬁeld of creativity support environments research . The workshop will initiate an open repository of CSE evaluation methodologies , testing data , and descriptions of situated contexts where methods were applied . The discussion of CSE evaluation methodologies will form a basis for repository content . Creativity involves engagement in a range of tasks and activities , from inspiration - based human expression that deﬁnes the arts to the development of scientiﬁc and humanities research , from the development of solutions to diverse open - ended questions to the invention of new products and services . Creativity is characterized by human experiences and processes , as well as products and outcomes . Creativity involves insight , emergence , innovation , ideation , and novelty . Both activities that are open - ended and activities characterized by constraints , can be creative . Open - ended tasks may involve divergent thinking questions with multiple correct answers [ 15 ] . Examples of divergent thinking tasks include the identiﬁcation of paper topics , the development of theses , and contextualized design problems in diverse domains , including visual design , interaction design , architecture , and engineering . The products of divergent thinking tasks have been assessed using quantitative ideation metrics , such as novelty , variety , ﬂuency , and quality [ 15 ] . Creativity Support Environments Creativity support environments span diverse domains , including music , art , education , science , programming , performance , and entrepreneurship . An environment may consist of a desktop application . Interaction may be embodied . It may involve specialized hardware , instrumented spaces , networked topologies , and mobile devices . CSEs may address temporal - spatial dimensions of collaborative work , requiring evaluation methods that address synchronous , asynchronous , co - located , and distributed interaction . CSEs need to be sensitive to the contextualized practices which they are situated , potentially accounting for diﬀerentiated roles . CSEs are designed to provide stimuli to provoke , enhance , and promote creative processes and products . While some disruption can promote creativity , other types can disrupt the sense of immersion in the creative task . Designers seek to prevent disruptions and interface distractions that hinder creative processes . Thus , to evaluate CSEs , one may need to assess both how creative work is supported in terms of factors that promote creativity , and also factors that hinder creativity . Although creativity is often associated with expertise in a particular domain , creativity likely occurs across a spectrum – from large - scale creativity ( Big - C creativity ) to everyday creativity ( little - c ) to personal creativity ( mini - c ) [ 2 , 7 ] . CSEs are designed to support creative work across this spectrum . Evaluation approaches need to determine how well an environment supports diﬀering skill and creativity levels . Prior Work The NSF workshop on creativity support tools ( CSTs ) discussed the importance of going beyond traditional productivity measurements [ 8 ] . Likewise , in the BELIV workshops , the information visualization community advocated novel evaluation methods [ 3 ] . Creative tasks , as with information visualization tasks , require researchers to move beyond time and error metrics . The NSF workshop emphasized that CST evaluation should involve a mixed - methods approach to help researchers better understand the role of CSTs in creative processes . The CHI workshop on HCI and new media arts suggested using mixed - methods for evaluating art - science collaborations in which systematic methodologies are missing [ 1 ] . Researchers in CSEs have already developed a range of evaluation methods , including metrics and longitudinal studies . Kerne et al . developed a quantitative methodology for the evaluation of information - based ideation tasks , invoking a battery of mutually independent ideation metrics to assess creative products [ 11 ] . Webb and Kerne reﬁned methods for calculating the novelty and variety information - based ideation metrics [ 18 ] . Carroll et al . [ 5 ] developed a psychometric tool called the Creativity Support Index . Kim and Maher used a protocol analysis to show that tangible interaction improved engagement in spatial cognition [ 12 ] . Carroll and Latulipe also used a physiological sensor approach to detect high creative experience in CSTs using machine learning [ 4 ] . Qualitative approaches are also important . Shneiderman and Plaisant [ 17 ] conducted longitudinal case studies with expert users of information visualization tools . Latulipe et al . ’s longitudinal studies investigated the integration of technology into the creative dance process [ 13 ] . Kerne and Koh used grounded theory [ 6 ] to analyze case studies of the utilization of information composition in a course on creativity and entrepreneurship [ 10 ] . H¨o¨ok et al nicely problematized potential conﬂicts in artistic and scientiﬁc goals when evaluating interaction installations [ 9 ] . The Workshop We seek to gather the community of researchers developing and evaluating CSEs , so that people can share approaches and develop best practices . We assume that there is no one right way to evaluate creativity , but that diﬀerent methods can provide valuable perspectives . We seek papers that develop in - depth methods for evaluating CSEs . The methods must be explained with suﬃcient clarity and detail that others can apply them . Submitted papers should ground proposed methods by showing how they have been applied in the study of particular CSEs , and develop implications . They should motivate the kinds of CSEs for which a particular evaluation method is suited . We envision a lasting impact from this workshop through development of an open CSE evaluation repository . References [ 1 ] P . D . Adamczyk , K . Hamilton , M . B . Twidale , and B . P . Bailey . Hci and new media arts : methodology and evaluation . In CHI ’07 EA , 2007 . [ 2 ] R . Beghetto . Toward a broader conception of creativity : A case for” mini - c” creativity . Psychology of Aesthetics , 2007 . [ 3 ] E . Bertini , A . Perer , C . Plaisant , and G . Santucci . Beyond time and errors : novel evaluation methods for information visualization . In Proc ACM CHI , 2008 . [ 4 ] E . A . Carroll and C . Latulipe . Triangulating the Personal Creative Experience : Self - Report , External Judgments , and Physiology . In Proceedings of Graphics Interface 2012 , pages 53 – 60 , 2012 . [ 5 ] E . A . Carroll , C . Latulipe , R . Fung , and M . Terry . Creativity Factor Evaluation : Towards a Standardized Survey Metric for Creativity Support . In Proc ACM Creativity & Cognition 2009 . [ 6 ] J . M . Corbin and A . L . Strauss . Basics of qualitative research : Techniques and procedures for developing grounded theory . Sage Publications , Inc , 2008 . [ 7 ] M . Csikszentmihalyi . Creativity . ﬂow and the psychology of discovery and invention . Harper , 1997 . [ 8 ] T . Hewett , M . Czerwinkski , M . Terry , J . Nunamaker , L . Candy , B . Kules , and E . Sylvan . Creativity Support Tool Evaluation Methods and Metrics . In NSF Workshop on Creativity Support Tools , 2005 . [ 9 ] K . H¨o¨ok , P . Sengers , and G . Andersson . Sense and sensibility : evaluation and interactive art . In Proc CHI , 2003 . [ 10 ] A . Kerne and E . Koh . Representing collections as compositions to support distributed creative cognition and situated creative learning . NRHM , 13 ( 2 ) : 135 – 162 , 2007 . [ 11 ] A . Kerne , S . M . Smith , E . Koh , H . Choi , and R . Graeber . An experimental method for measuring the emergence of new ideas in information discovery . IJHCI , 24 ( 5 ) : 460 – 477 , 2008 . [ 12 ] M . Kim and M . Maher . Comparison of designers using a tangible user interface & graphical user interface and impact on spatial cognition . In Proc . Human Behaviour in Design , 2005 . [ 13 ] C . Latulipe , E . A . Carroll , and D . Lottridge . Evaluating Longitudinal Projects Combining Technology with Temporal Arts . In Proc CHI 2011 . [ 14 ] National Academy of Engineering . Engineering Research and America’s Future : Meeting the Challenges of a Global Economy . 2005 . [ 15 ] J . J . Shah , S . M . Smith , and N . Vargas - Hernandez . Metrics for measuring ideation eﬀectiveness . Design Studies , 24 ( 2 ) : 111 – 134 , 2003 . [ 16 ] B . Shneiderman , G . Fischer , and M . Czerwinski . Creativity support tools : Report from a US NSF sponsored workshop . IJHCI , 20 : 61 – 77 , 2006 . [ 17 ] B . Shneiderman and C . Plaisant . Strategies for evaluating information visualization tools : Multi - dimensional in - depth long - term case studies . In BELIV Workshop of AVI 2006 , pages 1 – 7 . [ 18 ] A . M . Webb and A . Kerne . Integrating implicit structure visualization with authoring promotes ideation . In Proc JCDL , pages 203 – 212 , 2011 .