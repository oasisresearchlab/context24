Convergence of a Stochastic Gradient Method with Momentum for Nonsmooth Nonconvex Optimization Vien V . Mai ∗ Mikael Johansson ∗ February 14 , 2020 Abstract Stochastic gradient methods with momentum are widely used in applications and at the core of optimization subroutines in many popular machine learning libraries . However , their sample complexities have never been obtained for problems that are non - convex and non - smooth . This paper establishes the convergence rate of a stochastic subgradient method with a momentum term of Polyak type for a broad class of non - smooth , non - convex , and constrained optimization problems . Our key innovation is the construction of a special Lyapunov function for which the proven complexity can be achieved without any tunning of the momentum parameter . For smooth problems , we extend the known complexity bound to the constrained case and demonstrate how the unconstrained case can be analyzed under weaker assumptions than the state - of - the - art . Numerical results conﬁrm our theoretical developments . 1 Introduction We study the stochastic optimization problem minimize x ∈X f ( x ) : = E P [ f ( x ; S ) ] = (cid:90) S f ( x ; s ) dP ( s ) , ( 1 ) where S ∼ P is a random variable ; f ( x ; s ) is the instantaneous loss parameterized by x on a sample s ∈ S ; and X ⊆ R n is a closed convex set . In this paper , we move beyond convex and / or smooth optimization and consider f that belongs to a broad class of non - smooth and non - convex functions called ρ - weakly convex , meaning that x (cid:55)→ f ( x ) + ρ (cid:107) x (cid:107) 22 is convex . This function class is very rich and important in optimization [ 39 , 47 ] . It trivially includes all convex functions , all smooth functions with Lipschitz continuous gradient , and all additive composite functions of the two former classes . More broadly , it includes all compositions of the form f ( x ) = h ( c ( x ) ) , ( 2 ) where h : R m → R is convex and L h - Lipschitz and c : R n → R m is a smooth map with L c - Lipschitz Jacobian . Indeed , the composite function f = h ◦ c is then weakly convex with ∗ Division of Decision and Control Systems , School of Electrical Engineering and Computer Science , KTH Royal Institute of Technology , SE - 100 44 Stockholm , Sweden . Emails : { maivv , mikaelj } @ kth . se . 1 a r X i v : 2002 . 05466v1 [ m a t h . O C ] 1 3 F e b 2020 ρ = L h L c [ 11 ] . Some representative applications in this problem class include nonlinear least squares [ 10 ] , robust phase retrieval [ 13 ] , Robust PCA [ 6 ] , robust low rank matrix recovery [ 7 ] , optimization of the Conditional Value - at - Risk [ 40 ] , graph synchronization [ 44 ] , and many others . Stochastic optimization algorithms for solving ( 1 ) , based on random samples S k drawn from P , are of fundamental importance in many applied sciences [ 4 , 43 ] . Since the introduction of the classical stochastic ( sub ) gradient descent method ( SGD ) in [ 38 ] , several modiﬁcations of SGD have been proposed to improve its practical and theoretical performance . A notable example is the use of a momentum term to construct an update direction [ 24 , 37 , 42 , 46 , 45 , 19 ] . The basis form of such a method ( when X = R n ) reads : x k + 1 = x k − α k z k ( 3a ) z k + 1 = β k g k + 1 + ( 1 − β k ) z k , ( 3b ) where z k is the search direction , g k is a stochastic subgradient ; α k is the stepsize , and β k ∈ ( 0 , 1 ] is the momentum parameter . For instance , the scheme ( 3 ) reduces to the stochastic heavy ball method ( SHB ) [ 37 ] : x k + 1 = x k − η k g k + λ k ( x k − x k − 1 ) , with η k = α k β k − 1 and λ k = ( 1 − β k − 1 ) α k / α k − 1 . Methods of this type enjoy widespread empirical success in large - scale convex and non - convex optimization , especially in training neural networks , where they have been used to produce several state - of - the - art results on important learning tasks , e . g . , [ 29 , 45 , 25 , 28 ] . Sample complexity , namely , the number of observations S 0 , . . . , S K required to reach a desired accuracy (cid:15) , has been the most widely adapted metric for evaluating the performance of stochastic optimization algorithms . Although sample complexity results for the standard SGD on problems of the form ( 1 ) have been obtained for convex and / or smooth problems [ 30 , 18 ] , much less is known in the non - smooth non - convex case [ 8 ] . The problem is even more prevalent in momentum - based methods as there is virtually no known complexity results for problems beyond those that are convex or smooth . 1 . 1 Related work As many applications in modern machine learning and signal processing cannot be captured by convex models , ( stochastic ) algorithms for solving non - convex problems have been studied extensively . Below we review some of the topics most closely related to our work . Stochastic weakly convex minimization Earlier works on this topic date back to Nur - minskii who showed subsequential convergence to stationary points for the subgradient method applied to deterministic problems [ 34 ] . The work [ 41 ] proposes a stochastic gradient averaging - based method and shows the ﬁrst almost sure convergence for this problem class . Basic suﬃ - cient conditions for convergence of stochastic projected subgradient methods is established in [ 15 ] . Thanks to the recent advances in statistical learning and signal processing , the problem class has been reinvigorated with several new theoretical results and practical applications ( see , e . g . , [ 13 , 12 , 9 , 8 ] and references therein ) . In particular , based on the theory of non - convex diﬀerential inclusions , almost sure convergence is derived in [ 14 ] for a collection of model - based minimization strategies , albeit no rates of convergence are given . An important step toward non - asymptotic convergence of stochastic methods is made in [ 9 ] . There , the 2 authors employ a proximal point technique for which they can show the sample complexity O ( 1 / (cid:15) 2 ) with a certain stationarity measure . Later , the work [ 8 ] shows that the ( approximate ) proximal point step in [ 9 ] is not necessary and establishes the similar complexity for a class of model - based methods including the standard SGD . We also note that there has been a large volume of works in smooth non - convex optimization , e . g . , [ 18 , 19 ] . Stochastic momentum for non - convex functions Optimization algorithms based on momentum averaging techniques go back to Polyak [ 36 ] who proposed the heavy ball method . In [ 31 ] , Nesterov introduced the accelerated gradient method and showed its optimal iteration complexity for the minimization of smooth convex functions . In the last few decades , research on accelerated ﬁrst - order methods has exploded both in theory and in practice [ 3 , 33 , 5 ] . The eﬀectiveness of such techniques in the deterministic context has inspired researchers to incor - porate momentum terms into stochastic optimization algorithms [ 37 , 41 , 46 , 45 , 19 ] . Despite evident success , especially , in training neural networks [ 29 , 45 , 25 , 50 , 28 ] , the theory for stochastic momentum methods is not as clear as its deterministic counterpart ( cf . [ 22 ] ) . As a result , there has been a growing interest in obtaining convergence guarantees for those methods under noisy gradients [ 27 , 22 , 49 , 16 , 19 ] . In non - convex optimization , almost certain convergence of Algorithm ( 3 ) for smooth and unconstrained problems is derived in [ 42 ] . Under the bounded gradient hypothesis , the convergence rate of the same algorithm has been estab - lished in [ 49 ] . The work [ 21 ] obtains the complexity of a gradient averaging - based method for constrained problems . In [ 19 ] , the authors study a variant of Nesterov acceleration and estab - lish a similar complexity for smooth and unconstrained problems , while for the constrained case , a mini - batch of samples at each iteration is required to guarantee convergence . 1 . 2 Contributions Minimization of weakly convex functions has been a challenging task , especially for stochastic problems , as the objective is neither smooth nor convex . With the recent breakthrough in [ 8 ] , this problem class has been the widest one for which provable sample complexity of the standard SGD is known . It is thus intriguing to ask whether such a result can also be obtained for momentum - based methods . The work in this paper aims to address this question . To that end , we make the following contributions : • We establish the sample complexity of a stochastic subgradient method with momentum of Polyak type for a broad class of non - smooth , non - convex , and constrained optimization problems . Concretely , using a special Lyapunov function , we show the complexity O ( 1 / (cid:15) 2 ) for the minimization of weakly convex functions . The proven complexity is attained in a parameter - free and single time - scale fashion , namely , the stepsize and the momentum constant are independent of any problem parameters and they have the same scale with respect to the iteration count . To the best of our knowledge , this is the ﬁrst complex - ity guarantee for a stochastic method with momentum on non - smooth and non - convex problems . • We also study the sample complexity of the considered algorithm for smooth and con - strained optimization problems . Note that even in this setting , no complexity guarantee of SGD with Polyak momentum has been established before . Under a bounded gradient assumption , we obtain a similar O ( 1 / (cid:15) 2 ) complexity without the need of forming a batch of samples at each iteration , which is commonly required for constrained non - convex stochas - tic optimization [ 20 ] . We then demonstrate how the unconstrained case can be analyzed without the above assumption . 3 Interestingly , the stated result is achieved in the regime where β can be as small as O ( 1 / √ K ) , i . e . , one can put much more weight to the momentum term than the fresh sub - gradient in a search direction . This complements the complexity of SGD attained as β → 1 . Note that the worst - case complexity O ( 1 / (cid:15) 2 ) is unimprovable even in the smooth case [ 1 ] . 2 Background In this section , we ﬁrst introduce the notation and then provide the necessary preliminaries for the paper . For any x , y ∈ R n , we denote by (cid:104) x , y (cid:105) the Euclidean inner product of x and y . We use (cid:107)·(cid:107) 2 to denote the Euclidean norm . For a closed and convex set X , Π X denotes the orthogonal projection onto Z , i . e . , y = Π X ( x ) if y ∈ X and (cid:107) y − x (cid:107) 2 = min z ∈X (cid:107) z − x (cid:107) 2 ; I X ( · ) denotes the indicator function of X , i . e . , I X ( x ) = 0 if x ∈ X and + ∞ otherwise . Finally , we denote by F k : = σ ( S 0 , . . . , S k ) the σ - ﬁeld generated by the ﬁrst k + 1 random variables S 0 , . . . , S k . For a function f : R n → R ∪ { + ∞ } , the Fr´echet subdiﬀerential of f at x , denoted by ∂f ( x ) , consists of all vectors g ∈ R n such that f ( y ) ≥ f ( x ) + (cid:104) g , y − x (cid:105) + o ( (cid:107) y − x (cid:107) ) as y → x . The Fr´echet and conventional subdiﬀerentials coincide for convex functions , while for smooth functions f , ∂f ( x ) reduces to the gradient { ∇ f ( x ) } . A point x ∈ R n is said to be stationary for problem ( 1 ) if 0 ∈ ∂f ( x ) + ∂ I X ( x ) . The following lemma collects standard properties of weakly convex functions [ 47 ] . Lemma 2 . 1 ( Weak convexity ) . Let f : R n → R ∪ { + ∞ } be a ρ - weakly convex function . Then the following hold : 1 . For any x , y ∈ R n with g ∈ ∂f ( x ) , we have f ( y ) ≥ f ( x ) + (cid:104) g , y − x (cid:105) − ρ 2 (cid:107) y − x (cid:107) 22 . 2 . For all x , y ∈ R n and α ∈ [ 0 , 1 ] , we have f ( αx + ( 1 − α ) y ) ≤ αf ( x ) + ( 1 − α ) f ( y ) + ρα ( 1 − α ) 2 (cid:107) x − y (cid:107) 22 . Weakly convex functions admit an implicit smooth approximation through the Moreau envelope : f λ ( x ) = inf y ∈ R n (cid:26) f ( y ) + 1 2 λ (cid:107) x − y (cid:107) 22 (cid:27) . ( 4 ) For small enough λ , the point achieving f λ ( x ) in ( 4 ) , denoted by prox λf ( x ) , is unique and given by : prox λf ( x ) = argmin y ∈ R n (cid:26) f ( y ) + 1 2 λ (cid:107) x − y (cid:107) 2 2 (cid:27) . ( 5 ) The lemma below summarizes two well - known properties of the Moreau envelope and its associated proximal map [ 26 ] . 4 Lemma 2 . 2 ( Moreau envelope ) . Suppose that f : R n → R ∪ { + ∞ } is a ρ - weakly convex function . Then , for a ﬁxed parameter λ − 1 > ρ , the following hold : 1 . f λ is C 1 - smooth with the gradient given by ∇ f λ ( x ) = λ − 1 (cid:0) x − prox λf ( x ) (cid:1) . 2 . f λ is λ − 1 - smooth , i . e . , for all x , y ∈ R n : (cid:12)(cid:12) f λ ( y ) − f λ ( x ) − (cid:104)∇ f λ ( x ) , y − x (cid:105) (cid:12)(cid:12) ≤ 1 2 λ (cid:107) x − y (cid:107) 22 . Failure of stationary test A major source of diﬃculty in convergence analysis of non - smooth optimization methods is the lack of a controllable stationary measure . For smooth functions , it is natural to use the norm of the gradient as a surrogate for near stationary . However , this rule does not make sense in the non - smooth case , even if the function is convex and its gradient existed at all iterates . For example , the convex function f ( x ) = | x | has (cid:107)∇ f ( x ) (cid:107) 2 = 1 at each x (cid:54) = 0 , no mater how close x is to the stationary point x = 0 . To circumvent this diﬃculty , we adopt the techniques pioneered in [ 8 ] for convergence of stochastic methods on weakly convex problems . More concretely , we rely on the connection of the Moreau envelope to ( near ) stationarity : For any x ∈ R n , the point ˆ x = prox λF ( x ) , where F ( x ) = f ( x ) + I X ( x ) , satisﬁes : (cid:40) (cid:107) x − ˆ x (cid:107) 2 = λ (cid:107)∇ F λ ( x ) (cid:107) 2 , dist ( 0 , ∂F ( ˆ x ) ) ≤ (cid:107)∇ F λ ( x ) (cid:107) 2 . ( 6 ) Therefore , a small gradient (cid:107)∇ F λ ( x ) (cid:107) 2 implies that x is close to a point ˆ x ∈ X that is near stationary for F . Note that ˆ x is just a virtual point , there is no need to compute it . 3 Algorithm and convergence analysis We assume that the only access to f is through a stochastic subgradient oracle . In particular , we study algorithms that attempt to solve problem ( 1 ) using i . i . d . samples S 0 , S 1 , . . . , S K iid ∼ P . Assumption A ( Stochastic oracle ) . Fix a probability space ( S , F , P ) . Let S be a sample drawn from P and f (cid:48) ( x , S ) ∈ ∂f ( x , S ) . We make the following assumptions : ( A1 ) For each x ∈ dom ( f ) , we have E P (cid:2) f (cid:48) ( x , S ) (cid:3) ∈ ∂f ( x ) . ( A2 ) There exists a real L > 0 such that for all x ∈ X : E P (cid:104)(cid:13)(cid:13) f (cid:48) ( x , S ) (cid:13)(cid:13) 2 2 (cid:105) ≤ L 2 . The above assumptions are standard in stochastic optimization of non - smooth functions ( see , e . g . , [ 30 , 8 ] ) . 5 Algorithm To solve problem ( 1 ) , we use an iterative procedure that starts from x 0 ∈ X , z 0 ∈ ∂f ( x 0 , S 0 ) and generates sequences of points x k ∈ X and z k ∈ R n by repeating the following steps for k = 0 , 1 , 2 , . . . : x k + 1 = argmin x ∈X (cid:26) (cid:104) z k , x − x k (cid:105) + 1 2 α (cid:107) x − x k (cid:107) 22 (cid:27) ( 7a ) z k + 1 = βg k + 1 + ( 1 − β ) x k − x k + 1 α , ( 7b ) where g k + 1 ∈ ∂f ( x k + 1 , S k + 1 ) . When X = R n , this algorithm reduces to the procedure ( 3 ) . For a general convex set X , this scheme is known as the iPiano method in the smooth and deterministic setting [ 35 ] . For simplicity , we refer to Algorithm 7 as stochastic heavy ball ( SHB ) . Throughout the paper , we will frequently use the following two quantities : p k = 1 − β β ( x k − x k − 1 ) and d k = 1 α ( x k − 1 − x k ) . Before detailing our convergence analysis , we note that most proofs of O ( 1 / (cid:15) 2 ) sample complexity for subgradient - based methods rely on establishing an iterate relationship on the form ( see , e . g . , [ 30 , 8 , 18 ] ) : E [ V k + 1 ] ≤ E [ V k ] − α E [ opt mea ] + α 2 C 2 , ( 8 ) where opt mea denotes some suboptimality measure such as f ( · ) − f (cid:63) for convex and (cid:107)∇ f ( · ) (cid:107) 22 for smooth ( possibly non - convex ) problems , V k are certain Lyapunov functions , α is the stepsize , and C is some constant . Once ( 8 ) is given , a simple manipulation results in the desired complexity provided that α is chosen appropriately . The case with decaying stepsize can be analyzed in the same way with minor adjustments . We follow the same route and identify a Lyapunov function that allows to establish relation ( 8 ) for the quantity (cid:107)∇ F λ ( · ) (cid:107) 2 in ( 6 ) . Since our Lyapunov function is nontrivial , we shall build it up through a series of key results . We start by presenting the following lemma , which quantiﬁes the averaged progress made by one step of the algorithm . Lemma 3 . 1 . Let Assumptions ( A1 ) – ( A2 ) hold . Let β = να for some constant ν > 0 such that β ∈ ( 0 , 1 ] . Let x k be generated by procedure ( 7 ) . It holds for any k ∈ N that ( 1 − β ) f ( x k ) + E (cid:104) ν 2 (cid:107) p k + 1 (cid:107) 22 | F k − 1 (cid:105) ≤ ( 1 − β ) f ( x k − 1 ) + ν 2 (cid:107) p k (cid:107) 22 − α E (cid:104) (cid:107) d k + 1 (cid:107) 22 | F k − 1 (cid:105) + α 2 (cid:18) ρ ( 1 − β ) 2 + ν (cid:19) L 2 . ( 9 ) Proof . See Appendix A . In view of ( 8 ) , the lemma shows that the quantity E [ (cid:107) d k (cid:107) 22 ] can be made arbitrarily small . However , this alone is not suﬃcient to show convergence to stationary points . Nonetheless , we shall show that a small E [ (cid:107) d k (cid:107) 22 ] indeed implies a small ( averaged ) value of the norm of the Moreau envelope deﬁned at a speciﬁc point . Toward this goal , we ﬁrst need to detail the points x and ˆ x in ( 6 ) . It seems that taking the most natural candidate x = x k is unlikely to produce the desired result . Instead , we rely on the following iterates : ¯ x k : = x k + 1 − β β ( x k − x k − 1 ) , 6 and construct corresponding virtual reference points : ˆ x k = argmin x ∈ R n (cid:26) F ( x ) + 1 2 λ (cid:107) x − ¯ x k (cid:107) 22 (cid:27) , for λ < 1 / ρ . By Lemma 2 . 2 , ∇ F λ ( ¯ x k ) = λ − 1 ( ¯ x k − ˆ x k ) , where F λ ( · ) is the Moreau envelope of F ( · ) = f ( · ) + I X ( · ) . With these deﬁnitions , we can now state the next lemma . Lemma 3 . 2 . Assume the same setting of Lemma 3 . 1 . Let λ > 0 be such that λ − 1 ≥ 2 ρ . Let ξ = ( 1 − β ) / ν and deﬁne the function : V k = F λ ( ¯ x k ) + νξ 2 4 λ 2 (cid:107) p k (cid:107) 22 + αξ 2 2 λ 2 (cid:107) d k (cid:107) 22 + (cid:18) ( 1 − β ) ξ 2 2 λ 2 + ξ λ (cid:19) f ( x k − 1 ) . ( 10 ) Then , for any k ∈ N , E [ V k + 1 | F k − 1 ] ≤ V k − α 2 (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 + γα 2 L 2 2 λ , ( 11 ) where γ = ξ 2 ( ρ ( 1 − β ) / 2 + ν ) / λ + ρξ / 2 + 1 . The proof of this lemma is rather involved and can be found in Appendix B . Lemma 3 . 2 has established a relation akin to ( 8 ) with the Lyapunov function V k deﬁned in ( 10 ) . We can now use standard analysis to obtain our sample complexity . Theorem 1 . Let Assumptions ( A1 ) - ( A2 ) hold . Let k ∗ be sampled uniformly at random from { 0 , . . . , K } . Let f (cid:63) = inf x ∈X f ( x ) and denote ∆ = f ( x 0 ) − f (cid:63) . If we set α = α 0 √ K + 1 and ν = 1 / α 0 for some real α 0 > 0 , then under the same setting of Lemma 3 . 2 : E (cid:104) (cid:107)∇ F λ ( ¯ x k ∗ ) (cid:107) 22 (cid:105) ≤ 2 · γ 1 ∆ + γL 2 2 λ α 0 √ K + 1 , ( 12 ) where γ ≤ ρ 2 α 20 + 3 ρα 0 + 1 and γ 1 ≤ 2 ρ 2 α 20 + 2 ρα 0 + 1 . Furthermore , if α 0 is set to 1 / ρ , we obtain E (cid:104)(cid:13)(cid:13) ∇ F 1 / ( 2 ρ ) ( ¯ x k ∗ ) (cid:13)(cid:13) 2 2 (cid:105) ≤ 10 · ρ ∆ + L 2 √ K + 1 . Proof . Taking the expectation on both sides of ( 11 ) and summing the result over k = 0 , . . . , K yield E [ V K + 1 ] ≤ V 0 − α 0 2 √ K + 1 K (cid:88) k = 0 E (cid:104) (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 (cid:105) + γL 2 α 20 2 λ . Let γ 1 = 1 + ( 1 − β ) ξ 2 / ( 2 λ 2 ) + ξ / λ , the left - hand - side of the above inequality can be lower - bounded by γ 1 f (cid:63) . Using the facts that F λ ( x 0 ) ≤ f ( x 0 ) and x − 1 = x 0 , we get V 0 ≤ γ 1 f ( x 0 ) . Consequently , E (cid:104) (cid:107)∇ F λ ( ¯ x k ∗ ) (cid:107) 22 (cid:105) ≤ 2 · γ 1 ∆ + γL 2 α 20 2 λ α 0 √ K + 1 , 7 where the last expectation is taken with respect to all random sequences generated by the method and the uniformly distributed random variable k ∗ . Note that ν = 1 / α 0 , ξ = ( 1 − β ) / ν , and 1 − β ≤ 1 . Thus , letting λ = 1 / ( 2 ρ ) , the constants γ and γ 1 can be upper - bounded by ρ 2 α 20 + 3 ρα 0 + 1 and 2 ρ 2 α 20 + 2 ρα 0 + 1 , respectively . Therefore , if we let α 0 = 1 / ρ , we arrive at E (cid:104)(cid:13)(cid:13) ∇ F 1 / ( 2 ρ ) ( ¯ x k ∗ ) (cid:13)(cid:13) 2 2 (cid:105) ≤ 10 · ρ ∆ + L 2 √ K + 1 , as desired . Some remarks regarding Theorem 1 are in order : i ) The choice ν = 1 / α 0 is just for simplicity ; one can pick any constant such that β = να ∈ ( 0 , 1 ] . Note that the stepsize used to achieve the rate in ( 12 ) does not depend on any problem parameters . Once α is set , the momentum parameter selection is completely parameter - free . Since both α and β scale like O ( 1 / √ K ) , Algorithm 7 can be seen as a single time - scale method [ 21 , 42 ] . Such methods contrast those that require at least two time - scales to ensure convergence . For example , stochastic dual averaging for convex optimization [ 48 ] requires one fast scale O ( 1 / K ) for averaging the subgradients , and one slower scale O ( 1 / √ K ) for the stepsize . To show almost sure convergence of SHB for smooth and unconstrained problems , the work [ 22 ] requires that both the stepsize and the momentum parameter tend to zero but the former one must do so at a faster speed . 1 ii ) To some extent , Theorem 1 supports the use of a small momentum parameter such as β = 0 . 1 or β = 0 . 01 , which corresponds to the default value 1 − β = 0 . 9 in PyTorch 2 or the smaller 1 − β = 0 . 99 suggested in [ 23 ] Indeed , the theorem allows to have β as small as O ( 1 / √ K ) , i . e . , one can put much more weight to the momentum term than the fresh subgradient and still preserve the complexity . Recall also that SHB reduces to SGD as β → 1 , which also admits a similar complexity . It is thus quite ﬂexible to set β , without sacriﬁcing the worst - case complexity . We refer to [ 22 , Theorem 2 ] for a similar discussion in the context of almost sure convergence on smooth problems . iii ) In view of ( 6 ) , the theorem indicates that ¯ x k is nearby a near - stationary point ˆ x k . Since ¯ x k may not belong to X , it is thus more preferable to have the similar guarantee for the iterate x k . Indeed , we have λ − 2 (cid:107) x k − ˆ x k (cid:107) 22 ≤ 2 λ − 2 (cid:107) ¯ x k − ˆ x k (cid:107) 2 2 + 2 λ − 2 (cid:107) x k − ¯ x k (cid:107) 22 = 2 (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 + 2 λ − 2 (cid:107) x k − ¯ x k (cid:107) 22 = 2 (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 + 2 λ − 2 ξ 2 (cid:107) d k (cid:107) 22 . Since both terms on the right converge at the rate O ( 1 / √ K ) , it immediately translates into the same guarantee for the term on the left , as desired . In summary , we have established the convergence rate O ( 1 / √ K ) or , equivalently , the sample complexity O ( 1 / (cid:15) 2 ) of SHB for the minimization of weakly convex functions . 1 Note that our β corresponds to 1 − β in [ 22 ] . 2 https : / / pytorch . org / 8 4 Extension to smooth non - convex functions In this section , we study the convergence property of Algorithm ( 7 ) for the minimization of ρ - smooth functions : (cid:107)∇ f ( x ) − ∇ f ( x ) (cid:107) 2 ≤ ρ (cid:107) x − y (cid:107) 2 , ∀ x , y ∈ dom f . Note that ρ - smooth functions are automatically ρ - weakly convex . In this setting , it is more common to replace Assumption ( A2 ) by the following . Assumption ( A3 ) . There exists a real σ > 0 such that for all x ∈ X : E (cid:104)(cid:13)(cid:13) f (cid:48) ( x , S ) − ∇ f ( x ) (cid:13)(cid:13) 2 2 (cid:105) ≤ σ 2 . Deriving convergence rates of stochastic schemes with momentum for non - convex functions under Assumption ( A3 ) can be quite challenging . Indeed , even in unconstrained optimization , previous studies often need to make the assumption that the true gradient is bounded , i . e . , (cid:107)∇ f ( x ) (cid:107) 2 ≤ G for all x ∈ R n ( see , e . g . , [ 49 , 16 , 22 ] ) . This assumption is strong and does not hold even for quadratic convex functions . It is more realistic in constrained problems , for example when X is compact , albeit the constant G could then be large . Our objective in this section is twofold : First , we aim to extend the convergence results of SHB in the previous section to smooth optimization problems under Assumption ( A3 ) . Note that even in this setting , the sample complexity of SHB has not been established before . The rate is obtained without the need of forming a batch of samples at each iteration , which is commonly required for constrained non - convex stochastic optimization [ 19 , 20 ] . Second , for unconstrained problems , we demonstrate how to achieve the same complexity without the bounded gradient assumption above . Let h ( x ) = 12 (cid:107) x (cid:107) 22 + I X ( x ) and let h ∗ ( z ) be its convex conjugate . Our convergence analysis relies on the function : ϕ k = h ∗ ( x k − αz k ) − 1 2 (cid:107) x k (cid:107) 22 + α (cid:104) x k , z k (cid:105) . ( 13 ) The use of this function is inspired by [ 41 ] . Roughly speaking , ϕ k is the negative of the optimal value of the function on the RHS of ( 7a ) , and hence , ϕ k ≥ 0 for all k . This function also underpins the analysis of the dual averaging scheme in [ 32 ] . The following result plays a similar role as Lemma 3 . 1 . Lemma 4 . 1 . Let Assumptions ( A1 ) and ( A3 ) hold . Let α ∈ ( 0 , 1 / ρ ) and β = να for some constant ν > 0 such that β ∈ ( 0 , 1 ] . Let α ∈ ( 0 , 1 / ( 4 ρ ) ] and ξ = ( 1 − β ) / ν , and deﬁne the function : W k = 2 f ( x k ) + ϕ k να 2 + ξ 2 (cid:107) d k (cid:107) 22 . Then , it holds for any k ∈ N that E [ W k + 1 | F k ] ≤ W k − α (cid:107) d k + 1 (cid:107) 2 2 + 4 να 2 σ 2 . ( 14 ) Proof . Since the proof is rather technical , we defer details to Appendix C and sketch only the main arguments here . 9 By smoothness of h ∗ , weak convexity of f and the optimality condition for the update formula ( 7a ) we get E (cid:104) f ( x k + 1 ) + ϕ k + 1 να 2 (cid:12)(cid:12)(cid:12) F k (cid:105) ≤ f ( x k ) + ϕ k να 2 − ( α − ρα 2 2 ) (cid:107) d k + 1 (cid:107) 22 + 1 2 ν E (cid:104) (cid:107) z k − z k + 1 (cid:107) 22 | F k (cid:105) . The proof of this relation can be found in Lemma C . 1 . The preceding inequality admits very useful properties as we have terms that form a telescoping sum , and the constant associated with (cid:107) d k + 1 (cid:107) 22 has the right order - dependence on the stepsize . However , we still have a re - maining term (cid:107) z k − z k + 1 (cid:107) 22 . Thus , in view of relation ( 8 ) , our next strategy is to bound this term in a way that still keeps all the favourable features described above , and at the most introduces an additional term of order O ( α 2 σ 2 ) . As shown in Lemma C . 2 , we can establish the following inequality E (cid:20) 1 2 ν (cid:107) z k + 1 − z k (cid:107) 22 | F k (cid:21) ≤ f ( x k ) − f ( x k + 1 ) + ξ 2 (cid:107) d k (cid:107) 22 − ξ 2 (cid:107) d k + 1 (cid:107) 22 − (cid:18) α − α 3 ρ 2 + 3 ρα 2 2 (cid:19) (cid:107) d k + 1 (cid:107) 22 + 4 να 2 σ 2 . Now , ( 14 ) follows immediately from combining the two previous inequalities and the fact that α ∈ ( 0 , 1 / ( 4 ρ ) ] . We remark that Lemma 4 . 1 does not require the bounded gradient assumption and readily indicates the convergence rate O ( 1 / √ K ) for E [ (cid:107) d k (cid:107) 22 ] . However , to establish the rate for E [ (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 ] , we need to impose such an assumption in the theorem below . Nonetheless , the assumption is much more realistic in this setting than the unconstrained case . Theorem 2 . Let Assumptions ( A1 ) and ( A3 ) hold . Assume further that (cid:107)∇ f ( x ) (cid:107) 2 ≤ G for all x ∈ X . Let k ∗ , ¯ x k ∗ , λ , ∆ , γ , and γ 1 be deﬁned as in Theorem 1 . If we set α = α 0 √ K + 1 and ν = 1 / α 0 for some real α 0 > 0 , then E (cid:104) (cid:107)∇ F λ ( ¯ x k ∗ ) (cid:107) 22 (cid:105) ≤ 2 · γ 1 ∆ + γ ( σ 2 + G 2 ) / ( 2 λ ) α 0 √ K + 1 . Furthermore , if α 0 is set to 1 / ρ , we obtain E (cid:104)(cid:13) (cid:13) ∇ F 1 / ( 2 ρ ) ( ¯ x k ∗ ) (cid:13)(cid:13) 2 2 (cid:105) ≤ 10 · ρ ∆ + σ 2 + G 2 √ K + 1 . Proof . The proof is a verbatim copy of that of Theorem 1 with L 2 replaced by σ 2 + G 2 ; see Appendix D . Some remarks are in order : i ) To the best of our knowledge , this is the ﬁrst convergence rate result of a stochastic ( or even deterministic ) method with Polyak momentum for smooth , non - convex , and constrained problems . ii ) The algorithm enjoys the same single time - scale and parameter - free properties as in the non - smooth case . 10 iii ) The rate in the theorem readily translates into an analogous estimate for the norm of the so - called gradient mapping G 1 / ρ , which is commonly adapted in the literature , e . g . , [ 20 ] . This is because for ρ - smooth functions [ 11 ] : (cid:13)(cid:13) G 1 / ρ ( x ) (cid:13)(cid:13) 2 ≤ 3 2 ( 1 + 1 √ 2 ) (cid:13)(cid:13) ∇ F 1 / ( 2 ρ ) ( x ) (cid:13)(cid:13) 2 , ∀ x ∈ R n . Since the bounded gradient assumption is rather restrictive in the unconstrained case , our ﬁnal result shows how the desired complexity can be attained without this assumption . Theorem 3 . Let Assumptions ( A1 ) and ( A3 ) hold . Let λ − 1 ∈ ( 3 ρ / 2 , 2 ρ ] . Let k ∗ be sampled uniformly at random from { − 1 , . . . , K − 1 } . If we set α = α 0 √ K + 1 and ν = 1 / α 0 , where α 0 ∈ ( 0 , 1 / ( 4 ρ ) ] , then under the same setting of Lemma 4 . 1 : E (cid:104) (cid:107)∇ F λ ( ¯ x k ∗ ) (cid:107) 22 (cid:105) ≤ c · ( 1 + 2 α 20 / λ 2 ) ∆ + ( 1 + 8 α 0 / λ ) σ 2 α 20 2 λ α 0 √ K + 1 , where c = 2 λ − 1 / ( 2 λ − 1 − 3 ρ ) . Furthermore , let λ = 1 / ( 2 ρ ) , we obtain E (cid:104)(cid:13)(cid:13) ∇ F 1 / ( 2 ρ ) ( ¯ x k ∗ ) (cid:13)(cid:13) 2 2 (cid:105) ≤ 4 · (cid:0) 1 + 8 ρ 2 α 20 (cid:1) ∆ + ( ρ + 16 α 0 ρ 2 ) σ 2 α 30 α 0 √ K + 1 . Proof . See Appendix E . It should be mentioned that a similar result has been attained very recently in [ 21 ] using a diﬀerent analysis , albeit no sample complexity is given for the non - smooth case . 5 Numerical evaluations In this section , we perform experiments to validate our theoretical developments and to demonstrate that despite sharing the same worst - case complexity , SHB can be much bet - ter in terms of speed and robustness to problem and algorithm parameters than SGD . We consider the robust phase retrieval problem [ 13 , 14 ] : Given a set of m measurements ( a i , b i ) ∈ R n × R , the phase retrieval problem seeks for a vector x (cid:63) such that (cid:104) a i , x (cid:63) (cid:105) 2 ≈ b i for most i = 1 , . . . , m . Whenever the problem is corrupted with gross outliers , a natural exact penalty form of this ( approximate ) system of equations yields the minimization problem : minimize x ∈ R n 1 m m (cid:88) i = 1 (cid:12)(cid:12) (cid:104) a i , x (cid:105) 2 − b i (cid:12)(cid:12) . This objective function is non - smooth and non - convex . In view of ( 2 ) , it is the composition of the Lipschitz - continuous convex function h ( y ) = (cid:107) y (cid:107) 1 and the smooth map c with c i ( x ) = (cid:104) a i , x (cid:105) 2 − b i . Hence , it is weakly convex . In each experiment , we set m = 300 , n = 100 and select x (cid:63) uniformly from the unit sphere . We generate A as A = QD , where Q ∈ R m × n is a matrix with standard normal distributed entries , and D is a diagonal matrix with linearly spaced elements between 1 / κ and 1 , with κ ≥ 1 playing the role of a condition number . The elements b i of the vector b are generated as b i = (cid:104) a i , x (cid:63) (cid:105) 2 + δζ i , i = 1 , . . . , m , where ζ i ∼ N ( 0 , 25 ) models the corruptions , and δ ∈ { 0 , 1 } is 11 ( a ) κ = 10 , α 0 = 0 . 1 ( b ) κ = 10 , α 0 = 0 . 25 ( c ) κ = 1 , α 0 = 0 . 1 ( d ) κ = 1 , α 0 = 0 . 15 Figure 1 : The function gap f ( x k ) − f ( x (cid:63) ) versus iteration count for phase retrieval with p fail = 0 . 2 , β = 10 / √ K . ( a ) p fail = 0 . 3 , β = 1 / √ K ( b ) p fail = 0 . 3 , β = 1 / α 0 / √ K Figure 2 : The number of epochs to achieve (cid:15) - accuracy versus initial stepsize α 0 for phase retrieval with κ = 10 . a binary random variable taking the value 1 with probability p fail , so that p fail · m measurements are corrupted . The algorithms are all randomly initialized at x 0 ∼ N ( 0 , 1 ) . The stochastic 12 ( a ) p fail = 0 . 3 , β = 0 . 1 ( b ) p fail = 0 . 3 , β = 0 . 01 Figure 3 : The number of epochs to achieve (cid:15) - accuracy versus initial stepsize α 0 for phase retrieval with κ = 10 and popular choices of β . subgradient is simply given as an element of the subdiﬀerential of g ( x ) = (cid:12)(cid:12) (cid:104) a , x (cid:105) 2 − b (cid:12)(cid:12) : ∂g ( x ) = 2 (cid:104) a , x (cid:105) a · (cid:40) sign ( (cid:104) a , x (cid:105) − b ) 2 if (cid:104) a , x (cid:105) 2 (cid:54) = b , [ − 1 , 1 ] otherwise . In each of our experiments , we set the stepsize as α k = α 0 / √ k + 1 , where α 0 is an initial stepsize . We also refer m stochastic iterations as one epoch ( pass over the data ) . Within each individual run , we allow the considered stochastic methods to perform K = 400 m iterations . We conduct 50 experiments for each stepsize and report the median of the epochs - to - (cid:15) - accuracy ; the shaded areas in each plot cover the 10th to 90th percentile of convergence times . Figure 1 shows the function gap versus iteration count for diﬀerent values of κ and α 0 , with p fail = 0 . 2 , β = 10 / √ K . It is evident that SHB converges with a theoretically justiﬁed parameter β and is much less sensitive to problem and algorithm parameters than the vanilla SGD . Note that the sensitivity issue of SGD is rather well documented ; a slight change in its parameters can have a severe eﬀect on the overall performance of the algorithm [ 30 , 2 ] . For example , Fig . 1d shows that SGD exhibits a transient exponential growth before eventual convergence . This behaviour can occur even when minimizing the smooth quadratic function 12 x 2 [ 2 , Example 2 ] . In contrast , SHB converges in all settings of the ﬁgure , suggesting that using a momentum term can help to improve the robustness of the standard SGD . This is expected as the update formula ( 3b ) acts like a lowpass ﬁlter , averaging past stochastic subgradients , which may have stabilizing eﬀect on the sequence { x k } . To further clarify this observation , in the next set of experiments , we test the sensitivity of SHB and SGD to the initial stepsize α 0 . Figure 2 shows the number of epochs required to reach (cid:15) - accuracy for phase retrieval with κ = 10 and p fail = 0 . 3 . We can see that the standard SGD has good performance for a narrow range of stepsizes , while wider convergence range can be achieved with SHB . Finally , it would be incomplete without reporting experiments for some of the most popular momentum parameters used by practitioners . Figure 3 shows a similar story to Fig . 2 for the parameter 1 − β = 0 . 9 and 1 − β = 0 . 99 as discussed in remark ii ) after Theorem 1 . This together with Fig . 2 demonstrates that SHB is able to ﬁnd good approximate solutions 13 for diverse values of the momentum constant over a wider ( often signiﬁcantly so ) range of algorithm parameters than SGD . 6 Conclusion Using a carefully constructed Lyapunov function , we established the ﬁrst sample complexity results for the SHB method on a broad class of non - smooth , non - convex , and constrained optimization problems . The complexity is attained in a parameter - free fashion in a single time - scale . A notable feature of our results is that they justify the use of a large amount of momentum in search directions . We also improved some complexity results for SHB on smooth problems . Numerical results show that SHB exhibits good performance and low sensitivity to problem and algorithm parameters compared to the standard SGD . References [ 1 ] Y . Arjevani , Y . Carmon , J . C . Duchi , D . J . Foster , N . Srebro , and B . Woodworth . Lower bounds for non - convex stochastic optimization . arXiv preprint arXiv : 1912 . 02365 , 2019 . [ 2 ] H . Asi and J . C . Duchi . Stochastic ( approximate ) proximal point methods : Convergence , optimality , and adaptivity . SIAM Journal on Optimization , 29 ( 3 ) : 2257 – 2290 , 2019 . [ 3 ] A . Beck . First - order methods in optimization , volume 25 . SIAM , 2017 . [ 4 ] L . Bottou . Large - scale machine learning with stochastic gradient descent . In Proceedings of COMPSTAT’2010 , pages 177 – 186 . Springer , 2010 . [ 5 ] S . Bubeck . Convex optimization : Algorithms and complexity . Foundations and Trends in Machine Learning , 8 ( 3 - 4 ) : 231 – 357 , 2015 . [ 6 ] E . J . Cand ` es , X . Li , Y . Ma , and J . Wright . Robust principal component analysis ? Journal of the ACM ( JACM ) , 58 ( 3 ) : 1 – 37 , 2011 . [ 7 ] V . Charisopoulos , Y . Chen , D . Davis , M . D´ıaz , L . Ding , and D . Drusvyatskiy . Low - rank matrix recovery with composite optimization : good conditioning and rapid convergence . arXiv preprint arXiv : 1904 . 10020 , 2019 . [ 8 ] D . Davis and D . Drusvyatskiy . Stochastic model - based minimization of weakly convex functions . SIAM Journal on Optimization , 29 ( 1 ) : 207 – 239 , 2019 . [ 9 ] D . Davis and B . Grimmer . Proximally guided stochastic subgradient method for nons - mooth , nonconvex problems . SIAM Journal on Optimization , 29 ( 3 ) : 1908 – 1930 , 2019 . [ 10 ] D . Drusvyatskiy . The proximal point method revisited . arXiv preprint arXiv : 1712 . 06038 , 2017 . [ 11 ] D . Drusvyatskiy and C . Paquette . Eﬃciency of minimizing compositions of convex func - tions and smooth maps . Mathematical Programming , 178 ( 1 - 2 ) : 503 – 558 , 2019 . [ 12 ] J . C . Duchi and F . Ruan . Asymptotic optimality in stochastic optimization . arXiv preprint arXiv : 1612 . 05612 , 2016 . 14 [ 13 ] J . C . Duchi and F . Ruan . Solving ( most ) of a set of quadratic equalities : Composite optimization for robust phase retrieval . Information and Inference : A Journal of the IMA , 8 ( 3 ) : 471 – 529 , 2018 . [ 14 ] J . C . Duchi and F . Ruan . Stochastic methods for composite and weakly convex opti - mization problems . SIAM Journal on Optimization , 28 ( 4 ) : 3229 – 3259 , 2018 . [ 15 ] Y . M . Ermol’ev and V . Norkin . Stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization . Cybernetics and Systems Analysis , 34 ( 2 ) : 196 – 215 , 1998 . [ 16 ] S . Gadat , F . Panloup , and S . Saadane . Stochastic heavy ball . Electronic Journal of Statistics , 12 ( 1 ) : 461 – 529 , 2018 . [ 17 ] E . Ghadimi , H . R . Feyzmahdavian , and M . Johansson . Global convergence of the heavy - ball method for convex optimization . In 2015 European Control Conference ( ECC ) , pages 310 – 315 . IEEE , 2015 . [ 18 ] S . Ghadimi and G . Lan . Stochastic ﬁrst - and zeroth - order methods for nonconvex stochas - tic programming . SIAM Journal on Optimization , 23 ( 4 ) : 2341 – 2368 , 2013 . [ 19 ] S . Ghadimi and G . Lan . Accelerated gradient methods for nonconvex nonlinear and stochastic programming . Mathematical Programming , 156 ( 1 - 2 ) : 59 – 99 , 2016 . [ 20 ] S . Ghadimi , G . Lan , and H . Zhang . Mini - batch stochastic approximation methods for nonconvex stochastic composite optimization . Mathematical Programming , 155 ( 1 - 2 ) : 267 – 305 , 2016 . [ 21 ] S . Ghadimi , A . Ruszczy´nski , and M . Wang . A single time - scale stochastic approximation method for nested stochastic optimization . SIAM J . on Optimization , 2020 . Accepted for publication ( arXiv preprint arXiv : 1812 . 01094 ) . [ 22 ] I . Gitman , H . Lang , P . Zhang , and L . Xiao . Understanding the role of momentum in stochastic gradient methods . In Advances in Neural Information Processing Systems , pages 9630 – 9640 , 2019 . [ 23 ] G . Goh . Why momentum really works . Distill , 2017 . [ 24 ] A . M . Gupal and L . G . Bazhenov . Stochastic analog of the conjugant - gradient method . Cybernetics and Systems Analysis , 8 ( 1 ) : 138 – 140 , 1972 . [ 25 ] K . He , X . Zhang , S . Ren , and J . Sun . Deep residual learning for image recognition . In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770 – 778 , 2016 . [ 26 ] J . - B . Hiriart - Urruty and C . Lemar´echal . Convex analysis and minimization algorithms , volume 305 . Springer science & business media , 1993 . [ 27 ] C . Hu , W . Pan , and J . T . Kwok . Accelerated gradient methods for stochastic optimization and online learning . In Advances in Neural Information Processing Systems , pages 781 – 789 , 2009 . 15 [ 28 ] G . Huang , Z . Liu , L . Van Der Maaten , and K . Q . Weinberger . Densely connected convo - lutional networks . In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4700 – 4708 , 2017 . [ 29 ] A . Krizhevsky , I . Sutskever , and G . E . Hinton . Imagenet classiﬁcation with deep convo - lutional neural networks . In Advances in neural information processing systems , pages 1097 – 1105 , 2012 . [ 30 ] A . Nemirovski , A . Juditsky , G . Lan , and A . Shapiro . Robust stochastic approximation approach to stochastic programming . SIAM Journal on optimization , 19 ( 4 ) : 1574 – 1609 , 2009 . [ 31 ] Y . Nesterov . A method for solving the convex programming problem with convergence rate O ( 1 / k 2 ) . In Dokl . akad . nauk Sssr , volume 269 , pages 543 – 547 , 1983 . [ 32 ] Y . Nesterov . Primal - dual subgradient methods for convex problems . Mathematical pro - gramming , 120 ( 1 ) : 221 – 259 , 2009 . [ 33 ] Y . Nesterov . Lectures on convex optimization , volume 137 . Springer , 2018 . [ 34 ] E . A . Nurminskii . The quasigradient method for the solving of the nonlinear programming problems . Cybernetics , 9 ( 1 ) : 145 – 150 , 1973 . [ 35 ] P . Ochs , Y . Chen , T . Brox , and T . Pock . iPiano : Inertial proximal algorithm for non - convex optimization . SIAM Journal on Imaging Sciences , 7 ( 2 ) : 1388 – 1419 , 2014 . [ 36 ] B . T . Polyak . Some methods of speeding up the convergence of iteration methods . USSR Computational Mathematics and Mathematical Physics , 4 ( 5 ) : 1 – 17 , 1964 . [ 37 ] B . T . Polyak . Introduction to optimization . Optimization Software , 1987 . [ 38 ] H . Robbins and S . Monro . A stochastic approximation method . The annals of mathe - matical statistics , pages 400 – 407 , 1951 . [ 39 ] R . T . Rockafellar . Favorable classes of Lipschitz continuous functions in subgradient optimization . In E . A . Nurminski , editor , Progress in Nondiﬀerentiable Optimization , CP - 82 - S8 , pages 125 – 143 , 1982 . [ 40 ] R . T . Rockafellar and S . Uryasev . Optimization of conditional value - at - risk . Journal of risk , ( 2 ) : 21 – 42 , 2000 . [ 41 ] A . Ruszczy´nski . A linearization method for nonsmooth stochastic programming prob - lems . Mathematics of Operations Research , 12 ( 1 ) : 32 – 49 , 1987 . [ 42 ] A . Ruszczynski and W . Syski . Stochastic approximation method with gradient averaging for unconstrained problems . IEEE Transactions on Automatic Control , 28 ( 12 ) : 1097 – 1105 , 1983 . [ 43 ] A . Shapiro , D . Dentcheva , and A . Ruszczy´nski . Lectures on stochastic programming : modeling and theory . SIAM , 2014 . [ 44 ] A . Singer . Angular synchronization by eigenvectors and semideﬁnite programming . Ap - plied and computational harmonic analysis , 30 ( 1 ) : 20 – 36 , 2011 . 16 [ 45 ] I . Sutskever , J . Martens , G . Dahl , and G . Hinton . On the importance of initialization and momentum in deep learning . In International conference on machine learning , pages 1139 – 1147 , 2013 . [ 46 ] P . Tseng . An incremental gradient ( - projection ) method with momentum term and adaptive stepsize rule . SIAM Journal on Optimization , 8 ( 2 ) : 506 – 531 , 1998 . [ 47 ] J . - P . Vial . Strong and weak convexity of sets and functions . Mathematics of Operations Research , 8 ( 2 ) : 231 – 259 , 1983 . [ 48 ] L . Xiao . Dual averaging methods for regularized stochastic learning and online optimiza - tion . Journal of Machine Learning Research , 11 ( Oct ) : 2543 – 2596 , 2010 . [ 49 ] T . Yang , Q . Lin , and Z . Li . Uniﬁed convergence analysis of stochastic momentum methods for convex and non - convex optimization . arXiv preprint arXiv : 1604 . 03257 , 2016 . [ 50 ] S . Zagoruyko and N . Komodakis . Wide residual networks . arXiv preprint arXiv : 1605 . 07146 , 2016 . [ 51 ] S . Zavriev and F . Kostyuk . Heavy - ball method in nonconvex optimization problems . Computational Mathematics and Modeling , 4 ( 4 ) : 336 – 341 , 1993 . A Proof of Lemma 3 . 1 First , we write the update formula of x k + 1 in ( 7 ) on a more compact form as : x k + 1 = Π X ( x k − αz k ) ( 15 ) z k + 1 = βg k + 1 + ( 1 − β ) x k − x k + 1 α . ( 16 ) We have 1 2 (cid:13)(cid:13)(cid:13)(cid:13) 1 − β β ( x k + 1 − x k ) (cid:13)(cid:13)(cid:13)(cid:13) 2 2 = 1 2 β 2 (cid:107) x k + 1 − ( ( 1 − β ) x k + βx k + 1 ) (cid:107) 22 . Since ( 1 − β ) x k + βx k + 1 ∈ X and Π X ( · ) is nonexpansive , it holds that 1 2 (cid:13)(cid:13)(cid:13) (cid:13) 1 − β β ( x k + 1 − x k ) (cid:13)(cid:13)(cid:13) (cid:13) 2 2 ≤ 1 2 β 2 (cid:107) x k − αz k − ( ( 1 − β ) x k + βx k + 1 ) (cid:107) 2 2 = 1 2 (cid:13)(cid:13)(cid:13)(cid:13) x k + 1 − β β ( x k − x k − 1 ) − αg k − x k + 1 (cid:13)(cid:13)(cid:13)(cid:13) 2 2 . ( 17 ) Next , we decompose the right - hand - side of the preceding inequality as 1 2 (cid:13)(cid:13)(cid:13)(cid:13) x k + 1 − β β ( x k − x k − 1 ) − αg k − x k + 1 (cid:13)(cid:13)(cid:13)(cid:13) 2 2 = 1 2 (cid:13)(cid:13)(cid:13)(cid:13) 1 − β β ( x k − x k − 1 ) (cid:13)(cid:13)(cid:13)(cid:13) 2 2 + ( 1 − β ) α β (cid:104) g k , x k − 1 − x k (cid:105) + 1 − β β (cid:104) x k − x k − 1 , x k − x k + 1 (cid:105) + α (cid:104) g k , x k + 1 − x k (cid:105) + 1 2 (cid:107) x k + 1 − x k (cid:107) 22 + α 2 2 (cid:107) g k (cid:107) 22 . ( 18 ) 17 Let p k : = 1 − ββ ( x k − x k − 1 ) , then by multiplying both sides of ( 17 ) by ν = β / α > 0 , together with ( 18 ) and the deﬁnition of z k , we get ν 2 (cid:107) p k + 1 (cid:107) 22 ≤ ν 2 (cid:107) p k (cid:107) 22 + ( 1 − β ) (cid:104) g k , x k − 1 − x k (cid:105) + (cid:104) z k , x k + 1 − x k (cid:105) + ν 2 (cid:107) x k + 1 − x k (cid:107) 22 + να 2 2 (cid:107) g k (cid:107) 22 . ( 19 ) By the weak convexity of f and Assumption ( A1 ) , it holds that E [ (cid:104) g k , x k − 1 − x k (cid:105) | F k − 1 ] = (cid:104) E [ g k | F k − 1 ] , x k − 1 − x k (cid:105) ≤ f ( x k − 1 ) − f ( x k ) + ρ 2 (cid:107) x k − x k − 1 (cid:107) 22 . ( 20 ) Next using the optimality condition of x k + 1 in ( 7a ) : (cid:104) x k + 1 − x k + αz k , x − x k + 1 (cid:105) ≥ 0 for all x ∈ X , ( 21 ) we deduce ( by selecting x = x k ∈ X ) that (cid:104) z k , x k + 1 − x k (cid:105) ≤ − α − 1 (cid:107) x k + 1 − x k (cid:107) 22 . ( 22 ) Therefore , by taking the conditional expectation in ( 19 ) , combining the result with ( 20 ) and ( 22 ) , and rearranging terms , we arrive at ( 1 − β ) f ( x k ) + E (cid:104) ν 2 (cid:107) p k + 1 (cid:107) 22 | F k − 1 (cid:105) ≤ ( 1 − β ) f ( x k − 1 ) + ν 2 (cid:107) p k (cid:107) 22 − (cid:18) 1 α − ν 2 (cid:19) E (cid:104) (cid:107) x k + 1 − x k (cid:107) 22 | F k − 1 (cid:105) + ( 1 − β ) ρ 2 (cid:107) x k − x k − 1 (cid:107) 22 + να 2 2 E [ (cid:107) g k (cid:107) 22 | F k − 1 ] . ( 23 ) Next , we show by induction that E [ (cid:107) z k (cid:107) 22 | F k − 1 ] ≤ L 2 ∀ k ∈ N . Since z 0 ∈ ∂f ( x 0 , S 0 ) , the base case follows directly from Assumption ( A2 ) . Suppose the hypothesis holds for i = 0 , . . . , k , we have E [ (cid:107) z k + 1 (cid:107) 22 | F k ] = E (cid:104) (cid:107) βg k + 1 + ( 1 − β ) ( x k − x k + 1 ) / α (cid:107) 22 (cid:12)(cid:12) F k (cid:105) ( a ) ≤ β E (cid:104) (cid:107) g k + 1 (cid:107) 22 | F k (cid:105) + ( 1 − β ) E (cid:104) (cid:107) ( x k − x k + 1 ) / α (cid:107) 22 | F k (cid:105) ( b ) ≤ βL 2 + ( 1 − β ) E (cid:104) (cid:107) z k (cid:107) 22 | F k − 1 (cid:105) ( c ) ≤ L 2 , ( 24 ) where ( a ) is true since (cid:107)·(cid:107) 22 is convex ; ( b ) follows from Assumption ( A2 ) and the nonexpan - siveness of Π X ( · ) ; and ( c ) follows from the induction hypothesis . This together with the nonexpansiveness of Π X ( · ) imply that E [ (cid:107) x k + 1 − x k (cid:107) 22 | F k − 1 ] ≤ E [ (cid:107) αz k (cid:107) 22 | F k − 1 ] ≤ α 2 L 2 for all k ∈ N . ( 25 ) Finally , using ( 25 ) and Assumption ( A2 ) , we obtain ( 1 − β ) f ( x k ) + E (cid:104) ν 2 (cid:107) p k + 1 (cid:107) 22 | F k − 1 (cid:105) ≤ ( 1 − β ) f ( x k − 1 ) + ν 2 (cid:107) p k (cid:107) 22 − 1 α E (cid:104) (cid:107) x k + 1 − x k (cid:107) 22 | F k − 1 (cid:105) + α 2 (cid:18) ρ ( 1 − β ) 2 + ν (cid:19) L 2 , ( 26 ) completing the proof . 18 B Proof of Lemma 3 . 2 Let ¯ x k : = x k + 1 − ββ ( x k − x k − 1 ) and deﬁne the virtual iterates : ˆ x k : = prox λF ( ¯ x k ) = argmin x ∈ R n (cid:26) F ( x ) + 1 2 λ (cid:107) x − ¯ x k (cid:107) 22 (cid:27) . ( 27 ) In view of Lemma 2 . 2 , we have ∇ F λ ( ¯ x k ) = λ − 1 ( ¯ x k − ˆ x k ) , where F λ ( · ) denotes the Moreau envelope of F ( x ) = f ( x ) + I X ( x ) . By the deﬁnition of F λ ( ¯ x k + 1 ) , it holds that F λ ( ¯ x k + 1 ) = f ( ˆ x k + 1 ) + 1 2 λ (cid:13)(cid:13)(cid:13)(cid:13) x k + 1 + 1 − β β ( x k + 1 − x k ) − ˆ x k + 1 (cid:13)(cid:13)(cid:13)(cid:13) 2 2 ≤ f ( ˆ x k ) + 1 2 λ (cid:13) (cid:13) (cid:13)(cid:13) x k + 1 + 1 − β β ( x k + 1 − x k ) − ˆ x k (cid:13) (cid:13) (cid:13)(cid:13) 2 2 = f ( ˆ x k ) + 1 2 λβ 2 (cid:107) x k + 1 − ( ( 1 − β ) x k + β ˆ x k ) (cid:107) 22 . ( 28 ) Since ( 1 − β ) x k + β ˆ x k ∈ X and Π X ( · ) is nonexpansive , we obtain 1 2 λβ 2 (cid:107) x k + 1 − ( ( 1 − β ) x k + β ˆ x k ) (cid:107) 22 ≤ 1 2 λβ 2 (cid:107) x k − αz k − ( ( 1 − β ) x k + β ˆ x k ) (cid:107) 22 = 1 2 λ (cid:13)(cid:13)(cid:13)(cid:13) x k + 1 − β β ( x k − x k − 1 ) − αg k − ˆ x k (cid:13)(cid:13)(cid:13)(cid:13) 2 2 = 1 2 λ (cid:107) ¯ x k − ˆ x k − αg k (cid:107) 22 . ( 29 ) Combining ( 28 ) and ( 29 ) yields F λ ( ¯ x k + 1 ) ≤ f ( ˆ x k + 1 ) + 1 2 λ (cid:107) ¯ x k − ˆ x k − αg k (cid:107) 22 . ( 30 ) Using the deﬁnition of ¯ x k , we have (cid:107) ¯ x k − ˆ x k − αg k (cid:107) 2 2 = (cid:107) ¯ x k − ˆ x k (cid:107) 2 2 + 2 α (cid:104) ˆ x k − x k , g k (cid:105) + 2 α ( 1 − β ) β (cid:104) x k − 1 − x k , g k (cid:105) + α 2 (cid:107) g k (cid:107) 2 2 . ( 31 ) It follows from ( 30 ) and ( 31 ) that F λ ( ¯ x k + 1 ) ≤ f ( ˆ x k ) + 1 2 λ (cid:107) ¯ x k − ˆ x k (cid:107) 22 + α λ (cid:104) ˆ x k − x k , g k (cid:105) + α ( 1 − β ) λβ (cid:104) x k − 1 − x k , g k (cid:105) + α 2 (cid:107) g k (cid:107) 22 2 λ = F λ ( ¯ x k ) + α λ (cid:104) ˆ x k − x k , g k (cid:105) + α ( 1 − β ) λβ (cid:104) x k − 1 − x k , g k (cid:105) + α 2 (cid:107) g k (cid:107) 22 2 λ . ( 32 ) We next bound the ﬁrst inner product in ( 32 ) . Since f ( · ) is ρ - weakly convex , it holds that E [ (cid:104) ˆ x k − x k , g k (cid:105) | F k − 1 ] ≤ f ( ˆ x k ) − f ( x k ) + ρ 2 (cid:107) ˆ x k − x k (cid:107) 22 = F ( ˆ x k ) − F ( x k ) + ρ 2 (cid:107) ˆ x k − x k (cid:107) 22 , ( 33 ) 19 where the last step is true since I X ( x k ) = I X ( ˆ x k ) = 0 . We also have that the function x (cid:55)→ F ( x ) + 12 λ (cid:107) x − ¯ x k (cid:107) 22 is ( λ − 1 − ρ ) - strongly convex with ˆ x k being its minimizer , it follows from [ 3 , Theorem 5 . 25 ] that F ( x k ) + 1 2 λ (cid:107) x k − ¯ x k (cid:107) 22 − (cid:18) F ( ˆ x k ) + 1 2 λ (cid:107) ˆ x k − ¯ x k (cid:107) 22 (cid:19) ≥ λ − 1 − ρ 2 (cid:107) ˆ x k − x k (cid:107) 22 . ( 34 ) We thus have F ( x k ) − F ( ˆ x k ) − ρ 2 (cid:107) ˆ x k − x k (cid:107) 22 = F ( x k ) + 1 2 λ (cid:107) x k − ¯ x k (cid:107) 22 − (cid:18) F ( ˆ x k ) + 1 2 λ (cid:107) ˆ x k − ¯ x k (cid:107) 22 (cid:19) − 1 2 λ (cid:107) x k − ¯ x k (cid:107) 22 + 1 2 λ (cid:107) ˆ x k − ¯ x k (cid:107) 22 − ρ 2 (cid:107) ˆ x k − x k (cid:107) 22 ( a ) ≥ λ − 1 − ρ 2 (cid:107) ˆ x k − x k (cid:107) 22 − 1 2 λ (cid:107) x k − ¯ x k (cid:107) 22 + 1 2 λ (cid:107) ˆ x k − ¯ x k (cid:107) 2 2 − ρ 2 (cid:107) ˆ x k − x k (cid:107) 2 2 ( b ) = λ 2 (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 + λ − 1 − 2 ρ 2 (cid:107) ˆ x k − x k (cid:107) 22 − 1 2 λ (cid:107) x k − ¯ x k (cid:107) 22 ( c ) ≥ λ 2 (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 − 1 2 λ (cid:107) x k − ¯ x k (cid:107) 22 , ( 35 ) where ( a ) is due to ( 34 ) , ( b ) follows from the deﬁnition of ∇ F λ ( ¯ x k ) , and ( c ) holds since λ − 1 ≥ 2 ρ . For the second inner product in ( 32 ) , we have E [ (cid:104) x k − 1 − x k , g k (cid:105) | F k − 1 ] ≤ f ( x k − 1 ) − f ( x k ) + ρ 2 (cid:107) x k − 1 − x k (cid:107) 22 . ( 36 ) Therefore , combining ( 33 ) , ( 35 ) , ( 36 ) and plugging the result into ( 32 ) yield E [ F λ ( ¯ x k + 1 ) | F k − 1 ] + α ( 1 − β ) λβ f ( x k ) ≤ F λ ( ¯ x k ) + α ( 1 − β ) λβ f ( x k − 1 ) − α 2 (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 + α 2 λ 2 (cid:107) x k − ¯ x k (cid:107) 22 + ρα ( 1 − β ) 2 λβ (cid:107) x k − 1 − x k (cid:107) 22 + α 2 E [ (cid:107) g k (cid:107) 22 | F k − 1 ] 2 λ . Now , using ( 25 ) , Assumption ( A2 ) , the deﬁnition of ¯ x k , and the fact that β = να give E [ F λ ( ¯ x k + 1 ) | F k − 1 ] + 1 − β λν f ( x k ) ≤ F λ ( ¯ x k ) + 1 − β λν f ( x k − 1 ) − α 2 (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 + ( 1 − β ) 2 2 λ 2 ν 2 1 α (cid:107) x k − x k − 1 (cid:107) 22 + α 2 (cid:18) ρ ( 1 − β ) 2 ν + 1 (cid:19) L 2 2 λ ( 37 ) For simplicity , deﬁne ξ = ( 1 − β ) / ν . Finally , to form a telescoping sum , we simply multiply both sides of eq . ( 9 ) in Lemma ( 3 . 1 ) by ξ 2 / ( 2 λ 2 ) and combine the result with ( 37 ) to get E (cid:20) F λ ( ¯ x k + 1 ) + νξ 2 4 λ 2 (cid:107) p k + 1 (cid:107) 22 + ξ 2 2 αλ 2 (cid:107) x k + 1 − x k (cid:107) 22 (cid:12)(cid:12)(cid:12) F k − 1 (cid:21) + (cid:18) ( 1 − β ) ξ 2 2 λ 2 + ξ λ (cid:19) f ( x k ) ≤ F λ ( ¯ x k ) + νξ 2 4 λ 2 (cid:107) p k (cid:107) 22 + ξ 2 2 αλ 2 (cid:107) x k − x k − 1 (cid:107) 22 + (cid:18) ( 1 − β ) ξ 2 2 λ 2 + ξ λ (cid:19) f ( x k − 1 ) − α 2 (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 + γα 2 L 2 2 λ , 20 where γ = ξ 2 (cid:16) ρ ( 1 − β ) 2 + ν (cid:17) λ + ρξ 2 + 1 . The proof is complete . C Proof of Lemma 4 . 1 In this section , we prove Lemma 4 . 1 . We begin with the following lemma . Lemma C . 1 . Let Assumptions ( A1 ) and ( A3 ) hold . Let α ∈ ( 0 , 1 / ρ ) and β = να for some constant ν > 0 such that β ∈ ( 0 , 1 ] . Then , for any k ∈ N , the iterates generated by procedure ( 7 ) satisfy : E (cid:104) f ( x k + 1 ) + ϕ k + 1 να 2 (cid:12)(cid:12) (cid:12) F k (cid:105) ≤ f ( x k ) + ϕ k να 2 − ( α − ρα 2 2 ) (cid:107) d k + 1 (cid:107) 2 2 + 1 2 ν E (cid:104) (cid:107) z k − z k + 1 (cid:107) 2 2 | F k (cid:105) . Proof . We ﬁrst rewrite the update of x k + 1 in ( 7a ) on the following form : x k + 1 = argmin x ∈X (cid:26) (cid:104) αz k − x k , x (cid:105) + 1 2 (cid:107) x (cid:107) 22 (cid:27) . ( 38 ) Let h ( x ) = 12 (cid:107) x (cid:107) 22 + I X ( x ) and deﬁne its convex conjugate h ∗ ( y ) = sup x ∈X { (cid:104) y , x (cid:105) − h ( x ) } . It is well - known that ∇ h ∗ is 1 - Lipschitz with gradient ∇ h ∗ ( y ) = argmax x ∈X { (cid:104) y , x (cid:105) − h ( x ) } [ 26 , Chapter X ] . Therefore , the update formula ( 38 ) implies that ∇ h ∗ ( x k − αz k ) = x k + 1 . By the smoothness of h ∗ , we have h ∗ ( x k + 1 − αz k + 1 ) ≤ h ∗ ( x k − αz k ) + (cid:104) x k + 1 , x k + 1 − αz k + 1 − x k + αz k (cid:105) + 1 2 (cid:107) x k − αz k − x k + 1 + αz k + 1 (cid:107) 22 . Let ϕ k : = h ∗ ( x k − αz k ) − 12 (cid:107) x k (cid:107) 22 + α (cid:104) x k , z k (cid:105) , it then follows that ϕ k + 1 − ϕ k = h ∗ ( x k + 1 − αz k + 1 ) − 1 2 (cid:107) x k + 1 (cid:107) 22 + α (cid:104) x k + 1 , z k + 1 (cid:105) − h ∗ ( x k − αz k ) + 1 2 (cid:107) x k (cid:107) 22 − α (cid:104) x k , z k (cid:105) ≤ (cid:104) x k + 1 , x k + 1 − αz k + 1 − x k + αz k (cid:105) + 1 2 (cid:107) x k − αz k − x k + 1 + αz k + 1 (cid:107) 22 − 1 2 (cid:107) x k + 1 (cid:107) 22 + α (cid:104) x k + 1 , z k + 1 (cid:105) + 1 2 (cid:107) x k (cid:107) 22 − α (cid:104) x k , z k (cid:105) . ( 39 ) We next apply the identity (cid:104) a , b (cid:105) = 1 2 (cid:107) a (cid:107) 22 + 1 2 (cid:107) b (cid:107) 22 − 1 2 (cid:107) a − b (cid:107) 22 ( 40 ) to obtain (cid:104) x k , αz k (cid:105) = 1 2 (cid:107) x k (cid:107) 22 + α 2 2 (cid:107) z k (cid:107) 22 − 1 2 (cid:107) x k − αz k (cid:107) 22 ( 41 ) (cid:104) x k + 1 , αz k + 1 (cid:105) = 1 2 (cid:107) x k + 1 (cid:107) 22 + α 2 2 (cid:107) z k + 1 (cid:107) 22 − 1 2 (cid:107) x k + 1 − αz k + 1 (cid:107) 22 . ( 42 ) 21 Using the identity ( 40 ) again , we get 1 2 (cid:107) x k − αz k (cid:107) 22 − 1 2 (cid:107) x k + 1 − αz k + 1 (cid:107) 22 = (cid:104) x k − αz k , x k − αz k − x k + 1 + αz k + 1 (cid:105) − 1 2 (cid:107) x k − αz k − x k + 1 + αz k + 1 (cid:107) 22 . ( 43 ) Plugging ( 41 ) – ( 43 ) into ( 39 ) yields ϕ k + 1 − ϕ k ≤ α 2 2 (cid:107) z k + 1 (cid:107) 22 − α 2 2 (cid:107) z k (cid:107) 22 + (cid:104) x k + 1 − x k + αz k , x k + 1 − x k + αz k − αz k + 1 (cid:105) = α 2 2 (cid:107) z k + 1 (cid:107) 22 − α 2 2 (cid:107) z k (cid:107) 22 + α 2 (cid:104) z k , z k − z k + 1 (cid:105) + (cid:104) x k + 1 − x k + αz k , x k + 1 − x k (cid:105) + α (cid:104) x k + 1 − x k , z k − z k + 1 (cid:105) ( 44 ) By identity ( 40 ) , we have 1 2 (cid:107) z k + 1 (cid:107) 22 − 1 2 (cid:107) z k (cid:107) 22 + (cid:104) z k , z k − z k + 1 (cid:105) = 1 2 (cid:107) z k − z k + 1 (cid:107) 22 . ( 45 ) Note that the optimality condition of x k + 1 in ( 21 ) implies that (cid:104) x k + 1 − x k + αz k , x k + 1 − x k (cid:105) ≤ 0 . ( 46 ) By the deﬁnition of z k + 1 , it holds that (cid:104) x k + 1 − x k , αz k − αz k + 1 (cid:105) = (cid:104) x k + 1 − x k , αz k (cid:105) − αβ (cid:104) x k + 1 − x k , g k + 1 (cid:105) + ( 1 − β ) (cid:107) x k + 1 − x k (cid:107) 22 . ( 47 ) We can also deduce from ( 46 ) that (cid:104) x k + 1 − x k , αz k (cid:105) ≤ − (cid:107) x k + 1 − x k (cid:107) 22 , and hence ( 47 ) can be further bounded by (cid:104) x k + 1 − x k , αz k − αz k + 1 (cid:105) ≤ − β (cid:107) x k + 1 − x k (cid:107) 22 + αβ (cid:104) x k − x k + 1 , g k + 1 (cid:105) . ( 48 ) Thus , by combining ( 44 ) , ( 45 ) , ( 46 ) , and ( 48 ) , we arrive at ϕ k + 1 ≤ ϕ k − β (cid:107) x k + 1 − x k (cid:107) 22 + αβ (cid:104) x k − x k + 1 , g k + 1 (cid:105) + α 2 2 (cid:107) z k − z k + 1 (cid:107) 22 . ( 49 ) Next , by the weak convexity of f and Assumption ( A1 ) , we have f ( x k + 1 ) ≤ f ( x k ) − E [ (cid:104) x k − x k + 1 , g k + 1 (cid:105) | F k ] + ρ 2 (cid:107) x k + 1 − x k (cid:107) 22 . ( 50 ) Thus , multiplying both sides of ( 49 ) by 1 / ( βα ) = 1 / ( να 2 ) , taking the conditional expectation , and adding the result to ( 50 ) give E (cid:104) f ( x k + 1 ) + ϕ k + 1 να 2 (cid:12)(cid:12)(cid:12) F k (cid:105) ≤ f ( x k ) + ϕ k να 2 − ( 1 α − ρ 2 ) (cid:107) x k + 1 − x k (cid:107) 22 + 1 2 ν E (cid:104) (cid:107) z k − z k + 1 (cid:107) 22 | F k (cid:105) . ( 51 ) Using the deﬁnition of d k + 1 completes the proof . 22 The next lemma bounds the term 12 ν E (cid:104) (cid:107) z k − z k + 1 (cid:107) 22 | F k (cid:105) in ( 51 ) . Lemma C . 2 . Let ξ = ( 1 − β ) / ν . Under the same setting of Lemma C . 1 , we have E (cid:20) 1 2 ν (cid:107) z k + 1 − z k (cid:107) 22 | F k (cid:21) ≤ f ( x k ) − f ( x k + 1 ) + ξ 2 (cid:107) d k (cid:107) 22 − ξ 2 (cid:107) d k + 1 (cid:107) 22 − (cid:18) α − α 3 ρ 2 + 3 ρα 2 2 (cid:19) (cid:107) d k + 1 (cid:107) 22 + 4 να 2 σ 2 . Proof . Let ∆ k : = g k − ∇ f ( x k ) , we have (cid:107) z k + 1 − z k (cid:107) 22 = (cid:107) β ( g k + 1 − g k ) + ( 1 − β ) ( d k + 1 − d k ) (cid:107) 22 = (cid:107) β ( ∆ k + 1 − ∆ k ) + β ( ∇ f ( x k + 1 ) − ∇ f ( x k ) ) + ( 1 − β ) ( d k + 1 − d k ) (cid:107) 22 = β 2 (cid:107) ∆ k + 1 − ∆ k (cid:107) 22 + (cid:107) β ( ∇ f ( x k + 1 ) − ∇ f ( x k ) ) + ( 1 − β ) ( d k + 1 − d k ) (cid:107) 22 + 2 β (cid:104) ∆ k + 1 − ∆ k , β ( ∇ f ( x k + 1 ) − ∇ f ( x k ) ) + ( 1 − β ) ( d k + 1 − d k ) (cid:105) . ( 52 ) First , by Assumption ( A3 ) , we have β 2 E (cid:104) (cid:107) ∆ k + 1 − ∆ k (cid:107) 22 | F k (cid:105) ≤ β 2 E (cid:104) 2 (cid:107) ∆ k + 1 (cid:107) 22 + 2 (cid:107) ∆ k (cid:107) 22 | F k (cid:105) ≤ 4 β 2 σ 2 . ( 53 ) Deﬁne p k : = β ∇ f ( x k ) + ( 1 − β ) d k and let T be the last term in ( 52 ) , then T = 2 β (cid:104) ∆ k + 1 − ∆ k , p k + 1 − p k (cid:105) . ( 54 ) Since p k + 1 − p k is a function of F k = σ ( S 0 , . . . , S k ) , it follows from Assumption ( A1 ) that E [ (cid:104) ∆ k + 1 , p k + 1 − p k (cid:105) | F k ] = E [ (cid:104) g k + 1 − ∇ f ( x k + 1 ) , p k + 1 − p k (cid:105) | F k ] = (cid:104) E [ g k + 1 | F k ] − ∇ f ( x k + 1 ) , p k + 1 − p k (cid:105) = 0 . ( 55 ) Similarly , conditioned on F k , both terms in the inner product of E [ (cid:104) ∆ k , p k + 1 − p k (cid:105) | F k ] are deterministic , and hence − E [ (cid:104) ∆ k , p k + 1 − p k (cid:105) | F k ] = − E [ (cid:104) ∆ k , p k + 1 − p k (cid:105) | F k − 1 ] . ( 56 ) Similar to ( 55 ) , we obtain E [ (cid:104) ∆ k , p k (cid:105) | F k − 1 ] = (cid:104) E [ g k | F k − 1 ] − ∇ f ( x k ) , p k (cid:105) = 0 . ( 57 ) To bound the remaining term E [ (cid:104) ∆ k , p k + 1 (cid:105) | F k − 1 ] , we introduce the following virtual iterate : x (cid:48) k + 1 = Π X ( x k − αp k ) , and deﬁne p (cid:48) k + 1 = β ∇ f ( x (cid:48) k + 1 ) + ( 1 − β ) x k − x (cid:48) k + 1 α . By deﬁnitions , both x (cid:48) k + 1 and p (cid:48) k + 1 only depend on F k − 1 , it follows that − E [ (cid:104) ∆ k , p k + 1 (cid:105) | F k − 1 ] = − E (cid:2)(cid:10) ∆ k , p (cid:48) k + 1 (cid:11) | F k − 1 (cid:3) + E (cid:2)(cid:10) ∆ k , p (cid:48) k + 1 − p k + 1 (cid:11) | F k − 1 (cid:3) = 0 + E (cid:2)(cid:10) ∆ k , p (cid:48) k + 1 − p k + 1 (cid:11) | F k − 1 (cid:3) ≤ E (cid:2) (cid:107) ∆ k (cid:107) 2 (cid:13)(cid:13) p k + 1 − p (cid:48) k + 1 (cid:13)(cid:13) 2 | F k − 1 (cid:3) . 23 We have (cid:13)(cid:13) p k + 1 − p (cid:48) k + 1 (cid:13)(cid:13) 2 = (cid:13)(cid:13)(cid:13)(cid:13) β ( ∇ f ( x k + 1 ) − ∇ f ( x (cid:48) k + 1 ) ) + ( 1 − β ) x (cid:48) k + 1 − x k + 1 α (cid:13)(cid:13)(cid:13)(cid:13) 2 ( a ) ≤ β (cid:13)(cid:13) ( ∇ f ( x k + 1 ) − ∇ f ( x (cid:48) k + 1 ) ) (cid:13)(cid:13) 2 + 1 − β α (cid:13)(cid:13) x (cid:48) k + 1 − x k + 1 (cid:13)(cid:13) 2 ( b ) ≤ (cid:18) βρ + 1 − β α (cid:19) (cid:13)(cid:13) x k + 1 − x (cid:48) k + 1 (cid:13)(cid:13) 2 ( c ) ≤ αβ (cid:18) βρ + 1 − β α (cid:19) (cid:107) g k − ∇ f ( x k ) (cid:107) 2 ( d ) ≤ β (cid:107) ∆ k (cid:107) 2 , ( 58 ) where ( a ) holds since (cid:107)·(cid:107) 2 is convex ; ( b ) follows since ∇ f is ρ - Lipschitz ; ( c ) follows from the deﬁnition of x (cid:48) k + 1 , p k , and the nonexpansiveness of Π X ( · ) ; and ( d ) is true since α ∈ ( 0 , 1 / ρ ) . We thus have − E [ (cid:104) ∆ k , p k + 1 (cid:105) | F k − 1 ] ≤ β E (cid:104) (cid:107) ∆ k (cid:107) 22 | F k − 1 (cid:105) ≤ βσ 2 . ( 59 ) Therefore , taking the conditional expectation in ( 54 ) and using ( 55 ) , ( 56 ) , ( 57 ) , ( 59 ) yield E [ T | F k ] ≤ 2 β 2 σ 2 . ( 60 ) Plugging ( 53 ) and ( 60 ) into ( 52 ) , we arrive at E (cid:104) (cid:107) z k + 1 − z k (cid:107) 22 | F k (cid:105) ≤ (cid:107) β ( ∇ f ( x k + 1 ) − ∇ f ( x k ) ) + ( 1 − β ) ( d k + 1 − d k ) (cid:107) 22 + 6 β 2 σ 2 ≤ β (cid:107)∇ f ( x k + 1 ) − ∇ f ( x k ) (cid:107) 22 + ( 1 − β ) (cid:107) d k + 1 − d k (cid:107) 22 + 6 β 2 σ 2 ≤ βρ 2 (cid:107) x k + 1 − x k (cid:107) 22 + ( 1 − β ) (cid:107) d k + 1 − d k (cid:107) 22 + 6 β 2 σ 2 . = βρ 2 α 2 (cid:107) d k + 1 (cid:107) 22 + ( 1 − β ) (cid:107) d k + 1 − d k (cid:107) 22 + 6 β 2 σ 2 . ( 61 ) where we used the convexity of (cid:107)·(cid:107) 22 in the second inequality . We now decompose and bound the term (cid:107) d k + 1 − d k (cid:107) 22 as follows (cid:107) d k + 1 − d k (cid:107) 22 = (cid:107) d k (cid:107) 22 − (cid:107) d k + 1 (cid:107) 22 + 2 (cid:104) d k + 1 , d k + 1 − d k (cid:105) . ( 62 ) First , it follows from the optimality condition of x k + 1 in ( 21 ) and the deﬁnition of d k that (cid:104)− d k + 1 + βg k + ( 1 − β ) d k , − d k + 1 (cid:105) ≤ 0 . Thus , (cid:104) d k + 1 , d k + 1 − d k (cid:105) ≤ (cid:107) d k + 1 (cid:107) 22 + 1 1 − β (cid:104)− d k + 1 + βg k , d k + 1 (cid:105) = − β 1 − β (cid:107) d k + 1 (cid:107) 22 + β α ( 1 − β ) (cid:104) g k , x k − x k + 1 (cid:105) . ( 63 ) We have (cid:104) g k , x k − x k + 1 (cid:105) = (cid:104) g k − ∇ f ( x k ) , x k − x k + 1 (cid:105) + (cid:104)∇ f ( x k + 1 ) , x k − x k + 1 (cid:105) + (cid:104)∇ f ( x k ) − ∇ f ( x k + 1 ) , x k − x k + 1 (cid:105) . 24 For the ﬁrst term , since E [ (cid:104) g k − ∇ f ( x k ) , x k − x k + 1 (cid:105) | F k ] = E (cid:2)(cid:10) g k − ∇ f ( x k ) , x k − x (cid:48) k + 1 (cid:11) | F k − 1 (cid:3) + E (cid:2)(cid:10) g k − ∇ f ( x k ) , x (cid:48) k + 1 − x k + 1 (cid:11) | F k − 1 (cid:3) = E (cid:2)(cid:10) g k − ∇ f ( x k ) , x (cid:48) k + 1 − x k + 1 (cid:11) | F k − 1 (cid:3) , following the same steps leading to ( 58 ) , it can be bounded as E [ (cid:104) g k − ∇ f ( x k ) , x k − x k + 1 (cid:105) | F k ] ≤ αβ E (cid:104) (cid:107) ∆ k (cid:107) 22 | F k − 1 (cid:105) ≤ αβσ 2 . ( 64 ) By the smoothness of ∇ f , we also have (cid:104)∇ f ( x k + 1 ) , x k − x k + 1 (cid:105) ≤ f ( x k ) − f ( x k + 1 ) + ρ 2 (cid:107) x k + 1 − x k (cid:107) 22 ( 65 ) (cid:104)∇ f ( x k ) − ∇ f ( x k + 1 ) , x k − x k + 1 (cid:105) ≤ ρ (cid:107) x k + 1 − x k (cid:107) 22 . ( 66 ) Hence , it follows from ( 63 ) – ( 66 ) that (cid:104) d k + 1 , d k + 1 − d k (cid:105) ≤ − β 1 − β (cid:107) d k + 1 (cid:107) 22 + β α ( 1 − β ) (cid:20) f ( x k ) − f ( x k + 1 ) + 3 ρα 2 2 (cid:107) d k + 1 (cid:107) 22 + αβσ 2 (cid:21) , ( 67 ) where we also used the fact that (cid:107) x k + 1 − x k (cid:107) 22 = α 2 (cid:107) d k + 1 (cid:107) 22 . By multiplying both sides of ( 62 ) by 1 − β and combining with ( 67 ) give ( 1 − β ) (cid:107) d k + 1 − d k (cid:107) 22 = ( 1 − β ) (cid:16) (cid:107) d k (cid:107) 22 − (cid:107) d k + 1 (cid:107) 22 (cid:17) − ( 2 β − 3 ραβ ) (cid:107) d k + 1 (cid:107) 22 + 2 β α ( f ( x k ) − f ( x k + 1 ) ) + 2 β 2 σ 2 . ( 68 ) Plugging ( 68 ) into ( 61 ) yields E (cid:104) (cid:107) z k + 1 − z k (cid:107) 22 | F k (cid:105) ≤ ( 1 − β ) (cid:16) (cid:107) d k (cid:107) 22 − (cid:107) d k + 1 (cid:107) 22 (cid:17) + 2 β α ( f ( x k ) − f ( x k + 1 ) ) − (cid:0) 2 β − 3 ραβ − βρ 2 α 2 (cid:1) (cid:107) d k + 1 (cid:107) 22 + 8 β 2 σ 2 . ( 69 ) Multiplying both sides of ( 69 ) by 1 / ( 2 ν ) = α / ( 2 β ) and noting that ξ = ( 1 − β ) / ν complete the proof . Having established Lemmas C . 1 and C . 2 , the result of Lemma 4 . 1 follows immediately from the deﬁnition of the function W k and the fact that α ∈ ( 0 , 1 / ( 4 ρ ) ] . D Proof of Theorem 2 We ﬁrst show by induction that E [ (cid:107) z k (cid:107) 22 ] ≤ σ 2 + G 2 for all k ∈ N . Since z 0 = ∇ f ( x 0 , S 0 ) and (cid:107)∇ f ( x 0 ) (cid:107) 2 ≤ G , it follows from Assumptions ( A1 ) and ( A3 ) that E [ (cid:107) z 0 (cid:107) 22 ] = E [ (cid:107)∇ f ( x 0 , S 0 ) − ∇ f ( x 0 ) + ∇ f ( x 0 ) (cid:107) 22 ] = E [ (cid:107)∇ f ( x 0 , S 0 ) − ∇ f ( x 0 ) (cid:107) 22 ] + (cid:107)∇ f ( x 0 ) (cid:107) 22 ≤ σ 2 + G 2 . 25 Suppose that E [ (cid:107) z i (cid:107) 22 ] ≤ σ 2 + G 2 for i = 0 , . . . , k , we have E [ (cid:107) z k + 1 (cid:107) 22 ] = E (cid:34)(cid:13)(cid:13)(cid:13)(cid:13) β ( g k + 1 − ∇ f ( x k + 1 ) ) + β ∇ f ( x k + 1 ) + ( 1 − β ) x k − x k + 1 α (cid:13)(cid:13)(cid:13)(cid:13) 2 2 (cid:35) = β 2 E [ (cid:107) g k + 1 − ∇ f ( x k + 1 ) (cid:107) 22 ] + E (cid:34)(cid:13)(cid:13)(cid:13)(cid:13) β ∇ f ( x k + 1 ) + ( 1 − β ) x k − x k + 1 α (cid:13)(cid:13)(cid:13)(cid:13) 2 2 (cid:35) ≤ β 2 σ 2 + β (cid:107)∇ f ( x k + 1 ) (cid:107) 22 + ( 1 − β ) E (cid:34)(cid:13)(cid:13)(cid:13)(cid:13) x k − x k + 1 α (cid:13)(cid:13)(cid:13)(cid:13) 2 2 (cid:35) , where we used the convexity of (cid:107)·(cid:107) 22 in the last step . Since x k + 1 = Π X ( x k − αz k ) , the nonexpansiveness of Π X ( · ) and the induction hypothesis imply that E (cid:34)(cid:13)(cid:13)(cid:13)(cid:13) x k − x k + 1 α (cid:13)(cid:13)(cid:13)(cid:13) 2 2 (cid:35) ≤ E (cid:104) (cid:107) z k (cid:107) 22 (cid:105) ≤ σ 2 + G 2 . Since β ∈ ( 0 , 1 ] , we have β 2 ≤ β , and hence E [ (cid:107) z k + 1 (cid:107) 22 ] ≤ σ 2 + G 2 , as desired . From this , together with the fact that E [ (cid:107) g k (cid:107) 22 ] ≤ σ 2 + G 2 , the theorem follows immediately from replacing L 2 by σ 2 + G 2 everywhere in the proofs of Lemmas 3 . 1 – 3 . 2 and Theorem 1 . E Proof of Theorem 3 First , from ( 30 ) , we have F λ ( ¯ x k + 1 ) ≤ f ( ˆ x k ) + 1 2 λ (cid:107) ¯ x k − ˆ x k − αg k (cid:107) 22 . ( 70 ) We next follow [ 8 ] and write ˆ x k as ˆ x k = αλ − 1 ¯ x k − α ∇ f ( ˆ x k ) + ( 1 − αλ − 1 ) ˆ x k , which is true since in this case ∇ F λ ( ¯ x k ) = ∇ f ( ˆ x k ) . Let δ = 1 − αλ − 1 ∈ ( 0 , 1 ) , we have E [ (cid:107) ¯ x k − ˆ x k − αg k (cid:107) 22 ] = E [ (cid:107) δ ( ¯ x k − ˆ x k ) − α ( g k − ∇ f ( x k ) ) − α ( ∇ f ( x k ) − ∇ f ( ˆ x k ) ) (cid:107) 22 ] = δ 2 E [ (cid:107) ¯ x k − ˆ x k (cid:107) 22 ] − 2 αδ E [ (cid:104) ¯ x k − ˆ x k , ∇ f ( x k ) − ∇ f ( ˆ x k ) (cid:105) ] + α 2 E [ (cid:107)∇ f ( x k ) − ∇ f ( ˆ x k ) (cid:107) 2 2 ] + α 2 σ 2 . Using Young’s inequality | ab | ≤ a 2 / ( 2 θ ) + θb 2 / 2 for a , b ∈ R and θ > 0 , we get − 2 αδ E [ (cid:104) ¯ x k − ˆ x k , ∇ f ( x k ) − ∇ f ( ˆ x k ) (cid:105) ] ≤ αδρ E [ (cid:107) ¯ x k − ˆ x k (cid:107) 22 ] + αδ / ρ E [ (cid:107)∇ f ( x k ) − ∇ f ( ˆ x k ) (cid:107) 22 ] . Since ∇ f is ρ - Lipschitz , it follows that E [ (cid:107) ¯ x k − ˆ x k − αg k (cid:107) 22 ] ≤ ( δ 2 + ραδ ) E [ (cid:107) ¯ x k − ˆ x k (cid:107) 22 ] + ( αδ / ρ + α 2 ) E [ (cid:107)∇ f ( x k ) − ∇ f ( ˆ x k ) (cid:107) 22 ] + α 2 σ 2 ≤ ( δ 2 + ραδ ) E [ (cid:107) ¯ x k − ˆ x k (cid:107) 22 ] + ( αδρ + α 2 ρ 2 ) E [ (cid:107) x k − ˆ x k (cid:107) 22 ] + α 2 σ 2 ≤ ( δ 2 + 3 ραδ + 2 α 2 ρ 2 ) E [ (cid:107) ¯ x k − ˆ x k (cid:107) 22 ] + 2 ( αδρ + α 2 ρ 2 ) E [ (cid:107) x k − ¯ x k (cid:107) 22 ] + α 2 σ 2 , where we also used the inequality (cid:107) a + b (cid:107) 22 ≤ 2 (cid:107) a (cid:107) 22 + 2 (cid:107) b (cid:107) 22 in the last step . We have δ 2 + 3 ραδ + 2 α 2 ρ 2 = 1 − α ( 2 λ − 1 − 3 ρ ) + α 2 λ − 1 ( λ − 1 − 3 ρ + 2 ρ 2 λ ) 26 Note that for λ − 1 ∈ [ 3 ρ / 2 , 2 ρ ] , the term associated with α 2 is nonpositive , and hence E [ (cid:107) ¯ x k − ˆ x k − αg k (cid:107) 22 ] ≤ E [ (cid:107) ¯ x k − ˆ x k (cid:107) 22 ] − α ( 2 λ − 1 − 3 ρ ) E [ (cid:107) ¯ x k − ˆ x k (cid:107) 22 ] + 2 ( αδρ + α 2 ρ 2 ) E [ (cid:107) x k − ¯ x k (cid:107) 22 ] + α 2 σ 2 . ( 71 ) Therefore , it follows from ( 70 ) and ( 71 ) that E [ F λ ( ¯ x k + 1 ) ] ≤ E [ F λ ( ¯ x k ) ] − α 2 λ − 1 − 3 ρ 2 λ − 1 E [ (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 ] + αδρ + α 2 ρ 2 λ E [ (cid:107) x k − ¯ x k (cid:107) 22 ] + α 2 σ 2 2 λ . We also have αδρ + α 2 ρ 2 λ = αρ λ + α 2 ( ρ 2 − ρλ − 1 ) ≤ αρ λ ≤ α λ 2 , where last two step hold since λ − 1 ≥ ρ . Therefore , using the fact that ξ = ( 1 − β ) / ν , ν = β / α , ¯ x k = x k + 1 − ββ ( x k − x k − 1 ) , and d k = x k − 1 − x k α , we obtain E [ F λ ( ¯ x k + 1 ) ] ≤ E [ F λ ( ¯ x k ) ] − α 2 λ − 1 − 3 ρ 2 λ − 1 E [ (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 ] + αξ 2 λ 2 E [ (cid:107) d k (cid:107) 22 ] + α 2 σ 2 2 λ . ( 72 ) Now , multiplying both sides of ( 14 ) by ξ 2 / λ 2 and combining the result with ( 72 ) , we obtain E (cid:20) F λ ( ¯ x k + 1 ) + ξ 2 λ 2 W k + 1 + αξ 2 λ 2 (cid:107) d k + 1 (cid:107) 22 (cid:21) ≤ E (cid:20) F λ ( ¯ x k ) + ξ 2 λ 2 W k + αξ 2 λ 2 (cid:107) d k (cid:107) 22 (cid:21) − α 2 λ − 1 − 3 ρ 2 λ − 1 E [ (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 ] + ( 1 + 8 νξ 2 / λ ) α 2 σ 2 2 λ . ( 73 ) Let V k + 1 denote the left - hand - side of ( 73 ) , summing the result over k = − 1 , . . . , K − 1 gives V K ≤ V − 1 − α 0 √ K + 1 2 λ − 1 − 3 ρ 2 λ − 1 K (cid:88) k = 0 E [ (cid:107)∇ F λ ( ¯ x k ) (cid:107) 22 ] + ( K + 1 ) ( 1 + 8 νξ 2 / λ ) σ 2 α 2 2 λ . Note that V − 1 = F λ ( ¯ x − 1 ) + ξ 2 λ 2 (cid:18) 2 f ( x − 1 ) + ϕ − 1 να 2 + ξ 2 (cid:107) d − 1 (cid:107) 22 (cid:19) + αξ 2 λ 2 (cid:107) d − 1 (cid:107) 22 ≤ (cid:18) 1 + 2 ξ 2 λ 2 (cid:19) f ( x 0 ) , where we used the facts that x − 1 = x 0 , ϕ − 1 = 0 ( since z − 1 = 0 ) , d − 1 = 0 , and F λ ( ¯ x − 1 ) = F λ ( ¯ x 0 ) ≤ f ( x 0 ) . Therefore , lower - bounding the left - hand - side by ( 1 + 2 ξ 2 / λ 2 ) f (cid:63) and rear - ranging terms , we obtain E (cid:104) (cid:107)∇ F λ ( ¯ x k ∗ ) (cid:107) 22 (cid:105) ≤ 2 λ − 1 2 λ − 1 − 3 ρ · ( 1 + 2 ξ 2 / λ 2 ) ( f ( x 0 ) − f (cid:63) ) + ( 1 + 8 νξ 2 / λ ) σ 2 α 20 2 λ α 0 √ K + 1 , ( 74 ) where the last expectation is taken with respect to all random sequences generated by the method and the uniformly distributed random variable k ∗ . Since ν = 1 / α 0 and ξ = ( 1 − β ) / ν ≤ 1 / ν , hence E (cid:104) (cid:107)∇ F λ ( ¯ x k ∗ ) (cid:107) 22 (cid:105) ≤ 2 λ − 1 2 λ − 1 − 3 ρ · ( 1 + 2 α 20 / λ 2 ) ( f ( x 0 ) − f (cid:63) ) + ( 1 + 8 α 0 / λ ) σ 2 α 20 2 λ α 0 √ K + 1 . Finally , letting λ = 1 / ( 2 ρ ) completes the proof . 27