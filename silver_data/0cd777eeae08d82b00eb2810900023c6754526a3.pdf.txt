Hone as You Read : A Practical Type of Interactive Summarization Tanner Bohn Western University London , ON , Canada tbohn @ uwo . ca Charles X . Ling Western University London , ON , Canada charles . ling @ uwo . ca Abstract We present HARE , a new task where reader feedback is used to optimize document sum - maries for personal interest during the nor - mal ﬂow of reading . This task is related to interactive summarization , where person - alized summaries are produced following a long feedback stage where users may read the same sentences many times . However , this process severely interrupts the ﬂow of read - ing , making it impractical for leisurely read - ing . We propose to gather minimally - invasive feedback during the reading process to adapt to user interests and augment the document in real - time . Building off of recent advances in unsupervised summarization evaluation , we propose a suitable metric for this task and use it to evaluate a variety of approaches . Our approaches range from simple heuristics to preference - learning and their analysis pro - vides insight into this important task . Hu - man evaluation additionally supports the prac - ticality of HARE . The code to reproduce this work is available at https : / / github . com / tannerbohn / HoneAsYouRead . 1 Introduction Keeping readers engaged in an article and helping them ﬁnd desired information are important objec - tives ( Calder et al . , 2009 ; Nenkova and McKeown , 2011 ) . These objectives help readers deal with the explosion of online content and provide an edge to content publishers in a competitive industry . To help readers ﬁnd personally relevant content while maintaining the ﬂow of natural reading , we propose a new text summarization problem where the sum - mary is h oned a s you re ad ( HARE ) . The challenge is to learn from unobtrusive user feedback , such as the types in Figure 1 , to identify uninteresting content to hop over . This new task is related to both query - based summarization ( QS ) and interactive personalized Two scientists from CNRS and Sorbonne University working at the Institute of Celestial Mechanics and Ephemeris Calculation ( Paris Observatory - - PSL / CNRS ) have just shown that the influence of Saturn ' s satellites can explain the tilt of the rotation axis of the gas giant . Their work , published on 18 January 2021 in the journal Nature Astronomy , also predicts that the tilt will increase even further over the next few billion years . Rather like David versus Goliath , it appears that Saturn ' s tilt may in fact be caused by its moons . The decisive event that tilted Saturn is thought to have occurred relatively recently . For over three billion years after its Two scientists from CNRS and Sorbonne University working at the Institute of Celestial Mechanics and Ephemeris Calculation ( Paris Observatory - - PSL / CNRS ) have just shown that the influence of Saturn ' s satellites can explain the tilt of the rotation axis of the gas giant . Their work , published on 18 January 2021 in the journal Nature Astronomy , also predicts that the tilt will increase even further over the next few billion years . Rather like David versus Goliath , it appears that Saturn ' s tilt may in fact be caused by its moons . The decisive event that tilted Saturn is thought to have occurred relatively recently . For over three billion years after its Two scientists from CNRS and Sorbonne University working at the Institute of Celestial Mechanics and Ephemeris Calculation ( Paris Observatory - - PSL / CNRS ) have just shown that the influence of Saturn ' s satellites can explain the tilt of the rotation axis of the gas giant . Their work , published on 18 January 2021 in the journal Nature Astronomy , also predicts that the tilt will increase even further over the next few billion years . Rather like David versus Goliath , it appears that Saturn ' s tilt may in fact be caused by its moons . The decisive event that tilted Saturn is thought to have occurred relatively recently . For over three billion years after its ( a ) ( b ) ( c ) hidden hidden hidden Figure 1 : Potential feedback methods for HARE used on a smartphone . In ( a ) , users can choose to swipe left or right to indicate interest or disinterest in sections of text as they read . Users may also provide implicit feed - back in the form of dwell time in center window ( b ) or gaze location , as measured by camera for example ( c ) . More interesting text may have longer gazes or dwell time . The approaches evaluated in this paper rely on feedback similar to ( a ) , but further development in HARE can extend to ( b ) or ( c ) . summarization ( IPS ) . In QS , users must specify a query to guide the resultant summary ( Damova and Koychev , 2010 ) . For users performing focused research , specifying queries is useful , but for more leisurely reading , this requirement interrupts the natural ﬂow . Approaches to IPS avoid the problem of having to explicitly provide a query . However , they suffer a similar problem by requiring users to go through several iterations of summary reading and feedback - providing before a ﬁnal summary is produced ( Yan et al . , 2011 ; Avinesh et al . , 2018 ; Gao et al . , 2019 ; Simpson et al . , 2019 ) . In contrast , HARE places high importance on non - intrusiveness by satisfying multiple properties detailed in Section 3 . 1 ( such as feedback being non - invasive ) . We ﬁnd that due to the high cost of generating a dataset for this task , evaluation poses a difﬁculty . To overcome this , we adapt recent re - search in unsupervised summary evaluation . We a r X i v : 2105 . 02923v1 [ c s . C L ] 6 M a y 2021 also describe a variety of approaches for HARE that estimate what the user is interested in and how much they want to read . Automated evaluation ﬁnds that relatively simple approaches based on hiding sentences nearby or similar to disliked ones , or explicitly modelling user interests , outperforms the control , where no personalization is done . Hu - man evaluation suggests that not only is deciding the relevance of sentences rather easy in practice , but that even with simple binary feedback , HARE models may truly provide useful reading assistance . The major contributions of this work are : 1 . We deﬁne the novel HARE task , and describe a suitable evaluation technique ( Section 3 ) . 2 . We describe a wide range of motivated ap - proaches for HARE that should serve as useful baselines for future research ( Section 4 ) . 3 . We evaluate our approaches to gain a deeper understanding of the task ( Section 5 ) . 2 Related Work In this section , we examine related work on QS , IPS , and unsupervised summarization evaluation . 2 . 1 Query - based Summarization Both tasks of HARE and QS aim to produce per - sonalized summaries . Unlike generic summariza - tion where many large datasets exist ( Hermann et al . , 2015 ; Fabbri et al . , 2019 ; Narayan et al . , 2018 ) , development in QS has been affected by a lack of suitable training data ( Xu and Lapata , 2020 ) . To cope , approaches have relied on hand - crafted features ( Conroy et al . , 2005 ) , unsuper - vised techniques ( Van Lierde and Chow , 2019 ) , and cross - task knowledge transfer ( Xu and Lapata , 2020 ) . The approach of Mohamed and Rajasekaran ( 2006 ) highlights how query - based summarizers often work by adapting a generic summarization algorithm and incorporating the query with an ad - ditional sentence scoring or ﬁltering component . Alternatively , one can avoid training on QS data by decomposing the task into several steps , each performed by a module constructed for a related task ( Xu and Lapata , 2020 ) . A pervasive assumption in QS is that users have a query for which a brief summary is expected . This is reﬂected in QS datasets where dozens of documents are expected to be summarized in a max - imum of 250 words ( Dang , 2005 ; Hoa , 2006 ) or single documents summarized in a single sentence ( Hasselqvist et al . , 2017 ) . However , in HARE , we are interested in a wider range of reading prefer - ences . This includes users who are interested in reading the whole article and users whose interests are not efﬁciently expressed in a written query . 2 . 2 Interactive Personalized Summarization The iterative reﬁnement of summaries based on user feedback is also considered by IPS approaches . An early approach by Yan et al . ( 2011 ) considers progressively learning user interests by providing a summary ( of user - speciﬁed length ) and allowing them to click on sentences they want to know more about . Based on the words in clicked sentences , a new summary can be generated and the process re - peated . Instead of per - sentence feedback , Avinesh and Meyer ( 2017 ) allows users to indicate which bigrams of a candidate summary are relevant to their interests . A successor to this system reduces the computation time to produce each summary down to an interactive level of 500ms ( Avinesh et al . , 2018 ) . The APRIL system ( Gao et al . , 2019 ) aims to reduce the cognitive burden of IPS by in - stead allowing users to indicate preference between candidate summaries . Using this preference infor - mation , a summary - ranking model is trained and used to select the next pair of candidate summaries . Shared among these previous works is that the user is involved in an interactive process which in - terrupts the normal reading ﬂow with the reviewing of many intermediate summaries . In HARE , the user reads the document as it is being summarized , so that any given sentence is read at most once ( if it has not already been removed ) . These previous works also focus on multi - document summariza - tion , whereas we wish to improve the reading expe - rience during the reading of individual documents . 2 . 3 Unsupervised Summary Evaluation When gold - standard human - written summaries are available for a document or question - document pair , the quality of a model - produced summary is commonly computed with the ROUGE metric ( Lin and Och , 2004 ) . Driven by high costs of obtaining human - written summaries at a large scale , espe - cially for tasks such as multi - document summariza - tion or QS , unsupervised evaluation of summaries ( i . e . without using gold - standards ) has rapidly de - veloped ( Louis and Nenkova , 2013 ) . Louis and Nenkova ( 2009 ) found that the Jensen Shannon divergence between the word distribu - tions in a summary and reference document out - performs many other candidates and achieves a high correlation with manual summary ratings , but not quite as high as ROUGE combined with ref - erence summaries . Sun and Nenkova ( 2019 ) con - sider a variety of distributed text embeddings and propose to use the cosine similarity of summary and document ELMo embeddings ( Peters et al . , 2018 ) . B¨ohm et al . ( 2019 ) consider learning a re - ward function from existing human ratings . Their reward function only requires a model summary and document as input and achieves higher correla - tion with human ratings than other metrics ( includ - ing ROUGE which requires reference summaries ) . Stiennon et al . ( 2020 ) also consider this approach , with a larger collection of human ratings and larger models . However , Gao et al . ( 2020 ) found that com - paring ELMo embeddings or using the learned re - ward from B¨ohm et al . does not generalize to other summarization tasks . Their evaluation of more advanced contextualized embeddings found that Sentence - BERT ( SBERT ) embeddings ( Reimers and Gurevych , 2019 ) with word mover’s - based dis - tance ( Kusner et al . , 2015 ) outperforms other un - supervised options . Post - publication experiments by B¨ohm et al . further support the generalizability of this approach 1 . In Section 3 . 3 , we adapt the method of Gao et al . to HARE evaluation . 3 Task Formulation To deﬁne the proposed task , we will ﬁrst describe how a user interacts with an HARE summarizer ( Section 3 . 1 ) . Second , we describe a method for modelling user interests and feedback for automatic evaluation ( Section 3 . 2 ) . Third , we propose an evaluation metric for this new task ( Section 3 . 3 ) . 3 . 1 User - Summarizer Interaction Loop The interaction between a user and HARE summa - rizer , as shown in Figure 2 and sketched in Algo - rithm 1 , consists of the user reading the shown sen - tences and providing feedback on their relevance . Using this feedback , the summarizer decides which remaining sentences to show , aiming to hide un - interesting sentences . This interaction is designed to smoothly integrate into the natural reading pro - cess by exhibiting three important properties : 1 ) feedback is either implicit or non - intrusive , 2 ) sen - tences are presented in their original order to try 1 The additional results can be found here : https : / / github . com / yg211 / summary - reward - no - reference . Studies in animal models have found that increasing the aggregation of Aβ in the hippocampus Synaptic plasticity is crucial to the development of learning and cognitive functions in the hippocampus . Thus , Aβ and its role in causing cognitive memory and deficits have been the focus of most research aimed at finding treatments for Alzheimer ' s . Upon additional perfusion with oxytocin , however , the signaling abilities increased , AcceptedAccepted Rejected Accepted HiddenAccepted Rejected Accepted ( will be rejected ) HiddenHidden ( will be hidden ) Hidden User reads next visible sentence and gives feedback Feedback used to tune model on user preferences Model updates unread portion of summary Article sentences rejected hidden rejected Figure 2 : In HARE , users are shown sentences in their original order , and can provide relevance feedback . A model uses this feedback to optimize the remainder of the article , automatically hiding uninteresting sen - tences . maintain coherence , and 3 ) updates to the summary should occur beyond the current reading point so as to not distract the user . Next , we discuss how to model a user in this interaction for the purposes of automatic evaluation . 3 . 2 User Modelling In order to model user interaction during HARE , we need to know what kind of feedback they would provide when shown a sentence . This requires understanding how much a user would be interested in a given sentence and how feedback is provided . User interests For our work , user interests will be modelled as a weighted set of concept vec - tors from a semantic embedding space . Given a weighted set of k user interests , U = { < w 1 , c 1 > , . . . , < w k , c k > } such that w i ∈ [ 0 , 1 ] and max ( w ) = 1 , and a sentence embedding , x , the interest level ( which we also refer to as impor - tance ) is calculated with Equation 1 . We use cosine distance for ∆ . Intuitively , the importance of a sentence reﬂects the maximum weighted similarity to any of the interests . This method of comput - ing importance is similar to that used by Avinesh et al . ( 2018 ) ; Wu et al . ( 2019 ) ; Teevan et al . ( 2005 ) . However , we adapt it to accommodate modern dis - tributed sentence embeddings ( SBERT ) . R ( U , x ) = max i = 1 , . . . , k w i ( 1 − ∆ ( c i , x ) ) ( 1 ) Algorithm 1 : User - Summarizer Interaction 1 user chooses a document D = [ x 1 , . . . , x | D | ] to read with help from summarizer M 2 S = ∅ / / summary sentences 3 for i = 1 , . . . , | D | do 4 if M decides to show x i to user then 5 show sentence x i to user 6 S : = S ∪ { x i } 7 incorporate any feedback into M 8 end 9 if user is done reading then 10 break 11 end 12 end 13 return S Feedback types Given a sentence interest score of r x ∈ [ 0 , 1 ] , what feedback will be observed by the model ? If using implicit feedback like dwell time or gaze tracking , feedback could be continu - ously valued . With explicit feedback , like ratings or thumbs up / down , feedback could be discrete . For an in - depth discussion on types of user feedback , see Jayarathna and Shipman ( 2017 ) . In this work , we will consider an explicit feed - back inspired by the “Tinder sort” gesture popular - ized by the Tinder dating app 2 , where users swipe left to indicate disinterest , and right to indicate interest . This feedback interaction has proven to be very quick and easy . Users will routinely sort through hundreds of items in a sitting ( David and Cambre , 2016 ) . To adapt this feedback method to our interactive summarization system , we can consider users to “accept” a sentence if they swipe right , and “reject” it if they swipe left ( see Figure 1a and Figure 2 ) 3 . To model the noisy feedback a user provides , we adopt a logistic model , shown in Equation 2 , following Gao et al . ( 2019 ) ; Viappiani and Boutilier ( 2010 ) ; Simpson et al . ( 2019 ) . Our feedback model is parameterized by a decision threshold , α ∈ [ 0 , 1 ] , and a noise level , m > 0 . Low α means that users are willing to accept sentences with lower importance . We consider the model to receive a feedback value of 0 if they reject a sentence , and 1 if they accept . In setting α for feedback modelling , 2 https : / / tinder . com / ? lang = en 3 If we wanted to make the feedback optional , we could simply let no swipe indicate acceptance , and left swipe indi - cate rejection . we tie it to the users length preference to better simulate realistic behavior . When users want to read very little for example , they only accept the best sentences . If a user wants to read l out of | D | , then we set α = 1 − l / | D | . For user modelling , we sample l uniformly from the range [ 1 , | D | ] . P α , m ( accept x ) = 1 − (cid:20) 1 + exp (cid:18) α − r x m (cid:19)(cid:21) − 1 ( 2 ) 3 . 3 Unsupervised Evaluation Unsupervised evaluation is tricky to do properly . You must show that it correlates well with human judgement , but also be conﬁdent that maximizing the metric does not result in garbage ( Barratt and Sharma , 2018 ) . As discussed in Section 2 , we adapt the unsu - pervised summary evaluation method described by Gao et al . ( 2020 ) . This metric computes a mover’s - based distance between the SBERT embeddings of the summary and a heuristically - chosen subset of document sentences ( a “pseudo - reference” sum - mary ) . They show that it correlates well will human ratings and that using it as a reward for training a re - inforcement learning - based summarizer produces state - of - the - art models . The authors found that basing the pseudo - reference summary on the lead heuristic , which generally produces good single and multi - document summaries , worked best . For HARE , we can apply the analogous idea : when computing the summary score , we can use all doc - ument sentences in the pseudo - reference summary , but weight them by their importance : score ( U , D , S ) = 1 − 1 (cid:80) x ∈ D r x (cid:88) x ∈ D r x min s ∈ S ∆ ( x , s ) ( 3 ) This metric has the behavior of rewarding cases where an important sentence is highly similar to at least one summary sentence . For this reason , coverage of the different user interests is also en - couraged by this metric : since sentences drawing their importance from similarity to the same con - cept are going to be similar to each other , having summaries representing a variety of important con - cepts is better . 4 Methods We consider three groups of approaches ranging in complexity : ( 1 ) simple heuristics , ( 2 ) adapted generic summarizers , and ( 3 ) preference learning . 4 . 1 Simple Heuristics This ﬁrst set of approaches are as follows : S HOW M ODULO This approach shows every k th sentence to the user . When k = 1 , this is equivalent to the control , where every sentence is shown . By moving through the article faster , we suspect that greater coverage is obtained , making it more likely that important concepts are represented . H IDE N EXT This approach shows all sentences , except for the k following any rejected sentence . E . g . when k = 2 and the user rejects a sentence , the two after it are hidden . The motivation for this model is that nearby sentences are often related , so if one is disliked , a neighbour might also be . Larger k suggests a larger window of relatedness . H IDE A LL S IMILAR While H IDE N EXT hides physically nearby sentences , this model hides all sentences that are actually conceptually similar to a rejected one , where similarity is measure with co - sine similarity of SBERT embeddings . We also in - clude a compromise between hiding based on phys - ical and conceptual similarity : H IDE N EXT S IMI - LAR . This model hides only the unbroken chain of similar sentences after a rejected one . 4 . 2 Adapted Generic Summarizers This set of approaches make use of generic extrac - tive summarizers . The motivation for considering them is that even though they are independent of user interests , they are often designed to provide good coverage of an article . In this way , they may accommodate all user interests to some degree . For a given generic summarizer , we consider the fol - lowing options : G EN F IXED This approach ﬁrst uses the generic summarizer to rank the sentences , and then shows a ﬁxed percentage of the top sentences . G EN D YNAMIC This approach estimates an im - portance threshold , ˆ α , of sentences the user is will - ing to read , and hides the less important sentences . Importance is computed by scoring the sentences with the generic summarizer and rescaling the val - ues to [ 0 , 1 ] . The initial estimate is ˆ α = 0 , which means that all sentences are important enough . Each time a sentence is rejected , the new estimate is updated to be the average importance of all rejected sentences . To help avoid prematurely extreme es - timates , we also incorporate (cid:15) - greedy exploration . With probability 1 − (cid:15) , the sentence is only shown if the importance meets the threshold , otherwise it is shown anyways . A larger (cid:15) will help ﬁnd a closer approximation of the threshold , but at the cost of showing more unimportant sentences . 4 . 3 Preference Learning The approaches in this group use more capable adaptive algorithms to learn user preferences in terms of both preferred length and concepts : LR This approach continually updates a logis - tic regression classiﬁer to predict feedback given sentence embeddings . Before a classiﬁer can be trained , all sentences are shown . We propose two variations of this approach . The ﬁrst uses an (cid:15) - greedy strategy similar to G EN D YNAMIC . The second uses an (cid:15) - decreasing strategy : for a sen - tence at a given fraction , frac , of the way through the article , (cid:15) = ( 1 − frac ) β , for β > 0 . C OVERAGE O PT This approach explicitly mod - els user interests and length preference . It scores potential sentences by how much they improve cov - erage of the user interests . However , since we do not know the user’s true interests or their length preference , both are estimated as they read . This approach prepares for each article by us - ing K - Means clustering of sentence embeddings to identify core concepts of the article . The initial estimate of concept importances is computed with : ˆ C = (cid:20) 1 + exp (cid:18) cfsum β (cid:19)(cid:21) − 1 ( 4 ) We initialize the vector cfsum with the same value c ∈ R for each concept . A larger c means that more evidence is required before a concept is determined to be unimportant . β > 0 controls how smoothly a concept shifts between important and unimportant ( larger value means more smoothly ) . To update the estimate of user interests with feedback ∈ { 0 , 1 } for sentence x , we update cfsum with : cfsum ← cfsum + 2 ( feedback − 0 . 5 ) concepts ( x ) ( 5 ) If feedback = 0 for example , this moves cfsum away from the article concepts represented by that sentence . The function concepts ( ) returns the rele - vance of each concept for the speciﬁed sentence . After updating ˆ C , we re - compute sentence im - portances based on their contribution to improving concept coverage , weighted by concept importance . Next , we update the estimated length preference , ˆ l frac , by averaging the importance of rejected sen - tences . The summary is updated to show sentences among the top ˆ l frac | D | important sentences . If the user has rejected low and medium importance sentences , then only the most coverage - improving sentences will be shown . 5 Experiments In this section , we ﬁrst describe the experimental setup , and then provide an analysis of the results . 5 . 1 Setup Dataset We evaluate on the test articles from the non - anonymized CNN / DailyMail dataset ( Her - mann et al . , 2015 ) 4 . We remove articles with less than 10 sentences so as to cluster sentences into more meaningful groups for user interest modelling . This leaves us with 11222 articles , with an average of 34 . 0 sentences per article . User modelling We apply K - Means clustering to SBERT sentence embeddings for each article to identify k = 4 cluster centers / concepts . User interests are a random weighting over these con - cepts , as described in Section 3 . 2 . For feedback noise , we use m = 0 . 01 ( essentially no noise ) and m = 0 . 1 ( intended to capture the difﬁculty in de - ciding whether a single sentence is of interest or not ) . α is chosen as described in Section 3 . 2 . Metrics Evaluation with the two noise values of m = 0 . 01 and m = 0 . 1 correspond to score sharp and score noisy respectively . score adv corresponds to the difference between score noisy and the con - trol score ( no personalization ) . Positive values in - dicate outperforming the control . Since the scores fall between 0 and 1 , we multiply them by 100 . Privileged information comparison models We consider for comparison three oracle models and the control . O RACLE G REEDY has access to the user preferences and greedily selects sentences to maximize the score , until the length limit is reached . O RACLE S ORT selects sentences based only on their interest level . O RACLE U NIFORM selects sentences at random throughout the article until the length limit is reached 5 . 4 Accessed through HuggingFace : https : / / huggingface . co / datasets / cnn _ dailymail . 5 Readers cannot be guaranteed a uniform sampling of sentences unless their length preference is known in advance . Model score sharp score noisy score adv O RACLE G REEDY 87 . 04 4 . 89 O RACLE S ORTED 82 . 74 0 . 58 O RACLE U NIFORM * 82 . 77 0 . 62 Control ( show all ) 82 . 15 0 . 0 S HOW M ODULO 78 . 83 - 3 . 32 H IDE N EXT 82 . 66 82 . 66 0 . 51 H IDE N EXT S IMILAR 82 . 79 82 . 86 0 . 71 H IDE A LL S IMILAR 83 . 03 83 . 09 0 . 94 G EN F IXED 81 . 97 - 0 . 19 G EN D YNAMIC * 82 . 39 82 . 24 0 . 09 LR ( (cid:15) - greedy ) * 82 . 48 82 . 50 0 . 34 LR ( (cid:15) - decreasing ) * 82 . 28 82 . 31 0 . 15 C OVERAGE O PT 83 . 11 82 . 81 0 . 65 Table 1 : A comparison of each model proposed . For parameterized models , results with the best variation are reported ( for all models , we found that the same parameters performed best for both score sharp and score noisy ) . Non - deterministic models are marked by a * . score adv is the difference between score noisy and the control score ( which is independent of feedback ) . 5 . 2 Results Table 1 reports the results for each model with its best performing set of hyperparameters . While score sharp and score noisy can range from 0 to 100 , the difference between the control and O R - ACLE G REEDY is less that 5 points ( reﬂected in score adv ) . This suggests that even relatively small performance differences are important . For stochas - tic models ( marked by a * in Table 1 ) , results are averaged across 3 trials and standard deviations were all found to be below 0 . 05 . Overall , we ﬁnd that the simple heuristics pro - vide robust performance , unaffected ( and possibly helped ) by noise . While the more complex C OV - ERAGE O PT approach is able to perform best with low - noise feedback , it falls behind when noise in - creases . Next we discuss in more detail the results for each group of models , then comment on aspects of efﬁciency , and ﬁnally discuss the results of our human evaluation . 5 . 2 . 1 Privileged Information Models O RACLE U NIFORM outperforms the control as well as O RACLE S ORTED . This may seem counter - intuitive , since O RACLE U NIFORM has the disad - vantage of not knowing true user interests . How - ever , the strength of O RACLE U NIFORM is that it provides uniform coverage over the whole article , weakly accommodating any interest distribution . By choosing only the most interesting sentences , O RACLE S ORTED runs the risk of only showing those related to the most important concept . If our user model simulated more focused interests , O RACLE S ORTED may perform better however . It is also interesting to see how much higher O R - ACLE G REEDY is than every other model , suggest - ing that there is plenty of room for improvement . The reason the oracle does not reach 100 is that the summary length is restricted by user preference . If future approaches consider abstractive summariza - tion techniques , it may be possible to move beyond this performance barrier . 5 . 2 . 2 Simple Heuristics While we suspected that the S HOW M ODULO strat - egy might beneﬁt from exposing readers to more concepts faster , we found that this does not work as well as O RACLE U NIFORM . The top performance of score adv = − 3 . 32 is reached with k = 2 , and it quickly drops to − 7 . 06 with k = 3 . The mini - mally adaptive approach of hiding a ﬁxed number of sentences after swiped ones , as per H IDE N EXT , does help however , especially with n = 2 . The related models of H IDE N EXT S IMILAR and H IDE A LL S IMILAR , which simply hide sentences similar to ones the user swipes away , work surpris - ingly well , in both moderate and low noise . In Figure 3 , we can see that their performance peaks when the similarity threshold is around 0 . 5 to 0 . 6 . similarity threshold sc o r e _ ad v - 15 - 10 - 5 0 5 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 HideNextSimilar HideAllSimilar Figure 3 : The performance for H IDE N EXT S IMILAR and H IDE A LL S IMILAR for a range of similarity thresh - olds . When the threshold is high , it means that only the most similar sentences are hidden . 5 . 2 . 3 Adapted Generic Summarizers We use the following extractive summarizers : LexRank ( Erkan and Radev , 2004 ) , SumBasic ( Nenkova and Vanderwende , 2005 ) , and TextRank ( Mihalcea and Tarau , 2004 ) 6 . 6 Implementations provided by Sumy library , available at https : / / pypi . python . org / pypi / sumy . LR ( constant (cid:15) ) LR ( decreasing (cid:15) ) (cid:15) score adv β score adv 0 - 7 . 27 0 . 25 0 . 05 0 . 1 - 1 . 58 0 . 5 0 . 09 0 . 2 - 0 . 18 1 0 . 15 0 . 3 0 . 25 2 0 . 07 0 . 4 0 . 34 4 - 0 . 61 0 . 5 0 . 34 Table 2 : Results for the two LR model version . For the constant - (cid:15) variation , a greater (cid:15) indicates greater explo - ration . For the decreasing - (cid:15) variation , larger β indicates a faster decay in exploration probability . We ﬁnd that the generic summarizer - based mod - els always perform worse than the control when showing a ﬁxed fraction of the article ( G EN F IXED ) . The best model of this type used the SumBasic summarizer , showing 75 % of sentences . When dynamically estimating target summary length ( G EN D YNAMIC ) , the control is outperformed by only 0 . 09 points . This is achieved by the SumBasic summarizers with (cid:15) = 0 . 5 . For both variations , we ﬁnd that the best hyperparameters tend to be those that make them show the most sentences . 5 . 2 . 4 Preference - learning Models The LR models out - perform the control , as shown in Table 2 , but fail to match the simpler approaches . Using a decaying (cid:15) actually hurt performance , sug - gesting that the model is simply not able to learn user preferences fast enough . However , there is a sweet spot for the rate of (cid:15) decay at β = 1 . We ﬁnd that C OVERAGE O PT consistently im - proves with larger initial concept weights ( c ) and a slower concept weight - saturation rate ( β ) , with the performance plateauing around β = 4 and c = 5 . When both c and β are both large , there is a longer exploration phase with more evidence required to indicate that any given concept should be hidden . 5 . 3 Efﬁciency Acceptance rate When measuring the fraction of shown sentences that are accepted , we ﬁnd no consistent connection to their performance . For example , the control and the best H IDE N EXT , H I - DE N EXT S IMILAR , H IDE A LL S IMILAR , and C OV - ERAGE O PT models all have rates between 64 - 66 % in the noisy feedback case . O RACLE S ORTED has the highest however , at 79 % , while O RACLE - G REEDY is only at 69 % acceptance . As discussed in Section 5 . 2 . 1 , this is because the sentence set which maximizes the score is not necessarily the same as the set with the highest importance sum . Speed The approaches presented here are able to update the summary in real - time . Running on a consumer - grade laptop , each full user - article simu - lation ( which consists of many interactions ) takes between 100ms for the slowest model ( G EN F IXED with TextRank ) , to 2 . 8ms for H IDE A LL S IMILAR , to 1 . 3ms for H IDE N EXT . 5 . 4 Human Evaluation Finally , we run a human evaluation to test a variety of approaches on multiple measures . Setup We selected 10 news articles from a va - riety of sources and on a variety of topics ( such as politics , sports , and science ) , with an average sentence length of 20 . 6 , and asked 13 volunteers to read articles with the help of randomly assigned HARE models . In total , we collected 70 trials . Par - ticipants were shown sentences one at a time and provided feedback to either accept or reject sen - tences . They were also able to stop reading each article at any time . After reading each article , they were asked several questions about the experience , including the coherence of what they read ( how well - connected consecutive sentences were , from 1 to 5 ) and how easy it was to decide whether to accept or reject sentences ( from 1 to 5 ) . We also showed them any unread sentences afterwards in order to determine how many would - be accepted sentences were not shown . Coverage , roughly cor - responding to our automated evaluation metric , can then be estimated with the fraction of interesting sentences that were actually shown . 40 50 60 70 80 Percent of sentences read 60 65 70 75 80 85 90 95 100 C o v e r a g e Control ShowModulo HideNext HideAllSimilarGenDynamic LR ( - greedy ) CoverageOpt coherence 3 . 8 4 . 0 4 . 1 4 . 4 4 . 5 4 . 8 Figure 4 : Summary of human evaluation results . Error bars indicate 90 % conﬁdence intervals . Results From the human evaluation , we ﬁnd that making the decision to accept or reject sentences is quite easy , with an average decision - ease rating of 4 . 4 / 5 . However , departing from the assumptions of our user model , people ended up reading more than an average of 50 % of the articles ( up to 70 % for the control ) . This could inﬂuence the relative performance of the various models , with a skew towards models that tend to hide fewer sentences . We ﬁnd the acceptance rate to vary from 47 % for LR to 75 % for C OVERAGE O PT , with the remain - der around 60 % . From Figure 4 we can see that the best model ( highest coverage ) appears to be C OVERAGE O PT . This is followed by the control and LR model , with their 90 % conﬁdence intervals overlapping . This highlights that achieving good coverage of interesting sentences is not the same as achieving a high acceptance rate . The worst performing model according to both human and automated evaluation is S HOW M ODULO . The re - maining four models signiﬁcantly overlap in their conﬁdence intervals . However , it is interesting to note that H IDE A LL S IMILAR performs poorer than we would expect . Given the positive correlation be - tween the percent of the article users end up reading and the model coverage , we can guess that this is a result of the model automatically hiding too many sentences . This also leads to low reported summary coherence , as many sentences are skipped . In con - trast , the control achieves the highest coherence ( since nothing is skipped ) , with C OVERAGE O PT near the middle of the pack . 6 Conclusion In this paper we proposed a new interactive sum - marization task where the document is automat - ically reﬁned during the normal ﬂow of reading . By not requiring an explicit query or relying on time - consuming and invasive feedback , relevant information can be conveniently provided for a wide range of user preferences . We provided an approximate user model and suitable evaluation metric for this task , building upon recent advances in unsupervised summary evaluation . To guide ex - amination of this new task , we proposed a variety of approaches , and perform both automated and human evaluation . Future research on this task in - cludes adapting the interaction model to implicit feedback and trying more advanced approaches . 7 Ethical Considerations Diversity of viewpoints The HARE task is in - tended for the design of future user - facing applica - tions . By design , these applications have the ability to control what a user reads from a given article . It is possible that , when deployed without sufﬁ - cient care , these tools could exacerbate the “echo chamber” effect already produced by automated news feeds , search results , and online communities ( Pariser , 2011 ) . However , the ability to inﬂuence what readers are exposed to can also be leveraged to mitigate the echo chamber effect . Rather than considering only what user interests appear to be at a given moment , future HARE models could in - corporate a diversity factor to explicitly encourage exposure to alternative views when possible . The weighting of this factor could be tuned to provide both an engaging reading experience and exposure to a diversity of ideas . Beneﬁciaries As mentioned in Section 1 , those most likely to beneﬁt from HARE applications once successfully deployed will be those using them to read ( by saving time and increased en - gagement ) as well as any content publishers who encourage their use . References PVS Avinesh , Carsten Binnig , Benjamin H ¨ attasch , Christian M Meyer , and Orkan ¨Ozyurt . 2018 . Sher - lock : A system for interactive summarization of large text collections . Proc . VLDB Endow . , 11 ( 12 ) : 1902 – 1905 . PVS Avinesh and Christian M Meyer . 2017 . Joint opti - mization of user - desired content in multi - document summaries by learning from user feedback . In Pro - ceedings of the 55th Annual Meeting of the Associa - tion for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1353 – 1363 . Shane Barratt and Rishi Sharma . 2018 . A note on the inception score . arXiv preprint arXiv : 1801 . 01973 . Florian B¨ohm , Yang Gao , Christian M Meyer , Ori Shapira , Ido Dagan , and Iryna Gurevych . 2019 . Better rewards yield better summaries : Learning to summarise without references . arXiv preprint arXiv : 1909 . 01214 . Bobby J Calder , Edward C Malthouse , and Ute Schaedel . 2009 . An experimental study of the re - lationship between online engagement and advertis - ing effectiveness . Journal of interactive marketing , 23 ( 4 ) : 321 – 331 . John M Conroy , Judith D Schlesinger , and Jade Gold - stein Stewart . 2005 . Classy query - based multi - document summarization . In Proceedings of the 2005 Document Understanding Workshop , Boston . Citeseer . Mariana Damova and Ivan Koychev . 2010 . Query - based summarization : A survey . Hoa Trang Dang . 2005 . Overview of duc 2005 . In Pro - ceedings of the document understanding conference , volume 2005 , pages 1 – 12 . Gaby David and Carolina Cambre . 2016 . Screened in - timacies : Tinder and the swipe logic . Social media + society , 2 ( 2 ) : 2056305116641976 . G¨unes Erkan and Dragomir R Radev . 2004 . Lexrank : Graph - based lexical centrality as salience in text summarization . Journal of Artiﬁcial Intelligence Re - search , 22 : 457 – 479 . Alexander R . Fabbri , Irene Li , Tianwei She , Suyi Li , and Dragomir R . Radev . 2019 . Multi - news : a large - scale multi - document summarization dataset and ab - stractive hierarchical model . Yang Gao , Christian M Meyer , and Iryna Gurevych . 2019 . Preference - based interactive multi - document summarisation . Information Retrieval Journal , pages 1 – 31 . Yang Gao , Wei Zhao , and Steffen Eger . 2020 . Supert : Towards new frontiers in unsupervised evaluation metrics for multi - document summarization . arXiv preprint arXiv : 2005 . 03724 . Johan Hasselqvist , Niklas Helmertz , and Mikael K ˚ ageb ¨ ack . 2017 . Query - based abstractive summa - rization using neural networks . Karl Moritz Hermann , Tomas Kocisky , Edward Grefen - stette , Lasse Espeholt , Will Kay , Mustafa Suleyman , and Phil Blunsom . 2015 . Teaching machines to read and comprehend . In Advances in Neural Informa - tion Processing Systems , pages 1693 – 1701 . TD Hoa . 2006 . Overview of duc 2006 . In Document Understanding Conference . Sampath Jayarathna and Frank Shipman . 2017 . Anal - ysis and modeling of uniﬁed user interest . In 2017 IEEE International Conference on Information Reuse and Integration ( IRI ) , pages 298 – 307 . IEEE . Matt Kusner , Yu Sun , Nicholas Kolkin , and Kilian Weinberger . 2015 . From word embeddings to doc - ument distances . In International conference on ma - chine learning , pages 957 – 966 . Chin - Yew Lin and Franz Josef Och . 2004 . Auto - matic evaluation of machine translation quality us - ing longest common subsequence and skip - bigram statistics . In Proceedings of the 42nd Annual Meet - ing of the Association for Computational Linguistics ( ACL - 04 ) , pages 605 – 612 . Annie Louis and Ani Nenkova . 2009 . Automatically evaluating content selection in summarization with - out human models . In Proceedings of the 2009 Con - ference on Empirical Methods in Natural Language Processing , pages 306 – 314 . Annie Louis and Ani Nenkova . 2013 . Automatically assessing machine summary content without a gold standard . Computational Linguistics , 39 ( 2 ) : 267 – 300 . Rada Mihalcea and Paul Tarau . 2004 . Textrank : Bring - ing order into text . In Proceedings of the 2004 Con - ference on Empirical Methods in Natural Language Processing , pages 404 – 411 . Ahmed A Mohamed and Sanguthevar Rajasekaran . 2006 . Improving query - based summarization using document graphs . In 2006 IEEE international sym - posium on signal processing and information tech - nology , pages 408 – 410 . IEEE . Shashi Narayan , Shay B . Cohen , and Mirella Lapata . 2018 . Don’t give me the details , just the summary ! topic - aware convolutional neural networks for ex - treme summarization . ArXiv , abs / 1808 . 08745 . Ani Nenkova and Kathleen McKeown . 2011 . Auto - matic summarization . Now Publishers Inc . Ani Nenkova and Lucy Vanderwende . 2005 . The im - pact of frequency on summarization . Microsoft Re - search , Redmond , Washington , Tech . Rep . MSR - TR - 2005 , 101 . Eli Pariser . 2011 . The ﬁlter bubble : What the Internet is hiding from you . Penguin UK . Matthew E Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer . 2018 . Deep contextualized word repre - sentations . arXiv preprint arXiv : 1802 . 05365 . Nils Reimers and Iryna Gurevych . 2019 . Sentence - bert : Sentence embeddings using siamese bert - networks . arXiv preprint arXiv : 1908 . 10084 . Edwin Simpson , Yang Gao , and Iryna Gurevych . 2019 . Interactive text ranking with bayesian optimisation : A case study on community qa and summarisation . arXiv preprint arXiv : 1911 . 10183 . Nisan Stiennon , Long Ouyang , Jeff Wu , Daniel M Ziegler , Ryan Lowe , Chelsea Voss , Alec Radford , Dario Amodei , and Paul Christiano . 2020 . Learning to summarize from human feedback . arXiv preprint arXiv : 2009 . 01325 . Simeng Sun and Ani Nenkova . 2019 . The feasibility of embedding based automatic evaluation for sin - gle document summarization . In Proceedings of the 2019 Conference on Empirical Methods in Nat - ural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 1216 – 1221 . Jaime Teevan , Susan T Dumais , and Eric Horvitz . 2005 . Personalizing search via automated analysis of in - terests and activities . In Proceedings of the 28th annual international ACM SIGIR conference on Re - search and development in information retrieval , pages 449 – 456 . Hadrien Van Lierde and Tommy WS Chow . 2019 . Query - oriented text summarization based on hyper - graph transversals . Information Processing & Man - agement , 56 ( 4 ) : 1317 – 1338 . Paolo Viappiani and Craig Boutilier . 2010 . Optimal bayesian recommendation sets and myopically opti - mal choice query sets . In Advances in neural infor - mation processing systems , pages 2352 – 2360 . Chuhan Wu , Fangzhao Wu , Mingxiao An , Tao Qi , Jian - qiang Huang , Yongfeng Huang , and Xing Xie . 2019 . Neural news recommendation with heterogeneous user behavior . In Proceedings of the 2019 Confer - ence on Empirical Methods in Natural Language Processing and the 9th International Joint Confer - ence on Natural Language Processing ( EMNLP - IJCNLP ) , pages 4876 – 4885 . Yumo Xu and Mirella Lapata . 2020 . Query focused multi - document summarization with distant supervi - sion . arXiv preprint arXiv : 2004 . 03027 . Rui Yan , Jian - Yun Nie , and Xiaoming Li . 2011 . Sum - marize what you are interested in : An optimization framework for interactive personalized summariza - tion . In Proceedings of the 2011 conference on empirical methods in natural language processing , pages 1342 – 1351 . A Experimental Setup Computing infrastructure All experiments were performed on a machine with an Intel Core i7 - 6700HQ CPU with 16G RAM and a GeForce GTX 960M GPU . Hyperparameter searches For parameterized models , grid searches over the following ranges were performed : • S HOW M ODULO : k ∈ { 2 , 3 , 4 , 5 } • H IDE N EXT : n ∈ { 1 , 2 , 3 , 4 } • H IDE N EXT S IMILAR and H IDE - A LL S IMILAR : threshold ∈ { 0 . 1 , 0 . 2 , 0 . 3 , 0 . 4 , 0 . 5 , 0 . 6 , 0 . 7 , 0 . 8 , 0 . 9 } • G EN F IXED : frac ∈ { 0 . 25 , 0 . 5 , 0 . 75 } • G EN D YNAMIC : (cid:15) ∈ { 0 , 0 . 1 , 0 . 2 , 0 . 3 , 0 . 4 , 0 . 5 } • LR ( constant (cid:15) ) : (cid:15) ∈ { 0 , 0 . 1 , 0 . 2 , 0 . 3 , 0 . 4 , 0 . 5 } S HOW M ODULO H IDE N EXT k score adv n score adv 2 - 3 . 32 1 0 . 45 3 - 7 . 06 2 0 . 51 4 - 9 . 87 3 0 . 19 5 - 12 . 00 4 - 0 . 41 Table B . 1 : Results for the ﬁrst two simple heuristic models . For S HOW M ODULO , every k th sentence is shown . For H IDE N EXT , the n sentences following a swiped one are hidden . frac ( for G EN F IXED ) summarizer 0 . 25 0 . 5 0 . 75 LexRank - 11 . 18 - 3 . 77 - 0 . 79 SumBasic - 10 . 75 - 3 . 22 - 0 . 19 TextRank - 12 . 28 - 4 . 99 - 1 . 53 (cid:15) ( for G EN D YNAMIC ) summarizer 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 LexRank - 1 . 37 - 0 . 53 - 0 . 22 - 0 . 07 0 . 01 0 . 06 SumBasic - 3 . 19 - 1 . 47 - 0 . 72 - 0 . 28 - 0 . 05 0 . 09 TextRank - 1 . 95 - 1 . 02 - 0 . 59 - 0 . 31 - 0 . 18 - 0 . 08 Table B . 2 : Results for the two variations of adapted generic summarizer models , for each of three extractive summarizers tested . For G EN F IXED , frac indicates what fraction of the document is shown , after ﬁrst sort - ing sentences by importance . For G EN D YNAMIC , (cid:15) is used for (cid:15) - greedy exploration to estimate length prefer - ence . • LR ( decreasing (cid:15) ) : β ∈ { 0 . 25 , 0 . 5 , 1 , 2 , 4 } • C OVERAGE O PT : β ∈ { 0 . 25 , 0 . 5 , 1 , 2 , 4 } and c ∈ { 0 , 1 , 2 , 3 , 4 } B Detailed Results Detailed results for those models without full results reported in the paper are shown here . For S HOW M ODULO and H IDE N EXT , results are shown in Table B . 1 . For summarizer - based mod - els , results are shown in Table B . 2 . For C OVERA - GE O PT , results are shown in Table B . 3 . C Human Evaluation Human evaluation was performed via a chatbot de - ployed on the Telegram chat app 7 using their con - venient API 8 . A screenshot of the chatbot serving as a simple HARE interface is shown in Figure 5 . 7 https : / / telegram . org / 8 https : / / core . telegram . org / bots / api β c 1 / 4 1 / 2 1 2 4 0 0 . 12 0 . 22 0 . 33 0 . 42 0 . 50 1 0 . 51 0 . 50 0 . 51 0 . 52 0 . 55 2 0 . 49 0 . 57 0 . 60 0 . 59 0 . 59 3 0 . 50 0 . 53 0 . 61 0 . 63 0 . 63 4 0 . 49 0 . 50 0 . 59 0 . 64 0 . 64 5 0 . 49 0 . 50 0 . 55 0 . 64 0 . 65 Table B . 3 : Results for the C OVERAGE O PT model . c controls the initial estimate for concept importances and β controls how smoothly a concept shifts between important and unimportant . To participate , volunteers were instructed to en - gage with the publicly accessible bot in the app and follow instructions provided therein . Figure 5 : A screenshot of the demo in action . For each sentence , users were able to accept , reject , or stop read - ing the article at that point .