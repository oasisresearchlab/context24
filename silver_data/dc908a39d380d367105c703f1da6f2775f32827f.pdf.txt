a r X i v : 2211 . 10234v4 [ m a t h . O C ] 13 D ec 2022 Iteration Complexity of Fixed - Step Methods by Nesterov and Polyak for Convex Quadratic Functions Melinda Hagedorn , Heinrich Heine Univ . , D¨usseldorf , Germany , melinda . hagedorn @ hhu . de Florian Jarre , Heinrich Heine Univ . , D¨usseldorf , Germany , jarre @ hhu . de Dec . 13 , 2022 , Dedicated to Kees Roos and Florian Potra All data generated or analysed in this article are available in [ 17 ] Abstract This note considers the momentum method by Polyak and the accelerated gradient method by Nesterov , both without line search but with ﬁxed step length applied to strictly convex quadratic functions assuming that exact gradients are used and ap - propriate upper and lower bounds for the extreme eigenvalues of the Hessian matrix are known . Simple 2 - d - examples show that the Euclidean distance of the iterates to the optimal solution is non - monotone . In this context an explicit bound is derived on the number of iterations needed to guarantee a reduction of the Euclidean distance to the optimal solution by a factor ǫ . For both methods the bound is optimal up to a constant factor , it complements earlier asymptotically optimal results for the momen - tum method , and it establishes another link of the momentum method and Nesterov’s accelerated gradient method . Key words : Momentum method , convex quadratic optimization , iteration complexity 1 . Introduction Let f : R n → R be a strictly convex quadratic function . For the minimization of f starting at an initial point x 0 ∈ R n with negative gradient m 0 : = −∇ f ( x 0 ) the following “Momentum Method” is considered for k ≥ 0 : ( MM ) x k + 1 = x k + αm k , m k + 1 = βm k − ∇ f ( x k + 1 ) . Here α > 0 is a suﬃciently small step length that will be analyzed below and β ∈ [ 0 , 1 ) is a parameter that determines how much of the previous search direction will be added to the new search direction . This parameter is also discussed below . 1 As pointed out for example in [ 16 ] , with the initialization x − 1 : = x 0 the Momentum Method ( MM ) is equivalent to the “Heavy Ball Method” ( [ 15 ] ) : ( HBM ) x k + 1 = x k − α ∇ f ( x k ) + β ( x k − x k − 1 ) . In spite of the proof of asymptotic optimality of the momentum method ( MM ) estab - lished almost 60 years ago in [ 15 ] this method has not received due attention until recently in the context of machine learning and neural networks . It has proved to be very eﬃcient in nu - merical implementations , also when the gradients are replaced with approximate stochastic gradients , see e . g . [ 2 , 16 ] . Today modiﬁcations of the ( MM ) are widely used , in particular also in machine learning libraries . While being eﬃcient in practice , the convergence behavior of momentum methods is “somewhat irregular” . Figure 1 shows a typical non - monotone convergence behavior of the ( MM ) for the simple 2 - d - example f ( x ) ≡ 1 2 ( x 2 1 + 100 x 2 2 ) with β = 0 . 85 and α = 1 . 9 100 during the ﬁrst 100 iterations for the initial values x 0 = ( 1100 , 1 ) T , x 0 = ( 1 , 1 ) T , and x 0 = ( 100 , 1 ) T . Figure 1 : Convergence of the ( MM ) for a 2 - d - example with β = 0 . 85 , and α = 1 . 9 . 0 50 100 10 - 5 10 - 4 10 - 3 10 - 2 10 - 1 10 0 0 50 100 10 - 5 10 - 4 10 - 3 10 - 2 10 - 1 10 0 0 50 100 10 - 3 10 - 2 10 - 1 10 0 10 1 10 2 For convex quadratic functions the iterative process ( MM ) or ( HBM ) can be written as a recursion with a ﬁxed matrix M , and the spectral radius of M is the main factor determining the convergence behavior . A secondary factor is the approximation of the spectral radius by a matrix norm . The connection of both factors and the implication on the choice of parameters is addressed in this paper . Closely related to the ( MM ) is Nesterov’s accelerated gradient method [ 13 ] . Following the presentation in [ 14 ] ( Theorem 2 . 2 . 3 ) , it can be written as follows : Given x 0 ∈ R n set y 0 : = x 0 and for k ≥ 0 : ( NAG ) x k + 1 : = y k + 1 + β ( y k + 1 − y k ) where y k + 1 : = x k − α ∇ f ( x k ) for suitable parameters α , β > 0 . As observed in [ 18 ] , for example , this method falls in the same general framework as ( MM ) : Indeed , by eliminating the variable y k and setting 2 x − 1 : = x 0 , the iteration ( NAG ) can also be written in the following compact form , x k + 1 : = x k − α ∇ f ( x k ) + β (cid:0) x k − x k − 1 − α ( ∇ f ( x k ) − ∇ f ( x k − 1 ) ) (cid:1) , ( 1 ) where in contrast to ( HBM ) the momentum term not only includes the previous iterates x k but also the previous descent steps “ − α ∇ f ( x k ) ” . ( To obtain the identical initilization as in ( NAG ) , in the very ﬁrst step ∇ f ( x − 1 ) needs to be replaced with 0 . ) 1 . 1 Main Results The main theoretical result of this paper is summarized in the following theorem . Theorem 1 Let f : R n → R be a strictly convex quadratic function with minimizer x ∗ and denote H : = ∇ 2 f ( x ∗ ) . Let m be an upper bound for the eigenvalues of H and let 0 < m be a lower bound for the eigenvalues of H . Then cond ( H ) : = m / m is an upper bound for the condition number of H . Assume that cond ( H ) ≥ 28 . In ( HBM ) deﬁne α = 2 / m , β = (cid:16) 1 − p 2 / cond ( H ) (cid:17) 2 and let ǫ ≤ 1 / cond ( H ) be given . Then , given x 0 ∈ R n , after 1 + ⌈ p 2 cond ( H ) ln ( 2 ǫ ) ⌉ steps of the ( HBM ) , an approximate solution ¯ x k : = 12 ( x k − 1 + x k ) is generated with k ¯ x k − x ∗ k 2 ≤ ǫ k x 0 − x ∗ k 2 . A corollary of the analysis of the ( MM ) is the following well known observation ( see [ 8 ] , for example ) : It may seem intuitive that with the use of a momentum β > 0 , the step length α in ( MM ) needs to be chosen more carefully . However , at least for the minimization of convex quadratic functions and in the absence of noise , the opposite is true : “If the step itself is made longer by adding more of the previous search step to the new step ( i . e . by increasing β ∈ [ 0 , 1 ) ) then also the step length can be made longer while maintaining global convergence . ” When m and m are known , then – as detailed in Section 2 . 6 – the analysis for ( MM ) also applies to ( NAG ) , and for an appropriate choice of α , β , an iteration complexity can be established for ( NAG ) that is a factor √ 2 higher than the bound for ( MM ) in Theorem 1 . The factor √ 2 arises since the step length is reduced by a factor 2 – which essentially amounts to an increase of the estimate cond ( H ) by a factor 2 . 3 1 . 2 Related work The paper [ 15 ] establishes for ( HBM ) applied to ( non - quadratic smooth ) strictly convex functions local convergence of the form k x − x ∗ k 2 ≤ c ( ǫ ) ( 1 − 2 1 + √ cond ( H ) + ǫ ) k q k x 1 − x ∗ k 22 + k x 0 − x ∗ k 22 where c ( ǫ ) is a constant that only depends on ǫ ( Theorem 9 , Statement ( 3 ) ) . This result is obtained by analyzing the spectral condition of a recursion associated with ( HBM ) and using the fact that for a given matrix there always is a vector norm and an associated matrix norm that approximates the spectral radius to a given precision ǫ > 0 . For the case of convex quadratic functions the constant c ( ǫ ) is worked out explicitly in a slightly diﬀerent setting in [ 8 ] . The step length in Theorem 1 is limited such that the ﬁrst step is guaranteed not to increase the function value . For larger values of cond ( H ) the optimal step length α in [ 15 , 8 ] is nearly twice as long and leads to a substantial increase of the error components associated with large eigenvalues of H during the ﬁrst several iterations . ( This increase can be reduced by taking at most half the step length of [ 15 , 8 ] in the ﬁrst iteration , a strategy that also applies to methods with restart . ) As the generalization to non - quadratic functions also calls for a shorter step length ( [ 7 ] , Footnote 3 ) and since the use of long steps leads to an ampliﬁcation of the stochastic error associated with an inexact evaluation of the gradient , subsequently , only step lengths at most 2 / m are considered in this paper . This imposes a restriction on α cutting oﬀ the optimal step lengths identiﬁed in [ 15 , 8 ] at the expense of a small constant factor in the theoretical eﬃciency of the method . Further studies of ( MM ) for smooth strongly convex optimization where the strong convexity parameter is to be estimated are presented in [ 1 , 5 ] . Momentum methods and modiﬁcations thereof are in the focus of numerous further recent research projects , in particular also for the non - convex case – which is much more diﬃcult to analyze than the convex quadratic case considered in this paper . The articles quoted below form a rather brief and thus incomplete glimpse on recent lines of research on momentum methods . An analysis comparing the ( MM ) for the choice β = 0 ( steepest descent ) and β > 0 in the non - convex case and in the presence of noise is presented , for example , in [ 2 ] . This paper also gives a theoretical explanation for the observed eﬃciency of ( MM ) . The ( MM ) is closely related to ADAM or ADAGRAD for which a recent analysis covering the non - convex case is given in [ 3 ] . In particular , a new tight dependency on a certain “heavy ball momen - tum decay rate” ( which is zero in the context considered in this paper ) is established in [ 3 ] . Another recent approach that also considers the non - convex situation without line search is analyzed in [ 9 , 10 , 11 ] . In this situation convergence to a second - order minimizer – along with estimates on the rate of convergence – is established under mild conditions . An analysis generalizing ( HBM ) and Nesterov’s method to a broad class of momentum methods and giving a uniﬁed convergence analysis is given in [ 4 ] . A uniﬁed analysis of stochastic momentum methods in the weakly convex case – and in 4 the non - convex case – is given in [ 18 ] , and a further uniﬁed analysis that considers “Quasi - Hyperbolic Momentum Methods”can be found in [ 7 ] . Limitations that arise when general - izing ( MM ) to stochastic optimization are addressed in the recent paper [ 6 ] . 2 . Analysis of ( HBM ) The analysis of ( HBM ) and thus also of ( MM ) is carried out with the following steps : 1 . Section 2 . 1 follows the approach in [ 15 , 5 ] and derives a linear recursion for the iterates x k and analyzes the spectral radius of the underlying system matrix . 2 . Based on this analysis suitable parameters α , β for ( HBM ) are identiﬁed in Section 2 . 2 . 3 . Then the condition number of the similarity transformations to diagonalize the system matrix is analyzed . 4 . It is shown that for the above parameters α , β this condition number is unbounded in general . 5 . A transformation to Schur canonical form is analyzed “instead” . 6 . Based on this transformation the main theoretical result is proved . The derivations in Steps 3 ) to Step 5 ) appear to be new ; the results of these steps , however , are essentially the same as in [ 8 ] while the bound on the number of iterations derived in Step 6 ) may not have been derived explicitly before . As the ( HBM ) of Section 1 . is invariant with respect to a shift of the variable and with respect to orthogonal transformations , for the analysis it is assumed without loss of generality that f ( x ) ≡ 12 x T Dx ( 2 ) with a positive deﬁnite diagonal matrix D . ( The eigenvalue decomposition of the Hessian of f that leads to the simple reformulation ( 2 ) is used only for the analysis of the algorithm , but not for the algorithm itself . ) In this case , the plain steepest descent method with β = 0 ( no momentum ) converges if , and only if , α ∈ ( 0 , 2 / max 1 ≤ i ≤ n { D i , i } ) . The following slightly weaker assumption will be made : Assumption 1 It is assumed throughout that f is given by ( 2 ) and β ∈ [ 0 , 1 ) and α ∈ ( 0 , ¯ α ] where ¯ α : = 2 / max 1 ≤ i ≤ n { D i , i } The ( HBM ) can then be written with the following recursion (cid:18) x k x k + 1 (cid:19) = (cid:18) 0 I − βI ( 1 + β ) I − αD (cid:19) (cid:18) x k − 1 x k (cid:19) . ( 3 ) 5 This is a discrete linear dynamical system ˆ z k + 1 = ˆ M ˆ z k with the variable ˆ z k : = (cid:18) x k − 1 x k (cid:19) and ˆ M : = (cid:18) 0 I − βI ( 1 + β ) I − αD (cid:19) . Recursion ( 3 ) is block - separable , i . e . for i 6 = j the variables x ki , x k + 1 i do not depend on x ℓj for any ℓ ≤ k + 1 . Thus , when setting z k ( i ) : = (cid:18) x k − 1 i x ki (cid:19) for k ≥ 0 and 1 ≤ i ≤ n , then Recursion ( 3 ) can be written as z k + 1 ( i ) = M ( i ) z k ( i ) for 1 ≤ i ≤ n with M ( i ) : = (cid:18) 0 1 − β 1 + β − αD i , i (cid:19) . ( 4 ) ( This means that rows and columns of ˆ M can be permuted so that a block - diagonal matrix M is obtained with 2 × 2 diagonal blocks M ( i ) on the diagonal . ) 2 . 1 The spectral radius of M ( i ) in dependence of α and β Possible convergence of the iterates ˆ z k depends on the norm k ( M ( i ) ) k k 2 for large k , i . e . on the spectral radius of ˆ M which coincides with the maximum spectral radius ρ ( M ( i ) ) of M ( i ) for all i . In the following the index i is kept ﬁxed . To simplify the notation , let α i : = αD i , i ∈ ( 0 , 2 ] , β i : = 1 + β − α i ∈ [ − 1 , 2 ) , and γ i : = q β 2 i − 4 β . Here , γ i is either a non - negative real number or a ( purely imaginary ) number with positive imaginary part . In both cases , γ 2 i = β 2 i − 4 β will be used below . With these abbreviations M ( i ) is given by M ( i ) = (cid:18) 0 1 − β β i (cid:19) . Let further λ + : = 12 ( β i + γ i ) , λ − : = 12 ( β i − γ i ) , v + : = (cid:18) 1 λ + (cid:19) , and v − : = (cid:18) 1 λ − (cid:19) . ( 5 ) Observing that λ 2 ± = 14 β 2 i ± 12 β i γ i + 14 γ 2 i = 14 β 2 i ± 12 β i γ i + 14 β 2 i − β = − β + 12 β 2 i ± 12 β i γ i = − β + β i λ ± it follows that M ( i ) v ± = λ ± v ± . Thus , the – possibly complex – eigenvalues of M ( i ) are given by λ + and λ − and the spectral radius ρ ( M ( i ) ) of M ( i ) is given by the larger one of the two values | λ ± | = 12 | β i ± γ i | = 1 2 (cid:12) (cid:12) (cid:12) (cid:12) β i ± q β 2 i − 4 β (cid:12) (cid:12) (cid:12) (cid:12) . 6 If the square root is purely imaginary , i . e . , if β 2 i < 4 β then the square of the absolute value is given by the sum of the squares of real and imaginary part , i . e . ρ ( M ( i ) ) 2 = 1 4 (cid:0) β 2 i + | γ i | 2 ) (cid:1) = 1 4 (cid:0) β 2 i + ( 4 β − β 2 i ) (cid:1) = β . Thus , the expression for ρ ( M ( i ) ) simpliﬁes to ρ : = ρ ( M ( i ) ) = (cid:26) √ β if β 2 i < 4 β 12 ( | β i | + γ i ) else . ( 6 ) Note that there is a double eigenvalue λ + = λ − if 0 = γ i = β 2 i − 4 β = ( 1 + β − αD i , i ) 2 − 4 β with β ∈ [ 0 , 1 ) , i . e . if , and only if , β = ( 1 − p αD i , i ) 2 where the deﬁnition of ¯ α in Assumption 2 implies that αD i , i ∈ ( 0 , 2 ] for all i . ( This is sketched on the right of Figure 5 below . ) Figure 2 : Spectral radius of M ( i ) as a function of αD i , i ∈ ( 0 , 2 ] and β ∈ [ 0 , 1 ) . In Figure 2 , the value of αD i , i ∈ ( 0 , 2 ] is plotted from the middle to the right and the value of β from the middle to the rear - left . The associated values of ρ are plotted on the vertical axis . It is assumed that an upper bound m for the eigenvalues D i , i is known and that α is chosen less than or equal to 2 / m so that the possible values of αD i , i vary over a ( typically 7 wide ) range in the interval ( 0 , 2 ] and depend on the distribution of the eigenvalues . This range is problem dependent and not subject to the design of the method . A fast convergence of the HBM ( or of the momentum method MM ) is obtained if the value of β is chosen such that ρ is small for a wide range of values αD i , i . Figure 2 gives the impression that this is achieved when choosing β = 0 . And indeed , when the values of αD i , i all cluster about the value “1” , then the problem of minimizing f not only is quite easy ( since the condition number of D is close to 1 ) but also choosing β = 0 is nearly optimal . However , when the condition number of D is poor , then very small values of αD i , i > 0 will occur . In this case , Figure 2 is not suitable for understanding the best possible choice of β . Instead , Figure 3 displays the plots of ρ ( M ( i ) ) as a function of αD i , i ∈ ( 0 , 2 ] for β ≡ 0 . 9 in red , β ≡ 0 . 5 in green , and β ≡ 0 . 1 in blue . Figure 3 : ρ ( M ( i ) ) as a function of αD i , i ∈ ( 0 , 2 ] for ﬁxed values of β . 0 0 . 5 1 1 . 5 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 = 0 . 1 = 0 . 5 = 0 . 9 In Figure 3 the choice β = 0 . 1 in blue leads to small values of ρ ( M ( i ) ) when αD i , i ∈ [ 0 . 5 , 1 . 5 ] ( and a little beyond this interval ) . For values αD i , i ≈ 0 . 2 , the choice β = 0 . 5 in green results in a much smaller value of ρ ( M ( i ) ) than β = 0 . 1 , and as displayed in Figure 4 , when considering poorly conditioned problems with some values of αD i , i ≈ 0 . 004 the choice of β = 0 . 9 in red results in a smaller value of ρ ( M ( i ) ) than β = 0 . 5 or β = 0 . 1 . ( Figure 4 is a zoom of Figure 3 ; of course , small values of αD i , i occurring in ill - conditioned problems result in an increase of ρ ( M ( i ) ) but for small αD i , i > 0 the increase is much less for larger values of β < 1 . ) 8 An interesting observation that can be deduced from Figure 3 is the following : Recall that a step length αD i , i > 2 is “too long” in the sense that it results in a divergent algorithm for the plain steepest descent method . For larger values of β ≥ 0 . 5 , however , the green line and the red line seem to ( and do ) continue “for a little while” with constant values to the right of αD i , i = 2 which implies that for larger values of β a step length that is a little bit “too long” still results in a convergent algorithm . In contrast to the intuition that with the use of momentum the step length needs to be chosen more carefully , actually the opposite is true . Figure 4 : ρ ( M ( i ) ) as a function of αD i , i ∈ ( 0 , 0 . 1 ) for ﬁxed values of β . 0 0 . 02 0 . 04 0 . 06 0 . 08 0 . 1 0 . 7 0 . 75 0 . 8 0 . 85 0 . 9 0 . 95 1 = 0 . 1 = 0 . 5 = 0 . 9 2 . 2 Selecting α and β In the following it is assumed that a lower bound 0 < m ≤ min 1 ≤ i ≤ n D i , i and an upper bound m ≥ max 1 ≤ i ≤ n D i , i are known . Given m , the step length α is ﬁxed to α : = 2 / m . ( 7 ) In the following let cond ( D ) : = m / m ≥ cond ( D ) 9 be an upper bound for cond ( D ) . Often , an upper bound for m can be obtained , for example by Gershgorin’s theorem , while it may be diﬃcult to determine a positive lower bound for m . ( The numerical eﬀects of overestimating or underestimating the condition number of H are analyzed in [ 5 ] along with an approach to adaptively adjust the estimate of the condition number , provided that function evaluations are available – an assumption that unfortunately is not satisﬁed in many stochastic settings . ) To simplify the discussion below , it is further assumed that cond ( D ) > 2 . ( 8 ) ( In Theorem 1 a stronger restriction on cond ( D ) is made ; for now ( 8 ) is suﬃcient . For well - conditioned problems with cond ( D ) ≤ 2 the plain steepest descent method with β = 0 is optimal up to a factor of at most 2 . ) When β < 3 − 2 √ 2 ≈ 0 . 1716 then there are two values of αD i , i for which ( 1 + β − αD i , i ) 2 = 4 β ( corresponding , for example , to the end points of the blue horizontal line in Figure 3 ) . The considerations below apply to values β ≥ 3 − 2 √ 2 which have proved to be eﬃcient in numerical experiments , see , for example , [ 16 ] , and where only the left end point of the interval with constant values ρ ( i . e . of the horizontal lines in Figure 3 ) is relevant . Let α = αm be a lower bound for possible values of αD i , i . Assumption ( 8 ) implies α < 1 . Setting β : = ( 1 − √ α ) 2 , the spectral radius of all M ( i ) ( and thus , ρ ( M ) ) ) is upper bounded by √ β = 1 − √ α , i . e . ρ ( M ) ≤ p β = 1 − √ α = 1 − √ 2 p cond ( D ) . When cond ( D ) approximates cond ( D ) up to a constant factor ( this is what is meant with “appropriate” bound in the abstract of this paper ) then the resulting order 1 − 1 O ( √ cond ( D ) ) is the same order as the optimal rate of convergence of the conjugate gradient method , see e . g . [ 12 ] . Moreover , the 2 × 2 transformation matrices transforming M ( i ) to diagonal form or , more generally , to the Jordan canonical form are independent of the dimension , so that the 2 - norm of the transformation matrices that transform ˆ M to a canonical form also is independent of n . However , the optimal bound for the conjugate gradient iterations applies to the D - norm , k z k D = √ z T Dz , while the above rate applies to a transformed problem , and the transfor - mation matrix may be very ill - conditioned . This is studied further in Section 2 . 3 below . 2 . 3 On the norm of the transformation matrices Consider the case in ( 5 ) that γ i = p β 2 i − 4 β is nonzero . Then v + 6 = v − and M ( i ) is diagonal - izable , i . e . M ( i ) = S Λ S − 1 where Λ is the diagonal matrix with the eigenvalues λ ± = 12 ( β i ± γ i ) of ( 5 ) and S : = S ( i ) : = (cid:18) 1 1 λ + λ − (cid:19) with columns v ± as deﬁned in ( 5 ) . 10 There are two cases : Either γ i > 0 is real or γ i is purely imaginary . If γ i > 0 is real then λ + λ − = 14 ( β 2 i − γ 2 i ) = 14 ( β 2 i − ( β 2 i − 4 β ) ) = β and S T S = (cid:18) 1 + 14 ( β i + γ i ) 2 1 + β 1 + β 1 + 14 ( β i − γ i ) 2 (cid:19) . Denote the eigenvalues of S T S by µ + ≥ µ − > 0 . Then µ ± = 1 + 14 ( β 2 i + γ 2 i ) ± 12 q β 2 i γ 2 i + 4 ( 1 + β ) 2 . ( 9 ) If γ i is purely imaginary and S H denotes the complex conjugate transpose of S , then S H S = 1 + | λ + | 2 1 + ¯ λ + λ − 1 + λ + ¯ λ − 1 + | λ − | 2 ! . Using the deﬁnitions of λ ± and γ i it follows λ + ¯ λ − = λ 2 + and | λ + | 2 = | λ − | 2 = β , and S H S is given by S H S = (cid:18) 1 + β 1 + λ 2 − 1 + λ 2 + 1 + β (cid:19) with the eigenvalues µ ± = 1 + β ± | 1 + λ 2 + | . ( 10 ) In both cases the condition number of S is cond ( S ) = p µ + / µ − with µ ± deﬁned in ( 9 ) or ( 10 ) . Let Λ denote the diagonal matrix with diagonal entries λ + and λ − . As λ + and λ − approach each other , i . e . as γ i → 0 , the columns of S = S ( i ) become nearly linearly dependent and cond ( S ) tends to inﬁnity . This observation is illustrated in Figure 5 , and it implies that the argument k ( M ( i ) ) k k 2 = k S Λ k S − 1 k 2 ≤ k S k 2 k Λ k k 2 k S − 1 k 2 = cond ( S ) k Λ k k 2 ( 11 ) for the convergence of ( M ( i ) ) k → 0 as k → ∞ needs to be reﬁned when D has ( small ) positive eigenvalues for which γ i ≈ 0 . Components i for which γ i ≈ 0 will be called critical components below . Given the choice β = ( 1 − √ α ) 2 , critical components are those with D i , i close to m . 2 . 4 The Schur canonical form It turns out that the case γ i ≈ 0 can be analyzed by using the Schur - decomposition of M ( i ) with an upper right triangular matrix R and a transformation matrix T given by T RT − 1 : = (cid:18) 1 0 λ + 1 (cid:19) (cid:18) λ + 1 0 λ − (cid:19) (cid:18) 1 0 − λ + 1 (cid:19) = (cid:18) 0 1 − λ + λ − λ + + λ − (cid:19) = (cid:18) 0 1 − β β i (cid:19) = M ( i ) . 11 Figure 5 : min { cond ( S ( i ) ) , 20 } as a function of αD i , i ∈ ( 0 , 2 ] and β ∈ [ 0 , 1 ) on the left , and zeros of the terms γ i depending on αD i , i and β on the right plot . 0 0 . 5 1 1 . 5 2 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 Note that this decomposition is valid for both cases : for γ i = 0 as well as for γ i 6 = 0 . ( When γ i = 0 i . e . when λ + = λ − this is the Jordan canonical form . ) Now , in place of ( 11 ) , also the argument k ( M ( i ) ) k k = k ( T RT − 1 ) k k = k T R k T − 1 k ≤ k T k · k T − 1 k · k R k k = cond ( T ) · k R k k ( 12 ) can be used . While both ( 11 ) and ( 12 ) lead to valid bounds for k ( M ( i ) ) k k whenever ( 11 ) is deﬁned and thus , the better of both bounds can be used , the estimate below relies on ( 12 ) which is always well deﬁned . Bounds on cond ( T ) and on k R k k are derived next : By Gershgorin’s theorem , given a triangular matrix ˆ R = (cid:18) a b 0 c (cid:19) with complex numbers a , b , c and | a | ≥ | c | , the 2 - norm of ˆ R is bounded by k ˆ R k 2 = λ max (cid:18)(cid:18) ¯ a 0 ¯ b ¯ c (cid:19) (cid:18) a b 0 c (cid:19)(cid:19) 1 / 2 = λ max (cid:18)(cid:18) | a | 2 ¯ ab a ¯ b | c | 2 + | b | 2 (cid:19)(cid:19) 1 / 2 ≤ p | a | 2 + | ab | + | b | 2 ( 13 ) ( with λ max denoting the maximum eigenvalue ) . Since k ˆ R k 2 = k ˆ R H k 2 the bound ( 13 ) also applies to the matrix T and yields k T k 22 ≤ 1 + | λ + | + | λ + | 2 ≤ 3 ( 14 ) since | λ + | ≤ ρ ( M ( i ) ) < 1 . The same estimate applies to k T − 1 k 22 , so that cond ( T ) ≤ 3 . 12 Inductively , it can be veriﬁed that the k th power of the matrix R is given by R k = (cid:18) λ k + P k − 1 ℓ = 0 λ ℓ + λ k − 1 − ℓ − 0 λ k − (cid:19) . By deﬁnition | λ + | ≤ ρ and | λ − | ≤ ρ apply . ( If γ i = 0 , then actually ρ = | λ − | = | λ + | — and this is the “worst case” in the estimate below ) . In any case , it follows with the triangle inequality that the absolute value of the upper right entry of R k is bounded by | ( R k ) 1 , 2 | ≤ kρ k − 1 , and then , by ( 13 ) , that k R k k ≤ ρ k − 1 p ρ 2 + kρ + k 2 ≤ ρ k − 1 √ 1 + k + k 2 ≤ ρ k − 1 ( k + 1 ) . ( 15 ) Since the condition number of T is at most 3 , the norm k ( M ( i ) ) k k is at least 13 k R k k and at most 3 k R k k . The latter observation can be used to bound the number of iterations needed to reduce the ( unknown ) Euclidean distance of a given point x 0 to the optimal solution by a factor ǫ > 0 . This is done in the next section with a slightly tighter bound of k ( M ( i ) ) k k . 2 . 5 On the rate of convergence Inserting ( 14 ) and ( 15 ) in ( 12 ) yields k ( M ( i ) ) k k 2 ≤ 3 ρ k − 1 ( k + 1 ) . This bound is based on the sub - multiplicativity of the 2 - norm . When computing ( M ( i ) ) k explicitly , it can be reduced to k ( M ( i ) ) k k 2 ≤ 2 ρ k − 1 ( k + 1 ) . ( 16 ) The somewhat tedious calculations leading to ( 16 ) are given in the Appendix . Let a desired accuracy ǫ ∈ ( 0 , 1 ) be given . To obtain a suﬃcient condition for 2 ρ k − 1 ( k + 1 ) ≤ ǫ ⇐⇒ ( k − 1 ) ln ( ρ ) + ln ( k + 1 ) ≤ ln ( ǫ 2 ) ( 17 ) the bound ln ( ρ ) ≤ ρ − 1 is used , ( 17 ) ⇐ = ( k − 1 ) ( ρ − 1 ) + ln ( k + 1 ) ≤ ln ( ǫ 2 ) ⇐⇒ ( 1 − ρ ) ( k − 1 ) ≥ ln ( 2 ǫ ) + ln ( k + 1 ) ( 18 ) Denote δ : = 1 √ 2 cond ( D ) = 1 −√ β 2 ≤ 1 − ρ 2 . If the following two inequalities are satisﬁed then ( 18 ) is satisﬁed as well , δ ( k − 1 ) ≥ ln ( 2 ǫ ) and δ ( k − 1 ) ≥ ln ( k + 1 ) . ( 19 ) Here δ > 0 depends on β = ( 1 − √ α ) 2 , i . e . on the choice of the parameters α , β that in turn depend on the bounds m and m of the eigenvalues of D . Below it is assumed that δ ≤ exp ( − 2 ) – which is the case when cond ( D ) ≥ 28 . Then , the second relation in ( 19 ) is satisﬁed whenever k ≥ ¯ k : = 2 δ ln ( 1 δ ) − 1 . 13 Indeed , δ ( ¯ k − 1 ) − ln ( ¯ k + 1 ) = 2 ln ( 1 δ ) − 2 δ − ln ( 2 δ ln ( 1 δ ) ) = ln ( 1 δ ) − 2 δ − ln ( ln ( 1 δ ) ) − ln ( 2 ) > 0 for 0 < δ ≤ exp ( − 2 ) . Thus it suﬃces to ensure that the ﬁrst inequality in ( 19 ) implies the second . This is the case when ǫ is suﬃciently small : Indeed , the ﬁrst bound in ( 19 ) is satisﬁed for k = ¯ k with equality if ǫ = ¯ ǫ : = 2 δ 2 e 2 δ . For ǫ ≤ ¯ ǫ all values of k satisfying the ﬁrst relation in ( 19 ) are greater or equal to ¯ k so that the second relation is satisﬁed as well . Since e 2 δ > 1 it is suﬃcient to choose ǫ ≤ 2 δ 2 = 1 / cond ( D ) . The ﬁrst relation in ( 19 ) can be written as k ≥ ln (cid:0) 2 ǫ (cid:1) · 1 δ + 1 = ln (cid:0) 2 ǫ (cid:1) · p 2 cond ( D ) + 1 . ( 20 ) Thus , when k satisﬁes ( 20 ) with ǫ ≤ 2 δ 2 , relation ( 17 ) is satisﬁed and √ 2 k 12 ( x k − 1 + x k ) k 2 = (cid:13)(cid:13)(cid:13)(cid:13) 12 (cid:18)(cid:18) x k − 1 x k (cid:19) + (cid:18) x k x k − 1 (cid:19)(cid:19)(cid:13)(cid:13)(cid:13)(cid:13) 2 ≤ 12 (cid:18)(cid:13)(cid:13)(cid:13)(cid:13)(cid:18) x k − 1 x k (cid:19)(cid:13)(cid:13)(cid:13)(cid:13) 2 + (cid:13)(cid:13)(cid:13)(cid:13)(cid:18) x k x k − 1 (cid:19)(cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:19) = 12 (cid:18)(cid:13)(cid:13)(cid:13)(cid:13)(cid:18) x k − 1 x k (cid:19)(cid:13)(cid:13)(cid:13)(cid:13) 2 + (cid:13)(cid:13)(cid:13)(cid:13)(cid:18) x k − 1 x k (cid:19)(cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:19) = (cid:13)(cid:13)(cid:13)(cid:13)(cid:18) x k − 1 x k (cid:19)(cid:13)(cid:13)(cid:13)(cid:13) 2 ≤ ǫ (cid:13)(cid:13)(cid:13)(cid:13)(cid:18) x 0 x 1 (cid:19)(cid:13)(cid:13)(cid:13)(cid:13) 2 ≤ √ 2 ǫ k x 0 k 2 where the last inequality follows from k x 1 k 2 ≤ k x 0 k 2 , the ﬁrst step being a plain steepest descent step with step length 2 / m . Summarizing , the claim of Theorem 1 follows . (cid:3) 2 . 6 Nesterov’s accelerated gradient method For the analysis of Nesterov’s accelerated gradient method again it suﬃces to consider the function f in ( 2 ) with ∇ f ( x ) = Dx . Replacing ( HBM ) with ( NAG ) in the form ( 1 ) it follows that the matrix M ( i ) = (cid:18) 0 1 − β β i (cid:19) in ( 4 ) is to be replaced with M ( i ) : = (cid:18) 0 1 − β ( 1 − α i ) ( 1 + β ) ( 1 − α i ) (cid:19) where again , α i : = αD i , i . Thus , replacing β and β i in ( 4 ) with β ( 1 − α i ) and ( 1 + β ) ( 1 − α i ) it follows that ρ in ( 6 ) changes to ρ : = ρ ( M ( i ) ) = ( p β ( 1 − α i ) if ( 1 + β ) 2 ( 1 − α i ) 2 ≤ 4 β ( 1 − α i ) 12 ( | ( 1 + β ) ( 1 − α i ) | + ¯ γ i ) else ( 21 ) 14 where ¯ γ i : = p ( 1 + β ) 2 ( 1 − α i ) 2 − 4 β ( 1 − α i ) . The ﬁrst case of deﬁnition ( 21 ) can only be obtained for α i ≤ 1 . This results in half the step length α : = 1 m compared to ( 7 ) for ( HBM ) and it then follows for α i ∈ [ 1 / cond ( D ) , 1 ] and β : = (cid:18) √ cond ( D ) − 1 (cid:19) 2 cond ( D ) − 1 that ( 1 + β ) 2 ( 1 − α i ) ≤ 4 β and ρ = p β ( 1 − α i ) ≤ 1 − 1 √ cond ( D ) . This is the same bound as for the ( MM ) when taking half the step length α or – which is the same – when replacing m in ( MM ) with 2 m . In contrast to the ( MM ) , however , based on this analysis a theoretical justiﬁcation of a longer step length α > 1 / m is not possible . The eigenvalues of M ( i ) are now given by λ ± = 1 2 ( ( 1 + β ) ( 1 − α i ) ± p ( 1 + β ) 2 ( 1 − α i ) 2 − 4 β ( 1 − α i ) ) and with this deﬁnition the matrix T transforming M ( i ) to the Schur canonical form is the same as in Section 2 . 4 . The iteration complexity for ( MM ) thus applies to ( NAG ) as well when replacing cond ( D ) with 2 cond ( D ) . Summarizing , the following result is obtained : Theorem 2 Let f : R n → R be a strictly convex quadratic function with minimizer x ∗ and denote H : = ∇ 2 f ( x ∗ ) . Let m be an upper bound for the eigenvalues of H and let 0 < m be a lower bound for the eigenvalues of H . Then cond ( H ) : = m / m is an upper bound for the condition number of H . Assume that cond ( H ) ≥ 28 . In ( HBM ) deﬁne α = 1 / m , α = 1 / cond ( H ) , β = ( 1 −√ α ) 2 1 − α and let ǫ ≤ 1 / cond ( H ) be given . Then , given x 0 ∈ R n , after 1 + ⌈ 2 p cond ( H ) ln ( 2 ǫ ) ⌉ steps of the ( NAG ) , an approximate solution ¯ x k : = 12 ( x k − 1 + x k ) is generated with k ¯ x k − x ∗ k 2 ≤ ǫ k x 0 − x ∗ k 2 . 2 . 7 A note on the rate of convergence Theorem 1 and Theorem 2 address the iteration complexity with respect to the Euclidean norm while the asymptotic rate of convergence of ( NAG ) or ( HBM ) ( with respect to any ﬁxed norm ) is given by the maximum spectral radius of all matrices M ( i ) which coincides with the value 1 − √ α under the assumptions of Theorems 1 and 2 . Thus , the asymptotic rate of convergence is • 1 − p 1 / cond ( H ) for ( NAG ) and • 1 − p 2 / cond ( H ) for ( HBM ) . 15 Here , the rate for ( HBM ) is somewhat worse than the asymptotic rate 1 − 2 / p cond ( H ) for ( HBM ) established in [ 15 ] , Theorem 9 ( 3 ) , the diﬀerence being due to the limitation of the step length α which is nearly twice as long in [ 15 ] as considered here – and four times as long as for Nesterov’s accelerated gradient method . The rates of convergence are not just upper bounds but exact rates once the values m and m are given , and hence , the diﬀerence in the rates of convergence for ( NAG ) and ( HBM ) can be explained just by the limitations of the step length : when the ( HBM ) is restricted to steps α = 1 / m as for ( NAG ) , its asymptotic rate of convergence is exactly the same as ( NAG ) . The above rate for Nesterov’s accelerated gradient method is the same as established for the convergence of the function values in [ 14 ] , Theorem 2 . 2 . 3 . Translated to the error measure k x − x ∗ k H = p f ( x ) − f ( x ∗ ) the rate of convergence of [ 14 ] , Theorem 2 . 2 . 3 reduces to approximately 1 − p 1 / 2 cond ( H ) . 3 . Conclusion The asymptotic rate of convergence of the momentum method for ﬁxed parameters has been analyzed in the classical work [ 15 ] . Here , the constants in this work are considered and an explicit bound for the iteration complexity is derived that also applies in slightly modiﬁed form to Nesterov’s accelerated gradient method . Acknowledgment The authors would like to thank Robert Gower for helpful e - mail - discussions . 16 Appendix The matrix M i , k : = ( M ( i ) ) k can also be written as ( M ( i ) ) k = T R k T − 1 = (cid:18) 1 0 λ + 1 (cid:19) λ k + P k − 1 ℓ = 0 λ ℓ + λ k − 1 − ℓ − 0 λ k − ! (cid:18) 1 0 − λ + 1 (cid:19) = (cid:18) 1 0 λ + 1 (cid:19) λ k + − λ + P k − 1 ℓ = 0 λ ℓ + λ k − 1 − ℓ − P k − 1 ℓ = 0 λ ℓ + λ k − 1 − ℓ − − λ + λ k − λ k − ! = λ k + − λ + P k − 1 ℓ = 0 λ ℓ + λ k − 1 − ℓ − P k − 1 ℓ = 0 λ ℓ + λ k − 1 − ℓ − λ k + 1 + − λ 2 + P k − 1 ℓ = 0 λ ℓ + λ k − 1 − ℓ − − λ + λ k − λ + P k − 1 ℓ = 0 λ ℓ + λ k − 1 − ℓ − + λ k − ! = λ k + − P k − 1 ℓ = 0 λ ℓ + 1 + λ k − 1 − ℓ − P k − 1 ℓ = 0 λ ℓ + λ k − 1 − ℓ − λ k + 1 + − P k − 1 ℓ = 0 λ ℓ + 2 + λ k − 1 − ℓ − − λ + λ k − P k − 1 ℓ = 0 λ ℓ + 1 + λ k − 1 − ℓ − + λ k − ! = − P k − 2 ℓ = 0 λ ℓ + 1 + λ k − 1 − ℓ − P k − 1 ℓ = 0 λ ℓ + λ k − 1 − ℓ − − P k − 2 ℓ = − 1 λ ℓ + 2 + λ k − 1 − ℓ − P k − 1 ℓ = − 1 λ ℓ + 1 + λ k − 1 − ℓ − ! = − P k − 1 t = 1 λ t + λ k − t − P k − 1 ℓ = 0 λ ℓ + λ k − 1 − ℓ − − P k − 1 t = 0 λ t + 1 + λ k − t − P kt = 0 λ t + λ k − t − ! = : (cid:18) p q r s (cid:19) ∈ R 2 × 2 , where in the last row the index shift t : = ℓ + 1 was used . ( The deﬁnition of the real numbers p , q , r , s depends on the possibly complex eigenvalues λ + and λ − . ) The matrix product ( M i , k ) T M i , k is ( M i , k ) T M i , k = (cid:18) p r q s (cid:19) (cid:18) p q r s (cid:19) = (cid:18) p 2 + r 2 pq + rs pq + rs q 2 + s 2 (cid:19) = (cid:18) a b b d (cid:19) with a : = p 2 + r 2 = k − 1 X ℓ = 1 λ ℓ + λ k − ℓ − ! 2 + k − 1 X ℓ = 0 λ ℓ + 1 + λ k − ℓ − ! 2 , b : = pq + rs = − k − 1 X ℓ = 1 λ ℓ + λ k − ℓ − ! k − 1 X ℓ = 0 λ ℓ + λ k − 1 − ℓ − ! − k − 1 X ℓ = 0 λ ℓ + 1 + λ k − ℓ − ! k X ℓ = 0 λ ℓ + λ k − ℓ − ! , d : = q 2 + s 2 = k − 1 X ℓ = 0 λ ℓ + λ k − 1 − ℓ − ! 2 + k X ℓ = 0 λ ℓ + λ k − ℓ − ! 2 . By deﬁnition of the spectral radius ρ = ρ ( M ( i ) ) < 1 the inequalities | λ + | ≤ ρ and | λ − | ≤ ρ hold , and thus , a ≤ 2 k 2 ρ 2 k and d ≤ 2 ( k + 1 ) 2 ρ 2 k − 2 . Since ( M i , k ) T M i , k (cid:23) 0 the maximum eigenvalue of ( M i , k ) T M i , k is at most equal to the trace , i . e . λ max ( ( M i , k ) T M i , k ) ≤ a + d ≤ 4 ρ 2 ( k − 1 ) ( k + 1 ) 2 . 17 This way , an upper bound for the norm of M i , k is given by k M i , k k 2 ≤ 2 ρ k − 1 ( k + 1 ) . ( 22 ) To estimate in how far the above bound is tight , consider the case of γ i = 0 , i . e . ρ = | λ + | = | λ − | . It then follows a ≥ 2 ( k − 1 ) 2 ρ 2 k + 2 , b ≤ − 2 ( k − 1 ) 2 ρ 2 k + 1 , d ≥ 2 ( k − 1 ) 2 ρ 2 k and with z : = ( 1 , − 1 ) T that λ max ( ( M i , k ) T M i , k ) ≥ z T ( M i , k ) T M i , k z z T z ≥ 8 ( k − 1 ) 2 ρ 2 k + 2 2 . Thus , k M i , k k 2 ≥ 2 ρ k + 1 ( k − 1 ) , i . e . up to the changes k + 1 ←→ k − 1 the bound ( 22 ) is sharp . Again , the “critical components” i . e . those with γ i ≈ 0 lead to a poor convergence , i . e . to the case where ( 22 ) is almost sharp . On the other hand , ( M i , k ) T M i , k is almost singular , and for a critical component i the quantity α i is small so that the iterates x 0 and x 1 of ( MM ) with step length 2 / m satisfy x 0 i ≈ x 1 i . This again implies that the vector z 1 ( i ) : = ( x 0 i , x 1 i ) T is close to the eigenvector of ( M i , k ) T M i , k associated with the small eigenvalue , i . e . k M i , k z 1 ( i ) k 22 = ( z 1 ( i ) ) T ( M i , k ) T M i , k z 1 ( i ) ≪ k M i , k k 22 k z 1 ( i ) k 22 . References [ 1 ] Barr´e , M . , Taylor , A . , d’Aspremont , A . ( 2020 ) : Complexity Guarantees for Polyak Steps with Momentum . 33rd Annual Conference on Learning Theory , Proceedings of Machine Learning Research , Vol . 125 , 1 - 27 . [ 2 ] Defazio , A . ( 2021 ) : Momentum via Primal Averaging : Theoretical Insights and Learn - ing Rate Schedules for Non - Convex Optimization . https : / / arxiv . org / pdf / 2010 . 00406 . pdf [ 3 ] D´efossez , A . , Bottou , L . , Bach , F . , Usunier , N . ( 2022 ) : A Simple Conver - gence Proof of Adam and Adagrad . Transactions on Machine Learning Research . https : / / arxiv . org / pdf / 2003 . 02395 . pdf [ 4 ] Diakonioklas , J . , Jordan , M . I . ( 2021 ) : Generalized Momentum - Based Methods : a Hamiltonian Perspective . SIAM J . Optim . Vol . 31 , No . 1 , 915 - 944 . [ 5 ] O’Donoghue , B . , Cand ` es . E . ( 2015 ) : Adaptive Restart for Accelerated Gradient Schemes . Foundations of computational mathematics , Vol . 15 , No 3 , 715 - 732 . 18 [ 6 ] Ganesh , S . , Deb , R . , Thoppe , G . , Budhiraja , A . ( 2022 ) : Does Mo - mentum Help in Stochastic Optimization ? A Sample Complexity Analysis . https : / / arxiv . org / abs / 2110 . 15547v3 [ 7 ] Gitman , I . , Lang , H . , Zhang , P . , Xiao , L . ( 2019 ) : Understanding the Role of Momentum in Stochastic Gradient Methods . H . Wallach , H . Larochelle , A . Beygelzimer , F . d’Alch´e - Buc , E . Fox , R . Garnett ( eds ) : Advances in Neural Information Processing Systems , 32 ( NeurIPS 2019 ) , https : / / proceedings . neurips . cc / paper / 2019 [ 8 ] Goh , G . ( 2017 ) : Why Momentum Really Works . Distill , http : / / distill . pub / 2017 / momentum [ 9 ] Gratton , S . , Jerad , S . , Toint , Ph . L . ( 2022 ) : First - Order Objective - Function - Free Opti - mization Algorithms and Their Complexity . [ 10 ] Gratton , S . , Jerad , S . , Toint , Ph . L . ( 2022 ) : Parametric complexity analysis for a class of ﬁrst - order Adagrad - like algorithms . https : / / arxiv . org / pdf / 2203 . 01647 . pdf [ 11 ] Gratton , S . , Toint , Ph . L . ( 2022 ) : OFFO minimization algorithms for second - order optimality and their complexity . https : / / arxiv . org / pdf / 2203 . 03351 . pdf [ 12 ] Li , R . C . ( 2008 ) : On Meinardus’ examples for the conjugate gradient method , Mathe - matics of Computation , Vol . 77 , Nr . 261 , 335 - 352 . [ 13 ] Nesterov , Y . E . ( 1983 ) : A method for solving the convex programming problem with convergence rate O ( 1 / k 2 ) . Dokl . akad . nauk Sssr 269 , 543 - 547 . [ 14 ] Nesterov , Y . E . ( 2003 ) : Introductory lectures on convex optimization : A basic course . Springer Science and Business Media , Vol . 87 . [ 15 ] Polyak , B . T . ( 1964 ) : Some methods of speeding up the convergence of iteration meth - ods . USSR Computational Mathematics and Mathematical Physics , 4 ( 5 ) : 1 - 17 . [ 16 ] Sebbouh , O . , Gower , R . M . , Defazio , A . ( 2020 ) : On the convergence of the Stochastic Heavy Ball Method . https : / / othmanesebbouh . github . io / publications / heavy ball . pdf [ 17 ] The GitHub repository : https : / / github . com / MHagedorn / momentum ( 2022 ) [ 18 ] Yang , T . , Lin , Q . , Li , Z . ( 2016 ) : Uniﬁed Convergence Analysis of Stochastic Momentum Methods for Convex and Non - convex Optimization . https : / / arxiv . org / pdf / 1604 . 03257 . pdf 19