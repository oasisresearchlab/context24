Using the Ecological Approach to Create Simulations of Learning Environments Graham Erickson 1 , Stephanie Frost 2 , Scott Bateman 3 , and Gord McCalla 2 1 Dept . of Computing Science , U . of Alberta , Edmonton , Canada gkericks @ ualberta . ca 2 ARIES Lab , Dept . of Computer Science , U . of Saskatchewan , Saskatoon , Canada stephanie . frost @ usask . ca , mccalla @ cs . usask . ca 3 Dept . of Computer Science , U . of Prince Edward Island , Charlottetown , Canada sbateman @ upei . ca Abstract . Simulated pedagogical agents have a long history in AIED research . We are interested in simulation from another , less well explored perspective : simulating the entire learning environment ( including learn - ers ) to inform the system design process . An AIED system designer can carry out experiments in the simulation environment that would other - wise be too costly ( or time consuming ) with real learners using a real system . We suggest that an architecture called the “ecological approach ( EA ) ” [ 1 ] can form the basis for creating such simulations . To demon - strate , we describe how to develop a proof - of - concept simulated ITS pro - totype , modelled in the EA architecture . We also show how to factor in data from two human subject studies ( done for other purposes ) to gain a degree of cognitive ﬁdelity . An experiment is carried out with the pro - totype . The approach is general and can apply to learning systems with a wide variety of “pedagogical styles” ( not just ITSs ) at various stages of their life cycle . We conclude that simulation is a critically needed methodology in AIED . Keywords : simulated learning environments , simulated learners , design of elearning systems , ecological approach 1 Introduction This paper is about the simulation of learning environments . The designer of a learning environment can use simulation to observe the impact of various de - sign decisions under many combinations of circumstances : novice vs advanced learners , a few students vs many students , system vs learner control , etc . Sim - ulations can allow learning system designers to easily experiment with many aspects of their systems without the need for expensive human subject studies , a similar role to mathematical models and physical models ( like wind tunnels ) for engineers when building physical artifacts . Simulations can be done at the outset of system design , or interleaved with human subject studies as a learning system iteratively evolves , or even during actual deployment in order to explore particular issues or to discover possible causes of various observed phenomena . In order to gain useful insights from a simulation , however , it must be pos - sible to capture the key aspects of the learning system to be simulated . For a learning system this means , particularly , capturing the system’s “pedagogical style” and capturing key aspects of the learners who will use the system . The simulated system can then interact with the simulated learners and the resulting performance can be measured in various ways to make predictions about how a real world version of the system might behave . In this paper , we show how the ecological approach ( EA ) architecture [ 1 ] can serve as the framework for building learning systems of many diﬀerent peda - gogical styles . We discuss how a system designer can map a proposed learning system into the EA architecture . We then describe a prototype simulation that serves as a “proof of concept” of the feasibility of the approach . In particular we show how a speciﬁc pedagogical style ( an ITS ) can be implemented , and how the behaviour of the simulated learners can be informed by performance characteristics captured by human subject studies done for other purposes , pro - viding a degree of cognitive ﬁdelity with actual learners . We then discuss the result of an experiment run using the proof of concept prototype , that allows the extraction of a prediction about how the proposed learning system might function if actually built and used by real learners . We conclude the paper with a discussion of the role of simulation in the design of AIED systems , and why we feel that simulation must be in the arsenal of design methodologies available to AIED system designers , especially if we are to reduce the long development times normally associated with building “intelligent” learning systems ( which can stretch to years or even decades ) . 2 The Ecological Approach The ecological approach ( EA ) is an architecture for the design of learning en - vironments that allows the capture of learner actions appropriately scoped to the content with which the learners are interacting . The architecture assumes that the learning content is packaged into learning objects ( LOs ) and that the learners are each represented by a learner model that contains both static at - tributes ( the characteristics part of the model ) and clickstream data gathered as they interact with the LOs ( the episodic part ) . After each interaction by a learner with a LO , an instance of the learner model is attached to the LO as “metadata” . Over time , many instances build up around each LO and can be the basis of reasoning for many purposes , such as recommending LOs that have been successful for learners who are similar to a given learner , ﬁnding out which LOs are useful or not , and so on . The approach is called ecological because as the metadata builds up naturally over time , the system can carry out its purposes with increasing precision , essentially “evolving” its capabilities in response to what has actually happened in the environment and what it is trying to do . The ecological architecture also has the ability to represent many diﬀerent styles of learning system . The concept of learning object is very general , and can include text , graphics , simulations , interactive pages , forums , etc . The learning objects and the learner models can contain many diﬀerent attributes . Learners can interact in many diﬀerent ways with LOs or with each other . No speciﬁc ped - agogy is assumed : LOs can be presented in sequence , as in a traditional ITS ; LOs can be recommended according to various recommender algorithms , collabora - tive or feature - based or hybrid ; learners can help each other select content or to overcome impasses ; and so on . The designer of a learning system can thus build a system using the EA architecture that matches his or her desired pedagogical style . This can be a system to be used with real learners , but more importantly for the purposes of this paper it can also be a simulation . We provide a “proof of concept” for this in the next section . 3 Building A Proof of Concept Prototype In this section we show the development of a “proof of concept” prototype sim - ulation created for a particular educational scenario . We demonstrate how the designer of a learning system can model his or her proposed system in the EA architecture ( section 3 . 1 ) and then run experiments to answer questions about the proposed system ( section 3 . 2 ) . The goal is to provide a case study of our approach to simulation and its potential role in helping the process of learning system design . 3 . 1 Mapping to the Ecological Approach Architecture For the proof - of - concept prototype , we developed the scenario of a designer building an intelligent tutoring system ( ITS ) for the introductory programming domain ( a common one for an ITS ) . The stage of design is preliminary . The designer’s purpose in building the simulation is to explore issues about the or - dering of concepts and their eﬀects on learning . In what follows , we talk about “the designer” and show the steps the designer takes as the simulation is being developed in order to illustrate the process in some detail . The simulation de - scribed below was actually designed and implemented by the authors , not some hypothetical designer , and the experiments using the simulation were actually carried out by the authors too . In a traditional ITS , the content is typically packaged into modules that are related to one another by pre - requisite relations . The system typically also keeps learner models for each learner with both proﬁle information and information gleaned during a learner’s interactions with the ITS . As a learner interacts with a module , they are observed and evaluated as to how well they understand the content , and this information is incorporated into the learner model . Based on the learner model and the pre - requisite information , the ITS then recommends another module appropriate to the learner , and so on until some termination conditions are satisﬁed , ideally that the learner ( as evidenced in their model ) has mastered the important content . An ITS can be easily mapped into the EA architecture . Each content mod - ule can be represented as a learning object . Prerequisites are kept as part of the information in each learning object . The learner proﬁle for each learner is represented in the learner characteristics part of the learner model . Traces of learner interaction with learning objects are gathered in the episodic part . Al - gorithms are created to support the learner’s interaction with a learning object , to evaluate the learner’s performance , and to help in the selection of the next learning object . These all have to be emulated in a simulation of an ITS . For the ITS simulation in this scenario , the designer can model the learning objects on the actual concepts in a typical introductory programming course , available readily from a course outline of an existing course . In the simulation , the designer also decides that ( for his or her purpose ) each learning object ( LO ) need only contain three elements : its level in the Bloom taxonomy [ 2 ] , a set of parent LOs ( that are prerequisites to this LO ) , and a set of child LOs ( for which this LO is a prerequisite ) . Since this is a simulation , the learning objects need no other content . A similar process could be used to model almost any domain . Having mapped the domain into learning objects , consistent with the EA ar - chitecture the designer also needs to model important attributes of each learner ( the characteristics ) , to represent how each learner interacts with a learning object ( the behaviour function ) , and to determine how successful ( or not ) a learner’s interaction with a learning object is in pedagogical terms ( the eval - uation function ) . In this proof - of - concept prototype , the designer decides that he / she doesn’t need to capture speciﬁc characteristics of the learners . Such char - acteristics ( e . g . learning styles , gender ) need only be added if certain attributes are deemed to generate important diﬀerences in learners’ interactions relative to the designer’s purpose . To provide some realism to the behaviour function , the designer re - uses data collected in a study carried out by Bateman [ 3 ] exploring the workplace web browsing behaviour of 25 graduate and undergraduate research students . In the Bateman study a web browser plug - in called SaskWatch [ 3 ] logged each user’s ﬁne - grained actions for an entire year . By counting the number of times a user has performed a particular action divided by the number of minutes of system use , various rates for that user can be computed : copyRate , cutRate , keypressRate , mouseClickRate , scrollRate , searchRate , changeLearningObjectRate , browseRate . ( Note : we only used a subset of the very large SaskWatch dataset . ) For example , if a user had 3 mouse - clicks and used the system for 4 minutes , then their mouseClickRate is 0 . 75 . Rates above 1 represent a user performing the action more than once per minute . Since the SaskWatch data set is so large , only a representative sample of each user’s data is used . This collection of rates can then fuel a model of actual human browsing behaviour . After obtaining each SaskWatch user’s rates for each action , the designer creates a histogram for each rate across the population of users . This provides a distribution of the values for each rate . For example , the browseRate histogram showed that most users had a fairly short time between viewing pages , but indeed there were a few users with longer browse rates . Next , the designer ﬁts a curve to each histogram so that the distribution of rate values can be represented by a function of the form f ( x ) = a / ( b ∗ x ) + c , where a , b , c are constants and x is the rate value ( for example , 0 . 75 mouseClickRate ) . Then f ( x ) is the portion of the population who should have that rate . Next , each function is normalized to a probability density function so that the x - axis becomes the density ( and not the actual rate value ) . To do this , f ( x ) is integrated across its domain . The domain of f ( x ) is the range of rate values given in the histogram . Using the probability density function , the cumulative distribution function , F ( x ) , is obtained . This gives the proportion of the population with a rate less than or equal to x . If F ( x ) is inverted to obtain F − 1 ( x ) , this will give rates that mimic the density described by the original f ( x ) . Learner interaction attributes can then be assigned using an initialization process structured like this : for each rate , R : ( ex : browseRate , mouseClickRate , etc . ) for each learner agent , L : draw a uniform random number x set L . R = F − 1 ( x ) Using this approach while there may be no real learner whose collection of web browsing attributes exactly matches a given simulated learner , the popula - tion of simulated learners will browse the learning objects in way that is statisti - cally similar to the actual human web browsing activity logged by SaskWatch . At the design phase for which the proof - of - concept simulation is aimed , this allows the simulated learners to behave with some cognitive ﬁdelity in the absence of other data about real learners . Of course , this approach ignores possible depen - dencies among the various rates . An alternative if there were more than 25 users in the study , would be to capture in a user model each user’s behaviour over all the rates , and then design the artiﬁcial population to match the distribution of the user models rather than the speciﬁc rates . Later in the system life cycle , it might be possible to capture ﬁner grained diﬀerences in how diﬀerent users interact diﬀerently with speciﬁc learning objects , which would bring even more cognitive ﬁdelity . The designer also needs to capture how well a given simulated learner has understood a given learning object : that is , the evaluation function needs to be designed . For this , the designer decides to draw on another experiment car - ried out by Peckham and McCalla [ 4 ] , who found patterns correlating learners’ browsing behaviour with their success at answering questions at various levels of Bloom’s taxonomy as they interacted with written material on - line . Since the simulated learning objects have Bloom levels , and since the simulated learners have been equipped with browsing behaviours ( based on the SaskWatch data ) , this behaviour need simply be mapped onto Peckham’s patterns , which then can predict the level of success in understanding the learning object . Here are some details . Participant data were clustered into 4 groups of read - ing / scanning / scrolling behaviour : Light Reading , Light Medium Reading , Heavy Medium Reading or Heavy Reading . The Light Reading students spent the smallest proportion of time reading and the highest proportion of time scan - ning / scrolling . This proportion gradually changes all the way up to Heavy Read - ing , with the highest proportion of time spent reading and very little scan - ning / scrolling . Peckham’s study uncovered signiﬁcant correlations between this behaviour and the score on comprehension questions , depending on the Bloom level of the LO . This correlation can be re - used in the simulation model to determine a score ( degree of mastery of the concept ) . For example , consider a particular LO with a low Bloom level . Peckham’s study shows that students with Light Reading behaviour achieved full marks or close to full marks on lower level Bloom ques - tions , while students with Heavy Reading actually scored poorly : most scored 0 marks , with none achieving more than half marks ( Peckham speculated heavy readers were confused by the material , and wasted too much time ) . Thus , in the simulation model , if a simulated learner exhibiting Light Reading behaviour interacts with a LO with a low Bloom level , the simulation model can assume that the learner will score high marks . Similarly , whenever a simulated learner exhibiting Heavy Reading behaviour comes across the same LO , the simulation model can assume that the interaction will end up with a poor score for the learner . A stochastic element is added to this score calculation , to account for the many other unknown factors that could impact a learner’s score . Thus , the proof - of - concept simulation has been equipped with simulated learning objects and simulated learners who interact with these learning ob - jects . The modelling is based on real world data , but is still limited . The system designer could easily add in additional sophistication . For example , he or she might say that learners will achieve a higher score if they have already con - sumed prerequisite LOs . Or , higher scores might result when the learner’s pre - ferred learning style matches the style of the LO ( which would require adding learning style attributes to both learners and learning objects ) . Even without data from human subject studies to inform the behaviour function , common - sense assumptions could be used . This is the approach taken by Champaign [ 5 ] to make a number of interesting predictions about learning systems . We em - phasize that it is not necessary to deeply model the whole learner or the whole learning environment , only the parts relevant to the questions being asked by the designer . The stage of design will also be a big factor . In early stages of designing a learning system there will not be any data gleaned from an actual system deployed with real learners to inform the simulation , so re - purposing data gathered in other studies , as we have done here , is a plausible alternative . Later in the system life cycle , after various versions of the actual learning system have been built and tested with human subjects , real data would be available to any simulation modelling the designer wants to undertake . Having created the simulation model in terms of the ecological architecture , the designer can then run experiments using the proof - of - concept prototype to show what happens when the model is used under various assumptions about how the next learning object is to be selected . This is discussed in the next section . 3 . 2 Running Experiments Using the Simulation In the proof - of - concept scenario , the designer wishes to run the simulation to determine the impact on learning of various approaches to selecting the next learning object . To this end , an experiment is created where the simulation is run under three diﬀerent conditions : unstructured , semi - structured , and structured . The unstructured condition is a baseline , where learners select the next LO at random . In the semi - structured and structured conditions learners are not allowed to choose a LO unless they have “mastered” the pre - requisite LOs ( i . e . the score is computed to be high after an interaction ) . Once an LO has been selected , in the structured condition learners must keep attempting to complete the LO until it has been mastered . In the semi - structured condition , learners can choose to abandon an LO in favour of another LO ( whose pre - requisites they have mastered ) . In all conditions learners are not allowed to choose LOs that they have already mastered . The experimental data includes a record of all learner - LO interactions and the resulting scores under the three conditions . From this data , the designer can choose among a wide variety of measurements of success : the average score for all LOs , the score for only leaf LOs ( where ’leaf’ means the LOs at the end of the course ) , the score for only mastered LOs ( as opposed to the LOs not attempted or attempted and failed ) . These measurements can be compared across the three conditions . Note that the ﬁne - grained data about learner - LO interaction collected in the EA would also allow other unanticipated patterns to be “mined” from this same data , should the designer wish to explore other issues . Fig . 1 . Results of the Simulation ( Box plot ) To illustrate , Figure 1 summarizes experimental results of the scenario . The experiment consisted of 100 runs ( or iterations ) of a simulated course with 400 learners . Out of the 80 LOs in the course , 14 were leaf objects . The red line shows the mean number of mastered leaf LOs ( left ) or the mean number of LOs mastered among all LOs ( right ) , for each learner . The extent of the boxes represents the upper and lower quartile . The extent of the capped lines represent the max . and min . value still within 1 . 5 interquartile range . Outliers are marked by blue crosses . The results show ( unsurprisingly ) that the unstructured condition is unde - sirable : structuring clearly is useful . Moreover , it is interesting that the semi - structured environment is marginally better than the structured environment for mastered LOs , but that this is substantially reversed for mastered leaf LOs . One hypothesis is that the structured environment reaches more leaf nodes be - cause by not providing as much choice learners end up traversing the prerequisite tree more deeply ( right down to the leaf LOs ) in the same number of iterations . These results can inform the system designer who presumably would decide to incorporate some version of structuring into the actual ITS to be used by real learners . Of course , the designer may decide to run any number of other simulations . It doesn’t cost much once the simulation environment has been set up . For example , the designer could change the Bloom levels or pre - requisite requirements of a few key LOs and run the simulation again to get an idea of the impact . Another possible experiment would be to create other structuring conditions ; e . g . instead of following the prerequisite structure so closely , the Bloom level could be more of a factor , perhaps favouring lower Bloom level LOs over higher ones when there is a choice . All of these could be done very easily , without changing any of the basic abstractions informing the simulation . But , it also is not too hard to start making the simulated learners more sophisticated ( adding new characteristics ) , or to change the behaviour or evaluation functions , or to give additional attributes to the learning objects . With more eﬀort , even the traditional ITS architecture could be changed . Thus , the designer could incorporate a LO recommendation engine , perhaps using the behaviour of the learner to help personalize the recommendation . Or the designer could create a collaborative environment , with protocols for the simulated learners to interact with one another ( as Champaign [ 5 ] has done in one of his experiments ) . The ecological approach can support the modelling of virtually any kind of learning environment . 4 Research Context and Discussion Over 15 years ago VanLehn et al . [ 6 ] outlined three main uses for simulation in the design of learning systems : ( i ) to provide a practice environment for human teachers ; ( ii ) to provide simulated students who act as peers for human students ; or ( iii ) to provide an environment for pilot testing instructional design issues . The second of these uses has had by far the most follow - up research in the intervening years . In fact , there have been many systems where simulated humans can take an explicit role in the learning environment , for example as learning companions [ 7 ] , or as animated pedagogical agents [ 8 ] , or as “teachable agents” in a reciprocal learning context [ 9 ] , or even as tutors [ 10 ] . Our work is strongly aimed at the third use - to enable deep exploration of design choices when building a learning system . There has been some research in this context over the years . One branch is about cognitive modelling , e . g . Ohlsson et al . ’s [ 11 ] simulations to provide insight into how students learn subtraction , and Matsuda et al . ’s [ 12 ] increasingly sophisticated versions of SimStudent to capture ﬁne - grained models of human skill acquisition . Another branch , recently championed by Champaign [ 5 ] , does take a system level view to building simu - lated learning environments to answer speciﬁc pedagogical questions ( based on commonsense assumptions about learners ) . Nobody to our knowledge has tried to provide a general framework for doing such simulations that could apply to the design of any system to support learning . This is the main contribution of our research . We have argued that the ecological approach architecture allows systems of many pedagogical styles to be represented through mapping into learning objects , learner models , and appropriate interaction strategies . In a case study we shed light on how to actually build a “proof - of - concept” simulation of a learning system ( an ITS in this case ) . An especially original aspect is the re - use of human subject data from other studies to inform the learner modelling and interaction behaviour . This may be the best that is possible early in the life cycle of a learning system . But after gaining insight from the simulation ( s ) , the designer will eventually build a real system for human learners , so simulations developed later in the life cycle could use data gathered from actual learners . Of course , our work provides only a proof - of - concept . Much more work has to be done to be completely convincing about the abstraction into the ecological architecture , its generality and power , and even the value of simulation itself . However , we strongly feel that the designers of learning systems need to add simulation to the arsenal of available tools . Simulation allows total designer con - trol over any experiment . Measurements inaccessible in human subject studies can be made . Simulated learners are plentiful and cheap and are not required to give informed consent ! Simulation allows a space involving a vast number of parameters to be explored with relative ease . Many questions ( such as issues around load limits or appropriate response times ) can be answered without need - ing cognitive ﬁdelity in the learner models . Even if cognitive ﬁdelity is desirable , there are so many sources of ﬁne - grained user data being generated these days ( e . g . from the PSLC data shop , https : / / pslcdatashop . web . cmu . edu / ) that ap - propriate data could be found to inform the models ( data re - purposing ) , as we have demonstrated here . Further , it is not necessary to model every detail of the learning process ; valuable information can be gained simply by modelling char - acteristics most relevant to the questions being asked by the designer . This is a key point . We feel that simulation modelling may be most valuable for rejecting certain designs early in the design process , but if it is fairly easy to create a simulation then it can be used throughout the system life cycle , to test speciﬁc hypotheses ( as Ohlsson et al . [ 11 ] did ) or to gather data that can be mined for informative patterns . This is easier to do if both the simulation and the actual learning system share the same architecture ( e . g . the EA architecture ) . Perhaps the most convincing argument for simulation , however , is the na - ture of our ﬁeld . The development time for a fully deployed learning system is often measured in decades , in no small part because of the huge cost in time and money of running human subject studies at each design cycle . Simulation can change that . As we begin to roll out learning environments meant for hun - dreds of thousands of learners ( e . g . MOOCs , cognitive tutors , etc . ) , it will be important to test them ﬁrst in simulation where we do not risk huge numbers of “drop outs” and to be able to continue to explore through simulation various hypotheses as these environments evolve over time . As AIED goes even further and begins to study lifelong learning , we will need some way to test lifelong learning techniques in less than a lifetime : simulation is a clear and promising possibility . Acknowledgements We would like to thank the Natural Sciences and Engineering Research Council of Canada for funding this research through a Discovery Grant to the last author . References [ 1 ] McCalla , G : The Ecological Approach to the Design of e - Learning Environments : Purpose - based Capture and Use of Information about Learners . Journal of Interac - tive Media in Education , http : / / jime . open . ac . uk / jime / article / view / 2004 - 7 - mccalla ( 2004 ) [ 2 ] Bloom , B . : Taxonomy of Educational Objectives , Cognitive and Aﬀective Domains . Longman Group , United Kingdom ( 1969 ) [ 3 ] Bateman , S . : Using Group Interaction History in the Wild . In The Doctoral Col - loquium of the 2010 ACM Conf . Conference on Computer Supported Cooperative Work , Savannah , GA . 523 - 524 ( 2010 ) [ 4 ] Peckham , T . , McCalla , G . : Mining Student Behavior Patterns in Reading Compre - hension Tasks . In : Yacef , K . , Za¨ıane , O . , Hershkovitz , H . , Yudelson , M . , and Stamper , J . ( eds . ) , 5th Int . Conf . on Educational Data Mining , Greece , 87 - 94 ( 2012 ) [ 5 ] Champaign , J . : Peer - Based Intelligent Tutoring Systems : A Corpus - Oriented Ap - proach . Ph . D . Thesis , University of Waterloo , Waterloo , Canada ( 2012 ) [ 6 ] VanLehn , K . Ohlsson , S . , and Nason , R . : Applications of Simulated Students : An Exploration . Int . J . Artiﬁcial Intelligence in Education , 5 , 135 - 175 ( 1996 ) [ 7 ] Chan , T . W . : Learning Companion Systems . In : Frasson , C . and Gauthier , G . ( eds ) Intelligent Tutoring Systems : At the Crossroads of Artiﬁcial Intelligence and Education , Ablex , 6 - 33 ( 1990 ) [ 8 ] Johnson , L . , Rickel , J . and Lester , J . : Animated Pedagogical Agents : Face - to - Face Interaction in Interactive Learning Environments . Int . J . of Artiﬁcial Intelligence in Education , 11 , 47 - 78 ( 2000 ) [ 9 ] Leelawong , K . and Biswas , G . : Designing Learning by Teaching Agents : The Betty’s Brain System . Int . J . Artiﬁcial Intelligence in Education , 18 ( 3 ) , 181 - 208 ( 2008 ) [ 10 ] Graesser , A . , Chipman , P . , Haynes , B . and Olney , A . M . : AutoTutor : An Intelligent Tutoring System with Mixed - Initiative Dialogue . IEEE Transactions on Education , 48 , 612 - 618 ( 2005 ) [ 11 ] Ohlsson , S . , Ernst , A . M . and Rees , E . : The Cognitive Complexity of Doing and Learning Arithmetic . Research in Mathematics Education , 23 , 441 - 467 ( 1992 ) [ 12 ] Matsuda , N . , Cohen , W . W . , Sewall , J . , Lacerda , G . and Koedinger , K . R . : Pre - dicting Students Performance with SimStudent that Learns Cognitive Skills from Observation . In Luckin , R . , Koedinger , K . R . , and Greer , J . ( eds . ) , Proc . 12th Int . Conf . on AIED , Marina del Rey , 467 - 476 ( 2007 )