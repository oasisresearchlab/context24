Rules and mechanisms for eﬃcient two - stage learning in neural circuits Tiberiu Te¸sileanu 1 , 2 , Bence ¨Olveczky 3 , and Vijay Balasubramanian 1 , 2 1 Initiative for the Theoretical Sciences , The Graduate Center , CUNY , New York , NY 10016 2 David Rittenhouse Laboratories , University of Pennsylvania , Philadelphia , PA 19104 3 Department of Organismic and Evolutionary Biology and Center for Brain Science , Harvard University , Cambridge , MA 02138 November 8 , 2018 Abstract Trial - and - error learning requires evaluating variable actions and reinforcing successful variants . In songbirds , vocal exploration is induced by LMAN , the output of a basal ganglia - circuit that also contributes a corrective bias to the vocal output . This bias is gradually consolidated in RA , a motor cortex analogue downstream of LMAN . We develop a new model of such two - stage learning . Using stochastic gradient descent , we derive how the activity in ‘tutor’ circuits ( e . g . , LMAN ) should match plasticity mechanisms in ‘student’ circuits ( e . g . , RA ) to achieve eﬃcient learning . We further describe a reinforcement learning framework through which the tutor can build its teaching signal . We show that mismatches between the tutor signal and the plasticity mechanism can impair learning . Applied to birdsong , our results predict the temporal structure of the corrective bias from LMAN given a plasticity rule in RA . Our framework can be applied predictively to other paired brain areas showing two - stage learning . 1 Introduction Two - stage learning has been described in a variety of diﬀerent contexts and neural circuits . During hippocampal memory consolidation , recent memories , that are dependent on the hippocampus , are transferred to the neocortex for long - term storage ( Frankland and Bontempi 2005 ) . Similarly , the rat motor cortex provides essential input to sub - cortical circuits during skill learning , but then becomes dispensable for executing certain skills ( Kawai et al . 2015 ) . A paradigmatic example of two - stage learning occurs in songbirds learning their courtship songs ( Andalman and Fee 2009 ; Turner and Desmurget 2010 ; Warren et al . 2011 ) . Zebra ﬁnches , commonly used in birdsong research , learn their song from their fathers as juveniles and keep the same song for life ( Immelmann 1969 ) . The birdsong circuit has been extensively studied ; see Figure 1A for an outline . Area HVC is a timebase circuit , with projection neurons that ﬁre sparse spike bursts in precise synchrony with the song ( Hahnloser , Kozhevnikov , and Fee 2002 ; Lynch et al . 2016 ; Picardo et al . 2016 ) . A population of neurons from HVC projects to the robust nucleus of the arcopallium ( RA ) , a pre - motor area , which then projects to motor neurons controlling respiratory and syringeal muscles ( Leonardo and Fee 2005 ; Simpson and Vicario 1990 ; Yu and Margoliash 1996 ) . A second input to RA comes from the lateral magnocellular nucleus of the anterior nidopallium ( LMAN ) . Unlike HVC and RA activity patterns , LMAN spiking is highly variable across diﬀerent renditions of the song ( Kao , Wright , and Doupe 2008 ; ¨Olveczky , Andalman , and Fee 2005 ) . LMAN is the output of the anterior forebrain pathway , a circuit involving the song - specialized basal ganglia ( Perkel 2004 ) . Because of the variability in its activity patterns , it was thought that LMAN’s role was simply to inject variability into the song ( ¨Olveczky , Andalman , and Fee 2005 ) . The resulting vocal experimentation would enable reinforcement - based learning . For this reason , prior models tended to treat LMAN as a pure Poisson noise generator , and assume that a reward signal is received directly in RA ( Fiete , Fee , and Seung 2007 ) . More recent evidence , however , suggests that the reward signal reaches Area X , the song - specialized basal ganglia , 1 a r X i v : 1608 . 08040v2 [ q - b i o . N C ] 8 M a r 2017 rather than RA ( Gadagkar et al . 2016 ; Hoﬀmann et al . 2016 ; Kubikova and Koˇsˇt´al 2010 ) . Taken together with the fact that LMAN ﬁring patterns are not uniformly random , but rather contain a corrective bias guiding plasticity in RA ( Andalman and Fee 2009 ; Warren et al . 2011 ) , this suggests that we should rethink our models of song acquisition . Here we build a general model of two - stage learning where one neural circuit “tutors” another . We develop a formalism for determining how the teaching signal should be adapted to a speciﬁc plasticity rule , to best instruct a student circuit to improve its performance at each learning step . We develop analytical results in a rate based model , and show through simulations that the general ﬁndings carry over to realistic spiking neurons . Applied to the vocal control circuit of songbirds , our model reproduces the observed changes in the spiking statistics of RA neurons as juvenile birds learn their song . Our framework also predicts how the LMAN signal should be adapted to properties of RA synapses . This prediction can be tested in future experiments . Our approach separates the mechanistic question of how learning is implemented from what the resulting learning rules are . We nevertheless demonstrate that a simple reinforcement learning algorithm suﬃces to implement the learning rule we propose . Our framework makes general predictions for how instructive signals are matched to plasticity rules whenever information is transferred between diﬀerent brain regions . m o t o r HVC RA LMAN Area X DLM conductor student tutor output reinforcement A B C D Figure 1 : Relation between the song system in zebra ﬁnches and our model . A . Diagram of the major brain regions involved in birdsong . B . Conceptual model inspired by the birdsong system . The line from output to tutor is dashed because the reinforcement signal can reach the tutor either directly or , as in songbirds , indirectly . C . Plasticity rule measured in bird RA ( measurement done in slice ) . When an HVC burst leads an LMAN burst by about 100 ms , the HVC – RA synapse is strengthened , while coincident ﬁring leads to suppression . Figure adapted from ( Mehaﬀey and Doupe 2015 ) . D . Plasticity rule in our model that mimics the Mehaﬀey and Doupe ( 2015 ) rule . 2 2 Results 2 . 1 Model We considered a model for information transfer that is composed of three sub - circuits : a conductor , a student , and a tutor ( see Figure 1B ) . The conductor provides input to the student in the form of temporally precise patterns . The goal of learning is for the student to convert this input to a predeﬁned output pattern . The tutor provides a signal that guides plasticity at the conductor – student synapses . For simplicity , we assumed that the conductor always presents the input patterns in the same order , and without repetitions . This allowed us to use the time t to label input patterns , making it easier to analyze the on - line learning rules that we studied . This model of learning is based on the logic implemented by the vocal circuits of the songbird ( Figure 1A ) . Relating this to the songbird , the conductor is HVC , the student is RA , and the tutor is LMAN . The song can be viewed as a mapping between clock - like HVC activity patterns and muscle - related RA outputs . The goal of learning is to ﬁnd a mapping that reproduces the tutor song . Birdsong provides interesting insights into the role of variability in tutor signals . If we focus solely on information transfer , the tutor output need not be variable ; it can deterministically provide the best instructive signal to guide the student . This , however , would require the tutor to have a detailed model of the student . More realistically , the tutor might only have access to a scalar representation of how successful the student rendition of the desired output is , perhaps in the form of a reward signal . A tutor in this case has to solve the so - called ‘credit assignment problem’—it needs to identify which student neurons are responsible for the reward . A standard way to achieve this is to inject variability in the student output and reinforcing the ﬁring of neurons that precede reward ( see for example ( Fiete , Fee , and Seung 2007 ) in the birdsong context ) . Thus , in our model , the tutor has a dual role of providing both an instructive signal and variability , as in birdsong . We described the output of our model using a vector y a ( t ) where a indexed the various output channels ( Figure 2A ) . In the context of motor control a might index the muscle to be controlled , or , more abstractly , diﬀerent features of the motor output , such as pitch and amplitude in the case of birdsong . The output y a ( t ) was a function of the activity of the student neurons s j ( t ) . The student neurons were in turn driven by the activity of the conductor neurons c i ( t ) . The student also received tutor signals to guide plasticity ; in the songbird , the guiding signals for each RA neuron come from several LMAN neurons ( Canady et al . 1988 ; Garst - Orozco , Babadi , and ¨Olveczky 2014 ; Herrmann and Arnold 1991 ) . In our model , we summarized the net input from the tutor to the j th student neuron as a single function g j ( t ) . We started with a rate - based implementation of the model ( Figure 2A ) that was analytically tractable but averaged over tutor variability . We further took the neurons to be in a linear operating regime ( Figure 2A ) away from the threshold and saturation present in real neurons . We then relaxed these conditions and tested our results in spiking networks with initial parameters selected to imitate measured ﬁring patterns in juvenile birds prior to song learning . The student circuit in both the rate - based and spiking models included a global inhibitory signal that helped to suppress excess activity driven by ongoing conductor and tutor input . Such recurrent inhibition is present in area RA of the bird ( Spiro , Dalva , and Mooney 1999 ) . In the spiking model we implemented the suppression as an activity - dependent inhibition , while for the analytic calculations we used a constant negative bias for the student neurons . 2 . 2 Learning in a rate - based model Learning in our model was enabled by plasticity at the conductor – student synapses that was modulated by signals from tutor neurons ( Figure 2B ) . Many diﬀerent forms of such hetero - synaptic plasticity have been observed . For example , in rate - based synaptic plasticity high tutor ﬁring rates lead to synaptic potentiation and low tutor ﬁring rates lead to depression ( Chistiakova , Bannon , et al . 2014 ; Chistiakova and Volgushev 2009 ) . In timing - dependent rules , such as the one recently measured by Mehaﬀey and Doupe ( 2015 ) in slices of zebra ﬁnch RA ( see Figure 1C ) , the relative arrival times of spike bursts from diﬀerent input pathways set the sign of synaptic change . To model learning that lies between these rate and timing - based extremes , we introduced a class of plasticity rules governed by two parameters α and β ( see also Methods and Figure 2B ) : dW ij dt = η ˜ c i ( t ) (cid:0) g j ( t ) − θ (cid:1) , ˜ c i ( t ) = (cid:90) t 0 dt (cid:48) c i ( t (cid:48) ) (cid:20) α τ 1 e − ( t − t (cid:48) ) / τ 1 − β τ 2 e − ( t − t (cid:48) ) / τ 2 (cid:21) , ( 1 ) 3 conductor student ∑ tutor output - 400 400 - 400 400 tutor conductor error A B C loss landscape synaptic plasticity ( loss function ) convolution ∑ ∑ ∑ ∑ ∑ t ( ms ) t ( ms ) ( ms ) ( ms ) Figure 2 : Schematic representation of our rate - based model . A . Conductor neurons ﬁre precisely - timed bursts , similar to HVC neurons in songbirds . Conductor and tutor activities , c ( t ) and g ( t ) , provide excitation to student neurons , which integrate these inputs and respond linearly , with activity s ( t ) . Student neurons also receive a constant inhibitory input , x inh . The output neurons linearly combine the activities from groups of student neurons using weights M aj . The linearity assumptions were made for mathematical convenience but are not essential for our qualitative results ( see Appendix ) . B . The conductor – student synaptic weights W ij are updated based on a plasticity rule that depends on two parameters , α and β , and two timescales , τ 1 and τ 2 ( see eq . ( 1 ) and Methods ) . The tutor signal enters this rule as a deviation from a constant threshold θ . The ﬁgure shows how synaptic weights change ( ∆ W ) for a student neuron that receives a tutor burst and a conductor burst separated by a short lag . Two diﬀerent choices of plasticity parameters are illustrated in the case when the threshold θ = 0 . C . The amount of mismatch between the system’s output and the target output is quantiﬁed using a loss ( error ) function . The ﬁgure sketches the loss landscape obtained by varying the synaptic weights W ij and calculating the loss function in each case ( only two of the weight axes are shown ) . The blue dot shows the lowest value of the loss function , corresponding to the best match between the motor output and the target , while the orange dot shows the starting point . The dashed line shows how learning would proceed in a gradient descent approach , where the weights change in the direction of steepest descent in the loss landscape . 4 where W ij is the weight of the synapse from the i th conductor to the j th student neuron , η is a learning rate , θ is a threshold on the ﬁring rate of tutor neurons , and τ 1 and τ 2 are timescales associated with the plasticity . This is similar to an STDP rule , except that the dependence on postsynaptic activity was replaced by dependence on the input from the tutor . Thus plasticity acts heterosynaptically , with activation of the tutor – student synapse controlling the change in the conductor – student synaptic weight . The timescales τ 1 and τ 2 , as well as the coeﬃcients α and β , can be thought of as eﬀective parameters describing the plasticity observed in student neurons . As such , they do not necessarily have a simple correspondence in terms of the biochemistry of the plasticity mechanism , and the framework we describe here is not speciﬁcally tied to such an interpretation . If we set α or β to zero in our rule , eq . ( 1 ) , the sign of the synaptic change is determined solely by the ﬁring rate of the tutor g j ( t ) as compared to a threshold , reproducing the rate rules observed in experiments . When α / β ≈ 1 , if the conductor leads the tutor , potentiation occurs , while coincident signals lead to depression ( Figure 2B ) , which mimics the empirical ﬁndings from ( Mehaﬀey and Doupe 2015 ) . For general α and β , the sign of plasticity is controlled by both the ﬁring rate of the tutor relative to the baseline , and by the relative timing of tutor and conductor . The overall scale of the parameters α and β can be absorbed into the learning rate η and so we set α − β = 1 in all our simulations without loss of generality ( see Methods ) . Note that if α and β are both large , it can be that α − β = 1 and α / β ≈ 1 also , as needed to realize the Mehaﬀey and Doupe ( 2015 ) curve . We can ask how the conductor – student weights W ij ( Figure 2A ) should change in order to best improve the output y a ( t ) . We ﬁrst need a loss function L that quantiﬁes the distance between the current output y a ( t ) and the target ¯ y a ( t ) ( Figure 2C ) . We used a quadratic loss function , but other choices can also be incorporated into our framework ( see Appendix ) . Learning should change the synaptic weights so that the loss function is minimized , leading to a good rendition of the targeted output . This can be achieved by changing the synaptic weights in the direction of steepest descent of the loss function ( Figure 2C ) . We used the synaptic plasticity rule from eq . ( 1 ) to calculate the overall change of the weights , ∆ W ij , over the course of the motor program . This is a function of the time course of the tutor signal , g j ( t ) . Not every choice for the tutor signal leads to motor output changes that best improve the match to the target . Imposing the condition that these changes follow the gradient descent procedure described above , we derived the tutor signal that was best matched to the student plasticity rule ( detailed derivation in Methods ) . The result is that the best tutor for driving gradient descent learning must keep track of the motor error (cid:15) j ( t ) = (cid:88) a M aj ( y a ( t ) − ¯ y a ( t ) ) ( 2 ) integrated over the recent past g j ( t ) = θ − ζ α − β 1 τ tutor (cid:90) t 0 (cid:15) j ( t (cid:48) ) e − ( t − t (cid:48) ) / τ tutor dt (cid:48) , ( 3 ) where M aj are the weights describing the linear relationship between student activities and motor outputs ( Figure 2A ) and ζ is a learning rate . Moreover , for eﬀective learning , the timescale τ tutor appearing in eq . ( 3 ) , which quantiﬁes the timescale on which error information is integrated into the tutor signal , should be related to the synaptic plasticity parameters according to τ tutor = τ ∗ tutor , where τ ∗ tutor ≡ ατ 1 − βτ 2 α − β ( 4 ) is the optimal timescale for the error integration . In short , motor learning with a heterosynaptic plasticity rule requires convolving the motor error with a kernel whose timescale is related to the structure of the plasticity rule , but is otherwise independent of the motor program . 1 As explained in more detail in Methods , this result is derived in an approximation that assumes that the tutor signal does not vary signiﬁcantly over timescales of the order of the student timescales τ 1 and τ 2 . Given eq . ( 4 ) , this implies that we are assuming τ tutor (cid:29) τ 1 , 2 . This is a reasonable approximation because variations in the tutor signal that are much faster than the student timescales τ 1 , 2 have little eﬀect on learning since the plasticity rule ( 1 ) blurs conductor inputs over these timescales . 1 We thank the referees for suggesting this way of describing our results . 5 2 . 3 Matched vs . unmatched learning Our rate - based model predicts that when the timescale on which error information is integrated into the tutor signal ( τ tutor ) is matched to the student plasticity rule as described above , learning will proceed eﬃciently . A mismatched tutor should slow or disrupt convergence to the desired output . To test this , we numerically simulated the birdsong circuit using the linear model from Figure 2A with a motor output y a ﬁltered to more realistically reﬂect muscle response times ( see Methods ) . We selected plasticity rules as described in eq . ( 1 ) and Figure 2B and picked a target output pattern to learn . The target was chosen to resemble recordings of air - sac pressure from singing zebra ﬁnches in terms of smoothness and characteristic timescales ( Veit , Aronov , and Fee 2011 ) , but was otherwise arbitrary . In our simulations , the output typically involved two diﬀerent channels , each with its own target , but for brevity , in ﬁgures we typically showed the output from only one of these . For our analytical calculations , we made a series of assumptions and approximations meant to enhance tractability , such as linearity of the model and a focus on the regime τ tutor (cid:29) τ 1 , 2 . These constraints can be lifted in our simulations , and indeed below we test our numerical model in regimes that go beyond the approximations made in our derivation . In many cases , we found that the basic ﬁndings regarding tutor – student matching from our analytical model remain true even when some of the assumptions we used to derive it no longer hold . We tested tutors that were matched or mismatched to the plasticity rule to see how eﬀectively they instructed the student . Figure 3A and online Video 1 show convergence with a matched tutor when the sign of plasticity is determined by the tutor’s ﬁring rate . We see that the student output rapidly converged to the target . Figure 3B and online Video 2 show convergence with a matched tutor when the sign of plasticity is largely determined by the relative timing of the tutor signal and the student output . We see again that the student converged steadily to the desired output , but at a somewhat slower rate than in Figure 3A . To test the eﬀects of mismatch between tutor and student , we used tutors with timescales that did not match eq . ( 4 ) . All student plasticity rules had the same eﬀective time constants τ 1 and τ 2 , but diﬀerent parameters α and β ( see eq . ( 1 ) ) , subject to the constraint α − β = 1 described in section 2 . 2 . Diﬀerent tutors had diﬀerent memory time scales τ tutor ( eq . ( 3 ) ) . Figures 3C and 3D demonstrate that learning was more rapid for well - matched tutor - student pairs ( the diagonal neighborhood , where τ tutor ≈ τ ∗ tutor ) . When the tutor error integration timescale was shorter than the matched value in eq . ( 4 ) , τ tutor < τ ∗ tutor , learning was often completely disrupted ( many pairs below the diagonal in Figures 3C and 3D ) . When the tutor error integration timescale was longer than the matched value in eq . ( 4 ) , τ tutor > τ ∗ tutor learning was slowed down . Figure 3C also shows that a certain amount of mismatch between the tutor error integration timescale τ tutor and the matched timescale τ ∗ tutor implied by the student plasticity rule is tolerated by the system . Interestingly , the diagonal band over which learning is eﬀective in Figure 3C is roughly of constant width—note that the scale on both axes is logarithmic , so that this means that the tutor error integration timescale τ tutor has to be within a constant factor of the optimal timescale τ ∗ tutor for good learning . We also see that the breakdown in learning is more abrupt when τ tutor < τ ∗ tutor than in the opposite regime . An interesting feature of the results from Figures 3C , 3D is that the diﬀerence in performance between matched and mismatched pairs becomes less pronounced for timescales shorter than about 100 ms . This is due to the fact that the plasticity rule ( eq . ( 1 ) ) implicitly smooths over timescales of the order of τ 1 , 2 , which in our simulations were equal to τ 1 = 80 ms , τ 2 = 40 ms . Thus , variations of the tutor signal on shorter timescales have little eﬀect on learning . Using diﬀerent values for the eﬀective timescales τ 1 , 2 describing the plasticity rule can increase or decrease the range of parameters over which learning is robust against tutor – student mismatches ( see Appendix ) . 2 . 4 Robust learning with nonlinearities In the model above , ﬁring rates for the tutor were allowed to grow as large as necessary to implement the most eﬃcient learning . However , the ﬁring rates of realistic neurons typically saturate at some ﬁxed bound . To test the eﬀects of this nonlinearity in the tutor , we passed the ideal tutor activity ( 3 ) through a sigmoidal nonlinearity , ˜ g j ( t ) = θ − ρ tanh ζ α − β 1 τ tutor (cid:90) t 0 (cid:15) j ( t (cid:48) ) e − ( t − t (cid:48) ) / τ tutor dt (cid:48) . ( 5 ) where 2 ρ is the range of ﬁring rates . We typically chose θ = ρ = 80 Hz to constrain the rates to the range 0 – 160 Hz ( Garst - Orozco , Babadi , and ¨Olveczky 2014 ; ¨Olveczky , Andalman , and Fee 2005 ) . Learning slowed 6 10 20 40 80 1603206401280256051201024020480 τ tutor 102040801603206401280256051201024020480 τ ∗ t u t o r = α τ 1 − β τ 2 α − β 10 20 40 80 1603206401280256051201024020480 τ tutor 102040801603206401280256051201024020480 τ ∗ t u t o r = α τ 1 − β τ 2 α − β 0 . 50 1 . 00 2 . 00 5 . 00 10 . 00 0 50 100 150 200 250 repetitions 0 2 4 6 8 10 12 14 e rr o r α = 7 . 0 , β = 6 . 0 , τ tutor = 320 . 0 time 010203040506070 o u t pu t target output 0 50 100 150 200 250 repetitions 0 2 4 6 8 10 12 14 e rr o r α = 0 . 0 , β = − 1 . 0 , τ tutor = 40 . 0 time 01020304050607080 o u t pu t target output 0 600 A B C D 0 600 worse better Figure 3 : Learning with matched or mismatched tutors in rate - based simulations . A . Error trace showing how the average motor error evolved with the number of repetitions of the motor program for a rate - based ( α = 0 ) plasticity rule paired with a matching tutor . ( See online Video 1 . ) B . The error trace and ﬁnal motor output shown for a timing - based learning rule matched by a tutor with a long integration timescale . ( See online Video 2 . ) In both A and B the inset shows the ﬁnal motor output for one of the two output channels ( thick orange line ) compared to the target output for that channel ( dotted black line ) . The output on the ﬁrst rendition and at two other stages of learning indicated by orange arrows on the error trace are also shown as thin orange lines . C . Eﬀects of mismatch between student and tutor on reproduction accuracy . The heatmap shows the ﬁnal reproduction error of the motor output after 1000 learning cycles in a rate - based simulation where a student with parameters α , β , τ 1 , and τ 2 was paired with a tutor with memory timescale τ tutor . On the y axis , τ 1 and τ 2 were kept ﬁxed at 80 ms and 40 ms , respectively , while α and β were varied ( subject to the constraint α − β = 1 ; see text ) . Diﬀerent choices of α and β lead to diﬀerent optimal timescales τ ∗ tutor according to eq . ( 4 ) . The diagonal elements correspond to matched tutor and student , τ tutor = τ ∗ tutor . Note that the color scale is logarithmic . D . Error evolution curves as a function of the mismatch between student and tutor . Each plot shows how the error in the motor program changed during 1000 learning cycles for the same conditions as those shown in the heatmap . The region shaded in light pink shows simulations where the mismatch between student and tutor led to a deteriorating instead of improving performance during learning . Video 1 : Evolution of motor output during learning in a rate - based simulation using a rate - based ( α = 0 ) plasticity rule paired with a matching tutor . This video relates to Figure 3A . Video 2 : Evolution of motor output during learning in a rate - based simulation using a timing - based ( α ≈ β ) plasticity rule paired with a matching tutor . This video relates to Figure 3B . 7 down with this change ( Figure 4A and online Video 3 ) as a result of the tutor ﬁring rates saturating when the mismatch between the motor output and the target output was large . However , the accuracy of the ﬁnal rendition was not aﬀected by saturation in the tutor ( Figure 4A , inset ) . An interesting eﬀect occurred when the ﬁring rate constraint was imposed on a matched tutor with a long memory timescale . When this happened and the motor error was large , the tutor signal saturated and stopped growing in relation to the motor error before the end of the motor program . In the extreme case of very long integration timescales , learning became sequential : early features in the output were learned ﬁrst , before later features were addressed , as in Figure 4B and online Video 4 . This is reminiscent of the learning rule described in ( Memmesheimer et al . 2014 ) . 0 600 time 0 20 40 60 80 Stage 1 target output 0 600 time 0 20 40 60 80 Stage 17 target output 0 600 time 0 20 40 60 80 Stage 34 target output 0 50 100 150 200 250 repetition 0 5 10 15 20 e rr o r time o u t p u t target no constraint with constraint A B 0 600 Figure 4 : Eﬀects of adding a constraint on the tutor ﬁring rate to the simulations . A . Learning was slowed down by the ﬁring rate constraint , but the accuracy of the ﬁnal rendition stayed the same ( inset , shown here for one of two simulated output channels ) . Here α = 0 , β = − 1 , and τ tutor = τ ∗ tutor = 40 ms . ( See online Video 3 . ) B . Sequential learning occurred when the ﬁring rate constraint was imposed on a matched tutor with a long memory scale . The plots show the evolution of the motor output for one of the two channels that were used in the simulation . Here α = 24 , β = 23 , and τ tutor = τ ∗ tutor = 1000 ms . ( See online Video 4 . ) Video 3 : Eﬀects of adding a constraint on tutor ﬁring rates on the evolution of motor output during learning in a rate - based simulation . The plasticity rule here was rate - based ( α = 0 ) . This video relates to Figure 4A . Video 4 : Evolution of the motor output showing sequential learning in a rate - based simulation , which occurs when the ﬁring rate constraint is imposed on a tutor with a long memory timescale . This video relates to Figure 4B . Nonlinearities can similarly aﬀect the activities of student neurons . Our model can be readily extended to describe eﬃcient learning even in this case . The key result is that for eﬃcient learning to occur , the synaptic plasticity rule should depend not just on the tutor and conductor , but also on the activity of the postsynaptic student neurons ( details in Appendix ) . Such dependence on postsynaptic activity is commonly seen in experiments ( Chistiakova , Bannon , et al . 2014 ; Chistiakova and Volgushev 2009 ) . The relation between student neuron activations s j ( t ) and motor outputs y a ( t ) ( Figure 2A ) is in general also nonlinear . Compared to the linear assumption that we used , the eﬀect of a monotonic nonlinearity , 8 y a = N a ( (cid:80) j M aj s j ) , with N a an increasing function , is similar to modifying the loss function L , and does not signiﬁcantly change our results ( see Appendix ) . We also checked that imposing a rectiﬁcation constraint that conductor – student weights W ij must be positive does not modify our results either ( see Appendix ) . This shows that our model continues to work with biologically realistic synapses that cannot change sign from excitatory to inhibitory during learning . 2 . 5 Spiking neurons and birdsong To apply our model to vocal learning in birds , we extended our analysis to networks of spiking neurons . Juvenile songbirds produce a “babble” that converges through learning to an adult song strongly resembling the tutor song . This is reﬂected in the song - aligned spiking patterns in pre - motor area RA , which become more stereotyped and cluster in shorter , better - deﬁned bursts as the bird matures ( Figure 5A ) . We tested whether our model could reproduce key statistics of spiking in RA over the course of song learning . In this context , our theory of eﬃcient learning , derived in a rate - based scenario , predicts a speciﬁc relation between the teaching signal embedded in LMAN ﬁring patterns , and the plasticity rule implemented in RA . We tested whether these predictions continued to hold in the spiking context . Following the experiments of Hahnloser , Kozhevnikov , and Fee ( 2002 ) , we modeled each neuron in HVC ( the conductor ) as ﬁring one short , precisely timed burst of 5 - 6 spikes at a single moment in the motor program . Thus the population of HVC neurons produced a precise timebase for the song . LMAN ( tutor ) neurons are known to have highly variable ﬁring patterns that facilitate experimentation , but also contain a corrective bias ( Andalman and Fee 2009 ) . Thus we modeled LMAN as producing inhomogeneous Poisson spike trains with a time - dependent ﬁring rate given by eq . ( 5 ) in our model . Although biologically there are several LMAN neurons projecting to each RA neuron , we again simpliﬁed by “summing” the LMAN inputs into a single , eﬀective tutor neuron , similarly to the approach in ( Fiete , Fee , and Seung 2007 ) . The LMAN - RA synapses were modeled in a current - based approach as a mixture of AMPA and NMDA receptors , following the songbird data ( Garst - Orozco , Babadi , and ¨Olveczky 2014 ; Stark and Perkel 1999 ) . The initial weights for all synapses were tuned to produce RA ﬁring patterns resembling juvenile birds ( ¨Olveczky , Otchy , et al . 2011 ) , subject to constraints from direct measurements in slice recordings ( Garst - Orozco , Babadi , and ¨Olveczky 2014 ) ( see Methods for details , and Figure 5B for a comparison between neural recordings and spiking in our model ) . In contrast to the constant inhibitory bias that we used in our rate - based simulations , for the spiking simulations we chose an activity - dependent global inhibition for RA neurons . We also tested that a constant bias produced similar results ( see Appendix ) . Synaptic strength updates followed the same two - timescale dynamics that was used in the rate - based models ( Figure 2B ) . The ﬁring rates c i ( t ) and g j ( t ) that appear in the plasticity equation were calculated in the spiking model by ﬁltering the spike trains from conductor and tutor neurons with exponential kernels . The synaptic weights were constrained to be non - negative . ( See Methods for details . ) As long as the tutor error integration timescale was not too large , learning proceeded eﬀectively when the tutor error integration timescale and the student plasticity rule were matched ( see Figure 5C and online Video 5 ) , with mismatches slowing down or abolishing learning , just as in our rate - based study ( compare Figure 5D with Figure 3C ) . The rate of learning and the accuracy of the trained state were lower in the spiking model compared to the rate - based model . The lower accuracy arises because the tutor neurons ﬁre stochastically , unlike the deterministic neurons used in the rate - based simulations . The stochastic nature of the tutor ﬁring also led to a decrease in learning accuracy as the tutor error integration timescale τ tutor increased ( Figure 5D ) . This happens through two related eﬀects : ( 1 ) the signal - to - noise ratio in the tutor guiding signal decreases as τ tutor increases once the tutor error integration timescale is longer than the duration T of the motor program ( see Appendix ) ; and ( 2 ) the ﬂuctuations in the conductor – student weights lead to some weights getting clamped at 0 due to the positivity constraint , which leads to the motor program overshooting the target ( see Appendix ) . The latter eﬀect can be reduced by either allowing for negative weights , or changing the motor output to a push - pull architecture in which some student neurons enhance the output while others inhibit it . The signal - to - noise ratio eﬀect can be attenuated by increasing the gain of the tutor signal , which inhibits early learning , but improves the quality of the guiding signal in the latter stages of the learning process . It is also worth emphasizing that these eﬀects only become relevant once the tutor error integration timescale τ tutor becomes signiﬁcantly longer than the duration of the motor program , T , which for a birdsong motif would be around 1 second . Spiking in our model tends to be a little more regular than that in the recordings ( compare Figure 5A 9 10 20 40 80 1603206401280256051201024020480 τ tutor 102040801603206401280256051201024020480 τ ∗ t u t o r = α τ 1 − β τ 2 α − β 0 . 50 1 . 00 2 . 00 5 . 00 10 . 00 0 100 200 300 400 500 600 repetitions 0 2 4 6 8 10 12 14 e rr o r α = 1 . 0 , β = 0 . 0 , τ tutor = 80 . 0 time 0102030405060708090 o u t pu t target output 0 600 0 100 200 300 400 500 600 t ( ms ) 0 100 200 300 400 500 600 juvenile adult juvenile adult A B C D 0 100 200 300 400 500 0 100 200 300 400 500 t ( ms ) worse better Figure 5 : Results from simulations in spiking neural networks . A . Spike patterns recorded from zebra ﬁnch RA during song production , for a juvenile ( top ) and an adult ( bottom ) . Each color corresponds to a single neuron , and the song - aligned spikes for six renditions of the song are shown . Adapted from ( ¨Olveczky , Otchy , et al . 2011 ) . B . Spike patterns from model student neurons in our simulations , for the untrained ( top ) and trained ( bottom ) models . The training used α = 1 , β = 0 , and τ tutor = 80 ms , and ran for 600 iterations of the song . Each model neuron corresponds to a diﬀerent output channel of the simulation . In this case , the targets for each channel were chosen to roughly approximate the time course observed in the neural recordings . C . Progression of reproduction error in the spiking simulation as a function of the number of repetitions for the same conditions as in panel B . The inset shows the accuracy of reproduction in the trained model for one of the output channels . ( See online Video 5 . ) D . Eﬀects of mismatch between student and tutor on reproduction accuracy in the spiking model . The heatmap shows the ﬁnal reproduction error of the motor output after 1000 learning cycles in a spiking simulation where a student with parameters α , β , τ 1 , and τ 2 was paired with a tutor with memory timescale τ tutor . On the y axis , τ 1 and τ 2 were kept ﬁxed at 80 ms and 40 ms , respectively , while α and β were varied ( subject to the constraint α − β = 1 ; see section 2 . 2 ) . Diﬀerent choices of α and β lead to diﬀerent optimal timescales τ ∗ tutor according to eq . ( 4 ) . The diagonal elements correspond to matched tutor and student , τ tutor = τ ∗ tutor . Note that the color scale is logarithmic . Video 5 : Evolution of motor output during learning in a spiking simulation . The plasticity rule parameters were α = 1 , β = 0 , and the tutor had a matching timescale τ tutor = τ ∗ tutor = 80 ms . This video relates to Figure 5C . 10 and Figure 5B ) . This could be due to sources of noise that are present in the brain which we did not model . One detail that our model does not capture is the fact that many LMAN spikes occur in bursts , while in our simulation LMAN ﬁring is Poisson . Bursts are more likely to produce spikes in downstream RA neurons particularly because of the NMDA dynamics , and thus a bursty LMAN will be more eﬀective at injecting variability into RA ( Kojima , Kao , and Doupe 2013 ) . Small inaccuracies in aligning the recorded spikes to the song are also likely to contribute apparent variability between renditions in the experiment . Indeed , some of the variability in Figure 5A looks like it could be due to time warping and global time shifts that were not fully corrected . 2 . 6 Robust learning with credit assignment errors The calculation of the tutor output in our rule involved estimating the motor error (cid:15) j from eq . ( 2 ) . This required knowledge of the assignment between student activities and motor output , which in our model was represented by the matrix M aj ( Figure 2A ) . In our simulations , we typically chose an assignment in which each student neuron contributed to a single output channel , mimicking the empirical ﬁndings for neurons in bird RA . Mathematically , this implies that each column of M aj contained a single non - zero element . In Figure 6A , we show what happened in the rate - based model when the tutor incorrectly assigned a certain fraction of the neurons to the wrong output . Speciﬁcally , we considered two output channels , y 1 and y 2 , with half of the student neurons contributing only to y 1 and the other half contributing only to y 2 . We then scrambled a fraction ρ of this assignment when calculating the motor error , so that the tutor eﬀectively had an imperfect knowledge of the student – output relation . Figure 6A shows that learning is robust to this kind of mis - assignment even for fairly large values of the error fraction ρ up to about 40 % , but quickly deteriorates as this fraction approaches 50 % . Due to environmental factors that aﬀect development of diﬀerent individuals in diﬀerent ways , it is unlikely that the student – output mapping can be innate . As such , the tutor circuit must learn the mapping . Indeed , it is known that LMAN in the bird receives an indirect evaluation signal via Area X , which might be used to eﬀect this learning ( Andalman and Fee 2009 ; Gadagkar et al . 2016 ; Hoﬀmann et al . 2016 ; Kubikova and Koˇsˇt´al 2010 ) . One way in which this can be achieved is through a reinforcement paradigm . We thus considered a learning rule where the tutor circuit receives a reward signal that enables it to infer the student – output mapping . In general the output of the tutor circuit should depend on an integral of the motor error , as in eq . ( 3 ) , to best instruct the student . For simplicity , we start with the memory - less case , τ tutor = 0 , in which only the instantaneous value of the motor error is reﬂected in the tutor signal ; we then show how to generalize this for τ tutor > 0 . As before , we took the tutor neurons to ﬁre Poisson spikes with time - dependent rates f j ( t ) , which were initialized arbitrarily . Because of stochastic ﬂuctuations , the actual tutor activity on any given trial , g j ( t ) , diﬀers somewhat from the average , ¯ g j ( t ) . Denoting the diﬀerence by ξ j ( t ) = g j ( t ) − ¯ g j ( t ) , the update rule for the tutor ﬁring rates was given by ∆ f j ( t ) = η tutor ( R ( t ) − ¯ R ) ξ j ( t ) , ( 6 ) where η tutor is a learning rate , R ( t ) is the instantaneous reward signal , and ¯ R is its average over recent renditions of the motor program . In our implementation , ¯ R is obtained by convolving R ( t ) with an exponential kernel ( timescale = 1 second ) . The reward R ( t max ) at the end of one rendition becomes the baseline at the start of the next rendition R ( 0 ) . The baseline ¯ g j ( t ) of the tutor activity is calculated by averaging over recent renditions of the song with exponentially decaying weights ( one e - fold of decay for every 5 renditions ) . Further implementation details are available in our code at https : / / github . com / ttesileanu / twostagelearning . The intuition behind this rule is that , whenever a ﬂuctuation in the tutor activity leads to better - than - average reward ( R ( t ) > ¯ R ) , the tutor ﬁring rate changes in the direction of the ﬂuctuation for subsequent trials , “freezing in” the improvement . Conversely , the ﬁring rate moves away from the directions in which ﬂuctuations tend to reduce the reward . To test our learning rule , we ran simulations using this reinforcement strategy and found that learning again converges to an accurate rendition of the target output ( Figure 6B , inset and online Video 6 ) . The number of repetitions needed for training is greatly increased compared to the case in which the credit assignment is assumed known by the tutor circuit ( compare Figure 6B to Figure 5C ) . This is because the tutor needs to use many training rounds for experimentation before it can guide the conductor – student plasticity . The rate of learning in our model is similar to the songbird ( i . e . , order 10 000 repetitions for learning , given that a zebra 11 0 2000 4000 6000 8000 10000 repetitions 0 50 100 150 200 c o ndu c t o r i npu t s p e r s t ud e n t n e u r o n 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 fraction mismatch 0 2 4 6 8 10 ﬁn a l e rr o r 0 2000 4000 6000 8000 repetitions 0 2 4 6 8 10 12 14 e rr o r α = 10 . 0 , β = 9 . 0 , τ tutor = 440 . 0 time 0 20 40 60 80 o u t pu t target output 0 2000 4000 6000 8000 repetitions 0 2 4 6 8 10 12 14 e rr o r α = 1 . 0 , β = 0 . 0 , τ tutor = 0 . 0 time 0 20 40 60 80 o u t pu t target output 0 600 A B C D 0 600 Figure 6 : Credit assignment and reinforcement learning . A . Eﬀects of credit mis - assignment on learning in a rate - based simulation . Here , the system learned output sequences for two independent channels . The student – output weights M aj were chosen so that the tutor wrongly assigned a fraction of student neurons to an output channel diﬀerent from the one it actually mapped to . The graph shows how the accuracy of the motor output after 1000 learning steps depended on the fraction of mis - assigned credit . B . Learning curve and trained motor output ( inset ) for one of the channels showing two - stage reinforcement - based learning for the memory - less tutor ( τ tutor = 0 ) . The accuracy of the trained model is as good as in the case where the tutor was assumed to have a perfect model of the student – output relation . However , the speed of learning is reduced . ( See online Video 6 . ) C . Learning curve and trained motor output ( inset ) for one of the output channels showing two - stage reinforcement - based learning when the tutor circuit needs to integrate information about the motor error on a certain timescale . Again , learning was slow , but the accuracy of the trained state was unchanged . ( See online Video 7 . ) D . Evolution of the average number of HVC inputs per RA neuron with learning in a reinforcement example . Synapses were considered pruned if they admitted a current smaller than 1 nA after a pre - synaptic spike in our simulations . Video 6 : Evolution of motor output during learning in a spiking simulation with a reinforcement - based tutor . Here the tutor was memory - less ( τ tutor = 0 ) . This video relates to Figure 6B . Video 7 : Evolution of motor output during learning in a spiking simulation with a reinforcement - based tutor . Here the tutor needed to integrate information about the motor error on a timescale τ tutor = 440 ms . This video relates to Figure 6C . 12 ﬁnch typically sings about 1000 repetitions of its song each day , and takes about one month to fully develop adult song ) . Because of the extra training time needed for the tutor to adapt its signal , the motor output in our reward - based simulations tends to initially overshoot the target ( leading to the kink in the error at around 2000 repetitions in Figure 6B ) . Interestingly , the subsequent reduction in output that leads to convergence of the motor program , combined with the positivity constraint on the synaptic strengths , leads to many conductor – student connections being pruned ( Figure 6D ) . This mirrors experiments on songbirds , where the number of connections between HVC and RA ﬁrst increases with learning and then decreases ( Garst - Orozco , Babadi , and ¨Olveczky 2014 ) . The reinforcement rule described above responds only to instantaneous values of the reward signal and tutor ﬁring rate ﬂuctuations . In general , eﬀective learning requires that the tutor keep a memory trace of its activity over a timescale τ tutor > 0 , as in eq . ( 4 ) . To achieve this in the reinforcement paradigm , we can use a simple generalization of eq . ( 6 ) where the update rule is ﬁltered over the tutor memory timescale : ∆ f j ( t ) = η tutor 1 τ tutor (cid:90) t dt (cid:48) ( R ( t (cid:48) ) − ¯ R ) ξ j ( t (cid:48) ) e − ( t − t (cid:48) ) / τ tutor . ( 7 ) We tested that this rule leads to eﬀective learning when paired with the corresponding student , i . e . , one for which eq . ( 4 ) is obeyed ( Figure 6C and online Video 7 ) . The reinforcement rules proposed here are related to the learning rules from ( Fiete , Fee , and Seung 2007 ; Fiete and Seung 2006 ) and ( Farries and Fairhall 2007 ) . However , those models focused on learning in a single pass , instead of the two - stage architecture that we studied . In particular , in ( Fiete , Fee , and Seung 2007 ) , area LMAN was assumed to generate pure Poisson noise and reinforcement learning took place at the HVC – RA synapses . In our model , which is in better agreement with recent evidence regarding the roles of RA and LMAN in birdsong ( Andalman and Fee 2009 ) , reinforcement learning ﬁrst takes place in the anterior forebrain pathway ( AFP ) , for which LMAN is the output . A reward - independent heterosynaptic plasticity rule then solidiﬁes the information in RA . In our simulations , tutor neurons ﬁre Poisson spikes with speciﬁc time - dependent rates which change during learning . The timecourse of the ﬁring rates in each repetition must then be stored somewhere in the brain . In fact , in the songbird , there are indirect projections from HVC to LMAN , going through the basal ganglia ( Area X ) and the dorso - lateral division of the medial thalamus ( DLM ) in the anterior forebrain pathway ( Figure 1A ) ( Perkel 2004 ) . These synapses could store the required time - dependence of the tutor ﬁring rates . In addition , the same synapses can provide the timebase input that would ensure synchrony between LMAN ﬁring and RA output , as necessary for learning . Our reinforcement learning rule for the tutor area , eq . ( 6 ) , can be viewed as an eﬀective model for plasticity in the projections between HVC , Area X , DLM , and LMAN , as in ( Fee and Goldberg 2011 ) . In this picture , the indirect HVC – LMAN connections behave somewhat like the “hedonistic synapses” from ( Seung 2003 ) , though we use a simpler synaptic model here . Implementing the integral from eq . ( 7 ) would require further recurrent circuitry in LMAN which is beyond the scope of this paper , but would be interesting to investigate in future work . 3 Discussion We built a two - stage model of learning in which one area ( the student ) learns to perform a sequence of actions under guidance from a tutor area . This architecture is inspired by the song system of zebra ﬁnches , where area LMAN provides a corrective bias to the song that is then consolidated in the HVC – RA synapses . Using an approach rooted in the eﬃcient coding literature , we showed analytically that , in a simple model , the tutor output that is most likely to lead to eﬀective learning by the student involves an integral over the recent magnitude of the motor error . We found that eﬃciency requires that the timescale for this integral should be related to the synaptic plasticity rule used by the student . Using simulations , we tested our ﬁndings in more general settings . In particular , we demonstrated that tutor - student matching is important for learning in a spiking - neuron model constructed to reproduce spiking patterns similar to those measured in zebra ﬁnches . Learning in this model changes the spiking statistics of student neurons in realistic ways , for example , by producing more bursty , stereotyped ﬁring events as learning progresses . Finally , we showed how the tutor can build its error - correcting signal by means of reinforcement learning . 13 If the birdsong system supports eﬃcient learning , our model can predict the temporal structure of the ﬁring patterns of RA - projecting LMAN neurons , given the plasticity rule implemented at the HVC – RA synapses . These predictions can be directly tested by recordings from LMAN neurons in singing birds , assuming that a good measure of motor error is available , and that we can estimate how the neurons contribute to this error . Moreover , recordings from a tutor circuit , such as LMAN , could be combined with a measure of motor error to infer the plasticity rule in a downstream student circuit , such as RA . This could be compared with direct measurements of the plasticity rule obtained in slice . Conversely , knowledge of the student plasticity rule could be used to predict the time - dependence of tutor ﬁring rates . According to our model , the ﬁring rate should reﬂect the integral of the motor error with the timescale predicted by the model . A diﬀerent approach would be to artiﬁcially tutor RA by stimulating LMAN neurons electrically or optogenetically . We would predict that if the tutor signal is delivered appropriately ( e . g . , in conjunction with a particular syllable ( Tumer and Brainard 2007 ) ) , then the premotor bias produced by the stimulation should become incorporated into the motor pathway faster when the timescale of the artiﬁcial LMAN signal is properly matched to the RA synaptic plasticity rule . Our model can be applied more generally to other systems in the brain exhibiting two - stage learning , such as motor learning in mammals . If the plasticity mechanisms in these systems are diﬀerent from those in songbirds , our predictions for the structure of the guiding signal will vary correspondingly . This would allow a further test of our model of “eﬃcient learning” in the brain . It is worth pointing out that our model was derived assuming a certain hierarchy among the timescales that model the student plasticity and the tutor signal . A mismatch between the model predictions and observations could also imply a breakdown of these approximations , rather than failure of the hypothesis that the particular system under study evolved to support eﬃcient learning . Of course our analysis could be extended by relaxing these assumptions , for example by keeping more terms in the Taylor expansion that we used in our derivation of the matched tutor signal . Applied to birdsong , our model is best seen as a mechanism for learning song syllables . The ordering of syllables in song motifs seems to have a second level of control within HVC and perhaps beyond ( Basista et al . 2014 ; Hamaguchi , Tanaka , and Mooney 2016 ) . Songs can also be distorted by warping their timebase through changes in HVC ﬁring without alterations of the HVC – RA connectivity ( Ali et al . 2013 ) . In view of these phenomena , it would be interesting to incorporate our model into a larger hierarchical framework in which the sequencing and temporal structure of the syllables are also learned . A model of transitions between syllables can be found in ( Doya and Sejnowski 2000 ) , where the authors use a “weight perturbation” optimization scheme in which each HVC – RA synaptic weight is perturbed individually . We did not follow this approach because there is no plausible mechanism for LMAN to provide separate guidance to each HVC – RA synapse ; in particular , there are not enough LMAN neurons ( Fiete , Fee , and Seung 2007 ) . In this paper we assumed a two - stage architecture for learning , inspired by birdsong . An interesting question is whether and under what conditions such an architecture is more eﬀective than a single - step model . Possibly , having two stages is better when a single tutor area is responsible for training several diﬀerent dedicated controllers , as is likely the case in motor learning . It would then be beneﬁcial to have an area that can learn arbitrary behaviors , perhaps at the cost of using more resources and having slower reaction times , along with the ability to transfer these behaviors into low - level circuitry that is only capable of producing stereotyped motor programs . The question then arises whether having more than two levels in this hierarchy could be useful , what the other levels might do , and whether such hierarchical learning systems are implemented in the brain . Acknowledgments We would like to thank Serena Bradde for fruitful discussions during the early stages of this work . We also thank Xuexin Wei and Christopher Glaze for useful discussions . We are grateful to Timothy Otchy for providing us with some of the data we used in this paper . During this work VB was supported by NSF grant PHY - 1066293 at the Aspen Center for Physics and by NSF Physics of Living Systems grant PHY - 1058202 . TT was supported by the Swartz Foundation . 14 A Methods A . 1 Equations for rate - based model The basic equations we used for describing our rate - based model ( Figure 2A ) are the following : y a ( t ) = (cid:88) j M aj s j ( t ) , s j ( t ) = (cid:88) i W ij c i ( t ) + wg j ( t ) − x inh . ( A . 1 ) In simulations , we further ﬁltered the output using an exponential kernel , ˜ y a ( t ) = (cid:88) j M aj (cid:90) t 0 s j ( t (cid:48) ) e − ( t − t (cid:48) ) / τ out dt (cid:48) , ( A . 2 ) with a timescale τ out that we typically set to 25 ms . The smoothing produces more realistic outputs by mimicking the relatively slow reaction time of real muscles , and stabilizes learning by ﬁltering out high - frequency components of the motor output . The latter interfere with learning because of the delay between the eﬀect of conductor activity on synaptic strengths vs . motor output . This delay is of the order τ 1 , 2 − τ out ( see the plasticity rule below ) . The conductor activity in the rate - based model is modeled after songbird HVC ( Hahnloser , Kozhevnikov , and Fee 2002 ) : each neuron ﬁres a single burst during the motor program . Each burst corresponds to a sharp increase of the ﬁring rate c i ( t ) from 0 to a constant value , and then a decrease 10 ms later . The activities of the diﬀerent neurons are spread out to tile the whole duration of the output program . Other choices for the conductor activity also work , provided no patterns are repeated ( see Appendix ) . A . 2 Mathematical description of plasticity rule In our model the rate of change of the synaptic weights obeys a rule that depends on a ﬁltered version of the conductor signal ( see Figure 2B ) . This is expressed mathematically as dW ij dt = η ˜ c i ( t ) ( g j ( t ) − θ ) , ( A . 3 ) where η is a learning rate and ˜ c i = K ∗ c i , with the star representing convolution and K being a ﬁltering kernel . We considered a linear combination of two exponential kernels with timescales τ 1 and τ 2 , K ( t ) = αK 1 ( t ) − βK 2 ( t ) , ( A . 4 ) with K i ( t ) given by K i ( t ) = (cid:40) τ − 1 i e − t / τ i for t ≥ 0 , 0 else . ( A . 5 ) Diﬀerent choices for the kernels give similar results ( see Appendix ) . The overall scale of α and β can be absorbed into the learning rate η in eq . ( A . 3 ) . In our simulations , we ﬁx α − β = 1 and keep the learning rate constant as we change the plasticity rule ( see eq . ( 3 ) ) . In the spiking simulations with and without reinforcement learning in the tutor circuit , the ﬁring rates c i ( t ) and g j ( t ) were estimated by ﬁltering spike trains with exponential kernels whose timescales were in the range 5 ms – 40 ms . The reinforcement studies typically required longer timescales for stability , possibly because of delays between conductor activity and reward signals . A . 3 Derivation of the matching tutor signal To ﬁnd the tutor signal that provides the most eﬀective teaching for the student , we ﬁrst calculate how much synaptic weights change according to our plasticity rule , eq . ( A . 3 ) . Then we require that this change matches the gradient descent direction . We have ∆ W ij = (cid:90) T 0 dW ij dt dt = η (cid:90) T 0 ˜ c i ( t ) ( g j ( t ) − θ ) dt . ( A . 6 ) 15 Because of the linearity assumptions in our model , it is suﬃcient to focus on a case in which each conductor neuron , i , ﬁres a single short burst , at a time t i . We write this as c i ( t ) = δ ( t − t i ) , and so ∆ W ij = (cid:90) T 0 dW ij dt dt = η (cid:90) T 0 K ( t − t i ) ( g j ( t ) − θ ) dt , ( A . 7 ) where we used the deﬁnition of ˜ c i ( t ) . If the time constants τ 1 , τ 2 are short compared to the timescale on which the tutor input g j ( t ) varies , only the values of g j ( t ) around time t i will contribute to the integral . If we further assume that T (cid:29) t i , we can use a Taylor expansion of g j ( t ) around t = t i to perform the calculation : ∆ W ij ≈ η (cid:90) ∞ t i K ( t − t i ) (cid:0) g j ( t i ) − θ + ( t − t i ) g (cid:48) j ( t i ) (cid:1) dt = η ( g j ( t i ) − θ ) (cid:90) ∞ 0 K ( t ) dt + ηg (cid:48) j ( t i ) (cid:90) ∞ 0 tK ( t ) dt = η ( g j ( t i ) − θ ) (cid:90) ∞ 0 (cid:0) αK 1 ( t ) − βK 2 ( t ) (cid:1) dt + ηg (cid:48) j ( t i ) (cid:90) ∞ 0 t (cid:0) αK 1 ( t ) − βK 2 ( t ) (cid:1) dt . ( A . 8 ) Doing the integrals involving the exponential kernels K 1 and K 2 , we get ∆ W ij = η (cid:2) ( α − β ) ( g j ( t i ) − θ ) + ( ατ 1 − βτ 2 ) g (cid:48) j ( t i ) (cid:3) . ( A . 9 ) We would like this synaptic change to optimally reduce a measure of mismatch between the output and the desired target as measured by a loss function . A generic smooth loss function L ( y a ( t ) , ¯ y a ( t ) ) can be quadratically approximated when y a is suﬃciently close to the target ¯ y a ( t ) . With this in mind , we consider a quadratic loss L = 1 2 (cid:88) a (cid:90) T 0 (cid:2) y a ( t ) − ¯ y a ( t ) (cid:3) 2 dt . ( A . 10 ) The loss function would decrease monotonically during learning if synaptic weights changed in proportion to the negative gradient of L : ∆ W ij = − γ ∂L ∂W ij , ( A . 11 ) where γ is a learning rate . This implies ∆ W ij = − γ (cid:88) a (cid:90) T 0 M aj (cid:2) y a ( t ) − ¯ y a ( t ) (cid:3) c i ( t ) . ( A . 12 ) Using again c i ( t ) = δ ( t − t i ) , we obtain ∆ W ij = − γ(cid:15) j ( t i ) , ( A . 13 ) where we used the notation from eq . ( 2 ) for the motor error at student neuron j . We now set ( A . 9 ) and ( A . 13 ) equal to each other . If the conductor ﬁres densely in time , we need the equality to hold for all times , and we thus get a diﬀerential equation for the tutor signal g j ( t ) . This identiﬁes the tutor signal that leads to gradient descent learning as a function of the motor error (cid:15) j ( t ) , eq . ( 3 ) ( with the notation ζ = γ / η ) . A . 4 Spiking simulations We used spiking models that were based on leaky integrate - and - ﬁre neurons with current - based dynamics for the synaptic inputs . The magnitude of synaptic potentials generated by the conductor – student synapses was independent of the membrane potential , approximating AMPA receptor dynamics , while the synaptic inputs from the tutor to the student were based on a mixture of AMPA and NMDA dynamics . Speciﬁcally , the 16 equations describing the dynamics of the spiking model were : τ m dV j dt = ( V R − V j ) + R (cid:0) I AMPA j + I NMDA j (cid:1) − V inh , ( except during refractory period ) dI AMPA j dt = − I AMPA j τ AMPA + (cid:88) i W ij (cid:88) k δ ( t − t conductor # i k ) + ( 1 − r ) w (cid:88) k δ ( t − t tutor k ) , dI NMDA j dt = − I NMDA j τ NMDA + rwG ( V j ) (cid:88) k δ ( t − t tutor k ) , V inh = g inh N student (cid:88) j S j ( t ) , dS j dt = − S j τ inh + (cid:88) k δ ( t − t student k ) , G ( V ) = (cid:20) 1 + [ Mg ] 3 . 57 mM exp ( − V / 16 . 13 mV ) (cid:21) − 1 . ( A . 14 ) Here V j is the membrane potential of the j th student neuron and V R is the resting potential , as well as the potential to which the membrane was reset after a spike . Spikes were registered whenever the membrane potential went above a threshold V th , after which a refractory period τ ref ensued . Apart from excitatory AMPA and NMDA inputs modeled by the I AMPA j and I NMDA j variables in our model , we also included a global inhibitory signal V inh which is proportional to the overall activity of student neurons averaged over a timescale τ inh . The averaging is performed using the auxiliary variables S j which are convolutions of student spike trains with an exponential kernel . These can be thought of as a simple model for the activities of inhibitory interneurons in the student . Table 1 gives the values of the parameters we used in the simulations . These values were chosen to match the ﬁring statistics of neurons in bird RA , as described below . Parameter Symbol Value Parameter Symbol Value No . of conductor neurons 300 No . of student neurons 80 Reset potential V R − 72 . 3 mV Input resistance R 353 MΩ Threshold potential V th − 48 . 6 mV Strength of inhibition g inh 1 . 80 mV Membrane time constant τ m 24 . 5 ms Fraction NMDA receptors r 0 . 9 Refractory period τ ref 1 . 1 ms Strength of synapses from tutor w 100 nA AMPA time constant τ AMPA 6 . 3 ms No . of conductor synapses per student neuron 148 NMDA time constant τ NMDA 81 . 5 ms Mean strength of synapses from conductor 32 . 6 nA Time constant for global in - hibition τ inh 20 ms Standard deviation of conductor – student weights 17 . 4 nA Conductor ﬁring rate during bursts 632 Hz Table 1 : Values for parameters used in the spiking simulations . The voltage dynamics for conductor and tutor neurons was not simulated explicitly . Instead , each conductor neuron was assumed to ﬁre a burst at a ﬁxed time during the simulation . The onset of each burst had additive timing jitter of ± 0 . 3 ms and each spike in the burst had a jitter of ± 0 . 2 ms . This modeled the uncertainty in spike times that is observed in in vivo recordings in birdsong ( Hahnloser , Kozhevnikov , and Fee 2002 ) . Tutor neurons ﬁred Poisson spikes with a time - dependent ﬁring rate that was set as described in the main text . The initial connectivity between conductor and student neurons was chosen to be sparse ( see Table 1 ) . The initial distribution of synaptic weights was log - normal , matching experimentally measured values for zebra ﬁnches ( Garst - Orozco , Babadi , and ¨Olveczky 2014 ) . Since these measurements are done in the slice , the absolute number of HVC synapses per RA neuron is likely to have been underestimated . The number 17 of conductor – student synapses we start with in our simulations is thus chosen to be higher than the value reported in that paper ( see Table 1 ) , and is allowed to change during learning . We checked that the learning paradigm described here is robust to substantial changes in these parameters , but we have chosen values that are faithful to birdsong experiments and which are thus able to imitate the RA spiking statistics during song . The synapses projecting onto each student neuron from the tutor have a weight that is ﬁxed during our simulations reﬂecting the ﬁnding in ( Garst - Orozco , Babadi , and ¨Olveczky 2014 ) that the average strength of LMAN – RA synapses for zebra ﬁnches does not change with age . There is some evidence that individual LMAN – RA synapses undergo plasticity concurrently with the HVC – RA synapses ( Mehaﬀey and Doupe 2015 ) but we did not seek to model this eﬀect . There are also developmental changes in the kinetics of NMDA - mediated synaptic currents in both HVC – RA and LMAN – RA synapses which we do not model ( Stark and Perkel 1999 ) . These , however , happen early in development , and thus are unlikely to have an eﬀect on song crystallization , which is what our model focuses on . Stark and Perkel ( 1999 ) also observed changes in the relative contribution of NMDA to AMPA responses in the HVC – RA synapses . We do not incorporate such eﬀects in our model since we do not explicitly model the dynamics of HVC neurons in this paper . However , this is an interesting avenue for future work , especially since there is evidence that area HVC can also contribute to learning , in particular in relation to the temporal structure of song ( Ali et al . 2013 ) . A . 5 Matching spiking statistics with experimental data We used an optimization technique to choose parameters to maximize the similarity between the statistics of spiking in our simulations and the ﬁring statistics observed in neural recordings from the songbird . The comparison was based on several descriptive statistics : the average ﬁring rate ; the coeﬃcient of variation and skewness of the distribution of inter - spike intervals ; the frequency and average duration of bursts ; and the ﬁring rate during bursts . For calculating these statistics , bursts were deﬁned to start if the ﬁring rate went above 80 Hz and last until the rate decreased below 40 Hz . To carry out such optimizations in the stochastic context of our simulations , we used an evolutionary algorithm—the covariance matrix adaptation evolution strategy ( CMA - ES ) ( Hansen 2006 ) . The objective function was based on the relative error between the simulation statistics x sim i and the observed statistics x obs i , error = (cid:34)(cid:88) i (cid:18) x sim i x obs i − 1 (cid:19) 2 (cid:35) 1 / 2 . ( A . 15 ) Equal weight was placed on optimizing the ﬁring statistics in the juvenile ( based on a recording from a 43 dph bird ) and optimizing ﬁring in the adult ( based on a recording from a 160 dph bird ) . In this optimization there was no learning between the juvenile and adult stages . We simply required that the number of HVC synapses per RA neuron , and the mean and standard deviation of the corresponding synaptic weights were in the ranges seen in the juvenile and adult by Garst - Orozco , Babadi , and ¨Olveczky ( 2014 ) . The optimization was carried out in Python ( RRID : SCR _ 008394 ) , using code from https : / / www . lri . fr / ~ hansen / cmaes _ inmatlab . html . The results ﬁxed the parameter choices in Table 1 which were then used to study our learning paradigm . While these choices are important for achieving ﬁring statistics that are similar to those seen in recordings from the bird , our learning paradigm is robust to large variations in the parameters in Table 1 . A . 6 Software and data We used custom - built Python ( RRID : SCR _ 008394 ) code for simulations and data analysis . The software and data that we used can be accessed online on GitHub ( RRID : SCR _ 002630 ) at https : / / github . com / ttesileanu / twostagelearning . 18 B Appendix B . 1 Eﬀect of nonlinearities We can generalize the model from eq . ( A . 1 ) by using a nonlinear transfer function from student activities to motor output , and a nonlinear activation function for student neurons : y a ( t ) = N a (cid:16)(cid:88) j M aj s j ( t ) (cid:17) , s j ( t ) = F (cid:16)(cid:88) i W ij c i ( t ) + wg j ( t ) − x inh (cid:17) . ( B . 1 ) Suppose further that we use a general loss function , L = (cid:90) T 0 L (cid:0) { y a ( t ) − ¯ y a ( t ) } (cid:1) dt . ( B . 2 ) Carrying out the same argument as that from section A . 3 , the gradient descent condition , eq . ( A . 11 ) , implies ∆ W ij = − γ (cid:90) T 0 (cid:88) a M aj N (cid:48) a F (cid:48) c i ( t ) ∂ L ∂y a (cid:12)(cid:12)(cid:12)(cid:12) y a ( t ) − ¯ y a ( t ) . ( B . 3 ) The departure from the quadratic loss function , L (cid:54) = 12 (cid:80) a ( y a ( t ) − ¯ y a ( t ) ) 2 , and the nonlinearities in the output , N a , have the eﬀect of redeﬁning the motor error , (cid:15) j ( t ) = (cid:88) a M aj N (cid:48) a ∂ L ∂y a (cid:12)(cid:12)(cid:12)(cid:12) y a ( t ) − ¯ y a ( t ) . ( B . 4 ) A proper loss function will be such that the derivatives ∂ L / ∂y a vanish when y a ( t ) = ¯ y a ( t ) , and so the motor error (cid:15) j as deﬁned here is zero when the rendition is perfect , as expected . If we use a tutor that ignores the nonlinearities in a nonlinear system , i . e . , if we use eq . ( 2 ) instead of eq . ( B . 4 ) to calculate the tutor signal that is plugged into eq . ( 3 ) , we still expect successful learning provided that N (cid:48) a > 0 and that L is itself an increasing function of | y a − ¯ y a | ( see section B . 2 ) . This is because replacing eq . ( B . 4 ) with eq . ( 2 ) would aﬀect the magnitude of the motor error without signiﬁcantly changing its direction . In more complicated scenarios , if the transfer function to the output is not monotonic , there is the potential that using eq . ( 2 ) would push the system away from convergence instead of towards it . In such a case , an adaptive mechanism , such as the reinforcement rules from eqns . ( 6 ) or ( 7 ) can be used to adapt to the local values of the derivatives N (cid:48) a and ∂ L / ∂y a . Finally , the nonlinear activation function F introduces a dependence on the student output s j ( t ) in eq . ( B . 3 ) , since F (cid:48) is evaluated at F − 1 ( s j ( t ) ) . To obtain a good match between the student and the tutor in this context , we can modify the student plasticity rule ( A . 3 ) by adding a dependence on the postsynaptic activity , dW ij dt = η ˜ c i ( t ) ( g j ( t ) − θ ) F (cid:48) ( F − 1 ( s j ( t ) ) ) . ( B . 5 ) In general , synaptic plasticity has been observed to indeed depend on postsynaptic activity ( Chistiakova , Bannon , et al . 2014 ; Chistiakova and Volgushev 2009 ) . Our derivation suggests that the eﬀectiveness of learning could be improved by tuning this dependence of synaptic change on postsynaptic activity to the activation function of postsynaptic neurons , according to eq . ( B . 5 ) . It would be interesting to check whether such tuning occurs in real neurons . B . 2 Eﬀect of diﬀerent output functions In the main text , we assumed a linear mapping between student activities and motor output . Moreover , we assumed a myotopic organization , in which each student neuron projected to a single muscle , leading to a student – output assignment matrix M aj in which each column had a single non - zero entry . We also assumed 19 that student neurons only contributed additively to the outputs , with no inhibitory activity . Here we show that our results hold for other choices of student – output mappings . For example , assume a push - pull architecture , in which half of the student neurons controlling one output are excitatory and half are inhibitory . This can be used to decouple the overall ﬁring rate in the student from the magnitude of the outputs . Learning works just as eﬀectively as in the case of the purely additive student – output mapping when using matched tutors , Appendix Figures 1A and 1B . The consequences of mismatching student and tutor circuits are also not signiﬁcantly changed , Appendix Figures 1C and 1D . We can also consider nonlinear mappings between the student activity and the ﬁnal output . If there is a monotonic output nonlinearity , as in eq . ( B . 1 ) with N (cid:48) a > 0 , the tutor signal derived for the linear case , eq . ( 3 ) , can still achieve convergence , though at a slower rate and with a somewhat lower accuracy ( see Appendix Figure 1E for the case of a sigmoidal nonlinearity ) . For non - monotonic nonlinearities , the direction from which the optimum is approached can be crucial , as learning can get stuck in local minima of the loss function . 2 Studying this might provide an interesting avenue to test whether learning in songbirds is based on a gradient descent - type rule or on a more sophisticated optimization technique . B . 3 Diﬀerent inhibition models In the spiking model , we used an activity - dependent inhibitory signal that was proportional to the average student activity . Using a constant inhibition instead , V inh = constant , does not signiﬁcantly change the results : see Appendix Figure 1F for an example . B . 4 Eﬀect of changing plasticity kernels In the main text , we used exponential kernels with τ 1 = 80 ms and τ 2 = 40 ms for the smoothing of the conductor signal that enters the synaptic plasticity rule , eq . ( A . 3 ) . We can generalize this in two ways : we can use diﬀerent timescales τ 1 , τ 2 , or we can use a diﬀerent functional form for the kernels . ( Note that in the main text we showed the eﬀects of varying the parameters α and β in the plasticity rule , while the timescales τ 1 and τ 2 were kept ﬁxed . ) The values for the timescales τ 1 , 2 were chosen to roughly match the shape of the plasticity curve measured in slices of zebra ﬁnch RA ( Mehaﬀey and Doupe 2015 ) ( see Figures 1C , 1D ) . The main predictions of our model , that learning is most eﬀective when the tutor signal is matched to the student plasticity rule , and that large mismatches between tutor and student lead to impaired learning , hold well when the student timescales change : see Appendix Figure 2A for the case when τ 1 = 20 ms and τ 2 = 10 ms . In the main text we saw that the negative eﬀects of tutor – student mismatch diminish for timescales that are shorter than ∼ τ 1 , 2 . In Appendix Figure 2A , the range of timescales where a precise matching is not essential becomes very small because the student timescales are short . Another generalization of our plasticity rule can be obtained by changing the functional form of the kernels used to smooth the conductor input . As an example , suppose K 2 is kept exponential , while K 1 is replaced by ¯ K 1 ( t ) = (cid:40) 1 ¯ τ 21 te − t / ¯ τ 1 for t ≥ 0 , 0 else . ( B . 6 ) An example of learning using an STDP rule based on kernels ¯ K 1 and K 2 where ¯ τ 1 = τ 2 is shown in Appendix Figure 2B . The matching tutor has the same form as before , eq . ( 3 ) with timescale τ tutor = τ ∗ tutor given by eq . ( 4 ) , but with τ 1 = 2¯ τ 1 = 2 τ 2 . We can see that learning is as eﬀective as in the case of purely exponential kernels . B . 5 More general conductor patterns In the main text , we have focused on a conductor whose activity matches that observed in area HVC of songbirds ( Hahnloser , Kozhevnikov , and Fee 2002 ) : each neuron ﬁres a single burst during the motor program . Our model , however , is not restricted to this case . We generated alternative conductor patterns by using arbitrarily - placed bursts of activity , as in Appendix Figure 3A . The model converges to a good rendition of the 2 We thank Josh Gold for this observation . 20 10 20 40 80 1603206401280256051201024020480 τ tutor 10204080160 3206401280256051201024020480 τ ∗ t u t o r = α τ 1 − β τ 2 α − β 10 20 40 80 1603206401280256051201024020480 τ tutor 10204080160 3206401280256051201024020480 τ ∗ t u t o r = α τ 1 − β τ 2 α − β 0 . 50 1 . 00 2 . 00 5 . 00 10 . 00 0 50 100 150 200 250 repetitions 0 2 4 6 8 10 12 14 e rr o r α = 7 . 0 , β = 6 . 0 , τ tutor = 320 . 0 time 01020304050607080 o u t pu t target output 0 50 100 150 200 250 repetitions 0 2 4 6 8 10 12 14 e rr o r α = 0 . 0 , β = − 1 . 0 , τ tutor = 40 . 0 time 010203040506070 o u t pu t target output 0 600 A B C D 0 100 200 300 400 500 repetitions 0 2 4 6 8 10 12 14 e rr o r α = 24 . 0 , β = 23 . 0 , τ tutor = 1000 . 0 time 01020304050607080 o u t pu t target output 0 50 100 150 200 repetitions 0 2 4 6 8 10 12 14 e rr o r α = 1 . 0 , β = 0 . 0 , τ tutor = 80 . 0 time 01020304050607080 o u t pu t target output E F 0 600 0 600 0 600 worse better Appendix ﬁgure 1 : Robustness of learning . A . Error trace showing how average motor error evolves with repetitions of the motor program for rate - based plasticity paired with a matching tutor , when the student – output mapping has a push - pull architecture . The inset shows the ﬁnal motor output ( thick red line ) compared to the target output ( dotted black line ) . The output on the ﬁrst rendition and at two other stages of learning are also shown . B . The error trace and ﬁnal motor output shown for timing - based plasticity matched by a tutor with a long integration timescale . C . Eﬀects of mismatch between student and tutor on reproduction accuracy when using a push - pull architecture for the student – output mapping . The heatmap shows the ﬁnal reproduction error of the motor output after 1000 learning cycles when a student with plasticity parameters α and β is paired with a tutor with memory timescale τ tutor . Here τ 1 = 80 ms and τ 2 = 40 ms . D . Error evolution curves as a function of the mismatch between student and tutor . Each plot shows how the error in the motor program changes during 1000 learning cycles for the same conditions as those shown in the heatmap . The region shaded in light pink shows simulations where the mismatch between student and tutor leads to a deteriorating instead of improving performance during learning . E . Convergence in the rate - based model with a linear - nonlinear controller that uses a sigmoidal nonlinearity . F . Convergence in the spiking model when inhibition is constant instead of activity - dependent ( V inh = constant ) . 21 10 20 40 80 1603206401280256051201024020480 τ tutor 102040801603206401280256051201024020480 τ ∗ t u t o r = α τ 1 − β τ 2 α − β 0 . 50 1 . 00 2 . 00 5 . 00 10 . 00 0 50 100 150 200 repetitions 0 2 4 6 8 10 12 14 e rr o r α = 24 . 0 , β = 23 . 0 , τ tutor = 1000 . 0 time 010203040506070 o u t pu t target output 0 600 A B worse better Appendix ﬁgure 2 : Eﬀect of changing conductor smoothing kernels in the plasticity rule . ( A ) Matrix showing learning accuracy when using diﬀerent timescales for the student plasticity rule . Each entry in the heatmap shows the average rendition error after 1000 learning steps when pairing a tutor with timescale τ tutor with a non - matched student . Here the kernels are exponential , with timescales τ 1 = 20 ms , τ 2 = 10 ms . ( B ) Evolution of motor error with learning using kernels ∼ e − t / τ and ∼ te − t / τ , instead of the two exponentials used in the main text . The tutor signal is as before , eq . ( 3 ) . The inset shows the ﬁnal output for the trained model , for one of the two output channels . Learning is as eﬀective and fast as before . target program , Appendix Figure 3B . Learning is harder in this case because many conductor neurons can be active at the same time , and the weight updates aﬀect not only the output of the system at the current position in the motor program , but also at all the other positions where the conductor neurons ﬁre . This is in contrast to the HVC - like conductor , where each neuron ﬁres at a single point in the motor program , and thus the eﬀect of weight updates is better localized . More generally , simulations show that the sparser the conductor ﬁring , the faster the convergence ( data not shown ) . The accuracy of the ﬁnal rendition of the motor program ( Appendix Figure 3B , inset ) is also not as good as before . B . 6 Edge eﬀects In our derivation of the matching tutor rule , we assumed that the system has enough time to integrate all the synaptic weight changes from eq . ( A . 3 ) . However , some of these changes occur tens or hundreds of milliseconds after the inputs that generated them , due to the timescales used in the plasticity kernel . Since our simulations are only run for a ﬁnite amount of time , there will in general be edge eﬀects , where periods of the motor program towards the end of the simulations will have diﬃculty converging . To oﬀset such numerical issues , we ran the simulations for a few hundred milliseconds longer than the duration of the motor program , and ignored the data from this extra period . Our simulations typically run for 600 ms , and the time reserved for relaxation after the end of the program was set to 1200 ms . The long relaxation time was chosen to allow for cases where the tutor was chosen to have a very long memory timescale . B . 7 Parameter optimization for reproducing juvenile and adult spiking statistics We set the parameters in our simulations to reproduce spiking statistics from recordings in zebra ﬁnch RA as closely as possible . Appendix Figure 4 shows how the distribution of summary statistics obtained from 50 runs of the simulation compares to the distributions calculated from recordings in birds at various developmental stages . Each plot shows a standard box and whisker plot superimposed over a kernel - density estimate of the distribution of a given summary statistic , either over simulation runs or over recordings from birds at various stages of song learning . We ran two sets of simulations , one for a bird with juvenile - like connectivity between HVC and RA , and one with adult - like connectivity ( see Methods ) . In these simulations there was no learning to match the timecourse of songs—the goal was simply to identify parameters that lead to birdsong - like ﬁring statistics . 22 0 500 1000 1500 2000 2500 3000 repetitions 0 2 4 6 8 10 12 14 e rr o r α = 5 . 0 , β = 4 . 0 , τ tutor = 240 . 0 time 01020304050607080 o u t pu t target output 0 600 0 100 200 300 400 500 600 time ( ms ) 1 3 5 7 9 11 13 15 17 19 c o ndu c t o r n e u r o n i nd e x A B Appendix ﬁgure 3 : Learning with arbitrary conductor activity . A . Typical activity of conductor neurons . 20 of the 100 neurons included in the simulation are shown . The activity pattern is chosen so that about 10 % of the neurons are active at any given time . The pattern is chosen randomly but is ﬁxed during learning . Each conductor burst lasts 30 ms . B . Convergence curve and ﬁnal rendition of the motor program ( in inset ) . Learning included two output channels but the ﬁnal output is shown for only one of them . The qualitative match between our simulations and recordings is good , but the simulations are less variable than the measurements . This may be due to sources of variability that we have ignored—for example , all our simulated neurons had exactly the same membrane time constants , refractory periods , and threshold potentials , which is not the case for real neurons . Another reason might be that in our simulations , all the runs were performed for the same network , while the measurements are from diﬀerent cells in diﬀerent birds . B . 8 Eﬀect of spiking stochasticity on learning As pointed out in the main text , learning is aﬀected in the spiking simulations when the tutor error integration timescale τ tutor becomes very long . More speciﬁcally , two distinct eﬀects occur . First , the ﬂuctuations in the motor output increase , leading to a poorer match to the shape of the target motor program . And second , the whole output gets shifted up , towards higher muscle activation values . Both of these eﬀects can be traced back to the stochasticity of the tutor signal . In the spiking simulations , tutor neurons are assumed to ﬁre Poisson spikes following a time - dependent ﬁring rate that obeys eq . ( 5 ) . By the nature of the Poisson process , the tutor output in this case will contain ﬂuctuations around the mean , g ( t ) ∼ ¯ g ( t ) + ξ ( t ) . Recall that the scale of g ( t ) is set by the threshold θ and thus so is the scale of the variability ξ ( t ) . As long as the tutor error integration timescale is not very long , g ( t ) roughly corresponds to a smoothed version of the motor error (cid:15) ( t ) ( cf . eq . ( 5 ) ) . However , as τ tutor grows past the duration T of the motor program , the exponential term in eq . ( 5 ) becomes essentially constant , leading to a tutor signal ¯ g ( t ) whose departures from the center value θ decrease in proportion to the timescale τ tutor . As far as the student is concerned , the relevant signal is g ( t ) − θ ( eq . ( 1 ) ) , and thus , when τ tutor > T , the signal - to - noise ratio in the tutor guiding signal starts to decrease as 1 / τ tutor . This ultimately leads to a very noisy rendition of the target program . One way to improve this would be to increase the gain factor ζ that controls the relation between the motor error and the tutor signal ( see eq . ( 5 ) ) . This improves the ability of the system to converge onto its target in the late stages of learning . In the early stages of learning , however , this could lead to saturation problems . One way to ﬁx this would be to use a variable gain factor ζ that ensures the whole range of tutor ﬁring rates is used without generating too much saturation . This would be an interesting avenue for future research . Reducing the ﬂuctuations in the tutor signal also decreases the ﬂuctuations in the conductor – student synaptic weights , which leads to fewer weights being clamped at 0 because of the positivity constraint . This reduces the shift between the learned motor program and the target . As mentioned in the main text , another approach to reducing or eliminating this shift is to allow for negative weights or ( more realistically ) to use a push - pull mechanism , in which the activity of some student neurons acts to increase muscle output , while the 23 s i m j u v e n il e 40 - 6565 - 90 90 - 115 115 - 140 140 - 165 s i m a du l t 020406080100120140160 A v e r ag e ﬁ r i n g r a t e ( H z ) s i m j u v e n il e 40 - 6565 - 90 90 - 115 115 - 140 140 - 165 s i m a du l t 0 1 2 3 4 5 6 C V o f I S I s i m j u v e n il e 40 - 6565 - 90 90 - 115 115 - 140 140 - 165 s i m a du l t 0 5 10 15 20 25 S k e w n e s s o f I S I s i m j u v e n il e 40 - 6565 - 90 90 - 115 115 - 140 140 - 165 s i m a du l t 0 2 4 6 8 10 12 B u r s t f r e q u e n c y ( H z ) s i m j u v e n il e 40 - 6565 - 90 90 - 115 115 - 140 140 - 165 s i m a du l t 0 . 00 0 . 05 0 . 10 0 . 15 0 . 20 A v e r ag e bu r s t l e n g t h ( s ) s i m j u v e n il e 40 - 6565 - 90 90 - 115 115 - 140 140 - 165 s i m a du l t 050100150200250300350400450 F i r i n g r a t e du r i n g bu r s t s ( H z ) Appendix ﬁgure 4 : Violin plots showing how the spiking statistics from our simulation compared to the statistics obtained from neural recordings . Each violin shows a kernel - density estimate of the distribution that a particular summary statistic had in either several runs of a simulation , or in several recordings from behaving birds . The circle and the box within each violin show the median and the interquartile range . 24 activity of other student neurons acts as an inhibition on muscle output . C Plasticity parameter values In the heatmaps that appear in many of the ﬁgures in the main text and in the supplementary information , we kept the timescales τ 1 and τ 2 constant while varying α and β to modify the student plasticity rule . Since the overall scale of α and β is inconsequential as it can be absorbed into the learning rate ( as explained in section 2 . 2 ) , we imposed the further constraint α − β = 1 . This implies that we eﬀectively focused on a one - parameter family of student plasticity rule , as identiﬁed by the value of α ( and the corresponding value for β = α − 1 ) . In the ﬁgures , we expressed this instead in terms of the timescale of the optimally - matching tutor , τ ∗ tutor , as deﬁned in eq . ( 4 ) . Below we give the explicit values of α and β that we used for each row in the heatmaps . These can be calculated by solving for α in eq . ( 4 ) , using β = α − 1 , and assume that τ 1 = 80 ms and τ 2 = 40 ms . τ ∗ tutor α β 10 − 0 . 75 − 1 . 75 20 − 0 . 5 − 1 . 5 40 0 . 0 − 1 . 0 80 1 . 0 0 . 0 160 3 . 0 2 . 0 320 7 . 0 6 . 0 640 15 . 0 14 . 0 1280 31 . 0 30 . 0 2560 63 . 0 62 . 0 5120 127 . 0 126 . 0 10240 255 . 0 254 . 0 20480 511 . 0 510 . 0 References [ 1 ] Farhan Ali et al . “The basal ganglia is necessary for learning spectral , but not temporal , features of birdsong” . In : Neuron 80 . 2 ( Oct . 2013 ) , pp . 494 – 506 . [ 2 ] Aaron S . Andalman and Michale S . Fee . “A basal ganglia - forebrain circuit in the songbird biases motor output to avoid vocal errors . ” In : Proceedings of the National Academy of Sciences of the United States of America 106 . 30 ( July 2009 ) , pp . 12518 – 23 . [ 3 ] Mark J . Basista et al . “Independent Premotor Encoding of the Sequence and Structure of Birdsong in Avian Cortex” . In : Journal of Neuroscience 34 . 50 ( Dec . 2014 ) , pp . 16821 – 16834 . [ 4 ] R . A . Canady et al . “Eﬀect of testosterone on input received by an identiﬁed neuron type of the canary song system : a Golgi / electron microscopy / degeneration study” . In : The Journal of Neuroscience 8 . 10 ( 1988 ) , pp . 3770 – 3784 . [ 5 ] Marina Chistiakova , Nicholas M . Bannon , et al . “Heterosynaptic Plasticity : Multiple Mechanisms and Multiple Roles” . In : The Neuroscientist : a review journal bringing neurobiology , neurology and psychiatry April ( 2014 ) . [ 6 ] Marina Chistiakova and Maxim Volgushev . “Heterosynaptic plasticity in the neocortex” . In : Experimental Brain Research 199 . 3 - 4 ( 2009 ) , pp . 377 – 390 . [ 7 ] Kenji Doya and Terrence J . Sejnowski . “A computational model of avian song learning” . In : The new cognitive neurosciences . Ed . by Michael S . Gazzaniga . The MIT Press , 2000 , pp . 469 – 482 . isbn : 0262071959 . arXiv : arXiv : 1011 . 1669v3 . [ 8 ] Michael A . Farries and Adrienne L . Fairhall . “Reinforcement Learning With Modulated Spike Timing - Dependent Synaptic Plasticity” . In : Journal of neurophysiology 98 . 6 ( 2007 ) , pp . 3648 – 3665 . [ 9 ] Michale S . Fee and Jesse H . Goldberg . “A hypothesis for basal ganglia - dependent reinforcement learning in the songbird” . In : Neuroscience 198 ( 2011 ) , pp . 152 – 170 . 25 [ 10 ] Ila R . Fiete , Michale S . Fee , and H . Sebastian Seung . “Model of birdsong learning based on gradient estimation by dynamic perturbation of neural conductances” . In : Journal of neurophysiology 98 . 4 ( Oct . 2007 ) , pp . 2038 – 57 . [ 11 ] Ila R . Fiete and H . Sebastian Seung . “Gradient learning in spiking neural networks by dynamic per - turbation of conductances” . In : Physical Review Letters 97 . 4 ( 2006 ) , p . 048104 . arXiv : 0601028v1 [ arXiv : q - bio ] . [ 12 ] Paul W . Frankland and Bruno Bontempi . “The organization of recent and remote memories” . In : Nature reviews . Neuroscience 6 . 2 ( 2005 ) , pp . 119 – 30 . [ 13 ] Vikram Gadagkar et al . “Dopamine neurons encode performance error in singing birds” . In : Science 354 . 6317 ( 2016 ) , pp . 1278 – 1282 . eprint : http : / / science . sciencemag . org / content / 354 / 6317 / 1278 . full . pdf . [ 14 ] Jonathan Garst - Orozco , Baktash Babadi , and Bence P . ¨Olveczky . “A neural circuit mechanism for regulating motor variability during skill learning” . In : eLife 3 ( 2014 ) , e03697 . [ 15 ] Richard H . R . Hahnloser , Alexay A . Kozhevnikov , and Michale S . Fee . “An ultra - sparse code underlies the generation of neural sequences in a songbird” . In : Nature 419 . 1989 ( 2002 ) , pp . 796 – 797 . [ 16 ] Kosuke Hamaguchi , Masashi Tanaka , and Richard Mooney . “A Distributed Recurrent Network Contributes to Temporally Precise Vocalizations” . In : Neuron 91 ( 2016 ) , pp . 680 – 693 . [ 17 ] Nikolaus Hansen . “The CMA Evolution Strategy : A Comparing Review” . In : Towards a new evolutionary computation . Advances on estimation of distribution algorithms . 2006 , pp . 75 – 102 . [ 18 ] Kathrin Herrmann and Arthur P . Arnold . “The development of aﬀerent projections to the robust archistriatal nucleus in male zebra ﬁnches : a quantitative electron microscopic study” . In : The Journal of Neuroscience 11 . 7 ( 1991 ) , pp . 2063 – 2074 . [ 19 ] Lukas A . Hoﬀmann et al . “Dopaminergic Contributions to Vocal Learning” . In : The Journal of Neuro - science 36 . 7 ( 2016 ) , pp . 2176 – 2189 . [ 20 ] K Immelmann . “Song development in the zebra ﬁnch and other estrildid ﬁnches” . In : Bird vocalizations . Ed . by R . A . Hinde . 1969 , pp . 61 – 74 . [ 21 ] Mimi H . Kao , Brian D . Wright , and Allison J . Doupe . “Neurons in a Forebrain Nucleus Required for Vocal Plasticity Rapidly Switch between Precise Firing and Variable Bursting Depending on Social Context” . In : The Journal of Neuroscience 28 . 49 ( 2008 ) , pp . 13232 – 13247 . [ 22 ] Risa Kawai et al . “Motor Cortex Is Required for Learning but Not for Executing a Motor Skill” . In : Neuron 86 . 3 ( 2015 ) , pp . 800 – 812 . [ 23 ] Satoshi Kojima , Mimi H . Kao , and Allison J . Doupe . “Task - related ”cortical” bursting depends critically on basal ganglia input and is linked to vocal plasticity” . In : PNAS 110 . 12 ( 2013 ) , pp . 4756 – 4761 . [ 24 ] L’ubica Kubikova and L’ubor Koˇsˇt´al . “Dopaminergic system in birdsong learning and maintenance” . In : Journal of Chemical Neuroanatomy 39 ( 2010 ) , pp . 112 – 123 . [ 25 ] Anthony Leonardo and Michale S . Fee . “Ensemble Coding of Vocal Control in Birdsong” . In : The Journal of Neuroscience 25 . 3 ( 2005 ) , pp . 652 – 661 . [ 26 ] Galen F . Lynch et al . “Rhythmic Continuous - Time Coding in the Songbird Analog of Vocal Motor Cortex” . In : Neuron 90 . 4 ( 2016 ) , pp . 877 – 892 . [ 27 ] W . Hamish Mehaﬀey and Allison J . Doupe . “Naturalistic stimulation drives opposing heterosynaptic plasticity at two inputs to songbird cortex” . In : Nature Neuroscience 18 . 9 ( 2015 ) , pp . 1272 – 1280 . [ 28 ] Raoul - Martin Memmesheimer et al . “Learning Precisely Timed Spikes” . In : Neuron 82 ( 2014 ) , pp . 1 – 14 . [ 29 ] Bence P . ¨Olveczky , Aaron S . Andalman , and Michale S . Fee . “Vocal experimentation in the juvenile songbird requires a basal ganglia circuit . ” In : PLoS biology 3 . 5 ( May 2005 ) , e153 . [ 30 ] Bence P . ¨Olveczky , Timothy M . Otchy , et al . “Changes in the neural control of a complex motor sequence during learning” . In : Journal of Neurophysiology 106 . 1 ( 2011 ) , pp . 386 – 397 . [ 31 ] David J . Perkel . “Origin of the Anterior Forebrain Pathway” . In : Annals of the New York Academy of Sciences 1016 ( 2004 ) , pp . 736 – 748 . 26 [ 32 ] Michel A . Picardo et al . “Population - Level Representation of a Temporal Sequence Underlying Song Production in the Zebra Finch” . In : Neuron 90 . 4 ( 2016 ) , pp . 866 – 876 . [ 33 ] H . Sebastian Seung . “Learning in Spiking Neural Networks by Reinforcement of Stochastic Synaptic Transmission” . In : 40 ( 2003 ) , pp . 1063 – 1073 . [ 34 ] H . Blair Simpson and David S . Vicario . “Brain Pathways for Learned and Unlearned Vocalizations Diﬀer in Zebra Finches” . In : The Journal of Neuroscience 10 . 5 ( 1990 ) , pp . 1541 – 1556 . [ 35 ] John E . Spiro , Matthew B . Dalva , and Richard Mooney . “Long - Range Inhibition Within the Zebra Finch Song Nucleus RA Can Coordinate the Firing of Multiple Projection Neurons” . In : Journal of neurophysiology 81 ( 1999 ) , pp . 3007 – 3020 . [ 36 ] Laura L . Stark and David J . Perkel . “Two - stage , input - speciﬁc synaptic maturation in a nucleus essential for vocal production in the zebra ﬁnch” . In : The Journal of Neuroscience 19 . 20 ( 1999 ) , pp . 9107 – 9116 . [ 37 ] Evren C . Tumer and Michael S . Brainard . “Performance variability enables adaptive plasticity of ’crystallized’ adult birdsong” . In : Nature 450 . December ( 2007 ) , pp . 1240 – 1244 . [ 38 ] Robert S . Turner and Michel Desmurget . “Basal ganglia contributions to motor control : a vigorous tutor” . In : Current Opinion in Neurobiology 20 . 6 ( 2010 ) , pp . 704 – 716 . [ 39 ] Lena Veit , Dmitriy Aronov , and Michale S . Fee . “Learning to breathe and sing : development of respiratory - vocal coordination in young songbirds” . In : Journal of Neurophysiology 106 . 4 ( 2011 ) , pp . 1747 – 1765 . eprint : http : / / jn . physiology . org / content / 106 / 4 / 1747 . full . pdf . [ 40 ] Timothy L . Warren et al . “Mechanisms and time course of vocal learning and consolidation in the adult songbird” . In : Journal of Neurophysiology 106 ( 2011 ) , pp . 1806 – 1821 . [ 41 ] Albert C . Yu and Daniel Margoliash . “Temporal hierarchical control of singing in birds” . In : Science 273 . 5283 ( 1996 ) , pp . 1871 – 1875 . 27