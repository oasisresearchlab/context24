Student Affect in CS1 : Insights from an Easy Data Collection Tool Patricia Haden Otago Polytechnic Dunedin New Zealand patricia . haden @ op . ac . nz Dale Parsons Otago Polytechnic Dunedin New Zealand dale . parsons @ op . ac . nz Krissi Wood Otago Polytechnic Dunedin New Zealand krissi . wood @ op . ac . nz Joy Gasson Otago Polytechnic Dunedin New Zealand joy . gasson @ op . ac . nz ABSTRACT Recent research , and our own experience as educators , has highlighted the need for an approach to CS1 that includes consideration of students ' emotional state . Unfortunately , collection of affect data usually requires a large investment of time and resources . In this article , we describe a simple and easy tool for collection of student affect data . We illustrate how these data can provide detailed insight into the quality of curricular materials and make accurate predictions of student performance . Based on our first year using the tool , we identify specific response patterns that can identify a student at risk of CS1 failure . Students who find classroom exercises both difficult and boring , or who recognize a programming problem as involving familiar material but have no clear plan as to how to solve the problem , are likely to struggle as the course proceeds . CCS CONCEPTS • Computer Science Education - > CS1 KEYWORDS Programming education , student affect , data collection ACM Reference format : P . Haden , D . Parsons , K . Wood , and J . Gasson . 2017 . Student Affect in CS1 : Insights from an Easy Data Collection Tool . In Proceedings of the 17th Koli Calling International Conference on Computing Education Research , Koli Calling ' 17 , Koli , Finland , November 16 - 19 , 2017 , 10 pages . DOI : https : / / doi . org / 10 . 1145 / 3141880 . 3141881 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from Permissions @ acm . org . Koli Calling 2017 , November 16 – 19 , 2017 , Koli , Finland © 2017 Association for Computing Machinery . ACM ISBN 978 - 1 - 4503 - 5301 - 4 / 17 / 11… $ 15 . 00 https : / / doi . org / 10 . 1145 / 3141880 . 3141881 1 INTRODUCTION The drawing is simple , yet dramatic . A stick figure man , with wide eyes and screaming mouth , is waist - deep in water , surrounded by circling sharks . On the surface of the water , a long series of binary numbers floats past him as he struggles to stay afloat . The drawing was made by a student in his first computer programming class , on the final exam , in response to the instructions : " Draw , sketch , illustrate , paint , depict or otherwise portray what programming means to you " . For the last several years , we have included this exercise at the end of our CS1 exams each semester . Worth either 0 or 1 mark , it was originally intended as a light - hearted way to close out the CS1 course and give students an alternative mode to provide some feedback about their experience - - we expected most of them to simply skip the question . We were startled by the results . Not only did the vast majority of students provide a drawing , many of them put a lot of effort into it , producing complex and detailed images , often with dialogue , frequently with multiple frames showing the passage of time . Most noticeably , when asked to depict what programming meant to them , most students concentrated not on technical or professional aspects , but on the emotional aspects . In a typical semester , 63 % of the submitted images depict some emotional state via emoji - like drawings of facial expression , affect - laden annotations such as " Yay ! ! " or explicit naming . This leads us to believe that , in our ongoing battle to facilitate student success in CS1 , we need to include consideration of students ' emotional ( affective ) experience . Most programming education research over the last four decades has concentrated on either cognitive models of the learning process , or in - depth explorations of teaching tools and paradigms ( see [ 30 ] for review ) . However , recent work has begun to consider the role of affect . Most simply , a student ' s emotional experience of programming can have direct behavioural impacts on successful learning . For example , if required activities are fun , students will spend more time on them ; if students value the outcome of their programming 40 education , they will work harder . But recent research indicates that affect also has subtler impacts at the cognitive level . Villavicencio [ 31 ] posits that different emotional states " trigger different modes of information processing and problem solving " ( pg . 330 ) . Pekrun et al . [ 23 , 24 ] present compelling evidence that emotional state has a direct effect on cognitive resources , learning strategy and learning regulation mechanisms . ( See also [ 3 , 32 ] for overviews . ) A substantial portion of the research into the role of affect in programming education has been in the interest of developing automated intelligent tutoring systems ( e . g . [ 5 , 6 , 17 ] ) . Properly constructed intelligent tutoring systems can provide individually - tailored instruction to an extent that is not possible for a human instructor to deliver for substantial class sizes [ 16 , 29 ] . Given the widely accepted importance of appropriate pedagogical intervention in programming education , especially in the earliest stages [ 22 ] , advances in intelligent tutoring systems for programming instruction could serve to ameliorate the traditional difficulties ( as evidenced by high failure rates ) which students experience in these courses . The intelligent tutoring system research is underpinned primarily by a model of learning which describes an optimal learning process as one where the student proceeds through moments of " cognitive disequilibrium " to achieve deep learning [ 5 , 8 , 10 , 16 , 18 ] . That is , in order to learn , the student must be challenged by new material or new concepts , which causes momentary confusion . If the student is able to resolve that confusion , deep learning is facilitated . Without such moments of challenge , the complexity of internal knowledge schemata does not increase . However , if confusion states are not able to be resolved , the student descends into frustration and eventual boredom and little substantive learning can occur [ 1 , 9 , 19 ] . This model is like the story of Goldilocks : the amount of challenge must be just right - - neither too much nor too little - - for optimal learning to occur . The focus of the intelligent tutoring system work is on attempts to automatically detect when the student is experiencing that " just right " amount of challenge . These studies search for ways to detect when , for example , a student has slipped into a frustrated or bored state . These systems generally use two different types of data source : First , some systems log students ' actions in the development environment while programming . For example , they record the frequency of compilation and note patterns of compilation errors [ e . g . 7 ] . They correlate these values with external measures of student frustration such as the reports of human observers [ 4 ] or use of an " I am Frustrated " button [ 13 ] . The second source of data on student affective state is behavioural observation , including facial expression , posture and body movement . These metrics are collected via specialised hardware such as cameras , pressure sensitive chairs and mice , and galvanic skin response recorders [ cf . 13 ] . In some of these projects , the researchers have constructed extremely sophisticated software systems for automated facial expression analysis ( see [ 6 ] for an overview ) . Both sources of automated affect detection - - software interaction and behavioural observation - - are then used to guide the intelligent tutoring systems , often using elegant machine learning approaches [ 2 , 12 ] . When frustration is confidently detected , the system can reduce the difficulty of problems or provide additional supporting material . Recently , some authors have also begun to explore the role of affect in programming education more broadly , outside of its value in development of intelligent tutoring systems . Kinnunen and Simon [ 14 ] looked in depth at the emotional impact of programming education , particularly on students ' judgments of their own abilities ( i . e . self - efficacy ) . Using detailed interview data , and approaching the matter from a socio - cognitive perspective , they discovered that student affective state has a profound impact on self - efficacy judgments , which in turn affect students ' approach to , and success with , their programming education ( see also [ 28 ] ) . Some of Kinnunen and Simon ' s results are counter - intuitive . For example , they find that success does not always lead to a greater sense of self - efficacy ; their data clearly indicate that the role of affect in programming education is complex . However , they also identify the " Goldilocks " pattern described above , in that " . . . too - easy tasks do not enhance efficacy perceptions , too - hard tasks diminish efficacy perceptions " ( pg . 21 ) . Noting that consistent , repeated negative self - assessment can lead directly to failure ( i . e . dropping out of the course ) , their results further illustrate the importance of keeping students as much as possible in a cognitive and affective " sweet spot " , where they are experiencing a sufficient , but not overwhelming sense of challenge , which they are usually able to resolve to a state of deep - learning and a sense of personal accomplishment . Both of these areas of research highlight the potential impact of student affect in programming education . Unfortunately , in both approaches the data collection tools are very sophisticated and very expensive in terms of resources ( physical and human ) and time . We are interested , therefore , in exploring alternative methods that could quickly , easily , and cheaply provide insight into student affect . While such methods would not provide data of equivalent complexity and depth , they might still be able to direct programming educators in the design of curricular materials [ cf . 11 ] and identify a need for intervention with students at risk of failure [ cf . 26 ] . To this end , we have built a simple automated tool that collects student self - reports on a variety of subjective and affective dimensions related to their CS1 experience 1 . The tool takes only minutes to use and can be presented at each class session with minimal disruption . In the following sections of this manuscript we describe in detail the structure of the tool and the results of our first year of its use . Our focus is on the following research questions :  Can useful data about student affect in a programming course be collected quickly and easily ?  Can such data provide insight into the design of curricular materials to help educators keep their students in the affective " sweet spot " ? 1 Source code for all parts of the system available from the 1st author . 41  Can such data identify , in the critical early stages , vulnerable students who would benefit from additional tutor support ? 2 EXPERIMENTAL AND COMPUTATIONAL DETAILS Participants were 72 first year students in the Bachelor of Information Technology Programme at Otago Polytechnic in Dunedin , New Zealand . 48 students took the CS1 class in the first semester ( February to June ) of 2016 ; 31 students took the class in the second semester ( July to November ) . 7 students who failed in the first semester repeated the class in the second semester . For those students , only data from their first semester were included in the study , to preserve its focus on novice programmers . One student was excluded from the analysis because he gave identical responses for every question on every lab , indicating that he was not following the tool usage instructions . Use of the response collection tool ( see details below ) was voluntary and informed consent was obtained from all participants before their data were included in data summaries . The response tool was employed at the end of each class session in which an in - class practical exercise was performed ( 26 sessions for each semester ) . The tool is a desktop application launched by an executable file resident on a server accessible from all classroom computers . The application is written in C # , and responses are saved in an MSSQL database located on a remote server . A student launches the tool when he or she has completed the day ' s practical work , and the instructor or teaching assistant enters a password to confirm the student ' s identity and completion of the programming exercise . The tutor then moves away ( to support confidentiality ) , and the student is presented with the questions for the session . At each session , the student is given three separate question items , each in the form of a two - dimensional grid . A screenshot of the first of the three questions is shown in Figure 1 . Figure 1 : Typical response screen Each question grid is divided into 20 x 20 small squares . The two axes describe two attributes of the session exercise , or of the student ' s experience with the session ( see details below ) . The instructions , presented at the top of the screen , are " Click on the graph at the point that best describes your opinion of today ' s lab " . A red dot appears in the clicked location , as shown in Figure 1 . After making a response , students click a button to move to the next question grid . They also have the option of clicking a " Skip This Question " button to proceed through the tool without supplying responses . Interaction with the tool takes only a few minutes at each lab . All responses are programmatically written to the remote database . An additional utility program is used to query the database and output the results as CSV files that can be imported into Excel , SPSS or other data analysis software . All data collection and analysis is performed by a non - teaching member of the research team . Students are informed of this protocol at the start of the semester to encourage frank responses . The three grids therefore comprise six response scales , in three sets of two . The response scales cover elements of the affective experience which have been shown in previous research to be pedagogically relevant , and are combined in pairs which , based on existing theory , could be expected to potentially be interactive ( see , for example , [ 15 ] ) . The combination of axes reduces the number of responses required , making the tool quicker and presumably less onerous for respondents . In addition , we believe that requiring a response simultaneously on two dimensions encourages a holistic response , perhaps avoiding the worrisome " always click on the largest value " pattern sometimes seen with single dimension Likert scales [ 20 ] . The response scales are summarised in Table 1 . The " scale descriptor " was not shown to the student , but merely provides a convenient terminology for discussion . Responses were recorded as whole numbers ranging from - 10 to 10 on each axis ( - 10 being the leftmost position for horizontal scales and the lowest position for vertical scales ) , but no numerical labels were displayed on the student ' s response screen . Table 1 : Affect Tool Grid Axes The grid pairs thus each relate to a particular feature of programming pedagogy : Difficulty and Interest are paired to observe the extent to which perceived challenge is related to engagement . Plan and Familiarity are paired to observe the extent to which students employ learned schemata in solving programming problems . Satisfaction and Improvement are paired to observe the extent to which students experience overtly the cognitive disequilibrium cycle . Scale Descriptor Scale Endpoint ( - 10 ) Scale Endpoint ( + 10 ) Axis 1 Interest Boring Interesting Horizontal Difficulty Easy Hard Vertical 2 Plan I didn ' t know how to approach these exercises . I had a clear plan for these exercises Horizontal Familiarity Content was all new Content was familiar Vertical 3 Satisfaction I feel frustrated I feel triumphant Horizontal Improvement My programming skills have not improved My programming skills have improved Vertical 42 3 RESULTS AND DISCUSSION 3 . 1 Insight into Curricular Materials In this section , we demonstrate how our simple response tool can produce " profiles " for individual labs 2 . The purpose of this analysis is not to present an evaluation of our specific course material , but to illustrate how the tool can be used to guide curricular development . For each lab , we can compute means for each of the six response scales , averaged across students . Together , these six scale means produce a useful descriptive summary of an individual lab ; several examples are discussed below . However , the grand scale mean ( i . e . the value obtained by averaging all responses on a single scale , across all students and all labs ) varies considerably between the different response scales . For example , the grand mean on the Interest scale is 5 . 9 , whereas the grand mean on the Difficulty scale is 1 . 09 . Therefore an Interest score of 3 . 0 is relatively low , while a Difficulty score of 3 . 0 is relatively high . We cannot sensibly compare these two scores directly . To allow us to make direct comparisons between scales , we must therefore normalise the average responses for each lab on each scale as : ( raw scale mean for this lab - grand mean for this scale ) / ( standard deviation for this scale across all labs and students ) . In our above example , an Interest score of 3 . 0 has a negative normalised value and a Difficulty score of 3 . 0 has a positive normalised value , reflecting their correct relative relationships . For each lab , we compute the normalised mean response on each of the six scales . The six normalised means produce a " lab profile " that gives a snapshot of the material which is easy to view and understand . Here we show a selection of labs whose profiles illustrate particular situations . Figure 2 : Interesting and Easy Profile Figure 2 shows the profile of the first lab of the semester ( Introduction to the IDE ) . By simple inspection , we can see that students found this lab to be interesting ( normalised mean on the Interesting scale = 1 . 24 ) and easy ( normalised mean on the Difficulty scale = - 2 . 56 ) . This lab is intended to be a gentle , 2 A detailed description of the content and learning objectives of each lab can be found at http : / / kate . ict . op . ac . nz / ~ dale / LabObjectives . pdf introductory exercise for the first day of classes , and therefore this pattern is appropriate . Figure 3 : Challenging Profile In contrast , Figure 3 shows the profile of Lab 2 ( Variables ) , the first " real " lab , where technical material is introduced . The Familiarity scale runs from New ( - ) to Familiar ( + ) . The mean Familiarity score of - 1 . 32 therefore indicates that students found the material to be very novel . This is to be expected , as this lab introduces data types , which students having no prior programming experience would have not previously encountered . The profile shows that the lab was both easy ( Difficulty scale mean = - 1 . 53 ) and a little boring ( Interest scale mean = - 0 . 95 ) . However , the negative mean ( - 0 . 79 ) on the Satisfaction scale ( negative values = Frustrated ; positive values = Triumphant ) indicates that the task was sufficiently challenging to cause some students to report a non - trivial degree of frustration . Again , this pattern is appropriate for the intended purpose of the lab , which was to introduce completely new material . Not all lab profiles indicate that curricular materials are having the desired impact . Figure 4 shows the profile for the third lab of the semester ( Data Types ) , and may indicate a problem with the lab ' s design . Students find this lab both Boring ( - 1 . 66 on the Interest scale ) and Frustrating ( - 2 . 53 on the Satisfaction scale ) . Although the material is not overwhelmingly seen as novel ( 0 . 01 on the Familiarity scale ) students report not knowing how to approach it ( - 2 . 10 on the Plan scale ) . Most critically , having completed it , students don ' t feel that they have improved ( - 1 . 36 on the Improvement scale ) . This profile may be a signal that the course material needs reworking . As discussed above , a certain amount of challenge ( difficulty and / or frustration ) is beneficial and may , in fact , be essential to learning . However , students must be able to resolve their cognitive disequilibrium to achieve improved learning . With this lab , it appears that students were not , on average , successfully achieving that resolution . This lab covers arithmetic operations and the formatting of numerical output when using floating point numbers . This topic is more fussy than inherently difficult , and the lab profile may indicate the need for tasks that are better motivated , more fully contextualised , or simply more fun . 1 . 24 - 2 . 56 - 0 . 01 0 . 15 0 . 87 - 0 . 66 - 3 . 00 - 2 . 00 - 1 . 00 0 . 00 1 . 00 2 . 00 3 . 00 Interest Difficulty Satisfaction Familiarity Plan Improvement N o r m a li s e d S c a l e M e a n Normalised Scale Mean Profile for Lab 1 - 0 . 95 - 1 . 53 - 0 . 79 - 1 . 32 0 . 71 0 . 18 - 3 . 00 - 2 . 00 - 1 . 00 0 . 00 1 . 00 2 . 00 3 . 00 Interest Difficulty Satisfaction Familiarity Plan Improvement N o r m a li s e d S c a l e M e a n Normalised Scale Mean Profile for Lab 2 43 Figure 4 : Problematic Profile Sometimes , the lab profile seems to indicate a crisis . Figure 5 shows a lab that is seen as hard , boring and frustrating . Students don ' t have clear plans , and even after completing the exercise , they don ' t feel they have improved . This is either a very bad lab , or it is the student ' s first glimpse of an especially opaque topic . There is some evidence that it may be the latter - - this lab is the introduction to arrays . Anecdotally , this first complex data structure poses a significant challenge to many new programming students . Given this context , it is informative to look at the subsequent lab - - the second in a two - part series on arrays . This lab is shown in Figure 6 . In this second session on arrays , things have calmed down considerably . While students still don ' t feel they have a clear plan , they are finding the material more interesting , less difficult and less frustrating . They also feel that , compared with the previous lab , this one has improved their skills . Viewing these two labs as a pair , then , we might revise any initial impression that Lab 12 was poorly designed , or too difficult . Perhaps Lab 12 takes the students into the state of cognitive disequilibrium identified by Bosch and others as an essential precursor to deep learning . In Lab 13 students are able to resolve that challenge and its consequent sense of frustration , advancing toward mastery of the new skill . Instructors would wish to analyse carefully the specific contents of the labs to distinguish between these alternatives . Figure 5 : Possible Crisis Lab Profile Figure 6 : Resolution Lab Profile Similar analysis identifies a variety of typical patterns . For example , high Familiarity and high Improvement scores are what one would like to see in a task that is intended to extend an existing skill , rather than introduce a completely new skill . A revision lab should show high Familiarity and Plan , with low Difficulty and Improvement . And so on . The profiles are easy to prepare and interpret . By viewing the lab profile for each classroom or homework activity , the lecturer can potentially identify a range of problems and shortcomings . Careful inspection of the course materials , combined with related evidence such as the frequency and nature of student queries during the activity 3 , can guide the lecturer to the resolution of these problems . 3 . 2 Insight into Student Performance Our third primary research question is whether our simple response tool can provide insight into the relationship between the student affective experience and student success in a CS1 course . In this section , we present a detailed discussion of the data we obtained during the year of study , and identify some response patterns which are statistically predictive of performance difficulties . We suggest that lecturers monitor students for these response patterns and intervene as appropriate . To correct for the increased risk of Type I error ( false alarm ) when performing multiple hypothesis tests , we use Simes ' modified Bonferroni procedure [ 27 ] throughout . Scale Correlations : We begin with the paired response scales . The scales were paired based on our a priori expectation of relationships between them , as discussed above . The most direct measure of the relationships between our paired scales is the linear correlation of responses to those scales . We compute the mean score for each student on each scale , collapsing across labs . We then use these values to compute the Pearson Product Moment correlations between the pairs of scales comprising the axes of each of the three response grids . The results are shown in Table 2 . 3 Our thanks to Anonymous Reviewer 3 for this suggestion . - 1 . 66 0 . 20 - 2 . 53 0 . 01 - 2 . 10 - 1 . 36 - 3 . 00 - 2 . 00 - 1 . 00 0 . 00 1 . 00 2 . 00 3 . 00 Interest Difficulty Satisfaction Familiarity Plan Improvement N o r m a li s e d S c a l e M e a n Normalised Scale Mean Profile for Lab 3 - 1 . 27 1 . 00 - 2 . 39 - 1 . 59 - 1 . 20 - 2 . 27 - 3 . 00 - 2 . 00 - 1 . 00 0 . 00 1 . 00 2 . 00 3 . 00 Interest Difficulty Satisfaction Familiarity Plan Improvement N o r m a li s e S c a l e M e a n Normalised Scale Mean Profile for Lab 12 0 . 04 0 . 22 - 0 . 25 0 . 19 - 0 . 83 0 . 65 - 3 . 00 - 2 . 00 - 1 . 00 0 . 00 1 . 00 2 . 00 3 . 00 Interest Difficulty Satisfaction Familiarity Plan Improvement N o r m a li s e d S c a l e M e a n Normalised Scale Mean Profile for Lab 13 44 Table 2 : Paired Scale Correlations * uncorrected p = . 031 ; not significant by Simes . * * uncorrected p < . 001 , significant at α = . 05 by Simes . There is a very small , non - significant negative correlation between the Interest and Difficulty scales . Our expectation was that Difficulty and Interest would be positively correlated , in that labs that were considered more difficult would have been viewed as more interesting . The data do not support this prediction . However the situation is complex , as we can see by looking at the individual Interest x Difficulty correlations separately for each lab . Of the 23 individual labs , 12 show a negative Interest x Difficulty correlation ( students who regarded the lab as easier tended to regard the lab as more interesting ) ; the remaining 11 show the expected positive correlation . Thus the low overall correlation does not seem to reflect the absence of a relationship between difficulty and interest , but rather a distribution of relationships across labs which , when averaged , effectively cancel each other out . Thus the relationship between difficulty and interest seems to be highly dependent on the specific material - - perhaps on the perceived cause of the difficulty . Labs that are difficult because they involve challenging tools or problems may be seen as interesting . Labs that are difficult because they are unclear , or require fussy attention to detail may be seen as boring . A more thorough exploration of this pattern could provide helpful insights for materials construction . There is a moderate , marginally - significant , positive correlation between the Plan and Familiarity scales ( uncorrected p = . 031 ) . In our observed data , when a lab is more familiar , students have a clearer plan of how to approach it . This is consistent with general theories of programming skill development which acknowledge the importance of pattern recognition and schematic learning [ 25 ] . The pattern is consistent across labs , with a positive correlation observed for 22 of 23 labs ( 96 % ) . There is a strong , significant , positive correlation between Satisfaction and Improvement . When highly frustrated , students generally did not feel that their skills were improving . Alternatively , a sense of improvement was coupled with a sense of triumph ( low frustration ) . This is generally as expected - - as described by Kinnunen & Simon [ 14 ] a high degree of frustration does not improve student ' judgments of self - efficacy , and prolonged or extreme frustration should be avoided . This effect is very consistent , with all 23 labs showing a positive correlation on these two scales . The grid axis pairs were chosen to facilitate exploration of the specific interactions they illustrate . However , a relationship may exist between any pair of scales , and these correlations can also be computed . The complete correlation matrix for the six scales is shown in Table 3 . Table 3 : All Scales Correlation Matrix * uncorrected p < . 05 ; not significant by Simes . * * significant at α = . 05 by Simes A number of additional significant correlations are found , demonstrating the interconnection between pedagogical features relating to the broader student experience . The Interest scale is positively correlated with Satisfaction ( marginally significant ) and Improvement . Labs that are more interesting are associated with a higher sense self - efficacy and , for this sample , of triumph . ( Or alternatively , labs which make students feel triumphant and skilled are judged as being more interesting . ) The Plan scale is negatively correlated with Difficulty and positively correlated with Satisfaction and Improvement . Students describe those labs for which they have a clear plan as being easier , as would be expected , but they also report a greater sense of triumph and self - efficacy when they have a clear plan . These results highlight again the Goldilocks principle : practical exercises must be challenging enough to move students forward , but not so difficult as to make them feel lost . A Principal Component Analysis 4 shows congruent results , producing two highly loaded components - - one comprised of the Improvement , Interest , Satisfaction and Plan scales , the second comprised of the Difficulty and Familiarity scales . While PCA is not generally used to discover theoretically important underlying factors ( for that , Factor Analysis is more appropriate ) the discovered components do often divide themselves into semantically or pragmatically distinct groups . In this case , it could be argued that the four scales in Component 1 describe the student : his or her subjective experience of improving , being interested , feeling triumphant and having a plan . The two scales in Component 2 , in contrast , speak only to the character of the course materials : their perceived difficulty and novelty . Thus , these two elements - - how students view the course materials and how they view the classroom experience - - appear to be fairly distinct . Educators should be aware of this disconnect - - concentrating solely on the nature of curriculum and materials may not give us sufficient insight into the emotional life of the student . Relationship to Performance : The scale analysis gives some insight into the student affective experience in CS1 . However , as programming educators our particular interest is in promoting good student performance . To this end we explore the extent to which scale responses are related to student performance . 4 Varimax rotation . Complete computational details available from the 1st author . 45 Throughout this discussion , we will use final mark in the class as a measure of student performance . An alternative metric which might be a more accurate reflection of programming skill would be marks on software development projects [ 21 ] , but for these students , the two scores are highly correlated ( r = . 92 ) and , while every student has a final mark , some students do not have an assignment mark ( because they failed to submit the major assignment ) . The correlations between a student ' s mean scale response across lab and his or her final percentage mark are shown in Table 4 . Table 4 : Scale by Final Mark Correlations * significant at α = . 05 by Simes Responses on the Interest , Familiarity and Difficulty scales are not significantly correlated with final mark . For the first two scales , this is not unexpected . Assuming that course materials have been carefully designed so as to not be intractable for introductory level students , the inherent interest level of a lab should not necessarily be determined by one ' s ability to solve the problems it contains . Similarly , recognition of content as novel or familiar should not be dependent on programming facility However , it is somewhat surprising that there is no evidence in our data that Difficulty responses are predictive of eventual class performance . One would expect weak students to rate labs as more difficult than do strong students . It may be of concern that students who are , based on performance , having more difficulty completing programming tasks do not consistently report them as being difficult . The Satisfaction , Plan and Improvement scales are all significantly correlated with final mark . In each case , the observed correlation is positive , meaning higher responses on the scale are associated with higher final mark . Students who feel more triumphant , have clearer plans , and increasing self - efficacy tend to get better grades in this programming course . Students who submit relatively low scores on these three scales tend to perform poorly in the course , and we would suggest that educators could treat such results in their own students as an indicator of potential future difficulty 5 . These results are congruent with patterns predicted by the affect and self - efficacy literature . Thus they support our hope 5 Due to the high correlations between scales , a stepwise multiple regression involving all six scales cannot improve on the prediction accuracy of the single predictor model for Satisfaction , thus detailed regression analyses are not presented . that the response tool , though extremely easy to administer , can provide useful information about students ' affective experience . The above analyses were performed using data from the full 16 - week semesters . Interestingly , the observed patterns actually begin to emerge much earlier . After only 8 weeks ( the end of the first term ) , the Satisfaction ( r = . 296 ; p = . 013 ) and Plan ( r = . 261 ; p = . 029 ) scales are already positively correlated with final mark . In view of the compelling evidence for the value of early intervention for programming students at risk [ 26 ] , these results indicate that it may be useful to begin inspection of students ' affective state very early in their training . Quadrant Analyses : An alternative view using the interactions between the main paired response scales ( Interest and Difficulty ; Plan and Familiarity ; Satisfaction and Improvement ) can be gained by considering each response not in terms of its absolute numerical value , but in terms of the grid quadrant in which it is placed . That is , a response with negative values on both the horizontal and vertical scales is in the lower left quadrant ; a positive horizontal value and a negative vertical value is in the lower right quadrant , and so on . Using the Difficulty / Interest grid as an example , a score in the lower left quadrant indicates that the student found the lab Boring and Easy ( to the degree that the numerical values were below 0 ) . A score in the upper right quadrant indicates that the student found the lab Interesting and Hard . For each of the three pairs of scales , we can observe the frequencies of response in each quadrant , as a function of student performance . Of particular interest is the proportion of responses which fall in the Hard / Boring quadrant . As discussed above , while numerous models of learning have proposed a positive relationship between difficulty and engagement , they all maintain that too much difficulty can push the student into a pathological learning state , which would presumably be described as hard and boring . The pattern is evident in our data : the proportion of responses in the Hard / Boring quadrant is negatively correlated with final mark ( r = - 0 . 257 , p = . 027 , marginally significant by Simes ) . That is , students who will eventually earn a poor grade in the class tend to place a higher proportion of responses in the Hard / Boring quadrant than do students who will eventually earn a good grade in the class . A similar analysis can be performed on the other two grids . This analysis identifies a second quadrant - " No Plan / Familiar " - where frequent responses are highly predictive of a low final mark ( r = - 0 . 393 , p = . 001 , significant at α = . 05 by Simes ) . This pattern is predicted by the schemata - learning models of [ 25 ] , which describe the progression from novice to expert programmer as , in part , the development of the ability to recognise common patterns in programming problems and to match them to learned solution structures . A student who recognises a problem but cannot identify the corresponding solution pattern has not successfully built the necessary schema . We suggest that the Hard / Boring and No Plan / Familiar quadrants are " danger cells " . Students who respond in these cells relatively frequently have a strong tendency to earn low final marks . We would suggest that students be closely observed for responses in the danger cells . Students demonstrating this 46 response pattern may benefit from early lecturer intervention and other additional support . Student Profiles : Earlier we saw that inspection of the six scale means could provide a " profile " of an individual lab . Given the various identified relationships between student performance group and scale responses , a similar approach could potentially be used to gain a better understanding of individual students . We begin by computing each student ' s six average scale scores . To correct for the differences in the range of absolute responses for the scales discussed above , we can convert these scores to normal scores allowing us to see how each student responded on each scale , relative to the overall distribution of scores on that scale . As before , a positive normal score means the original raw score was above the grand mean , and a negative normal score means the original raw score was below the grand mean ( in standard deviation units ) . We can then average these normalised scores across each of the four letter grade categories ( Failing ; C range ; B range ; A range ) . Figure 7 presents these averages with the scales ordered to make the patterns clearer upon inspection . Figure 7 : Scale Means by Letter Grade The figure shows that weakest students give relatively low scores on the Improvement , Plan , Satisfaction and to a lesser extent , Interest scales . Moving through the grade categories , the values on these scales tend to increase with increasing final mark . This pattern is in concert with the earlier regression analyses , but the use of normalised means allows us to gain a complete picture for an individual student in a single graph - - a " student profile " . ( To increase clarity in the profile images , we omit the Familiarity and Difficulty scores which have been shown to not be predictive of performance in any analysis of our data ) . For example , Figure 8 compares the normalised scale means for a weak student ( mark = 30 ) and a strong student ( mark = 95 ) . The negative scores and slope of the weaker student are clearly distinguished from the positive scores and slope of the stronger student . This type of illustration is easy to prepare and interpret - - the " weak " pattern may indicate the need for intervention and additional support . Figure 8 : Profiles of High and Low Performing Students Of course , not every student will follow this simple pattern . Deviations can provide additional useful insight beyond the simple prediction of performance difficulties . For example , Figure 9 shows the normalised scale means for a student who earned a 92 % mark in the class . This student violates the strong student pattern with a surprisingly low Satisfaction score ( i . e . this student reported unusually high levels of frustration on the Frustration to Triumph scale ) . In spite of performing well on assessments , this student is having an affective experience which has been identified by numerous studies as toxic . An educator has the opportunity to improve this student ' s educational experience by providing tools or other support for managing frustration . Figure 9 : Strong Student with Extreme Frustration Figure 10 shows the profile of another strong student who deviates from the standard pattern . This student , despite performing well on assessments , consistently reports that he does not have a plan for approaching assigned lab tasks . One can imagine this student feels that he is attacking problems somewhat randomly , an affective state that is unlikely to lead to confidence in his own ability to continue to perform at a high level . A student exhibiting this pattern might benefit from additional training in the mechanical tools of software development , such as activity diagrams , UML , or flowcharts to - 0 . 80 - 0 . 60 - 0 . 40 - 0 . 20 0 . 00 0 . 20 0 . 40 0 . 60 Fam . Diff . Int . Imp . Plan Sat . N o r m a li s e d G r o up S c a l e M e a n s Normalised Scale Means by Grade Category A Grades B Grades C Grades Failing Grades - 4 . 00 - 3 . 00 - 2 . 00 - 1 . 00 0 . 00 1 . 00 2 . 00 3 . 00 Interest Improvement Plan Satisfaction N o r m a li s e d S c a l e M e a n s Student Profiles for High and Low Performing Students Mark = 95 Mark = 30 - 1 . 00 - 0 . 80 - 0 . 60 - 0 . 40 - 0 . 20 0 . 00 0 . 20 0 . 40 0 . 60 0 . 80 Interest Improvement Plan Satisfaction N o r m a li s e d S c a l e M e a n s Student Profile . Final Mark = 92 47 allow her to impose more structure on her programming process . Figure 10 : Strong student with poor planning skills 4 FUTURE DIRECTIONS The data presented in this manuscript were obtained over two semesters of CS1 . The variety of clear patterns in the results encourages us to believe that the Affect Tool can provide a quick , non - invasive method of acquiring useful information about the nature and efficacy of our CS1 teaching materials and methodology . We intend to continue to use the tool in our introductory programming courses , developing the project along two simultaneous fronts – extensions to the functionality of the tool itself , and the use of individual patterns of response to support early identification of students at risk of failure . The current version of the Affect Tool is a desktop C # application and requires code modification and recompilation to change the grid label axes , numbers of questions , etc . It also requires that each classroom computer has access to the tool’s executable . To simplify delivery , we are currently porting the tool to a web - based implementation that can be accessed with a browser . This version will provide an authoring facility via which instructors can add grids and / or modify axis labels dynamically . The current version of the tool stores all student responses in an MSSQL database and provides a variety of built - in methods to perform general SQL queries against this database . Subsequent detailed analysis can then be performed using standard statistical software . The web - based version of the tool will continue to implement the back - end database for flexibility , but will include automated reporting tools for core summaries such as those presented in this manuscript . It will also provide automated summary reporting of responses for individual students ( across all labs ) and individual labs ( across all students ) at any point during the CS1 semester . As described earlier , certain patterns of responses ( for example , frequent selection of the “No Plan / Familiar” quadrant ) are correlated with poor eventual performance in the course . Our hypothesis is that the " at risk " patterns which are observable over the course of an entire semester might also serve as early warning signs of trouble . One of our highest priorities is early identification of problems to allow for early intervention . It is our experience that in CS1 , if a student falls behind any significant amount , he or she often fails the course . Thus we wish to be able to identify students at risk very early [ cf . 26 ] . Therefore , in next year ' s course offering , we intend to use the extended reporting functionality of the Affect Tool ( see above ) to monitor students on a lab - by - lab basis for the " at risk " patterns . When those patterns are observed , we will provide intervention in the form of increased tutor support and practice exercises . We will then be able to compare overall marks and pass rates for course offerings with and without this early intervention programme . Any observed improvements would encourage us to believe that the tool data are , in fact , effective in identifying students at risk before they fall irretrievably behind . This protocol will also give us the opportunity to explore the efficacy of various types of early intervention . 5 CONCLUSIONS Decades of research into the technical and pedagogical aspects of computer programming education have failed to eliminate the high failure and drop - out rates which have traditionally plagued the discipline . In response , many researchers are taking a nuanced perspective , which encompasses students ' pastoral and emotional experiences . While the results of this research have been illuminating , the experimental processes are often longitudinal , time - consuming and / or very costly in both human and technical resources . In an effort to find a more economical way of exploring student affect , we constructed a simple digital response tool that can be easily deployed , used , and interpreted . Our analyses of the data obtained indicate that student responses using this tool can highlight quite specific problems with course materials . Further , their patterns of responses can serve as a predictor of performance problems . In particular , students who are finding classroom exercises both difficult and boring , or who recognise a programming problem as involving familiar material , but have no clear plan to solve the problem , are particularly likely to struggle as the course proceeds . We also found that a simple plot of a student ' s normalised mean responses to four of the response scales ( Interest , Improvement , Plan and Satisfaction ) can not only paint an eloquent picture of students at risk of failure , but can also illustrate quite specific problematic issues for students whose overall performance is strong . The evidence for the impact of students ' emotional experience on learning is mounting . We encourage further exploration of all techniques to gain insight into that experience , and into the ways which our better understanding can be used to improve pedagogical practice in CS1 . AKNOWLEDGEMENTS We would like to thank Professor Jeffrey Miller of the University of Otago , and an anonymous reviewer , for feedback on the statistical analyses . - 2 . 50 - 2 . 00 - 1 . 50 - 1 . 00 - 0 . 50 0 . 00 0 . 50 1 . 00 Interest Improvement Plan Satisfaction N o r m a li s e d S c a l e M e a n s Student Profile . Final Mark = 93 48 REFERENCES [ 1 ] R . Baker , S . K . D ' Mello , M . Rodrigo and A . C . Graesser . 2010 . Better to Be Frustrated than Bored : The Incidence , Persistence , and Impact of Learners’ Cognitive - Affective States during Interactions with Three Different Computer - Based Learning Environments . International Journal of Human - Computer Studies April 2010 . [ 2 ] R . S . Baker , S . M . Gowda , M . Wixon , J . Kalka , A . Z . Wagner , A . Salvi , V . Aleven , G . W . Kusbit , J . Ocumpaugh , and L . Rossi . 2012 . Towards sensor - free aﬀect detection in cognitive tutor algebra . In Proceedings of the 5th International Conference on Educational Data Mining ( EDM 2012 ) , 126 – 133 . [ 3 ] I . Blanchette and A . Richards . 2010 . The influence of affect on higher level cognition : A review of research on interpretation , judgement , decision making and reasoning . Cognition and Emotion , 2010 , 24 ( 4 ) , 561 _ 595 . [ 4 ] N . Bosch . 2016 . Detecting Student Engagement : Human Versus Machine UMAP ' 16 , July 13 - 17 , 2016 , Halifax , NS , Canada [ 5 ] N . Bosch and S . D’Mello . 2013 . Sequential Patterns of Affective States of Novice Programmers . The First Workshop on AI - supported Education for Computer Science ( AIEDCS 2013 ) . 1 - 10 . [ 6 ] N . Bosch , S . K . D’Mello , J . Ocumpaugh , R , S , Baker and V . Shute . 2016 . Using Video to Automatically Detect Learner Affect in Computer - Enabled Classrooms , ACM Transactions on Interactive Intelligent Systems , 6 , 2 , 17 . [ 7 ] J . Carter and P . Dewan . 2010 . Design , Implementation , and Evaluation of an Approach for Determining When Programmers are Having Difficulty . GROUP’10 , November 7 – 10 , 2010 , Sanibel Island , Florida , USA . [ 8 ] S . D . Craig , A . C . Graesser , J . Sullins and B . Gholson . 2004 . Affect and learning : an exploratory look into the role of affect in learning with AutoTutor . Journal of Educational Media , 29 , 3 , 241 - 250 , Oct 2004 [ 9 ] S . D’Mello , N . Person and B . Lehman . 2009 . Antecedent - Consequent Relationships and Cyclical Patterns between Affective States and Problem Solving Outcomes . Proceedings of the 2009 conference on Artificial Intelligence in Education : Building Learning Systems that Care : From Knowledge Representation to Affective Modelling . 57 - 64 . [ 10 ] A . Graesser and S . K . D’Mello . 2012 . Emotions during the learning of difficult material . In B . Ross ( Ed . ) , Psychology of Learning and Motivation , 57 , 183 - 226 . [ 11 ] S . Hansen and E . Eddy . 2007 . Engagement and Frustration in Programming Projects . SIGCSE ' 07 , March 7 – 10 , 2007 , Covington , Kentucky , USA . [ 12 ] E . F . Iepsen , M . Bercht and E . Reategui . 2013 . Detection and Assistance to Students Who Show Frustration in Learning of Algorithms , Proceedings of IEEE Frontier in Education Conference , October 2013 , 1183 - 1189 . [ 13 ] A . Kapoor , W . Burleson and R . W . Picard . 2007 . Automatic prediction of frustration . Int . J . Human - Computer Studies , 65 , 724 - 736 . [ 14 ] P . Kinnunen and B . Simon , B . 2011 . CS Majorsʼ Self - Efficacy Perceptions in CS1 : Results in Light of Social Cognitive Theory . ICER’11 , August 8 – 9 , 2011 , Providence , Rhode Island , USA . [ 15 ] M . Kleine , T . Goetz , R . Pekrun and N . Hall . 2005 . The structure of students ' emotions experienced during a mathematical achievement test . Zentralblatt für Didaktik der Mathematik ( ZDM 2005 ) , 37 , 3 . [ 16 ] B . Kort , R . Reilly and R . W . Picard . 2001 . An Affective Model of Interplay Between Emotions and Learning : Reengineering Educational Pedagogy - Building a Learning Companion , Proceedings of the 2nd IEEE Int’l Conf . Advanced Learning Technologies ( ICALT 2001 ) , IEEE CS Press . [ 17 ] D . Lee , M . Rodrigo , R . Baker , J . d , Sugay and A . Coronel . 2011 . Exploring the relationship between novice programmer confusion and achievement . In : S . D’Mello , A . Graesser , B . Schuller and J . C . Martin , J . C . ( eds . ) Affective Computing and Intelligent Interaction . pp . 175 – 184 . Springer , Berlin Heidelberg ( 2011 ) . [ 18 ] M . J . Lee and A . J . Ko . 2011 . Personifying Programming Tool Feedback Improves Novice Programmers ' Learning . ICER’11 , August 8 – 9 , 2011 , Providence , RI , USA . [ 19 ] Z . Liu , V . Pataranutaporn , J . Ocumpaugh , R . Baker ( 2013 ) Sequences of Frustration and Confusion , and Learning . Proceedings of the 6th International Conference on Educational Data Mining , 114 - 120 . [ 20 ] A . W . Meade and S . B . Craig . 2011 . Identifying Careless Responses in Survey Data . Proceedings of the 26th Annual Meeting of the Society for Industrial and Organizational Psychology , Chicago , IL . [ 21 ] D . Parsons , K . Wood and P . Haden . 2015 . What are we Doing When we Assess Programming ? Proceedings of the 17th Australasian Computing Education Conference ( ACE 2015 ) , Sydney Australia , 27 - 30 January . 2015 . [ 22 ] Pears , A . , East , P . , McCartney , R . , Ratcliffe , M . B . , Stamouli , I . , Berglund , A . , Kinnunen , P . , Moström , J - E . , Schulte , C . , Eckerdal , A . , Malmi , L . , Murphy , L . , Simon , B . and Lynda Thomas . 2007 . What ' s the problem ? : teachers ' experience of student learning successes and failures . Proceedings of the Seventh Baltic Sea Conference on Computing Education Research , November 15 - 18 , 2007 , Koli National Park , Finland [ 23 ] R . Pekrun , T . Goetz , L . Daniels , R . Stupinsky and R . Perry . 2010 . Boredom in Achievement Settings : Exploring Control – Value Antecedents and Performance Outcomes of a Neglected Emotion . Journal of Educational Psychology , 102 , 3 , 531 – 549 [ 24 ] R . Pekrun , T . Goetz , W . Titz and R . Perry . 2002 . Academic Emotions in Students’ Self - Regulated Learning and Achievement : A Program of Qualitative and Quantitative Research , Educat ional Psychologist , 37 , 2 , 91 - 106 [ 25 ] A Robins , J . Rountree and N . Rountree . 2003 . Learning and Teaching Programming : A Review . Computer Science Education . 13 , 2 , 137 - 172 . [ 26 ] A . Robins . 2010 . Learning Edge Momentum : A new account of outcomes in CS1 . Computer Science Education , 20 , 1 , 37 - 71 . [ 27 ] Simes , R . J . 1986 . An improved Bonferroni procedure for multiple tests of significance . Biometrika , 73 , 3 , 751 - 754 . [ 28 ] M . J . Scott and G . Ghinea . 2014 . Measuring Enrichment : The Assembly and Validation of an Instrument to Assess Student Self - Beliefs in CS1 . ICER’14 , August 11 - 13 , 2014 , Glasgow , Scotland , UK . [ 29 ] V . Shute , S . D ' Mello , R . Baker , C . Cho , N . Bosch , J . Ocumpaugh , M . Ventura and V . Almeda , V . 2015 . Modeling how incoming knowledge , persistence , affective states , and in - game progress influence student learning from an educational game . Computers and Education , 86 , 224 - 235 . [ 30 ] A . Vihavainen , J . Airaksinen and C . Watson . 2014 . A Systematic Review of Approaches for Teaching Introductory Programming and Their Influence on Success ICER ’14 , August 11 - 13 2014 , Glasgow , United Kingdom [ 31 ] F . T . Villavicencio and A . B . Bernardo . 2013 . Positive academic emotions moderate the relationship between self - regulation and academic achievement . British Journal of Educational Psychology ( 2013 ) , 83 , 329 – 340 . [ 32 ] M . G . Zikuda , I Stuchlikova , I . and T . Jank . 2013 . Emotional Aspects of Learning and Teaching : Reviewing the Field − Discussing the Issues Orbis scholae , 7 , 2 , 7 − 22 . 49