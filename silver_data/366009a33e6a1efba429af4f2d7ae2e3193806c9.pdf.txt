Consistent and Flexible Selectivity Estimation for High - Dimensional Data Yaoshu Wang 1 , Chuan Xiao 2 , 3 , Jianbin Qin 1 , Rui Mao 1 , Makoto Onizuka 2 , Wei Wang 4 , 5 , Rui Zhang 6 , and Yoshiharu Ishikawa 3 1 Shenzhen Institute of Computing Sciences , Shenzhen University , 2 Osaka University , 3 Nagoya University , 4 Dongguan University of Technology , 5 University of New South Wales , 6 www . ruizhang . info yaoshuw @ sics . ac . cn , { chuanx , onizuka } @ ist . osaka - u . ac . jp , { qinjianbin , mao } @ szu . edu . cn , weiw @ cse . unsw . edu . au , rui . zhang @ ieee . org , ishikawa @ i . nagoya - u . ac . jp ABSTRACT Selectivity estimation aims at estimating the number of database objects that satisfy a selection criterion . Answering this problem accurately and efficiently is essential to many applications , such as density estimation , outlier detection , query optimization , and data integration . The estimation problem is especially challenging for large - scale high - dimensional data due to the curse of dimension - ality , the large variance of selectivity across different queries , and the need to make the estimator consistent ( i . e . , the selectivity is non - decreasing in the threshold ) . We propose a new deep learning - based model that learns a query - dependent piecewise linear function as selectivity estimator , which is flexible to fit the selectivity curve of any distance function and query object , while guaranteeing that the output is non - decreasing in the threshold . To improve the ac - curacy for large datasets , we propose to partition the dataset into multiple disjoint subsets and build a local model on each of them . We perform experiments on real datasets and show that the pro - posed model consistently outperforms state - of - the - art models in accuracy in an efficient way and is useful for real applications . CCS CONCEPTS ‚Ä¢ Information systems ‚Üí Query optimization ; Entity resolu - tion ; ‚Ä¢ Computing methodologies ‚Üí Neural networks . KEYWORDS selectivity estimation ; high - dimensional data ; piecewise linear func - tion ; deep neural network 1 INTRODUCTION In this paper , we consider the following selectivity estimation prob - lem for high - dimensional data : given a query object x , a distance function ùëëùëñùë†ùë° ( ¬∑ , ¬∑ ) , and a distance threshold ùë° , estimate the number of objects o s in a database that satisfy ùëëùëñùë†ùë° ( x , o ) ‚â§ ùë° . This prob - lem is also known as local density estimation [ 54 ] or spherical range counting [ 9 ] in theoretical computer science . It is an essen - tial procedure in density estimation in statistics [ 52 ] and density - based outlier detection [ 11 ] in data mining . For example , for text analysis , one may want to determine the popularity of a topic ; for e - commerce , an analyst may want to find out if a user / item is an outlier ; for clustering , the algorithm may converge faster if we start with seeds in denser regions . In the database area , accu - rate estimation helps to find an optimal query execution plan in W . Wang , R . MaoandJ . Qinarethejointcorrespondingauthors . databases dealing with high - dimensional data [ 24 ] . Hands - off en - tity matching systems [ 16 ] extract paths from random forests and take each path ‚Äì a conjunction of similarity predicates over multi - ple attributes ( e . g . , ‚Äú EU ( name ) ‚â§ 0 . 25 AND EU ( affiliations ) ‚â§ 0 . 4 AND EU ( research interests ) ‚â§ 0 . 45‚Äù , where EU ( ) measures the Eu - clidean distance between word embeddings ) ‚Äì as a blocking rule , and efficient blocking can be achieved if we find a good query exe - cution plan [ 50 ] . In addition , many text or image retrieval systems resort to distributed representations . Given a query , a similarity selection is often invoked to obtain a set of candidates to be fur - ther verified by sophisticated models . Estimating the number of candidates helps to estimate the overall query processing time an end - to - end system to create a service level agreement . Selectivity estimation for large - scale high - dimensional data is still an open problem due to the following factors : ( 1 ) Large vari - ance of selectivity . The selectivity varies across queries and may differ by several orders of magnitude . A good estimator is supposed to predict accurately for both small and large selectivity values . ( 2 ) Curse of dimensionality . Many methods that work well on low - dimensional data , such as histograms [ 26 ] , are intractable when we seek an optimal solution , and they significantly lose accuracy with the increase of dimensionality . ( 3 ) Consistency requirement . When the query object is fixed , selectivity is non - decreasing in the threshold . Hence users may want the estimated selectivity to be non - decreasing and interpretable in applications such as density estimation . This requirement rules out many existing methods . To address the above challenges , we propose a novel deep re - gression method that guarantees consistency . We holistically ap - proximate the selectivity curve using a query - dependent piecewise linear function consisting of control points that are learned from training data . This function family is flexible in the sense that it can closely approximate the selectivity curve of any distance function and any input query object ; e . g . , using more control points for the part of the curve where selectivity changes rapidly . Together with a robust loss function , we are able to alleviate the impact of large variance across different queries . To handle high dimensionality , we incorporate an autoencoder that learns the latent representation of the query object with respect to the data distribution . The query object and its latent representation are fed to a query - dependent control point model , enhancing the fit to the selectivity curve of the query object . To ensure consistency , we achieve the monotonicity of estimated selectivity by converting the problem to a standard neural network prediction task , rather than imposing additional limitations such as restricting weights to be non - negative [ 15 ] or limiting to multi - linear functions [ 17 ] . To improve the accuracy a r X i v : 2005 . 09908v4 [ c s . D B ] 27 M a y 2021 YaoshuWang 1 , ChuanXiao 2 , 3 , JianbinQin 1 , RuiMao 1 , MakotoOnizuka 2 , WeiWang 4 , 5 , RuiZhang 6 , andYoshiharuIshikawa 3 on large - scale datasets , we propose a partition - based method to divide the database into disjoint subsets and learn a local model on each of them . Since update may exists in the database , we employ incremental learning to cope with this issue . We perform experiments on six real datasets . The results show that our method outperforms various state - of - the - art models . Com - pared to the best existing model [ 50 ] , the improvement of accuracy is up to 5 times in mean squared error and consistent across datasets , distance functions , and error metrics . The experiments also demon - strate that our method is competitive in estimation speed , robust against update in the database , and useful in estimating overall query processing time in a semantic search application . 2 RELATED WORK Traditional Estimation Models . Selectivity estimation has been ex - tensively studied in database systems , where prevalent approaches are based on sampling [ 53 , 55 ] , histograms [ 26 ] , or sketches [ 14 ] . However , few of them are applicable to high - dimensional data due to data sparsity or the curse of dimensionality . For cosine similarity , Wu et al . [ 54 ] proposed to use locality - sensitive hashing ( LSH ) as a means of importance sampling to tackle data sparsity . Kernel density estimation ( KDE ) [ 24 , 36 ] has been proposed to handle se - lectivity estimation in metric space . Mattig et al . [ 36 ] proposed to alleviat the curse of dimensionality by focusing on the distribution in metric space . However , strong assumptions are usually imposed on the kernel function ( e . g . , only diagonal covariance matrix for Gaussian kernels ) , and one kernel function may be inadequate to model complex distributions in high - dimensional data . Regression Models without Consistency Guarantee . Selectivity es - timation can be formalized as a regression problem with query object and threshold as input features , if the consistent require - ment is not enforced . A representative approach is quantized re - gression [ 7 , 8 ] . Recent trend uses deep regression models . Vanilla deep regression [ 32 , 46 , 47 ] learns good representations of input patterns . The mixture of expert model ( MoE ) [ 43 ] has a sparsely - gated mixture - of - experts layer that assigns data to proper experts ( models ) which lead to better generalization . The recursive model index ( RMI ) [ 31 ] is a regression model that can be used to replace the B - tree index in relational databases . Deep regression has also been used to predict selectivities ( cardinalities ) [ 29 , 45 ] in relational databases , amid a set of recent advances in learning methods for this task [ 23 , 38 , 39 , 48 , 56 ] . They target SQL queries where each predi - cate involves one attribute . [ 23 , 56 ] employ autoregressive models . [ 39 ] only deals with low dimensionality . [ 29 , 38 , 45 ] become a deep neural network if we regard a vector as an attribute . Models with Consistency Guarantee . Gradient boosting trees ( e . g . , XGBoost [ 13 ] and LightGBM [ 49 ] ) support monotonic regression . Lattice regression [ 17 , 19 , 21 , 57 ] uses a multi - linearly interpolated lookup table for regression . By enforcing constraints on its param - eter values , it can guarantee monotonicity . To accommodate high dimensional inputs , Fard et al . [ 17 ] proposed to build an ensem - ble of lattice using subsets of input features . Deep lattice network ( DLN ) [ 57 ] was proposed to interlace non - linear calibration layers and ensemble of lattice layers . Recently , lattice regression has also been used to learn a spatial index [ 33 ] . UMNN [ 51 ] is an autore - gressive flow model which adopts Clenshaw - Curtis quadrature to achieve monotonicity . Other monotonic models include isotonic regression [ 22 , 44 ] and MinMaxNet [ 15 ] . CardNet [ 50 ] is a recently proposed method for monotonic selectivity estimation of similarity selection query for various data types . It maps original data to bi - nary vectors and the threshold to an integer ùúè , and then predicts the selectivity for distance [ 0 , 1 , . . . , ùúè ] respectively with ( ùúè + 1 ) encoder - decoder models . When applying to high - dimensional data , its has the following drawbacks : the mapping from the input thresh - old to ùúè is not injective , i . e . , multiple thresholds may be mapped to the same ùúè and the same selectivity is always output for them ; the overall accuracy is significantly affected if one of the ( ùúè + 1 ) decoders is not accurate for some query . 3 PRELIMINARIES Problem 1 ( Selectivity Estimation for High - Dimensional Data ) . Given a database of ùëë - dimensional vectors D = { o ùëñ } ùëõ ùëñ = 1 , o ùëñ ‚àà R ùëë , adistancefunction ùëëùëñùë†ùë° ( ¬∑ , ¬∑ ) , ascalarthreshold ùë° , andaqueryobject x ‚àà R ùëë , estimate the selectivity in the database , i . e . , | { o | ùëëùëñùë†ùë° ( x , o ) ‚â§ ùë° , o ‚àà D } | . While we assume ùëë is a distance function , it is easy to extend it to consider ùëë as a similarity function by changing ‚â§ to ‚â• in the above definition . In the rest of the paper , to describe our method , we focus on the case when ùëë is a distance function . In addition , the query object does not have to be in the database , and we do not make any assumption on the distance function , meaning that the function does not have to be metric . We can view the selectivity ( i . e . , the ground truth label ) ùë¶ of a query object x and a threshold ùë° as generated by a function ùë¶ = ùëì ( x , ùë° , D ) . We call ùëì the value function . Our goal is to estimate ùëì ( x , ùë° , D ) using another function ÀÜ ùëì ( x , ùë° , D ) . One unique requirement of our problem is that the estimator ÀÜ ùëì needs to be consistent : ÀÜ ùëì is consistent if and only if it is non - decreasing in the threshold ùë° for every query object x ; i . e . , ‚àÄ x , ÀÜ ùëì ( x , ùë° ‚Ä≤ , D ) ‚â• ÀÜ ùëì ( x , ùë° , D ) iff . ùë° ‚Ä≤ ‚â• ùë° . 4 OBSERVATIONS AND IDEAS When | D | is large , it is hard to estimate ùëì directly . One of the main challenges is that ùëì may be non - smooth with respect to the input variables . In the worst case , we have : ‚Ä¢ For any vector Œî x , there exists a database D of ùëõ objects and a query ( x , ùë° ) such that ùëì ( x , ùë° , D ) = 0 and ùëì ( x + Œî x , ùë° , D ) = ùëõ . ‚Ä¢ For any ùúñ > 0 , there exists a database D of ùëõ objects and a query ( x , ùë° ) such that ùëì ( x , ùë° , D ) = 0 and ùëì ( x , ùë° + ùúñ , D ) = ùëõ . This means any model that directly approximates ùëì is hard . Our idea to mitigate this issue is : instead of estimating one func - tion ùëì , we estimate multiple functions such that each function‚Äôs output range is a small fraction of the selectivity ùë¶ . For example , suppose ùë¶ = ùë¶ 1 + ùë¶ 2 and ùë° = ùë° 1 + ùë° 2 . If ùë¶ 1 and ùë¶ 2 are approximately linear in [ 0 , ùë° 1 ] and ( ùë° 1 , ùë° 2 ] , respectively , but with different slopes , then we can use two linear models for the two threshold ranges . We may also exploit this idea and divide ùë¶ with disjoint subsets of D . Hence we adopt the following two partitioning schemes . ConsistentandFlexibleSelectivityEstimationforHigh - DimensionalData Threshold Partitioning . Assume the maximum threshold we sup - port is ùë° max . We consider dividing it with an increasing sequence of ( ùêø + 2 ) values : [ ùúè 0 , ùúè 1 , . . . , ùúè ùêø + 1 ] such that ùúè ùëñ < ùúè ùëó if ùëñ < ùëó , ùúè 0 = 0 , and ùúè ùêø + 1 = ùë° max + ùúñ , where ùúñ is a small positive quantity 1 . Let ùëî ùëñ ( x , ùë° ) be an interpolant function for interval [ ùúè ùëñ ‚àí 1 , ùúè ùëñ ) . Then we have ÀÜ ùëì ( x , ùë° , D ) = ùêø + 1 ‚àëÔ∏Å ùëñ = 1 1 (cid:74) ùë° ‚àà [ ùúè ùëñ ‚àí 1 , ùúè ùëñ ) (cid:75) ¬∑ ùëî ùëñ ( x , ùë° ) , ( 1 ) where 1 (cid:74)(cid:75) denotes the indicator function . Data Partitioning . We partition the database D into ùêæ disjoint parts D 1 , . . . , D ùêæ , and let ùëì ùëñ denote the value function defined on the ùëñ - th part . Then we have ÀÜ ùëì ( x , ùë° , D ) = (cid:205) ùêæùëñ = 1 ÀÜ ùëì ùëñ ( x , ùë° , D ùëñ ) . 5 SELECTIVITY ESTIMATOR 5 . 1 Threshold Partitioning Our idea is to approximate ùëì using a regression model ÀÜ ùëì ( x , ùë° , D ; Œò ) . Recall the sequence [ ùúè 0 , ùúè 1 , . . . , ùúè ùêø + 1 ] in Section 4 . We consider the family of continuous piecewise linear function to implement the interpolation ùëî ùëñ ( x , ùë° ) , ùëñ ‚àà [ 0 , ùêø + 1 ] . A piecewise linear function is a continuous function of ( ùêø + 1 ) pieces , each being a linear function defined on [ ùúè ùëñ ‚àí 1 , ùúè ùëñ ) . The ùúè ùëñ values are called control points . Given a query object x , let ùëù ùëñ denote the estimated selectivity for a threshold ùúè ùëñ . For the ùëî ùëñ function in Eq . ( 1 ) , we have ùëî ùëñ ( x , ùë° ) = ùëù ùëñ ‚àí 1 + ùë° ‚àí ùúè ùëñ ‚àí 1 ùúè ùëñ ‚àí ùúè ùëñ ‚àí 1 ¬∑ ( ùëù ùëñ ‚àí ùëù ùëñ ‚àí 1 ) . ( 2 ) Hence the regression model is parameterized by Œò def = { ( ùúè ùëñ , ùëù ùëñ ) } ùêø + 1 ùëñ = 0 . Note that ùúè ùëñ and ùëù ùëñ values are dependent on x ; i . e . , the piecewise linear function is query - dependent . Using the above design for Œò has the following property to guar - antee the consistency 2 . Lemma 1 . Given a database D and a query object x , if ùëù ùëñ ‚â• ùëù ùëñ ‚àí 1 for ‚àÄ ùëñ ‚àà [ 1 , ùêø + 1 ] , then ÀÜ ùëì ( x , ùë° , D ; Œò ) is non - decreasing in ùë° . Another salient property of our model is that it is flexible in the sense that it can arbitrarily well approximate the selectivity curve . Piecewise linear functions have been well explored to fit one - dimensional curves [ 40 ] . With a sufficient number of control points , one can find an optimal piecewise linear function to fit any one - dimensional curve . The idea is that a small range of input is highly likely to be linear with the output . When x and D are fixed , the selectivity only depends on ùë° , and thus the value function can be treated as a one - dimensional curve . To distinguish different x , we will design a deep learning approach to learn good control points and corresponding selectivities . As such , our model not only inherits the good performance of piecewise linear function but also handles different query objects . Estimation Loss . In the regression model , the ùêø ùúè ùëñ values and the ( ùêø + 2 ) ùëù ùëñ values are the parameters to be learned . We use the expected loss between ùëì and ÀÜ ùëì : ùêΩ est ( ÀÜ ùëì ) = ‚àëÔ∏Å ( ( x , ùë° ) , ùë¶ ) ‚ààT train ‚Ñì ( ùëì ( x , ùë° , D ) , ÀÜ ùëì ( x , ùë° , D ) ) , ( 3 ) 1 ùúñ isusedtocoverthecornercaseof ùë° = ùë° max inEq . ( 1 ) . 2 ProofisprovidedinAppendixA . where T train denotes the set of training data , and ‚Ñì ( ùë¶ , ÀÜ ùë¶ ) is a loss function between the true selectivity ùë¶ and the estimated value ÀÜ ùë¶ of a query ( x , ùë° ) . We choose the Huber loss [ 25 ] applied to the logarithmic values of ùë¶ and ÀÜ ùë¶ . To prevent numeric errors , we also pad the input by a small positive quantity ùúñ . Let ùëü def = ln ( ùë¶ + ùúñ ) ‚àí ln ( ÀÜ ùë¶ + ùúñ ) . Then ‚Ñì ( ùë¶ , ÀÜ ùë¶ ) = (cid:40) ùëü 2 2 , if | ùëü | ‚â§ ùõø ; ùõø ( | ùëü | ‚àí ùõø 2 ) , otherwise . ùõø is set to 1 . 345 , the standard recommended value [ 18 ] . The reason for designing such a loss function is that the selectivity may differ by several orders of magnitude for different queries . If we use the ‚Ñì 2 loss , it encourages the model to fit large selectivities well , and if we use ‚Ñì 1 loss , it pays more attention to small selectivities . To achieve robust prediction , we reduce the value range by logarithm and the Huber loss . 5 . 2 Learning Piecewise Linear Function We choose a deep neural network to learn the piecewise linear function . It has the following advantages : ( 1 ) Deep learning is able to capture the complex patterns in control points and corresponding selectivities for accurate estimation of different queries . ( 2 ) Deep learning generalizes well on queries that are not covered by training data . ( 3 ) The training data for our problem can be unlimitedly acquired by running a selection algorithm on the database , and this favors deep learning which often requires large training sets . In our model , ùúè ùëñ and ùëù ùëñ values are generated separately for the input query object . We also require non - negative increments be - tween consecutive parameters to ensure they are non - decreasing . In the following , we explain the learning of ùúè ùëñ s and ùëù ùëñ s , followed by the overall neural network architecture . Control Points ( ùúè ùëñ s ) . We learn the increments between ùúè ùëñ s . ùúè ùëñ ( x ) = ùëñ ‚àí 1 ‚àëÔ∏Å ùëó = 0 Œî ùúè ( x ) [ ùëó ] , ( 4 ) where Œî ùúè ( x ) = Norm ùëô 2 ( ùëî ( ùúè ) ( x ) ) ¬∑ ùë° ùëöùëéùë• . ( 5 ) Norm ùëô 2 is a normalized squared function defined as Norm ùëô 2 ( t ) = [ ùë° 21 + ùúñùêø t T t + ùúñ , . . . , ùë° 2 ùêø + ùúñ ùêø t T t + ùúñ ] , where ùúñ is a small positive quantity to avoid dividing by zero , and ùë° ùëñ denotes the value of the ùëñ - th dimension of t . The model takes x as input and outputs ùêø distinct thresholds in ( 0 , ùë° max ) . ùëî ( ùúè ) is implemented by a neural network . Then we have a vector ùúè = [ 0 ; ùúè 1 ; ùúè 2 ; . . . ; ùúè ùêø ; ùë° ùëöùëéùë• ] . One may consider using Softmax ( t ) , which is widely used for multi - classificationand ( self - ) attention . WechooseNorm ùëô 2 ( t ) rather than Softmax ( t ) for the following reasons : ( 1 ) Due to the exponen - tial function in Softmax ( t ) , a small change of t might lead to large variations of the output . ( 2 ) Softmax aims to highlight the impor - tant part rather than partitioning t , while our goal is to rationally partition the range [ 0 , ùúè ùëöùëéùë• ] into several intervals such that the piecewise linear function can fit well . YaoshuWang 1 , ChuanXiao 2 , 3 , JianbinQin 1 , RuiMao 1 , MakotoOnizuka 2 , WeiWang 4 , 5 , RuiZhang 6 , andYoshiharuIshikawa 3 x x x z AE FFN M ùúè p t ‚àë * R e L U N o r m l 2 y t max S M psum S Figure 1 : Network architecture . Selectivities at Control Points ( ùëù ùëñ s ) . We learn ( ùêø + 2 ) ùëù ùëñ values in a similar fashion to control points , using another neural network to implement ùëî ( ùëù ) . ùëù ùëñ ( x ) = ùëñ ‚àëÔ∏Å ùëó = 0 Œî ùëù ( x ) [ ùëó ] , ( 6 ) where Œî ùëù ( x ) = ReLU ( ùëî ( ùëù ) ( x ) ) . ( 7 ) Then we have a vector p = [ ùëù 0 ; ùëù 1 ; . . . ; ùëù ùêø + 1 ] . Here , we learn ( ùêø + 1 ) increments ( ùëù ùëñ ‚àí ùëù ùëñ ‚àí 1 ) instead of directly learning ( ùêø + 2 ) ùëù ùëñ s . Thereby , we do not have to enforce a constraint ùëù ùëñ ‚àí 1 ‚â§ ùëù ùëñ for ùëñ ‚àà [ 1 , ùêø + 1 ] in the learning process , and thus the learned model can better fit the selectivity curve . Network Architecture . Figure 1 shows our network architecture . The input x is first transformed to z , a latent representation ob - tained by an autoencoder ( AE ) . The use of the AE encourages the model to exploit latent data and query distributions in learning the piecewise linear function , and this helps the model generalize to query objects outside the training data . To learn the latent distri - butions of D , we pretrain the AE on all the objects of D , and then continue to train the AE with the queries in the training data . Due to the use of AE , the final loss function is a linear combination of the estimation loss ( Eq . ( 3 ) ) and the loss of the AE for the training data ( denoted by ùêΩ AE ) : ùêΩ ( ÀÜ ùëì ) = ùêΩ est ( ÀÜ ùëì ) + ùúÜ ¬∑ ùêΩ AE . ( 8 ) x is concatenated with z , i . e . , [ x ; z ] . Then [ x ; z ] is fed into two independent neural networks : a feed - forward network ( FFN ) and a model ùëÄ ( introduced later ) . Two multiplications , denoted by ùëÜ operators in Figure 1 , are needed to separately convert the output of FFN and the output of model ùëÄ to the ùúè and p vectors , respectively . They use a scalar ùë° max and a matrix M psum which , once multiplied on the right to a vector , perform prefix sum operation on the vector . M psum = Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£Ø Ô£∞ 1 0 . . . 0 1 1 . . . 0 . . . . . . . . . . . . 1 1 . . . 1 Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫ Ô£ª . The output of these networks , together with the threshold ùë° , are fed into the operator (cid:205) ‚àó in Figure 1 , which is implemented by Eqs . ( 2 ) , ( 5 ) , and ( 7 ) , to compute the output of the piecewise linear function , i . e . , the estimated selectivity . Figure 2 : Data partitioning by cover tree . Model ùëÄ . To achieve better performance , we learn p using an encoder - decoder model . In the encoder , an FFN is used to generate ( ùêø + 2 ) embeddings : [ h 0 ; h 1 ; . . . ; h ùêø + 1 ] = FFN ( [ x ; z ] ) , ( 9 ) where h ùëñ s are high - dimensional representations . Here , we adopt ( ùêø + 2 ) embeddings , i . e . , h 0 , . . . , h ùêø + 1 , to represent the latent infor - mation of p . In the decoder , we adopt ( ùêø + 2 ) linear transformations with the ReLU activation function : ùëò ùëñ = ReLU ( w T ùëñ h ùëñ + ùëè ùëñ ) . Then we have p = [ ùëò 0 , ùëò 0 + ùëò 1 , . . . , (cid:205) ùêø + 1 ùëñ = 0 ùëò ùëñ ] . 5 . 3 Data Partitioning To improve the accuracy of estimation on large - scale datasets , we divide the database into multiple disjoint subsets D 1 , . . . , D ùêæ with approximately the same size , and build a local model on each of them . Let ÀÜ ùëì ùëñ denote each local model . Then the global model for selectivity estimation is ÀÜ ùëì = (cid:205) ùëñ ÀÜ ùëì ùëñ . We have considered several design choices and propose the fol - lowing configuration that achieves the best empirical performance : ( 1 ) Partitioning is obtained by a cover tree - based strategy . ( 2 ) We adopt the structure in Figure 1 so that all local models share the same input representation [ x ; z ] , but each has its own neural net - works to learn the control points . Partitioning Method . We utilize a cover tree [ 27 ] to partition D into several parts . A partition ratio ùëü is predefined such that the cover tree will not expand its nodes if the number of inside objects is smaller than ùëü | D | . Given a query ( x , ùë° ) , the valid region is the circles that intersect the circle with x as center and ùë° as radius . For example , in Figure 2 , x ( the red point ) and ùë° form the red circle , and data are partitioned into 6 regions . The valid region of ( x , ùë° ) is the green circles that intersect the red circle . Albeit imposing constraints , cover tree might still generate too many ball regions , i . e . , leaf nodes , which lead to large number of parameters of the model and the difficulty of training . Reducing the number of ball regions is necessary . To remedy this , we adopt a merging strategy as follows . First , we still partition D into ùêæ ‚Ä≤ regions using cover tree . Then we cluster these regions into ùêæ ( ùêæ ‚Ä≤ ‚â§ ùêæ ) clusters D 1 , . . . , D ùêæ by the following greedy strategy : The ùêæ ‚Ä≤ regions are sorted in decreasing order of the number of inside objects . We begin with ùêæ empty clusters . Then we scan each region and assign it to the cluster with the smallest size . The regions that belong to the same cluster are merged to one region . We consider an indicator ConsistentandFlexibleSelectivityEstimationforHigh - DimensionalData ùëì ùëê : ( x , ùë° ) ‚Üí { 0 , 1 } ùêæ such that ùëì ùëê ( x , ùë° ) [ ùëñ ] = 1 if and only if the query ( x , ùë° ) intersects cluster D ùëñ , and employ it in our model : ÀÜ ùëì ( x , ùë° , D ) = ùêæ ‚àëÔ∏Å ùëñ = 0 ùëì ùëê ( x , ùë° ) [ ùëñ ] ¬∑ ÀÜ ùëì ùëñ ( x , ùë° , D ùëñ ) . Since cover trees deal with metric spaces , for non - metric functions ( e . g , cosine similarity ) , if possible , we equivalently convert it to a metric ( e . g , Euclidean distance , as cos ( u , v ) = 1 ‚àí ‚à• u , v ‚à• 2 2 for unit vectors u and v ) . Then the cover tree partitioning still works . For those that cannot be equivalently converted to a metric , we adopt random partitioning and modify ùëì ùëê as ùëì ùëê : ( x , ùë° ) ‚Üí { 1 } ùêæ . Training Procedure . We have several choices on how to train the models from multiple partitions . The default is directly training the global model ÀÜ ùëì , with the advantage that no extra work is needed . The other choice is to train each local model independently , using the selectivity computed on the local partition as training label . We propose yet another choice : we pretrain the local models for ùëá epochs , and then train them jointly . In the joint training stage , we use the following loss function : ùêΩ joint = ùêΩ est ( ÀÜ ùëì ) + ùõΩ ¬∑ ‚àëÔ∏Å ùëñ ùêΩ est ( ÀÜ ùëì ùëñ ) + ùúÜ ¬∑ ùêΩ AE . The indicators ùëì ùëê ( ¬∑ , ¬∑ ) s of all ( x , ùë° ) are precomputed before training . 5 . 4 Dealing with Data Updates When the database D is updated with insertion or deletion , we first check whether our model ÀÜ ùëì ( x , ùë° , D ) is necessary to update . In other words , when minor updates occur and ÀÜ ùëì ( x , ùë° , D ) is still accurate enough , we ignore them . To check the accuracy of ÀÜ ùëì ( x , ùë° , D ) , we update the labels of all validation data , and re - test the mean abso - lute error ( MAE ) of ÀÜ ùëì ( x , ùë° , D ) . If the difference between the original MAE and the new one is no larger than a predefined threshold ùõø ùëà , we do not update our model . Otherwise , we adopt an incremental learning approach as follows . First , we update the labels in the training and the validation data to reflect the update in the data - base . Second , we continue training our model with the updated training data until the validation error ( MAE ) does not increase in 3 consecutive epochs . Here the training does not start from scratch but from the current model . We incrementally train our model with all the training data to prevent catastrophic forgetting . 6 DISCUSSIONS 6 . 1 Model Complexity Analysis We assume an FFN has hidden layers a 1 , . . . , a ùëõ . The complexity of an FFN with input x and output y is | FFN ( x , y ) | = | x | ¬∑ | a 1 | + (cid:205) ùëõ ‚àí 1 ùëñ = 1 | a ùëñ | ¬∑ | a ùëñ + 1 | + | a ùëõ | ¬∑ | y | . Our model contains three components : AE , FFN , and ùëÄ . The com - plexity of AE is | FFN ( x , z ) | . The complexity of FFN is | FFN ( [ x ; z ] , t ) | , where t is the ùêø - dimensional vector after Norm ùëô 2 . Component ùëÄ consists of an FFN and ( ùêø + 2 ) linear transformations . Its complexity is | FFN ( [ x ; z ] , H ) | + ( ùêø + 2 ) ¬∑ | h ùëñ | + ( ùêø + 2 ) , where H = [ h 0 ; . . . ; h ùêø + 1 ] . Thus , the final model complexity is | FFN ( x , z ) | + | FFN ( [ x ; z ] , t ) | + | FFN ( [ x ; z ] , H ) | + ( ùêø + 2 ) ¬∑ | h ùëñ | + ( ùêø + 2 ) . ( a ) Simplified DLN ( b ) OurModel Figure 3 : Comparison of simplified DLN and our model . 6 . 2 Comparison with Other Models Lattice Regression . Lattice regression models [ 17 , 19 , 21 , 57 ] are the latest deep learning architectures for monotonic regression . We provide a comparison between ours and them applied to selectivity estimation . For the sake of an analytical comparison , we assume x and D are fixed so the selectivity only depends on ùë° , and consider a shallow version of DLN [ 57 ] with one layer of calibrator and one layer of a single lattice . With the above simplification , the DLN can be analytically rep - resented as : ÀÜ ùëì DLN ( ùë° ) = ‚Ñé ( ùëî ( ùë° ; w ) ; ùúÉ 0 , ùúÉ 1 ) , where ùëî : ùë° ‚àà [ 0 , ùë° max ] ‚Ü¶‚Üí ùëß ‚àà [ 0 , 1 ] and ‚Ñé ( ùëß ; ùúÉ 0 , ùúÉ 1 ) = ( 1 ‚àí ùëß ) ùúÉ 0 + ùëßùúÉ 1 . Hence it degenerates to fitting a linear interpolation in a latent space . There is little learning for the function ‚Ñé , as its two parameters ùúÉ 0 and ùúÉ 1 are determined by the minimum and maximum selectivity values in the training data . Thus , the workhorse of the model is to learn the non - linear mapping of ùëî . The calibrator also uses piecewise linear functions with ùêø control points equivalent to our ( ùúè ùëñ , ùëù ùëñ ) ùêøùëñ = 1 . However , ùúè ùëñ s are equally spaced between 0 and ùë° max , and only ùëù ùëñ s are learnable . This design is not flexible for many value functions ; e . g . , if the function values change rapidly within a small interval , the calibra - tor will not adaptively allocate more control points to this area . We show this with 8 control points for both models to learn the function ùë¶ = ùëì ( ùë° ) = 110 exp ( ùë° ) , ùë° ‚àà [ 0 , 10 ] . The training data are 80 ( ùë° ùëñ , ùëì ( ùë° ùëñ ) ) pairs where ùë° ùëñ s are uniformly sampled in [ 0 , 10 ] . We plot both models‚Äô estimation curves and their learned control points in Figure 3 . The ùëß values at the control points of DLN are shown on the right side of Figure 3 ( a ) . We observe : ( 1 ) The calibrator virtually determines the estimation as ‚Ñé ( ) degenerates to a simple scaling . ( 2 ) The calibrator‚Äôs control points are evenly spaced in ùë° , while our model learns to place more controls points in the ‚Äúinteresting area‚Äù , i . e . , where ùë¶ values change rapidly . ( 3 ) As a result , our model approximates the value function much better than DLN . Further , for DLN , the non - linear mapping on ùë° is independent of x ( even though we do not model x here ) . Even in the full - fledged DLN model , the calibration is performed on each input dimension independently . The full - fledged DLN model is too complex to ana - lyze , so we only study it in our empirical evaluation . Nonetheless , we believe that the above inherent limitations still remain . Our empirical evaluation will also show that query - dependent fitting of the value function is critical in our problem . Apart from DLN , recent studies also employ lattice regression and / or piecewise linear functions for learned index [ 30 , 33 ] . Like DLN , their control points are also query independent , albeit not equally spaced . YaoshuWang 1 , ChuanXiao 2 , 3 , JianbinQin 1 , RuiMao 1 , MakotoOnizuka 2 , WeiWang 4 , 5 , RuiZhang 6 , andYoshiharuIshikawa 3 Table 1 : Statistics of datasets . Dataset Source Domain # Objects Dimensionality Distance fastText [ 1 ] text 1M 300 Euclidean GloVe [ 2 ] text 1 . 9M 300 Euclidean MS - Celeb [ 20 ] image 2M 128 cosine YouTube [ 3 ] video 0 . 35M 1770 cosine DEEP [ 4 ] image 100M 96 cosine SIFT [ 5 ] image 200M 128 cosine Clenshaw - Curtis Quadrature . Clenshaw - Curtis quadrature [ 37 ] is able to approximate the integral ‚à´ ùúè ùëöùëéùë• 0 ÀÜ ùëî ( x , ùë° , D ) d ùë° , where ÀÜ ùëî = ùúï ÀÜ ùëì ( x , ùë° , D ) ùúïùë° in our problem . UMNN [ 51 ] is a recent work that adopts the idea to solve the autoregressive flow problem , and uses a neu - ral network to model ÀÜ ùëî . In [ 37 ] , the cosine transform of ÀÜ ùëî ( ùëêùëúùë†ùúÉ ) is adopted and the discrete finite cosine transform is sampled at equidistant points ùúÉ = ùúãùë†ùëÅ , where ùë† = 1 , . . . , ùëÅ , and ùëÅ is the number of sample points . Similar to DLN , it adopts the same integral approx - imation for different queries and ignores that integral points should depend on x . In contrast , our method addresses this issue by using a query - dependent model , thereby delivering more flexibility . Query - Driven Quantized Regression . The main idea of query - driven quantized regression [ 7 , 8 ] is to quantize the query space and find prototypes ( the closest one or multiple related ones ) for the given query object . Then the output space is quantized by proto - types , and localized regressions are used to estimate the selectivity for corresponding prototypes . Like our model , they also employ a query - dependent design . The differences from ours are : ( 1 ) [ 7 , 8 ] divide the query space of ( x , ùë° ) while we divide the range of thresh - old ùë° using x . ( 2 ) The number of prototypes is finite and often up to thousands in [ 7 , 8 ] , while our model chooses the selectivity curve for the query object via an FFN and model ùëÄ ( Figure 1 ) , which yield an unlimited number of curves in R ùêø + 2 . ( 3 ) We employ deep regres - sion for higher accuracy . ( 4 ) We directly partition the database D and train multiple deep models to deal with the subsets of D that may differ in data distribution , while the data subspace in [ 8 ] is defined by its query prototype . 7 EVALUATIONS 7 . 1 Experimental Settings Datasets . We use six datasets . The statistics is given in Table 1 . We preprocess MS - Celeb by faceNet [ 42 ] to obtain vectors . The other datasets have already been transformed to high - dimensional data . GloVe , YouTube , DEEP , and SIFT were also used in previous work [ 50 ] or nearest neighbor search benchmarks [ 10 , 34 ] . We randomly sample 0 . 25M vectors from each dataset D as query objects . The resulting query workload , denoted by Q , was uniformly split in 8 : 1 : 1 ( by query objects ) into training , validation , and test sets . So none of the test query objects has been seen by the model during training or validation . Note that labels ( i . e . , true selectivities ) are computed on D , not Q . For each training query object , we iterate through all the generated thresholds and add them to the training set . We randomly choose 3 generated thresholds for each validation or test query object . Due to the large number of training data , we randomly select training instances for each batch instead of continuously loading them , and the training procedure terminates when the mean squared error of the validation set does not increase in 5 consecutive epochs . For each setting , we tested on 5 sampled workloads to mitigate the effect of sampling error . Methods . We compare the following approaches 3 . ‚Ä¢ RS is a random sampling approach . For each query , we uniformly sample 0 . 1 % | D | objects for the first four datasets and 0 . 01 % | D | objectsfor DEEP and SIFT . Thenweuse scipy . spatial . distance . cdist to compute the distances to the query objects in a batch manner . ‚Ä¢ IS [ 54 ] isanimportancesampling approach usinglocality - sensitive hashing . It only works for cosine similarity due to the use of SimHash [ 12 ] . We enforce monotonicity by using deterministic sampling w . r . t . the query object . ‚Ä¢ KDE [ 36 ] is based on adaptive kernel density estimation for metric distance functions . To cope with cosine similarity , we nor - malize data to unit vectors and run KDE for Euclidean distance . ‚Ä¢ QR - 1 [ 7 ] and QR - 2 [ 8 ] are two query - driven quantized regres - sion models . We use the linear model in [ 7 ] for QR - 1 . ‚Ä¢ LightGBM [ 49 ] is based on gradient boosting decision trees ( CARTs ) . Each rule in a CART is in the form of ùë• ùëñ < ùëé ( ùë• ùëñ is the ùëñ - th dimension of x ) or ùë° < ùëè . ‚Ä¢ Deep regression models : DNN , a vanilla feed - forward network ; MoE [ 43 ] , a mixture of expert model with sparse activation ; RMI [ 31 ] , a hierarchical mixture of expert model ; and Card - Net [ 50 ] , a regression model based on incremental prediction ( we enable the accelerated estimation [ 50 ] ) . ‚Ä¢ Lattice regression models : We adopt DLN [ 57 ] in this category . ‚Ä¢ Clenshaw - Curtis quadrature model : We adopt UMNN [ 51 ] . ‚Ä¢ Our model is dubbed SelNet 4 . The default setting of ùêø ( number of control points ) is 50 and ùêæ ( partition size ) is 3 . The predefined threshold ùõø ùëà for incremental learning is 20 . We also evaluate two ablated models : ( 1 ) SelNet - ct is SelNet without the cover tree partitioning , and ( 2 ) SelNet - ad - ct is SelNet - ct without the query - dependent feature for control points ( disabled by feeding a constant vector into the FFN that generates the ùúè vector ) . Error Metrics . We evaluate Mean Squared Error ( MSE ) , Mean Ab - solute Percentage Error ( MAPE ) , and Mean Absolute Error ( MAE ) . Environment . Experiments were run on a server with an Intel Xeon E5 - 2640 @ 2 . 40GHz CPU and 256GB RAM , running Ubuntu 16 . 04 . 4 LTS . Models were implemented in Python and Tensorflow . 7 . 2 Accuracy We report accuracies in Table 2 , where monotonic models are marked with * , and best values are marked in boldface . Our model , SelNet , consistently outperforms existing models . It achieves sub - stantial error reduction against the best of state - of - the - art methods , in all the three error metrics and all the settings . Compare to the runner - up model on each dataset , the improvement is 2 . 0 ‚Äì 5 . 0 times in MSE , 1 . 3 ‚Äì 3 . 3 times in MAE , and 1 . 2 ‚Äì 1 . 7 times in MAPE , and is more significant on larger datasets . We examine each category of models . We start with the sampling - based methods . KDE works better than RS and IS in most settings . 3 PleaseseeAppendixBformodelsettings . 4 Thesourcecodeisavailableat [ 6 ] . ConsistentandFlexibleSelectivityEstimationforHigh - DimensionalData Table 2 : Accuracy ( MSE and MAE measured in 10 5 and 10 2 , respectively ) . Model fastText GloVe MS - Celeb YouTube DEEP SIFT MSE MAE MAPE MSE MAE MAPE MSE MAE MAPE MSE MAE MAPE MSE MAE MAPE MSE MAE MAPE RS * 22 . 38 7 . 45 1 . 40 34 . 85 8 . 71 1 . 09 28 . 64 8 . 09 1 . 30 2 . 95 1 . 89 0 . 88 84732 . 32 725 . 21 1 . 26 30317437 . 60 9496 . 55 0 . 98 IS * - - - - - - 104 . 58 14 . 25 1 . 25 2 . 85 1 . 83 0 . 76 50242 . 10 314 . 12 0 . 97 32049511 . 88 10612 . 42 0 . 92 KDE * 21 . 46 6 . 57 1 . 28 37 . 52 8 . 93 0 . 87 36 . 43 8 . 42 1 . 02 2 . 93 1 . 90 0 . 70 39497 . 24 298 . 10 0 . 85 27651109 . 31 9581 . 89 0 . 91 QR - 1 41 . 79 8 . 95 1 . 02 50 . 13 9 . 21 0 . 99 74 . 35 11 . 60 1 . 06 3 . 42 2 . 01 0 . 71 37054 . 11 292 . 16 0 . 78 28077423 . 18 9953 . 34 0 . 94 QR - 2 34 . 35 8 . 03 0 . 97 42 . 47 9 . 01 0 . 86 42 . 94 9 . 05 0 . 89 2 . 74 1 . 85 0 . 55 31485 . 85 273 . 22 0 . 67 22048511 . 27 8542 . 73 0 . 71 LightGBM 98 . 77 9 . 56 1 . 04 72 . 11 10 . 87 0 . 89 101 . 29 9 . 51 0 . 45 4 . 01 2 . 00 0 . 52 44036 . 45 301 . 88 0 . 85 23849121 . 36 9005 . 17 0 . 75 DNN 63 . 54 11 . 25 1 . 33 52 . 31 9 . 39 0 . 91 110 . 77 17 . 14 0 . 89 2 . 78 1 . 77 0 . 51 20454 . 11 192 . 13 0 . 69 24465910 . 26 7144 . 68 0 . 55 MoE 45 . 90 8 . 50 0 . 91 30 . 14 7 . 05 0 . 91 21 . 25 4 . 32 0 . 30 1 . 58 1 . 59 0 . 53 18068 . 93 170 . 51 0 . 65 14750194 . 30 6327 . 47 0 . 40 RMI 26 . 16 6 . 10 0 . 87 29 . 32 6 . 89 0 . 74 22 . 16 6 . 07 0 . 35 1 . 77 1 . 62 0 . 55 9498 . 21 116 . 54 0 . 67 8906108 . 00 4650 . 29 0 . 42 CardNet * 25 . 67 6 . 16 0 . 90 27 . 05 6 . 19 0 . 78 13 . 67 4 . 08 0 . 27 1 . 41 1 . 44 0 . 48 9230 . 48 117 . 37 0 . 67 7248693 . 54 4851 . 43 0 . 40 DLN * 77 . 50 11 . 56 1 . 53 52 . 26 10 . 27 0 . 89 82 . 35 11 . 85 0 . 97 2 . 94 1 . 92 0 . 69 58291 . 42 353 . 08 0 . 94 23059384 . 16 8058 . 49 0 . 51 UMNN * 33 . 26 7 . 20 0 . 92 33 . 50 7 . 98 0 . 86 16 . 75 4 . 70 0 . 36 2 . 06 1 . 69 0 . 49 10603 . 68 131 . 04 0 . 73 10201332 . 32 5443 . 31 0 . 43 SelNet * 7 . 87 3 . 56 0 . 76 9 . 17 3 . 83 0 . 68 4 . 96 2 . 43 0 . 23 0 . 72 1 . 13 0 . 36 2243 . 42 51 . 92 0 . 51 1464247 . 70 1406 . 63 0 . 23 Table 3 : Empirical monotonicity ( % ) on MS - Celeb . RS * IS * KDE * QR - 1 QR - 2 100 100 100 85 . 39 84 . 86 LightGBM DNN MoE RMI 86 . 34 78 . 22 94 . 82 90 . 48 CardNet * DLN * UMNN * SelNet * 100 100 100 100 In fact , KDE ‚Äôs performance even outperforms some deep learning regression based methods in a few cases ( e . g . , MSE on fastText ) . Among non - deep learning models , these is no best model across all the datasets , though QR - 2 prevails on more datasets than oth - ers . Among the deep learning models other than ours , CardNet is generally the best thanks to its incremental prediction for each threshold interval . The performance of DLN is mediocre . The main reason is analyzed in Section 6 . 2 . The accuracy of UMNN , which uses the same integral points for different queries , though better than DLN , still trails behind ours by a large margin . 7 . 3 Consistency Test We compute the empirical monotonicity measure [ 15 ] and show the results in Table 3 . The measure is the percentage of estimated pairs that violate the monotonicity , averaged over 200 queries . For each query , we sampled 100 thresholds , which form (cid:0) 100 2 (cid:1) pairs . A low score indicates more inconsistent estimates . As expected , models without consistency guarantee cannot produce 100 % monotonicity . 7 . 4 Ablation Study Table 4 shows that the partitioning ( SelNet v . s . SelNet - ct ) improves MSE , MAE , and MAPE by up to 3 . 4 , 2 . 1 , and 1 . 2 times , respectively , and the effect is more remarkable on large datasets . This is because each model deals with a subset of the dataset for better fit and the ground truth label values for each model are reduced , which makes it easier to fit our piecewise linear function with the same number of control points , as the value function is less steep . Using query - dependent control points ( SelNet - ct v . s . SelNet - ad - ct ) also has a significant impact on accuracy across all the settings and all the error metrics . The improvements in MSE , MAE , and MAPE are up to 3 . 1 , 2 . 0 , and 3 . 6 times , respectively . Table 4 : Ablation study . Dataset Model MSE ( √ó 10 5 ) MAE ( √ó 10 2 ) MAPE fastText SelNet 7 . 87 3 . 56 0 . 76 SelNet - ct 12 . 63 4 . 37 0 . 81 SelNet - ad - ct 39 . 59 8 . 72 2 . 90 GloVe SelNet 9 . 17 3 . 83 0 . 68 SelNet - ct 22 . 43 5 . 82 0 . 70 SelNet - ad - ct 32 . 59 6 . 92 0 . 90 MS - Celeb SelNet 4 . 96 2 . 43 0 . 23 SelNet - ct 5 . 31 2 . 92 0 . 24 SelNet - ad - ct 16 . 02 4 . 65 0 . 37 YouTube SelNet 0 . 72 1 . 13 0 . 36 SelNet - ct 0 . 90 1 . 20 0 . 39 SelNet - ad - ct 1 . 65 1 . 59 0 . 53 DEEP SelNet 2243 . 42 51 . 92 0 . 51 SelNet - ct 5861 . 43 72 . 18 0 . 58 SelNet - ad - ct 9012 . 57 101 . 42 0 . 71 SIFT SelNet 1464247 . 70 1406 . 63 0 . 23 SelNet - ct 4958113 . 22 2911 . 86 0 . 27 SelNet - ad - ct 6904808 . 25 3855 . 53 0 . 54 7 . 5 Estimation Time Table 5 reports the estimation times of the competitors . We also report the time of running a state - of - the - art selection algorithm ( CoverTree [ 27 ] ) toobtain the exactselectivity . All the models except IS are at least one order of magnitude faster than CoverTree , and the gaps increase to three orders of magnitude on DEEP and SIFT . Our model is on a par with other deep learning models ( except DNN ) and faster than sampling and quantized regression methods . 7 . 6 Training Table 6 shows the training times . Non - deep models are faster to train . Our models spend 5 ‚Äì 6 hours , similar to other deep models . In Figure 4 , we show the performances , measured by MSE , of the deep learning models by varying the scale of training examples from 20 % to 100 % of the original training data . All the models perform worse with fewer training data , but our models are more robust , showing moderate accuracy loss . 7 . 7 Data Update We generate a stream of 100 update operations , each with an inser - tion or deletion of 5 records on fastText and MS - Celeb , to evaluate YaoshuWang 1 , ChuanXiao 2 , 3 , JianbinQin 1 , RuiMao 1 , MakotoOnizuka 2 , WeiWang 4 , 5 , RuiZhang 6 , andYoshiharuIshikawa 3 Table 5 : Average estimation time ( milliseconds ) . Model fastText GloVe MS - Celeb YouTube DEEP SIFT CoverTree 8 . 14 8 . 85 9 . 65 6 . 11 214 395 RS * 0 . 46 0 . 51 0 . 49 0 . 52 2 . 54 4 . 72 IS * - - 1 . 08 2 . 35 4 . 97 6 . 81 KDE * 0 . 79 0 . 68 0 . 59 0 . 94 1 . 48 2 . 05 QR - 1 0 . 86 0 . 98 0 . 97 1 . 03 2 . 21 2 . 82 QR - 2 0 . 79 0 . 99 0 . 95 1 . 10 2 . 32 2 . 91 LightGBM 0 . 28 0 . 30 0 . 18 0 . 52 0 . 26 0 . 26 DNN 0 . 07 0 . 10 0 . 03 0 . 16 0 . 11 0 . 10 MoE 0 . 36 0 . 33 0 . 27 0 . 49 0 . 29 0 . 33 RMI 0 . 34 0 . 38 0 . 25 0 . 47 0 . 27 0 . 30 CardNet * 0 . 19 0 . 26 0 . 14 0 . 31 0 . 22 0 . 28 DLN * 0 . 83 0 . 69 0 . 65 1 . 22 0 . 64 0 . 80 UMNN * 0 . 39 0 . 32 0 . 24 0 . 52 0 . 26 0 . 32 SelNet * 0 . 35 0 . 31 0 . 24 0 . 51 0 . 29 0 . 36 Table 6 : Training time ( hours ) . Model fastText GloVe MS - Celeb YouTube DEEP SIFT KDE * 1 . 1 1 . 5 0 . 7 0 . 8 2 . 5 3 . 6 QR - 1 1 . 8 2 . 1 1 . 5 1 . 2 3 . 9 4 . 6 QR - 2 1 . 5 2 . 0 1 . 6 1 . 2 3 . 6 4 . 7 LightGBM 2 . 1 1 . 9 2 . 2 2 . 1 1 . 9 2 . 1 DNN 2 . 9 2 . 1 2 . 8 2 . 9 2 . 5 2 . 9 MoE 4 . 9 5 . 4 4 . 9 4 . 7 4 . 3 4 . 4 RMI 5 . 4 5 . 6 4 . 8 5 . 3 4 . 6 4 . 8 CardNet * 3 . 8 3 . 9 3 . 3 3 . 2 3 . 5 3 . 8 DLN * 6 . 9 7 . 1 6 . 0 6 . 5 6 . 3 6 . 4 UMNN * 5 . 5 5 . 7 4 . 9 5 . 2 5 . 4 4 . 6 SelNet * 6 . 0 5 . 5 5 . 2 5 . 6 5 . 2 5 . 0 Table 7 : Varying number of control points on fastText . Error Metric Number of Control Points 10 50 90 130 MSE ( √ó 10 5 ) 13 . 06 7 . 87 7 . 93 10 . 47 MAE ( √ó 10 2 ) 4 . 85 3 . 56 3 . 56 3 . 92 MAPE 0 . 87 0 . 76 0 . 76 0 . 79 our incremental learning technique . Figure 5 plots how MSE and MAPE change with the stream . The general trend is that the MSE is decreasing when there are more updates , while MAPE fluctuates or keeps almost the same . Such difference is caused by the change of labels ( i . e . , true selectivities ) in the stream . Nonetheless , the result indicates that incremental learning is able to keep up with the up - dated data . Besides , SelNet only spends 1 . 5 ‚Äì 2 . 0 minutes for each incremental learning , showcasing its speed to cope with updates . 7 . 8 Evaluation of Hyper - Parameters Table 7 shows the accuracy when we vary the number of control points ùêø on fastText . A small value leads to underfitting towards the curve of thresholds , while a large value increases the learning difficulty . ùêø = 50 achieves the best performance . Table 8 reports the accuracy when we vary the partition size ùêæ on fastText . There is no partitioning when ùêæ = 1 . We observe that the partitioning is useful , but the improvement is small when partition size exceeds 3 , and estimation time also substantially in - creases . This means a small partition size ( ùêæ = 3 ) suffices to achieve 20 40 60 80 100 TrainingSize 10 1 10 2 M SE ( 10 5 ) SelNetCardNet RMIMoE DLNUMNN ( a ) MSE , fastText 20 40 60 80 100 TrainingSize 20 40 60 80 M SE ( 10 5 ) SelNetCardNet RMIMoE DLNUMNN ( b ) MSE , GloVe 20 40 60 80 100 TrainingSize 10 1 10 2 M SE ( 10 5 ) SelNetCardNet RMIMoE DLNUMNN ( c ) MSE , MS - Celeb 20 40 60 80 100 TrainingSize 10 20 30 40 50 60 M SE ( 10 4 ) SelNetCardNet RMIMoE DLNUMNN ( d ) MSE , YouTube 20 40 60 80 100 TrainingSize 0 20 40 60 80 M SE ( 10 8 ) SelNetCardNet RMIMoE DLNUMNN ( e ) MSE , DEEP 20 40 60 80 100 TrainingSize 0 10 20 30 40 50 M SE ( 10 11 ) SelNetCardNet RMIMoE DLNUMNN ( f ) MSE , SIFT Figure 4 : Varying training data size . 0 20 40 60 80 100 Sequence of Operations 4 5 6 7 8 M S E ( 10 5 ) fastText MS - Celeb ( a ) MSE 0 20 40 60 80 100 Sequence of Operations 20 40 60 80 100 M A P E ( % ) fastText MS - Celeb ( b ) MAPE Figure 5 : Data update . good performance . For partitioning strategy , we compare cover tree partitioning ( CT ) with random partitioning ( RP ) and ùëò - means partitioning ( KM ) in Table 9 . CT delivers the best performance . KM is the worst because it tends to cause imbalance in the partition . 7 . 9 Generalizability To show the generalizability of our model , we evaluate the per - formance on the queries that significantly differ from the records in the training data . To prepare such queries , we first perform a ConsistentandFlexibleSelectivityEstimationforHigh - DimensionalData Table 8 : Varying partition size on fastText . Error Metric Partition Size 1 3 6 9 MSE ( √ó 10 5 ) 12 . 63 7 . 87 6 . 82 6 . 75 MAE ( √ó 10 2 ) 4 . 37 3 . 56 3 . 36 3 . 11 MAPE 0 . 81 0 . 76 0 . 77 0 . 74 Estimation Time ( ms ) 0 . 16 0 . 35 0 . 79 1 . 24 Table 9 : Varying partitioning method on fastText . Error Metric CT ( 3 ) RP ( 3 ) KM ( 3 ) MSE ( √ó 10 5 ) 7 . 87 8 . 02 9 . 14 MAE ( √ó 10 2 ) 3 . 56 3 . 57 3 . 64 MAPE 0 . 76 0 . 78 0 . 79 [ 0 , 1 ) [ 1 , 10 ) [ 10 , 100 ) ‚â• 100 SelectivityRange ( 10 3 ) 10 5 10 6 10 7 10 8 10 9 10 10 M SE SelNet CardNet RMI KDE ( a ) DEEP [ 0 , 1 ) [ 1 , 10 ) [ 10 , 100 ) ‚â• 100 SelectivityRange ( 10 3 ) 10 6 10 8 10 10 M SE SelNet CardNet RMI KDE ( b ) SIFT Figure 6 : Generalizability . T r ue T i m e E x a c t S e l S e l N e t C a r d N e t R M I K D E R S 0 10000 20000 30000 40000 E s t i m a t ed T i m e ( s ) ( a ) AMiner - Paper T r ue T i m e E x a c t S e l S e l N e t C a r d N e t R M I K D E R S 0 500 1000 1500 2000 2500 E s t i m a t ed T i m e ( s ) ( b ) Quora Figure 7 : Estimated search time ( 10 , 000 queries ) . ùëò - means clustering on D . We randomly sample 10 , 000 query ob - jects from D ( excluding the queries used for training ) and add Gaussian noise [ 58 ] . Then we pick the top - 2 , 000 ones having the largest sum of squared distance to the ùëò centroids . Figure 6 show the performances of KDE , RMI , CardNet , and SelNet on DEEP and SIFT , measured by MSE . The queries are grouped by selectivity range . In each selectivity group , SelNet consistently outperforms the other models , and the advantage is around one order of magni - tude . This result demonstrates that our model generalizes well for out - of - dataset queries . 7 . 10 Performance in Semantic Search To evaluate the usefulness of SelNet , we consider estimating the overall processing time for a query workload of semantic search : given a query text entry , we want to find matching records in the database . Estimating the query processing time may help to create a service level agreement . We use two datasets , AMiner - Paper publications ( 2 . 1M records ) and Quora questions ( 0 . 8M records ) . AMiner - Paper has four attributes : title , authors , venue , and year . We follow [ 35 ] and concatenate attribute names and values as one string . Quora has one attribute . Then we embed each record to a 768 - dimensional vector by Sentence - BERT [ 41 ] . 10 , 000 records are sampled from each dataset as queries . To pro - cess a query , we first embed it by Sentence - BERT [ 41 ] , and then use Faiss [ 28 ] to find candidate records whose cosine similarity to the query embedding is no less than 0 . 9 . The candidates are ver - ified using DITTO [ 35 ] . Hence the overall query processing time can be estimated as : avg _ Faiss _ time √ó 10000 + avg _ DITTO _ time √ó Faiss _ recall √ó (cid:205) 100001 estimated _ selectivity _ of _ query _ ùëñ . The aver - age times and Faiss recall are obtained by running a small query workload . For selectivity , we consider RS , KDE , RMI , CardNet , SelNet , and an oracle that outputs the exact selectivity ( ExactSel ) . We plot the estimated time of processing 10 , 000 queries in Fig - ure 7 , where TrueTime indicates the ground truth . The models tend to underestimate on AMiner - Paper and overestimate on Quora . SelNet ‚Äôs high accuracy in selectivity estimation pays off . Compared to the ground truth , SelNet ‚Äôs error is 13 % on AMiner - Paper and 16 % on Quora , close to ExactSel ‚Äôs and much lower than the other models‚Äô ( at least 39 % ) . SelNet is also efficient ; e . g . , running the workload to obtain the ground truth on AMiner - Paper spends 13 hours , which is twice the time of preparing training data + training SelNet + estimating for 10 , 000 queries . Seeing SelNet ‚Äôs scalability in estimation time ( Table 5 ) and training time ( Table 6 ) , we believe that the advantage will be more substantial on larger datasets . 8 CONCLUSION We tackled the selectivity estimation problem for high - dimensional data . Our method is based on learning monotonic query - dependent piece - wise linear function . This provides the flexibility of our model to approximate the selectivity curve while guaranteeing the con - sistency of estimation . We proposed a partitioning technique to cope with large - scale datasets and an incremental learning tech - nique for updates . Our experiments showed the superiority of the proposed model in accuracy across a variety of datasets , distance functions , and error metrics . The experiments also demonstrated the usefulness of our model in a semantic search application . Acknowledgements This work was supported by NSFC 62072311 and U2001212 , Guangdong Basic and Applied Basic Research Foun - dation 2019A1515111047 and 2020B1515120028 , Guangdong Peral River Recruitment Program of Talents 2019ZT08X603 , JSPS Kak - enhi 16H01722 , 17H06099 , 18H04093 , and 19K11979 , and ARC DPs 170103710 and 180103411 . REFERENCES [ 1 ] https : / / fasttext . cc / docs / en / english - vectors . html . [ 2 ] https : / / nlp . stanford . edu / projects / glove / . [ 3 ] http : / / www . cs . tau . ac . il / ~ wolf / ytfaces / index . html . [ 4 ] http : / / http : / / sites . skoltech . ru / compvision / noimi / . [ 5 ] http : / / http : / / corpus - texmex . irisa . fr / . [ 6 ] https : / / github . com / yyssl88 / SelNet - Estimation . [ 7 ] C . Anagnostopoulos and P . Triantafillou . Learning set cardinality in distance nearestneighbours . In ICDM , pages691 ‚Äì 696 , 2015 . [ 8 ] C . AnagnostopoulosandP . Triantafillou . Query - drivenlearningforpredictive analyticsofdatasubspacecardinality . ACMTrans . Knowl . Discov . Data , 11 ( 4 ) : 47 : 1 ‚Äì 47 : 46 , 2017 . [ 9 ] S . Arya , T . Malamatos , andD . M . Mount . Space - timetradeoffsforapproximate sphericalrangecounting . In SODA , pages535 ‚Äì 544 , 2005 . [ 10 ] M . Aum√ºller , E . Bernhardsson , and A . J . Faithfull . Ann - benchmarks : A bench - markingtoolforapproximatenearestneighboralgorithms . Inf . Syst . , 87 , 2020 . YaoshuWang 1 , ChuanXiao 2 , 3 , JianbinQin 1 , RuiMao 1 , MakotoOnizuka 2 , WeiWang 4 , 5 , RuiZhang 6 , andYoshiharuIshikawa 3 [ 11 ] M . M . Breunig , H . Kriegel , R . T . Ng , andJ . Sander . LOF : identifyingdensity - based localoutliers . In SIGMOD , pages93 ‚Äì 104 , 2000 . [ 12 ] M . S . Charikar . Similarityestimationtechniquesfromroundingalgorithms . In STOC , pages380 ‚Äì 388 , 2002 . [ 13 ] T . ChenandC . Guestrin . Xgboost : Ascalabletreeboostingsystem . In KDD , pages 785 ‚Äì 794 , 2016 . [ 14 ] G . Cormode , M . N . Garofalakis , P . J . Haas , andC . Jermaine . Synopsesformassive data : Samples , histograms , wavelets , sketches . FoundationsandTrendsinDatabases , 4 ( 1 - 3 ) : 1 ‚Äì 294 , 2012 . [ 15 ] H . DanielsandM . Velikova . Monotoneandpartiallymonotoneneuralnetworks . IEEETransactionsonNeuralNetworks , 21 ( 6 ) : 906 ‚Äì 917 , 2010 . [ 16 ] S . Das , P . S . G . C . , A . Doan , J . F . Naughton , G . Krishnan , R . Deep , E . Arcaute , V . Raghavendra , andY . Park . Falcon : Scalinguphands - offcrowdsourcedentity matchingtobuildcloudservices . In SIGMOD , pages1431 ‚Äì 1446 , 2017 . [ 17 ] M . M . Fard , K . Canini , A . Cotter , J . Pfeifer , andM . Gupta . Fastandflexiblemonotonic functionswithensemblesoflattices . In NIPS , pages2919 ‚Äì 2927 , 2016 . [ 18 ] J . Fox . Robust regression : Appendix to an r and s - plus companion to applied regression , 2002 . [ 19 ] E . GarciaandM . Gupta . Latticeregression . In NIPS , pages594 ‚Äì 602 , 2009 . [ 20 ] Y . Guo , L . Zhang , Y . Hu , X . He , andJ . Gao . MS - Celeb - 1M : Adatasetandbenchmark forlargescalefacerecognition . In ECCV , 2016 . [ 21 ] M . Gupta , A . Cotter , J . Pfeifer , K . Voevodski , K . Canini , A . Mangylov , W . Moczyd - lowski , andA . VanEsbroeck . Monotoniccalibratedinterpolatedlook - uptables . TheJournalofMachineLearningResearch , 17 ( 1 ) : 3790 ‚Äì 3836 , 2016 . [ 22 ] Q . Han , T . Wang , S . Chatterjee , andR . J . Samworth . Isotonicregressioningeneral dimensions . arXivpreprintarXiv : 1708 . 09468 , 2017 . [ 23 ] S . Hasan , S . Thirumuruganathan , J . Augustine , N . Koudas , and G . Das . Deep learningmodelsforselectivityestimationofmulti - attributequeries . In SIGMOD , pages1035 ‚Äì 1050 , 2020 . [ 24 ] M . Heimel , M . Kiefer , andV . Markl . Self - tuning , GPU - acceleratedkerneldensity modelsformultidimensionalselectivityestimation . In SIGMOD , pages1477 ‚Äì 1492 , 2015 . [ 25 ] P . J . Huberetal . Robustestimationofalocationparameter . Theannalsofmathe - maticalstatistics , 35 ( 1 ) : 73 ‚Äì 101 , 1964 . [ 26 ] Y . Ioannidis . Thehistoryofhistograms ( abridged ) . In VLDB , pages19 ‚Äì 30 , 2003 . [ 27 ] M . IzbickiandC . R . Shelton . Fastercovertrees . In ICML , pages1162 ‚Äì 1170 , 2015 . [ 28 ] H . J√©gou , tthijsDouze , andJ . Johnson . Facebookaisimilaritysearch ( faiss ) . https : / / github . com / facebookresearch / faiss . [ 29 ] A . Kipf , T . Kipf , B . Radke , V . Leis , P . A . Boncz , andA . Kemper . Learnedcardinalities : Estimatingcorrelatedjoinswithdeeplearning . In CIDR , 2019 . [ 30 ] A . Kipf , R . Marcus , A . vanRenen , M . Stoian , A . Kemper , T . Kraska , andT . Neumann . Radixspline : asingle - passlearnedindex . In aiDM @ SIGMOD , pages5 : 1 ‚Äì 5 : 5 , 2020 . [ 31 ] T . Kraska , A . Beutel , E . H . Chi , J . Dean , andN . Polyzotis . Thecaseforlearnedindex structures . In SIGMOD , pages489 ‚Äì 504 , 2018 . [ 32 ] S . Lathuili√®re , P . Mesejo , X . Alameda - Pineda , andR . Horaud . Acomprehensive analysisofdeepregression . arXivpreprintarXiv : 1803 . 08450 , 2018 . [ 33 ] P . Li , H . Lu , Q . Zheng , L . Yang , and G . Pan . LISA : A learned index structure for spatialdata . In SIGMOD , pages2119 ‚Äì 2133 , 2020 . [ 34 ] W . Li , Y . Zhang , Y . Sun , W . Wang , M . Li , W . Zhang , and X . Lin . Approximate nearestneighborsearchonhighdimensionaldata - experiments , analyses , and improvement . IEEETrans . Knowl . DataEng . , 32 ( 8 ) : 1475 ‚Äì 1488 , 2020 . [ 35 ] Y . Li , J . Li , Y . Suhara , A . Doan , andW . - C . Tan . Deepentitymatchingwithpre - trained languagemodels . PVLDB , 14 ( 1 ) : 50 ‚Äì 60 , 2020 . [ 36 ] M . Mattig , T . Fober , C . Beilschmidt , andB . Seeger . Kernel - basedcardinalityesti - mationonmetricdata . In EDBT , pages349 ‚Äì 360 , 2018 . [ 37 ] M . Novelinkova . Comparisonofclenshaw - curtisandgaussquadrature . In WDS , volume11 , pages67 ‚Äì 71 , 2011 . [ 38 ] J . Ortiz , M . Balazinska , J . Gehrke , andS . S . Keerthi . Anempiricalanalysisofdeep learningforcardinalityestimation . CoRR , abs / 1905 . 06425 , 2019 . [ 39 ] Y . Park , S . Zhong , and B . Mozafari . Quicksel : Quick selectivity learning with mixturemodels . In SIGMOD , pages1017 ‚Äì 1033 , 2020 . [ 40 ] L . Prunty . Curvefittingwithsmoothfunctionsthatarepiecewise - linearinthe limit . Biometrics , pages857 ‚Äì 866 , 1983 . [ 41 ] N . ReimersandI . Gurevych . Sentence - BERT : Sentenceembeddingsusingsiamese BERT - networks . In EMNLP - IJCNLP , pages3980 ‚Äì 3990 , 2019 . [ 42 ] F . Schroff , D . Kalenichenko , andJ . Philbin . Facenet : Aunifiedembeddingforface recognitionandclustering . In CVPR , pages815 ‚Äì 823 , 2015 . [ 43 ] N . Shazeer , A . Mirhoseini , K . Maziarz , A . Davis , Q . Le , G . Hinton , and J . Dean . Outrageouslylargeneuralnetworks : Thesparsely - gatedmixture - of - expertslayer . arXivpreprintarXiv : 1701 . 06538 , 2017 . [ 44 ] J . Spouge , H . Wan , and W . Wilbur . Least squares isotonic regression in two dimensions . Journal of Optimization Theory and Applications , 117 ( 3 ) : 585 ‚Äì 605 , 2003 . [ 45 ] J . SunandG . Li . Anend - to - endlearning - basedcostestimator . PVLDB , 13 ( 3 ) : 307 ‚Äì 319 , 2019 . [ 46 ] Y . Sun , X . Wang , andX . Tang . Deepconvolutionalnetworkcascadeforfacialpoint detection . In CVPR , pages3476 ‚Äì 3483 , 2013 . [ 47 ] A . Toshev and C . Szegedy . Deeppose : Human pose estimation via deep neural networks . In CVPR , pages1653 ‚Äì 1660 , 2014 . [ 48 ] B . Walenz , S . Sintos , S . Roy , and J . Yang . Learning to sample : Counting with complexqueries . PVLDB , 13 ( 3 ) : 390 ‚Äì 402 , 2019 . [ 49 ] D . Wang , Y . Zhang , and Y . Zhao . Lightgbm : An effective mirna classification methodinbreastcancerpatients . In ICCBB , pages7 ‚Äì 11 , 2017 . [ 50 ] Y . Wang , C . Xiao , J . Qin , X . Cao , Y . Sun , W . Wang , andM . Onizuka . Monotoniccar - dinalityestimationofsimilarityselection : Adeeplearningapproach . In SIGMOD , pages1197 ‚Äì 1212 , 2020 . [ 51 ] A . Wehenkel and G . Louppe . Unconstrained monotonic neural networks . In NeurIPS , pages1543 ‚Äì 1553 , 2019 . [ 52 ] K . - Y . Whang , S . - W . Kim , and G . Wiederhold . Dynamic maintenance of data distributionforselectivityestimation . VLDBJ . , 3 ( 1 ) : 29 ‚Äì 51 , 1994 . [ 53 ] W . Wu , J . F . Naughton , andH . Singh . Sampling - basedqueryre - optimization . In SIGMOD , pages1721 ‚Äì 1736 , 2016 . [ 54 ] X . Wu , M . Charikar , andV . Natchu . Localdensityestimationinhighdimensions . In ICML , pages5293 ‚Äì 5301 , 2018 . [ 55 ] Y . Wu , D . Agrawal , andA . ElAbbadi . Queryestimationbyadaptivesampling . In ICDE , pages639 ‚Äì 648 , 2002 . [ 56 ] Z . Yang , E . Liang , A . Kamsetty , C . Wu , Y . Duan , P . Chen , P . Abbeel , J . M . Hellerstein , S . Krishnan , and I . Stoica . Deep unsupervised cardinality estimation . PVLDB , 13 ( 3 ) : 279 ‚Äì 292 , 2019 . [ 57 ] S . You , D . Ding , K . Canini , J . Pfeifer , and M . Gupta . Deep lattice networks and partialmonotonicfunctions . In NIPS , pages2981 ‚Äì 2989 , 2017 . [ 58 ] D . ZhangandZ . Yang . Wordembeddingperturbationforsentenceclassification . arXivpreprintarXiv : 1804 . 08166 , 2018 . ConsistentandFlexibleSelectivityEstimationforHigh - DimensionalData APPENDIX A PROOF Lemma 1 . Proof . Assume ùë° ‚àà [ ùúè ùëñ ‚àí 1 , ùúè ùëñ ) , then ùë° + ùúñ is in [ ùúè ùëñ ‚àí 1 , ùúè ùëñ ) or [ ùúè ùëñ , ùúè ùëñ + 1 ) . In the first case , ÀÜ ùëì ( x , ùë° + ùúñ , D ; Œò ) ‚àí ÀÜ ùëì ( x , ùë° , D ; Œò ) = ùúñ ùúè ùëñ ‚àí ùúè ùëñ ‚àí 1 ¬∑ ( ùëù ùëñ ‚àí ùëù ùëñ ‚àí 1 ) ‚â• 0 . In the second case , ÀÜ ùëì ( x , ùë° , D ; Œò ) ‚â§ ùëù ùëñ and ÀÜ ùëì ( x , ùë° + ùúñ , D ; Œò ) ‚â• ùëù ùëñ . Therefore , ÀÜ ùëì ( x , ùë° , D ; Œò ) is non - decreasing in ùë° . ‚ñ° B EXPERIMENT SETUP B . 1 Model Settings Hyperparameter and training settings are given below . ‚Ä¢ IS and KDE : The sample size is 2000 . ‚Ä¢ QR - 1 and QR - 2 : The number of query prototypes is 2000 . ‚Ä¢ LightGBM : The number of CARTs is 1000 . ‚Ä¢ DNN is a vanilla FFN with four hidden layers of sizes 512 , 512 , 512 , and 256 . ‚Ä¢ MoE consists of 30 expert models , each an FFN with three hidden layers of sizes 512 , 512 , and 512 . We used top - 3 experts for the prediction . ‚Ä¢ RMI has three levels , with 1 , 4 , and 8 models , respectively . Each model is an FFN with four hidden layers with sizes 512 , 512 , 512 , and 256 . ‚Ä¢ DLN is an architecture of six layers : calibrators , linear embedding , calibrators , ensemble of lattices , calibrators , and linear embed - ding . ‚Ä¢ UMNN is an FFN with four hidden layers of sizes 512 , 512 , 512 and 256 to implement the derivative . ùúïùëì ( x , ùë° , D ) ùúïùë° . ùëì ( x , ùë° , D ) is com - puted by Clenshaw - Curtis quadrature with learned derivatives . ‚Ä¢ SelNet : We use an FFN with two hidden layers to estimate ùúè , and an FFN in Equation 9 with four hidden layers to estimate p . The encoder and decoder of AE are implemented with an FFN with three hidden layers . For MS - Celeb and YouTube , the sizes of the first three ( or two , if it only has two ) hidden layers of the three FFNs are 512 , and the sizes of all the other hidden layers are 256 . For fastText , GloVe , DEEP , and SIFT , the sizes of the first hidden layer of the these FFNs are 1024 , and the others remain the same as above . The number of control parameters ùêø is 50 . The default partition size ùêæ is 3 . ùë° max is 54 for Euclidean distance . For cosine similarity , we equivalently convert it to Euclidean distance on unit vectors , and set ùë° max = 1 . The learning rates of MS - Celeb , fastText , YouTube , GloVe , DEEP , and SIFT are 0 . 00003 , 0 . 00002 , 0 . 00003 , 0 . 0001 , 0 . 0001 , and 0 . 0001 , respectively . | h ùëñ | ( 0 ‚â§ ùëñ ‚â§ ùêø + 1 ) in model ùëÄ is 100 . The batch size is 512 for all the datasets . We train all the models in 1500 epochs and select the ones with the smallest validation error . For training with data partitioning , we use ùëá = 300 and ùõΩ = 0 . 1 . ùõø ùëà for incremental learning is 20 . For the learning models , we train them with the same Huber loss over the logarithms of the ground truth and the predicted value . All the hyper - parameters are fine - tuned to minimize the validation error . DNN , MoE and RMI cannot directly handle the threshold ùë° . We learn a non - linear transformation of ùë° into an ùëö - dimensional embedding vector , i . e . , t = ReLU ( w ùë° ) . Then we concatenate it with x as the input to these models . B . 2 Evaluation Metrics We evaluate Mean Squared Error ( MSE ) , Mean Absolute Error ( MAE ) , and Mean Absolute Percentage Error ( MAPE ) . They are defined as : MSE = 1 ùëö ùëö ‚àëÔ∏Å ùëñ = 1 ( ÀÜ ùë¶ ùëñ ‚àí ùë¶ ùëñ ) 2 , MAE = 1 ùëö ùëö ‚àëÔ∏Å ùëñ = 1 | ÀÜ ùë¶ ùëñ ‚àí ùë¶ ùëñ | , MAPE = 1 ùëö ùëö ‚àëÔ∏Å ùëñ = 1 (cid:12)(cid:12)(cid:12)(cid:12) ÀÜ ùë¶ ùëñ ‚àí ùë¶ ùëñ ùë¶ ùëñ (cid:12)(cid:12)(cid:12)(cid:12) , where ùë¶ ùëñ is the ground truth value and ÀÜ ùë¶ ùëñ is the estimated value .