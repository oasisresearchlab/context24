Consistent and Flexible Selectivity Estimation for High - Dimensional Data Yaoshu Wang 1 , Chuan Xiao 2 , 3 , Jianbin Qin 1 , Rui Mao 1 , Makoto Onizuka 2 , Wei Wang 4 , 5 , Rui Zhang 6 , and Yoshiharu Ishikawa 3 1 Shenzhen Institute of Computing Sciences , Shenzhen University , 2 Osaka University , 3 Nagoya University , 4 Dongguan University of Technology , 5 University of New South Wales , 6 www . ruizhang . info yaoshuw @ sics . ac . cn , { chuanx , onizuka } @ ist . osaka - u . ac . jp , { qinjianbin , mao } @ szu . edu . cn , weiw @ cse . unsw . edu . au , rui . zhang @ ieee . org , ishikawa @ i . nagoya - u . ac . jp ABSTRACT Selectivity estimation aims at estimating the number of database objects that satisfy a selection criterion . Answering this problem accurately and efficiently is essential to many applications , such as density estimation , outlier detection , query optimization , and data integration . The estimation problem is especially challenging for large - scale high - dimensional data due to the curse of dimension - ality , the large variance of selectivity across different queries , and the need to make the estimator consistent ( i . e . , the selectivity is non - decreasing in the threshold ) . We propose a new deep learning - based model that learns a query - dependent piecewise linear function as selectivity estimator , which is flexible to fit the selectivity curve of any distance function and query object , while guaranteeing that the output is non - decreasing in the threshold . To improve the ac - curacy for large datasets , we propose to partition the dataset into multiple disjoint subsets and build a local model on each of them . We perform experiments on real datasets and show that the pro - posed model consistently outperforms state - of - the - art models in accuracy in an efficient way and is useful for real applications . CCS CONCEPTS â€¢ Information systems â†’ Query optimization ; Entity resolu - tion ; â€¢ Computing methodologies â†’ Neural networks . KEYWORDS selectivity estimation ; high - dimensional data ; piecewise linear func - tion ; deep neural network 1 INTRODUCTION In this paper , we consider the following selectivity estimation prob - lem for high - dimensional data : given a query object x , a distance function ğ‘‘ğ‘–ğ‘ ğ‘¡ ( Â· , Â· ) , and a distance threshold ğ‘¡ , estimate the number of objects o s in a database that satisfy ğ‘‘ğ‘–ğ‘ ğ‘¡ ( x , o ) â‰¤ ğ‘¡ . This prob - lem is also known as local density estimation [ 54 ] or spherical range counting [ 9 ] in theoretical computer science . It is an essen - tial procedure in density estimation in statistics [ 52 ] and density - based outlier detection [ 11 ] in data mining . For example , for text analysis , one may want to determine the popularity of a topic ; for e - commerce , an analyst may want to find out if a user / item is an outlier ; for clustering , the algorithm may converge faster if we start with seeds in denser regions . In the database area , accu - rate estimation helps to find an optimal query execution plan in W . Wang , R . MaoandJ . Qinarethejointcorrespondingauthors . databases dealing with high - dimensional data [ 24 ] . Hands - off en - tity matching systems [ 16 ] extract paths from random forests and take each path â€“ a conjunction of similarity predicates over multi - ple attributes ( e . g . , â€œ EU ( name ) â‰¤ 0 . 25 AND EU ( affiliations ) â‰¤ 0 . 4 AND EU ( research interests ) â‰¤ 0 . 45â€ , where EU ( ) measures the Eu - clidean distance between word embeddings ) â€“ as a blocking rule , and efficient blocking can be achieved if we find a good query exe - cution plan [ 50 ] . In addition , many text or image retrieval systems resort to distributed representations . Given a query , a similarity selection is often invoked to obtain a set of candidates to be fur - ther verified by sophisticated models . Estimating the number of candidates helps to estimate the overall query processing time an end - to - end system to create a service level agreement . Selectivity estimation for large - scale high - dimensional data is still an open problem due to the following factors : ( 1 ) Large vari - ance of selectivity . The selectivity varies across queries and may differ by several orders of magnitude . A good estimator is supposed to predict accurately for both small and large selectivity values . ( 2 ) Curse of dimensionality . Many methods that work well on low - dimensional data , such as histograms [ 26 ] , are intractable when we seek an optimal solution , and they significantly lose accuracy with the increase of dimensionality . ( 3 ) Consistency requirement . When the query object is fixed , selectivity is non - decreasing in the threshold . Hence users may want the estimated selectivity to be non - decreasing and interpretable in applications such as density estimation . This requirement rules out many existing methods . To address the above challenges , we propose a novel deep re - gression method that guarantees consistency . We holistically ap - proximate the selectivity curve using a query - dependent piecewise linear function consisting of control points that are learned from training data . This function family is flexible in the sense that it can closely approximate the selectivity curve of any distance function and any input query object ; e . g . , using more control points for the part of the curve where selectivity changes rapidly . Together with a robust loss function , we are able to alleviate the impact of large variance across different queries . To handle high dimensionality , we incorporate an autoencoder that learns the latent representation of the query object with respect to the data distribution . The query object and its latent representation are fed to a query - dependent control point model , enhancing the fit to the selectivity curve of the query object . To ensure consistency , we achieve the monotonicity of estimated selectivity by converting the problem to a standard neural network prediction task , rather than imposing additional limitations such as restricting weights to be non - negative [ 15 ] or limiting to multi - linear functions [ 17 ] . To improve the accuracy a r X i v : 2005 . 09908v4 [ c s . D B ] 27 M a y 2021 YaoshuWang 1 , ChuanXiao 2 , 3 , JianbinQin 1 , RuiMao 1 , MakotoOnizuka 2 , WeiWang 4 , 5 , RuiZhang 6 , andYoshiharuIshikawa 3 on large - scale datasets , we propose a partition - based method to divide the database into disjoint subsets and learn a local model on each of them . Since update may exists in the database , we employ incremental learning to cope with this issue . We perform experiments on six real datasets . The results show that our method outperforms various state - of - the - art models . Com - pared to the best existing model [ 50 ] , the improvement of accuracy is up to 5 times in mean squared error and consistent across datasets , distance functions , and error metrics . The experiments also demon - strate that our method is competitive in estimation speed , robust against update in the database , and useful in estimating overall query processing time in a semantic search application . 2 RELATED WORK Traditional Estimation Models . Selectivity estimation has been ex - tensively studied in database systems , where prevalent approaches are based on sampling [ 53 , 55 ] , histograms [ 26 ] , or sketches [ 14 ] . However , few of them are applicable to high - dimensional data due to data sparsity or the curse of dimensionality . For cosine similarity , Wu et al . [ 54 ] proposed to use locality - sensitive hashing ( LSH ) as a means of importance sampling to tackle data sparsity . Kernel density estimation ( KDE ) [ 24 , 36 ] has been proposed to handle se - lectivity estimation in metric space . Mattig et al . [ 36 ] proposed to alleviat the curse of dimensionality by focusing on the distribution in metric space . However , strong assumptions are usually imposed on the kernel function ( e . g . , only diagonal covariance matrix for Gaussian kernels ) , and one kernel function may be inadequate to model complex distributions in high - dimensional data . Regression Models without Consistency Guarantee . Selectivity es - timation can be formalized as a regression problem with query object and threshold as input features , if the consistent require - ment is not enforced . A representative approach is quantized re - gression [ 7 , 8 ] . Recent trend uses deep regression models . Vanilla deep regression [ 32 , 46 , 47 ] learns good representations of input patterns . The mixture of expert model ( MoE ) [ 43 ] has a sparsely - gated mixture - of - experts layer that assigns data to proper experts ( models ) which lead to better generalization . The recursive model index ( RMI ) [ 31 ] is a regression model that can be used to replace the B - tree index in relational databases . Deep regression has also been used to predict selectivities ( cardinalities ) [ 29 , 45 ] in relational databases , amid a set of recent advances in learning methods for this task [ 23 , 38 , 39 , 48 , 56 ] . They target SQL queries where each predi - cate involves one attribute . [ 23 , 56 ] employ autoregressive models . [ 39 ] only deals with low dimensionality . [ 29 , 38 , 45 ] become a deep neural network if we regard a vector as an attribute . Models with Consistency Guarantee . Gradient boosting trees ( e . g . , XGBoost [ 13 ] and LightGBM [ 49 ] ) support monotonic regression . Lattice regression [ 17 , 19 , 21 , 57 ] uses a multi - linearly interpolated lookup table for regression . By enforcing constraints on its param - eter values , it can guarantee monotonicity . To accommodate high dimensional inputs , Fard et al . [ 17 ] proposed to build an ensem - ble of lattice using subsets of input features . Deep lattice network ( DLN ) [ 57 ] was proposed to interlace non - linear calibration layers and ensemble of lattice layers . Recently , lattice regression has also been used to learn a spatial index [ 33 ] . UMNN [ 51 ] is an autore - gressive flow model which adopts Clenshaw - Curtis quadrature to achieve monotonicity . Other monotonic models include isotonic regression [ 22 , 44 ] and MinMaxNet [ 15 ] . CardNet [ 50 ] is a recently proposed method for monotonic selectivity estimation of similarity selection query for various data types . It maps original data to bi - nary vectors and the threshold to an integer ğœ , and then predicts the selectivity for distance [ 0 , 1 , . . . , ğœ ] respectively with ( ğœ + 1 ) encoder - decoder models . When applying to high - dimensional data , its has the following drawbacks : the mapping from the input thresh - old to ğœ is not injective , i . e . , multiple thresholds may be mapped to the same ğœ and the same selectivity is always output for them ; the overall accuracy is significantly affected if one of the ( ğœ + 1 ) decoders is not accurate for some query . 3 PRELIMINARIES Problem 1 ( Selectivity Estimation for High - Dimensional Data ) . Given a database of ğ‘‘ - dimensional vectors D = { o ğ‘– } ğ‘› ğ‘– = 1 , o ğ‘– âˆˆ R ğ‘‘ , adistancefunction ğ‘‘ğ‘–ğ‘ ğ‘¡ ( Â· , Â· ) , ascalarthreshold ğ‘¡ , andaqueryobject x âˆˆ R ğ‘‘ , estimate the selectivity in the database , i . e . , | { o | ğ‘‘ğ‘–ğ‘ ğ‘¡ ( x , o ) â‰¤ ğ‘¡ , o âˆˆ D } | . While we assume ğ‘‘ is a distance function , it is easy to extend it to consider ğ‘‘ as a similarity function by changing â‰¤ to â‰¥ in the above definition . In the rest of the paper , to describe our method , we focus on the case when ğ‘‘ is a distance function . In addition , the query object does not have to be in the database , and we do not make any assumption on the distance function , meaning that the function does not have to be metric . We can view the selectivity ( i . e . , the ground truth label ) ğ‘¦ of a query object x and a threshold ğ‘¡ as generated by a function ğ‘¦ = ğ‘“ ( x , ğ‘¡ , D ) . We call ğ‘“ the value function . Our goal is to estimate ğ‘“ ( x , ğ‘¡ , D ) using another function Ë† ğ‘“ ( x , ğ‘¡ , D ) . One unique requirement of our problem is that the estimator Ë† ğ‘“ needs to be consistent : Ë† ğ‘“ is consistent if and only if it is non - decreasing in the threshold ğ‘¡ for every query object x ; i . e . , âˆ€ x , Ë† ğ‘“ ( x , ğ‘¡ â€² , D ) â‰¥ Ë† ğ‘“ ( x , ğ‘¡ , D ) iff . ğ‘¡ â€² â‰¥ ğ‘¡ . 4 OBSERVATIONS AND IDEAS When | D | is large , it is hard to estimate ğ‘“ directly . One of the main challenges is that ğ‘“ may be non - smooth with respect to the input variables . In the worst case , we have : â€¢ For any vector Î” x , there exists a database D of ğ‘› objects and a query ( x , ğ‘¡ ) such that ğ‘“ ( x , ğ‘¡ , D ) = 0 and ğ‘“ ( x + Î” x , ğ‘¡ , D ) = ğ‘› . â€¢ For any ğœ– > 0 , there exists a database D of ğ‘› objects and a query ( x , ğ‘¡ ) such that ğ‘“ ( x , ğ‘¡ , D ) = 0 and ğ‘“ ( x , ğ‘¡ + ğœ– , D ) = ğ‘› . This means any model that directly approximates ğ‘“ is hard . Our idea to mitigate this issue is : instead of estimating one func - tion ğ‘“ , we estimate multiple functions such that each functionâ€™s output range is a small fraction of the selectivity ğ‘¦ . For example , suppose ğ‘¦ = ğ‘¦ 1 + ğ‘¦ 2 and ğ‘¡ = ğ‘¡ 1 + ğ‘¡ 2 . If ğ‘¦ 1 and ğ‘¦ 2 are approximately linear in [ 0 , ğ‘¡ 1 ] and ( ğ‘¡ 1 , ğ‘¡ 2 ] , respectively , but with different slopes , then we can use two linear models for the two threshold ranges . We may also exploit this idea and divide ğ‘¦ with disjoint subsets of D . Hence we adopt the following two partitioning schemes . ConsistentandFlexibleSelectivityEstimationforHigh - DimensionalData Threshold Partitioning . Assume the maximum threshold we sup - port is ğ‘¡ max . We consider dividing it with an increasing sequence of ( ğ¿ + 2 ) values : [ ğœ 0 , ğœ 1 , . . . , ğœ ğ¿ + 1 ] such that ğœ ğ‘– < ğœ ğ‘— if ğ‘– < ğ‘— , ğœ 0 = 0 , and ğœ ğ¿ + 1 = ğ‘¡ max + ğœ– , where ğœ– is a small positive quantity 1 . Let ğ‘” ğ‘– ( x , ğ‘¡ ) be an interpolant function for interval [ ğœ ğ‘– âˆ’ 1 , ğœ ğ‘– ) . Then we have Ë† ğ‘“ ( x , ğ‘¡ , D ) = ğ¿ + 1 âˆ‘ï¸ ğ‘– = 1 1 (cid:74) ğ‘¡ âˆˆ [ ğœ ğ‘– âˆ’ 1 , ğœ ğ‘– ) (cid:75) Â· ğ‘” ğ‘– ( x , ğ‘¡ ) , ( 1 ) where 1 (cid:74)(cid:75) denotes the indicator function . Data Partitioning . We partition the database D into ğ¾ disjoint parts D 1 , . . . , D ğ¾ , and let ğ‘“ ğ‘– denote the value function defined on the ğ‘– - th part . Then we have Ë† ğ‘“ ( x , ğ‘¡ , D ) = (cid:205) ğ¾ğ‘– = 1 Ë† ğ‘“ ğ‘– ( x , ğ‘¡ , D ğ‘– ) . 5 SELECTIVITY ESTIMATOR 5 . 1 Threshold Partitioning Our idea is to approximate ğ‘“ using a regression model Ë† ğ‘“ ( x , ğ‘¡ , D ; Î˜ ) . Recall the sequence [ ğœ 0 , ğœ 1 , . . . , ğœ ğ¿ + 1 ] in Section 4 . We consider the family of continuous piecewise linear function to implement the interpolation ğ‘” ğ‘– ( x , ğ‘¡ ) , ğ‘– âˆˆ [ 0 , ğ¿ + 1 ] . A piecewise linear function is a continuous function of ( ğ¿ + 1 ) pieces , each being a linear function defined on [ ğœ ğ‘– âˆ’ 1 , ğœ ğ‘– ) . The ğœ ğ‘– values are called control points . Given a query object x , let ğ‘ ğ‘– denote the estimated selectivity for a threshold ğœ ğ‘– . For the ğ‘” ğ‘– function in Eq . ( 1 ) , we have ğ‘” ğ‘– ( x , ğ‘¡ ) = ğ‘ ğ‘– âˆ’ 1 + ğ‘¡ âˆ’ ğœ ğ‘– âˆ’ 1 ğœ ğ‘– âˆ’ ğœ ğ‘– âˆ’ 1 Â· ( ğ‘ ğ‘– âˆ’ ğ‘ ğ‘– âˆ’ 1 ) . ( 2 ) Hence the regression model is parameterized by Î˜ def = { ( ğœ ğ‘– , ğ‘ ğ‘– ) } ğ¿ + 1 ğ‘– = 0 . Note that ğœ ğ‘– and ğ‘ ğ‘– values are dependent on x ; i . e . , the piecewise linear function is query - dependent . Using the above design for Î˜ has the following property to guar - antee the consistency 2 . Lemma 1 . Given a database D and a query object x , if ğ‘ ğ‘– â‰¥ ğ‘ ğ‘– âˆ’ 1 for âˆ€ ğ‘– âˆˆ [ 1 , ğ¿ + 1 ] , then Ë† ğ‘“ ( x , ğ‘¡ , D ; Î˜ ) is non - decreasing in ğ‘¡ . Another salient property of our model is that it is flexible in the sense that it can arbitrarily well approximate the selectivity curve . Piecewise linear functions have been well explored to fit one - dimensional curves [ 40 ] . With a sufficient number of control points , one can find an optimal piecewise linear function to fit any one - dimensional curve . The idea is that a small range of input is highly likely to be linear with the output . When x and D are fixed , the selectivity only depends on ğ‘¡ , and thus the value function can be treated as a one - dimensional curve . To distinguish different x , we will design a deep learning approach to learn good control points and corresponding selectivities . As such , our model not only inherits the good performance of piecewise linear function but also handles different query objects . Estimation Loss . In the regression model , the ğ¿ ğœ ğ‘– values and the ( ğ¿ + 2 ) ğ‘ ğ‘– values are the parameters to be learned . We use the expected loss between ğ‘“ and Ë† ğ‘“ : ğ½ est ( Ë† ğ‘“ ) = âˆ‘ï¸ ( ( x , ğ‘¡ ) , ğ‘¦ ) âˆˆT train â„“ ( ğ‘“ ( x , ğ‘¡ , D ) , Ë† ğ‘“ ( x , ğ‘¡ , D ) ) , ( 3 ) 1 ğœ– isusedtocoverthecornercaseof ğ‘¡ = ğ‘¡ max inEq . ( 1 ) . 2 ProofisprovidedinAppendixA . where T train denotes the set of training data , and â„“ ( ğ‘¦ , Ë† ğ‘¦ ) is a loss function between the true selectivity ğ‘¦ and the estimated value Ë† ğ‘¦ of a query ( x , ğ‘¡ ) . We choose the Huber loss [ 25 ] applied to the logarithmic values of ğ‘¦ and Ë† ğ‘¦ . To prevent numeric errors , we also pad the input by a small positive quantity ğœ– . Let ğ‘Ÿ def = ln ( ğ‘¦ + ğœ– ) âˆ’ ln ( Ë† ğ‘¦ + ğœ– ) . Then â„“ ( ğ‘¦ , Ë† ğ‘¦ ) = (cid:40) ğ‘Ÿ 2 2 , if | ğ‘Ÿ | â‰¤ ğ›¿ ; ğ›¿ ( | ğ‘Ÿ | âˆ’ ğ›¿ 2 ) , otherwise . ğ›¿ is set to 1 . 345 , the standard recommended value [ 18 ] . The reason for designing such a loss function is that the selectivity may differ by several orders of magnitude for different queries . If we use the â„“ 2 loss , it encourages the model to fit large selectivities well , and if we use â„“ 1 loss , it pays more attention to small selectivities . To achieve robust prediction , we reduce the value range by logarithm and the Huber loss . 5 . 2 Learning Piecewise Linear Function We choose a deep neural network to learn the piecewise linear function . It has the following advantages : ( 1 ) Deep learning is able to capture the complex patterns in control points and corresponding selectivities for accurate estimation of different queries . ( 2 ) Deep learning generalizes well on queries that are not covered by training data . ( 3 ) The training data for our problem can be unlimitedly acquired by running a selection algorithm on the database , and this favors deep learning which often requires large training sets . In our model , ğœ ğ‘– and ğ‘ ğ‘– values are generated separately for the input query object . We also require non - negative increments be - tween consecutive parameters to ensure they are non - decreasing . In the following , we explain the learning of ğœ ğ‘– s and ğ‘ ğ‘– s , followed by the overall neural network architecture . Control Points ( ğœ ğ‘– s ) . We learn the increments between ğœ ğ‘– s . ğœ ğ‘– ( x ) = ğ‘– âˆ’ 1 âˆ‘ï¸ ğ‘— = 0 Î” ğœ ( x ) [ ğ‘— ] , ( 4 ) where Î” ğœ ( x ) = Norm ğ‘™ 2 ( ğ‘” ( ğœ ) ( x ) ) Â· ğ‘¡ ğ‘šğ‘ğ‘¥ . ( 5 ) Norm ğ‘™ 2 is a normalized squared function defined as Norm ğ‘™ 2 ( t ) = [ ğ‘¡ 21 + ğœ–ğ¿ t T t + ğœ– , . . . , ğ‘¡ 2 ğ¿ + ğœ– ğ¿ t T t + ğœ– ] , where ğœ– is a small positive quantity to avoid dividing by zero , and ğ‘¡ ğ‘– denotes the value of the ğ‘– - th dimension of t . The model takes x as input and outputs ğ¿ distinct thresholds in ( 0 , ğ‘¡ max ) . ğ‘” ( ğœ ) is implemented by a neural network . Then we have a vector ğœ = [ 0 ; ğœ 1 ; ğœ 2 ; . . . ; ğœ ğ¿ ; ğ‘¡ ğ‘šğ‘ğ‘¥ ] . One may consider using Softmax ( t ) , which is widely used for multi - classificationand ( self - ) attention . WechooseNorm ğ‘™ 2 ( t ) rather than Softmax ( t ) for the following reasons : ( 1 ) Due to the exponen - tial function in Softmax ( t ) , a small change of t might lead to large variations of the output . ( 2 ) Softmax aims to highlight the impor - tant part rather than partitioning t , while our goal is to rationally partition the range [ 0 , ğœ ğ‘šğ‘ğ‘¥ ] into several intervals such that the piecewise linear function can fit well . YaoshuWang 1 , ChuanXiao 2 , 3 , JianbinQin 1 , RuiMao 1 , MakotoOnizuka 2 , WeiWang 4 , 5 , RuiZhang 6 , andYoshiharuIshikawa 3 x x x z AE FFN M ğœ p t âˆ‘ * R e L U N o r m l 2 y t max S M psum S Figure 1 : Network architecture . Selectivities at Control Points ( ğ‘ ğ‘– s ) . We learn ( ğ¿ + 2 ) ğ‘ ğ‘– values in a similar fashion to control points , using another neural network to implement ğ‘” ( ğ‘ ) . ğ‘ ğ‘– ( x ) = ğ‘– âˆ‘ï¸ ğ‘— = 0 Î” ğ‘ ( x ) [ ğ‘— ] , ( 6 ) where Î” ğ‘ ( x ) = ReLU ( ğ‘” ( ğ‘ ) ( x ) ) . ( 7 ) Then we have a vector p = [ ğ‘ 0 ; ğ‘ 1 ; . . . ; ğ‘ ğ¿ + 1 ] . Here , we learn ( ğ¿ + 1 ) increments ( ğ‘ ğ‘– âˆ’ ğ‘ ğ‘– âˆ’ 1 ) instead of directly learning ( ğ¿ + 2 ) ğ‘ ğ‘– s . Thereby , we do not have to enforce a constraint ğ‘ ğ‘– âˆ’ 1 â‰¤ ğ‘ ğ‘– for ğ‘– âˆˆ [ 1 , ğ¿ + 1 ] in the learning process , and thus the learned model can better fit the selectivity curve . Network Architecture . Figure 1 shows our network architecture . The input x is first transformed to z , a latent representation ob - tained by an autoencoder ( AE ) . The use of the AE encourages the model to exploit latent data and query distributions in learning the piecewise linear function , and this helps the model generalize to query objects outside the training data . To learn the latent distri - butions of D , we pretrain the AE on all the objects of D , and then continue to train the AE with the queries in the training data . Due to the use of AE , the final loss function is a linear combination of the estimation loss ( Eq . ( 3 ) ) and the loss of the AE for the training data ( denoted by ğ½ AE ) : ğ½ ( Ë† ğ‘“ ) = ğ½ est ( Ë† ğ‘“ ) + ğœ† Â· ğ½ AE . ( 8 ) x is concatenated with z , i . e . , [ x ; z ] . Then [ x ; z ] is fed into two independent neural networks : a feed - forward network ( FFN ) and a model ğ‘€ ( introduced later ) . Two multiplications , denoted by ğ‘† operators in Figure 1 , are needed to separately convert the output of FFN and the output of model ğ‘€ to the ğœ and p vectors , respectively . They use a scalar ğ‘¡ max and a matrix M psum which , once multiplied on the right to a vector , perform prefix sum operation on the vector . M psum = ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ ï£° 1 0 . . . 0 1 1 . . . 0 . . . . . . . . . . . . 1 1 . . . 1 ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£º ï£» . The output of these networks , together with the threshold ğ‘¡ , are fed into the operator (cid:205) âˆ— in Figure 1 , which is implemented by Eqs . ( 2 ) , ( 5 ) , and ( 7 ) , to compute the output of the piecewise linear function , i . e . , the estimated selectivity . Figure 2 : Data partitioning by cover tree . Model ğ‘€ . To achieve better performance , we learn p using an encoder - decoder model . In the encoder , an FFN is used to generate ( ğ¿ + 2 ) embeddings : [ h 0 ; h 1 ; . . . ; h ğ¿ + 1 ] = FFN ( [ x ; z ] ) , ( 9 ) where h ğ‘– s are high - dimensional representations . Here , we adopt ( ğ¿ + 2 ) embeddings , i . e . , h 0 , . . . , h ğ¿ + 1 , to represent the latent infor - mation of p . In the decoder , we adopt ( ğ¿ + 2 ) linear transformations with the ReLU activation function : ğ‘˜ ğ‘– = ReLU ( w T ğ‘– h ğ‘– + ğ‘ ğ‘– ) . Then we have p = [ ğ‘˜ 0 , ğ‘˜ 0 + ğ‘˜ 1 , . . . , (cid:205) ğ¿ + 1 ğ‘– = 0 ğ‘˜ ğ‘– ] . 5 . 3 Data Partitioning To improve the accuracy of estimation on large - scale datasets , we divide the database into multiple disjoint subsets D 1 , . . . , D ğ¾ with approximately the same size , and build a local model on each of them . Let Ë† ğ‘“ ğ‘– denote each local model . Then the global model for selectivity estimation is Ë† ğ‘“ = (cid:205) ğ‘– Ë† ğ‘“ ğ‘– . We have considered several design choices and propose the fol - lowing configuration that achieves the best empirical performance : ( 1 ) Partitioning is obtained by a cover tree - based strategy . ( 2 ) We adopt the structure in Figure 1 so that all local models share the same input representation [ x ; z ] , but each has its own neural net - works to learn the control points . Partitioning Method . We utilize a cover tree [ 27 ] to partition D into several parts . A partition ratio ğ‘Ÿ is predefined such that the cover tree will not expand its nodes if the number of inside objects is smaller than ğ‘Ÿ | D | . Given a query ( x , ğ‘¡ ) , the valid region is the circles that intersect the circle with x as center and ğ‘¡ as radius . For example , in Figure 2 , x ( the red point ) and ğ‘¡ form the red circle , and data are partitioned into 6 regions . The valid region of ( x , ğ‘¡ ) is the green circles that intersect the red circle . Albeit imposing constraints , cover tree might still generate too many ball regions , i . e . , leaf nodes , which lead to large number of parameters of the model and the difficulty of training . Reducing the number of ball regions is necessary . To remedy this , we adopt a merging strategy as follows . First , we still partition D into ğ¾ â€² regions using cover tree . Then we cluster these regions into ğ¾ ( ğ¾ â€² â‰¤ ğ¾ ) clusters D 1 , . . . , D ğ¾ by the following greedy strategy : The ğ¾ â€² regions are sorted in decreasing order of the number of inside objects . We begin with ğ¾ empty clusters . Then we scan each region and assign it to the cluster with the smallest size . The regions that belong to the same cluster are merged to one region . We consider an indicator ConsistentandFlexibleSelectivityEstimationforHigh - DimensionalData ğ‘“ ğ‘ : ( x , ğ‘¡ ) â†’ { 0 , 1 } ğ¾ such that ğ‘“ ğ‘ ( x , ğ‘¡ ) [ ğ‘– ] = 1 if and only if the query ( x , ğ‘¡ ) intersects cluster D ğ‘– , and employ it in our model : Ë† ğ‘“ ( x , ğ‘¡ , D ) = ğ¾ âˆ‘ï¸ ğ‘– = 0 ğ‘“ ğ‘ ( x , ğ‘¡ ) [ ğ‘– ] Â· Ë† ğ‘“ ğ‘– ( x , ğ‘¡ , D ğ‘– ) . Since cover trees deal with metric spaces , for non - metric functions ( e . g , cosine similarity ) , if possible , we equivalently convert it to a metric ( e . g , Euclidean distance , as cos ( u , v ) = 1 âˆ’ âˆ¥ u , v âˆ¥ 2 2 for unit vectors u and v ) . Then the cover tree partitioning still works . For those that cannot be equivalently converted to a metric , we adopt random partitioning and modify ğ‘“ ğ‘ as ğ‘“ ğ‘ : ( x , ğ‘¡ ) â†’ { 1 } ğ¾ . Training Procedure . We have several choices on how to train the models from multiple partitions . The default is directly training the global model Ë† ğ‘“ , with the advantage that no extra work is needed . The other choice is to train each local model independently , using the selectivity computed on the local partition as training label . We propose yet another choice : we pretrain the local models for ğ‘‡ epochs , and then train them jointly . In the joint training stage , we use the following loss function : ğ½ joint = ğ½ est ( Ë† ğ‘“ ) + ğ›½ Â· âˆ‘ï¸ ğ‘– ğ½ est ( Ë† ğ‘“ ğ‘– ) + ğœ† Â· ğ½ AE . The indicators ğ‘“ ğ‘ ( Â· , Â· ) s of all ( x , ğ‘¡ ) are precomputed before training . 5 . 4 Dealing with Data Updates When the database D is updated with insertion or deletion , we first check whether our model Ë† ğ‘“ ( x , ğ‘¡ , D ) is necessary to update . In other words , when minor updates occur and Ë† ğ‘“ ( x , ğ‘¡ , D ) is still accurate enough , we ignore them . To check the accuracy of Ë† ğ‘“ ( x , ğ‘¡ , D ) , we update the labels of all validation data , and re - test the mean abso - lute error ( MAE ) of Ë† ğ‘“ ( x , ğ‘¡ , D ) . If the difference between the original MAE and the new one is no larger than a predefined threshold ğ›¿ ğ‘ˆ , we do not update our model . Otherwise , we adopt an incremental learning approach as follows . First , we update the labels in the training and the validation data to reflect the update in the data - base . Second , we continue training our model with the updated training data until the validation error ( MAE ) does not increase in 3 consecutive epochs . Here the training does not start from scratch but from the current model . We incrementally train our model with all the training data to prevent catastrophic forgetting . 6 DISCUSSIONS 6 . 1 Model Complexity Analysis We assume an FFN has hidden layers a 1 , . . . , a ğ‘› . The complexity of an FFN with input x and output y is | FFN ( x , y ) | = | x | Â· | a 1 | + (cid:205) ğ‘› âˆ’ 1 ğ‘– = 1 | a ğ‘– | Â· | a ğ‘– + 1 | + | a ğ‘› | Â· | y | . Our model contains three components : AE , FFN , and ğ‘€ . The com - plexity of AE is | FFN ( x , z ) | . The complexity of FFN is | FFN ( [ x ; z ] , t ) | , where t is the ğ¿ - dimensional vector after Norm ğ‘™ 2 . Component ğ‘€ consists of an FFN and ( ğ¿ + 2 ) linear transformations . Its complexity is | FFN ( [ x ; z ] , H ) | + ( ğ¿ + 2 ) Â· | h ğ‘– | + ( ğ¿ + 2 ) , where H = [ h 0 ; . . . ; h ğ¿ + 1 ] . Thus , the final model complexity is | FFN ( x , z ) | + | FFN ( [ x ; z ] , t ) | + | FFN ( [ x ; z ] , H ) | + ( ğ¿ + 2 ) Â· | h ğ‘– | + ( ğ¿ + 2 ) . ( a ) Simplified DLN ( b ) OurModel Figure 3 : Comparison of simplified DLN and our model . 6 . 2 Comparison with Other Models Lattice Regression . Lattice regression models [ 17 , 19 , 21 , 57 ] are the latest deep learning architectures for monotonic regression . We provide a comparison between ours and them applied to selectivity estimation . For the sake of an analytical comparison , we assume x and D are fixed so the selectivity only depends on ğ‘¡ , and consider a shallow version of DLN [ 57 ] with one layer of calibrator and one layer of a single lattice . With the above simplification , the DLN can be analytically rep - resented as : Ë† ğ‘“ DLN ( ğ‘¡ ) = â„ ( ğ‘” ( ğ‘¡ ; w ) ; ğœƒ 0 , ğœƒ 1 ) , where ğ‘” : ğ‘¡ âˆˆ [ 0 , ğ‘¡ max ] â†¦â†’ ğ‘§ âˆˆ [ 0 , 1 ] and â„ ( ğ‘§ ; ğœƒ 0 , ğœƒ 1 ) = ( 1 âˆ’ ğ‘§ ) ğœƒ 0 + ğ‘§ğœƒ 1 . Hence it degenerates to fitting a linear interpolation in a latent space . There is little learning for the function â„ , as its two parameters ğœƒ 0 and ğœƒ 1 are determined by the minimum and maximum selectivity values in the training data . Thus , the workhorse of the model is to learn the non - linear mapping of ğ‘” . The calibrator also uses piecewise linear functions with ğ¿ control points equivalent to our ( ğœ ğ‘– , ğ‘ ğ‘– ) ğ¿ğ‘– = 1 . However , ğœ ğ‘– s are equally spaced between 0 and ğ‘¡ max , and only ğ‘ ğ‘– s are learnable . This design is not flexible for many value functions ; e . g . , if the function values change rapidly within a small interval , the calibra - tor will not adaptively allocate more control points to this area . We show this with 8 control points for both models to learn the function ğ‘¦ = ğ‘“ ( ğ‘¡ ) = 110 exp ( ğ‘¡ ) , ğ‘¡ âˆˆ [ 0 , 10 ] . The training data are 80 ( ğ‘¡ ğ‘– , ğ‘“ ( ğ‘¡ ğ‘– ) ) pairs where ğ‘¡ ğ‘– s are uniformly sampled in [ 0 , 10 ] . We plot both modelsâ€™ estimation curves and their learned control points in Figure 3 . The ğ‘§ values at the control points of DLN are shown on the right side of Figure 3 ( a ) . We observe : ( 1 ) The calibrator virtually determines the estimation as â„ ( ) degenerates to a simple scaling . ( 2 ) The calibratorâ€™s control points are evenly spaced in ğ‘¡ , while our model learns to place more controls points in the â€œinteresting areaâ€ , i . e . , where ğ‘¦ values change rapidly . ( 3 ) As a result , our model approximates the value function much better than DLN . Further , for DLN , the non - linear mapping on ğ‘¡ is independent of x ( even though we do not model x here ) . Even in the full - fledged DLN model , the calibration is performed on each input dimension independently . The full - fledged DLN model is too complex to ana - lyze , so we only study it in our empirical evaluation . Nonetheless , we believe that the above inherent limitations still remain . Our empirical evaluation will also show that query - dependent fitting of the value function is critical in our problem . Apart from DLN , recent studies also employ lattice regression and / or piecewise linear functions for learned index [ 30 , 33 ] . Like DLN , their control points are also query independent , albeit not equally spaced . YaoshuWang 1 , ChuanXiao 2 , 3 , JianbinQin 1 , RuiMao 1 , MakotoOnizuka 2 , WeiWang 4 , 5 , RuiZhang 6 , andYoshiharuIshikawa 3 Table 1 : Statistics of datasets . Dataset Source Domain # Objects Dimensionality Distance fastText [ 1 ] text 1M 300 Euclidean GloVe [ 2 ] text 1 . 9M 300 Euclidean MS - Celeb [ 20 ] image 2M 128 cosine YouTube [ 3 ] video 0 . 35M 1770 cosine DEEP [ 4 ] image 100M 96 cosine SIFT [ 5 ] image 200M 128 cosine Clenshaw - Curtis Quadrature . Clenshaw - Curtis quadrature [ 37 ] is able to approximate the integral âˆ« ğœ ğ‘šğ‘ğ‘¥ 0 Ë† ğ‘” ( x , ğ‘¡ , D ) d ğ‘¡ , where Ë† ğ‘” = ğœ• Ë† ğ‘“ ( x , ğ‘¡ , D ) ğœ•ğ‘¡ in our problem . UMNN [ 51 ] is a recent work that adopts the idea to solve the autoregressive flow problem , and uses a neu - ral network to model Ë† ğ‘” . In [ 37 ] , the cosine transform of Ë† ğ‘” ( ğ‘ğ‘œğ‘ ğœƒ ) is adopted and the discrete finite cosine transform is sampled at equidistant points ğœƒ = ğœ‹ğ‘ ğ‘ , where ğ‘  = 1 , . . . , ğ‘ , and ğ‘ is the number of sample points . Similar to DLN , it adopts the same integral approx - imation for different queries and ignores that integral points should depend on x . In contrast , our method addresses this issue by using a query - dependent model , thereby delivering more flexibility . Query - Driven Quantized Regression . The main idea of query - driven quantized regression [ 7 , 8 ] is to quantize the query space and find prototypes ( the closest one or multiple related ones ) for the given query object . Then the output space is quantized by proto - types , and localized regressions are used to estimate the selectivity for corresponding prototypes . Like our model , they also employ a query - dependent design . The differences from ours are : ( 1 ) [ 7 , 8 ] divide the query space of ( x , ğ‘¡ ) while we divide the range of thresh - old ğ‘¡ using x . ( 2 ) The number of prototypes is finite and often up to thousands in [ 7 , 8 ] , while our model chooses the selectivity curve for the query object via an FFN and model ğ‘€ ( Figure 1 ) , which yield an unlimited number of curves in R ğ¿ + 2 . ( 3 ) We employ deep regres - sion for higher accuracy . ( 4 ) We directly partition the database D and train multiple deep models to deal with the subsets of D that may differ in data distribution , while the data subspace in [ 8 ] is defined by its query prototype . 7 EVALUATIONS 7 . 1 Experimental Settings Datasets . We use six datasets . The statistics is given in Table 1 . We preprocess MS - Celeb by faceNet [ 42 ] to obtain vectors . The other datasets have already been transformed to high - dimensional data . GloVe , YouTube , DEEP , and SIFT were also used in previous work [ 50 ] or nearest neighbor search benchmarks [ 10 , 34 ] . We randomly sample 0 . 25M vectors from each dataset D as query objects . The resulting query workload , denoted by Q , was uniformly split in 8 : 1 : 1 ( by query objects ) into training , validation , and test sets . So none of the test query objects has been seen by the model during training or validation . Note that labels ( i . e . , true selectivities ) are computed on D , not Q . For each training query object , we iterate through all the generated thresholds and add them to the training set . We randomly choose 3 generated thresholds for each validation or test query object . Due to the large number of training data , we randomly select training instances for each batch instead of continuously loading them , and the training procedure terminates when the mean squared error of the validation set does not increase in 5 consecutive epochs . For each setting , we tested on 5 sampled workloads to mitigate the effect of sampling error . Methods . We compare the following approaches 3 . â€¢ RS is a random sampling approach . For each query , we uniformly sample 0 . 1 % | D | objects for the first four datasets and 0 . 01 % | D | objectsfor DEEP and SIFT . Thenweuse scipy . spatial . distance . cdist to compute the distances to the query objects in a batch manner . â€¢ IS [ 54 ] isanimportancesampling approach usinglocality - sensitive hashing . It only works for cosine similarity due to the use of SimHash [ 12 ] . We enforce monotonicity by using deterministic sampling w . r . t . the query object . â€¢ KDE [ 36 ] is based on adaptive kernel density estimation for metric distance functions . To cope with cosine similarity , we nor - malize data to unit vectors and run KDE for Euclidean distance . â€¢ QR - 1 [ 7 ] and QR - 2 [ 8 ] are two query - driven quantized regres - sion models . We use the linear model in [ 7 ] for QR - 1 . â€¢ LightGBM [ 49 ] is based on gradient boosting decision trees ( CARTs ) . Each rule in a CART is in the form of ğ‘¥ ğ‘– < ğ‘ ( ğ‘¥ ğ‘– is the ğ‘– - th dimension of x ) or ğ‘¡ < ğ‘ . â€¢ Deep regression models : DNN , a vanilla feed - forward network ; MoE [ 43 ] , a mixture of expert model with sparse activation ; RMI [ 31 ] , a hierarchical mixture of expert model ; and Card - Net [ 50 ] , a regression model based on incremental prediction ( we enable the accelerated estimation [ 50 ] ) . â€¢ Lattice regression models : We adopt DLN [ 57 ] in this category . â€¢ Clenshaw - Curtis quadrature model : We adopt UMNN [ 51 ] . â€¢ Our model is dubbed SelNet 4 . The default setting of ğ¿ ( number of control points ) is 50 and ğ¾ ( partition size ) is 3 . The predefined threshold ğ›¿ ğ‘ˆ for incremental learning is 20 . We also evaluate two ablated models : ( 1 ) SelNet - ct is SelNet without the cover tree partitioning , and ( 2 ) SelNet - ad - ct is SelNet - ct without the query - dependent feature for control points ( disabled by feeding a constant vector into the FFN that generates the ğœ vector ) . Error Metrics . We evaluate Mean Squared Error ( MSE ) , Mean Ab - solute Percentage Error ( MAPE ) , and Mean Absolute Error ( MAE ) . Environment . Experiments were run on a server with an Intel Xeon E5 - 2640 @ 2 . 40GHz CPU and 256GB RAM , running Ubuntu 16 . 04 . 4 LTS . Models were implemented in Python and Tensorflow . 7 . 2 Accuracy We report accuracies in Table 2 , where monotonic models are marked with * , and best values are marked in boldface . Our model , SelNet , consistently outperforms existing models . It achieves sub - stantial error reduction against the best of state - of - the - art methods , in all the three error metrics and all the settings . Compare to the runner - up model on each dataset , the improvement is 2 . 0 â€“ 5 . 0 times in MSE , 1 . 3 â€“ 3 . 3 times in MAE , and 1 . 2 â€“ 1 . 7 times in MAPE , and is more significant on larger datasets . We examine each category of models . We start with the sampling - based methods . KDE works better than RS and IS in most settings . 3 PleaseseeAppendixBformodelsettings . 4 Thesourcecodeisavailableat [ 6 ] . ConsistentandFlexibleSelectivityEstimationforHigh - DimensionalData Table 2 : Accuracy ( MSE and MAE measured in 10 5 and 10 2 , respectively ) . Model fastText GloVe MS - Celeb YouTube DEEP SIFT MSE MAE MAPE MSE MAE MAPE MSE MAE MAPE MSE MAE MAPE MSE MAE MAPE MSE MAE MAPE RS * 22 . 38 7 . 45 1 . 40 34 . 85 8 . 71 1 . 09 28 . 64 8 . 09 1 . 30 2 . 95 1 . 89 0 . 88 84732 . 32 725 . 21 1 . 26 30317437 . 60 9496 . 55 0 . 98 IS * - - - - - - 104 . 58 14 . 25 1 . 25 2 . 85 1 . 83 0 . 76 50242 . 10 314 . 12 0 . 97 32049511 . 88 10612 . 42 0 . 92 KDE * 21 . 46 6 . 57 1 . 28 37 . 52 8 . 93 0 . 87 36 . 43 8 . 42 1 . 02 2 . 93 1 . 90 0 . 70 39497 . 24 298 . 10 0 . 85 27651109 . 31 9581 . 89 0 . 91 QR - 1 41 . 79 8 . 95 1 . 02 50 . 13 9 . 21 0 . 99 74 . 35 11 . 60 1 . 06 3 . 42 2 . 01 0 . 71 37054 . 11 292 . 16 0 . 78 28077423 . 18 9953 . 34 0 . 94 QR - 2 34 . 35 8 . 03 0 . 97 42 . 47 9 . 01 0 . 86 42 . 94 9 . 05 0 . 89 2 . 74 1 . 85 0 . 55 31485 . 85 273 . 22 0 . 67 22048511 . 27 8542 . 73 0 . 71 LightGBM 98 . 77 9 . 56 1 . 04 72 . 11 10 . 87 0 . 89 101 . 29 9 . 51 0 . 45 4 . 01 2 . 00 0 . 52 44036 . 45 301 . 88 0 . 85 23849121 . 36 9005 . 17 0 . 75 DNN 63 . 54 11 . 25 1 . 33 52 . 31 9 . 39 0 . 91 110 . 77 17 . 14 0 . 89 2 . 78 1 . 77 0 . 51 20454 . 11 192 . 13 0 . 69 24465910 . 26 7144 . 68 0 . 55 MoE 45 . 90 8 . 50 0 . 91 30 . 14 7 . 05 0 . 91 21 . 25 4 . 32 0 . 30 1 . 58 1 . 59 0 . 53 18068 . 93 170 . 51 0 . 65 14750194 . 30 6327 . 47 0 . 40 RMI 26 . 16 6 . 10 0 . 87 29 . 32 6 . 89 0 . 74 22 . 16 6 . 07 0 . 35 1 . 77 1 . 62 0 . 55 9498 . 21 116 . 54 0 . 67 8906108 . 00 4650 . 29 0 . 42 CardNet * 25 . 67 6 . 16 0 . 90 27 . 05 6 . 19 0 . 78 13 . 67 4 . 08 0 . 27 1 . 41 1 . 44 0 . 48 9230 . 48 117 . 37 0 . 67 7248693 . 54 4851 . 43 0 . 40 DLN * 77 . 50 11 . 56 1 . 53 52 . 26 10 . 27 0 . 89 82 . 35 11 . 85 0 . 97 2 . 94 1 . 92 0 . 69 58291 . 42 353 . 08 0 . 94 23059384 . 16 8058 . 49 0 . 51 UMNN * 33 . 26 7 . 20 0 . 92 33 . 50 7 . 98 0 . 86 16 . 75 4 . 70 0 . 36 2 . 06 1 . 69 0 . 49 10603 . 68 131 . 04 0 . 73 10201332 . 32 5443 . 31 0 . 43 SelNet * 7 . 87 3 . 56 0 . 76 9 . 17 3 . 83 0 . 68 4 . 96 2 . 43 0 . 23 0 . 72 1 . 13 0 . 36 2243 . 42 51 . 92 0 . 51 1464247 . 70 1406 . 63 0 . 23 Table 3 : Empirical monotonicity ( % ) on MS - Celeb . RS * IS * KDE * QR - 1 QR - 2 100 100 100 85 . 39 84 . 86 LightGBM DNN MoE RMI 86 . 34 78 . 22 94 . 82 90 . 48 CardNet * DLN * UMNN * SelNet * 100 100 100 100 In fact , KDE â€™s performance even outperforms some deep learning regression based methods in a few cases ( e . g . , MSE on fastText ) . Among non - deep learning models , these is no best model across all the datasets , though QR - 2 prevails on more datasets than oth - ers . Among the deep learning models other than ours , CardNet is generally the best thanks to its incremental prediction for each threshold interval . The performance of DLN is mediocre . The main reason is analyzed in Section 6 . 2 . The accuracy of UMNN , which uses the same integral points for different queries , though better than DLN , still trails behind ours by a large margin . 7 . 3 Consistency Test We compute the empirical monotonicity measure [ 15 ] and show the results in Table 3 . The measure is the percentage of estimated pairs that violate the monotonicity , averaged over 200 queries . For each query , we sampled 100 thresholds , which form (cid:0) 100 2 (cid:1) pairs . A low score indicates more inconsistent estimates . As expected , models without consistency guarantee cannot produce 100 % monotonicity . 7 . 4 Ablation Study Table 4 shows that the partitioning ( SelNet v . s . SelNet - ct ) improves MSE , MAE , and MAPE by up to 3 . 4 , 2 . 1 , and 1 . 2 times , respectively , and the effect is more remarkable on large datasets . This is because each model deals with a subset of the dataset for better fit and the ground truth label values for each model are reduced , which makes it easier to fit our piecewise linear function with the same number of control points , as the value function is less steep . Using query - dependent control points ( SelNet - ct v . s . SelNet - ad - ct ) also has a significant impact on accuracy across all the settings and all the error metrics . The improvements in MSE , MAE , and MAPE are up to 3 . 1 , 2 . 0 , and 3 . 6 times , respectively . Table 4 : Ablation study . Dataset Model MSE ( Ã— 10 5 ) MAE ( Ã— 10 2 ) MAPE fastText SelNet 7 . 87 3 . 56 0 . 76 SelNet - ct 12 . 63 4 . 37 0 . 81 SelNet - ad - ct 39 . 59 8 . 72 2 . 90 GloVe SelNet 9 . 17 3 . 83 0 . 68 SelNet - ct 22 . 43 5 . 82 0 . 70 SelNet - ad - ct 32 . 59 6 . 92 0 . 90 MS - Celeb SelNet 4 . 96 2 . 43 0 . 23 SelNet - ct 5 . 31 2 . 92 0 . 24 SelNet - ad - ct 16 . 02 4 . 65 0 . 37 YouTube SelNet 0 . 72 1 . 13 0 . 36 SelNet - ct 0 . 90 1 . 20 0 . 39 SelNet - ad - ct 1 . 65 1 . 59 0 . 53 DEEP SelNet 2243 . 42 51 . 92 0 . 51 SelNet - ct 5861 . 43 72 . 18 0 . 58 SelNet - ad - ct 9012 . 57 101 . 42 0 . 71 SIFT SelNet 1464247 . 70 1406 . 63 0 . 23 SelNet - ct 4958113 . 22 2911 . 86 0 . 27 SelNet - ad - ct 6904808 . 25 3855 . 53 0 . 54 7 . 5 Estimation Time Table 5 reports the estimation times of the competitors . We also report the time of running a state - of - the - art selection algorithm ( CoverTree [ 27 ] ) toobtain the exactselectivity . All the models except IS are at least one order of magnitude faster than CoverTree , and the gaps increase to three orders of magnitude on DEEP and SIFT . Our model is on a par with other deep learning models ( except DNN ) and faster than sampling and quantized regression methods . 7 . 6 Training Table 6 shows the training times . Non - deep models are faster to train . Our models spend 5 â€“ 6 hours , similar to other deep models . In Figure 4 , we show the performances , measured by MSE , of the deep learning models by varying the scale of training examples from 20 % to 100 % of the original training data . All the models perform worse with fewer training data , but our models are more robust , showing moderate accuracy loss . 7 . 7 Data Update We generate a stream of 100 update operations , each with an inser - tion or deletion of 5 records on fastText and MS - Celeb , to evaluate YaoshuWang 1 , ChuanXiao 2 , 3 , JianbinQin 1 , RuiMao 1 , MakotoOnizuka 2 , WeiWang 4 , 5 , RuiZhang 6 , andYoshiharuIshikawa 3 Table 5 : Average estimation time ( milliseconds ) . Model fastText GloVe MS - Celeb YouTube DEEP SIFT CoverTree 8 . 14 8 . 85 9 . 65 6 . 11 214 395 RS * 0 . 46 0 . 51 0 . 49 0 . 52 2 . 54 4 . 72 IS * - - 1 . 08 2 . 35 4 . 97 6 . 81 KDE * 0 . 79 0 . 68 0 . 59 0 . 94 1 . 48 2 . 05 QR - 1 0 . 86 0 . 98 0 . 97 1 . 03 2 . 21 2 . 82 QR - 2 0 . 79 0 . 99 0 . 95 1 . 10 2 . 32 2 . 91 LightGBM 0 . 28 0 . 30 0 . 18 0 . 52 0 . 26 0 . 26 DNN 0 . 07 0 . 10 0 . 03 0 . 16 0 . 11 0 . 10 MoE 0 . 36 0 . 33 0 . 27 0 . 49 0 . 29 0 . 33 RMI 0 . 34 0 . 38 0 . 25 0 . 47 0 . 27 0 . 30 CardNet * 0 . 19 0 . 26 0 . 14 0 . 31 0 . 22 0 . 28 DLN * 0 . 83 0 . 69 0 . 65 1 . 22 0 . 64 0 . 80 UMNN * 0 . 39 0 . 32 0 . 24 0 . 52 0 . 26 0 . 32 SelNet * 0 . 35 0 . 31 0 . 24 0 . 51 0 . 29 0 . 36 Table 6 : Training time ( hours ) . Model fastText GloVe MS - Celeb YouTube DEEP SIFT KDE * 1 . 1 1 . 5 0 . 7 0 . 8 2 . 5 3 . 6 QR - 1 1 . 8 2 . 1 1 . 5 1 . 2 3 . 9 4 . 6 QR - 2 1 . 5 2 . 0 1 . 6 1 . 2 3 . 6 4 . 7 LightGBM 2 . 1 1 . 9 2 . 2 2 . 1 1 . 9 2 . 1 DNN 2 . 9 2 . 1 2 . 8 2 . 9 2 . 5 2 . 9 MoE 4 . 9 5 . 4 4 . 9 4 . 7 4 . 3 4 . 4 RMI 5 . 4 5 . 6 4 . 8 5 . 3 4 . 6 4 . 8 CardNet * 3 . 8 3 . 9 3 . 3 3 . 2 3 . 5 3 . 8 DLN * 6 . 9 7 . 1 6 . 0 6 . 5 6 . 3 6 . 4 UMNN * 5 . 5 5 . 7 4 . 9 5 . 2 5 . 4 4 . 6 SelNet * 6 . 0 5 . 5 5 . 2 5 . 6 5 . 2 5 . 0 Table 7 : Varying number of control points on fastText . Error Metric Number of Control Points 10 50 90 130 MSE ( Ã— 10 5 ) 13 . 06 7 . 87 7 . 93 10 . 47 MAE ( Ã— 10 2 ) 4 . 85 3 . 56 3 . 56 3 . 92 MAPE 0 . 87 0 . 76 0 . 76 0 . 79 our incremental learning technique . Figure 5 plots how MSE and MAPE change with the stream . The general trend is that the MSE is decreasing when there are more updates , while MAPE fluctuates or keeps almost the same . Such difference is caused by the change of labels ( i . e . , true selectivities ) in the stream . Nonetheless , the result indicates that incremental learning is able to keep up with the up - dated data . Besides , SelNet only spends 1 . 5 â€“ 2 . 0 minutes for each incremental learning , showcasing its speed to cope with updates . 7 . 8 Evaluation of Hyper - Parameters Table 7 shows the accuracy when we vary the number of control points ğ¿ on fastText . A small value leads to underfitting towards the curve of thresholds , while a large value increases the learning difficulty . ğ¿ = 50 achieves the best performance . Table 8 reports the accuracy when we vary the partition size ğ¾ on fastText . There is no partitioning when ğ¾ = 1 . We observe that the partitioning is useful , but the improvement is small when partition size exceeds 3 , and estimation time also substantially in - creases . This means a small partition size ( ğ¾ = 3 ) suffices to achieve 20 40 60 80 100 TrainingSize 10 1 10 2 M SE ( 10 5 ) SelNetCardNet RMIMoE DLNUMNN ( a ) MSE , fastText 20 40 60 80 100 TrainingSize 20 40 60 80 M SE ( 10 5 ) SelNetCardNet RMIMoE DLNUMNN ( b ) MSE , GloVe 20 40 60 80 100 TrainingSize 10 1 10 2 M SE ( 10 5 ) SelNetCardNet RMIMoE DLNUMNN ( c ) MSE , MS - Celeb 20 40 60 80 100 TrainingSize 10 20 30 40 50 60 M SE ( 10 4 ) SelNetCardNet RMIMoE DLNUMNN ( d ) MSE , YouTube 20 40 60 80 100 TrainingSize 0 20 40 60 80 M SE ( 10 8 ) SelNetCardNet RMIMoE DLNUMNN ( e ) MSE , DEEP 20 40 60 80 100 TrainingSize 0 10 20 30 40 50 M SE ( 10 11 ) SelNetCardNet RMIMoE DLNUMNN ( f ) MSE , SIFT Figure 4 : Varying training data size . 0 20 40 60 80 100 Sequence of Operations 4 5 6 7 8 M S E ( 10 5 ) fastText MS - Celeb ( a ) MSE 0 20 40 60 80 100 Sequence of Operations 20 40 60 80 100 M A P E ( % ) fastText MS - Celeb ( b ) MAPE Figure 5 : Data update . good performance . For partitioning strategy , we compare cover tree partitioning ( CT ) with random partitioning ( RP ) and ğ‘˜ - means partitioning ( KM ) in Table 9 . CT delivers the best performance . KM is the worst because it tends to cause imbalance in the partition . 7 . 9 Generalizability To show the generalizability of our model , we evaluate the per - formance on the queries that significantly differ from the records in the training data . To prepare such queries , we first perform a ConsistentandFlexibleSelectivityEstimationforHigh - DimensionalData Table 8 : Varying partition size on fastText . Error Metric Partition Size 1 3 6 9 MSE ( Ã— 10 5 ) 12 . 63 7 . 87 6 . 82 6 . 75 MAE ( Ã— 10 2 ) 4 . 37 3 . 56 3 . 36 3 . 11 MAPE 0 . 81 0 . 76 0 . 77 0 . 74 Estimation Time ( ms ) 0 . 16 0 . 35 0 . 79 1 . 24 Table 9 : Varying partitioning method on fastText . Error Metric CT ( 3 ) RP ( 3 ) KM ( 3 ) MSE ( Ã— 10 5 ) 7 . 87 8 . 02 9 . 14 MAE ( Ã— 10 2 ) 3 . 56 3 . 57 3 . 64 MAPE 0 . 76 0 . 78 0 . 79 [ 0 , 1 ) [ 1 , 10 ) [ 10 , 100 ) â‰¥ 100 SelectivityRange ( 10 3 ) 10 5 10 6 10 7 10 8 10 9 10 10 M SE SelNet CardNet RMI KDE ( a ) DEEP [ 0 , 1 ) [ 1 , 10 ) [ 10 , 100 ) â‰¥ 100 SelectivityRange ( 10 3 ) 10 6 10 8 10 10 M SE SelNet CardNet RMI KDE ( b ) SIFT Figure 6 : Generalizability . T r ue T i m e E x a c t S e l S e l N e t C a r d N e t R M I K D E R S 0 10000 20000 30000 40000 E s t i m a t ed T i m e ( s ) ( a ) AMiner - Paper T r ue T i m e E x a c t S e l S e l N e t C a r d N e t R M I K D E R S 0 500 1000 1500 2000 2500 E s t i m a t ed T i m e ( s ) ( b ) Quora Figure 7 : Estimated search time ( 10 , 000 queries ) . ğ‘˜ - means clustering on D . We randomly sample 10 , 000 query ob - jects from D ( excluding the queries used for training ) and add Gaussian noise [ 58 ] . Then we pick the top - 2 , 000 ones having the largest sum of squared distance to the ğ‘˜ centroids . Figure 6 show the performances of KDE , RMI , CardNet , and SelNet on DEEP and SIFT , measured by MSE . The queries are grouped by selectivity range . In each selectivity group , SelNet consistently outperforms the other models , and the advantage is around one order of magni - tude . This result demonstrates that our model generalizes well for out - of - dataset queries . 7 . 10 Performance in Semantic Search To evaluate the usefulness of SelNet , we consider estimating the overall processing time for a query workload of semantic search : given a query text entry , we want to find matching records in the database . Estimating the query processing time may help to create a service level agreement . We use two datasets , AMiner - Paper publications ( 2 . 1M records ) and Quora questions ( 0 . 8M records ) . AMiner - Paper has four attributes : title , authors , venue , and year . We follow [ 35 ] and concatenate attribute names and values as one string . Quora has one attribute . Then we embed each record to a 768 - dimensional vector by Sentence - BERT [ 41 ] . 10 , 000 records are sampled from each dataset as queries . To pro - cess a query , we first embed it by Sentence - BERT [ 41 ] , and then use Faiss [ 28 ] to find candidate records whose cosine similarity to the query embedding is no less than 0 . 9 . The candidates are ver - ified using DITTO [ 35 ] . Hence the overall query processing time can be estimated as : avg _ Faiss _ time Ã— 10000 + avg _ DITTO _ time Ã— Faiss _ recall Ã— (cid:205) 100001 estimated _ selectivity _ of _ query _ ğ‘– . The aver - age times and Faiss recall are obtained by running a small query workload . For selectivity , we consider RS , KDE , RMI , CardNet , SelNet , and an oracle that outputs the exact selectivity ( ExactSel ) . We plot the estimated time of processing 10 , 000 queries in Fig - ure 7 , where TrueTime indicates the ground truth . The models tend to underestimate on AMiner - Paper and overestimate on Quora . SelNet â€™s high accuracy in selectivity estimation pays off . Compared to the ground truth , SelNet â€™s error is 13 % on AMiner - Paper and 16 % on Quora , close to ExactSel â€™s and much lower than the other modelsâ€™ ( at least 39 % ) . SelNet is also efficient ; e . g . , running the workload to obtain the ground truth on AMiner - Paper spends 13 hours , which is twice the time of preparing training data + training SelNet + estimating for 10 , 000 queries . Seeing SelNet â€™s scalability in estimation time ( Table 5 ) and training time ( Table 6 ) , we believe that the advantage will be more substantial on larger datasets . 8 CONCLUSION We tackled the selectivity estimation problem for high - dimensional data . Our method is based on learning monotonic query - dependent piece - wise linear function . This provides the flexibility of our model to approximate the selectivity curve while guaranteeing the con - sistency of estimation . We proposed a partitioning technique to cope with large - scale datasets and an incremental learning tech - nique for updates . Our experiments showed the superiority of the proposed model in accuracy across a variety of datasets , distance functions , and error metrics . The experiments also demonstrated the usefulness of our model in a semantic search application . Acknowledgements This work was supported by NSFC 62072311 and U2001212 , Guangdong Basic and Applied Basic Research Foun - dation 2019A1515111047 and 2020B1515120028 , Guangdong Peral River Recruitment Program of Talents 2019ZT08X603 , JSPS Kak - enhi 16H01722 , 17H06099 , 18H04093 , and 19K11979 , and ARC DPs 170103710 and 180103411 . REFERENCES [ 1 ] https : / / fasttext . cc / docs / en / english - vectors . html . [ 2 ] https : / / nlp . stanford . edu / projects / glove / . [ 3 ] http : / / www . cs . tau . ac . il / ~ wolf / ytfaces / index . html . [ 4 ] http : / / http : / / sites . skoltech . ru / compvision / noimi / . [ 5 ] http : / / http : / / corpus - texmex . irisa . fr / . [ 6 ] https : / / github . com / yyssl88 / SelNet - Estimation . [ 7 ] C . Anagnostopoulos and P . Triantafillou . Learning set cardinality in distance nearestneighbours . In ICDM , pages691 â€“ 696 , 2015 . [ 8 ] C . AnagnostopoulosandP . Triantafillou . Query - drivenlearningforpredictive analyticsofdatasubspacecardinality . ACMTrans . Knowl . Discov . Data , 11 ( 4 ) : 47 : 1 â€“ 47 : 46 , 2017 . [ 9 ] S . Arya , T . Malamatos , andD . M . Mount . Space - timetradeoffsforapproximate sphericalrangecounting . In SODA , pages535 â€“ 544 , 2005 . [ 10 ] M . AumÃ¼ller , E . Bernhardsson , and A . J . Faithfull . Ann - benchmarks : A bench - markingtoolforapproximatenearestneighboralgorithms . Inf . Syst . , 87 , 2020 . YaoshuWang 1 , ChuanXiao 2 , 3 , JianbinQin 1 , RuiMao 1 , MakotoOnizuka 2 , WeiWang 4 , 5 , RuiZhang 6 , andYoshiharuIshikawa 3 [ 11 ] M . M . Breunig , H . Kriegel , R . T . Ng , andJ . Sander . LOF : identifyingdensity - based localoutliers . In SIGMOD , pages93 â€“ 104 , 2000 . [ 12 ] M . S . Charikar . Similarityestimationtechniquesfromroundingalgorithms . In STOC , pages380 â€“ 388 , 2002 . [ 13 ] T . ChenandC . Guestrin . Xgboost : Ascalabletreeboostingsystem . In KDD , pages 785 â€“ 794 , 2016 . [ 14 ] G . Cormode , M . N . Garofalakis , P . J . Haas , andC . Jermaine . Synopsesformassive data : Samples , histograms , wavelets , sketches . FoundationsandTrendsinDatabases , 4 ( 1 - 3 ) : 1 â€“ 294 , 2012 . [ 15 ] H . DanielsandM . Velikova . Monotoneandpartiallymonotoneneuralnetworks . IEEETransactionsonNeuralNetworks , 21 ( 6 ) : 906 â€“ 917 , 2010 . [ 16 ] S . Das , P . S . G . C . , A . Doan , J . F . Naughton , G . Krishnan , R . Deep , E . Arcaute , V . Raghavendra , andY . Park . Falcon : Scalinguphands - offcrowdsourcedentity matchingtobuildcloudservices . In SIGMOD , pages1431 â€“ 1446 , 2017 . [ 17 ] M . M . Fard , K . Canini , A . Cotter , J . Pfeifer , andM . Gupta . Fastandflexiblemonotonic functionswithensemblesoflattices . In NIPS , pages2919 â€“ 2927 , 2016 . [ 18 ] J . Fox . Robust regression : Appendix to an r and s - plus companion to applied regression , 2002 . [ 19 ] E . GarciaandM . Gupta . Latticeregression . In NIPS , pages594 â€“ 602 , 2009 . [ 20 ] Y . Guo , L . Zhang , Y . Hu , X . He , andJ . Gao . MS - Celeb - 1M : Adatasetandbenchmark forlargescalefacerecognition . In ECCV , 2016 . [ 21 ] M . Gupta , A . Cotter , J . Pfeifer , K . Voevodski , K . Canini , A . Mangylov , W . Moczyd - lowski , andA . VanEsbroeck . Monotoniccalibratedinterpolatedlook - uptables . TheJournalofMachineLearningResearch , 17 ( 1 ) : 3790 â€“ 3836 , 2016 . [ 22 ] Q . Han , T . Wang , S . Chatterjee , andR . J . Samworth . Isotonicregressioningeneral dimensions . arXivpreprintarXiv : 1708 . 09468 , 2017 . [ 23 ] S . Hasan , S . Thirumuruganathan , J . Augustine , N . Koudas , and G . Das . Deep learningmodelsforselectivityestimationofmulti - attributequeries . In SIGMOD , pages1035 â€“ 1050 , 2020 . [ 24 ] M . Heimel , M . Kiefer , andV . Markl . Self - tuning , GPU - acceleratedkerneldensity modelsformultidimensionalselectivityestimation . In SIGMOD , pages1477 â€“ 1492 , 2015 . [ 25 ] P . J . Huberetal . Robustestimationofalocationparameter . Theannalsofmathe - maticalstatistics , 35 ( 1 ) : 73 â€“ 101 , 1964 . [ 26 ] Y . Ioannidis . Thehistoryofhistograms ( abridged ) . In VLDB , pages19 â€“ 30 , 2003 . [ 27 ] M . IzbickiandC . R . Shelton . Fastercovertrees . In ICML , pages1162 â€“ 1170 , 2015 . [ 28 ] H . JÃ©gou , tthijsDouze , andJ . Johnson . Facebookaisimilaritysearch ( faiss ) . https : / / github . com / facebookresearch / faiss . [ 29 ] A . Kipf , T . Kipf , B . Radke , V . Leis , P . A . Boncz , andA . Kemper . Learnedcardinalities : Estimatingcorrelatedjoinswithdeeplearning . In CIDR , 2019 . [ 30 ] A . Kipf , R . Marcus , A . vanRenen , M . Stoian , A . Kemper , T . Kraska , andT . Neumann . Radixspline : asingle - passlearnedindex . In aiDM @ SIGMOD , pages5 : 1 â€“ 5 : 5 , 2020 . [ 31 ] T . Kraska , A . Beutel , E . H . Chi , J . Dean , andN . Polyzotis . Thecaseforlearnedindex structures . In SIGMOD , pages489 â€“ 504 , 2018 . [ 32 ] S . LathuiliÃ¨re , P . Mesejo , X . Alameda - Pineda , andR . Horaud . Acomprehensive analysisofdeepregression . arXivpreprintarXiv : 1803 . 08450 , 2018 . [ 33 ] P . Li , H . Lu , Q . Zheng , L . Yang , and G . Pan . LISA : A learned index structure for spatialdata . In SIGMOD , pages2119 â€“ 2133 , 2020 . [ 34 ] W . Li , Y . Zhang , Y . Sun , W . Wang , M . Li , W . Zhang , and X . Lin . Approximate nearestneighborsearchonhighdimensionaldata - experiments , analyses , and improvement . IEEETrans . Knowl . DataEng . , 32 ( 8 ) : 1475 â€“ 1488 , 2020 . [ 35 ] Y . Li , J . Li , Y . Suhara , A . Doan , andW . - C . Tan . Deepentitymatchingwithpre - trained languagemodels . PVLDB , 14 ( 1 ) : 50 â€“ 60 , 2020 . [ 36 ] M . Mattig , T . Fober , C . Beilschmidt , andB . Seeger . Kernel - basedcardinalityesti - mationonmetricdata . In EDBT , pages349 â€“ 360 , 2018 . [ 37 ] M . Novelinkova . Comparisonofclenshaw - curtisandgaussquadrature . In WDS , volume11 , pages67 â€“ 71 , 2011 . [ 38 ] J . Ortiz , M . Balazinska , J . Gehrke , andS . S . Keerthi . Anempiricalanalysisofdeep learningforcardinalityestimation . CoRR , abs / 1905 . 06425 , 2019 . [ 39 ] Y . Park , S . Zhong , and B . Mozafari . Quicksel : Quick selectivity learning with mixturemodels . In SIGMOD , pages1017 â€“ 1033 , 2020 . [ 40 ] L . Prunty . Curvefittingwithsmoothfunctionsthatarepiecewise - linearinthe limit . Biometrics , pages857 â€“ 866 , 1983 . [ 41 ] N . ReimersandI . Gurevych . Sentence - BERT : Sentenceembeddingsusingsiamese BERT - networks . In EMNLP - IJCNLP , pages3980 â€“ 3990 , 2019 . [ 42 ] F . Schroff , D . Kalenichenko , andJ . Philbin . Facenet : Aunifiedembeddingforface recognitionandclustering . In CVPR , pages815 â€“ 823 , 2015 . [ 43 ] N . Shazeer , A . Mirhoseini , K . Maziarz , A . Davis , Q . Le , G . Hinton , and J . Dean . Outrageouslylargeneuralnetworks : Thesparsely - gatedmixture - of - expertslayer . arXivpreprintarXiv : 1701 . 06538 , 2017 . [ 44 ] J . Spouge , H . Wan , and W . Wilbur . Least squares isotonic regression in two dimensions . Journal of Optimization Theory and Applications , 117 ( 3 ) : 585 â€“ 605 , 2003 . [ 45 ] J . SunandG . Li . Anend - to - endlearning - basedcostestimator . PVLDB , 13 ( 3 ) : 307 â€“ 319 , 2019 . [ 46 ] Y . Sun , X . Wang , andX . Tang . Deepconvolutionalnetworkcascadeforfacialpoint detection . In CVPR , pages3476 â€“ 3483 , 2013 . [ 47 ] A . Toshev and C . Szegedy . Deeppose : Human pose estimation via deep neural networks . In CVPR , pages1653 â€“ 1660 , 2014 . [ 48 ] B . Walenz , S . Sintos , S . Roy , and J . Yang . Learning to sample : Counting with complexqueries . PVLDB , 13 ( 3 ) : 390 â€“ 402 , 2019 . [ 49 ] D . Wang , Y . Zhang , and Y . Zhao . Lightgbm : An effective mirna classification methodinbreastcancerpatients . In ICCBB , pages7 â€“ 11 , 2017 . [ 50 ] Y . Wang , C . Xiao , J . Qin , X . Cao , Y . Sun , W . Wang , andM . Onizuka . Monotoniccar - dinalityestimationofsimilarityselection : Adeeplearningapproach . In SIGMOD , pages1197 â€“ 1212 , 2020 . [ 51 ] A . Wehenkel and G . Louppe . Unconstrained monotonic neural networks . In NeurIPS , pages1543 â€“ 1553 , 2019 . [ 52 ] K . - Y . Whang , S . - W . Kim , and G . Wiederhold . Dynamic maintenance of data distributionforselectivityestimation . VLDBJ . , 3 ( 1 ) : 29 â€“ 51 , 1994 . [ 53 ] W . Wu , J . F . Naughton , andH . Singh . Sampling - basedqueryre - optimization . In SIGMOD , pages1721 â€“ 1736 , 2016 . [ 54 ] X . Wu , M . Charikar , andV . Natchu . Localdensityestimationinhighdimensions . In ICML , pages5293 â€“ 5301 , 2018 . [ 55 ] Y . Wu , D . Agrawal , andA . ElAbbadi . Queryestimationbyadaptivesampling . In ICDE , pages639 â€“ 648 , 2002 . [ 56 ] Z . Yang , E . Liang , A . Kamsetty , C . Wu , Y . Duan , P . Chen , P . Abbeel , J . M . Hellerstein , S . Krishnan , and I . Stoica . Deep unsupervised cardinality estimation . PVLDB , 13 ( 3 ) : 279 â€“ 292 , 2019 . [ 57 ] S . You , D . Ding , K . Canini , J . Pfeifer , and M . Gupta . Deep lattice networks and partialmonotonicfunctions . In NIPS , pages2981 â€“ 2989 , 2017 . [ 58 ] D . ZhangandZ . Yang . Wordembeddingperturbationforsentenceclassification . arXivpreprintarXiv : 1804 . 08166 , 2018 . ConsistentandFlexibleSelectivityEstimationforHigh - DimensionalData APPENDIX A PROOF Lemma 1 . Proof . Assume ğ‘¡ âˆˆ [ ğœ ğ‘– âˆ’ 1 , ğœ ğ‘– ) , then ğ‘¡ + ğœ– is in [ ğœ ğ‘– âˆ’ 1 , ğœ ğ‘– ) or [ ğœ ğ‘– , ğœ ğ‘– + 1 ) . In the first case , Ë† ğ‘“ ( x , ğ‘¡ + ğœ– , D ; Î˜ ) âˆ’ Ë† ğ‘“ ( x , ğ‘¡ , D ; Î˜ ) = ğœ– ğœ ğ‘– âˆ’ ğœ ğ‘– âˆ’ 1 Â· ( ğ‘ ğ‘– âˆ’ ğ‘ ğ‘– âˆ’ 1 ) â‰¥ 0 . In the second case , Ë† ğ‘“ ( x , ğ‘¡ , D ; Î˜ ) â‰¤ ğ‘ ğ‘– and Ë† ğ‘“ ( x , ğ‘¡ + ğœ– , D ; Î˜ ) â‰¥ ğ‘ ğ‘– . Therefore , Ë† ğ‘“ ( x , ğ‘¡ , D ; Î˜ ) is non - decreasing in ğ‘¡ . â–¡ B EXPERIMENT SETUP B . 1 Model Settings Hyperparameter and training settings are given below . â€¢ IS and KDE : The sample size is 2000 . â€¢ QR - 1 and QR - 2 : The number of query prototypes is 2000 . â€¢ LightGBM : The number of CARTs is 1000 . â€¢ DNN is a vanilla FFN with four hidden layers of sizes 512 , 512 , 512 , and 256 . â€¢ MoE consists of 30 expert models , each an FFN with three hidden layers of sizes 512 , 512 , and 512 . We used top - 3 experts for the prediction . â€¢ RMI has three levels , with 1 , 4 , and 8 models , respectively . Each model is an FFN with four hidden layers with sizes 512 , 512 , 512 , and 256 . â€¢ DLN is an architecture of six layers : calibrators , linear embedding , calibrators , ensemble of lattices , calibrators , and linear embed - ding . â€¢ UMNN is an FFN with four hidden layers of sizes 512 , 512 , 512 and 256 to implement the derivative . ğœ•ğ‘“ ( x , ğ‘¡ , D ) ğœ•ğ‘¡ . ğ‘“ ( x , ğ‘¡ , D ) is com - puted by Clenshaw - Curtis quadrature with learned derivatives . â€¢ SelNet : We use an FFN with two hidden layers to estimate ğœ , and an FFN in Equation 9 with four hidden layers to estimate p . The encoder and decoder of AE are implemented with an FFN with three hidden layers . For MS - Celeb and YouTube , the sizes of the first three ( or two , if it only has two ) hidden layers of the three FFNs are 512 , and the sizes of all the other hidden layers are 256 . For fastText , GloVe , DEEP , and SIFT , the sizes of the first hidden layer of the these FFNs are 1024 , and the others remain the same as above . The number of control parameters ğ¿ is 50 . The default partition size ğ¾ is 3 . ğ‘¡ max is 54 for Euclidean distance . For cosine similarity , we equivalently convert it to Euclidean distance on unit vectors , and set ğ‘¡ max = 1 . The learning rates of MS - Celeb , fastText , YouTube , GloVe , DEEP , and SIFT are 0 . 00003 , 0 . 00002 , 0 . 00003 , 0 . 0001 , 0 . 0001 , and 0 . 0001 , respectively . | h ğ‘– | ( 0 â‰¤ ğ‘– â‰¤ ğ¿ + 1 ) in model ğ‘€ is 100 . The batch size is 512 for all the datasets . We train all the models in 1500 epochs and select the ones with the smallest validation error . For training with data partitioning , we use ğ‘‡ = 300 and ğ›½ = 0 . 1 . ğ›¿ ğ‘ˆ for incremental learning is 20 . For the learning models , we train them with the same Huber loss over the logarithms of the ground truth and the predicted value . All the hyper - parameters are fine - tuned to minimize the validation error . DNN , MoE and RMI cannot directly handle the threshold ğ‘¡ . We learn a non - linear transformation of ğ‘¡ into an ğ‘š - dimensional embedding vector , i . e . , t = ReLU ( w ğ‘¡ ) . Then we concatenate it with x as the input to these models . B . 2 Evaluation Metrics We evaluate Mean Squared Error ( MSE ) , Mean Absolute Error ( MAE ) , and Mean Absolute Percentage Error ( MAPE ) . They are defined as : MSE = 1 ğ‘š ğ‘š âˆ‘ï¸ ğ‘– = 1 ( Ë† ğ‘¦ ğ‘– âˆ’ ğ‘¦ ğ‘– ) 2 , MAE = 1 ğ‘š ğ‘š âˆ‘ï¸ ğ‘– = 1 | Ë† ğ‘¦ ğ‘– âˆ’ ğ‘¦ ğ‘– | , MAPE = 1 ğ‘š ğ‘š âˆ‘ï¸ ğ‘– = 1 (cid:12)(cid:12)(cid:12)(cid:12) Ë† ğ‘¦ ğ‘– âˆ’ ğ‘¦ ğ‘– ğ‘¦ ğ‘– (cid:12)(cid:12)(cid:12)(cid:12) , where ğ‘¦ ğ‘– is the ground truth value and Ë† ğ‘¦ ğ‘– is the estimated value .