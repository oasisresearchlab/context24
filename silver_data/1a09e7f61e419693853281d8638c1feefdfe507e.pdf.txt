See discussions , stats , and author profiles for this publication at : http : / / www . researchgate . net / publication / 232593680 Divergent thinking . Creativity research . ARTICLE READS 574 1 AUTHOR : Mark A . Runco University of Georgia 146 PUBLICATIONS 3 , 537 CITATIONS SEE PROFILE Available from : Mark A . Runco Retrieved on : 10 October 2015 HCRJ # 652929 , VOL 24 , ISS 1 Divergent Thinking as an Indicator of Creative Potential Mark A . Runco and Selcuk Acar QUERY SHEET This page lists questions we have about your paper . The numbers displayed at left can be found in the text of the paper for reference . In addition , please review your paper as a whole for correctness . Q1 : Au : Meeker 1985 not in References . Should it be Meeker et al . ? Q2 : Au : Cheung 2005 not in References . Q3 : Au : Runco & Mraz 1994 not in References ( check year ) . Q4 : Au : Basadur & Runco 1993 not in References . Q5 : Au : Reuters et al . not in References . Q6 : Au : Eysenck 1997 not cited in text . Q7 : Au : Manske & Davis 1968 not cited in text . Q8 : Au : Please update Runco & Acar in press if this has been published . Q9 : Au : Runco & Mraz 1992 not in References Q10 : Au : Please update Runco et al . 2010 if this has been published or accepted . Q11 : Au : Torrance 1995 not cited in text . Q12 : Au : Please add pg # for Wallach 1970 . Q13 : Au : Is DOK the full publisher for Williams , or should it be spelled out ? Q14 : Au : Please add ﬁnal pg # for Zarnegar et al . 1988 . Q15 : Au : Please provide address for correspondence . TABLE OF CONTENTS LISTING The table of contents for the journal will list your paper exactly as it appears below : Divergent Thinking as an Indicator of Creative Potential Mark A . Runco and Selcuk Acar Divergent Thinking as an Indicator of Creative Potential Mark A . Runco and Selcuk Acar University of Georgia , Athens Divergent thinking ( DT ) tests are very often used in creativity studies . Certainly DT 5 does not guarantee actual creative achievement , but tests of DT are reliable and reason - ably valid predictors of certain performance criteria . The validity of DT is described as reasonable because validity is not an all - or - nothing attribute , but is , instead , a matter of degree . Also , validity only makes sense relative to particular criteria . The criteria strongly associated with DT are detailed in this article . It also summarizes the uses 10 and limitations of DT , conceptually and psychometrically . After the psychometric evidence is reviewed , alternative tests and scoring procedures are described , including several that have only recently been published . Throughout this article related processes , such as problem ﬁnding and evaluative thinking , are linked to DT . The concept of divergent thinking is attractive for a 15 number of reasons . It is , for example , a good metaphor of the kind of cognition that should lead to original ideas . Divergent thinking is easy to contrast with conver - gent thinking , which typically leads to conventional and ‘‘correct’’ ideas and solutions rather than original 20 options . Yet divergent thinking , or what Guilford ( 1950 , 1968 ) called divergent production , is more than a metaphor . In fact , one reason the concept is so attractive is that it leads directly to testable hypotheses and allows reliable assessment of the potential for creative thought . 25 The key idea is that of potential . Divergent thinking is not the same as creative thinking . Divergent thinking often leads to originality , and originality is the central feature of creativity , but someone can do well on a test of divergent thinking and never actually perform in a 30 creative fashion . This argument parallels a theme of this Special Issue of the Creativity Research Journal , though at ﬁrst blush that may not be entirely obvious . The theme involves indicators of creative talent . What exactly is an indicator ? It is a kind of predictor ; and 35 any time a prediction is made , there is some uncertainty . Even someone who has always performed at a high level in the past may slip up in the future , especially if circum - stances change . On a more macro level , an organization or cultural group which has maintained a high level of 40 innovation in the past may miss the boat in the future . Pre - dictions are estimates because the future is always unknown . Indicators are best guesses . They are predictors . Good indicators are reliable and accurate , but there is always some uncertainty . Similarly , when looking at or 45 measuring individuals , it is possible to determine the degree of potential , but that potential may or may not be fulﬁlled . There is the same kind of uncertainty with esti - mates of potential , such as a test of divergent thinking , as there is with an indicator . Psychometric research suggests 50 that tests of divergent thinking provide useful estimates of the potential for creative thinking . They are good indi - cators of future creative performance . They are far from perfect , but ( a ) no test is perfect , and ( b ) the degree of accu - racy can be objectively determined . 55 This article brieﬂy reviews the research of relevance to the accuracy of divergent thinking tests . Much of it focuses on reliability or validity , the two primary psychometric cri - teria of accuracy and usefulness , but some focuses on the procedures for insuring that divergent thinking test are 60 administered and scored in the best possible fashion . Newer tests and scoring procedures are described and the conclusion points to future research considerations . THE MEASUREMENT OF CREATIVITY Hocevar ( 1981 ) identiﬁed four types of creativity 65 tests : divergent thinking tests , attitude and interest inventories , personality inventories , and biographical Correspondence should be sent to Mark A . Runco . E - mail : runco @ uga . edu Q15 CREATIVITY RESEARCH JOURNAL , 24 ( 1 ) , 1 – 10 , 2012 Copyright # Taylor & Francis Group , LLC ISSN : 1040 - 0419 print = 1532 - 6934 online DOI : 10 . 1080 / 10400419 . 2012 . 652929 3b2 Version Number : 7 . 51c / W ( Jun 11 2001 ) File path : p : / Santype / Journals / TandF _ Production / Hcrj / v24n1 / HCRJ652929 / hcrj652929 . 3d Date and Time : 23 / 01 / 12 and 18 : 37 inventories . Each offers useful information , but tests of divergent thinking have dominated the ﬁeld of creativity assessment for several decades . This has created one 70 problem , namely that occasionally they are regarded as tests of creativity . As noted , that is not a tenable view . The general idea of divergence of thought can be found in theories going back to the 19th century ( e . g . , Binet & Simon , 1905 ) , but it was J . P . Guilford ( 1950 , 75 1968 ) who clearly tied divergent production to creative potential . He hypothesized several components for divergent thinking ( Guilford & Hoepfner , 1971 ) but empirical evidences supported some more than the others . Most tests of DT now look only to ﬂuency , 80 originality , ﬂexibility , and elaboration . In fact , too often only ﬂuency is used . That is deﬁned in terms of pro - ductivity . A ﬂuent individual gives a large number of ideas . Originality is usually deﬁned in terms of novelty or statistical infrequency . Flexibility leads to diverse 85 ideas that use a variety of conceptual categories . Elabor - ation , the least common , is suggested when the individ - ual follows an associative pathway for some distance . For most purposes , ﬂuency should not be used alone . There is unique and reliable variance in the other indices 90 ( Runco & Albert , 1985 ) and ﬂuency is not as closely tied to creativity as is originality and ﬂexibility . More on this is presented in the following . Guilford ( 1950 , 1968 ) developed a large number of tasks , as did E . Paul Torrance . In fact , the Torrance 95 Tests of Creative Thinking ( TTCT ) are still enormously popular . Other batteries were developed by Wallach and Kogan ( 1965 ) and Williams ( 1980 ) . Meeker , Meeker , and Roid ( 1985 ) took up where Guilford left off and revised and extended his efforts . Later , Urban and Jellen 100 ( 1996 ) developed the Test of Creative Thinking ( Divergent Production ; TCT - DP ) , and most recently Auzmendi , Villa , and Abedi ( 1996 ) developed a multiple choice test to assess DT . Which is the best test ? One way to address that ques - 105 tion is to consider which is the most reliable . Reliability is a good place to start any discussion of tests because it is a prerequisite for validity . An unreliable test cannot be valid . Reliability of DT Tests 110 Reliability is indicative of the stability or consistency of a test result or score . There are different kinds of reliability , including interitem , interrater , and alterna - tive forms reliability . The second of these is most appro - priate if there is any subjectivity in the scoring system . 115 Often there is very little subjectivity . The number of ideas produced is determined by simply counting ideas and gives a ﬂuency score ; the originality of ideas is deter - mined by compiling ideas within a sample and identify - ing which are given infrequently ( those receive points for 120 originality ) ; and so on . But sometimes it is useful to ask judges to examine the ideas . When this is the case , inter - judge agreement must be checked . Fortunately , judges can be objective and give reliable ratings . Meeker ( 1985 ) Q1 reported an interrater reliability coefﬁcient of 125 well above . 90 in her work with Structure of the Intellect Learning Abilities Tests , for example . Urban and Jellen ( 1996 ) reported . 90 , and Wallach and Kogan ( 1965 ) . 92 . This is not to say that there are no concerns and ques - tions . There is some question about how much infor - 130 mation to provide to judges ( Runco , 1989 ) , for instance , and another question about how to select the best judges ( Murray , 1959 ; Runco , McCarthy , & Svensen , 1994 ) . Another kind of reliability involves interitem agree - 135 ment or internal consistency . In one review , these ranged from . 42 to . 97 ( Cropley , 2000 ) , but fortunately most were above . 70 . Still , at one point reliability analyses brought the usefulness of originality scores into question ( Hocevar , 1979a , 1979b ) . The problem was that orig - 140 inality scores were unreliable when ﬂuency scores were controlled and the conclusion was that ﬂuency contami - nated originality ( Clark & Mirels , 1970 ; Hocevar & Michael , 1979 ) . Runco and Albert ( 1985 ) determined that this is not always the case and that originality is 145 reliable , even after ﬂuency is statistically controlled , for some tests and for some samples , especially those at higher levels of ability . Validity There are various kinds of validity . The question implied 150 by each is , ‘‘Are you testing what you think you are test - ing ? ’’ Validity is in this sense the opposite of bias . A biased test is inﬂuenced by things outside of the target . A test may be verbally biased , for example , in which case linguistic skills determine performances more than 155 creative talents . Highly verbal examinees tend to do well , regardless of creative ability , and examinees with low verbal skills tend to do poorly , again regardless of their creative abilities . There are other potential biases , including SES and experience ( Runco , 1991 ; Runco & 160 Acar , in press ) . Discriminant validity is of especially importance for creativity because it is in some ways related to general intelligence . This is suggested by threshold theory , which posits that some level of general intelligence is necessary 165 but not sufﬁcient for creative work ( MacKinnon , 1965 ; Torrance , 1966 ; Yamamato , 1965 ) . Yet at moderate and high levels , evidence conﬁrms a separation . This supports the discriminant validity of divergent thinking tests . 170 Wallach and Kogan ( 1965 ) reported that test setting and instructions had an impact on the level of discrimi - nant validity such that , with test - like directions , it is low , 2 RUNCO AND ACER but with game - like instructions for DT tests , the separ - ation from intelligence was quite clear . Wallach ( 1970 ) 175 later attributed the low correlation between creativity and intelligence above certain intelligence level to the restricted distribution of IQ scores ( also see Crockenberg , 1972 ) . Without a doubt , the various DT indexes are dif - ferentially related to general intelligence . Torrance 180 ( 1969 ) was especially concerned about the elaboration index from the TTCT , but some of the problem with it may have been the difﬁculty in scoring the tests . Elabor - ation is difﬁcult for untrained raters to scorer ( Cramond , Matthews - Morgan , Bandalos , & Zou , 2005 ) . 185 The discriminant validity of creativity tests becomes more complicated by the recognition that both divergent and convergent thinking may work together in creative thinking and problem solving ( Runco , 2007 ) . They have been described as representing two ends of a continuum 190 ( Eysenck , 2003 ) and , alternatively , as two necessary phases in creative problem solving . In fact , several evol - utionary models have described the creative process in two phases , with blind variation responsible for the gen - eration alternative and selective retention based on 195 evaluation of ideas based on their appropriateness and usefulness ( Campbell , 1960 ) . This would seem to sup - port a divergent - convergent process . Basadur’s ( 1995 ) ideation - evaluation model described something very much like this , with ideation and evaluation occurring 200 one after another , but also with problem ﬁnding , prob - lem solving , and solution implementation stages . For Basadur , the importance of ideation and evaluation depends both on the stage and the task . Ideation is employed more in problem ﬁnding and tasks such as 205 basic research and development ; evaluation features more in solution implementation and applied tasks , such as distribution and sales in an organizational structure . Brophy ( 1998 ) also argued that creative problem solving requires alternating from divergent to convergent think - 210 ing at the right time , but he felt that only a minority can do both with a good balance . Moneta ( 1994 ) also hypothesized an optimal balance between the two . Actu - ally , these ideas are not far aﬁeld from the original work on DT . Guilford ( 1968 ) , for example , pointed to evalu - 215 ation allowed by conceptual foresight , penetration rede - ﬁnition , and problem sensitivity ( Mumford , 2000 – 2001 ) . Hence , although discriminant validity is a good thing , it is likely that a realistic view of creative problem will recognize that DT does not work in isolation ( Runco , 220 1994 ) . Predictive validity is based on the strength of relation - ships between DT tests and various criteria . The trick is ﬁnding a good criterion . Kogan and Pankove ( 1974 ) used extracurricular activities and accomplishments as 225 criteria and the Wallach and Kogan tests of creativity as predictors . They found poor predictions for 5th grade students and only slightly better predictions for 10th grade students . Wallach and Wing ( 1969 ) and Runco ( 1986 ) reported much better predictive validity for the 230 same tests . Runco took interactions among indexes into account ( e . g . , originality (cid:1) ﬂuency ) . Incidentally , after reviewing the research on creativity tests , Hocevar ( 1981 ) concluded that self - reported creative activities and achievements are the most defensible tests for the 235 identiﬁcation of creative talent despite the problems such as determining which activities to label as creative . Crockenberg ( 1972 ) , on the other hand , criticized the same tests because they include items representing char - acteristics of well - educated high - IQ people . 240 Runco , Plucker , and Lim ( 2000 – 2001 ) suggested that previous assessments of the predictive validity of DT tests were ill - conceived . The problem , they argued , was the cri - teria used previously were often inappropriate for DT . Certainly the criteria in previous studies were related to 245 creativity and quite informative . Torrance ( 1981 ) , for example , looked at ( a ) number of high school creative achievements , ( b ) the number of post high school achieve - ments , ( c ) number of creative style - of - life achievements , ( d ) quality of highest creative achievements , and ( e ) crea - 250 tiveness of future career image as criteria . Still , Runco et al . proposed that better predictive validities would be found if the criteria relied solely on ideation and not on opportunities and extracognitive factors . They examined the predictive validity of DT using the Runco Ideational 255 Behavior Scale , which is a self - report that only asks about ideation . Correlations between it and various DT tests were reassuring . Runco et al . also reported nonsigniﬁcant correlations with GPA , which is yet more evidence for discriminant validity . 260 Tests , Scores , and Scoring Just as new criteria have been developed , so too have new tests been developed . Several new tests were cited pre - viously ( e . g . , Urban & Jellen , 1996 ) . The older batteries ( e . g . , Guilford’s , the TTCT , Wallach & Kogan’s ) are still 265 employed , and some have the advantage precisely because they have been used so many times . There are norms and extensive research that can be consulted for interpreta - tions of scores and results . Yet sometimes there is a need to adapt a test for a speciﬁc purpose or sample . Runco , 270 Dow , and Smith ( 2006 ) did exactly this to test the role of memory and experience in DT . They developed open - ended tests that , like DT tests , allow numerous responses , but also required that the examinee rely on experience . Examinees ( from Orange County , California ) 275 were asked for street names , for example , as well as car part , plant names , and book titles . Analyses indicated that scores from the experience - based open - ended tests were correlated with scores on the more standard DT , and , in fact , Runco and Acar ( in press ) concluded that there is 280 an experiential bias in many DT tests . ESTIMATING CREATIVE POTENTIAL 3 Abedi ( 2002 ; Auzmendi et al . , 1996 ) developed another new test , called the Abedi - Schumacher Creativ - ity Test . It was designed to assess DT without adminis - tering open - ended questions . It is , in fact , a 285 multiple - choice test , but the response options are indica - tive of tendencies toward ﬂuency , originality , ﬂexibility , and so on . Abedi justiﬁed this in that the new test requires very little time to administer . It contains 60 questions , but several multiple choice items can be com - 290 pleted by an examinee per minute , so it is indeed a quick test . Of course , it is not really a test of DT , or at least not a skill or performance test of DT . Another quick test was developed by Cheung ( 2005 ) Q2 . This is an electronic version of Wallach and Kogan DT tests ( e - WKCT ) . 295 Cheung , Lau , Chan , and Wu ( 2004 ) have collected enough data to provide norms for Chinese samples . The electronic format allows virtually instantaneous scoring and comparison of individual scores to norms . Of more signiﬁcance than the new tests are the alterna - 300 tive scoring systems for DT tasks . That is because there are so many issues with DT indicators . The scoring used by Torrance ( 1966 , 1972 , 1980 ) , with four indexes—ﬂu - ency , originality , ﬂexibility , and elaboration—is still in use in the current version of TTCT . However , those 305 indexes were highly intercorrelated and using all of those indexes is laborious . Therefore , Wallach and Kogan ( 1965 ) suggested using only two indexes , namely , unique - ness and ﬂuency . This is still a time - consuming method , and interrater reliability must be determined because judg - 310 ments must be made some of the time to know whether or not an idea is in fact unique . Several researchers have suggested that a quality score should be used . Cropley ( 1967 ) , for example , sug - gested scoring by giving 4 points to ideas that given by 315 less than 1 % of the population , 3 points for those pro - duced by 1 – 2 % , 2 for 3 – 6 % , and 1 for 7 – 15 % , and 0 for the remainder . Harrington , Block , and Block ( 1983 ) added quality scoring to the novelty and quan - tity , and they found that presence of the quality scoring 320 resulted in an increment in the construct validity of DT scores . Quality is also implied by the appropriateness scores used by Runco , Illies , and Eisenman ( 2005 ) and Runco and Charles ( 1993 ) at least in the sense that both recognize the creative things are more than just original . 325 They are also effective , appropriate , or in some way ﬁt - ting . Ideas that are merely original can be of low quality and not effective . Vernon ( 1971 ) suggested basing originality on the responses from randomly selected group of a hundred 330 people . Responses that are given by only 5 or fewer people are given 2 points , and 0 points are given for ideas produced by 15 or more . Those in between were given a 1 . Hocevar and Michael ( 1979 ) , Vernon ( 1971 ) , Runco , 335 Okuda , and Thurston , ( 1987 ) , and Mansk and Davis ( 1968 ) all suggested that a proportional score could be used . This could be calculated by dividing originality ( or ﬂexiblity , for that matter ) by ﬂuency . It would , thereby , eliminate any confounding by ﬂuency 340 ( Hocevar , 1979b ) but would mask an examinee’s pro - ductivity . Someone who produced two ideas , one of which was unique , would have the same score as someone who produced 10 ideas , 5 of which were unique . Additionally , ratios and proportions are often unreliable . 345 Another alternative is to simply add together the various indexes of DT . This is probably the least tenable option , with the possible exception of using ﬂuency alone . A simple summation assumes that each index is on the same scale , for example , and without standardi - 350 zation the index with the highest mean would contribute more than the other indexes to the sum . At least as important is that it is contrary to theories of DT that describe each index as a unique aspect of DT . A confounding by ﬂuency is a serious consideration . 355 Such confounding is implied by bivariate correlations among the various DT indexes but was more dramati - cally demonstrated by Hocevar’s ( 1979b ) analyses of the reliability of originality scores from various divergent thinking tests ( alternate uses , plot titles , and 360 consequences ) . He found that , although reliable before adjustments , when the variance accounted for by ﬂuency was removed from originality , its reliability was quite low . Hocevar ( 1979a ) suggested dividing ideas into two 365 categories : original or unoriginal . This would help if there is , in fact , overlap and redundancy among indices . Milgram , Milgram , Rosenbloom , and Rabkin ( 1978 ) and Moran , Milgram , Sawyers , and Fu ( 1983 ) had very good luck with this kind of scoring , though they labeled 370 their DT ( nonoverlapping ) indexes popular and unusual . In this scoring , an idea can be either original or unoriginal ( and ﬂuent ) . Mouchiroud and Lubart ( 2001 ) investigated the use of median weights because median scores are less sensi - 375 tive to mean scores , and this could diminish the inﬂu - ence of ﬂuency scores . Five different originality indexes were deﬁned for the TTCT . Three of them were based on the norms reported the TTCT manual , sample - based frequencies ( 2 points for ideas given by 380 less than 2 % and 1 point for those given between 2 – 5 % ) , and sample - based uniqueness . Two others indexes were based Runco et al . ( 1987 ) ’s weighting sys - tem . No differences were found between norm - based and sample - based originality indices . 385 Hocevar ( 1979a ) compared subjective ratings of orig - inality , statistical infrequency scores for originality , and a random method ( rating each responses from 0 to 9 in a random number table ) . Each produced surprisingly adequate levels of interitem reliability . The subjective 390 ratings were far from perfectly correlated with the 4 RUNCO AND ACER objective method that uses statistical infrequency . The subjective scores seemed to have higher discriminant validity than the objective scores . Seddon ( 1983 ) and Snyder , Mitchell , Bossomaier , 395 and Pallier ( 2004 ) felt that it should be possible to treat DT as a single compound . Seddon ( 1983 ) proposed a method where originality was used as a weight given to the each response and category given , respectively . Snyder et al . ( 2004 ) employed a logarithmic function 400 to create a single score , but they combined ﬂuency and ﬂexibility indices rather than originality . Runco and Mraz ( 1994 ) Q3 questioned a basic premise of previous scoring systems . They suggested that a more practical method is to ask judges to rate each examinee’s 405 total ideational output , rather than basing scores on individual ideas . That would provide judges with much more information—they would see everything any one examinee produced rather than just one idea at a time—and should save a great deal of time as well . Their 410 results , and those of Charles and Runco ( 2000 – 2001 ) , conﬁrmed that this method , using ideational pools , works quite well . Runco et al . ( 1987 ) compared ( a ) the summation scores ( ﬂexibility , ﬂuency , and originality added 415 together ) , ( b ) common and uncommon ( the latter ideas produced by less than 5 % of the sample ) , ( c ) ratio scores ( originality or ﬂexibility divided by ﬂuency ) , and ( d ) weighted ﬂuency scores ( where weights are determined based on rarity of ideas , with less frequent ideas given 420 more weight ) . They concluded that the weighted system was preferable . Runco ( 1986 ) offered an alternative speciﬁcally for scoring ﬂexibility . The basic idea was to look at the number of changes , from ideational category to 425 category . This differs from ﬂexibility deﬁned as the total number of categories . Someone might change from cate - gory to category , but only use a few categories , or the person might only use a single category once but cover quite a few of them . Each would seem to relate to ﬂexi - 430 bility but they represent different cognitive processes . Apparently the reliability of DT scores can vary in different populations . Runco and Albert ( 1985 ) discov - ered that originality may depend on ﬂuency in nongifted populations , for example , but among the gifted there is 435 separation and originality supplies information that is not contained in ﬂuency . Much the same can be said about ﬂexibility ( Runco , 1985 ) . Clark and Mirels ( 1970 ) proposed that tests should be administered such that participants are limited to a cer - 440 tain number of ideas . Fluency would be irrelevant , then . Michael and Wright ( 1987 ) limited the number of responses to the three most creative ones , and Zarnegar , Hocevar , and Michael ( 1987 ) limited their sample to one single unique idea . These methods do eliminate the con - 445 cern over confounding by ﬂuency , and they may be realistic in that they require examinees to not only gen - erate ideas but also to select creative or at least unique ones—and that combination ( idea generation and evalu - ation ) is probably consistent with creative thinking in 450 the natural environment . Then again , limiting examinees to three or one ideas ignores theories ( e . g . , Mednick , 1962 ) and research ( Milgram et al . , 1978 ; Runco , 1986 ) that suggests that time is necessary for ﬁnding cre - ative ideas . If only a few ideas are allowed , the individ - 455 ual might not deplete common ideas and ﬁnd remote associates . Mumford , Marks , Connelly , Zaccaro , and Johnson ( 1998 ) suggested a domain - speciﬁc scoring method . They developed a speciﬁc scoring method for the 460 domain of leadership and therefore deﬁned scores for ‘‘time frame’’ ( decisions that bring immediate or long - term consequences ) , quality , realism , originality , complexity , use of principles , positive outcomes , and negative outcomes ( also see Vincent , Decker , & 465 Mumford , 2002 ) . Evaluative Skills Much of the research on DT has gone outside the test and focused , not just on how to make a better test of DT , but instead on what may interact with or even sup - 470 port DT . This makes a great deal of sense because cre - ative thinking is such a complex process . DT plays a role but judgment and evaluation , and perhaps even convergent thinking , are required as well ( Cropley , 2006 ; Runco , 1994 ) . DT alone would lead to wild ideas , 475 some of which might be creative , but many would likely be unrealistic , useless , and irrelevant . Indeed , Eysenck ( 2003 ) found psychotic individuals to be highly original , but not creative . The role of evaluation in creative thought and the 480 relationship with DT has been examined in various ways . Khandwalla ( 1993 ) , for instance , asked his parti - cipants to think aloud while trying to solve various problem ( i . e . , ‘‘list objects that are green , liquid , and funny’’ ) . They were encouraged to say whatever came 485 to mind . Participants’ protocols were recorded and then analyzed in an attempt to infer what speciﬁc processes lead to an original idea . Khandwalla identiﬁed ﬁve broad categories of divergent thinking from the proto - cols . He called these problem structuring , searching , 490 feeling , ideating , and evaluating . Ideating was the most common process , followed by evaluating . Interestingly , an associative path involving both ideating and evaluat - ing paths was the most frequent of the ﬁve paths ident - iﬁed by Khandwalla . He concluded that ‘‘evaluating and 495 ideating form a synergistic rather than an antagonistic combination in complex tasks’’ ( p . 253 ) . Inaccurate evaluations make it difﬁcult to think cre - atively . Inaccurate evaluations may turn the individual ESTIMATING CREATIVE POTENTIAL 5 away from a potentially fruitful associative path , for 500 example , or simply curtain ideation . This is premature closure ( Basadur & Runco , 1993 ) Q4 . One function of evaluation is , then , to insure that new ideas can be developed and will be explored . Other functions were described by Dailey and Mumford ( 2006 ) . For them , 505 evaluative skills allow people to manage their time and cognition effectively because trivial solutions or ideas are discarded and more promising ideas are focused and improved . Another function appears in the improvement of the ideas . Ideators can take context , conditions , con - 510 straints , resources , and standards into account to improve ideas and ease the implementation of them . Lonergan , Scott , and Mumford ( 2004 ) investigated the role of stan - dards used when judging ideas . They asked their parti - cipants to evaluate or suggest revisions to the ideas 515 developed for a new product . Different standards were applied during either evaluating ideas or when suggesting new revisions to them . Highly original ideas were more accurately judged in well - structured tasks . Mumford , Blair , Dailey , Leritz , and Osborn ( 2006 ) 520 identiﬁed more than 35 evaluative errors . These included isolation , sufﬁcing , discounting anomalies , premature reservation of ideas , and optimism . Licuanan , Dailey , and Mumford ( 2007 ) detected the errors in evaluation of ideas in cases of high versus low originality , high ver - 525 sus low complexity , employment of active processing , and creativity framing . Intriguingly , people tended to make more errors in their evaluations of highly original ideas by underestimating their merit . Also , as the com - plexity of the setting where the idea is developed 530 increased , errors in appraisal of creative ideas also increased . Further analyses provided evidence that people tend to underscore the originality of the ideas when they focus on performance under the complex conditions . Mumford et al . used these ﬁndings to sug - 535 gest methods for improving the accuracy of the evalua - tions . They showed that creativity framing ( training about the value of creativity in team settings and some variables involving interactional processes which improve creativity in team settings ) and active proces - 540 sing ( having the participants elaborate on the originality of the ideas deeply ) can improve the accuracy of the evaluation in highly original ideas . Evaluations are also involved when there is a need to predict the necessary resources ( e . g . , time , money ) and 545 outcomes of ideas when implemented . Dailey and Mumford ( 2006 ) proposed that implementation - based evaluation can decrease errors in estimating the resources needed for the new ideas and their outcomes . They found that people tended to underestimate the resources needed 550 and overestimate positive outcomes of idea implemen - tation . Also , personal characteristics ( e . g . , neuroticism ) inﬂuenced the accuracy of evaluations . Domain famili - arity decreases the accuracy of evaluations . Blair and Mumford ( 2007 ) focused their research on 555 the attributes of ideas that are taken into account when evaluating . They found that people favored ideas that were consistent with the current social norms , led expected outcomes , and were complex to execute but easy to understand . They rejected the ideas that were 560 original and risky as well those had detailed descrip - tions , probably because these were too obvious and sim - ple . Under the time pressure , people preferred ideas that were consistent with social norms and required time , in addition to those that had detailed descriptions and 565 were original . Original and risky ideas were also pre - ferred when nonstringent evaluation criteria were emphasized and when there was limited time available . Participants who expected that their evaluations were to be compared to others tended to prefer the ideas that 570 were more extreme . Runco and Smith ( 1992 ) compared inter - and intra - personal evaluations of ideas . Not surprisingly , these two kinds of evaluations were related to one another , so someone who is accurate at judging his or her own 575 ideas is also accurate when judging ideas given by others . The more surprising ﬁnding was that intraperso - nal evaluations were signiﬁcantly correlated with diver - gent thinking scores while interpersonal evaluations were not . The interpersonal evaluations , on the other 580 hand , were correlated with preference for ideation . Interestingly , participants were more accurate when judging the uniqueness than the popularity of their own ideas ( popularity being inversely related to orig - inality ) but were more accurate when judging the popu - 585 larity than the uniqueness of ideas given by others . Basadur , Wakabayashi , and Graen ( 1990 ) deﬁned evaluation as a kind of thinking style . They then exam - ined style as a moderator of the impact of training . Four styles of creative problem solving were compared : gener - 590 ator ( experience and ideation ) , conceptualizer ( thinking and ideation ) , implementor ( experience and evaluation ) , and optimizer ( ideation and evaluation ) . Analyses indi - cated that the optimizers gained signiﬁcantly more from the training than the other three styles . 595 Runco and Basadur ( 1993 ) also examined the beneﬁts of training but they actually focused the instruction o the evaluations of ideas ( not just on DT or ideation in general ) . Managers who received this training improved in the sense of increasing correct choices and decreasing 600 incorrect choices . They also examined the relationship between ideational and evaluative skills and found that the latter were most strongly related originality ( 58 % of common variance ) than with ﬂuency ( 13 % of com - mon variance ) . There was a higher correlation between 605 the posttreatment evaluative skills and ideational skills than the pretreatment counterparts . The strongest association when style was analyzed was with the conceptualizer style . 6 RUNCO AND ACER Runco and Charles ( 1993 ) compared subjective eva - 610 luations of originality , appropriateness , and creativity with objective ratings of the same ideas . Appropriate - ness was of special interest because creative things are always more than original ; they are in some way appro - priate . It may be effectiveness or aesthetic appeal , but in 615 some way creative things are all appropriate , as well as original . Runco and Charles discovered that the subjec - tive evaluations paralleled the objective ratings . More interesting was that originality and appropriateness were not correlated , as would be expected , given what was 620 just said about creative things . In fact , the lowest orig - inality ratings were given to the most appropriate ideas . Additionally , ( a ) originality was central to the creativity judgments , and ( b ) appropriateness without originality was judged to be indicative of low levels of creativity . 625 Later Charles and Runco ( 2000 – 2001 ) examined developmental changes in divergent thinking and eva - luative skills among 3th through 5th grade children . The 4th grade was of special interest because Torrrance ( 1968 ) and others previously found a slump at that age . 630 Charles and Runco found that evaluative skills for orig - inality and preference for appropriateness scores tend to increase by age . However , the relationship between divergent thinking ﬂuency scores and evaluative skills was quite small , except for one test item in which an 635 increasing preference of appropriateness accounted for a decline in the proportion of high - quality ideas . Inter - estingly , there was no 4th grade slump in this sample . Divergent thinking actually peaked in the 4th grade . Runco ( 1991 ) also investigated children’s evaluative 640 skills . He designed this study as a test of the theory that children’s creativity reﬂects lack of discretion ( about what is conventional ) rather than intentional creative expression . If this theory held up , and children were only unintentionally creative , they would tend to be poor 645 judges of the quality of ideas . Runco , therefore , asked one group of children to rate the creativity of ideas , and asked a second group to rate the same ideas for popu - larity . Popularity was examined because it might be more meaningful and workable a concept than creativity for 650 children . Children from 4th through 6th grades rated ideas ( some of which were entirely unique ) that had been generated by other children . Analyses indicated that eva - luative accuracy had discriminant validity in that it was unrelated to intelligence test scores . Statistical compari - 655 sons of the accuracy ratings indicated that children were , in fact , more accurate when rating ideas for popularity rather than creativity . Evaluative accuracy was related to DT , but as is the case in research with other samples ( e . g . , Runco & Smith , 1992 ) , the relationship is only mod - 660 erate . Being good at DT does not , then , insure accurate evaluations of ideas . Runco interpreted the ﬁndings of this study as conﬁrming that evaluative accuracy can be improved with appropriate instruction . Runco and Vega ( 1990 ) used a similar methodology 665 but examined evaluative accuracy of parents and tea - chers when judging ideas given by children . The accu - racy of evaluations of popularity and creativity was again compared . Interestingly , parents with more of their own children provided more accurate evaluations 670 than other parents . Evaluative accuracy and DT were again correlated , as was the case in other samples , but contrary to expectations , parents and teachers did not differ signiﬁcantly in the accuracy of their evaluations . Evaluative accuracy seems to vary with the speciﬁc 675 task , or at least when ﬁgural , verbal , and realistic tasks are compared . Runco and Dow ( 2004 ) found evaluative accuracy was the lowest in realistic problems and highest in verbal DT tests . The ﬁgural test was in the middle in terms evaluative accuracy . These ﬁndings are not at all 680 surprising given other empirical research on DT which also shows task differences in ﬂuency , originality , and ﬂexibility . Runco et al . ( 2005 ) , for example , adminis - tered both realistic and unrealistic tasks and asked part - icipants to produce as ( a ) many ideas as possible , ( b ) 685 only original , ( c ) only appropriate , or ( d ) only creative ideas . Results indicated that the participants produced more original but fewer appropriate ideas in unrealistic tasks than realistic tasks in all instructional groups . Flexibility and ﬂuency scores were highest for the unrea - 690 listic tests in all instructional groups except when instruction focused on originality . Appropriateness and originality were not strongly related ( . 26 ) . What is most important is no doubt that the realistic tasks elicit the lowest originality scores . This tendency had been 695 explained by the fact that individuals are likely , or at least able , to draw on experience and memory , and therefore do not need to actually use creative thinking to generate new ideas , when faced with realistic tasks , but evaluations certainly could also play a role . 700 CONCLUSIONS Thinking about DT has following the motion of a pendu - lum . There was great enthusiasm early on , many people apparently thinking that DT tests could replace IQ . Soon critics showed that predictive and discriminant validities 705 were moderate at best , and the pendulum went to the other extreme . At that point , DT and all tests of creativity were questioned and largely dismissed . The pendulum swung again when more careful psychometric studies demonstrated that the validities were , in fact , as good as 710 is found throughout psychological assessment ( some coef - ﬁcients in excess of . 6 ) , and as good as would be expected with a reasonable theory . The bedrock of that reasonable theory is that DT tests are not tests of creativity . They are estimates of the potential for creative problem solving . 715 DT is not synonymous with creativity . ESTIMATING CREATIVE POTENTIAL 7 To our reading of the literature , creativity studies are ready to progress beyond that point . The word is out , DT is not synonymous with creativity but DT tests pro - vide useful estimates of meaningful potential . Not sur - 720 prisingly , with this in mind , there is now research that starts with that premise and looks beyond validity and reliability . The most impressive of those focus on the underpinnings of DT . Consider in this regard recent research on the relationship between divergent thinking 725 and speed of relatedness judgment . Vartanian , Martindale , and Matthews ( 2009 ) proposed that creative people must be faster at judging the relatedness of con - cepts , as this would enable judgments about promising ideas and ideational pathways . Vartanian et al . were con - 730 ﬁrmed that people with higher divergent thinking ability were faster in their relatedness judgments . Furthermore , relatedness judgments were not related to IQ . Genetic factors underlying divergent thinking have been uncovered by Reuters et al . ( 2002 ) Q5 and Runco et al . 735 ( 2010 ) . Runco et al . , for example , found that verbal and ﬁgural ﬂuency were related to several genes , even after controlling intelligence , but originality was not . The key genes involve dopamine reception . One intriguing thing about this is that the genetic correlate 740 seems to be limited to ﬂuency and productivity , and not originality . Additionally , dopamine reception is associa - ted with tendencies towards obsessions and addictions , which may be relevant to the health issues in creativity studies ( Richards & Runco , 1997 ) . 745 The point is the same one we used as our opening sentence : There is great value in the concept of divergent thinking . Much of the research focuses on DT tests , and their reliability and validity , but additional research tells us more broadly how DT is various social and psycho - 750 logical factors ( e . g . , IQ , personality , family background ) and how it is associated with problem solving , ideation , and creative potential . Ideas are useful in many aspects of our lives , and the research on divergent thinking remains useful for understanding the quality of ideas 755 and the processes involved . REFERENCES Abedi , J . ( 2002 ) . A latent - variable modeling approach to assessing reliability and validity of a creativity instrument . Creativity Research Journal , 14 , 267 – 276 . 760 Auzmendi , E . , Villa , A . , & Abedi , J . ( 1996 ) . Reliability and validity of a newly constructed multiple - choice creativity instrument . Creativity Research Journal , 9 , 89 – 96 . Basadur , M . , Wakabayashi , M . , & Graen , G . B . ( 1990 ) . Individual problem solving styles and attitudes toward divergent thinking 765 before and after training . Creativity Research Journal , 3 , 22 – 32 . Basadur , M . ( 1995 ) . Optimal ideation - evaluation ratios . Creativity Research Journal , 8 , 63 – 75 . Binet , A . , & Simon , T . ( 1905 ) . The development of intelligence in children . L’Annee Psychologique , 11 , 163 – 191 . 770 Blair , C . S . , & Mumford , M . D . ( 2007 ) . Errors in idea evaluation : Preferencefortheunoriginal ? JournalofCreativebehavior , 41 , 197 – 222 . Brophy , D . R . ( 1998 ) . Understanding , measuring and enhancing indi - vidual creative problem - solving efforts . Creativity Research Journal , 11 , 123 – 150 . 775 Campbell , D . T . ( 1960 ) . Blind variation and selective retention in cre - ative thought as in other knowledge processes . Psychological Review , 67 , 380 – 400 . Charles , R . E . , & Runco , M . A . ( 2000 – 2001 ) . Developmental trends in the evaluation and divergent thinking of children . Creativity 780 Research Journal , 13 , 417 – 437 . Cheung , P . C . , Lau , S . , Chan , D . W . , & Wu , W . Y . H . ( 2004 ) . Creative potential of school children in Hong Kong : Norms of the Wallach - Kogan Creativity Tests and their implications . Creativity Research Journal , 16 , 69 – 78 . 785 Clark , P . M . , & Mirels , H . L . ( 1970 ) . Fluency as a pervasive element in the measurement of creativity . Journal of Educational Measurement , 7 , 83 – 86 . Cramond , B . , Matthews - Morgan , J . , Bandalos , D . , & Zuo , L . ( 2005 ) . A report on the 40 - year follow - up of the Torrance Tests of Creative 790 Thinking : Alive and well in the new millennium . Gifted Child Quarterly , 49 , 283 – 291 . Crockenberg , S . ( 1972 ) . Creativity tests : A boon or boondoggle in edu - cation . Review of Educational Research , 42 , 27 – 45 . Cropley , A . J . ( 1967 ) . Creativity , intelligence , and achievement . 795 Alberta Journal of Educational Research , 13 , 51 – 58 . Cropley , A . J . ( 2000 ) . Deﬁning and measuring creativity : Are creativity tests worth using ? Roeper Review , 23 , 72 – 79 . Cropley , A . J . ( 2006 ) . In praise of convergent thinking . Creativity Research Journal , 18 ( 3 ) , 391 – 404 . 800 Dailey , L . , & Mumford , M . D . ( 2006 ) . Evaluative aspects of creative thought : Errors in appraising the implications of new ideas . Creativ - ity Research Journal , 18 , 367 – 384 . Eysenck , H . J . ( 1997 ) . Creativity and personality . In M . A . Runco ( Ed . ) , The creativity research handbook ( Vol . 1 , pp . 41 – 66 ) . Cresskill , 805 NJ : Hampton Press . Q6 Eysenck , H . ( 2003 ) . Creativity , personality , and the convergent - divergent continuum . In M . A . Runco ( Ed . ) , Critical creative processes ( pp . 95 – 114 ) . Cresskill , NJ : Hampton . Guilford , J . P . ( 1950 ) . Creativity . American Psychologist , 5 , 444 – 454 . 810 Guilford , J . P . ( 1968 ) . Creativity , intelligence and their educational implications . San Diego , CA : EDITS = Knapp . Guilford , J . P . , & Hoepfner , R . ( 1971 ) . The analysis of intelligence . New York , NY : McGraw - Hill . Harrington , D . M . , Block , J . , & Block , J . H . ( 1983 ) . Predicting creativ - 815 ity in preadolescence from divergent thinking in early childhood . Journal of Personality and Social , 45 , 609 – 623 . Hocevar , D . ( 1979a ) . A comparison of statistical infrequency and subjective judgment as criteria in the measurement of originality . Journal of Personality Assessment , 43 , 297 – 299 . 820 Hocevar , D . ( 1979b ) . Ideational ﬂuency as a confounding factor in the measurement of originality . Journal of Educational Psychology , 71 , 191 – 196 . Hocevar , D . ( 1981 ) . Measurement of creativity : Review and critique . Journal of Personality Assessment , 45 , 450 – 464 . 825 Hocevar , D . , & Michael , W . ( 1979 ) . The effects of scoring formulas on the discriminant validity of tests of divergent thinking . Educational and Psychological Measurement , 39 , 917 – 921 . Khandwalla , P . N . ( 1993 ) . Anexploratoryinvestigationofdivergentthink - ing through protocol analysis . Creativity Research Journal , 6 , 241 – 259 . 830 Kogan , N . , & Pankove , E . ( 1974 ) . Long - term predictive validity of divergent - thinking tests . JournalofEducational Psychology , 66 , 802 – 810 . Licuanan , B . F . , Dailey , L . R . , & Mumford , M . D . ( 2007 ) . Idea evalu - ation : Error in evaluating highly original ideas . Journal of Creative Behavior , 41 , 1 – 27 . 8 RUNCO AND ACER 835 Lonergan , D . C . , Scott , G . M . , & Mumford , M . D . ( 2004 ) . Evaluative aspects of creative thought : Effects on appraisal and revision stan - dards . Creativity Research Journal , 16 , 231 – 246 . MacKinnon , D . ( 1965 ) . Personality and the realization of creative potential . American Psychologist , 20 , 273 – 281 . 840 Manske , M . R . , & Davis , G . A . ( 1968 ) . Effects of simple instructional biases upon performance in the Unusual Uses Test . Journal of General Psychology , 79 , 25 – 33 . Q7 Mednick , S . A . ( 1962 ) . The associative basis of the creative process . Psychological Review , 69 , 220 – 232 . 845 Meeker , M . , Meeker , R . , & Roid , G . H . ( 1985 ) . Structure of Intellect Learning Abilities Test ( SOI - LAT ) manual . Los Angeles , CA : West - ern Psychological Services . Michael , W . B . , & Wright , C . R . ( 1989 ) . Psychometric issues in the assessment of creativity . In J . A . Glover , R . R . Ronning , & C . R . 850 Reynolds ( Eds . ) , Handbook of creativity ( pp . 33 – 52 ) . New York , NY : Plenum Press . Milgram , R . M . , Milgram , N . A . , Rosenbloom , G . , & Rabkin , L . ( 1978 ) . Quantity and quality of creative thinking in children and adolescents . Child Development , 49 , 385 – 388 . 855 Moneta , G . B . ( 1994 ) . A model of scientists’ creative potential : The matching of cognitive structures and domain structure . Philosophi - cal Psychology , 6 , 23 – 37 . Moran , J . D . , Milgram , R . M . , Sawyers , J . K . , & Fu , V . R . ( 1983 ) . Stimulus speciﬁcity in the measurement of original thinking in pre - 860 school children . Journal of Psychology , 4 , 99 – 105 . Mouchiroud , C . , & Lubart , T . ( 2001 ) . Children’s original thinking : An empirical examination of alternative measures derived from diver - gent thinking tasks . Journal of Genetic Psychology , 162 , 382 – 401 . Mumford , M . D . ( 2000 – 2001 ) . Something old , something new : Revisit - 865 ing Guilford’s conception of creative problem solving . Creativity Research Journal , 13 , 267 – 276 . Mumford , M . D . , Blair , C . , Dailey , L . , Leritz , L . E . , & Osburn , H . K . ( 2006 ) . Errors in creative thought ? Cognitive biases in a complex processing activity . Journal of Creative Behavior , 40 , 75 – 110 . 870 Mumford , M . D . , Marks , M . A . , Connelly , M . S . , Zaccaro , S . J . , & Johnson , J . F . ( 1998 ) . Domain - based scoring of divergent thinking tests : Validation evidence in an occupational sample . Creativity Research Journal , 11 , 151 – 163 . Murray , H . A . ( 1959 ) . Vicissitudes of creativity . In H . H . Anderson ( Ed . ) , 875 Creativity and its cultivation ( pp . 203 – 221 ) . New York , NY : Harper . Richards , R . , & Runco , M . A . ( Eds . ) . ( 1997 ) . Eminent creativity , every - day creativity , and health . Greenwich , CT : Ablex . Runco , M . A . ( 1985 ) . Reliability and convergent validity of ideational ﬂexibility as a function of academic achievement . Perceptual and 880 Motor Skills , 61 , 1075 – 1081 . Runco , M . A . ( 1986 ) . The discriminant validity of gifted children’s divergent thinking test scores . Gifted Child Quarterly , 30 , 78 – 82 . Runco , M . A . ( 1989 ) . Parents’ and teachers’ ratings of the creativity of children . Journal of Social Behavior and Personality , 4 , 73 – 83 . 885 Runco , M . A . ( 1991 ) . The evaluative , valuative , and divergent thinking of children . Journal of Creative Behavior , 25 , 311 – 319 . Runco , M . A . ( Ed . ) . ( 1994 ) . Problem ﬁnding , problem solving , and creativity . Norwood , NJ : Ablex . Runco , M . A . ( 2007 ) . Creativity : Theories and themes : Research , devel - 890 opment , and practice . New York , NY : Academic Press . Runco , M . A . , & Acar , S . ( in press ) . Do tests of divergent thinking have an experiential bias ? Psychology of Aesthetics , Creativity , and the Arts . Q8 Runco , M . A . , & Albert , R . S . ( 1985 ) . The reliability and validity of ideational originality in the divergent thinking of academically 895 gifted and nongifted children . Educational and Psychological Measurement , 45 , 483 – 501 . Runco , M . A . , & Basadur , M . ( 1993 ) . Assessing ideational and evalua - tive skills and creative styles and attitudes . Creativity and Innovation Management , 2 , 166 – 173 . 900 Runco , M . A . , & Charles , R . ( 1993 ) . Judgments of originality and appropriateness as predictors of creativity . Personality and Individ - ual Differences , 15 , 537 – 546 . Runco , M . A . , & Dow , G . T . ( 2004 ) . Assessing the accuracy of judg - ment of originality on three divergent thinking tests . Korean Journal 905 of Thinking and Problem Solving , 14 , 5 – 14 . Runco , M . A . , Dow , G . T . , & Smith , W . R . ( 2006 ) . Information , experience , and divergent thinking : An empirical test . Creativity Research , 18 , 269 – 277 . Runco , M . A . , Illies , J . J . , & Eisenman , R . ( 2005 ) . Creativity , orig - 910 inality , and appropriateness : What do explicit instructions tell us about their relationships ? Journal of Creative behavior , 39 , 137 – 148 . Runco , M . A . , McCarthy , K . A . , & Svensen , E . ( 1994 ) . Judgments of the creativity of artwork from students and professional artists . Journal of Psychology , 128 , 23 – 31 . 915 Runco , M . A . , & Mraz , W . ( 1992 ) . Scoring divergent thinking tests using total ideational output and a creativity index . Educational and Psychological Measurement , 52 , 213 – 211 Q9 . Runco , M . A . , Noble , E . P . , Reiter - Palmon , R . , Acar , S . , Ritchie , T . , & Yurkovich , J . ( 2010 ) . The genetic basis of creativity and ideational 920 ﬂuency . Submitted for publication . Q10 Runco , M . A . , Okuda , S . M . , & Thurston , B . J . ( 1987 ) . The psycho - metric properties of four systems for scoring divergent thinking tests . Journal of Psychoeducational Assessment , 5 , 149 – 156 . Runco , M . A . , Plucker , J . A . , & Lim , W . ( 2000 ) . Development and 925 psychometric integrity of a measure of ideational behavior . Creativ - ity Research Journal , 13 , 391 – 398 . Runco , M . A . , & Smith , W . R . ( 1992 ) . Interpersonal and intrapersonal evaluations of creative ideas . Personality and Individual Differences , 13 , 295 – 302 . 930 Runco , M . A . , & Vega , L . ( 1990 ) . Evaluating the creativity of chil - dren’s ideas . Journal of Social Behavior and Personality , 5 , 439 – 452 . Seddon , G . M . ( 1983 ) . The measurement and properties of divergent thinking ability as a single compound entity . Journal of Educational Measurement , 20 , 393 – 402 . 935 Snyder , A . , Mitchell , D . , Bossomaier , T . , & Pallier , G . ( 2004 ) . The creativity quotient , an objective scoring of ideational ﬂuency . Creativity Research Journal , 16 , 415 – 420 . Torrance , E . P . ( 1966 ) . Torrancetestsofcreativethinking : Norms - technical manual ( Research Edition ) . Princeton , NJ : Personnel Press . 940 Torrance , E . P . ( 1968 ) . A longitudinal examination of the fourth grade slump in creativity . Gifted Child Quarterly , 12 , 195 – 199 . Torrance , E . P . ( 1969 ) . Prediction of adult creative achievement among high school seniors . Gifted Child Quarterly , 13 , 223 – 229 . Torrance , E . P . ( 1972 ) . Predictive validity of the Torrance Tests of 945 Creative Thinking . Journal of Creative Behavior , 6 , 236 – 252 . Torrance , E . P . ( 1980 ) . Norms - technical manual : Demonstrator form for the Torrance Tests of Creative Thinking . Unpublished Manuscript , Georgia Studies of Creative Behavior , Athens , GA . Torrance , E . P . ( 1981 ) . Predicting the creativity of elementary school 950 children ( 1958 – 1980 ) —And the teacher who ‘‘made a difference . ’’ Gifted Child Quarterly , 25 , 55 – 62 . Torrance , E . P . ( 1995 ) . Why ﬂy ? Norwood , NJ : Ablex . Q11 Urban , K . K . , & Jellen , H . G . ( 1996 ) . Test for Creative Thinking - Drawing Production ( TCT - DP ) . Lisse , Netherlands : Swets 955 and Zeitlinger . Vartanian , O . , Martindale , C . , & Matthews , J . ( 2009 ) . Divergent thinking ability is related to faster relatedness judgments . Psy - chology of Aesthetics , Creativity and the Arts , 3 , 99 – 103 . Vernon , P . E . ( 1971 ) . Effects of administration and scoring on diver - 960 gent thinking tests . British Journal of Educational Psychology , 41 , 245 – 257 . Vincent , A . S . , Decker , B . P . , & Mumford , M . D . ( 2002 ) . Divergent thinking , intelligence & expertise : A test of alternative models . Creativity Research Journal , 14 , 163 – 178 . ESTIMATING CREATIVE POTENTIAL 9 965 Wallach , M . A . ( 1970 ) . Creativity . In P . H . Mussen ( Ed . ) , Manual of child psychology ( Vol . 1 , pp . _ _ – _ _ ) . New York , NY : Wiley and Sons . Q12 Wallach , M . A . , & Kogan , N . ( 1965 ) . Modes of thinking in young chil - dren . New York , NY : Holt , Rinehart , & Winston . Wallach , M . A . , & Wing , C . W . ( 1969 ) . The talented student : A vali - 970 dation of the creativity - intelligence distinction . New York , NY : Holt , Rinehart & Winston . Williams , F . ( 1980 ) . Creativity assessment packet : Manual . East Aurora , NY : DOK . Q13 Yamamoto , K . ( 1965 ) . ‘‘Creativity’’—A blind man’s report on the 975 elephant . Journal of Counseling Psychology , 21 , 428 – 434 . Zarnegar , Z . , Hocevar , D . , & Michael , W . B . ( 1988 ) . Components of original thinking in gifted children . Educational and Psychological Measurement , 48 , 5 . Q14 10 RUNCO AND ACER