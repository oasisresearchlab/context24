Making Privacy - preserving Federated Graph Analytics with Strong Guarantees Practical ( for Certain Queries ) Kunlong Liu University of California , Santa Barbara Trinabh Gupta University of California , Santa Barbara ABSTRACT Privacy - preserving federated graph analytics is an emerging area of research . The goal is to run graph analytics queries over a set of devices that are organized as a graph while keeping the raw data on the devices rather than centralizing it . Further , no entity may learn any new information except for the final query result . For instance , a device may not learn a neighbor’s data . The state - of - the - art prior work for this problem provides privacy guarantees ( but not integrity ) for a broad set of queries in a strong threat model where the devices can be malicious . However , it imposes an impractical overhead : each device locally requires over 8 . 79 hours of cpu time and 5 . 73 GiBs of network transfers per query . This paper presents Colo , a new , low - cost system for privacy - preserving federated graph analytics that requires minutes of cpu time and a few MiBs in network transfers , for a particular subset of queries . At the heart of Colo is a new secure computation protocol that enables a device to securely and efficiently evaluate a graph query in its local neighborhood while hiding device data , edge data , and topology data . An implementation and evaluation of Colo shows that for running a variety of COVID - 19 queries over a population of 1M devices , it requires less than 8 . 4 minutes of a device’s cpu time and 4 . 93 MiBs in network transfers—improvements of up to three orders of magnitude . 1 INTRODUCTION As a motivating example , consider the following scenario between a mobile app maker of a contact tracing application for an infectious disease like COVID - 19 [ 22 , 74 , 75 ] , and an influential data analyst such as the Centers for Disease Control and Prevention ( CDC ) [ 17 ] . The app maker installs the app on a large number of mobile devices , where it collects information on whether a device owner is currently infected , and when , where , and for how long the device comes in contact with other devices . Abstractly , one can view the devices as a graph where they are the nodes and their interactions are the edges . Meanwhile , the analyst wants to use the data to study disease patterns . For instance , it wants to understand the prevalence of superspreaders by evaluating the average number of infected devices in an infected device’s neighborhood [ 40 , 63 ] . Can we enable the analyst to run such queries and learn their result ? Further , can we do it in a way that doesn’t require moving device and edge data outside the devices to a centralized location ? And further still , can we ensure that only the query result is revealed and no new individual device information is learned by any other party ? This is the problem of privacy - preserving federated graph analyt - ics . The federated aspect of this problem emphasizes keeping raw data at the devices , in contrast to centralizing the data , which is highly susceptible to data breaches , especially in bulk [ 2 , 66 , 68 , 76 ] . Meanwhile , the privacy guarantee of the problem emphasizes that an entity should get only the information that it absolutely needs . Thus , an analyst may learn the query result that is an aggregate across devices . And any device may not learn any more information than it knows locally through its own data and edges . In this scenario , we consider a threat model where devices and the analyst can be malicious as the data is highly sensitive and compromising devices or the analyst can often happen . Similar to prior work [ 70 ] , we primarily focus on privacy : we guarantee pri - vacy in the presence of malicious adversaries , but ensure integrity only when all parties are semi - honest . Ensuring malicious integrity would impose much overhead and make the system impractical . Thus we leave ensuring malicious integrity as future work . As we discuss in related work ( § 7 ) , privacy - preserving federated graph analytics is an emerging area of research and displays a trade - off between generality , privacy , and efficiency . For instance , Gunther et al . [ 34 , 35 ] have built a system RIPPLE to answer epidemiological questions . However , their system answers aggregation queries only where a device can securely sum its state ( e . g . , an integer ) with its neighbors’ state . It doesn’t support other secure operations such as multiplication and comparsion . Thus , it cannot answer our motivating query on superspreaders . RIPPLE also releases partial sums to the devices , and does not consider the strongest of threat models as a set of its aggregators are honest - but - curious ( but not malicious ) . In contrast , Roth et al . [ 70 ] have built a general - purpose system , Mycelium , that assumes a strong threat model where both the devices and a centralized aggregator can be malicious . However , Mycelium is expensive . For the superspreader query over 1M devices , each device incurs 8 . 79 hours of local cpu time and 5 . 73 GiB of network transfers . This paper introduces Colo , an efficient system for privacy - preserving federated graph analytics in a strong threat model . Colo allows an analyst to run simple queries ( like the superspreader query ) that have predicates with a limited set of inputs and out - puts , and that evaluate these predicates between a device and its neighbors and then aggregate the results across the devices . Colo guarantees privacy ( only the analyst learns the query result and no entity gets any other intermediate data ) while assuming malicious devices and a set of M aggregation servers of which a fraction f of them can be malicious . A possible configuration from existing work [ 41 , 42 ] is to set M to be 40 - 100 and f = 0 . 2 . Finally , Colo is scalable and efficient : it supports a large number of devices in the order of a few million , while requiring them to contribute a small amount of cpu and network . At a high level , Colo follows a workflow of local evaluation followed by a global aggregation across devices ( § 3 ) . In the local evaluation , each device evaluates the query between itself and its neighbors . The global aggregation then aggregates these per - device outputs . In this workflow , Colo must address two challenges . First , it must hide node , edge , and topology data during the local evalua - tion without imposing a large overhead on the devices . Second , it a r X i v : 2404 . 01619v1 [ c s . CR ] 2 A p r 2024 must aggregate per - node outputs across the population of devices without revealing the intermediate results , and again while being efficient for the devices . The first challenge of hiding node , edge , and topology data is tricky , especially with malicious devices ( § 2 . 4 ) . For instance , say two neighboring devices v A and v B want to compute the product v A . inf · v B . inf ( as needed for the superspreader query ) , where inf indicates their infection status . Then , a malicious device , say v B , may set its infection status to v B . inf = 10 8 . As a result , the query result secretly encodes v A ’s infection status ( result is large if v A . inf = 1 ) . One may use a general - purpose tool from cryptography to address this issue , but that would be expensive . Furthermore , even if there were an efficient protocol for this computation , say that requires a single interaction between v A and v B , then this protocol must also hide that v A and v B are communicating , to protect their topology data , i . e . , the fact they are neighbors . Colo addresses the first challenge , particularly , the part of hiding node and edge data , through a new , tailored secure computation protocol ( § 4 . 3 . 1 ) . Colo observes that the query predicates that Colo targets operate over a limited set of inputs and produce a limited set of outputs . For instance , the legitimate inputs and outputs for v A . inf · v B . inf are all either zero or one . Thus , instead of using a general purpose secure computation protocol such as Yao’s Garbled Circuits [ 85 ] that operates over arbitrary inputs and outputs , Colo uses a protocol that operates over a limited set of inputs and outputs . Specifically , one party , say v B , computes all possible legitimate query outputs in plaintext , and then allows the other party v A to pick one of these outputs privately using oblivious transfers ( OT ) [ 20 , 67 ] . Colo fortifies this protocol against malicious behavior of v B by incorporating random masks , efficient commitments [ 15 , 31 ] , and range proofs [ 29 , 32 ] ( § 4 . 3 . 1 ) . The protocol described above doesn’t yet address the require - ment of hiding the topology of devices . To hide this data efficiently , that is , the knowledge of who is a neighbor with whom , Colo pro - hibits the devices from directly interacting with each other . Rather , Colo arranges for them to communicate via a set of servers , specifi - cally , a set of 40 to 100 servers , where up to 20 % are malicious ( § 3 ) . This arrangement enables the servers to run a particular metadata hiding communication system , Karaoke [ 41 ] , that is provably secure and low cost for the devices ( although with significant overhead for the resourceful servers ) ( § 4 . 3 . 2 ) . Finally , Colo addresses the second challenge of global aggrega - tion across devices through straightforward secret sharing tech - niques while piggybacking on the set of servers ( § 3 , § 4 . 4 ) . Specifi - cally , devices add zero sum masks to their local outputs , and send shares of these local results to the servers . As long as one of the servers is honest , the analyst learns only the query output . We have implemented ( § 5 ) and evaluated ( § 6 ) a prototype of Colo . Our evaluation shows that for 1M devices connected to at most 50 neighbors each , and for a set of example queries ( Figure 1 ) which includes the superspreader query , a device in Colo incurs less than 8 . 4 minutes of ( single core ) cpu time and 4 . 93 MiB of network transfers . In contrast , the Mycelium system of Roth et al . [ 70 ] requires a device - side cost of 8 . 79 hours ( single core ) cpu time and 5 . 73 GiB network transfers . In addition , Colo’s server - side cost , depending on the query , ranges from $ 158 to $ 1 , 504 total for Colo’s 40 servers ( the lower number is for the superspreader query ) . In contrast , Mycelium’s server - side cost is over $ 57 , 490 per query . Colo’s limitations are substantial . In particular , it does not handle general purpose queries , rather only those that evaluate predicates over a bounded set of inputs and outputs . However , Colo scales to a significant number of devices and is efficient for them , in a strong threat model . But more importantly , unlike prior work , Colo shows that privacy - preserving federated graph analytics can be practical , and that the CDC could run certain queries over the devices’ data while guaranteeing privacy in a strong sense , without draining the devices’ resources , and without aggressively depleting its own budget ( e . g . , running the superspreader query in a large city every two weeks would cost around four thousand dollars annually ) . 2 PROBLEM STATEMENT 2 . 1 Scenario We consider a scenario consisting of a data analyst A and a large number of mobile devices v i for i ∈ [ 0 , N ) . For instance , N = 10 6 . The devices form a graph . As an example , they may run a contact tracing application for COVID - 19 [ 22 , 74 , 75 ] that collects infor - mation on the infection status of the device owners and identities of the devices they come in contact with , that is , their neighbors . More precisely , a device may have ( i ) node data , for example , a sta - tus variable inf indicating whether the device’s owner is currently infected , tInf indicating the time the owner got infected ( or null if the owner is not infected ) , and demographic information such as age and ethnicity ; ( ii ) edge data , for example , the number of times the device came in contact with a neighbor ( contacts ) , the cumu - lative duration ( duration ) of these interactions , and ( location , time , duration ) of each interaction ; and , ( iii ) topology data , for example , the list of the device’s neighbors . The analyst A , say a large hospital or CDC in the context of the contact tracing application , wants to analyze the device data by running graph queries . Figure 1 shows a few example queries from the literature [ 1 , 11 , 23 , 33 , 38 , 40 , 56 , 60 , 63 ] ( these queries are a subset of the ones considered in the Mycelium system of Roth et al . [ 70 ] ) . For instance , A may want to learn the number of active infections infected devices have in their neighborhood ( Q1 ) . As another example , it may want to learn how frequently infected devices contact their neighbors who are subsequently infected ( Q3 ) . Generally , these queries perform a local computation at every device and its neighborhood , and then aggregate the per - device results . 2 . 2 Threat model We assume that the devices are malicious , i . e . , an adversary can compromise an arbitrary subset of devices . A compromised device may try to learn information about an honest device beyond what it knows from its own data . For instance , if a compromised device is a neighbor of an honest device , then the compromised device already has edge data for their edge ( e . g . , the location and time of their last meeting ) ; however , the adversary may further want to learn if the honest device is infected ( neighbor’s node data ) , whether it recently met someone on the subway ( neighbor’s edge data ) , and whom it recently came in contact with ( neighbor’s topology data ) . 2 Query Description Q1 The total number of infections in an infected participant’s neighborhood SELECT COUNT ( * ) FROM neigh ( 1 ) WHERE self . inf & neighbor . inf Q2 The amount of time neighbor has spent near infected device if neighbor is infected within 5 - 15 days of contact with the device SELECT SUM ( edge . duration ) FROM neigh ( 1 ) WHERE self . inf & neighbor . inf & ( neighbor . tInf ∈ [ edge . lastContact + 5 days , edge . lastContact + 15 days ] ) Q3 The frequency of contact between device and neighbor , if device infected neighbor SELECT SUM ( edge . contacts ) / COUNT ( * ) FROM neigh ( 1 ) WHERE self . inf & neighbor . inf & ( neighbor . tInf > self . tInf + 2 days ) Q4 Secondary attack rate of infected devices if they traveled on the subway SELECT SUM ( neighbor . inf ) / COUNT ( * ) FROM neigh ( 1 ) WHERE self . inf & onSubway ( edge . lastContact . location ) Q5 The number of secondary infections caused by infected devices in different age groups SELECT COUNT ( * ) FROM neigh ( 1 ) WHERE self . inf & neighbor . inf & ( neighbor . tInf > self . tInf + 2 days ) GROUP BY self . age Q6 The number of secondary infections based on type of exposure ( such as family , social , work ) SELECT COUNT ( * ) FROM neigh ( 1 ) WHERE self . inf & neighbor . inf & ( neighbor . tInf > self . tInf + 2 days ) GROUP BY edge . setting Q7 Secondary attack rates in household vs non - household contacts SELECT SUM ( neighbor . inf ) / COUNT ( * ) FROM neigh ( 1 ) WHERE self . inf GROUP BY isHousehold ( edge . lastContact . location ) Q8 Secondary attack rates within case - contact pairs in the same age group SELECT SUM ( neighbor . inf ) / COUNT ( * ) FROM neigh ( 1 ) WHERE self . inf & neighbor . age ∈ [ 0 , 100 ] & self . age ∈ [ neighbor . age - 10 , neighbor . age + 10 ] Figure 1 : Example graph queries from Mycelium [ 70 ] and the literature on health analytics [ 1 , 11 , 23 , 33 , 38 , 40 , 56 , 60 , 63 ] . We assume that the domain of the inputs to these queries is bounded , for example , inf ∈ [ 0 , 1 ] and tinf ∈ [ 1 , 120 ] , referring to the days in the latest few months . The analyst may also be malicious and want to learn about individual device node , edge , or topology data—information that is more granular than the result of the queries . Finally , we assume that the adversary may also observe and manipulate network traffic , for instance , in the backbone network , and try to infer relationships between devices . 2 . 3 Goals and non - goals Target queries . Ideally , we would support arbitrary graph queries . However , as noted earlier ( § 1 ) , generality of queries is in tension with privacy and efficiency . Thus , in this paper we focus on simpler queries such as those in Figure 1 where the aggregations across devices are SUM , COUNT and AVG operations , and where devices compute simple predicates on a small set of possible inputs in their one hop neighborhood ( for example , the infection status inf is either zero or one , and the time of infection tinf may be in [ 1 , 120 ] referring to the days in the latest few months ) . We note that although these queries are a sub - class of a broad set of queries , they are important according to the health literature and form a precursor to more sophisticated queries in an analyst’s workflow . Besides the limitation on the generality , we don’t aim to support interactive queries . Privacy ( P1 ) . Private data should always be hidden from the ad - versary . From the point of view of a device , it may not learn any information beyond its own node , edge , and topology data . In par - ticular , it may not learn any information about a neighbor beyond what is contained in the direct edge to the neighbor . Similarly , the analyst must only learn the query result . Privacy ( P2 ) . Individual devices will cause limited changes to query result . With privacy goal P1 , only the query result would be learnt by the adversary . To prevent adversaries from learning ( additional ) information from the query result , the system must guarantee that individual devices will make bounded contribution to the result . Scaleand efficiency . First , our systemmust supportalarge number of devices , for example , one million . We assume that these devices have a bounded degree , for example , up to 50 neighbors . ( If the actual device graph has nodes with a larger degree , then they may select a subset uniformly for the query execution . ) Second , in terms of device overhead , we want that to be low , for example , minutes of cpu time and at most a few MiBs in network transfers , which is affordable for mobile devices . Meanwhile , if the system employs any servers , e . g . , cloud servers , then we want the server - side cost to be affordable . In terms of dollar cost of renting the servers , we want the per - server cost to be no more than a few tens of dollars ( per query over a million devices ) . Correctness ( non - goal ) . In this paper , we focus on privacy and require the query output to be correct only during periods when all parties ( devices and any servers ) are semi - honest . Ensuring correctness in the periods of malicious behavior could be very expensive . Thus we leave it as future work . Nevertheless , the system must ensure privacy at all times . 2 . 4 Challenge and straw man solutions Meeting these goals , particularly , privacy and efficiency , when the parties can behave maliciously is hard . We explain this point by discussing possible approaches and attacks below . We will use query Q1 ( Figure 1 ) , calculating the total number of infected direct neighbors of all infected devices , for illustration purposes . Centralized server . One approach is to assume a trusted central server and ask devices to upload their data in plaintext . For example , each device uploads the node data ( inf ) along with its neighbor list , and edge data for all the edges . The server can evaluate the query using a graph analytics system such as GraphX [ 30 ] as the data is in a single location in plaintext . This approach , however , breaks the privacy goal P1 immediately . The server sees individual device data and learns more than the query result . This approach also breaks the privacy goal P2 . Though the de - vices do not learn more than they should in this approach , mali - cious devices can execute subtle attacks to cause a victim device 3 . . . 2 ) local aggregation S M S 1 secure comp . local aggregation . . . 1 ) query distribution A S M S 2 S 1 q v 1 v N signed ( q ) 3 ) global aggregation v 1 v N . . . . . . A metadata private communication . . . S M S 2 S 1 V i Figure 2 : An overview of Colo’s query distribution , local aggregation , and global aggregation phases of query execution . The dotted line in local aggregation depicts metadata - hiding communication , and the dashed line depicts secure computation . contribute more than desired to the query result and thus leak in - formation . For the example query Q1 , the server needs to compute (cid:205) v A , v B v A . inf · v B . inf for all pairs of neighbors ( v A , v B ) to calculate the total number of infections in infected devices’ neighborhood . A malicious device v 0 can send a large constant , e . g . , v 0 . inf = 10 6 to the server . As a result , if an honest device v 1 is v 0 ’s neighbor , the product v 1 . inf · v 0 . inf will contribute a large value in the ag - gregation and the adversary can infer whether v 1 . inf is 0 or 1 by inspecting the query result . As another example of breaking P2 , a set of malicious devices can claim that a victim device is their neighbor . Again for the example query , if 10 6 malicious devices claim v 1 is their neighbor and set their inf to be 1 , the server will compute (cid:205) v A , v B v A . inf · v B . inf + (cid:205) 10 6 1 · v 1 . inf . This computation again amplifies the victim device v 1 ’s data in the aggregate output . To hide the data from the server ( privacy goal P1 ) , one approach is to use tools from cryptography such as homomorphic encryp - tion [ 27 , 69 ] or secret sharing [ 12 , 72 ] . Meanwhile , to meet privacy goal P2 , the server will have to run logic to verify devices’ ( secret ) inputs to prevent malicious devices from uploading arbitrary data . However , this approach doesn’t scale to a large number of devices . For example , for Q1 , the server will need to perform at least O ( N ) multiplications ( N is the number of devices ) , and also verifications of devices’ data . Indeed , as we discuss in related work ( § 7 ) , we are not aware of any prior work that follows the centralized server approach when the threat model assumes malicious parties . Federated analytics . Another approach to the problem is the idea of keeping the data federated . Each device keeps its raw data lo - cal and first communicates with its neighbors to perform local computation ( e . g . , aggregation at the neighborhood level ) , before uploading this local result to a server for aggregation . The feder - ated approach has distinct advantages relative to the centralized approach : ( i ) it allows devices to feel more trust in the system as they keep possession of their data , ( ii ) it doesn’t risk bulk data breaches as devices do not collectively centralize their data , and ( iii ) it handles data updates naturally as each device computes locally and can use its latest data . However , efficiency is still challenging . The state - of - the - art system that follows the federated approach , Mycelium [ 70 ] , assumes malicious devices and a single untrusted server and scales to a large number of devices . However , even for our simple example query Q1 and for 1M devices , a Mycelium de - vice spends 8 . 79 hours in cpu time and 5 . 73 GiB in network transfers ( § 6 ) . The server is also expensive : it spends 1304 h cpu time and 5737 TiB network transfers . One core challenge is that during the local computation ( where a device performs local aggregation over its own and its neighbors’ data ) , Mycelium still needs to protect against the subtle attacks against privacy goal P2 mentioned above ( e . g . , one malicious neighbor inputting an arbitrary value into the aggregation ) —attacks for which Mycelium uses general - purpose ( and expensive ) homomorphic encryption [ 82 ] and zero - knowledge proofs [ 32 ] . Besides , Mycelium must protect against arbitrary be - havior of the untrusted server , which may try to manipulate the aggregation , e . g . , add a device’s data a large number of times to learn it . 3 OVERVIEW OF COLO Given the unique advantages of the federated approach relative to the centralized approach , Colo follows the former . To facilitate fed - eration , Colo relies on a set of servers , for example , 40 - 100 servers , S i for i ∈ [ 0 , M ) ( Figure 2 ) . These servers may be in separate admin - istrative domains ( e . g . , Azure versus Amazon AWS versus Google Cloud Platform , and different geographical zones within these cloud providers ) . Colo assumes that at most f = 20 % of these servers may be compromised by the adversary . That is , an adversary may com - promise , e . g . , up to 8 servers when the number of servers is 40 . M and f are both configurable and we followed prior works [ 41 , 42 ] to use this default setting . The devices connect to the servers in a star topology , where the central hub of the star is the set of servers and a spoke connects to a device . Thus , the devices do not directly interact with each other ; rather , they communicate via the servers . This design is necessary for two reasons . First , the devices typically do not have each other’s IP address . Second , the communication via the servers helps hide the devices’ topology data . Colo has a one - time setup phase , and three phases of query distribution , local aggregation , and global aggregation for query execution ( Figure 2 ) . In the setup phase , the servers generate and distribute keys for a cryptographic protocol used in the local aggregation phase . Specifically , they generate the proving and verification keys for a zero - knowledge proof ( ZKP ) scheme [ 86 ] . In the query distribution phase ( the leftmost diagram in Figure 2 ) , the analyst A specifies the query , say q , and sends it to each of the servers . Since A can be malicious ( § 2 . 2 ) and may write a query that tries to infer a single device’s data , the servers validate A ’s query . 4 Setup ( one time ) • The servers S i , i ∈ [ 0 , M ) , generate a set of keys for a zero - knowledge proof ( ZKP ) scheme using a MPC protocol [ 86 ] . • The servers distribute the keys to the devices . A device downloads the set of keys from one server and hashes of these keys from the others to defend against attacks from malicious servers ( a malicious server can distribute malicious keys ) . The devices stores the keys locally . Query distribution ( 1 ) The analyst A submits a query q to all the servers . ( 2 ) The servers verify the query and sign it . Each server broadcasts its signed query to all devices to start query execution . Local aggregation ( 3 ) The servers run the Karaoke system [ 41 ] to enable the devices to communicate anonymously with each other . ( 4 ) Each device initializes its local result of query execution to 0 and runs the local aggregation secure computation protocol ( § 4 . 3 . 1 ; Figure 4 ) with its neighbors over the Karaoke system [ 41 ] to obtain a share of its local result . Global aggregation ( 5 ) Each device secret shares its own share of its local result with the servers . ( 6 ) Each server sums all secret shares it receives . Each server sends its sum to the analyst and the analyst who adds the sum across the servers to reconstruct the global result . Figure 3 : A high - level description of Colo’s phases . For example , if q contains a WHERE clause of the form WHERE self . ID = xxx , then the servers reject it . In general , checking whether a query leaks information about an individual device is an open and difficult research problem [ 24 , 25 , 28 , 45 ] . Colo does not aim to solve it and assumes that the servers have a list of certified queries that are allowed . Once each server validates the query , it signs and broadcasts it to the devices . A device starts the next phase after validating enough signatures , that is , more than the threshold of servers that can be compromised . In the local aggregation phase ( the center diagram in Figure 2 ) , each device evaluates the query in its neighborhood . For instance , for the query Q1 , SELECT COUNT ( * ) FROM neigh ( 1 ) WHERE self . inf & neighbor . inf , each node computes the local count of infected neighbors in its neighborhood . More precisely , each node computes the count of infected neighbor ( either zero or one ) for each edge , and then adds the results across the edges . Recall that our goal is to ensure that a device learns no more information than it needs to ( § 2 . 3 ) . Thus , a device uses a secure computation protocol with its neighbor such that at the end of the protocol the two parties receive secret shares of the result of the computation . This protocol admits malicious devices ; for instance , a malicious neighbor is prevented from supplying an arbitrary input such as setting its inf = 10 6 . Secure computation hides node and edge data ; however , an adver - sary that observes network traffic can infer topology by monitoring who is performing secure computation with whom . Thus , the de - vices in the local aggregation phase execute secure computation over a metadata - hiding communication network , particularly , the Karaoke system [ 41 ] . The servers facilitate and run this system . Finally , in the global aggregation phase ( the right diagram in Figure 2 ) , the devices send their results from the local aggregation to the servers , who aggregate them . Specifically , each device secret shares its result with the servers , who locally add the shares they receive from the devices . The servers send the result of their local computation to the analyst who combines these outputs across the servers to obtain the query result . Colo’s architecture ( with the star topology where the hub is a set of servers ) offers advantages for the privacy versus efficiency tradeoff of Colo . First , the servers form a natural infrastructure to instantiate a metadata - hiding communication system such as Karaoke [ 41 , 42 ] . Second , this architecture keeps the cost of global aggregation low by enabling servers to perform only local compu - tations in the global aggregation phase . In contrast , if we assumed a completely untrusted server as the hub ( as in Mycelium [ 70 ] ) , then this untrusted server would have to use a sophisticated verification protocol to prove that it performed the global aggregation correctly ( for instance , prove that it did not add a victim device’s input many times ) . There would also be no resourceful infrastructure to run a metadata - hiding communication system . Nevertheless , there are still several challenges in instantiating this architecture , one of which is the efficiency of the secure com - putation piece in the local aggregation phase . This protocol has to be particularly efficient in terms of network transfers because this overhead gets exacerbated when secure computation messages are routed through the metadata - hiding network . We next go over the design details of Colo and how it addresses the various challenges . 4 DESIGN DETAILS This section describes the details of Colo . Figure 3 provides a high - level overview and how the phases connect with each other . 4 . 1 Setup ( key generation ) The first challenge Colo must deal with is the overhead of key gen - eration and distribution . As we will describe later ( § 4 . 3 . 1 ) , devices in Colo’s local aggregation phase need to generate an array T of values and prove that each value is within a range [ L , U ] . The chal - lenge is that the efficient and popular zero - knowledge proof ( ZKP ) schemes such as that of Groth [ 32 ] bind the proving and verification keys to the statement ( the circuit ) the prover is proving . That is , this circuit is a function of the length len ( T ) of array T in our case . 5 Unfortunately , the len ( T ) further depends on the analyst’s query q , and may be different for different queries . On the one extreme , Colo could use a single proving and verifi - cation key that works for len ( T ) = 1 . Then , when proving the prop - erties of an array T with len ( T ) > 1 , it could break down the array into singletons and generate a separate proof for each . The benefit is efficiency of key generation and distribution as Colo’s servers will need to generate and distribute one key pair . However , the penalty is during query execution as Colo’s devices would have to generate many proofs . For instance , for a query with len ( T ) = 1024 , the cpu time doubles and the size of the proof increases from 192 B to 192 KiB , relative to generating a single , holistic proof . This net - work overhead is significant because each message goes through the metadata - hiding communication network ( § 3 ) . On the other extreme , Colo could generate , say , 1024 keys for all possible lengths of T between 1 and 1024 . In this extreme , key generation and distribution is expensive ; for instance , the set of keys which has to be shipped to each device ( and stored there ) will total 52 . 4 GiB . However , the proof generation is optimal with a single proof for the array T , thereby lowering cpu and network overhead during query execution . Colo resolves this tension between overhead of key and proof generationbygeneratingakeyset { key 2 0 , key 2 1 , . . . , key 2 log ( len ( T ) ) − 1 } containing log ( len ( T ) ) keys for all powers of two between 1 and anticipated maximum length len ( T ) , for example , 1024 . Now , the size of the key set is manageable , e . g . , 102 MiB , and during query execution devices generate no more than log ( len ( T ) ) proofs . 4 . 2 Query distribution As mentioned in the overview ( § 3 ) , the analyst A specifies a query q and gives it to the servers , who validate it . Here , we elaborate on how A specifies the query . Each query has two components : ( i ) a SQL query similar to the examples we have discussed ( Figure 1 ) , and ( ii ) a transformation function , PreProcess , that transforms the raw data at a device into a form needed by the SQL query . One can view the PreProcess function as implementing preprocessing or cleaning of data and the SQL query as the analysis over the prepossessed data . As noted earlier ( § 2 . 3 ) , Colo does not support arbitrary SQL queries . Rather , it targets simple queries of the form SELECT AGG - OP ( g ( self . data , neighbor . data ) ) FROM neigh ( 1 ) WHERE h ( self . data , neighbor . data ) , where AGG - OP is the SUM , COUNT , or AVG ag - gregation operation , g is a predicate on the data of two neighbors ( including their node and edge data ) , and h is a filter that runs over the same data to compute whether a particular edge will participate in aggregation or not . If A wants to run a query with a GROUP BY operation , A transforms it into several sub - queries and sub - mits them to the servers separately . For example , A converts Q5 in Figure 1 into a series of queries such as SELECT COUNT ( * ) FROM neigh ( 1 ) WHERE 20 < self . age < 30 for the different age groups . The PreProcess function specifies how a device should translate its raw data into the attributes accessed by the SQL query . For example , the analyst A may want to execute Q3 in Figure 1 ( SE - LECT SUM ( edge . contacts ) / COUNT ( * ) FROM neigh ( 1 ) WHERE self . inf & neighbor . inf & ( neighbor . tInf > self . tInf + 2 days ) ) for all infectees during a recent time period , e . g . , March 2023 , as suggested in the lit - erature [ 33 ] . The PreProcess function should specify how to derive the following three attributes for the SQL query : i ) device infection status inf as a binary number ; ii ) the infection timestamp tInf as an integer from 1 to 30 to represent March 1 to 30 ; iii ) edge . contacts , which is the number of interactions two neighbors have had , as a bounded integer , for example , 80 as in the epidemiology litera - ture [ 40 ] . These constraints ( bounds ) are necessary as otherwise malicious devices can supply arbitrary inputs . Once the servers receive A ’s query , they verify it by matching it to a list of certified queries . The servers then sign and broadcast the query ; if a device verifies signatures more than the fraction of servers that can be compromised , it starts query execution . 4 . 3 Local aggregation 4 . 3 . 1 Hiding node and edge data . Recall from the overview ( § 3 ) that the goal of local aggregation is to enable a device to com - pute the query locally just on its neighborhood . This computa - tion further breaks into evaluating the query for every neigh - bor edge of a node . That is , a node needs to evaluate a function F = g ◦ h ( self . data , neighbor . data ) with each of its neighbors and compute (cid:205) neighbor F ( self . data , neighbor . data ) . As an example , for query Q1 the function F equals F ( self . data , neighbor . data ) = self . inf · neighbor . inf . For the moment , assume that we do not need to hide the topol - ogy data at the devices ( we will relax this assumption in the next subsection ) . Then , a natural option for computing F is to use a two - party secure computation protocol such as Yao’s garbled cir - cuit ( Yao’s GC ) [ 83 ] . The neighbor , say , v B , could act as the garbler , generate a garbled circuit , send it to the other node , say , v A , who would act as the evaluator to obtain the result . Since the two nodes must not obtain the result of F in plaintext , the two nodes may compute y = F ( r v A , r v B , v A . inf , v B . inf ) = ( v A . inf · v B . inf ) + r v A + r v B , where r v A , r v B ∈ F are uniformly sampled masks supplied by the two parties to hide the output from each other . ( The field F could be 2 64 , for example . ) At the end of the protocol , v B may store − r v B as its output , while v A may store − r v A + y as its output . The challenge is in preventing malicious behavior efficiently . To protect against a malicious neighbor ( garbler v B ) , Colo would have to use a version of Yao’s GC that employs techniques such as cut - and - choose [ 46 , 47 , 55 ] that prevent a garbler from creating arbitrary circuits , for example , an F ′ that computes , ( v A . inf · 10 6 ) + r v A + r v B . These general - purpose primitives are expensive because they are not tailored for the queries and they offer more than we need : malicious integrity is not our goal ( § 2 . 3 ) . Observe that the predicates F that appear in Colo’s target queries have bounded inputs and outputs . For example , Q1 has two possible inputs of 0 and 1 for v A . inf and thus two possible outputs of 0 and 1 . As another example , if we take Q3 in Figure 1 and assume tInf has 30 possibilities ( for 30 days ) , then the number of possible inputs for v A is sixty . That is , v A ’s input pair ( inf , tInf ) can range from the case ( 0 , 0 ) to the case ( 1 , 29 ) . Corresponding to each of these inputs , an output edge . contacts may be a value in the range [ 0 , 79 ] . Leveraging this observation , the neighbor in Colo ( v B above ) precomputes outputs for all possible inputs of v A into an array T instead of generating them at runtime inside Yao’s GC . One can 6 Local aggregation protocol of Colo ( 1 ) Each device receives a query q containing a function PreProcess , a local computation F , and a bound Bound . PreProcess contains information on how to convert a device’s raw data into inputs for F . It also specifies the set of valid values for each input parameter to F . The bound Bound is the range of valid outputs of F . ( 2 ) Each device participates in this local aggregation protocol . Denote A as the device and B as a neighbor . ( 3 ) Each neighbor B of A does the following locally : ( a ) ( Generate mask ) Samples an element r in a field F uniformly randomly . ( b ) ( Enumerate all inputs of A ) Generates an array s containing all possible inputs of A based on PreProcess . ( c ) ( Compute outputs ) For every s [ i ] , computes T [ i ] = F ( s [ i ] , B in ) , where B in is B ’s input . ( d ) ( Mask outputs and commit to them ) For every T [ i ] , computes T ′ [ i ] = T [ i ] + r . It then samples R [ i ] and generates commitment CM [ i ] = Commit ( T ′ [ i ] , R [ i ] ) . ( e ) ( Prove bound of outputs ) Generates a ZKP that it knows the opening of all commitments and a mask such that each committed value is the addition of the mask and some value bounded by Bound . ( 4 ) Each neighbor B sends commitments CM and the ZKP to A which verifies the ZKP . ( 5 ) ( Function evaluation ) A runs OT with each neighbor B to retrieve ( T ′ [ j ] , R [ j ] ) , where s [ j ] is A ’s input to F . The device A verifies the opening to the commitment , that is , it verifies Commit ( T ′ [ j ] , R [ j ] ) equals CM [ j ] received in the previous step . ( 6 ) If the verifications above pass , A adds T ′ [ j ] to its local aggregation result . Its neighbor device B adds − r to its local aggregation result . Figure 4 : Colo’s local aggregation . view this arrangement as making v B commit to the outputs without looking at v ′ A s input . For instance , for Q3 , v B would generate a T of length 60 where each entry is in the range [ 0 , 79 ] . Then , v B can arrange for v A to get one of these values obliviously . Protocol details . Figure 4 shows the details of Colo’s protocol . For a moment , assume that the nodes v A and v B are honest - but - curious . Then , for every possible input of v A , the neighbor v B generates one entry of array T . It also adds a mask , r ∈ F , to each entry of T . That is , after generating the array T , v B offsets each entry by the same mask and computes T ′ [ i ] = T [ i ] + r , i ∈ [ 0 , len ( T ) ) , where r is private to v B . The node v A then obtains one of the entries of the table corresponding to its input using 1 - out - of - len ( T ) oblivious transfer [ 20 , 67 ] . Finally , v A uploads T [ j ] + r for the global aggregation , and v B uploads − r to cancel the mask . To account for a malicious neighbor v B who tries to break our privacy goal P2 ( § 2 . 3 ) , Colo’s protocol adds a zero - knowledge range proof [ 32 ] . Specifically , v B commits to the values in T and proves that each value is bounded and that each value is offset by the same private mask r . Recall from the setup step ( § 4 . 1 ) that the keys for the ZKP are specific to length len ( T ) of array T . Thus , v B splits up proof generation as needed depending on the binary representation of len ( T ) . After generating the proofs , v B sends them to v A along with one entry T ′ [ j ] of T ′ ( using OT ) as well as the opening for the commitment to T ′ [ j ] . Now , instead of using an OT protocol secure only against an honest - but - curious sender , Colo’s protocol switches to an OT that defends against a malicious sender ( v B ) [ 20 , 61 ] . Colo’s protocol is not general purpose and is not meant to replace Yao’s GC . In particular , its overhead is proportional to the input domain size of the predicate F . Thus , it only works for small input domains . However , its performance benefits are very significant , and allows the devices to keep their overhead low . First , node v B evaluates the predicate F in plaintext rather than inside Yao’s GC . Second , the protocol uses ZKP for range proofs , a computation which has received significant attention in the literature [ 16 , 21 ] . Third , and similar to the point above , the OT primitive is also a basic secure computation primitive that has received much attention on performance optimizations [ 20 , 51 , 61 , 71 ] . 4 . 3 . 2 Hiding topology . The local aggregation protocol so far hides node and edge data ; however , an adversary that can monitor net - work traffic can infer who is running secure computation with whom and thus figure out the topology data for the devices . As noted earlier ( § 3 ) , Colo protects this information by enabling the devices to communicate over a metadata - hiding communication system . In particular , Colo uses a state - of - the - art system called Karaoke [ 41 ] . Here , we briefly review Karaoke as it has significant implications on Colo’s server - side overhead . At a high level , Karaoke relies on the concept of dead drops —a pseudorandom location ( mailbox ) at a server . Say device v A wants to send a message to v B , and they have a shared secret k AB to facilitate this communication ( e . g . , they can agree on this secret when they become neighbors ) . To send a message msg to v B , the device v A first derives a dead drop and picks a server S drop hosting the dead drop , using k AB as the seed of a pseudorandom generator [ 10 , 13 , 84 ] . The device v A then drops ( writes ) the message at the dead drop , while device v B retrieves the message from the dead drop . Of course , the idea of dead drops alone is insufficient , as the dead drop server S drop can see which devices are communicating with the same dead drop . To fix this issue , Karaoke makes the devices access the dead drops via servers , similar to a parallel mixnet [ 19 ] . Suppose the key pairs of two devices v A , v B are ( pk A , sk A ) , ( pk B , sk B ) , and the key pairs for the M servers S i , i ∈ [ 0 , M ) ’s are ( pk i , sk i ) . Then , when the device v A wants to send a message to a dead drop server , it chooses a uniformly random path of m servers , say , S 0 , . . . , S m − 1 , to route the message . Specifically , v A onion encrypts [ 19 ] the message as Enc pk 0 ( . . . Enc pk m − 1 ( Enc pk drop ( msg ) ) ) and sends it to the first hop on the path . When a server on the path receives a message , it peels 7 one layer of the onion and forwards the remaining message to the next server , until the drop server receives it . To protect against malicious servers dropping messages to compromise privacy , the servers add dummy traffic ( noise messages to random dead drops ) . Karaoke adds both significant cpu and network overhead to the servers ( § 6 ) . Despite this overhead , we pick Karaoke for several reasons . First , the overhead is at the server - side , where there is more tolerance for overhead relative to the devices . Second , Karaoke can scale to millions of devices as needed for our scenario . Third , it defends against malicious servers who may duplicate or drop traffic to learn access patterns at the dead drops . And , fourth , it provides a rigorous guarantee of ( 𝜖 , 𝛿 ) differential privacy . For instance , it makes the case that v A and v B are communicating appear indistinguishable from the case when they are idle . In particular , with a route length m = 14 , assuming 80 % of the servers are honest , it guarantees indistinguishability with a probability of 1 − 2 · 10 − 8 . 4 . 4 Global aggregation The global aggregation phase is the last phase in query execution . Recall that at the end of the local aggregation phase , each device has a local result , say y i . For instance , y i equals the output of secure computation T ′ [ j ] in Figure 4 or the mask r added by a device that constructed the array T ′ . The goal of the global aggregation phase is to aggregate these outputs across devices to wrap up the execution of the query . For this aggregation , each device secret shares ( using additive shares in the field F ) its output y i and sends one share to each server . Each server then locally computes (cid:205) i [ y i ] , where [ y i ] is a share it receives . The analyst finally aggregates the results across the servers , that is , computes (cid:205) edge ( T [ j ] + r ) + (cid:205) edge ( − r ) , canceling out the random masks , and obtaining the query output . This simple global aggregation protocol ensures that the local ag - gregation results of an honest device get aggregated with the results of other honest devices exactly once . For instance , if a malicious server drops or duplicates a share [ y i ] supplied by an honest device , then the analyst A learns a uniformly random output , because the shares of [ y i ] at an honest server will not cancel out . Naturally , this protocol guarantees privacy as long as one of the servers is honest , which is satisfied by our assumption of only up to f = 20 % of servers being malicious ( and the rest are honest ) . 5 IMPLEMENTATION We have implemented a prototype of Colo in C + + . Our prototype is approximately 2 , 000 lines of code on top of existing frameworks and libraries . In particular , we use CAF [ 18 ] , an event - driven framework used commonly for building distributed systems . We instantiate Colo’s servers and devices as actors in this framework . The source code will be made available on Github . Server - side details . We use Groth16 [ 32 ] as our underlying zk - SNARK ( ZKP ) scheme and we use BLS12 - 381 as its underlying curve for a 128 - bit security . In the setup phase , our prototype gen - erates a set of ZKP keys in MPC using ZoKrates [ 86 ] . This protocol ensures that the randomness used to generate the keys is not re - vealed to an adversary , and that the generated keys are correct . In the local aggregation phase , servers run Karaoke [ 41 ] to provide Application Dataset # of nodes Social ego - Facebook 4 , 039 Communication Enron email 36 , 692 Collaboration DBLP 317 , 080 Collaboration live journal 3 , 997 , 962 Figure 5 : Commonly used graph datasets from the Stanford Large Network Dataset Collection ( SNAP ) [ 43 ] . metadata - hiding messaging . We follow Karaoke’s default config - uration to generate noise messages to achieve 𝜖 = ln4 , 𝛿 = 10 − 4 differential privacy after 245 rounds of message losses . In the global aggregation , servers accept devices’ additive secret shares of local results and reveal the summation of these results to the analyst . Our prototype uses 2 64 as the underlying field F for secret shares . Device - side details . In the query distribution phase , the devices take as input two functions in C + + , one for the predicate F and the other for the PreProcess transformation function ( § 4 . 2 ) . The devices also take as input a map object indicating the range of each attribute accessed by the query . For the local aggregation phase , recall ( Figure 4 ) that devices need cryptographic tools , particularly , a commitment scheme , a ZKP scheme , and an OT protocol . For commitments , our prototype uses a ZKP - friendly scheme called Poseidon [ 31 ] , by importing the ZKP circuit for this commitment from neptune [ 50 ] into ark _ groth16 [ 4 ] . It also implements a range proof to prove that the values in the array T are in the range bound . For OT , we use the simplestOT protocol [ 20 ] implemented in libOTe [ 64 ] . For enabling the devices to participate in Karaoke ( metadata - hiding communication ) , we use the onion encryption scheme implemented in libsodium [ 37 ] . Finally , for global aggrega - tion , the devices simply sample random numbers in 2 64 to generate the additive secret shares of their local results . 6 EVALUATION Our evaluation focuses on highlighting the device - and server - side overhead of Colo , for a variety of queries , and for different underlying graphs ( the number of nodes and edges , and the degree of these nodes ) . A summary of our main results is as follows : • For 1M devices connected to up to 50 neighbors each , and for all example queries ( Figure 1 ) , Colo’s per - device cost is less than 8 . 4 min of ( single core ) cpu time and 4 . 93 MiB of network transfers . • For the same number of devices and their neighbors as above , Colo’s server - side cost is $ 3 . 95 to $ 37 . 6 per server ( or $ 158 to $ 1 , 504 total for the 40 servers ) , depending on the query . • Colo’s overheads for the example queries are much lower than the state - of - the - art Mycelium , whose device - side cost is 8 . 79 hours ( single core ) cpu time and 5 . 73 GiB network transfers , per query , and the server - side cost is $ 57 , 490 per query . Thus , Colo brings privacy - preserving federated graph analytics into the realm of affordability for certain types of queries . Baselines . We compare Colo to two baseline systems : a federated non - private baseline and the state - of - the - art Mycelium [ 70 ] for privacy - preserving federated graph analytics . Our non - private baseline has a coordinator server . It forwards messages between devices as they typically do not have public IP 8 10 - 8 10 - 6 10 - 4 10 - 2 10 0 10 2 10 4 10 6 1 10 100 1000 C P U ( s ec ) Input domain size MyceliumColoNon - private ( a ) Device cpu 10 0 10 2 10 4 10 6 10 8 1 10 100 1000 N e t w o r k ( K i B ) Input domain size ( b ) Device network Figure 6 : Device - side cost with a varying input domain size len ( T ) of the query predicate . Queries Q1 , Q2 , Q4 , Q7 from Figure 1 have len ( T ) = 2 , Q3 , Q5 , Q6 have len ( T ) ∈ [ 60 , 240 ] , and Q8 has len ( T ) = 240 . and cannot interact directly . Using the coordinator , devices run graph queries in a federated manner ( but they are not privacy - preserving ) : each neighbor v B of v A sends its data to v A in plaintext , who then performs local computation and uploads the result to the coordinator . The coordinator aggregates the results across devices . This baseline is non - private as devices see their neighbors’ data , and the coordinator sees the local results computed at the devices . Besides , this baseline does not consider malicious devices . For a privacy - preserving baseline , we use the state - of - the - art Mycelium [ 70 ] . However , this is challenging as Mycelium only has a partial public implementation [ 59 ] . In particular , the device - side zero - knowledge proof implementation does not include the full proof , and the server - side code does not include the aggregation of computation across devices . To work around these constraints , we report lower - bound estimates for Mycelium . Briefly , however , for device - side cpu , we use a lower - bound estimate of ZKP since it is the dominant device - side cost ( Mycelium performs homomorphic operations over ciphertexts and proves that these operations are correct ) . For the server - side costs and the network overhead , we use a mix of the released code combined with reported numbers from their paper . We emphasize that Mycelium has a different and a stronger threat model than Colo—Mycelium assumes a single byzantine server , while Colo assumes a set of servers ( e . g . , 40 ) of which 20 % can be byzantine . Thus , Mycelium is not a direct comparison ( but the closest in the literature ) , and we use it to situate Colo’s costs , as both Mycelium and Colo operate in strong threat models . Queries and datasets . We measure and report the overhead for the example queries in Figure 1 . Recall that Colo’s overhead depends on the size of the input domain of the query predicate , that is , the length len ( T ) of array T which contains a value for every possible input to the predicate . Thus , we vary len ( T ) . For example , if we assume inf has 2 possible values , tInf is any value between 30 and 120 [ 1 , 11 , 33 , 38 ] , and age has 120 possibilities , the queries Q1 , Q2 , Q4 , Q7 have len ( T ) = 2 , Q3 , Q5 , Q6 have len ( T ) ∈ [ 60 , 240 ] , and Q8 has len ( T ) = 240 . Given these numbers , we vary len ( T ) between 2 and 1000 , to cover our example queries . For the number of nodes and edges in the underlying graph , we take inspiration from several public datasets from the Stanford Large Network Dataset Collection ( SNAP ) [ 43 ] ( Figure 5 ) . In terms of topology , we vary the maximum degree of nodes between 1 and 100 ; setting this maximum degree is necessary , as each device must communicate with a fixed number of nodes ( with itself if it does not have enough neighbors ) to hide its topology . Testbed & methodology . Our testbed is Amazon EC2 and we use machines of type c5 . 4xlarge to run the devices and the servers . Each such machine has 16 cores , 32 GiB memory , and costs $ 0 . 328 per hour [ 6 ] . For experiements , we don’t run all devices , e . g . , 1M , in one go , which would require thousands of machines . Instead , we sequentially run batches of 1K devices each . In a batch , each device connects to e . g . 50 neighbors . This is acceptable as a device’s overhead only depends on its neighborhood . Servers have a fixed - cost ( dummy / noise messages ; described in § 6 . 3 ) for 1M devices , plus incremental costs per - device . We measure and aggregate these separately . The one - time setup phase cost is not included in our experi - ments because our focus is the per - query cost . For completeness , we estimate this cost : the server - side cost is in the order of minutes of cpu time and a few GiBs in network transfers [ 14 , 86 ] and the device - side cost is 102 MiB ( § 4 . 1 ) . 6 . 1 Overhead for different queries This section evaluates and compares the overhead of Colo and the baselines for different queries , that is , different sizes of input domain len ( T ) of the query predicate . More precisely , we fix the query predicate to F ( v A . input , v B . input ) = v A . input · v B . input , that is , the predicate for query Q1 , and set different input sets for v A . input . For instance , v A . input ∈ [ 0 , 1 ] fixes len ( T ) = 2 , v A . input ∈ [ 0 , 9 ] fixes len ( T ) = 10 , and so on . For these experiments , we fix the number of devices to 1M where each device has at most 50 neighbors . Device - side overhead . Figure 6 shows per - device cpu and network overhead as a function of len ( T ) . Colo’s overhead for len ( T ) = 2 ( i . e . , query Q1 , Q2 , Q4 , and Q7 in Figure 1 ) is 10 . 27 s and 415 KiB , and increases to 8 . 42 min and 4 . 93 MiB for len ( T ) = 256 ( this covers all queries in Figure 1 ) , and 29 . 9 min and 18 . 12 MiB for len ( T ) = 1000 . Colo incurs significantly more overhead than the non - private baseline . For instance , for len ( T ) = 256 , the baseline’s overhead is 0 . 003 s and 6 . 36 KiB per device , while Colo’s overhead is 1 . 68 · 10 5 and 775 times higher ( 505 . 1 s and 4 . 93 MiB ) for the cpu and the network , respectively . This is because the non - private baseline does 9 10 - 6 10 - 4 10 - 2 10 0 10 2 10 4 10 6 1 10 100 1000 C P U ( hou r ) Input domain size Mycelium ( d = 10 ) Colo ( d = 50 ) Non - private ( d = 50 ) ( a ) Server cpu 10 - 4 10 - 2 10 0 10 2 10 4 10 6 10 8 1 10 100 1000 N e t w o r k ( G i B ) Input domain size MyceliumColoNon - private ( b ) Server network Figure 7 : Total server - side cost for Colo ( across its 40 servers combined ) and the baselines with a varying input domain size len ( T ) of the query predicate . The total number of devices is 1M and the degree of each is 50 ( except for Mycelium’s cpu cost where it is 10 ) . the involved cryptographic operations , and devices simply share their data with their neighbors , who perform local computations in plaintext . In contrast , in Colo a device’s overhead is dominated by the cost of running the local aggregation protocol ( § 4 . 3 . 1 ) that involves primitives such as commitments , ZKP , and OT ( Figure 4 ) . However , relative to Mycelium , Colo’s overhead are significantly lower . For instance , for len ( T ) = 256 , a device in Mycelium requires 31 , 650 s cpu time ( 8 . 79 h ) and 5 . 73 GiB network transfers , while Colo’s 505 . 1 s and 4 . 93 MiB is 62 . 6 × and 1 . 16 · 10 3 × lower . Mycelium’s costs are higher because of various reasons . First , it performs local aggregation using homomorphic encryption and ZKP . For instance , devices multiply ciphertexts containing the inputs of the neighbors , and prove that this multiplication is correct using a ZKP ( that a ma - licious device did not manipulate the computation to reveal another device’s input ) . Second , the homomorphic encryption ciphertexts are large ( e . g . , 4 . 3 MiB ) as Mycelium must choose large parameters for homomorphic encryption to support ciphertext - ciphertext mul - tiplications . This means that the circuit to do the proof is also large , thereby requiring hundreds of seconds of proving time per cipher - text multiplication . Third , a Mycelium device must participate in a verification protocol for global aggregation ( besides participating in local aggregation ) to check the byzantine aggregator’s work . And , fourth , Mycelium also instantiates a mixnet over the devices to hide their topology data , again contributing to the device overhead . In contrast to Mycelium , Colo’s cost is dominated by local aggregation only as the metatdata - hiding communication is instantiated over the resourceful servers , and the global aggregation is also light - weight ( § 3 , § 4 . 3 . 2 , § 4 . 4 ) . Further , the dominant local aggregation requires ZKP - friendly commitments and range proofs alongside OT , primitives that have been optimized in the literature ( § 4 . 3 . 1 ) . Although Colo’s overheads are lower , they increase with len ( T ) as the number of cryptographic operations in Colo’s local aggrega - tion protocol depend linearly on len ( T ) . For instance , the cpu time of a Colo device is dominated by the time to generate the ZKP—that each entry of T is within a range . This cost depends on the number of entries , with the proof for a single entry taking 0 . 04 s . Similarly , a Colo device’s network overhead increases approximately linearly with len ( T ) ; for instance , a device sends one commitment per entry of T . In contrast , the overheads for the baselines do not depend on len ( T ) as their computation model multiplies v A . input · v B . input directly , rather than enumerating all possible outputs . Thus , Colo’s overheads will surpass that of Mycelium for a large len ( T ) . Overall , for Colo’s target queries , its device - side overheads ( in the range of a few seconds to a few minutes in cpu time , and a few hundred KiBs to a few MiBs in network transfers ) appear to be in the realm of practicality for modern mobile devices . Server - side overhead . Figure 7 shows the server - side overhead for Colo and the baseline systems . Colo’s server - side cost for len ( T ) = 2 is 88 h cpu time ( on a single core ) and 214 . 78 GiB per server , increases to 147 . 36 h and 3 . 46 TiB for len ( T ) = 256 , and 224 . 17 h and 12 . 79 TiB for len ( T ) = 1000 . As with the device - side overhead , Colo’s costs are significantly higher than the non - private baseline , whose server incurs 58 . 6 min cpu time and 6 . 36 GiB network transfers . The baseline server only needs to forward 32 - bit messages from devices to other devices ; in contrast , Colo’s servers are responsible for hiding topology . They not only forward onion - encrypted messages from the devices over m = 28 hops , but also generate noise messages as part of the Karaoke system to protect against malicious servers ( § 4 . 3 . 2 ) . Relative to Mycelium , Colo’s server - side cpu is comparable , while the network is significantly lower . ( For Mycelium’s server - side cpu , since we take numbers from their paper we are only able to report for a degree bound of 10 , which is what they experiment with . ) For instance , for len ( T ) = 256 , the server - side cost for Mycelium is 1304 h and 5737 TiB , while it is 5894 h and 138 . 4 TiB for Colo’s 40 servers combined . The cpu cost for Mycelium is dominated by the cost to verify ZKPs ( for homomorphic operations ) and per - form global aggregation ( over its byzantine server ) , while the cpu cost for Colo is dominated by the time to process onion - encrypted messages . Both overhead turn out to be comparable . Meanwhile , the network overhead due to these dominant operations is lower in Colo . Although Colo’s server - side costs are substantial , since servers are resourceful ( e . g . , the cpu time can be split across many cores ) , we consider it affordable . We further assess this affordability by calculating the dollar cost of renting the servers . We take both the cpu and network cost and convert it into a dollar cost per query using the following pricing model . Each c5 . 4xlarge machine on AWS costs $ 0 . 328 per hour and has 16 cores and 32 GiB memory [ 6 ] . We assume that all hourly cost is due to the cpu , and compute the cpu cost per hour to be $ 0 . 0205 . For the network cost , 10 10 - 8 10 - 6 10 - 4 10 - 2 10 0 10 2 10 4 10 6 1 20 40 60 80 100 C P U ( s ec ) Device degree bound MyceliumColoNon - private ( a ) Device cpu 10 - 2 10 0 10 2 10 4 10 6 10 8 1 20 40 60 80 100 N e t w o r k ( K i B ) Device degree bound ( b ) Device network Figure 8 : Device - side costs with a varying node degree . These experiments set N = 1 M devices and input domain len ( T ) = 256 for the query . we set the cost of transferring one GiB to $ 0 . 01 according to the typical bulk network pricing of cloud providers [ 5 , 7 ] . Applying this pricing model , Colo’s dollar cost per query ( over 1M devices ) ranges from $ 3 . 95 to $ 37 . 6 per server , or $ 158 to $ 1504 total for the 40 servers , depending on the query . In contrast , the non - private baseline costs $ 0 . 08 and Mycelium costs $ 57 , 490 per query . These figures for Colo are substantial but within the reach of the budget of an entity like CDC . For instance , running the cheapest query ( which includes the superspreader query ) every two weeks will cost less than $ 4K annually . 6 . 2 Overhead with the degree of devices In this subsection , we evaluate how the device - side costs change with the degree bound ( the maximum degree of a node ) . Recall that setting a maximum degree is necessary , as each device must communicate with a fixed number of nodes ( with itself if it does not have enough neighbors ) to hide the communication pattern . Figure 8 shows the device - side cpu and network costs for the three systems for degree bounds of 1 , 10 , 50 , and 100 , for N = 1M devices and len ( T ) = 256 . We report only the device - side costs here because the server - side costs change with the total number of devices , and increasing the degree bound is equivalent to increasing the number of devices from a server’s point of view ( § 6 . 3 ) . When the degree bound is 10 , Colo’s devices spend 1 . 68 min in cpu and 989 . 77 KiB in network transfers . This cost increases to 16 . 8 min cpu and 9 . 85 MiB network with a degree bound of 100 . The costs increase and decrease with the degree bound because each device participates in one instance of secure computation ( Figure 4 ) per neighbor . This linear dependence is present even for the baselines . For instance , Mycelium’s per - device costs are 1 . 75 h and 1 . 16 GiB for a degree bound of 10 , and 17 . 5 h and 11 . 16 GiB for a degree bound of 100 . Overall , Colo can scale to a significant degree bound ( e . g . , 10 or 50 ) with affordable costs for the devices . 6 . 3 Overhead with the number of devices This subsection evaluates how the three systems scale with the total number of devices . Specifically , we set the number of de - vices ( nodes ) from our example datasets ( Figure 5 ) , that is , N ∈ { 4 K , 36 . 6 K , 317 K , 3 . 99 M } . Wealsoconsider N = 1 M . Wefix len ( T ) = 256 and the maximum degree of a node to 10 in alignment with Mycelium’s default setting for a better comparison to its server - side cpu costs ( § 6 . 1 ) . Further , since the device - side costs do not depend on the number of devices , rather just the configuration of their local neighborhood , we focus on the server - side costs . Figure 9 shows these costs , that is , cpu time , network trans - fers , and dollar cost for the servers . Colo’s cost is 3 . 37 h cpu time , 82 . 15 GiB network transfers , and 0 . 89 dollars , per server , for N = 4039 devices , increases to 30 . 74 h cpu time , 758 . 09 GiB network transfers , and 8 . 21 dollars per server for N = 1 M devices , and fur - ther increases to 116 . 67 h cpu time , 2 . 79 TiB network transfers , and 30 . 29 dollars per server for N = 3 . 99 M devices . The server - side costs for Colo ( and the baselines ) increases with the number of devices . This is expected as the servers do more work for more devices : send more onion - encrypted messages in the case of Colo , route more plaintext messages in the case of the non - private baseline , and operate over more homomorphic encryption ciphertexts ( e . g . , in global aggregation ) for Mycelium . However , Colo’s cost is flatter and higher for a lower N . This is because even for a small number of devices , Colo’s servers must generate and process several million noise messages ( § 4 . 3 . 2 ) as required by Karaoke to protect against malicious servers dropping traffic to learn access patterns to dead drops . For instance , for N = 4039 devices , 3 . 25 h ( 96 . 4 % ) and 79 . 41 GiB ( 96 . 7 % ) of the server - side cost in Colo comes from the noise messages . However , this fixed cost becomes less significant as the number of devices increases , thus bringing Colo’s costs lower than Mycelium . For instance , Colo incurs 35 . 6 dollars in total for its 40 servers relative to 48 dollars in Mycelium ( 1 . 34 × ) for N = 4 K , but 1211 . 6 dollars versus 47 , 690 dollars for Mycelium ( 39 . 3 × ) when N equals 3 . 99 M . Overall , if we consider the dollar cost per server as a key metric for Colo’s costs , then Colo appears to scale to several million devices . However , scaling it beyond these values would require further optimizations and refinements at its servers . 7 RELATED WORK 7 . 1 Graph analytics with a single data owner A long line of research in graph analytics [ 3 , 26 , 39 , 44 , 48 , 49 , 52 , 53 , 58 , 73 , 77 – 81 ] focuses on outsourcing where a single data owner outsources computations over its graph to one untrusted server or a set of non - colluding servers . One common goal for these works is to hide the graph data from the server ( s ) . For this purpose , they use different cryptographic tools or assumptions . 11 10 - 4 10 - 2 10 0 10 2 10 4 10 3 10 4 10 5 10 6 10 7 C P U ( hou r ) # of devices MyceliumColoNon - private ( a ) Server cpu 10 - 2 10 0 10 2 10 4 10 6 10 8 10 3 10 4 10 5 10 6 10 7 N e t w o r k ( G i B ) # of devices ( b ) Server network 10 - 4 10 - 2 10 0 10 2 10 4 10 6 10 3 10 4 10 5 10 6 10 7 D o ll a r c o s t ( $ ) # of devices ( c ) Dollar cost Figure 9 : Total server - side cost for Colo ( across its 40 servers combined ) and the baselines with a varying number of devices N . The degree of each device is 10 and the input domain size of the query predicate is set to len ( T ) = 256 . Flare [ 44 ] , for instance , outsources computation to servers with a trusted execution environment ( TEE ) , e . g . , Intel SGX . Specifically , it runs Spark - like computation inside TEE , which can support the GraphX interface . Other systems [ 26 , 39 , 48 , 49 , 79 , 81 ] use ho - momorphic encryption instead of TEEs to run a variety of graph queries on an encrypted graph , including search [ 39 , 79 ] and short - est distance computations [ 48 , 49 ] . In the multiple - server setting , a common technique is to use secret sharing and multiparty computa - tion to hide the graph from the servers [ 3 , 52 , 53 , 58 , 73 , 77 , 80 ] . For instance , GraphSC [ 58 ] assumes two honest - but - curious servers and supports parallel execution of secure computation for a broad class of tasks . While GraphSC targets honest - but - curious servers , MAGO [ 78 ] , which is designed for subgraph counting , assumes three servers where one can be malicious . The biggest difference between Colo and these existing works is the problem setting . While these prior works target a single data owner who outsources computation , Colo targets a setting with many data owners ( e . g . , a million devices ) . Naturally , in Colo’s setting it is natural to consider the threat of malicious devices . This difference in setting and threat model leads to a vastly different system architecture ( § 3 ) and protocol design ( § 4 ) . 7 . 2 Federated graph analytics Private data federation [ 8 , 9 , 36 , 54 , 57 , 65 ] focuses on a scenario where a set of data owners hold private relational data and a query coordinator orchestrates SQL queries on this data . The goal is to keep the sensitive data of the data owners private . Private data federation has two differences with Colo . First , they natively target the relational data model while Colo targets the graph data model . ( We include these works here because sometimes graph queries can be expressed in the relational model . ) Second , these existing works target a scenario with a few data owners ( e . g . , a few tens ) where each holds a significant partition of the relational data . For instance , Senate [ 65 ] experiments with 16 parties . In contrast , Colo targets millions of participants where each has a small amount of data . Third , while most works in private data federation consider parties to be honest - but - curious , it is natural in Colo to consider malicious parties given their number . DStress [ 62 ] focuses on graph queries and a larger number of participants : a few thousand . The key use case is understanding systemic risk for financial institutions , which requires analyzing inter - dependencies across institutions . However , DStress also as - sumes honest - but - curious participants as financial institutions are heavily regulated and audited , and thus unlikely to be malicious . Gunther et al . [ 34 , 35 ] consider malicious devices alongside a set of semi - honest servers to answer epidemiological queries such as an estimate of the change in the number infections if schools are closed for 14 days . However , they are limited to secure aggregation across neighbors’ data and their protocol does not hide the result of local aggregation . Mycelium [ 70 ] is the closest related work to Colo . It supports a broad set of queries , targets a large number of devices and assumes they can be malicious . However , as discussed earlier ( § 2 . 4 ) and evaluated empirically ( § 6 ) , its costs are very high . Colo is a more affordable alternative in a strong threat model , at least for the subset of queries that Colo targets . 8 SUMMARY Privacy - preserving federated graph analytics is an important prob - lem as graphs are natural in many contexts . It specifically is appeal - ing because it keeps raw data at the devices ( without centralizing ) and does not release any intermediate computation result except for the final query result . However , the state - of - the - art prior work for this problem is expensive , especially for the devices that have constrained resources . We presented Colo , a new system that oper - ates in a strong threat model while considering malicious devices . Colo addresses the challenge of gaining on efficiency through a new secure computation protocol that allows devices to compute privately and efficiently with their neighbors while hiding node , edge , and topology data ( § 4 . 3 . 1 , § 4 . 3 . 1 , § 3 ) . The per - device overhead in Colo is a few minutes of cpu and a few MiB in network transfers , while the server’s overhead ranges from several dollars to tens of dollars per server , per query ( § 6 ) . Our conclusion is that Colo brings privacy - preserving federated graph analytics into the realm of practicality for a certain class of queries . 12 REFERENCES [ 1 ] D . C . Adam , P . Wu , J . Y . Wong , E . H . Lau , T . K . Tsang , S . Cauchemez , G . M . Leung , and B . J . Cowling . Clustering and superspreading potential of SARS - CoV - 2 infections in Hong Kong . Nature Medicine , 2020 . [ 2 ] Anthem is warning consumers about its huge data breach . https : / / www . latimes . com / business / la - fi - mh - anthem - is - warning - consumers - 20150306 - column . html , 2015 . [ 3 ] T . Araki , J . Furukawa , K . Ohara , B . Pinkas , H . Rosemarin , and H . Tsuchida . Secure graph analysis at scale . In ACM Conference on Computer and Communications Security ( CCS ) , 2021 . [ 4 ] arkworks . ark - groth16 . https : / / github . com / arkworks - rs / groth16 . [ 5 ] AWS . Amazon EC2 On - Demand Pricing . https : / / aws . amazon . com / ec2 / pricing / on - demand / . [ 6 ] AWS . Compute Savings Plans - Amazon Web Services . https : / / aws . amazon . com / savingsplans / compute - pricing / . [ 7 ] Azure . Pricing - Bandwidth | Microsoft Azure . https : / / azure . microsoft . com / en - us / pricing / details / bandwidth / . [ 8 ] J . Bater , G . Elliott , C . Eggen , S . Goel , A . Kho , and J . Rogers . SMCQL : Secure querying for federated databases . International Conference on Very Large Data Bases ( VLDB ) , 2017 . [ 9 ] J . Bater , X . He , W . Ehrich , A . Machanavajjhala , and J . Rogers . Shrinkwrap : efficient SQL query processing in differentially private data federations . International Conference on Very Large Data Bases ( VLDB ) , 2018 . [ 10 ] D . J . Bernstein . ChaCha , a variant of Salsa20 . In Workshop record of SASC , 2008 . [ 11 ] Q . Bi , Y . Wu , S . Mei , C . Ye , X . Zou , Z . Zhang , X . Liu , L . Wei , S . A . Truelove , T . Zhang , et al . Epidemiology and transmission of COVID - 19 in 391 cases and 1286 of their close contacts in Shenzhen , China : A retrospective cohort study . The Lancet infectious diseases , 2020 . [ 12 ] G . R . Blakley . Safeguarding cryptographic keys . In Workshop on Managing Requirements Knowledge ( MARK ) , 1979 . [ 13 ] M . Blum and S . Micali . How to generate cryptographically strong sequences of pseudo random bits . In Providing Sound Foundations for Cryptography : On the Work of Shafi Goldwasser and Silvio Micali , pages 227 – 240 , 2019 . [ 14 ] S . Bowe , A . Gabizon , and I . Miers . Scalable multi - party computation for zk - SNARK parameters in the random beacon model . Cryptology ePrint Archive , 2017 . [ 15 ] G . Brassard , D . Chaum , and C . Crépeau . Minimum disclosure proofs of knowledge . Journal of computer and system sciences , 1988 . [ 16 ] B . Bünz , J . Bootle , D . Boneh , A . Poelstra , P . Wuille , and G . Maxwell . Bulletproofs : Short proofs for confidential transactions and more . In IEEE Symposium on Security and Privacy ( S & P ) , pages 315 – 334 , 2018 . [ 17 ] Centers for Disease Control and Prevention . https : / / www . cdc . gov / . [ 18 ] D . Charousset , R . Hiesgen , and T . C . Schmidt . Revisiting Actor Programming in C + + . Computer Languages , Systems & Structures , 45 : 105 – 131 , April 2016 . [ 19 ] D . L . Chaum . Untraceable electronic mail , return addresses , and digital pseudonyms . Communications of the ACM , 24 ( 2 ) : 84 – 90 , 1981 . [ 20 ] T . Chou and C . Orlandi . The simplest protocol for oblivious transfer . In International Conference on Cryptology and Information Security in Latin America , 2015 . [ 21 ] H . Chung , K . Han , C . Ju , M . Kim , and J . H . Seo . Bulletproofs + : Shorter proofs for a privacy - enhanced distributed ledger . IEEE Access , 10 : 42067 – 42082 , 2022 . [ 22 ] CovidSafe . https : / / covidsafe . cs . washington . edu / . [ 23 ] L . Danon , J . M . Read , T . A . House , M . C . Vernon , and M . J . Keeling . Social encounter networks : Characterizing Great Britain . Proceedings of the Royal Society B : Biological Sciences , 2013 . [ 24 ] W . - Y . Day , N . Li , and M . Lyu . Publishing graph degree distribution with node differential privacy . In ACM SIGMOD , 2016 . [ 25 ] X . Ding , X . Zhang , Z . Bao , and H . Jin . Privacy - preserving triangle counting in large graphs . In The Conference on Information and Knowledge Management ( CIKM ) , 2018 . [ 26 ] M . Du , S . Wu , Q . Wang , D . Chen , P . Jiang , and A . Mohaisen . GraphShield : Dynamic large graphs for secure queries with forward privacy . IEEE Transactions on Knowledge and Data Engineering , 34 ( 7 ) : 3295 – 3308 , 2022 . [ 27 ] J . Fan and F . Vercauteren . Somewhat practical fully homomorphic encryption . IACR Cryptol . ePrint Arch . , 2012 . [ 28 ] J . Gehrke , E . Lui , and R . Pass . Towards privacy for social networks : A zero - knowledge based definition of privacy . In Theory of Cryptography Conference ( TCC ) , 2011 . [ 29 ] S . Goldwasser , S . Micali , and C . Rackoff . The knowledge complexity of interactive proof - systems . In ACM Symposium on Theory of Computing ( STOC ) , 1985 . [ 30 ] J . E . Gonzalez , R . S . Xin , A . Dave , D . Crankshaw , M . J . Franklin , and I . Stoica . GraphX : Graph processing in a distributed dataflow framework . In USENIX Symposium on Operating Systems Design and Implementation ( OSDI ) , 2014 . [ 31 ] L . Grassi , D . Khovratovich , C . Rechberger , A . Roy , and M . Schofnegger . Poseidon : A new hash function for Zero - Knowledge proof systems . In USENIX Security Symposium , 2021 . [ 32 ] J . Groth . On the size of pairing - based non - interactive arguments . In Annual International Conference on the Theory and Applications of Cryptographic Techniques ( EUROCRYPT ) , pages 305 – 326 , 2016 . [ 33 ] D . F . Gudbjartsson , A . Helgason , H . Jonsson , O . T . Magnusson , P . Melsted , G . L . Norddahl , J . Saemundsdottir , A . Sigurdsson , P . Sulem , A . B . Agustsdottir , et al . Spread of SARS - CoV - 2 in the Icelandic population . New England Journal of Medicine , 2020 . [ 34 ] D . Günther , M . Holz , B . Judkewitz , H . Möllering , B . Pinkas , T . Schneider , and A . Suresh . Poster : Privacy - preserving epidemiological modeling on mobile graphs . In Proceedings of the ACM SIGSAC Conference on Computer and Communications Security , 2022 . [ 35 ] D . Günther , M . Holz , B . Judkewitz , H . Möllering , B . Pinkas , T . Schneider , and A . Suresh . Privacy - preserving epidemiological modeling on mobile graphs . arXiv preprint arXiv : 2206 . 00539 , 2022 . [ 36 ] F . Han , L . Zhang , H . Feng , W . Liu , and X . Li . Scape : Scalable collaborative analytics system on private database with malicious security . In IEEE International Conference on Data Engineering ( ICDE ) , 2022 . [ 37 ] libsodium . https : / / github . com / jedisct1 / libsodium . [ 38 ] Q . - L . Jing , M . - J . Liu , Z . - B . Zhang , L . - Q . Fang , J . Yuan , A . - R . Zhang , N . E . Dean , L . Luo , M . - M . Ma , I . Longini , et al . Household secondary attack rate of COVID - 19 and associated determinants in Guangzhou , China : A retrospective cohort study . The Lancet Infectious Diseases , 2020 . [ 39 ] S . Lai , X . Yuan , S . - F . Sun , J . K . Liu , Y . Liu , and D . Liu . Graphse 2 : An encrypted graph database for privacy - preserving social search . In ACM ASIA Conference on Computer and Communications Security ( CCS ) , 2019 . [ 40 ] R . Laxminarayan , B . Wahl , S . R . Dudala , K . Gopal , C . Mohan B , S . Neelima , K . Jawahar Reddy , J . Radhakrishnan , and J . A . Lewnard . Epidemiology and transmission dynamics of COVID - 19 in two Indian states . Science , 2020 . [ 41 ] D . Lazar , Y . Gilad , and N . Zeldovich . Karaoke : Distributed private messaging immune to passive traffic analysis . In USENIX Symposium on Operating Systems Design and Implementation ( OSDI ) , pages 711 – 725 , 2018 . [ 42 ] D . Lazar , Y . Gilad , and N . Zeldovich . Yodel : Strong metadata security for voice calls . In ACM Symposium on Operating Systems Principles ( SOSP ) , 2019 . [ 43 ] J . Leskovec and A . Krevl . SNAP Datasets : Stanford large network dataset collection . http : / / snap . stanford . edu / data , June 2014 . [ 44 ] X . Li , F . Li , and M . Gao . Flare : A fast , secure , and memory - efficient distributed analytics framework . International Conference on Very Large Data Bases ( VLDB ) , 2023 . [ 45 ] Y . Li , M . Purcell , T . Rakotoarivelo , D . Smith , T . Ranbaduge , and K . S . Ng . Private graph data release : A survey . ACM Computing Surveys , 2023 . [ 46 ] Y . Lindell and B . Pinkas . An efficient protocol for secure two - party computation in the presence of malicious adversaries . In Annual International Conference on the Theory and Applications of Cryptographic Techniques ( EUROCRYPT ) , 2007 . [ 47 ] Y . Lindell and B . Pinkas . Secure two - party computation via cut - and - choose oblivious transfer . Journal of Cryptology , 2012 . [ 48 ] C . Liu , L . Zhu , X . He , and J . Chen . Enabling privacy - preserving shortest distance queries on encrypted graph data . IEEE Transactions on Dependable and Secure Computing , 2018 . [ 49 ] Y . Luo , D . Wang , S . Fu , M . Xu , Y . Chen , and K . Huang . Approximate shortest distance queries with advanced graph analytics over large - scale encrypted graphs . In International Conference on Mobility , Sensing and Networking ( MSN ) , 2022 . [ 50 ] Neptune . https : / / github . com / lurk - lab / neptune . [ 51 ] D . Mansy and P . Rindal . Endemic oblivious transfer . In Proceedings of the ACM SIGSAC Conference on Computer and Communications Security , pages 309 – 326 , 2019 . [ 52 ] S . Mazloom and S . D . Gordon . Secure computation with differentially private access patterns . In ACM Conference on Computer and Communications Security ( CCS ) , 2018 . [ 53 ] S . Mazloom , P . H . Le , S . Ranellucci , and S . D . Gordon . Secure parallel computation on national scale volumes of data . In USENIX Security Symposium , 2020 . [ 54 ] F . D . McSherry . Privacy integrated queries : An extensible platform for privacy - preserving data analysis . In ACM SIGMOD , 2009 . [ 55 ] P . Mohassel and B . Riva . Garbled circuits checking garbled circuits : More efficient and secure two - party computation . In Advances in Cryptology—CRYPTO , 2013 . [ 56 ] J . Mossong , N . Hens , M . Jit , P . Beutels , K . Auranen , R . Mikolajczyk , M . Massari , S . Salmaso , G . S . Tomba , J . Wallinga , et al . Social contacts and mixing patterns relevant to the spread of infectious diseases . PLoS medicine , 2008 . [ 57 ] A . Narayan and A . Haeberlen . DJoin : Differentially private join queries over distributed databases . In USENIX Symposium on Operating Systems Design and Implementation ( OSDI ) , 2012 . [ 58 ] K . Nayak , X . S . Wang , S . Ioannidis , U . Weinsberg , N . Taft , and E . Shi . GraphSC : Parallel secure computation made easy . In IEEE Symposium on Security and Privacy ( S & P ) , 2015 . [ 59 ] K . Newatia . Mycelium . https : / / github . com / karannewatia / Mycelium . 13 [ 60 ] B . Nikolay , H . Salje , M . J . Hossain , A . D . Khan , H . M . Sazzad , M . Rahman , P . Daszak , U . Ströher , J . R . Pulliam , A . M . Kilpatrick , et al . Transmission of Nipah virus—14 years of investigations in Bangladesh . New England Journal of Medicine , 2019 . [ 61 ] M . Orrù , E . Orsini , and P . Scholl . Actively secure 1 - out - of - N OT extension with application to private set intersection . Cryptology ePrint Archive , Paper 2016 / 933 , 2016 . [ 62 ] A . Papadimitriou , A . Narayan , and A . Haeberlen . Dstress : Efficient differentially private computations on distributed data . In ACM European Conference on Computer Systems ( EuroSys ) , 2017 . [ 63 ] Y . J . Park , Y . J . Choe , O . Park , S . Y . Park , Y . - M . Kim , J . Kim , S . Kweon , Y . Woo , J . Gwack , S . S . Kim , et al . Contact tracing during coronavirus disease outbreak , South Korea , 2020 . Emerging infectious diseases , 2020 . [ 64 ] L . R . Peter Rindal . libOTe : an efficient , portable , and easy to use Oblivious Transfer Library . https : / / github . com / osu - crypto / libOTe . [ 65 ] R . Poddar , S . Kalra , A . Yanai , R . Deng , R . A . Popa , and J . M . Hellerstein . Senate : A maliciously - secure MPC platform for collaborative analytics . In USENIX Security Symposium , 2021 . [ 66 ] Premera Blue Cross Says Data Breach Exposed Medical Data . https : / / www . nytimes . com / 2015 / 03 / 18 / business / premera - blue - cross - says - data - breach - exposed - medical - data . html , 2015 . [ 67 ] M . O . Rabin . How to exchange secrets with oblivious transfer . Cryptology ePrint Archive , 2005 . [ 68 ] Recent Cyber Attacks & Data Breaches In 2023 . https : / / purplesec . us / security - insights / data - breaches / . [ 69 ] R . L . Rivest , L . Adleman , M . L . Dertouzos , et al . On data banks and privacy homomorphisms . Foundations of secure computation , 1978 . [ 70 ] E . Roth , K . Newatia , Y . Ma , K . Zhong , S . Angel , and A . Haeberlen . Mycelium : Large - scale distributed graph queries with differential privacy . In ACM Symposium on Operating Systems Principles ( SOSP ) , 2021 . [ 71 ] L . Roy . SoftSpokenOT : Communication - computation tradeoffs in OT extension . Cryptology ePrint Archive , Paper 2022 / 192 , 2022 . [ 72 ] A . Shamir . How to share a secret . Communications of the ACM , 22 ( 11 ) : 612 – 613 , 1979 . [ 73 ] S . Sharma , J . Powers , and K . Chen . PrivateGraph : Privacy - preserving spectral analysis of encrypted graphs in the cloud . IEEE Transactions on Knowledge and Data Engineering , 2018 . [ 74 ] SwissCovid . https : / / www . bag . admin . ch / bag / en / home / krankheiten / ausbrueche - epidemien - pandemien / aktuelle - ausbrueche - epidemien / novel - cov / swisscovid - app - und - contact - tracing . html . [ 75 ] TraceTogether . https : / / www . tracetogether . gov . sg / . [ 76 ] I . Vojinovic . Data breach statistics that will make you think twice before filling out an online form . https : / / dataprot . net / statistics / data - breach - statistics / . [ 77 ] S . Wang , Y . Zheng , X . Jia , H . Huang , and C . Wang . OblivGM : Oblivious attributed subgraph matching as a cloud service . IEEE Transactions on Information Forensics and Security , 2022 . [ 78 ] S . Wang , Y . Zheng , X . Jia , Q . Wang , and C . Wang . Mago : Maliciously secure subgraph counting on decentralized social graphs . IEEE Transactions on Information Forensics and Security , 2023 . [ 79 ] S . Wang , Y . Zheng , X . Jia , and X . Yi . Pegraph : A system for privacy - preserving and efficient search over encrypted social graphs . IEEE Transactions on Information Forensics and Security , 2022 . [ 80 ] S . Wang , Y . Zheng , X . Jia , and X . Yi . Privacy - preserving analytics on decentralized social graphs : The case of eigendecomposition . IEEE Transactions on Knowledge and Data Engineering , 2022 . [ 81 ] P . Xie and E . Xing . CryptGraph : Privacy preserving graph analytics on encrypted graph . arXiv preprint arXiv : 1409 . 5021 , 2014 . [ 82 ] M . Yagisawa . Fully homomorphic encryption without bootstrapping . Cryptology ePrint Archive , 2015 . [ 83 ] A . C . Yao . Protocols for secure computations . In Annual Symposium on Foundations of Computer Science ( SFCS ) , pages 160 – 164 , 1982 . [ 84 ] A . C . Yao . Theory and application of trapdoor functions . In Annual Symposium on Foundations of Computer Science ( SFCS ) , 1982 . [ 85 ] A . C . Yao . How to generate and exchange secrets . In Annual Symposium on Foundations of Computer Science ( SFCS ) , 1986 . [ 86 ] ZoKrates . https : / / zokrates . github . io / . 14