European Journal of Physics Eur . J . Phys . 42 ( 2021 ) 055701 ( 19pp ) https : / / doi . org / 10 . 1088 / 1361 - 6404 / ac0321 Utilizing peer teaching and reflection on low - stakes quizzes to improve concept learning outcomes in introductory calculus - based physics classes Jessica E Bickel ∗ , Leah M Bunnell and Thijs Heus Department of Physics , Cleveland State University , Cleveland , OH 44115 - 2214 , United States of America E - mail : j . e . bickel @ csuohio . edu Received 22 October 2020 , revised 19 March 2021 Accepted for publication 19 May 2021 Published 14 June 2021 Abstract There are a number of different research - based teaching methods that instruc - tors can use to improve student learning outcomes . This work examines a mod - ification of peer quizzing that introduces a reflection element . Students take individual quizzes and then are allowed to redo part of the quiz while working with a partner . By allowing the students to choose which problems they will redo , the students must pause and reflect on which concepts ( problems ) they do and do not feel confident about . The results show that there is a clear benefit to introducing weekly quizzes as measured by the Hake gain score on concept assessment tests . Further , a multivariate least squares regression indicates that a student’s gain score can be predicted by their exam mean , quiz mean , and their mean improvement on the quiz redo . Thus , students who score above average on their quiz redos will also show above average concept assessment gains . An analysis of when and how the students are changing their answers demonstrates that students across the spectrum ( both high - and low - achieving ) benefit from the quiz redo and that all students are engaged in teaching their peers . Finally , we see evidence that it is not single students teaching their peers , but rather that peer instruction is bidirectional and can even result in situations where two wrongs make a right . Overall , these results suggest that interventions aimed at students with low quiz redo scores for their quiz grade may have a significant impact in helping student learning . Keywords : physics education , metacognition , peer instruction , peer quizzing ( Some figures may appear in colour only in the online journal ) ∗ Author to whom any correspondence should be addressed . 0143 - 0807 / 21 / 055701 + 19 $ 33 . 00 © 2021 European Physical Society Printed in the UK 1 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al 1 . Introduction As instructors , our goal is to set our students up for success by creating a learning environment and classroom structure that are optimized to help our students gain mastery of both content material and problem - solving skills . Studies on peer instruction [ 1 , 2 ] , frequent low - stakes testing [ 3 – 8 ] , group testing [ 9 – 15 ] , and metacognition [ 7 , 16 , 17 ] suggest that they may all have positive impacts on student learning . This study examines the improvement of conceptual learning through the use of weekly low - stakes quizzes with a redo portion that utilizes both peer instruction and reflection . Low - stakes testing helps students learn by having them practice retrieving information in a testing situation before adding the stress of a high - stakes summative exam [ 3 – 8 , 18 , 19 ] . Stu - dents have been shown to score significantly higher on their summative exams when they have practiced first through low - stakes quizzing , and this effect has been shown to have long - term benefits as the same students score higher in the subsequent class a year later [ 19 ] . Another study examines the class performanceof students in low - stakes testing environmentsand shows that the benefits of such testing is higher when quiz performance contributed to a class grade and their quiz performance is strongly predictive of the students’ exam performance [ 20 ] . Together , this suggests that low - stakes quizzes can have a large , positive effect for students , and that improving the performance of students on low - stakes quizzes may positively benefit them in both the short and long term . Peer instruction is an effective teaching technique that has been well described in the liter - ature [ 1 , 2 ] . It is typically used in the classroom as a way to get students to actively engage and work with the material during class time when their peers and the instructor can help them overcome mistakes and misunderstandings . Recently , peer instruction has also be used in testing situations [ 9 – 15 , 21 – 23 ] . When the students take an exam first individually and then redo as a group , students can receive immediate feedback on their learning , and students generally find peer tests to be positive experiences [ 13 – 15 ] . However , there are mixed reports regarding efficacy ; some reports indicate an improvement in long term learning [ 9 – 11 , 21 ] while others show immediate improvements in understanding but no long term improvement [ 12 , 23 , 24 ] . Some of these mixed results may be due to the types of exams , with simple recall [ 10 ] and repetition of the exact question [ 11 ] showing more improvement than when the con - cept is repeated with a different question [ 24 ] . One study suggests that requiring students to justify their answer changes results in higher long - termretention [ 21 ] . It is also unclear whether peer testing benefits all students evenly , with one study suggesting that although all students benefit , lower - performing students benefit more than their higher - performing peers because the higher - performing peers can provide more help to the lower - performing peers than the reverse [ 13 ] . When considering peer - testing , it is important to understand how the students are learning during peer instruction and how they are changing their answers to see whether the instruction is leading to an increase in the number of correct responses . Couch et al show that about one - third of student answers submitted during peer instruction are changes from incor - rect to correct , with less than a quarter staying in correct and very few changing from correct to incorrect [ 1 ] . Similar results have been seen by others [ 25 – 28 ] . Further statistical analy - sis indicates that changing from incorrect to correct is not just correct students teaching their incorrect peers but that there is some indication that pairs of students with incorrect answers can come to the correct solution through discussion [ 28 ] . Tullis and Goldstone [ 29 ] saw similar results that , through discussion , two students with incorrect answers can reason out the correct solution . They further show that the confidence of both students in the pair can significantly impact how the answers are changed during the peer instruction reflection time . 2 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al Metacognition and metacognitive practices have been examined with respect to student learning and are typically cited as having a strong positive impact on learning [ 16 , 17 ] . In the physics classroom , some examples of implementation of metacognitive activities are self - reflection and articulatingreasoning duringpeer instruction [ 1 , 2 , 30 ] . Reflection can take many forms from the lowest threshold of ‘my answer was wrong / correct’ to deeper reflection such as ‘it was almost correct but . . . ’ [ 31 ] . Some studies have asked students to reflect on their exam performance and assess their own understanding [ 32 – 34 ] and found that lower - achieving stu - dents are typically less accurate [ 35 , 36 ] . However , work in a physics classroom using active learning and peer instruction suggests that the students do self - calibrate as they learn during the semester and are significantly better at predicting their grades at the end of the semester even without explicit training in metacognition [ 36 ] . This is likely because the students uti - lize metacognitive techniques , such as reflection , as part of the classroom activities . While our study does not utilize metacognition in the sense of explicitly asking the students to consider how they learn or how the peer - instruction quiz redo improved their learning , it requires them to use reflection as part of the quiz redo to choose what quiz problems they will redo . In this study we examine the effect of weekly quizzing and the introductionof a peer instruc - tion / reflection componenton conceptual learning in our introductoryphysics classes . Although other groups have examined peer - testing , our approach is novel in that students themselves choose what to redo rather than redoing the entire quiz / test . We do this by giving students weekly individual quizzes and then allowing the students to redo a limited portion of the quiz in small peer groups . The students must be reflective because , unlike other peer testing stud - ies [ 9 – 15 , 21 – 23 ] , they choose which problems to redo . To encourage reflections rather than a scatter - shot approach , the students not only earn credit for changing an incorrect answer to correct but also lose credit for changing a correct answer to incorrect . Finally , we examine how a student’s answers change to determine if all students are benefiting from the quiz redo . Our ultimate goal is to improve conceptual learning in introductory physics classes . 2 . Research questions In this study , we examine students’ conceptual learning and answer the following research questions : • RQ1 : does our data support the body of evidence that frequent low - stakes testing helps student learning ? • RQ2 : does adding a redo component that combines both peer instruction and reflection to the quizzes further improve students’ conceptual learning ? • RQ3 : do both high - and low - performing students benefit from the redo component ? • RQ4 : are the roles of teacher / learner in a peer group fixed or shared ? In RQ1 and RQ2 , we start with a more descriptive overview of the data . Using that , we will show the merit of a deeper statistical analysis to study the effects of the redo component on all of our students ( RQ2 ) and on different subsets of our student population ( RQ3 ) . Finally , we will explore the exact way that students learn during the redo component ( RQ4 ) . 3 . Methods 3 . 1 . Course description This study examines student conceptual learning in one instructor’s sections of calculus - based introductory physics at Cleveland State University ( CSU ) from Spring 2015 to Spring 2019 . 3 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al CSU is an urban university with a higher than average percentage of black and first genera - tion students [ 37 ] . Many of the students commute to campus and have other obligations ( i . e . children , jobs , etc ) and over 40 % receive Pell Grants [ 38 ] . The calculus - based introductory physics sequence consists of University Physics I ( intro - ductory mechanics ) and II ( introductory electricity and magnetism ) and is required not only for physics majors but also for most engineering , chemistry and math majors . The approxi - mately 400 students in the introductorysequences each semester are predominatelynon - majors ( ≈ 98 % ) who are required to take the course . The students exhibit a broad range of incoming math / physics knowledge and general preparedness . Each semester the introductory classes are broken into four or five sections of 40 – 60 stu - dents taught by a professor in approximately 4 h of lecture time per week . The individual instructor selects the pedagogical approach in those lectures although the topics are common between all sections . The lecture is supplemented by a one - credit TA - led required lab class that mixes students from different sections and focuses on giving the students a practical experience with the theory they learn in the lectures . 3 . 2 . Intervention : peer quizzing The quizzes in these courses are designed to be short , frequent , and low - stakes assessments . The quizzes are typically six questions and 18 min long , focusing on the material covered by the most recent homeworkassignment . Each quiz question is multiple - choicewith five possible answers , and each question addresses either a conceptual idea or applies a concept / equation with a few steps of math . In order to make the weekly quizzes low - stakes learning opportunities and to reduce stress , the quizzes are only 10 % of the course grade ( compared to 50 % for the summativeexams ) , and each student’s lowest two quiz grades ( includingzeros due to absences ) are dropped . Students take the quiz individually first and turn in their responses . After everyone has completed the quiz individually , the students form groups of two ( or occasionally three ) to redo a self - selected subsection of up to half of the quiz questions while working with their peers . The student’s final grade on each quiz question is the average of the two quizzes . This means a student can either gain 0 . 5 points credit by correcting an incorrect answer or lose 0 . 5 points credit by changing an initially correct answer to incorrect . As the students may chose what questions to redo , they must critically reflect on which answers they have confidence in and which they believe they did incorrectly . The students are required to submit a single answer sheet for the redo , meaning that they must agree on their answers . This means that on each quiz some students submit the same answer to allow their group member to correct theirs and vice versa on a different problem . If students cannot come to a consensus or run out of time , they do not respond to a given question . Students are discouraged from submitting answers they already agree on ; therefore sometimes groups submit blank sheets if they agree on all the answers . After the redo is complete , the instructor immediately dis - cusses the quizzes with the entire class to be certain that each student knows what they did correctly and incorrectly on the quiz and can utilize the quizzes to focus their studying for the exams . This study reports on both the work done by students individually on the quizzes and also on how they performed on the quiz redos . A student’s quiz mean is their average quiz score on the individual portion of the quizzes for the entire semester . This is the average of all the quizzes that the student took so we do not drop the two lowest quizzes , but we do ignore any missed quizzes due to absences . A student’s quiz improvement mean is their average improvement on the quizzes during the quiz redo . 4 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al Table 1 . Population size for different categories . All Has CA - gain No quiz Quiz All 286 213 79 134 Physics I 142 101 0 101 Physics II 144 112 79 33 Grade = AB 105 100 36 64 Grade = CD ( FW ) 181 113 43 70 3 . 3 . Instrument and analysis We use two well - established concept assessments ( CAs ) to assess student knowledge . For physics I we use the force concept inventory ( FCI ) [ 39 – 43 ] ) , and for physics II we use the conceptual survey of electricity and magnetism ( CSEM ) [ 44 , 45 ] . The instructor gave these CAs during the first and last weeks of the course as a pre / post analysis . Although these assess - ments only cover about 2 / 3 of the course material , the scores are a metric that is consistent over all semesters . By utilizing the CAs rather than the course grade , we are removing any biases that may be in the data such as differences in exam difficulty or curving between semesters . As a measure of student learning , we use the Hake normalized gain score [ 40 ] : gain = final − initial max − initial ∗ 100 ( 1 ) where final is the CA - final score ( post - test score ) , initial is the CA - initial score ( pre - test score ) and max is the total number of questions on the CA . This gain score adjusts for incoming knowledge through the denominator . Although there are known saturation effects and biases [ 46 , 47 ] , this metric is a good way to measure learning independent of incoming knowledge . While the Hake score was originally reported as an aggregate measure over the entire student population and can be thought of as an expression of effect size , we calculate the normalized gain for each student individually to use as a metric for their learning within this course , which can then be used for further analysis [ 48 , 49 ] . 3 . 4 . Sample population This study examines 286 students in one instructor’s introductory classrooms over multiple semesters , and the total population can be seen in table 1 . Students who did not have a CA - gain ( 73 ) because they missed either the pre - or post - test were removed from the data set . This does result in a slight bias , as students without a CA - gain score are predominantly lower scoring students ( 5 with an A or B grade in the class , versus 68 with a C , D , F , or W grade ) . Indeed , the CDFW category is mostly CD as only two students who earned an F or W grade are included in this analysis and so we will write it as CD ( FW ) to indicate this . We analyze the learning of remaining 213 students broken into groups of no quiz and quiz . The no quiz group ( 79 students ) is comprised of students from three semesters where the assessments were several mid - term exams and a final exam . The quiz group ( 134 students ) is comprised of students in semesters where an additional weekly low - stakes quiz ( described in section 3 . 2 ) was included in the assessments . The sample population is shown in table 1 , which includes a breakdown by class type ( physics I / II ) and by overall grade ( AB or CD ( FW ) ) in the class . For reference , students need to maintain an average of a C to graduate , and they fail the course in case of an F grade or a W ( withdraw ) . Finally , it should be noted that when cal - 5 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al Figure 1 . Course organization for the class . A pre - test is given on the first day of classes . Each topic covered begins with reading , followed by a lecture , homework assignment , quiz and quiz redo with the corresponding quiz improvement score ( for applicable semesters ) , and exams . culating the CA - gain , if the final score is lower than the initial score , the gain can be negative . These cases ( n = 9 ) are retained in our analysis . For the quiz mean and the quiz improvementmeancalculations , missed quizzes ( forinstance due to illness ) are excluded from the analysis . The vast majority ( 93 % ) of students missed no ( 110 ) or one ( 16 ) quiz , 6 students missed two quizzes , and 2 missed three quizzes . To ensure a fair comparison with the no quiz control group , no students were excluded because of missed quizzes . 3 . 5 . Procedure We use multiple analysis techniques to answer the different research questions . For each research question we examine a single course ( physics I or physics II ) to avoid any impact due to the fact that the two CAs have different average gains . We will divide the remainder of this section up by RQ for clarity . 3 . 5 . 1 . Procedure for RQ1 . For RQ1 we use only the students in the physics II sections , since this is the only class with a no quiz control section available . We use Cohen’s d [ 50 ] as a measure of the effect size : d = μ 1 − μ 2 s , ( 2 ) with μ i the mean of each individual distribution . s is the pooled standard deviation defined as : s = (cid:2) ( n 1 − 1 ) s 21 + ( n 2 − 1 ) s 22 n 1 + n 2 − 2 , ( 3 ) with n i the size and s i the standard deviation of each individual sample . Additionally , we use a two - sample Kolmogorov – Smirnov ( KS ) test to determine how distinct the distributions of the 6 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al CA - gain metric are between the quiz and no quiz sections . For clarity , we express the results of the KS test as an equivalent p - value 3 . 5 . 2 . Procedure for RQ2 . For the remaining research questions , including RQ2 , we utilize the physics I class as it has a higher number of students with a quiz ( n = 101 ) . In RQ2 we develop a best subsets model to predict a student’s CA - gain . The model is made by doing a multivariate ordinary least squares ( OLS ) linear regression in Python using Statsmodels [ 51 ] , which yields a slope , β , and a p - value for each predictor , as well as an overall correlation ( R 2 and R 2 - adjusted ) . We have assumed that the CA - gain for individual students can be fit with a linear curve using the following predictors : exam mean , quiz mean , uiz improvement mean , initial CA score , homework mean , and lab mean . Each of these values are calculated separately for each individual student . For clarity in these metrics , we show a schematic for the course organization in figure 1 . Each topic in the course is covered by reading the chapter , lecturing in class , and a homework assignment . In the applicable semesters , this is followed by a quiz with a quiz redo and the quiz improvement score for each student is calculated from these two scores . Multiple topics are combined into each of the four midterm exams , and the final exam is cumulative over all topics . The quiz improvement score for each individual student is calculated as : I ( in % ) = 0 . 5 ∗ n c + 0 ∗ n s − 0 . 5 ∗ n i Q tot × 100 . ( 4 ) I is the quiz improvement score for an individual student . n c is the number of questions an indi - vidual student changed from incorrect on the initial quiz to correct on the quiz redo . Similarly , n s and n i are the number of questions a student did not change ( stayed correct / incorrect ) and the number a student changed from correct on the quiz to incorrect on the quiz redo , respec - tively . Q tot is the total number of questions on the quiz . Thus , while the quiz redo is completed as a team , the quiz improvement score for a student reflects their individual improvement on a weekly quiz . The quiz improvement mean is the mean of the I score for the semester ( exclud - ing missed quizzes ) . ( Note : the same value of I can be calculated as : ( average of quiz and redo in % ) − ( quiz score in % ) ) . The calculated metrics for each individual student are therefore : • Exam mean = mean of all midterm exams and the cumulative final . Includes a zero if the student missed the exam . • Quiz mean = mean of weekly quiz scores . Zeros due to absences are ignored . • Quiz improvement mean = mean of weekly quiz improvement scores calculated by the equation ( 4 ) . Zeros due to absences are ignored . • Initial CA score = pre - test . • Homework mean = mean of weekly homework assignments . • Lab mean = mean lab score reported to the instructor by the TAs . For calculating the model , all scores are normalized between zero and one . For the quiz improvement mean , the maximum possible gain is 25 % . This is because students are allowed to redo 50 % of the questions and their score is the average of their score on the quiz and on the redo so they can earn back 50 % of the credit on each question they redo . For example , on a six - point quiz a student can redo three questions and earn an additional 0 . 5 points back per question for a total of 1 . 5 / 6 = 25 % . Therefore , these scores are normalized between 0 and 1 by dividing by 25 . 7 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al Figure 2 . Distribution of student assessment gains for with and without weekly quizzes in introductory electricity and magnetism . The best subset model is determined using the R 2 - adjusted value to avoid over - fitting the model ; the result is the same fitting to mean square error and using a step wise regressionmodel . In order to examine the fit , we report the R 2 values of the model which describe the variance of the actual data points with respect to the model . We further report on the p − values for each of our fit variables and discard variables from the model if the p - value is too high . We have confirmed that our final model is good by considering the R 2 value and by examining the residuals . We include in the manuscript the predicted versus actual graph ( see figure 5 ) . We have also examined the residuals versus the prediction and confirmed that they are centered around zero with no trends , and a histogram confirms that they are normally distributed ( not shown in the manuscript ) . 3 . 5 . 3 . Procedure for RQ3 . For RQ3 we again utilize the physics I course that has a larger number of students with a peer quiz . To address this research question , we examine the ceiling effects on the redo . We do this by showing a scatter plot of students’ quiz improvement mean scores versus their quiz mean scores and fit this data with a line determined by an OLS fit . We further examine how each individual student changes their answers : from incorrect to cor - rect , from correct to correct if they are convincing their peers of their answer , from correct to incorrect if they are erroneously convinced by their peers , or from incorrect to incorrect if the group is coming to a wrong conclusion in the discussion . Each student’s scores are examined individually to determine the number of times they changed an answer to correct , stayed the same , or changed an answer to incorrect . These results are tallied for the semester and nor - malized to 100 % . We also examined the average number of answers the students submitted on each quiz redo . The student’s individual changes were then examined according to their performance level , as defined by their quiz mean score , to determine the effects of the redo on low - and high - performing students respectively . 3 . 5 . 4 . Procedure for RQ4 . RQ4 was also examined utilizing the physics 1 students with a peer quiz . In this case , rather than examine individual student responses , we examine pairs of students and the answers they submit on the quiz redo . We look at whether both students were initially correct , incorrect , or had a mixed incorrect / correct , and we look at their final submissions . In this way we can determine whether a single student in the group is taking on the role of teacher and another is taking on the role of student or if they are sharing the roles . We can further see if two students with initially incorrect answers can , through discussion , come to the correct solution . 8 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al Table 2 . Table of the number ( n ) , mean , standard deviation ( σ ) , Cohen’s d effect size ( d ) and two - sample KS determined p - values for the no quiz and quiz groups for all students and broken up into AB and CD ( FW ) populations . Group n Mean σ Δ d p All No quiz 79 37 . 5 22 . 6 8 . 9 0 . 41 0 . 02 Quiz 33 46 . 4 19 . 2 AB No quiz 36 49 . 1 21 . 8 9 . 8 0 . 49 0 . 06 Quiz 14 58 . 9 14 . 4 CD ( FW ) No quiz 43 27 . 9 18 . 6 9 . 4 0 . 52 0 . 20 Quiz 19 37 . 3 17 . 3 Table 3 . Descriptive statistics for key independent and dependent variables , after normalization . Lab Homework Exam Quiz Quiz improvement CA - init mean mean mean mean mean CA - final CA - gain Mean 0 . 38 0 . 91 0 . 76 0 . 75 0 . 57 0 . 21 0 . 59 0 . 37 Std 0 . 19 0 . 060 0 . 20 0 . 13 0 . 17 0 . 16 0 . 21 0 . 25 Min 0 . 13 0 . 68 0 . 030 0 . 22 0 . 11 - 0 . 11 0 . 17 - 0 . 14 Median 0 . 33 0 . 92 0 . 81 0 . 76 0 . 54 0 . 22 0 . 57 0 . 36 Max 0 . 93 0 . 99 0 . 99 0 . 99 0 . 95 0 . 63 1 . 0 1 . 0 4 . Results and discussion 4 . 1 . RQ1 : does weekly quizzing support student learning ? Before examiningthe effect of the peer instruction / reflectionon student learning , it is important to first establish the impact of weekly quizzing for this group of students . Figure 2 shows probability density of the normalized gain scores on the CA for students in physics II classes with ( n = 33 ) and without ( n = 79 ) weekly quizzes . There is a clear increase seen in figure 2 in the average CA - gain of ≈ 9 % points from an overall average of 37 . 5 % without to 46 . 4 % with quizzes . This is a medium effect size ( Cohen’s d = 0 . 42 ) and these two distributions are statistically distinct ( p = 0 . 02 ) . Table 2 shows the same information for no quiz and quiz groups , with the groups further broken into high - performing ( final grade = AB ) and low - performing ( final grade = CD ( FW ) ) students . We see that the difference between the no quiz and quiz means ( Δ ) as well as the effect size ( d ) is about the same for both the AB and the CD ( FW ) students , which suggests that weekly quizzing is beneficial to both groups of students , although this cannot be stated definitively because of the small sample size ( high p - values ) . Overall these results agree with the plethora of literature that finds frequent , low - stakes testing results in increased learning [ 3 – 8 ] . Finally , it can be noted that if instead of looking at CA - gain we instead consider CA - final or CA - absolute - gain ( not normalized ) , we see shifts of Δ = 7 . 4 , 7 . 5 , d = 0 . 395 , 0 . 399 , and p = 0 . 098 , 0 . 055 , respectively . 9 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al Figure 3 . Distributions of key independent ( top - down / left to right : CA - init , lab mean , homework mean , exam mean , quiz mean , quiz improvement mean ) and dependent ( CA - final , CA - gain ) variables , after normalization . Diagonal : univariate histograms . Lower triangle : bivariate scatter plots . 4 . 2 . RQ2 : does the redo component further improve student’s conceptual learning ? The main goal of this work is to examine if the quiz redo is impactful in helping students gain conceptual understanding in introductory physics . Qualitatively , the students seem very involved in the redo . Nearly all students participate in lively discussions , and very few stu - dents are withdrawn or quiet during this time , similar to what is seen for other two - stage exams [ 14 , 15 ] . A common approach among the students is to triage their results by quickly going through their answers and marking which ones they disagree on so that they can focus on dis - cussing these during the allotted time . When asked by the instructor about the redo , student comments are generally positive . Whether that is because the students are actually learning , they feel like they are learning but are not really internalizing the knowledge , or they simply appreciate the chance to earn back points is unclear . Further , as the redo is a peer task , there is 10 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al Table 4 . Table of model parameters for a two feature model of quiz mean , and quiz improvement mean with the relevant β values and the p - values of the variables . Model Model parameter β p - value R 2 Quiz mean Quiz mean 0 . 8779 0 . 000 0 . 349 Constant − 0 . 1212 0 . 086 Quiz mean and Quiz mean 1 . 015 0 . 000 0 . 373 improvement mean Quiz improvement mean 0 . 282 0 . 057 Constant − 0 . 258 0 . 000 some impact due to who each student pairs with . In this section , we will first give a descrip - tive overview of the distribution of the data . After that , we will explore the effect of the redo component for individual students to understand whether it is helping them learn the material . In order to avoid any impacts of the different courses , we focus on the physics I course with quizzes ( n = 101 ) . To first gain a qualitative , descriptive sense of the data , we show descriptive statistics for our key data in table 3 . We also plot the univariate ( histograms ) and bivariate ( variable 1 versus variable 2 ) distributions of the independent ( CA - init , lab mean , homework mean , exam mean , quiz mean , quiz improvement mean ) and dependent ( CA - final , CA - gain ) variables , after nor - malization ( figure 3 ) . On the diagonal , we see the histograms for each variable . In the lower triangle , the scatter plots give a sense for the functional relationships between each pair of variables . We can clearly see that lab mean ( the second column in figure 3 ) and homework mean ( third column ) show skewed distributions , and do not show much of a correlation with the other variables . We therefore expect these scores to have little predictive value in our fur - ther analyses . The other variables are all more normally distributed , except for CA - init , where we see most students coming in with limited knowledge , as expected . Looking at the scatter plots , we see a strong relationship between CA - gain and CA - final , suggesting that our results should be mostly independent of the choice of dependent variable , with one caveat : the CA - final shows a stronger relationship with the CA - init than the CA - gain does , which validates our choice to use CA - gain as our primary dependent variable . We further observe clear relation - ships between the quiz mean and exam mean on the one hand , and the dependent variables on the other hand . Finally , we observe a strong inverse relationship between quiz mean and quiz improvement mean , which convolutes the relationship between the quiz improvement mean and the dependent variables . Therefore , due to this convolution , in order to properly explore the impact of the peer quizzes on the CA - gain , we will need to do a multiple variable regression analysis . We are now ready to zoom in on the effect of the redo component on individual students . To look at the impact of the quiz and redo , we first consider an OLS regression of two variables , the quiz mean and quiz improvementmean , to determine how well this predicts CA - gain relative to the predictions of only the quiz mean . The result of the predictions for only the quiz mean and for the quiz mean / quiz improvementmean two - feature model is shown in table 4 . The first thing to notice is that when both the quiz mean and the quiz improvement mean are used to predict the CA - gain values , the accuracy of the prediction increases as can be seen by the decrease of the variance as measured by R 2 ( R 2 = 0 . 349 for quiz mean to R 2 = 0 . 373 for quiz mean with quiz improvement mean ) . The p - value of the quiz improvement mean factor is 0 . 057 , which is at the boundary of statistical significance ( 0 . 05 ) . 11 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al Figure 4 . Scatter plots and OLS fits of student’s quiz improvement mean % ( improve - ment on redo ) versus quiz mean % ( individual score ) for the entire semester . Lines are an OLS fit and shading shows a 95 % confidence interval . The dotted black lines high - light quiz improvement means that are 8 % above or below average for a given quiz mean score . Figure 5 . ( Left ) Plot of R 2 - adjusted value for model subsets and ( right ) the predicted versus actual CA - gain values for the three feature best model . The final thingto examineis the β constant for the quiz improvementmean which is 0 . 282 . If a student improves by one question on the redo each week , this corresponds to a quiz improve - ment mean score of 8 % ( 0 . 5 / 6 ) . When normalized and multiplied by β we see that this corre - sponds to a 9 % increase in their CA - gain score at the end of the semester ( 825 ∗ 0 . 282 = 0 . 093 ) . The quiz mean has a higher impact , as can be seen by the fact that the β value is three times larger , but this shows that performing better on the quiz redo has a clear positive impact on student’s learning . This may be easier to understand when considering a graph of the quiz improvement mean versus the quiz mean as shown in figure 4 . This is a scatter plot of each student’s quiz improvement mean versus their quiz mean . The blue line is an OLS fit . The black dotted lines highlight a quiz improvement mean of 8 % above or below average for a given quiz mean . Any students with data points that intersect these lines can be predicted to have a CA - gain that is ≈ 9 % above or below what would be expected based on their quiz mean . While the above discussion suggests that the quiz redo is having a positive impact on stu - dents , it is important to consider whether that improvement is significant when all aspects of the course are considered . In order to better understand what factors within the class impact the CA - gain , we examined a best fits model , fitted using the R 2 - adjusted value to avoid over - fitting the model . The fit was performed for the quiz improvement mean , the initial score on 12 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al Table 5 . Best subsets fitting to R 2 and R 2 - adjusted . The best model is in bold and high - lighted with gray . The two feature model of quiz mean and quiz improvement mean discussed above is shown as subset 2b . the CA , and all the grades that go into the students final grade : quiz mean , exam mean , home - work mean , and lab mean . Figure 5 ( left ) shows the R 2 - adjusted values for all the examined subsets ; the two models discussed above as well as the model for only quiz improvement mean are highlighted with red points , and the value of the best subsets is highlighted with a dotted line . Table 5 has a summary of each of the best subsets with the features included in the fit , the R 2 value , and the R 2 - adjusted value . It also includes the quiz mean with quiz improvement mean model discussed above . Examining figure 5 ( left ) we can see a few things . First , exam mean ( R 2 = 0 . 352 ) and quiz mean ( R 2 = 0 . 349 ) alone are almost equal predictors of the CA - gain . Next , we can see that while quiz improvement mean alone is not a strong predictor of CA - gain ( red x in column 1 ) , the quiz mean and quiz improvement mean model ( red star in column 2 ) is close to the best two - feature model of exam mean and quiz mean . In both figure 5 and table 5 we see that increasing from a one - feature to three - feature model reduces the overall variance by 7 % ( from R 2 = 0 . 352 for the one feature model with exam mean to R 2 = 0 . 420 for the three - feature 13 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al Figure 6 . ( Left ) How often the students change their answers and ( right ) how they change their answers as sorted by quiz mean grade in % . The numbers in parentheses are the number of students in each grade group ( ≈ % as the total is n = 101 ) . model with exam mean , quiz mean , and quiz improvement mean ) . Adding in a fourth feature ( homework ) nominally decreases the variance , but the variable is not statistically significant ( p - value = 0 . 204 ) , so it is not included in our model . Thus , even when determining the best overall model , quiz improvement mean is a factor in predicting the overall conceptual learning of students in physics I . Further , the β value is very close to the two - feature model discussed above , so the conclusions still hold : a student doing one question better on the redo corresponds to a 9 % improvement in the CA - gain and that β is positive , indicating an overall benefit for the students regardless of performance level . One way to examine the fit of a model is to look at how well the predictions the model makes for a given set of inputs ( in this case exam mean , quiz mean , and quiz improvement mean scores ) match the actual values . Figure 5 ( right ) shows a plot of the actual values versus the predicted values . The blue line is a guide for a perfect model . In this graph we see that the model is good in that the points are evenly distributed above and below the line , and there are no trends for higher - or lower - performingstudents . Three of the four outlier points on the upper right of the graph are students who had a CA - initial score (cid:2) 0 . 70 compared to a class average CA - initial of 0 . 39 making it easier to obtain high CA - gain scores . The average CA - gain , as can be seen in the figure or in table 3 , is 37 % , which is reasonable for active learning classrooms [ 39 ] . A few final comments about the model : our model was developed by optimizing the R 2 - adjusted value , but the same model is determined when optimized using the AIC . If instead of predicting the CA - gain score we develop a model to predict the CA - final score or the CA - absolute - gain ( not normalized ) using R 2 - adjusted , the same model is predicted with the addi - tion of the CA - initial score . This means that in terms of absolute scores and gains the incoming knowledge plays a sizable role . But it is notable that an above average quiz improvement mean still predicts improvement for both of these metrics . 4 . 3 . RQ3 : do both high - and low - performing students benefit from the redo ? It is expected that there will be both an inverse relationship between a student’s quiz mean and a ceiling effect for top - performing students . Simply stated , you cannot improve from a perfect score , while lower - performing students have a lot of room for improvement during the redo . This is clearly seen in figure 4 which shows each student’s quiz improvement mean versus their quiz mean . The line is an OLS fit of the data ( R 2 = 0 . 26 ) , and the shading shows a 95 % confidence interval . Beyond the expected negative slope , it is interesting to examine the handful of data points that are below y = 0 . Negative values indicate that a student , on 14 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al average , changedtheir answers from correct to incorrect . The students with the largest decrease in their quiz scores are lower - performing students , while the highest performing students have scores that are only slightly negative . Another interesting observation is that the high - achieving students have scores that show much less variance and lie much closer to the fitted line , whereas the lower - achieving students’ scores scatter much further from the line . Together , these two observations suggest that the high - achieving students are displaying a clear knowledge of their course knowledge , i . e . these students are confident about their knowledge when discussing with their peers . Such reflection suggests that the quiz redo might be beneficial not only to the low - achieving students but also to the high - achieving students as they consider what they know well and what they may need to study before the exam . We can further look at how individual students use the quiz redo to change their answers . Figure 6 shows both how often the students change their answers ( left ) and how they change their answers ( right ) as sorted by their average quiz scores for the physics I quiz students . The number of students in each range is shown in parentheses and the overall quiz mean for all students was a 52 . 8 % . The first thing to notice in this figure is that all the students change their answers with the same frequency of ≈ 2 . 4 / 3 each quiz . Turning in fewer than three answers could be for many reasons : ( 1 ) they may have agreed on the other problems ( students even occasionally agree on all the problems and turn in a blank sheet ) ; ( 2 ) they may have run out of time to finish their discussions ; or ( 3 ) they may not have been able to come to a consensus answer for a problem . The next interesting trend is how the students choose to change . The bottom blue color is the average percent of questions that are changed to correct . This percentage increases from very small ( 8 % ) for the high - performing students to 45 % for the low - performing students . This makes sense because the students who already had high scores have less to correct on a given quiz . The next color , orange , shows the average percentage of times that students change their answer to something incorrect . This is small for all students ( 3 % – 7 % ) . The next color , green , is the average percentage of times the students have the correct answer and resubmit it to help their peers , i . e . how often the students are teaching someone else . This is largest for the high - performing students ( 77 % ) , but there is some teaching going on at all levels of performance ( 5 % at 0 – 29 and 21 % at 30 – 39 ) . This also highlights how the redo can help not only the low - performing students by giving them another chance to learn material , but also the high - performing students who are teaching their peers and reinforcing their own knowledge while they do so . The top color , red , shows how often students resubmit an incorrect answer . This is small for the high - performing students ( 8 % ) and increases for the low - performing stu - dents ( 46 % ) . This may be due to how the students are selecting their groups and whether low - performing students pair with other low - performing students or with higher - performing class members . Overall , these numbers are similar to those seen for peer instruction during lecture [ 1 ] . 4 . 4 . RQ4 : are the roles of teacher / learner in a peer group fixed or shared ? The above analysis clearly shows that the teaching is occurring for both high - and low - performing students . But this still leaves open a few questions : • Is this teaching unidirectional ? Does a single student take on the role of teacher while the other student ( s ) take on the role of learner ? • Do we see indications that students can work together to correct mistakes during peer instruction ? 15 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al Figure 7 . Pathways of answers from initial answers on the individual quizzes ( top ) to alignment of answers within a pair ( middle ) to final submitted answer of the pair ( bot - tom ) . The number in brackets is the raw number of pairs . Correct answers are highlighted green , incorrect are red , and mixed pairs have both colors . Bold outlines highlight the path of two wrongs make a right . In order answer these questions , we looked at the 1082 answers submitted by all the pairs of students in the physics I class ( i . e . with no students dropped because of missing CA - gains ) . Figure 7 shows the pathways of the students’ answers from their initial submissions on the individualquizzes ( top ) to the pairs and whether they agree or disagree with each other ( middle ) to the final submitted answers ( bottom ) . The quiz redo is helping , as the majority of the answers submitted at the end of the redo are correct ( 77 . 1 % ) , which agrees roughly with what we saw in figure 6 where the percentage of incorrect answers went from ≈ 50 % from the low - performing students to ≈ 5 % for the high - performing students . Now if we break the answers down into pairs of students , we can see how the pairs of students are changing . If we look at pairs of students that had the same correct answer ( left branch ) , they typically keep that answer ( 88 . 6 % ) and rarely change to an incorrect answer ( 11 . 4 % ) , as expected . Note : the total number of pairs with both correct → stay correct is small ( n = 93 → 8 . 6 % ) because students quickly realize submitting the same answer on the individ - ual quiz and the redo has no impact on their grade . Indeed , 59 ( 63 % ) of the both correct → stay correct answers were submitted on the first two quizzes ) . The next group is the mixed correct / incorrect answers that make up the bulk of the submis - sions ( 71 . 8 % ) . In this group we see evidence of teaching as one student convinces her / his group member ( s ) that they have the correct answer . The majority of the time this teaching results in the correct answer ( 84 % ) . This is most likely due to confidence in their answers since when students disagree and one student is very confident and the other student is not ( a large gap in confidence levels ) , the confidence often correlates with the correct answer the less confident student will change [ 29 ] . It is worth considering how the teaching occurs and whether it is uni - directional ( i . e . one student is teaching another student ) or multidirectional ( students share the roles of teacher and learner ) . In looking at the quizzes , there are 91 times where a group has two or more mixed correct / incorrect submissions on a quiz . 70 % of these times , a single student has the correct answer and the other student is changing from incorrect to correct . However , in 30 % of the cases , students swap roles between questions , indicating that peer instruction is not simply a transmission of information from a high - performing to a low - performing student but rather a constructive collaboration where all students are bringing knowledge to the table . Finally , we can look at the students that started with two incorrect answers . The majority of these answers stay incorrect ( 70 . 8 % ) . However , a significant fraction ( 29 . 2 % of the both incorrect answers ) demonstrate that two wrongs can make a right due to peer instruction con - versations as they catch each other’s mistakes and misconceptions . This is similar to the 42 % 16 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al seen by Smith et al [ 28 ] and higher than the 9 % ( 1 . 4 % of total ) seen by Tullis and Gold - stone [ 29 ] . Our study is different from both of these in the requirement to submit a single group answer and in the fact that students can choose which questions to redo . Further , we do not specifically look at whether the students initially agreed but rather whether they were both initially correct or incorrect , meaning two students may both be incorrect but not agree as is presented in other work [ 52 ] . However , this finding is in support of the growing body of literature that shows that peer instruction is not simply a transmission of knowledge from high - performing peers to lower - performing peers but that the discussion and the act of explaining and articulating their reasoning can lead students who both initially have incorrect answers to the correct answer . 5 . Limitations and future research directions A few limitations in this study must be acknowledged . The analysis is for a small number of students ( n = 101 for RQs 2 – 4 ) . Expanding this study to a larger number of students is necessary to confirm the findings of RQs 2 and 3 , which are at the heart of this study . This expansion is not entirely necessary for RQ1 and RQ4 because , as noted above , they are in agreement with results seen by other researchers . An expansion of the number of students included in the study may also clarify the statistical significance of the quiz improvement mean since it was near the boundary of significance . Additionally , this analysis was done for the first semester of the introductory physics sequence . Many students come into the sequence with some previous knowledge of mechanics but not of electricity and magnetism , and this can be seen in how they approach the class . Instructors regularly see students who do not bother with homework because they know the material and then do well on the exams ( which impacts the effect of homework score in the above model ) or students that do well the first half of the first semester and then do very poorly during the second half of the first semester when they can no longer coast on their incoming knowledge . This is less common in the second semester course . Thus there may be some clear differences in the model between the two semesters and it would be interesting to see if the peer quiz redo has a larger effect in one semester because of these differences . Finally , it would be useful to replicate this study with a more clear exami - nation on the metacognition the students use during the redo portion of the quiz through use of surveys . 6 . Conclusion This paper examines the effect of quizzes on improving student conceptual learning in intro - ductory physics courses . Specifically we examine the addition of a peer instruction / reflection component where , after completing the quiz individually , students work in small groups and redo a portion of the exam . Students self - select the questions they would like to redo , knowing that they can improve their score by correcting a mistake but will reduce their score if they change a correct answer to an incorrect answer . Our work shows multiple results . First , there is a clear impact of weekly quizzes on concep - tual learning with a 9 % improvement in CA - gain scores by adding in weekly quizzing . When comparing the class as a whole versus breaking it down into performance level , we see similar Δ shifts and similar distributions ( σ ) and effect sizes ( d ) for both high - and low - performing students . Second , a multivariate linear regression model shows that the exams , quizzes , and the redos have a clear impact on the CA - gain . While it can be expected that doing well on exams and quizzes would predict doing well on the conceptual assessment , there is a clear 17 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al signal indicating that students doing one question better on the redo than peers at the same ini - tial quiz score , will do ≈ 9 % better on their CA - gain at the end of the semester . If we assume no incoming knowledge this corresponds to 2 – 3 textitquestion improvement . Third , although there is a ceiling effect as top - performing students have less to gain on the redo , all students are benefiting from the redo . More importantly , we demonstrate that no matter what perfor - mance level a student is at , all levels of students are engaged in teaching their peers . Fourth and finally , we see evidence that this teaching is not just one student in a group teaching their peers but can be bidirectional . The process of discussing with their peers can also lead to situ - ations where two wrongs make a right 29 % of the time when pairs of students both start with an incorrect answer . This reinforces the idea that peer instruction helps our students not only by hearing another way of explaining the solution from their peers but also by reinforcing their own knowledge when they teach their peers . Ultimately , the weekly quizzes allow students to assess their own learning before the exams so that they can improve their overall learning . This work also suggests that there may be some ways to target and help weaker students by breaking them into controlled groups , but that these groups should be chosen with care so as not to remove the confidence boost that being the teacher may give to the lower - performing students . Finally , the data also suggests that interventions aimed at students whose quiz gains are low for their mean quiz score ( per - haps suggesting that they are under - prepared or lack good study skills or metacognitive skills ) may have a significant impact in helping lower achieving students improve their conceptual understanding . Acknowledgments The authors gratefully acknowledge the support of CSU’s Office of Research Undergraduate Student Research Award . This study was done in compliance with CSU’s Internal Review Board under IRB FY2018 - 319and is in accordance with the principals outlined in EJP’s ethical policy . ORCID iDs Jessica E Bickel https : / / orcid . org / 0000 - 0002 - 7506 - 1831 Thijs Heus https : / / orcid . org / 0000 - 0003 - 2650 - 2423 References [ 1 ] Crouch C H and Mazur E 2001 Am . J . Phys . 69 970 – 7 [ 2 ] Mazur E 1997 Peer Instruction : A User’s Manual ( Englewood Cliffs , NJ : Prentice - Hall ) [ 3 ] Kika F M , McLaughlin T F and Dixon J 1992 J . Educ . Res . 83 159 – 62 [ 4 ] Dunlosky J , Rawson K A , Marsh E J , Nathan M J and Willingham D T 2013 Psychol . Sci . Publ . Interest 14 4 – 58 [ 5 ] Roediger H L and Butler A C 2011 Trends Cognit . Sci . 15 20 – 7 [ 6 ] McDaniel M A , Agarwal PK , Huelser B J , McDermott K Band Roediger H L2011 J . Educ . Psychol . 103 399 – 414 [ 7 ] National Research Council 2000 How People Learn : Brain , Mind , Experience , and School : Expanded Edition ( Washington , DC : The National Academies Press ) https : / / nap . edu / catalog / 9853 / how - people - learn - brain - mind - experience - and - school - expanded - edition [ 8 ] Lang J M 2016 Small Teaching ( San Francisco , CA : Jossey - Bass ) [ 9 ] Gilley B H and Clarkston B 2014 J . Coll . Sci . Teach . 43 83 – 91 [ 10 ] Bloom D 2009 Coll . Teach . 57 216 – 20 18 Eur . J . Phys . 42 ( 2021 ) 055701 J E Bickel et al [ 11 ] Cortright R N , Collins H L , Rodenbaugh D W and DiCarlo S E 2003 Adv . Physiol . Educ . 27 102 – 8 [ 12 ] Woody W D , Woody L K and Bromley S 2008 Teach . Psychol . 35 13 – 7 [ 13 ] Giuliodori M J , Lujan H L and DiCarlo S E 2008 Adv . Physiol . Educ . 32 274 – 8 [ 14 ] Rieger G W and Heiner C E 2014 J . Coll . Sci . Teach . 43 41 – 7 [ 15 ] Weiman C E , Rieger G W and Heiner C E 2014 Phys . Teach . 52 51 – 3 [ 16 ] Wang M C , Haertel G D and Walberg H J 1993 Educ . Leader 51 74 – 9 [ 17 ] Wang M C , Haertel G D and Walberg H J 1990 J . Educ . Res . 84 30 – 43 [ 18 ] Brown P C , Roediger H L and McDaniel M A 2014 Make it Stick : The Science of Successful Learning ( Cambridge , MA : Harvard University Press ) [ 19 ] Agarwal P K , Bain P M and Chamberlain R W 2012 Educ . Psychol . Rev . 24 437 – 48 [ 20 ] Sotola L K and Crede M 2020 Educ . Psychol . Rev . [ 21 ] Zhang N and Henderson C N R 2017 J . Chiropr . Educ . 31 96 – 101 [ 22 ] Rao S P , COllins H L and DiCarlo S E 2002 Adv . Physiol . Educ . 26 37 – 41 [ 23 ] Petrunich - Rutherford M L and Daniel F 2019 Teach . Psychol . 46 115 – 20 [ 24 ] Leight H , Saunders C , Calkins R and Withers M 2012 CBE - Life Sci . Educ . 11 392 – 401 [ 25 ] Steer D , McConnell D , Gray K , Kortz K and Liang X 2009 Sci . Educat . 18 30 – 8 [ 26 ] Morgan J T and Wakefield C 2012 J . Coll . Sci . Teach . 41 51 – 6 [ 27 ] Giuliodori M J , Lujan H L and DiCarlo S E 2006 Adv . Physiol . Educ . 30 168 – 73 [ 28 ] Smith M K , Wood W B , Adams W K , Wieman C , Knight J K , Guild N and Su T T 2009 Science 323 122 – 4 [ 29 ] Tullis J G and Goldstone R L 2020 Cognit . Res . : Princ . Implic . 5 15 [ 30 ] Vickrey T , Rosploch K , Rahmanian R , Pilarz M and Stains M 2015 CBE - Life Sci . Educ . 14 1 – 11 [ 31 ] Lebedev P , Lindstrom C and Sharma M D 2020 Eur . J . Phys . 42 015707 [ 32 ] Claire T , Tippett T L and Boudreaux A 2015 2015 PERC Proc . pp 87 – 90 [ 33 ] Carpenter S K , Lund T J S , Coffman C R , Armstrong P I , Lamm M H and Reason R D 2016 Educ . Psychol . Rev . 28 353 – 75 [ 34 ] Osterhage J L , Usher E L , Douin T A and Bailey W M 2019 CBE - Life Sci . Educ . 18 1 – 10 [ 35 ] Saenz G D , Geraci L and Tirso R 2019 Appl . Cognit . Psychol . 33 918 – 29 [ 36 ] Galloway K 2013 AIP Conf . Proc . 1513 138 [ 37 ] Boboc M et al 2018 Cleveland State University Book of Trends 2018 https : / / csuohio . edu / sites / default / files / 2018 _ BookOfTrends . pdf [ 38 ] Nichols A 2015 The Pell partnership : ensuring a shared responsibility for low - income student success https : / / edtrust . org / resource / pellgradrates / # anchor [ 39 ] Korff J S V , Archibeque B , Gomez K A , Heckendorf T , McKagan S B , Sayre E C , Schenk E , Shepherd C and Sorrell L 2016 Am . J . Phys . 84 969 – 74 [ 40 ] Hake R R 1998 Am . J . Phys . 66 64 [ 41 ] Coletta V , Phillips J and Steinert J 2007 Phys . Rev . ST Phys . Educ . Res . 3 010106 [ 42 ] Thornton R , Kuhl D , Cummings K and Marx J 2009 Phys . Rev . ST Phys . Educ . Res . 5 010105 [ 43 ] Maloney D P , O’Kuma T L , Hieggelke C J and Van Heuvelen A 2001 Am . J . Phys . 69 S12 [ 44 ] Diff K and Tache N 2007 AIP Conf . Proc . 951 85 [ 45 ] Pollock S J 2008 AIP Conf . Proc . 1064 171 – 4 [ 46 ] Salehi S , Burkholder E , Lepages G P , Pollock S and Wieman C 2019 Phys . Rev . Phys . Educ . Res . 15 020114 [ 47 ] Henderson R , Stewart J and Traxler A 2019 Phys . Rev . Phys . Educ . Res . 15 010131 [ 48 ] Coletta V P and Steinert J J 2020 Phys . Rev . Phys . Educ . Res . 16 010108 [ 49 ] Nieminen P , Savinainen A and Viiri J 2012 Phys . Rev . ST Phys . Educ . Res . 8 010123 [ 50 ] Cohen J 1977 Statistical Power Analysis for the Behavioral Sciences ( New York : Academic ) [ 51 ] Seabold S and Perktold J 2010 Statsmodels : econometric and statistical modeling with python 9th Python in Science Conf . [ 52 ] Schwarz B B , Neuman Y and Biezuner S 2000 Cognit . Instruct . 18 461 – 94 19