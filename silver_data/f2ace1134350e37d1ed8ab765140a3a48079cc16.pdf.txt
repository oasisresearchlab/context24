The VLDB Journal ( 1998 ) 7 : 163 – 178 The VLDB Journal c (cid:13) Springer - Verlag 1998 Scalable feature selection , classiﬁcation and signature generation for organizing large text databases into hierarchical topic taxonomies Soumen Chakrabarti , Byron Dom , Rakesh Agrawal , Prabhakar Raghavan IBM Almaden Research Center , 650 Harry Road , San Jose , CA 95120 , USA ; e - mail : soumen , dom , ragrawal , pragh @ almaden . ibm . com Edited by M . Jarke . Received January 25 , 1998 / Accepted May 27 , 1998 Abstract . We explore how to organize large text databases hierarchically by topic to aid better searching , browsing and ﬁltering . Many corpora , such as internet directories , digi - tal libraries , and patent databases are manually organized into topic hierarchies , also called taxonomies . Similar to in - dices for relational data , taxonomies make search and ac - cess more efﬁcient . However , the exponential growth in the volume of on - line textual information makes it nearly im - possible to maintain such taxonomic organization for large , fast - changing corpora by hand . We describe an automatic system that starts with a small sample of the corpus in which topics have been assigned by hand , and then updates the database with new documents as the corpus grows , assigning topics to these new documents with high speed and accuracy . To do this , we use techniques from statistical pattern recognition to efﬁciently separate the feature words , or dis - criminants , from the noise words at each node of the taxon - omy . Using these , we build a multilevel classiﬁer . At each node , this classiﬁer can ignore the large number of “noise” words in a document . Thus , the classiﬁer has a small model size and is very fast . Owing to the use of context - sensitive features , the classiﬁer is very accurate . As a by - product , we can compute for each document a set of terms that occur signiﬁcantly more often in it than in the classes to which it belongs . We describe the design and implementation of our sys - tem , stressing how to exploit standard , efﬁcient relational op - erations like sorts and joins . We report on experiences with the Reuters newswire benchmark , the US patent database , and web document samples from Yahoo ! . We discuss appli - cations where our system can improve searching and ﬁltering capabilities . 1 Introduction The amount of on - line data in the form of free - format text is growing extremely rapidly . As text repositories grow in number and size and global connectivity improves , there is a pressing need to support efﬁcient and effective informa - tion retrieval ( IR ) , search and ﬁltering . A manifestation of this need is the recent proliferation of over 100 commercial text search engines that crawl and index the web , and sev - eral subscription - based information multicast mechanisms . Nevertheless , there is little structure on the overwhelming information content of the web . It is common to manage complexity by using a hierar - chy 1 , and text is no exception . Many internet directories , such as Yahoo ! 2 , are organized as hierarchies . IBM’s patent database 3 is organized by the US Patent Ofﬁce’s class codes , which form a hierarchy . Digital libraries that mimic hard - copy libraries support some form of subject indexing such as the Library of Congress Catalogue , which is again hier - archical . We will explore the opportunities and challenges that are posed by such topic hierarchies , also called taxonomies . As we shall show , taxonomies provide a means for designing vastly enhanced searching , browsing and ﬁltering systems . They can be used to relieve the user from the burden of sifting speciﬁc information from the large and low - quality response of most popular search engines [ 9 , 35 ] . Querying with respect to a taxonomy is more reliable than depending on presence or absence of speciﬁc keywords . By the same token , multicast systems such as PointCast 4 are likely to achieve higher quality by registering a user proﬁle in terms of classes in a taxonomy rather than keywords . The challenge is to build a system that enables search and navigation in taxonomies . Several requirements must be met . First , apart from keywords , documents loaded into such databases must be indexed on topic paths in the tax - onomy , for which a reliable automatic hierarchical classiﬁer is needed . As one goes deep into a taxonomy , shared jargon makes automatic topic separation difﬁcult . Documents on stock prices and on impressionist art look very different to us , but may be carelessly ﬁled as “human affairs” by a Mar - tian . Second , the taxonomy should be used also to present to 1 A hierarchy could be any directed acyclic graph , but in this paper we only deal with trees . 2 http : / / www . yahoo . com 3 http : / / www . ibm . com / patents 4 http : / / www . pointcast . com 164 the user a series of progressively reﬁned views of document collections in response to queries . Third , the system must be fast , especially since it will often be used in conjunction with a crawler or newswire service . Fourth , the system must efﬁciently update its knowledge when it makes mistakes and a human intervenes , or when an incremental update is made to the topic taxonomy . We describe such a taxonomy - and - path - enhanced - retrie - val system called TAPER . For every node in the taxonomy , it separates feature and noise terms by computing the best discriminants for that node . When classifying new docu - ments , only the feature terms are used . Good features are few in number , so the class models are small and the clas - siﬁcation is speedy . In contrast to existing classiﬁers that deal with a ﬂat set of classes , the feature set changes by context as the document proceeds down the taxonomy . This ﬁlters out common jargon at each step and boosts accuracy dramatically . Addition and deletion of documents is easily handled and discriminants recomputed efﬁciently . The text models built at each node also yield a means to summarize a number of documents using a few descriptive keywords , which we call their signature ( these are distinct from , and not necessarily related to , the features ) . In addition to the algorithmic contributions , we describe the design and implementation of our system in terms of familiar relational database idioms . This has the following beneﬁts . Our system can handle extremely large numbers of classes , documents and terms , limited essentially by word size of the computer . It adapts to a wide range of physical memory sizes , while maximizing fast sequential scans on disk when needed . Moreover , our design leads to insights into how database text extenders can exploit the core rela - tional engine . We report on our experience with TAPER using the Reuters newswire benchmark 5 , the US patent database , and samples of web documents from Yahoo ! . Depending on the corpus , we can classify 66 – 87 % of the documents correctly , which is comparable to or better than the best known num - bers . We can process raw text at 4 – 8 MB / min on a 133 - MHz RS6000 / 43P with 128 MB memory . Our largest train - ing runs , taking up to a day ( including fetching web pages ) , was with over 2 , 100 topics , over 266 , 000 URLs , and over 600 , 000 unique terms / tokens , is the most extensive web page classiﬁcation experiment known to us . Organization of the paper . In Sect . 2 , we demonstrate that using a taxonomy , topic paths , and signatures can greatly im - prove retrieval . Next , in Sect . 3 , we study the problems that must be solved to provide the above functionality . The prob - lems are hierarchical classiﬁcation , feature selection , and document signature extraction . These are explored in de - tail in Subsects . 3 . 2 , 3 . 3 , and 3 . 5 , respectively . Section 3 . 4 describes the architecture of our prototype . The proof of quality of signatures is necessarily anecdotal at this point ; some examples can be found in Sect . 2 . More rigorous eval - uation of feature selection and classiﬁcation is presented in Sect . 4 . Related work is reviewed in Sect . 5 , and concluding remarks made in Sect . 6 . 5 http : / / www . research . att . com / ˜lewis 2 Capabilities and features We discuss two important contexts in which accurate , high - resolution topic identiﬁcation is needed : querying and ﬁlter - ing . Querying . Most queries posted to search engines are very short . Such queries routinely suffer from the abundance problem : there are many aspects to , and even different in - terpretations of the keywords typed . Most of these are un - likely to be useful . Consider the wildlife researcher asking AltaVista 6 the query jaguar speed [ 9 ] . A bewildering va - riety of responses emerge , spanning the car , the Atari video game , the football team , and a LAN server , in no partic - ular order . The ﬁrst page about the animal is ranked 183 , and is a fable . Thwarted , she tries jaguar speed - car - auto . The top response goes as follows : “If you own a classic Jaguar , you are no doubt aware how difﬁcult it can be to ﬁnd certain replacement parts . This is particularly true of gearbox parts . ” The words car and auto do not occur on this page . There is no cat in sight for the ﬁrst 50 pages . She tries LiveTopics 7 , but all the clusters are about cars or football . She tries again : jaguar speed + cat . None of the top 24 hits concern the animal , but all these pages in - clude the term cat frequently . The 25th page is the ﬁrst with information about jaguars , but not exactly what we need . Instead , we can go to Yahoo ! , and visit the likely directory Science : Biology , and query jaguar . This takes us to Science : Biology : Zoology : Animals : Cats : Wild Cats and Science : Biology : Animal Behavior , but we could not ﬁnd a suitable page about jaguars there 8 . Filtering . Another paradigm of information retrieval is ﬁl - tering , in which a continual stream of documents are gen - erated on - line , as in newsgroups and newsfeed . The system collects interest proﬁles from users and uses these to imple - ment either content - based or collaborative ﬁltering , i . e . , it notiﬁes the user only of the documents they are likely to be interested in [ 4 , 19 , 29 , 41 ] . In its simplest form , a proﬁle may be a set of terms and phrases speciﬁed explicitly by the user . This has the same problem as querying without topic context as discussed above . A better notion of a proﬁle is the set of documents the user has seen and / or liked , perhaps with scores . This is a ﬁne - grained characterization of the user , and may work well with small systems , but for thousands of users and the web at large , a system storing this level of detail will not scale . A promising alternative is to characterize proﬁles not at the individual document level , but at the level of nar - row but canonical topics , such as the taxonomy of Yahoo ! . Such an approach is used in Surf Advisor , a collaborative recommendation system [ 31 ] . We identify a few core capabilities from these examples . Text search must be based on topical context as well as keywords . Once the broad topic of a document is known , it can be characterized by terms that are frequent compared to the topic model . To identify document topics , an automatic classiﬁcation system needs to separate words that contain 6 http : / / www . altavista . digital . com 7 http : / / www . altavista . digital . com / av / lt / help . html 8 The anecdote relates to the time of writing 165 topic information and words that are essentially noise . We discuss these at length next . 2 . 1 Querying in a taxonomy In a database that associated not only keywords but also topics with documents , the query jaguar speed could elicit not a list of documents , but a list of topic paths : Business and Economy : Companies : Automotive Recreation : AutomotiveGames : Video Games Sports : Football Science : Biology : Animal Behavior The user can now restrict queries by concept , not by key - word . Using samples , it is possible to show the above re - sponse even as the user types the query , before actually is - suing a search . At this point , the user can restrict the search to only a few topic paths . The artiﬁcial limit to the length of the response list from search engines , together with pages on cars and video games , will not crowd out pages related to the cat . As we have shown above , enforcing or forbidding additional keywords cannot always be as effective . If new documents can be binned into these topic paths in real - time , this ability may be very useful for multicast channels as well . User proﬁles will be topic paths rather than keywords . 2 . 2 Context - sensitive signatures An exhaustive keyword index , as in AltaVista , is perhaps more of a problem than a solution . A single occurrence of a term in a document , even one that is not a “stopword , ” no matter how useless an indicator of the contents , is indexed . The IR literature has advanced further ; there exist proto - types that extract signature terms , which are then used for indexing . These signatures can also be used as summaries or thumbnails ; their descriptive power can often compare favorably with that of arbitrary sentences as extracted by popular search engines . They are also effective for describ - ing a document cluster [ 2 ] . We claim that the common notion of a document abstract or signature as a function of the document alone is of limited utility . In the case of a taxonomy , we argue that a useful sig - nature is a function of both the document and the reference node ; the signature includes terms that are “surprising” given the path from the root to the reference node . In the above example , car and auto may be good signature terms at the top level or even at the Recreation level , but not when the user has zoomed down into Recreation : Automotive . Here is another illustration from a document 9 in Health : Nursing that starts like this : Beware of the too - good - to - be - true baby that is sleeping and sleeping and doesn’t want to nurse . Especially monitor the number of wet diapers , as seriously jaundiced babies are lethargic . The ﬁrst - level classiﬁcation is Health . We can compute the top signature terms with respect to Health as : 9 http : / / www2 . best . com / ˜goodnews / practice / faq . htm Jaundice , dampen , dehydration , lethargic , hydrate , forcibly , caregiver , laxative , disposable . This tells us the document is about treating jaundice . The second - level classiﬁcation is Health : Nursing . Shifting our reference class , we compute the new signature to be Baby , water , breast - feed , monitor , new - born , hormone . Now we know the document is about nursing babies ; this information comes from both the path and the signatures . In Sect . 3 . 5 , we shall propose some means of computing context - sensitive signatures . Thus , signiﬁcant improvement in search quality may be possible by maintaining function - ally separate indices at each taxonomy node , using only a few signature terms from each document . Another application of context - sensitive signatures is ﬁnding term associations . Using phrases for search and clas - siﬁcation can potentially boost accuracy . The usual way to ﬁnd phrases is to test a set of terms for occurrence rate far above that predicted by assuming independence between terms . Unfortunately , associations that are strong for a sec - tion of the corpus may not be strong globally and go unno - ticed . For example , precision may be visibly associated with recall in a set of documents on information retrieval , but not in a collection also including documents on machine tools . Computing signatures at each node makes it more likely that all such associations get exposed . 2 . 3 Context - sensitive feature selection Separating feature terms from noise terms is central to all of the capabilities we have talked about . In the above examples , car and auto should be “stopwords” within Recreation : Auto - motive and hence be pruned from the signatures . Feature and noise terms must be determined at each node in the taxonomy . It is tricky to hand - craft the stopwords out of domain knowledge of the language ; can is frequently included in stopword lists , but what about a corpus on waste manage - ment ? The contents of a stopword list should be highly de - pendent on the corpus . This issue looms large in searching using categories and clusters . In hierarchical categories , the importance of a search term depends on the position in the hierarchy [ 35 ] . In Sect . 3 , we will design an efﬁcient algorithm to ﬁnd , for each node in the taxonomy , the terms that are best suited for classifying documents to the next level of the taxonomy . Conversely , we detect the noise words that are of little help to distinguish the documents . We reuse the term “feature se - lection” from pattern recognition to describe this operation . Feature selection enables ﬁne - grained classiﬁcation on a taxonomy . For diverse top - level topics , a single - step classi - ﬁer sufﬁces . But as a document is routed deep into a taxon - omy , shared jargon makes sophisticated feature selection a necessity . Together with feature selection , we have to pick models for each class and a classiﬁer . Many options have been evaluated [ 40 ] . In spite of its simplicity , naive Bayesian classiﬁers are often almost as accurate as more sophisticated classiﬁers [ 24 ] . For a ﬁxed number of features , naive Bayes is faster than more complex classiﬁers . However , to ap - 166 proach the latter in accuracy , naive Bayes typically needs many more features . Finding feature terms for each node mitigates this prob - lem . Often , fewer than 5 – 10 % of the terms in the lexicon sufﬁce to discriminate between documents at any node in the taxonomy . This can greatly speed up classiﬁcation . The need for fast multilevel classiﬁcation is not restricted to the time a text database is populated . With increasing connec - tivity , it will be inevitable that some searches will go out to remote sites and retrieve results that are too large for direct viewing . There are already several “meta - search” tools that forward queries to a number of search engines and combine the results ; we have seen how a hierarchical view is much better . Applications of feature selection . Feature selection is use - ful in any setting where salient distinctions are sought be - tween two or more sets of documents . Consider the scenario where a set of documents ( e . g . , a keyword query result ) has been clustered into subsets , and we wish to annotate the clusters with salient keywords . We can regard the clus - ters as given classes , and use feature selection to ﬁnd these keywords . Other example applications include differentiat - ing between patents ﬁled by two companies , or by the same company at different times ( to expose any trend along time ) . We shall see some examples of such applications in Sect . 4 . 3 Models , algorithms and data structures In this section , we will deal with the core components of our system . Consider ﬁrst the task of computing the terms that induce the best distinction between the subtopics of a given topic . To do this , we have to ﬁnd terms that occur signiﬁcantly more frequently in some subtopics compared to others , as against those that show this property “by chance” owing to a ﬁnite sample . We can make such judgements only on the basis of some statistical model of document generation . This we discuss in Sect . 3 . 1 . The model leads to a natural classiﬁcation procedure , described in Sect . 3 . 2 . Finding good features for the classiﬁer to use is discussed in Sect . 3 . 3 . Performing all the above functions efﬁciently on large databases ranging into tens of gigabytes raises several performance issues that are discussed in Sect . 3 . 4 ; we also give details of our data structures and implementation there . Finally , in Sect . 3 . 5 , we discuss how to use the class models to extract context - sensitive document signatures . 3 . 1 Document model There have been many proposals for statistical models of text generation . One of the earliest indicators of the power of simple rules derived from both quantitative and textual data is Zipf’s law [ 48 ] . The models most frequently used in the IR community are Poisson and Poisson mixtures [ 37 , 42 ] . ( If X is distributed Poisson with rate µ , denoted X (cid:24) P ( µ ) , then Pr [ X = x ] = e − µ µ x / x ! and if Y is distributed Bernoulli with n trials and mean np , denoted Y (cid:24) B ( n , p ) , then Pr [ Y = y ] = (cid:0) ny (cid:1) p y ( 1 − p ) n − y . As n ! 1 and p ! 0 , the distributions B ( n , p ) and P ( np ) converge to each other . ) We will assume a Bernoulli model of document generation for most of the paper . In this model , a document d is gener - ated by ﬁrst picking a class . Each class c has an associated multifaced coin 10 ; each face represents a term t and has some associated probability θ ( c , t ) of turning up when “tossed . ” Conceptually , as the training text is being scanned , our classiﬁer database will be organized as a ( very sparse ) three - dimensional table . One axis is for terms ; their string forms being replaced by 32 - bit IDs , we call them TIDs and denote them t in formulae . The second axis is for documents ; these are called d in formulae . The third axis is for classes or topics . Topics have a hierarchy deﬁned on them ; for this paper , we will assume a tree hierarchy . These classes are also assigned IDs and called CIDs ; we denote them c in formulae . The measure maintained along these dimensions ( t , d , c ) is called n ( t , d , c ) , which is the number of times t occurs in d 2 c . This number is non - zero only when t 2 d 2 c . t 2 d means term t occurs in document d , and d 2 c means d is a sample document in the training set for class c . A superclass of c , i . e . , an ancestor in the topic tree , inherits all d 2 c . Aggregations along the dimensions give some important statistics about the corpus . – The length of document d is given by n ( d , c ) = P t n ( t , d , c ) . The length of all documents can be found using a GROUP BY on ( d , c ) . – The total length of training documents in class c , denoted n ( c ) . – The total number of times term t appeared in training documents of class c . – The fraction of times t occurs in d 2 c , i . e . , f ( t , d , c ) = n ( t , d , c ) / P t n ( t , d , c ) . This can be computed from the above , but we materialize this value for efﬁciency rea - sons . We will need the sum of f and f 2 over documents in a class as explained later . We will omit c when it is clear from the context . – The number of training documents in class c that have at least one occurrence of a speciﬁc term t . This is denoted m ( t , c ) . – The number of training documents in class c , denoted j c j . We will describe the details of arranging this table later in Sect . 3 . 4 . Assuming the Bernoulli model with parameters θ ( c , t ) , Pr [ d j c ] = (cid:0) n ( d ) f n ( d , t ) g (cid:1) Q t θ ( c , t ) n ( d , t ) , ( 1 ) where (cid:0) n ( d ) f n ( d , t ) g (cid:1) = n ( d ) ! n ( d , t 1 ) ! n ( d , t 2 ) ! (cid:1)(cid:1)(cid:1) is the multinomial coefﬁ - cient . The above Bernoulli model makes the assumption that the term occurrences are uncorrelated , which is certainly not correct . First , given a term has occurred once in a document it is more likely to occur again compared to a term about which we have no information . Second , the term frequency distributions are correlated . Our independence assumption leads to what is called a naive Bayes classiﬁer . ( A naive Bayes classiﬁer in essence builds density functions for each class that are marginally independent , and then classiﬁes a data point based on which density function has the maximum value at that point . ) In 10 We use the term coin here for what is perhaps better called a die . 167 practice , these simple classiﬁers perform surprisingly well compared to more sophisticated ones that attempt to approx - imate the dependence between attributes . Recently , this phenomenon has been investigated in depth by Friedman [ 17 ] . A classiﬁer that uses an estimate of class densities is subject to bias ( decision boundaries that are shifted from the “best” position , because the model is inac - curate ) and variance ( decision boundaries that overﬁt noisy data ) . Friedman analyzes how the low variance of naive den - sity estimates can mitigate the high bias to give simple clas - siﬁers that can often beat more sophisticated ones . It will also be clear from Sect . 3 . 4 that this simplicity lets us de - sign a system that can handle enormous problem sizes . Rare events and laws of succession . We return to the issue of estimating the model parameters θ ( c , t ) , the rate at which term t occurs in documents of class c . The average English speaker uses about 20 , 000 of the 1 , 000 , 000 or more terms in an English dictionary [ 36 ] . In that sense , many terms that occur in documents are “rare events . ” This means that , with reasonably small sample sets , we will see zero occurrences of many , many terms , and will still be required to estimate a non - zero value of f ( c , t ) . The maximum likelihood estimate , f ( c , t ) = n ( c , t ) / n ( c ) , is problematic : a class with f ( c , t ) = 0 will reject any document containing t . Finding such estimates , also called laws of succession , has been pursued in classical statistics for centuries . Laplace showed that , given the results of n tosses of a k - sided coin , i . e . , the number of times each face occurred , n 1 , . . . , n k , the correct Bayesian estimate for the probability of face i , de - noted Pr L ( i jf n i g , n ) , is not n i / n , but n i + 1 n + k [ 26 ] . This is the result of assuming that all possible associated k - component vectors of face probabilities ( p 1 , . . . , p k ) are a priori equally likely . This is called the uniform prior assumption . The above value of Pr L ( i jf n i g , n ) is obtained by using Bayes rule and evaluating 1 Pr [ n i ] R 1 0 θ Pr [ n i j θ ] dθ . Alternative priors have been suggested and justiﬁed . We experimented with many of these , and found that Laplace’s law wins by a few percent better classiﬁcation accuracy all the time . We re - fer the reader to Ristad’s paper for details [ 36 ] . With this adjustment ( and returning to our earlier notation ) , f ( c , t ) is estimated as ( 1 + n ( c , t ) ) / ( n ( c ) + L ( c ) ) , where L ( c ) is the size of the lexicon of class c . The binary model . Thus far , our model has been quite sensitive to the number of times a term is repeated in a document . The obvious criticism is that , given a term has occurred once , we expect it to occur again and again . A model at the other extreme is the binary model , in which the repetition count is ignored . The parameter θ ( c , t ) then becomes the fraction of documents in class c that contained t at least once . Laplace’s correction can be applied here as well . The TAPER framework also makes it possible to explore various models in between Bernoulli and binary , for example , models in which term counts are thresholded or bucketed into ranges , etc . Detailed investigation of the predictive accuracy of such models is left as future work . 3 . 2 Hierarchical classiﬁcation A classiﬁer inputs a document and outputs a class . For a document with a pre - speciﬁed class , if the class output by the classiﬁer does not match the pre - speciﬁed class , we say the classiﬁer misclassiﬁed that document . Typically , a classiﬁer is trained by giving it example documents with class labels attached . Our system has a classiﬁer at each internal node in the taxonomy , with diverse feature sets . Given a new document d , the goal is to ﬁnd a leaf node c such that the posterior Pr [ c j d ] is maximized among all leaves . There is a danger in greedily picking one’s way down the tree : an irrevocable er - ror may be made early in the process [ 24 ] . Let the path to a leaf c from the root be c 1 , c 2 , . . . , c k = c . Since the root sub - sumes all classes , Pr [ c 1 j d ] = 1 for all d . Furthermore , we can write Pr [ c i j d ] = Pr [ c i − 1 j d ] Pr [ c i j c i − 1 , d ] , for i = 2 , . . . , k . Taking logs , log Pr [ c i j d ] = log Pr [ c i − 1 j d ] + log Pr [ c i j c i − 1 , d ] . Suppose in the taxonomy we mark edge ( c i − 1 , c i ) with the edge cost − log Pr [ c i j c i − 1 , d ] . We are then seeking the least cost path from the root c 1 to some leaf . Computing the one - step conditional probability Pr [ c i j c i − 1 , d ] is straightforward . For notational convenience , name c i − 1 as r 0 and its children f r j g . Then , the probability that the document d belongs to the child node r i , given that it belongs to the parent node r 0 , is given by Pr [ r i j r 0 , d ] = Pr [ r i j d ] / Pr [ r 0 j d ] , where Pr [ r 0 j d ] = P j Pr [ r j j d ] ( where P j is over all the siblings of r i ) . Note that Pr [ r i j d ] = Pr [ d , r i ] / P j Pr [ d , r j ] by Bayes’ rule . If we use the Bernoulli model as before , Pr [ d j r j ] = (cid:0) n ( d ) f n ( d , t ) g (cid:1) Q t θ ( r j , t ) n ( d , t ) . Care is needed here with ﬁnite - precision numbers , because the probabilities are very small ( often less than 10 − 5000 ) and the scaling needed to condition the probability prevents us from maintaining the numbers always in log - form . For example , given very small positive numbers p 1 , . . . , p c , which can only be stored as their logs (cid:96) i = log p i , we need to normalize the sum to one , i . e . , compute p i / P j p j = e (cid:96) i / P j e (cid:96) j . We ﬁrst compute µ = max j (cid:96) j and then compute P j e (cid:96) j − µ by adding the terms in increasing order of magnitude . 3 . 3 Feature selection Now we will discuss how to select terms that the classiﬁer will use in its models . Suppose we are given two sets of points in n - dimensional Euclidean space , interpreted as two classes . Fisher’s discriminant method ﬁnds a direction on which to project all the points so as to maximize ( in the resulting one - dimensional space ) the relative class separa - tion as measured by the ratio of interclass to intraclass vari - ance . More speciﬁcally , let X and Y be the point sets , and µ X , µ Y be the respective centroids , i . e . , µ X = ( P X x ) / j X j and µ Y = ( P Y y ) / j Y j . Further , let the respective n (cid:2) n co - variance matrices be Σ X = ( 1 / j X j ) P X ( x − µ X ) ( x − µ X ) T and Σ Y = ( 1 / j Y j ) P Y ( y − µ Y ) ( y − µ Y ) T . Fisher’s discriminant seeks to ﬁnd a vector α such that the ratio of the projected difference in means j α T ( µ X − µ Y ) j to the average variance , 1 2 α T ( Σ X + Σ Y ) α = α T Σα is maximized . It can be shown that α = Σ − 1 ( µ X − µ Y ) achieves the extremum when Σ − 1 exists . Also , when X 168 and Y are drawn from multivariate Gaussian distributions with Σ X = Σ Y , this is the optimal discriminator in that thresholding on α T q for a test point q is the minimum error classiﬁer [ 14 , 47 ] . Computing α involves a generalized eigenvalue prob - lem involving the covariance matrices . In applications like signal processing where Fisher’s discriminant is used , n is typically a few hundred at most ; in the text domain , n is typically 50 , 000 – 100 , 000 ; and the covariance matrices may not be suitably sparse for efﬁcient computation . Moreover , it is hard to interpret a discriminant that is a linear sum of term frequencies , possibly with negative coefﬁcients ! Our approach is to take the directions α as given , namely , a coordinate axis for each term . We assign each term ( i . e . , each dimension ) a ﬁgure of merit , which we call its Fisher index , based on the variance ﬁgures above , which is j α T ( µ X − µ Y ) j α T Σα in the two - class case . For each term t , α = e t is a unit vector in the direction of t . Given the discriminat - ing power of terms , we will pick terms greedily until we get good discrimination between classes . In general , given a set of two or more classes f c g , with j c j documents in class c , we compute the ratio of the so - called between - class to within - class scatter . Switching back to our term frequency notations , we express this as : Fisher ( t ) = P c 1 , c 2 (cid:16) µ ( c 1 , t ) − µ ( c 2 , t ) (cid:17) 2 P c 1 j c j (cid:16) x ( d , t ) − µ ( c , t ) (cid:17) 2 , ( 2 ) where µ ( c , t ) = 1 j c j P d 2 c x ( d , t ) . ( 3 ) The information theory literature provides some other notions of good discriminants . One of the best known is mutual information [ 11 ] . Closer inspection shows that its computation is more complicated and not as easily amenable to the optimizations we implement in our system for the Bernoulli model . We will discuss other related work in text feature selection later in Sect . 5 . The remaining exercise , having sorted terms in decreas - ing order of Fisher index , is to pick a suitable number of top - ranking terms . Let F be the list of terms in our lexicon sorted by decreasing Fisher index . Our heuristic is to pick from F a preﬁx F k of the k most discriminating terms . F k must include most features and exclude most noise terms . If F k is small in size , we can cache a larger portion of the term statistics in memory . This results in faster classiﬁca - tion . Too large an F k will ﬁt the training data very well , but will result in degraded accuracy for test data , due to overﬁtting . There are various techniques for pruning feature sets . We minimize classiﬁcation error on a set of documents kept aside for model validation , shown in Fig . 1 . Some oth - ers approaches are to use the minimum description length principle , resampling or cross - validation . We randomly par - tition the preclassiﬁed samples into T , the training set and V , the validation set . We compute the Fisher index of each term based on T , and then classify V using various pre - ﬁxes F k . Let N k be the number of misclassiﬁed documents using F k ; then we seek that value of k , say k (cid:3) , for which N k is minimized . For classiﬁcation , we choose the class c that maximizes the following a posteriori class probability based on the Bernoulli model introduced in Sect . 3 . 1 : Pr [ c j d , F k ] = π ( c ) Q t 2 d \ F k θ ( c , t ) n ( d , t ) P c 0 π ( c 0 ) Q t 2 d \ F k θ ( c 0 , t ) n ( d , t ) , ( 4 ) where π is the prior distribution on the classes . Let c (cid:3) ( d ) be the “true” class of d 2 V , then N k = P d N k ( d ) , where N k ( d ) = (cid:26) 1 , 9 c / = c (cid:3) ( d ) : Pr [ c j d , F k ] > Pr [ c (cid:3) ( d ) j d , F k ] 0 , otherwise ( 5 ) 3 . 4 Data structures and pseudocode The modules described so far are parts of a topic analysis system that we have built , called TAPER . In building an “industry grade” topic analyzer , we set the following goals . – We must handle thousands of classes and millions of documents ; the current limits are 2 16 classes , 2 32 unique tokens , and 2 32 documents , on a 32 - bit architecture . By using the “ unsigned long long ” datatype or a 64 - bit architecture , we can easily get up to 2 32 classes and 2 64 tokens and documents . We make the reasonable as - sumption that we can hold a simple pointer representa - tion of the taxonomy tree in memory with a few words per node ( class ) . – Training must be essentially on - line , say , as in a crawling and indexing application . Testing or applying must be interactive . As we will see in Sect . 4 , we can train at 140 µ s per token and test at 30 µ s per term . Training should preferably make a single pass over the corpus . – Since our statistics modules maintain combinable aggre - gates , it is simple to incrementally update a ﬁxed taxon - omy with new document , and correct misclassiﬁcations by moving a document from an incorrect class to one ad - judged correct . With some more work , it is also possible to reorganize entire topic subtrees . A sketch of the TAPER system is shown in Fig . 1 . The training documents are randomly split into two subsets : one subset for collecting term statistics and estimating term pa - rameters , and another subset for “pruning” the models by deciding which terms are noise . TAPER supports leave - one - out cross validation , or a pre - liminary partitioning ( typically 13 : 23 ) of the pre - classiﬁed corpus into training and validation documents . The resulting class models restricted to features alone are stored on disk using an indexed access method . For some corpora , these statistics can be effectively cached in main memory . During classiﬁcation the classiﬁer loads , these model statistics on demand and emits a set of the most likely classes for the input document . TAPER has the following modules . – Maps between terms and 32 - bit IDs , and between classes and 16 - bit IDs . – A tree data structure for storing class - speciﬁc informa - tion . – A module for statistics collection . – A module for feature selection . – A module for applying the classiﬁer to a new or test document . We will discuss the last three in detail . 169 Select top ranking features by validation Store term , document and class statistics TrainingDocuments UnlabeledDocuments RandomSplit or Cross - validation Statistics Collection Set Model Validation Set Construct class models restricted to these features Classifier Class Raw Statistics Sort and aggregate Order features by discriminatingpower IndexedStatistics Fig . 1 . A sketch of the TAPER hierarchical feature selection and classiﬁcation engine SF1 TID KCID SMC SNC SF2 Sorted unique key Term Child class c Number of documents in the class c having the term t at least once Total count of the term t in documents in the class c (cid:229) ˛ c d d n t d n ) ( ) , ( 2 ) ( ) , ( (cid:229) ˛ (cid:247)(cid:247)ł(cid:246) (cid:231)(cid:231)Ł(cid:230) c d d n t d n PCID Parent class Fig . 2 . The columns of the frequency table after aggregation on ( TID , PCID , KCID ) . PCID is not explicitly stored ; it is looked up from the taxonomy tree stored in memory 3 . 4 . 1 Statistics collection The goal of this module is to collect term statistics from a document and dispense with it as fast as possible . After simple stopword ﬁltering and stemming while scanning , the document is converted to a sequence of 32 - bit TIDs ( term IDs ) . The main table maintained on disk is the frequency ta - ble shown in Fig . 2 . TID corresponds to a term that occurs in some document belonging to a class corresponding to KCID ( kid class ID ) . CIDs ( KCIDs and PCIDs ) are numbered from one onwards . PCID represents the parent of KCID ( zero if KCID is the root ) . There are four other numeric ﬁelds per row . All these four numbers are additive over documents , so , for each document d and term t , we can just append a row to the frequency table , with SMC set to one , SNC set to the number of times t occurred in d , called n ( d , t ) , SF1 set to n ( d , t ) / P t n ( d , t ) = n ( d , t ) / n ( d ) and SF2 set to ( SF1 ) 2 . SMC is used in the binary model ; SNC is needed in the Bernoulli model . Intermittently , this run is sorted and ag - gregated on ( TID , PCID , KCID ) and merged into a table with 170 TID KCID SMC SNC SF1 SF2 PCID Store term statisticsinto tree S ca n Append a record to the Fisher table Clear tree before next term runs Fig . 3 . Scanning the frequency table and com - puting the Fisher index of each term PCID FI TID Key Class Fisherindex Term Discardedfeatures Selectedfeatures DecreasingFisher index P C I D 1 P C I D 2 Fig . 4 . The Fisher table . When sorted using the ( PCID , FI ) key , we get a contiguous run for each internal topic node , the run containing the best features ﬁrst . If a suitable feature set size is speciﬁed , we can truncate the run to produce a table of feature TIDs for each internal topic unique key ( TID , PCID , KCID ) , other ﬁelds being aggre - gated . PCID is not stored explicitly , as it can be looked up from the taxonomy tree stored in memory . Figure 2 shows the contents of the ﬁelds after such aggregation : SMC con - tains the number of documents in the class KCID that con - tains the term TID at least once , SNC contains the total count of term TID in documents of class KCID , SF1 contains P d 2 c n ( d , t ) / n ( d ) and SF2 contains P d 2 c ( n ( d , t ) / n ( d ) ) 2 , where c corresponds to class KCID . This trades off space for time , and the frequency table grows rather quickly , but with a lot a duplicate keys . De - pending on how much disk space exists , once in a while we must pause to catch our breath and sort and aggregate the duplicate keys . For large corpora , this is vastly preferable to a disk hash table with random I / O . Since sorting is a super - linear operation , it is good to keep the size of the current run of the frequency table small , not only to conserve disk space , but also for faster sorting . We use simple heuristics to start a sort phase . First , we conﬁgure some upper bound to the number of rows for which a sort must be initiated to conserve space . We also estimate the sorting time given the number of rows and the average number of rows that a document adds to the table . From previous sorts , we also maintain an estimate of the fraction of rows compacted by aggregation . We can also do this guided by recent sampling techniques [ 1 ] . Given a moment in the execution and an inﬁnite supply of documents , we can continue training until we hit the up - per bound dictated by space limits , and then sort . We can estimate the time to complete this . We can also estimate the total time needed if we sorted right away , trained the same number of documents ( which need not create an overﬂow ) and sorted again . If the latter estimate is smaller , we initiate a sort . In our current implementation , processing documents stops while the sorting is in progress . To meet tough real - time requirements , one can open a new frequency table and fork a thread , perhaps on another processor , to aggregate the last run while more documents continue to be accepted . We could have chosen an indexed access method in - stead of the frequency table , and looked up and updated SMC , SNC , SF1 and SF2 as we scanned each document . That would have resulted in index lookups and random I / O potentially for every term in the training set . It was far more efﬁcient to append statistics in a logged fashion . The fre - quency table is a temporary ﬁle and no direct indexed access to it is actually required later . Another beneﬁt is compact - ness : this is the most space - intensive phase of training , and we avoid the storage overheads of indexed access and take control of compaction explicitly . The space overhead of stor - ing TID and PCID redundantly is moderate , as the rest of each row is already 18 bytes long . 3 . 4 . 2 Feature selection Before beginning feature selection , we aggregate the fre - quency table one last time , if necessary , to eliminate all du - plicates . We rewind the frequency table and prepare to scan it . At this stage , all rows with the same TID are collected in a contiguous run , going through all CIDs where that TID occurred ( see Fig . 3 ) , with all the children KCID of parent class PCID collected together . We also prepare to output another ﬁle , called the Fisher table . For the following dis - cussion , we will assume it has the format shown in Fig . 4 . Rows are keyed by PCID and a ﬂoating point number FI ( FI stands for Fisher index ) , where for each ﬁxed PCID the 171 PCID KCID SMC SNC TID TID FI PCID Key Key Fishertable Frequencytable SortMerge HashedIndex TID PCID KCID SMC SNC Fig . 5 . Creating an indexed topic model statis - tics table that is used by the classiﬁer while classifying new documents rows are sorted in decreasing order of FI . The last column is the TID whose corresponding CID and FI are the ﬁrst and second columns . Note that the notion of FI is associated only with internal nodes PCID . Because TID is the primary key in the frequency table , as we scan it , we will get a sequence of runs , each run having a ﬁxed TID . Associated with each topic node in memory , we keep a few words of statistics ( derived from SMC , SNC , etc . ) . When we start a run for a given TID , we clear these . As we scan through the various KCID’s for the given TID in the frequency table , we locate the node corresponding to the KCID in the taxonomy and update these statistics . In a large taxonomy , very few of the nodes will be updated during a run . If a node is updated , its parent will be updated as well . We can therefore reset these statistics efﬁciently after each run . When the run for a given TID completes , we can com - pute , exploring only the updated nodes , the Fisher index of that term for every internal node in the taxonomy ( using Eq . 2 ) . For each of these PCIDs , we append a row to the Fisher table . Next , we sort the Fisher table on the key ( PCID , FI ) . This collects all classes into contiguous segments , and for each PCID , orders terms by decreasing values of FI . Consider now the case in which , for each internal topic c , the number k (cid:3) ( c ) of features to pick is speciﬁed to TAPER directly . ( The next section discusses how k (cid:3) is determined in one pass over the portion of the training documents set apart earlier for model pruning . ) Given k (cid:3) ( c ) , we scan the sorted Fisher table , copying the ﬁrst k (cid:3) ( c ) rows for the run corresponding to class c to an output table , and discarding the remaining terms . This involves completely sequential I / O . Next we sort the truncated Fisher table on ( TID , PCID ) and merge it with the frequency table , which is already sorted on ( TID , PCID , KCID ) . We consider rows of the Fisher table one by one . For each row , once we ﬁnd the beginning of a key - matched row of the frequency table , we read it as long as the key remains unchanged , constructing a memory buffer of the form ( KCID , SMC , SNC ) . This buffer is then written into a hash table on disk as shown in Fig . 5 . 3 . 4 . 3 Finding a good cutoff k (cid:3) Given terms in decreasing Fisher index order , we wish to ﬁnd a good value for k (cid:3) , the number of chosen features . Fix a test document d 2 V and consider what happens as we grow the preﬁx k . Typically , d will be misclassiﬁed up to some preﬁx , because there are not enough discriminating terms in the preﬁx , and then at some point it may get cor - rectly classiﬁed owing to some good features being included . For some documents , at a larger preﬁx , a noise term may enter into the feature set and cause d to be misclassiﬁed . Let this 0 - 1 function ( 1 iff d is misclassiﬁed ) be N k ( d ) ; then we seek to ﬁnd P d N k ( d ) for all k . A naive approach to this would be the following pseudocode : Naive algorithm : For all ( or suitably many ) values of k Prepare models with only k features For each document d 2 V Determine N k ( d ) and add on to N k . This approach would make many unnecessary passes over the input text , wasting time tokenizing and mapping IDs . If one has enough disk space , part of this cost can be avoided by storing tokenized text , but a closer look at the determination of N k ( d ) also reveals that computation would also be repeated in determining N k ( d ) for a model M 2 that uses a superset of features used by another model M 1 . In fact , one can compute N k ( d ) under M 2 very efﬁciently and directly , provided the document d is held ﬁxed and succes - sive models are built by adding more and more features . We wish to compute this aggregate function N k in only one pass over all internal taxonomy nodes and all validation documents in V . For a small number of internal nodes in the taxonomy , we can assume that the N k functions for all the internal nodes can be held in memory ( a lexicon of size 100 , 000 takes only 400 KB ) . But for a larger taxonomy , we must be able to contain memory usage . We associate an output ﬁle with every internal node r of the taxonomy . ( If this requires too many open ﬁle han - dles , we resort to simple batch writes and keep most handles closed . ) We start scanning through the validation documents one by one . Consider document d , with an associated ( leaf ) class c . We locate c in the taxonomy , and its parent , say r . Earlier we have computed the Fisher score for all terms seen at node r . We sort the terms in document d in decreas - ing order of this score ; terms in d that are not found at r are discarded . Let these terms have rank t 1 , t 2 , . . . , t j d j . We construct a sequence of classiﬁers for r . The ﬁrst uses terms ranked between 1 and t 1 , the second uses terms ranked be - tween 1 and t 2 , etc . For the classiﬁer using terms ranked 1 172 through t , we see if d is correctly routed from r to c or not , and compare this outcome with that for using terms ranked 1 through t − 1 . If the outcome changes from t − 1 to t , we record the jump in N k ( d ) by writing out a “delta” record “ t , (cid:6) 1” to the output ﬁle associated with r ( + 1 means no error at t − 1 , error at t ; and vice versa ) . Finally , the output ﬁle at each r is sorted on the term rank column , and a cu - mulative sum generates N k for node r . Before discarding d from memory , we perform the above step for all the edges on the path between c and the root of the taxonomy . I / O - efﬁcient algorithm : For each document d 2 V Consider the edge ( r , c ) as above Sort the terms of d in decreasing order of score at r Let the term ranks be t 1 , t 2 , . . . , t j d j For i = 1 , 2 , . . . , j d j Construct a classiﬁer for r with terms ranked 1 . . . t i Let N t i ( d ) be 1 iff this classiﬁer misclassiﬁes d Compare with N t i − 1 ( d ) and output delta if different Repeat for all edges from c to root For each node r Sort ﬁles by term rank ( ﬁrst column ) Compute cumulative sums of the second column 3 . 4 . 4 Updates to the database For a batch job , the large frequency and Fisher tables can now be deleted , leaving the relatively smaller indexed topic statistics and the term - to - TID maps . If the system is used in a setting where new documents will be added to classes , it is necessary to preserve the frequency table . It continues to be used in the same way as before : rows are appended and occasionally it is compacted to aggregate duplicate keys . Running feature selection integrates the new data into the in - dexed statistics . Like running statistics generation for a rela - tional server , feature selection is not an interactive operation . For example , on a database with 2000 classes , average 150 documents per class , and average 100 terms per document , it may take a couple of hours . So this is invoked only when there is reason to believe that the refreshed statistics will improve classiﬁcation . Automatically detecting such times is an interesting question . Another issue is deletion of documents and moving of documents from one class to another ( perhaps because clas - siﬁcation was poor or erroneous for those documents ) . Since feature selection is always preceded by a frequency table ag - gregation , we can always place negative “correction” entries in it ! This means , we produce a frequency table row corre - sponding to each term in the deleted document and negate SMC , SNC , SF1 and SF2 for the class ( es ) the document is being deleted from . ( Here , we cannot ensure that the docu - ment was originally included in the aggregate , but that can be done by preserving IDs for training documents . ) A more difﬁcult issue is the reorganization of the tax - onomy itself . Although the current system leaves this issue unresolved , we believe its design will make reorganization relatively simple . Notice that in TAPER , a parent class in - herits , in an additive fashion , the statistics of its children , since each training document generates rows for each topic node from the assigned topic up to the root . We thus en - visage a procedure of reassigning CIDs and writing out a new frequency table with some negative “correction” en - tries . Consider detaching a subtree under node c 1 and at - taching it elsewhere under node c 2 . Statistics at or above the least common ancestor c (cid:96) of c 1 and c 2 remain unchanged . Negative ( respectively , positive ) rows are appended to the frequency table corresponding to all classes between c (cid:96) in - clusive and c 1 ( respectively , c 2 ) exclusive . And the parent and child links have to be modiﬁed in the taxonomy tree . 3 . 4 . 5 Classiﬁcation The rationale for the data organization described earlier be - comes clear when we consider what happens when the clas - siﬁer is invoked on a document . In the basic API , one loads a taxonomy and its precomputed statistics , and submits a document ( represented by term counts ) to the classiﬁer . In our model , the probability that the document is generated by the root topic is 1 by deﬁnition and decreases down any path in the taxonomy . Accordingly , also speciﬁed in the API is a probability cutoff for nodes reported back as close matches . Consider the document d at some internal node c 0 with children c 1 and c 2 . TAPER needs to intersect d with the feature set at c 0 , then , for each surviving term t , look up the class models for c 1 and c 2 . It is thus best for both space and I / O efﬁciency to index the statistics by ( c 0 , t ) and include in the record a vector of statistics for each c i , i = 1 , 2 . The obvious pseudocode has to be slightly modiﬁed to reﬂect this ( (cid:96) c denotes log Pr [ c j (cid:1) (cid:1) (cid:1) ] ) . Naive index lookup : For each child c i of c 0 , i = 1 , 2 , . . . Initialize (cid:96) c i to 0 For each term t 2 d Lookup term statistics for ( c i , t ) Update (cid:96) c i Normalize P i exp ( (cid:96) c i ) to one Add (cid:96) c 0 to each (cid:96) c i . Optimized index lookup : Initialize all (cid:96) c i to zero For each term t 2 d Skip if key ( c 0 , t ) is not in index Retrieve record for ( c 0 , t ) For each c i that appears in the record Update (cid:96) c i Normalize etc . 3 . 5 Context - sensitive document signatures Up to a point , the user can sift a query response based only on the topic paths . However , even the leaf classes are neces - sarily coarser than individual documents ; support is therefore needed to browse quickly through many documents without looking into the documents in detail . Most search engines 173 attach a few lines from each document . Often these are the title and ﬁrst few lines ; or they are sentences with the most search terms . For many documents , better keyword extrac - tion is needed . Moreover , as we have argued , these signa - tures should be extracted relative to a node in the taxonomy . Given this reference node c , one approach is to concate - nate the training documents associated with c into a super document d , and then rank terms t 2 d in decreasing order of the number of standard deviations that x ( d , t ) is away from f ( c , t ) . Here , our earlier simplistic document model gets into trouble : as mentioned in Sect . 3 . 1 , a term that has occurred once in a document is more likely to occur again . Since the Bernoulli model does not take this into account , frequent terms often remain surprising all along the taxonomy path . Matters are improved by moving to the binary model . First , suppose we have a single test document d , and consider t 2 d . If the observed fraction of training documents in class c containing term t is θ ( c , t ) , we simply sort all t 2 d by increasing θ ( c , t ) and report the top few . If there are (cid:96) > 1 test documents in c , we ﬁnd the fraction φ ( t ) that contains t , and sort the t s in increasing order of ( θ ( c , t ) − φ ( t ) ) p (cid:96) p θ ( c , t ) ( 1 − θ ( c , t ) ) . Both , in fact , correspond to P - values computed using the normal approximation to the binomial distribution . 4 Performance In this section , we study the performance of our system . There are three aspects to performance : ﬁrst , to what extent this paradigm assists in text search and browsing ; second , how accurate our techniques for feature selection and hierar - chical classiﬁcation are ; and third , how efﬁcient or scalable our system is . The ﬁrst item is at this point a matter of qualitative judgement , as is the evaluation of the signature - ﬁnding techniques . The quality of feature selection and clas - siﬁcation can be measured precisely , and we present these results here . As regards efﬁciency and scalability , we quote TAPER ’s running times for speciﬁc platforms , show that they scale favorably with corpus size , and compare with the performance of a well - known text classiﬁer from the prior art [ 3 ] . 4 . 1 Datasets and measures We used three data sources : the Reuters benchmark used widely in the IR community , the US patent database , here - after referred to as USPatent , and Yahoo ! . For evaluation , the simple scenario is an m - class problem , where each doc - ument belongs to exactly one class . We can draw up an m (cid:2) m contingency table , entry ( i , j ) showing how many test documents of class i were judged to be of class j . This is called the confusion matrix . One important number to com - pute from a confusion matrix is the sum of diagonal entries divided by the sum of all elements : this gives the fraction of documents correctly classiﬁed . If each document has exactly one class , this number is the same as microaveraged recall and precision as deﬁned by Lewis [ 27 ] . He also proposes how to evaluate performance on data sets in which docu - ments have multiple classes . The classiﬁer is required to out - put for document d the set C k ( d ) of the k classes having the highest “match . ” Let the set of “true” classes be C (cid:3) ( d ) . Then precision is deﬁned as P d j C k ( d ) \ C (cid:3) ( d ) j / P d j C k ( d ) j and recall is deﬁned as P d j C k ( d ) \ C (cid:3) ( d ) j / P d j C (cid:3) ( d ) j . By changing k , one can achieve a trade - off between recall and precision . The point where recall equals precision is called the break - even point . 4 . 2 Evaluation of feature selection Although Reuters has provided a taxonomy for its articles , the data available does not include taxonomy codes in the class header . For this subsection , we will work with other corpora where such information is explicitly provided . The sample of USPatent that we used has three nodes in the ﬁrst level , Communication , Electricity and Electronics . Each has four children in the second level . Figure 6 shows the taxonomy we used . The overlap in vocabulary between some of the nodes , e . g . , modulator , demodulator , ampliﬁer , oscillator ; and motive , heating , resistor make the classiﬁca - tion task appear more challenging than Reuters , which deals with a more diverse set of topics . 4 . 2 . 1 Classiﬁcation error vs feature set size Figure 7a – d shows the results of validation experiments over the patent database . Five hundred training patents and 300 validation patents were picked at random from each of the 12 leaves in Fig . 6 . The Fisher index ordering gives rapid reduction in classiﬁcation error within just a few hun - dred feature terms , out of the roughly 30 , 000 terms in our lexicon . For some classes , the error goes up slightly ( not visible in the range shown ) after a minimum due to over - ﬁtting . The smallest minima and corresponding errors are roughly at 160 terms , 25 . 1 % for Patent ; 200 terms , 11 . 7 % for Communication ; 890 terms , 17 . 8 % for Electricity ; and 9130 terms , 16 . 6 % for Electronics . The minima are not very sharp , but the diversity of the feature set sizes still questions the common practice of picking a ﬁxed number of most fre - quent terms in each class as features . 4 . 2 . 2 Best discriminants and applications We list the best features in the patent taxonomy below ; notice how the sets change down the levels . Patent : Signal , modulate , motor , receive , antenna , telephone , transmit , fre - quency , modulation , modulator , demodulator , current , voltage , data , carrier , power , ampliﬁer , phase , call , amplitude . Patent : Communication : Antenna , telephone , modulator , demodulator , sig - nal , modulate , output , call , modulation , input , demodulated , frequency , phase , communication , radar , demodulating , space , detector , line , de - modulation , transmit , circuit . Patent : Electricity : Motor , heat , voltage , transistor , output , circuit , connect , input , weld , extend , surface , current , position , gate , speed , control , terminal , drive , regulator , signal , rotor . Patent : Electronics : Ampliﬁer , oscillator , input , output , frequency , transistor , signal , laser , emitter , couple , amplify , gain , resistance , connect , extend , form , contact , differential , material , resistor . We have also applied the feature selection algorithm to ﬁnd salient differences between various sets of documents . 174 Social Science Society & Culture Health Recreation Reference Science Education Entertainment Government Computers & Internet Arts Business & Economy Communication Electricity Electronics 331 Oscillator 330 Amplifier 338 Resistor 361 System 307 Transmission 318 Motive 323 Regulator 219 Heating 329 Modulator 332 Demodulator 379 Telephony 343 Antenna P a t e n t Y a hoo Regional Class code News & Media Fig . 6a , b . Sample of topic taxonomies we have experimented with . a A portion of the US patent database taxonomy with numeric class codes . b The ﬁrst level of a web taxonomy derived from Yahoo ! , with a total of 2 , 218 nodes / Patent 0 20 40 60 80 100 0 100 200 300 400 500 # Features % E rr o r / Patent / Communication 0 20 40 60 80 100 0 100 200 300 400 500 # Features % E rr o r / Patent / Electricity 0 20 40 60 80 100 0 100 200 300 400 500 # Features % E rr o r / Patent / Electronics 0 20 40 60 80 100 0 100 200 300 400 500 # Features % E rr o r Fig . 7a – d . Evaluation of feature selection One application is to ﬁnd descriptions for clusters in unsu - pervised document clustering . For example , the query mouse gets hundreds of responses from the IBM Patent Server . To quickly zoom in to the right notion , one clusters the re - sponse and runs TAPER with the clusters treated as classes . A sample of results is shown : Cluster 1 : Tissue , thymus , transplanted , hematopoietic , treatment , exem - plary , organ , immunocompromised , host , trypsin . Cluster 2 : Computer , keyboard , hand , edge , top , location , keys , support , cleaning . Cluster 3 : Point , select , environment , object , display , correspondence , di - rect , image . 175 ( a ) 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 100 200 300 400 # Features A cc u r a cy Binary Bernoulli ( b ) 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 20 40 60 80 100 % Training set used A cc u r a cy Fig . 8a . Choice between the Bernoulli and binary text models for classiﬁcation . Bernoulli appears consistently superior in our experiments . b Veriﬁcation that the training set is adequate These “cluster digests” lets the user easily reﬁne the query . In the context of the patent database , TAPER can also be used , for example , to compare and contrast the patent portfolios of two assignees or one assignee at two different times . For example , a comparison between Sun Microsystems and Silicon Graphics gives the following : Sun Microsystems Silicon Graphics (cid:15) General - purpose programmable digital computer systems (cid:15) Electrical computers and data - processing systems (cid:15) Integrated circuit , processor , voltage , interface (cid:15) Information - processing system organization (cid:15) Data presentation , computer graphics (cid:15) Surface detail , texture (cid:15) Adjusting resolution or level of detail An even more interesting example is comparing Intel patents in 1993 – 94 with those in 1994 – 95 . TAPER detects a new line of research and patenting in the second year : Intel , 1993 – 94 Intel , 1994 – 95 (cid:15) General - purpose programmable digital computer systems (cid:15) Chip fabrication Counter , input (cid:15) Interactive television bandwidth reduction system (cid:15) Involving difference transmission (cid:15) Field or frame difference (cid:15) Involving adaptive transform coding ( It is possible to get the coherent phrases because the patent database effectively stores them as single terms associated with patents . ) 4 . 2 . 3 Choice of system conﬁgurations Binary vs Bernoulli . TAPER supports both the Bernoulli ( term counts are signiﬁcant ) and the binary ( term count is zero or one ) text models . As we discussed in Sect . 3 . 1 , the Bernoulli model might be criticized as paying too much at - tention to each additional occurrence of a term , so we com - pare Bernoulli with binary in Fig . 8a . The Bernoulli classiﬁer appears to be more effective in exploiting the count infor - mation , so , for the rest of the paper , we restrict ourselves to this model . Sufﬁciency of training data . Before we compare TAPER with other known approaches , we ensure that the training set is adequate . To do this , we train using random samples of increasing sizes from the training set and verify that the test performance has saturated . This is shown in Fig . 8b . 4 . 3 The Reuters benchmark The Reuters benchmark has 7775 training documents and 3019 testing documents from 115 classes . We experimented with Reuters to ensure that our basic classiﬁer is of accept - able quality . Less than a tenth of the articles are assigned multiple classes . In fact , in some cases , some class labels were reﬁnements of others , e . g . , grain and wheat , and it would be incorrect to regard them as classes at the same level , since some classes imply others . In our ﬁrst experiment , we used only the ﬁrst of the classes assigned to each document . Our micro - averaged re - call , which , for this setup , equals the micro - averaged preci - sion , was 87 % . We also studied recall - precision trade - offs in the multi - class scenario . The results are shown in Fig . 9 . Our interpolated breakeven point is 76 % . Since only 9 % of the documents have two or more classes , the best policy is to return the top class at a recall of 71 . 3 % and precision of 88 . 6 % . For this benchmark , there is no beneﬁt from hierarchy . To test the effect of our feature selection , we compared it with an implementation that performs singular value de - composition ( SVD ) on the original term - document matrix , projects documents down to a lower dimensional space , and uses a Bayesian classiﬁer in that space , assuming the Gaussian distribution ( S . Vaithyanathan , personal commu - nication ) . Our classiﬁer was more accurate by 10 – 15 % , in spite of its simplicity . Our explanation is that the SVD , in ignoring the class labels , ﬁnds projections along directions of large variance , which may not coincide with directions of best separation between documents with different classes . 176 0 0 . 2 0 . 4 0 . 6 0 . 8 1 0 0 . 2 0 . 4 0 . 6 0 . 8 1 Recall P r e c i s i on Fig . 9 . Recall - precision trade - off on Reuters . Our break - even point is 76 % Table 1 . Confusion matrix for the weighted cosine one - level classiﬁer . Each row sums to 100 , modulo rounding . The diagonal elements add up to only 0 . 48 of the total number of the documents . This is the microaveraged recall Class name 329 332 343 379 307 318 323 219 330 331 338 361 329 Demodulator 51 9 2 1 0 6 5 2 12 11 0 2 332 Modulator 21 27 3 2 3 7 6 4 10 12 2 2 343 Antennas 10 6 47 3 4 2 6 1 1 4 14 3 379 Telephony 9 1 1 65 1 5 2 5 3 1 5 3 307 Transmission 1 1 1 1 57 2 8 5 0 1 19 4 318 Motive 6 4 1 1 1 41 7 13 14 4 2 3 323 Regulator 8 3 1 3 7 4 59 7 2 1 2 5 219 Heating 2 1 0 0 0 18 9 49 12 1 2 5 330 Ampliﬁer 6 5 1 0 1 17 1 8 53 3 4 1 331 Oscillator 10 2 3 0 6 9 4 7 10 33 13 4 338 Resistor 0 0 0 0 3 0 3 2 0 0 87 4 361 System 2 1 1 1 9 8 8 9 1 1 30 29 Table 2 . Confusion matrix for our multilevel classiﬁer , showing much larger diagonal elements , i . e , more frequently correct classiﬁcation . The microav - eraged recall is 0 . 66 Class name 329 332 343 379 307 318 323 219 330 331 338 361 329 Demodulator 80 5 0 0 0 2 0 3 5 4 0 0 332 Modulator 16 55 1 0 1 2 1 3 9 11 0 0 343 Antennas 5 5 63 1 1 0 2 0 0 2 15 6 379 Telephony 4 2 1 82 0 1 0 2 1 1 1 4 307 Transmission 0 0 0 0 55 2 3 3 0 2 26 8 318 Motive 6 4 0 2 3 48 5 16 8 5 1 2 323 Regulator 3 1 1 2 3 2 81 6 0 0 1 1 219 Heating 1 1 0 0 0 10 4 72 7 0 3 1 330 Ampliﬁer 3 9 0 0 0 10 0 11 57 8 0 1 331 Oscillator 15 8 0 0 0 4 0 7 8 47 5 4 338 Resistor 0 0 0 0 1 0 2 0 1 0 92 4 361 System 1 0 0 0 2 6 6 10 1 1 12 61 4 . 4 Evaluation of hierarchical classiﬁcation In this section , we describe our experience with the hierar - chical USPatent dataset . We compare the hierarchical classi - ﬁer with a standard vector - space - based [ 38 ] classiﬁer . Each document is a vector in term space ; each class is the sum or centroid of its document vectors . The similarity between two vectors is their cosine . Weighting the terms usually re - sults in better relevance ranking . There are over 287 variants of term - weighting schemes with tuned magic constants re - Table 3 . The beneﬁts of hierarchy . The preﬁx ﬁeld in the second row correspond to the four internal nodes in the USPatent tree : / Patent , / Patent / Communication , / Patent / Electricity and / Patent / Electronics Classiﬁer Preﬁx Parameters Recall Time / doc Flat 250 2651 0 . 60 15ms Taxonomy 950 ; 200 ; 400 ; 800 2649 0 . 63 6ms ported [ 38 ] . We pick one version recommended by Sparck - Jones [ 20 , 23 ] . n max ( d ) = max t 2 d n ( d , t ) m = number of classes n t = P c sign ( n ( c , t ) ) w ( c , t ) = ( 1 + n ( d , t ) n max ( d ) ) ( 1 + lg mn t ) Score ( c , d ) = x d (cid:1) w c j x d j j w c j We see a substantial gain in accuracy over the standard weighted - cosine classiﬁer . We did further experiments to see how much of the gains was from feature selection as against the hierarchy . To do this , we can ﬁx the feature selection and classiﬁcation modules , and only change the taxonomy : one will be the taxonomy in Fig . 6 , the other will have the root and the 12 leaves . We have to be very careful to make this a fair competition , making sure that the class models are rep - resented with the same complexity ( number of parameters ) in the two settings . In counting the number of parameters we must also account for the sparsity of the term frequency tables ; we have no direct control on this . By trial and er - ror , we came up with the comparative evaluation shown in Table 3 . In this dataset , the accuracy beneﬁt from hierarchy is modest compared to the beneﬁt from feature selection . How - ever , note that the ﬂat classiﬁer has a steep performance penalty , because it has to compare too many classes all at once . This gap is dramatic for larger taxonomies , such as Yahoo ! . How to allocate a ﬁxed number of parameters among the taxonomy nodes for best overall classiﬁcation is an interesting issue . Summarizing , we showed that our feature selection is effective , and that our classiﬁer is signiﬁcantly more accurate than cosine - based ones and comparable to the best known for ﬂat sets of classes . Hierarchy enhances accuracy in modest amounts , but greatly increases speed . 4 . 5 Running times TAPER has undergone extensive revisions to make it efﬁ - cient . One can specify some maximum amount of memory it should use , and it uses the disk intelligently to stage larger data . Here , we will merely give some examples of the cur - rent performance , not compare it with all the preliminary versions . Also , an extensive comparison with other existing packages on a common hardware platform is left for fu - ture work . TAPER has been evaluated on two platforms : a 133 - MHz RS 6000 / 43P with 128 MB RAM , and a 200 - MHz Pentium - II with 256 MB RAM . On the former , we can train at 140 µ s per term and test at 30 µ s per term . These times 177 are measured after the document is in memory , and they ac - count for the database reorganization costs during training . The whole Reuters experiments , including I / O time for text , takes less than 610 seconds . The running time performance of the classiﬁer of Apte et al [ 3 ] is not published , although its break - even point , at 80 . 5 % , is 4 . 5 % above ours . A classiﬁer using a similar tech - nology and with similar accuracy called KitCat has a train - ing time that is about 60 times the training time of TAPER . On larger corpora where efﬁcient I / O management is more important , the difference in performance is even more sig - niﬁcant . On the latter platform , 266 , 000 web documents from 2118 Yahoo ! classes have been trained ( from a disk image obtained by crawling ) in about 19 h . TAPER processes doc - uments that are several hundred words long in about the time needed for a disk access . This makes it possible to directly connect TAPER to a web crawler and populate the search database with topic information as well as keyword index , on the ﬂy . 5 Related work We survey the following overlapping areas of related re - search and point out differences with our work where appro - priate : IR systems and text databases , data mining , statistical pattern recognition , and machine learning . Data mining , machine learning , and pattern recognition . The supervised classiﬁcation problem has been addressed in statistical decision theory ( both classical [ 44 ] and Bayesian [ 5 ] ) , statistical pattern recognition [ 14 , 18 ] and machine learning [ 5 , 32 , 45 ] . Classiﬁers can be parametric or non - parametric . Two well - known classes of non - parametric clas - siﬁers are decision trees , such as CART [ 6 ] and C4 . 5 [ 34 ] , and neural networks [ 21 , 22 , 28 ] . For such classiﬁers , feature sets larger than 100 are considered extremely large . Docu - ment classiﬁcation may require feature selection from more than 500 , 000 features , and result in models with over 10 , 000 features . IR systems and text databases . The most mature ideas in IR , which are also successfully integrated into commer - cial text search systems such as Verity 11 , ConText 12 and AltaVista , involve processing at a relatively syntactic level ; e . g . , stopword ﬁltering , tokenizing , stemming , building in - verted indices , computing heuristic term weights , and com - puting similarity measures between documents and queries in the vector - space model [ 16 , 39 , 42 ] . More recent work includes statistical modeling of doc - uments , unsupervised clustering ( where documents are not labeled with topics and the goal is to discover coherent clus - ters ) [ 2 ] , supervised classiﬁcation ( as in our work ) [ 3 , 10 ] , query expansion [ 40 , 43 ] . Singular value decomposition on the term - document matrix has been found to cluster seman - tically related documents together even if they do not share keywords [ 13 , 33 ] . For a survey see [ 15 ] . Contrast with our work . Our work emphasizes the per - formance issues of feature selection and classiﬁcation raised 11 http : / / www . verity . com 12 http : / / www . oracle . com / products / oracle7 / oracle7 . 3 / html / context qa . html by corpora ranging into tens to hundreds of gigabytes , rather than exploring many variants of learning algorithms on small corpora and lexicon ( 10 , 000 documents and terms ) . Most closely related to our work [ 7 ] are the concurrent investigations made by Koller and Sahami [ 24 ] and Yang and Pedersen [ 46 ] . Koller and Sahami propose a sophisti - cated feature selection algorithm that uses a Bayesian net to learn interterm dependencies . The complexity in the num - ber of features is supralinear ( e . g . , quadratic in the number of starting terms and exponential in the degree of depen - dence between terms ) . Consequently , the reported experi - ments have been restricted to a few thousand features and documents . Yang and Pedersen’s experiments appear to in - dicate that much simpler methods sufﬁce , in particular , that the approach of Apte et al . [ 3 ] of picking a ﬁxed fraction of most frequent terms per class performs reasonably . There is a possible danger that this fraction is very sensitive to corpus and methodology ( e . g . , whether stemming and stop - wording is performed ) . This is indicated by the poor perfor - mance of such simplistic methods observed in recent work by Mladenic [ 30 ] . Our goal has been to look for techniques that have good statistical foundation , while remaining within almost linear time and one pass over the corpus , even when doing feature selection simultaneously for many nodes in a large topic tax - onomy . Koller and Sahami also emphasize the importance of hierarchies , but they use a greedy search for the best leaf and point out the potential dangers of this approach . Our formulation ﬁxes this problem . Also , our approach of com - puting context - dependent document signatures to aid search and browsing appears to be a new extension to the scatter - gather type of retrieval interface [ 12 ] . 6 Conclusion We have demonstrated that hierarchical views of text data - bases can improve search and navigation in many ways , and presented some of the tools needed to maintain and navigate in such a database . A combination of hierarchy , feature selection , and context - sensitive document signatures greatly enhanced the retrieval experience . Our work raises several questions for future investiga - tion . Usually , TAPER ﬁnds good feature set sizes indepen - dently for each internal node ; the space needed to store the resulting model was not explicitly controlled . In Sect . 4 . 4 , we raised the question of designing classiﬁers that maxi - mize accuracy given bounded space , i . e . , model size , and bounded time . Table 3 suggests the interesting problem of allocating the total space among nodes of a hierarchy for best overall accuracy . Resolving the following performance issue can greatly cut down space and time during training : is it possible to prune off terms with poor Fisher index even as term statistics are being collected ? Another issue related to accuracy is whether the classiﬁer can reliably stop at a shallow level of the tree when classifying a document about which it is uncertain . Finally , in recent work , we have found that adding hyperlink information to the feature set used by TAPER greatly improves accuracy [ 8 ] . 178 Acknowledgements . Thanks to Mark Jackson , Alex Miller , Bruce H¨onig , and Carol Thompson from the IBM Patent Server Team , to Chandra Chekuri , Mike Goldwasser , and Eli Upfal for helpful discussions , and to Sunita Sarawagi and Martin van den Berg for comments on the paper . References 1 . Alon N , Matias Y , Szegedy M ( 1996 ) The space complexity of ap - proximating the frequency moments . In : Symposium on the Theory of Computing ( STOC ) , May 1996 , Philadelphia , Pa . , pp 20 – 29 2 . Anick P , Vaithyanathan S ( 1997 ) Exploiting clustering and phrases for context - based information retrieval . In : SIGIR , 1997 3 . Apte C , Damerau F , Weiss SM ( 1994 ) Automated learning of decision rules for text categorization . IBM Research Report RC18879 4 . Balabanovic M , Shoham Y ( 1997 ) Content - based , collaborative rec - ommendation . Commun ACM 40 ( 3 ) : 66 – 72 5 . Berger JO ( 1985 ) Statistical Decision Theory and Bayesian Analysis . Springer , Berlin Heidelberg New York 6 . Breiman L , Friedman JH , Olshen RA , Stone CJ ( 1984 ) Classiﬁcation and Regression Trees . Brooks / Cole , Paciﬁc Grove , Calif 7 . Chakrabarti S , Dom B , Agrawal R , Raghavan P ( 1997 ) Using taxon - omy , discriminants , and signatures for navigating in text databases . In : VLDB Conf , August 1997 , Athens , Greece 8 . Chakrabarti S , Dom B , Indyk P ( 1998 ) Enhanced hypertext categoriza - tion using hyperlinks . In : SIGMOD ACM , 1998 9 . Chekuri C , Goldwasser M , Raghavan P , Upfal E ( 1996 ) Web search using automatic classiﬁcation . In : Sixth World Wide Web Conference , 1996 , San Jose , Calif 10 . Cohen WW , Singer Y ( 1996 ) Context - sensitive learning methods for text categorization . In : SIGIR ACM , 1996 11 . Cover TM , Thomas JA ( 1991 ) Elements of Information Theory . Wiley , Chichester 12 . Cutting DR , Karger DR , Pedersen JO ( 1993 ) Constant interaction - time scatter / gather browsing of very large document collections . In : SIGIR , 1993 13 . Deerwester S , Dumais ST , Landauer TK , Furnas GW , Harshman RA ( 1990 ) Indexing by latent semantic analysis . J Soc Inf Sci 41 ( 6 ) : 391 – 407 14 . Duda R , Hart P ( 1973 ) Pattern Classiﬁcation and Scene Analysis . Wi - ley , New York 15 . Falaoutsos C , Oard DW ( 1995 ) A survey of information retrieval and ﬁltering methods . Technical Report CS - TR - 3514 . University of Mary - land , College Park , MD20742 16 . Frakes WB , Baeza - Yates R ( 1992 ) Information retrieval : Data struc - tures and algorithms . Prentice - Hall , Englewood Cliffs , NJ 17 . Friedman JH ( 1997 ) On bias , variance , 0 / 1 loss , and the curse - of - dimensionality . Data Min Knowl Discovery 1 ( 1 ) : 55 – 77 18 . Fukunaga K ( 1990 ) An Introduction to Statistical Pattern Recognition , 2nd ed . Academic Press , New York 19 . Goldberg D , Nichols D , Oki B , Terry D ( 1992 ) Using collaborative ﬁltering to weave an information tapestry . Commun ACM 35 ( 12 ) : 61 – 70 20 . Harman D Ranking algorithms . In : Frakes WB , Baeza - Yates R ( eds ) Information retrieval : Data structures and algorithms , Chapter 14 . Prentice - Hall , Englewood Cliffs , N . J . 21 . Hush DR , Horne BG ( 1993 ) Progress in supervised neural networks . IEEE Signal Process Mag : 8 – 39 22 . Jain AK , Mao J , Mohiuddin K ( 1996 ) Artiﬁcial neural networks : A tutorial . Computer 29 ( 3 ) : 31 – 44 23 . Jones KS A statistical interpretation of term speciﬁcity and its appli - cations in retrieval . J Documentation 28 ( 1 ) : 11 – 20 24 . Koller D , Sahami M ( 1997 ) Hierarchically classifying documents using very few words . In : International Conference on Machine Learning , Vol . 14 , 1997 , Morgan - Kaufmann , San Mateo , Calif 25 . Langley P ( 1996 ) Elements of Machine Learning . Morgan Kaufmann , San Mateo , Calif 26 . Laplace P - S ( 19959 Philosophical Essays on Probabilities ( Translated by A . I . Dale from the 5th French edition of 1825 ) . Springer , New York 27 . Lewis D ( 1991 ) Evaluating text categorization . In : Proceedings of the Speech and Natural Language Workshop , 1991 , Morgan - Kaufmann , San Mateo , Calif . , pp312 – 318 28 . Lippmann RP ( 1989 ) Pattern classiﬁcation using neural networks . IEEE Commun Mag : 47 – 64 29 . Miller BN , Reidl JT , Konstan JA ( 1997 ) Experiences with GroupLens : Making Usenet useful again . In : USENIX Annual Technical Confer - ence , 1997 , pp219 – 233 30 . Mladenic D ( 1998 ) Feature subset selection in text - learning . In : 10th European Conference on Machine Learning , 1998 31 . Mirsky S ( 1997 ) Untangling the web . IBM Res Mag 4 : 8 – 10 32 . Natarajan BK ( 199 ) Machine Learning : A Theoretical Approach . Morgan - Kaufmann , San Mateo , Calif 33 . Papadimitriou C , Raghavan P , Tamaki H , Vempala S ( 1996 ) Latent sematic indexing : A probabilistic analysis . 34 . Quinlan JR ( 1993 ) C4 . 5 : Programs for Machine Learning . Morgan Kaufmann , San Mateo , Calif 35 . Raghavan P ( 1997 ) Information retrieval algorithms : A survey . In : Symposium on Discrete Algorithms , 1997 36 . Ristad ES ( 1995 ) A natural law of succession . Research report CS - TR - 495 - 95 , Princeton University , Princeton , Calif 37 . Robertson SE , Walker S ( 1994 ) Some simple effective approximations to the 2 - Poisson model for probabilistic weighted retrieval . In : SIGIR , 1994 , pp232 – 241 38 . Salton G , Buckley C ( 1988 ) Term - weighting approaches in automatic text retrieval . Inf Process Manage 24 ( 5 ) : 513 – 523 39 . Salton G , McGill MJ ( 1983 ) Introduction to Modern Information Re - trieval . McGraw - Hill , New York 40 . Schutze H , Hull DA , Pederson JO ( 1995 ) A comparison of classiﬁers and document representations for the routing problem . In : SIGIR , 1995 , pp229 – 237 41 . Shardanand U , Maes P ( 1995 ) Social information ﬁltering : Algorithms for automating word of mouth . In : Computer Human Interaction ( CHI ) , 1995 , ACM Press , New York 42 . van Rijsbergen CJ ( 1979 ) Information Retrieval . Butterworths , Oxford ( Also available on - line at http : / / www . dcs . gla . ac . uk / Keith / Preface . html ) 43 . Voorhees EM ( 1993 ) Using WordNet to disambiguate word senses for text retrieval . In : SIGIR , 1993 , pp171 – 180 44 . Wald A ( 1950 ) Statistical Decision Functions . Wiley , New York 45 . Weiss SM , Kulikowski CA ( 1990 ) Computer Systems That Learn . Morgan - Kaufmann , San Mateo , Calif 46 . Yang Y , Pedersen J ( 1997 ) A comparative study on feature selection in text categorization . In : International Conference on Machine Learning , 1997 , pp412 – 420 47 . Young TY , Calvert TW ( 1974 ) Classiﬁcation , Estimation and Pattern Recognition . Elsevier Science Publishers , Amsterdam 48 . Zipf GK ( 1949 ) Human Behavior and the Principle of Least Effort : An Introduction to Human Ecology . Addison - Wesley , Reading , Mass