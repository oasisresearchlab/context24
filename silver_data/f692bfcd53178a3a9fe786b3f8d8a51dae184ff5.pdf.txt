ModelHub : Lifecycle Management for Deep Learning Hui Miao , Ang Li , Larry S . Davis , Amol Deshpande University of Maryland , College Park , MD , USA { hui , angli , lsd , amol } @ cs . umd . edu ABSTRACT Deep learning has improved state - of - the - art results in many impor - tant ﬁelds , and has been the subject of much research in recent years , leading to the development of several systems for facili - tating deep learning . Current systems , however , mainly focus on model building and training phases , while the issues of data man - agement , model sharing , and lifecycle management are largely ig - nored . Deep learning modeling lifecycle contains a rich set of arti - facts , such as learned parameters and training logs , and frequently conducted tasks , e . g . , to understand the model behaviors and to try out new models . Dealing with such artifacts and tasks is cumber - some and left to the users . To address these issues in a comprehen - sive manner , we propose M odel H ub , which includes a novel model versioning system ( dlv ) ; a domain speciﬁc language for searching through model space ( DQL ) ; and a hosted service ( M odel H ub ) to store developed models , explore existing models , enumerate new models and share models with others . 1 . INTRODUCTION Deep learning models ( also called deep neural networks ) have dramatically improved state - of - the - art results for many important reasoning and learning tasks including speech recognition , object recognition , and natural language processing in recent years [ 10 ] . Learned using massive amounts of training data , DNN models have superior generalization capabilities , and the intermediate layers in many deep learning models have been proven useful in providing e ﬀ ective semantic features that can be used with other learning techniques . However , there are many critical large - scale data man - agement issues in learning , storing , sharing , and using deep learn - ing models , which are largely ignored by researchers today , but are coming to the forefront with the increased use of deep learn - ing in a variety of domains . In this proposal , we discuss some of those challenges , and propose a comprehensive platform to address them . Given the large scale of data involved ( both training data and the learned models themselves ) , we argue that database researchers should play a much larger role in this area . Deep Neural Networks : We begin with a brief , simpliﬁed overview . A deep learning model is a deep neural network ( DNN ) consisting of many layers having nonlinear activation functions that are ca - pable of representing complex transformations between input data and desired output . Let D denote a data domain and O denote a prediction label domain ( e . g . , D may be a set of images ; O may be the names of the set of objects we wish to recognize ) . As with any prediction model , a DNN is a mapping function f : D → O that minimizes a certain loss function L , and is of the following form : f 0 = σ 0 ( W 0 d + b 0 ) d ∈ D f i = σ i ( W i f i − 1 + b i ) 0 < i ≤ n L ( f n , l d ) l d ∈ O d f 0 f 1 f 2 ˆ l d Here i denotes the layer number , ( W i , b i ) are learnable weights and bias parameters in layer i , and σ i is an activation function that non - linearly transforms the result of the previous layer ( common activa - tion functions include sigmoid , ReLU , etc . ) . Given a learned model and an object d , applying f 0 , f 1 , . . . , f n in order gives us the predic - tion label for that object . In the training phase , the model param - eters are learned by minimizing L ( f n , l d ) , typically done through iterative methods , such as stochastic gradient descent . DNN Modeling Lifecycle and Challenges : Compared with the tra - ditional approach of feature engineering followed by model learn - ing [ 14 ] , deep learning is an end - to - end learning approach , i . e . , the features are not given by a human but learned in an automatic man - ner from the input data . Moreover , the features are complex and have a hierarchy along with the network representation . This re - quires less domain expertise and experience from the modeler , but understanding and explaining the learned models is di ﬃ cult ; why even well - studied models work so well is still a mystery and un - der active research . Thus , when developing new models , chang - ing the learned model ( especially its network structure and hyper - parameters ) becomes an empirical search task . Create / Update Model ReferenceModels Train / Test Model Evaluate Model Data & Labels if accuracy is unsatisfactory , repeat ServeModel Figure 1 : Deep Learning Modeling Lifecycle In Fig . 1 , we show a typical deep learning modeling lifecycle . Given a prediction task , a modeler often starts from well - known models which have been successful in similar task domains ; she then speciﬁes input training data and output loss functions , and re - peatedly adjusts the DNN on operators and connections like Lego bricks , tunes model hyper - parameters , trains and evaluates the model , and repeats this loop until prediction accuracy does not improve . Due to a lack of understanding about why models work , the ad - justments and tuning inside the loop are driven by heuristics , e . g . , adjusting hyper - parameters that appear to have a signiﬁcant impact on the learned weights , applying novel layers or tricks seen in re - cent empirical studies , and so on . Thus , many similar models are trained and compared , and a series of model variants need to be explored and developed . Due to the expensive learning / training phase , each iteration of the modeling loop takes a long period of time and produces many ( checkpointed ) snapshots of the model . The modeling lifecycle exposes several systems and data man - agement challenges , which include : • It is di ﬃ cult to keep track of the many models developed and / or understand the di ﬀ erences amongst them . It is common to see a modeler write all conﬁgurations in a spreadsheet to keep track of temporary folders of input , setup scripts , snapshots and logs . • The development lifecycle itself has time - consuming repetitive sub - steps , such as adding a layer at di ﬀ erent places to adjust ModelHub Client Hosted DLV Repositories ModelHub Server ModelSearch Model Publish Model Learning Module DQL Module DQL Parser Query Optimizer DLV Module Command - line tool Interpreter caffe Wrapper Local DLV Repository publish reuse manage versions explore models enumerate models Modeler Figure 2 : M odel H ub System Architecture a model , searching through a set of hyper - parameters for the di ﬀ erent variations , reusing learned weights to train models , etc . • Similar models are possibly trained and run multiple times , reusing others’ weights as initialization , either because there is a change in the input or an error found in the same place ; there is also a need to keep track of multiple model versions over time . • The storage footprint of deep learning models tends to be very large . Recent top - ranked models in the ImageNet task have mil - lions of parameters and require hundreds of megabytes to store one snapshot during training . Due to resource constraints , the modeler has to limit the number of snapshots , even drop all snapshots of a model at the cost of retraining when needed . • Sharing and reusing models is not easy , especially because of the large model sizes and specialized tools used for learning . ModelHub : We propose the M odel H ub system to address these challenges . It consists of three key components : ( a ) a model ver - sioning system ( DLV ) to store and query the models and their ver - sions , ( b ) a model enumeration and hyper - parameter tuning domain speciﬁc language ( DQL ) to serve as an abstraction layer to help mod - elers focus on the creation of the models instead of repetitive steps in the lifecycle , ( c ) a hosted deep learning model sharing system ( M odel H ub ) to publish , discover and reuse models from others . The M odel H ub system is designed to be used with current training systems ( e . g . , caffe ) for both learning and evaluation . We focus on describing the key M odel H ub functionality in this proposal . In ongoing work , we are developing novel storage tech - niques for compactly storing large numbers of such models , algo - rithms for aligning two similar models , and approaches for e ﬃ - ciently executing complex DQL queries and searching through the user - speciﬁed search space . Related Work : There have been several high - proﬁle deep learning systems in recent years , but those typically focus on training as - pects ( e . g . , distributed training , utilizing GPUs , etc . ) [ 8 , 6 , 1 , 4 ] . The data management and lifecycle management challenges dis - cussed above have been largely ignored so far , but are becoming critical as the use of deep learning permeates through a variety of application domains , since those pose a high barrier to entry for many potential users . In the database community , there has been increasing work on developing general - purpose systems for sup - porting machine learning [ 9 , 12 , 11 ] , including pushing predictive models into databases [ 2 , 7 ] , accelerating learning by optimizing physical design [ 14 ] , and managing modeling lifecycles and serv - ing predictive models in advanced ways [ 13 , 5 ] . M odel H ub is mo - tivated by similar principles ; aside from a speciﬁc focus on deep learning models , M odel H ub also supports versioning as a ﬁrst - class construct [ 3 ] which di ﬀ erentiates it from that work . 2 . MODELHUB OVERVIEW Type Command Description version control init Initialize a dlv repository . add Add model ﬁles to be committed . commit Commit the added ﬁles . model exploration list List models and related lineages . desc Describe a particular model . diff Compare multiple models . eval Evaluate a model with given data . model enumeration query Run DQL clause . remote interaction publish Publish a model to ModelHub . search Search models in ModelHub . pull Download from ModelHub . Table 1 : A list of key dlv utilities . 2 . 1 System Architecture We show the M odel H ub architecture including the key compo - nents and their interactions in Fig . 2 . DLV is a version control sys - tem ( VCS ) implemented as a command - line tool ( dlv ) , that serves as an interface to interact with the rest of the components . Use of a specialized VCS instead of a general - purpose VCS such as git or svn allows us to better portray and query the internal structure of the artifacts generated in a modeling lifecycle , such as network def - initions , training logs , binary weights , and relationships between models . The key utilities of dlv are listed in Table 2 . 1 , grouped by their purpose ; we explain these in further detail in Sec . 2 . 3 . DQL is a DSL we propose to assist modelers in deriving new models ; the DQL query parser and optimizer components in the ﬁgure are used to support this language . The model learning module interacts with external deep learning tools that the modeler uses for training and testing . In the demonstration , we implement a concrete model learning module on top of caffe , which is a popular deep learning training system for computer vision modelers [ 8 ] . Finally , the M od - el H ub service is a hosted toolkit to support publishing , discovering and reusing models , and serves similar role for DNN models as github for software development or DataHub for data science [ 3 ] . 2 . 2 Data Model M odel H ub works on two levels of data models : conceptual DNN model , and data model for the model versions in the DLV repository . DNN Model : A DNN model can be understood in di ﬀ erent ways , as one can tell from the di ﬀ erent model creation APIs in popular deep learning systems . In the formulation mentioned in Sec . 1 , if we view a function f i as a node and dependency relationship ( f i , f i − 1 ) as an edge , it becomes a directed acyclic graph ( DAG ) . De - pending on the granularity of the function in the DAG , either at the tensor arithmetic operator level ( add , multiply ) , or at a logical com - position of those operators ( convolution layer , full layer ) , it forms di ﬀ erent types of DAGs . In M odel H ub , we consider a DNN model node as a composition of unit operators ( layers ) , often adopted by computer vision models . The main reason for the decision is that we focus on the productivity improvement in the lifecycle , rather than the implementation e ﬃ ciencies for training and testing . VCS Data Model : When managing DNN models in the VCS repos - itory , a model version represents the contents in a single version . It consists of a network deﬁnition , a collection of weights ( each of which is a value assignment for the weight parameters ) , a set of extracted metadata ( such as hyper - parameter , accuracy and loss generated in the training phase ) , and a collections of ﬁles used to - gether with the model instance ( e . g . , scripts , datasets ) . In addition , we enforce that a model version must be associated with a human readable name for better utility , which reﬂects the logical groups of a series of improvement e ﬀ orts over a DNN model in practice . In the implementation , model versions can be viewed as a rela - tion M ( name , id , N , W , M , F ) , where id is part of the primary key of model versions and is auto - generated to distinguish model ver - sions with the same name . In brief , N , W , M , F are the network deﬁnition , weight values , extracted metadata and associated ﬁles respectively . The DAG , N , is stored with two EDBs Node ( id , node , A ) , where A is a list of attributes such as name , and Edge ( from , to ) . W , M , F is not discussed in detail in the demonstration proposal due to space limits . Besides a set of model version s , the lineage of the model versions are captured using a separate parent relation , P . All of these relations need to be maintained / updated when the user runs the di ﬀ erent dlv commands that update the repository . 2 . 3 Query Facilities The query facilities we provide can be categorized into two types : a ) model exploration queries and b ) model enumeration queries . 2 . 3 . 1 Model Exploration Queries Model exploration queries interact with the models in a reposi - tory , and the users use them to understand a particular model , query lineages of the models , and compare several models . For usability , we design it as query templates via dlv sub - command with options , similar to other VCS . We describe the queries in the following sub - sections followed by query templates with most important options . List Models & Related Lineages : By default , the query lists all versions of all models including their commit descriptions and par - ent versions ; it also takes options , such as showing results for a particular model , or limiting the number of versions to be listed . dlv list [ - - model _ name ] [ - - commit _ msg ] [ - - last ] Describe Model : dlv desc shows the modeler extracted metadata from a model version , such as the network deﬁnition , learnable pa - rameters , execution footprint ( memory and runtime ) , activations of convolution networks , weight matrices , and evaluation results across iterations . Note the activation is the intermediate output of a DNN model in computer vision and often used as an important tool to understand the model . The current output formats are a result of discussions with computer vision modelers to deliver tools that ﬁt their needs . In addition to printing to console , the query sup - ports HTML output for displaying the images and visualizing the weight distribution , part of which is shown in Fig . 3 . dlv desc [ - - model _ name | - - version ] [ - - output ] Compare Models : dlv diff takes a list of model names or version ids and allows the modeler to compare the DNN models . Most of desc components are aligned and returned in the query result side by side . One challenge as well as motivation of model comparison , is that the models often have subtle di ﬀ erences , and an alignment needs to be done before composing the result . dlv diff [ - - model _ names | - - versions ] [ - - output ] Evaluate Model : dlv eval runs test phase of the managed models with an optional conﬁg specifying di ﬀ erent data or changes in the current hyper - parameters . The main usages of exploration query are two - fold : 1 ) for the users to get familiar with a new model , 2 ) for the user to test known models on di ﬀ erent data or settings . The query returns the accuracy and optionally the activations . It is worth pointing out that complex evaluations can be done via DQL . dlv eval [ - - model _ name | - - versions ] [ - - config ] 2 . 3 . 2 Model Enumeration Queries Model enumeration queries are used to explore variations of cur - rently available models in a repository by changing network struc - tures or tuning hyper - parameters . There are several operations that need to be done in order to derive new models : 1 ) Select models from the repository to improve ; 2 ) Slice particular models to get reusable components ; 3 ) Construct new models by mutating the existing ones ; 4 ) Try the new models on di ﬀ erent hyper - parameters and pick good ones to save and work with . When enumerating models , we also want to stop exploration of bad models early . To support this rich set of requirements , we propose the DQL do - main speciﬁc language , that can be executed using “ dlv query ” . Challenges of designing the language are a ) the data model is mixed with relational and the graph data models and b ) the enumeration include hyper - parameter tuning as well as network structure muta - tions , which are very di ﬀ erent operations . A thorough explanation of the language model is beyond the scope of the demo proposal , instead we show the key operators and constructs along with a set of examples ( Query 1 ∼ 4 ) to show how requirements are met . Query 1 : DQL select query to pick the models . select m1 where m1 . name like " alexnet _ % " and m1 . creation _ time > " 2015 - 11 - 22 " and m1 [ " conv [ 1 , 3 , 5 ] " ] . next has POOL ( " MAX " ) Query 2 : DQL slice query to get a sub - network . slice m2 from m1 where m1 . name like " alexnet - origin % " mutate m2 . input = m1 [ " conv1 " ] and m2 . output = m1 [ " fc7 " ] Query 3 : DQL construct query to derive more models on existing ones . construct m2 from m1 where m1 . name like " alexnet - avgv1 % " and m1 [ " conv * ( $ 1 ) " ] . next has POOL ( " AVG " ) mutate m1 [ " conv * ( $ 1 ) " ] . insert = RELU ( " relu $ 1 " ) Query 4 : DQL evaluate query to enumerate models with di ﬀ erent net - work deﬁnitions , search hyper - parameters , and eliminate models . evaluate m from " query3 " with config = " path to config " vary config . base _ lr in [ 0 . 1 , 0 . 01 , 0 . 001 ] and config . net [ " conv * " ] . lr auto and config . input _ data in [ " path1 " , " path2 " ] keep top ( 5 , m [ " loss " ] , 100 ) Key Operators : We adopt the standard SQL syntax to interact with the repository . DQL views the repository as a single model ver - sion table . A model version instance is a DAG , which can be viewed as object types in modern SQL conventions . In DQL , at - tributes can be referenced using attribute names ( e . g . m1 . name , m1 . creation _ time , m2 . input , m2 . output ) , while navigating the internal structures of the DAG , i . e . the Node and Edge EDB , we provide a regexp style selector operator on a model version to ac - cess individual DNN nodes , e . g . m1 [ " conv [ 1 , 3 , 5 ] " ] in Query 1 ﬁlters the nodes in m1 . Once the selector operator returns a set of nodes , prev and next attributes of the node allow 1 - hop traversal in the DAG . Note that POOL ( " MAX " ) is one of the standard built - in node templates for condition clauses . Using SPJ operators with object type attribute access and the selector operator , we allow re - lational queries to be mixed with graph traversal conditions . To retrieve reusable components in a DAG , and mutate it to get new models , we provide slice , construct and mutate operators . Slice originates in programming analysis research ; given an input and an output node , it returns a subgraph including all paths from the in - put to the output and the connections which are needed to produce the output . Construct can be found in graph query languages such as SPARQL to create new graphs . We allow construct to derive new DAGs by using selected nodes to insert nodes inside an out - going edges or to delete outgoing edge connecting to another node . Figure 3 : Describing a model using dlv desc Mutate limits the places where insert and delete can occur . For ex - ample , Query 2 and 3 generate reusable subgraphs and new graphs . Query 2 slices a sub - network from matching models between con - volution layer ‘conv1’ and full layer ‘fc7’ , while Query 3 derives new models by appending a ReLU layer after all convolution layers followed by an average pool . All queries can be nested . Finally , evaluate can be used to try out new models , with poten - tial for early out if expectations are not reached . We separate the network enumeration component from the hyper - parameter turning component ; while network enumeration can be nested in the from clause , we introduce a with operator to take an instance of a tuning conﬁg template , and a vary operator to express the combination of activated multi - dimensional hyper - parameters and search strate - gies . auto is keyword implemented using default search strategies ( currently grid search ) . To stop early and let the user control the stopping logic , we introduce a keep operator to take a rule of stop - ping conditions templates , such as top - k of the evaluating models , and accuracy threshold . Query 4 evaluates the models constructed and tries combinations of at least three di ﬀ erent hyper - parameters , and keeps the top 5 models w . r . t . the loss after 100 iterations . 3 . DEMONSTRATION PLAN In the demonstration , we introduce the current version of our M odel H ub system and illustrate how it manages the modeling life - cycle and DNN models . We prepare a collection of development models for face recognition . The goal is to train a DNN on a large face dataset ( CASIA - WebFace ) 1 , which contains 494 , 414 face im - ages from 10 , 575 subjects . The DNN models have been created over a period of time , and were not managed in M odel H ub by default . Along with the models , the experiment spreadsheets and setup scripts are shown as they are . The conference attendees can use di ﬀ erent components of the M odel H ub system to walk through the deep learning modeling lifecycle and explore real models . Interacting with dlv , the users are shown the full list of com - mands it currently supports , including the ones in Table 2 . 1 . We use dlv to demonstrate how to manage models using dlv add and dlv commit and the beneﬁt of metadata management with version - ing capacity . Furthermore , the managed models can be explored and compared using dlv desc and dlv diff . In Fig . 3 , we show an HTML output of the dlv desc . In the screenshot , the details of the ﬁrst convolution layer and the next ReLU layer are shown . On the left side , the layer parameters such as layer conﬁgurations , learnable parameter dimensions , and range of activation values are 1 CASIA - WebFace : http : / / goo . gl / 2KKGQM shown , while on the right side , the activation responses and the weight matrix of each channel are shown . After using dlv to manage and understand the models , our demon - stration includes enumerating new models using DQL by running the queries like the ones listed in Sec . 2 . On the other hand , with - out DQL , the default approach is illustrated for experienced users , such as editing the model , setting up running instances and parsing results , and how the DQL does related steps behind the scenes . Finally , we show how to publish and download models from M odel H ub . Modiﬁed face recognition models are published via dlv publish . Besides the face dataset , we also prepare well known models for other popular image datasets , such as MNIST , and host them beforehand in a M odel H ub instance . The attendees are al - lowed to discover and download models via dlv search and pull . As the take - away message , we hope the attendees can experi - ence the development of deep learning models for real world ap - plications , get an idea about the required skill sets in model de - velopment and the awkwardness of the current systems w . r . t . the lifecycle management , observe the subtle di ﬀ erences in the models created , and understand the importance of the problems we propose and the need for more principled data management approaches for addressing those . We also hope to derive additional features from the feedback of potential users among the attendees . 4 . REFERENCES [ 1 ] M . Abadi , A . Agarwal , P . Barham , E . Brevdo , Z . Chen , C . Citro , G . S . Corrado , A . Davis , J . Dean , M . Devin , et al . Tensorﬂow : Large - scale machine learning on heterogeneous systems , 2015 . Software available from tensorﬂow . org . [ 2 ] M . Akdere , U . Cetintemel , M . Riondato , E . Upfal , and S . B . Zdonik . The case for predictive database systems : Opportunities and challenges . In CIDR , 2011 . [ 3 ] A . Bhardwaj , S . Bhattacherjee , A . Chavan , A . Deshp , A . J . Elmore , S . Madden , and A . Parameswaran . Datahub : Collaborative data science & dataset version management at scale . In CIDR , 2015 . [ 4 ] T . Chilimbi , Y . Suzue , J . Apacible , and K . Kalyanaraman . Project adam : Building an e ﬃ cient and scalable deep learning training system . In OSDI , 2014 . [ 5 ] D . Crankshaw , X . Wang , J . E . Gonzalez , and M . J . Franklin . Scalable training and serving of personalized models . In LearningSys , 2015 . [ 6 ] J . Dean , G . Corrado , R . Monga , K . Chen , M . Devin , M . Mao , A . Senior , P . Tucker , K . Yang , Q . V . Le , et al . Large scale distributed deep networks . In NIPS , 2012 . [ 7 ] X . Feng , A . Kumar , B . Recht , and C . R´e . Towards a uniﬁed architecture for in - rdbms analytics . In SIGMOD , 2012 . [ 8 ] Y . Jia , E . Shelhamer , J . Donahue , S . Karayev , J . Long , R . Girshick , S . Guadarrama , and T . Darrell . Ca ﬀ e : Convolutional architecture for fast feature embedding . In ACM MM , 2014 . [ 9 ] A . Kumar , J . Naughton , and J . M . Patel . Learning generalized linear models over normalized data . In SIGMOD , 2015 . [ 10 ] Y . LeCun , Y . Bengio , and G . Hinton . Deep learning . Nature , 521 ( 7553 ) : 436 – 444 , 2015 . [ 11 ] Y . Low , D . Bickson , J . Gonzalez , C . Guestrin , A . Kyrola , and J . M . Hellerstein . Distributed graphlab : a framework for machine learning and data mining in the cloud . PVLDB , 5 ( 8 ) : 716 – 727 , 2012 . [ 12 ] E . R . Sparks , A . Talwalkar , V . Smith , J . Kottalam , X . Pan , J . Gonzalez , M . J . Franklin , M . I . Jordan , and T . Kraska . Mli : An api for distributed machine learning . In ICDM , 2013 . [ 13 ] M . Vartak , P . Ortiz , K . Siegel , H . Subramanyam , S . Madden , and M . Zaharia . Supporting fast iteration in model building . In LearningSys , 2015 . [ 14 ] C . Zhang , A . Kumar , and C . R´e . Materialization optimizations for feature selection workloads . In SIGMOD , 2014 .