High energy nuclear physics meets Machine Learning Wan - Bing He ID , 1 , 2 , ∗ Yu - Gang Ma ID , 1 , 2 , † Long - Gang Pang ID , 3 , ‡ Huichao Song ID , 4 , § and Kai Zhou ID 5 , ¶ 1 Key Laboratory of Nuclear Physics and Ion - beam Application ( MOE ) , Institute of Modern Physics , Fudan University , Shanghai 200433 , China 2 Shanghai Research Center for Theoretical Nuclear Physics , NSFC and Fudan University , Shanghai 200438 , China 3 Institute of Particle Physics and Key Laboratory of Quark and Lepton Physics ( MOE ) , Central China Normal University , Wuhan , 430079 , China 4 School of Physics and Center for High Energy Physics , Peking University , Beijing 100871 , China 5 Frankfurt Institute for Advanced Studies ( FIAS ) , D - 60438 Frankfurt am Main , Germany Though being seemingly disparate and with relatively new intersection , high energy nuclear physics and machine learning have already begun to merge and yield interesting results during the last few years . It’s worthy to raise the proﬁle of utilizing this novel mindset from machine learning in high energy nuclear physics , to help more interested readers see the breadth of activities around this intersection . The aim of this mini - review is to introduce to the community the current status and report an overview of applying machine learning for high energy nuclear physics , to present from different aspects and examples how scientiﬁc questions involved in high energy nuclear physics can be tackled using machine learning . Keywords : heavy ion collisions ; machine learning ; initial state ; bulk properties ; medium effects ; hard probes ; observables I . INTRODUCTION Machine learning has a long history of development and application , spanning several decades . In general , it is a rapidly growing ﬁeld of modern science that endows comput - ers with the ability to learn and make predictions from data without explicit programming . It falls under the umbrella of Artiﬁcial Intelligence ( AI ) and is closely related to statistical inference and pattern recognition . Recently , machine learn - ing technologies have experienced a revival and gained popu - larity , especially after AlphaGo from DeepMind defeated the human champion in the game of Go . This resurgence can be attributed to the advancement of algorithms , the increas - ing availability of powerful computational hardware such as GPUs , and the abundance of large - scale data . Nuclear physics seeks to understand the nature of nuclear matter , including its fundamental constituents and collective behavior under different conditions , as well as the fundamen - tal interactions that govern them . Traditional nuclear physics , especially for energies below about 1 GeV / nucleon , focuses on nuclear structures and reactions , where the degree of free - dom is the nucleon . In high - energy nuclear physics , how - ever , the degree of freedom is dominated by quarks and glu - ons . Theoretical calculations and experiments or observations with large scientiﬁc infrastructures play a leading role , but are now reaching unprecedented complexity and scale . In the context of high energy nuclear physics , researchers are al - ready at the forefront of big data analysis . The detectors used in high - energy nuclear collisions , such as RHIC or the LHC , can easily produce petabytes of raw data per year . A major ∗ Email : hewanbing @ fudan . edu . cn † Email : mayugang @ fudan . edu . cn ‡ Email : lgpang @ mail . ccnu . edu . cn § Email : huichaosong @ pku . edu . cn ¶ Email : zhou @ ﬁas . uni - frankfurt . de challenge is to make sense of the vast amounts of data gener - ated in experiments or simulated in theory . This data is often highly complex and difﬁcult to interpret . It’s a daunting task to analyze this sheer volume of data using traditional methods of physics research . Therefore , new efﬁcient computational methods are urgently needed to facilitate physics discovery in these computational and data - intensive research areas . One of the primary physical goals of high energy nuclear physics is to understand QCD matter under extreme condi - tions . It’s expected that at extremely high temperature and / or high density , the nuclear matter , which is governed by the QCD dictated strong interaction , will turn into a deconﬁned quark - gluon plasma ( QGP ) state , with elementary particles – quarks and gluons to be their basic degrees of freedom . The formation and properties of this new state of matter , as well as its transition to normal nuclear matter , are widely studied but still open questions in high - energy nuclear physics . This de - conﬁned QGP state is believed to exist in the early universe , roughly a few microseconds after the big bang . Another way to study such QGP is in the course of neutron stars ( or binary neutron star mergers ) , a compact astrophysical object whose interior serves as a cosmic laboratory for cold and dense QCD matter . Increasing astronomical observations , in particular from the progress of gravitational wave analysis , will pro - vide more and more constraints on the extreme properties of QCD matter in this cold and dense regime , for which novel techniques to deal with the associated inverse problem will be essential . Theoretically , ﬁrst - principle lattice QCD cal - culations at vanishing and small baryon chemical potentials predict a smooth crossover transition from a dilute hadronic resonance gas to the deconﬁned QGP state . However , in the high baryon density regime , direct lattice QCD simulations are currently hampered by the fermionic sign problem . On Earth , the only chance to study this new state of QGP mat - ter is through heavy - ion collision ( HIC ) programs , where two heavy nuclei are accelerated and smashed to deposit the colli - sion energy in the overlapping region to achieve the extreme a r X i v : 2303 . 06752v1 [ h e p - ph ] 12 M a r 2023 2 conditions , thus causing “heating / compression” on the nor - mal nuclear matter to be excited . The great challenge associated with heavy ion collisions is that the collision of heavy nuclei is a highly dynamic , com - plex and rapidly evolving process : although the deconﬁned QGP state may indeed be formed during the collision , it will undergo rapid expansion and cooling for its temperature , and at some point its degrees of freedom will be reconﬁned into colour - neutral hadrons , which will continue to interact and decay until the detector in the experiment receives its sig - nals . The whole collision process is too short and too small to be resolved . Experimentally , we also have no direct ac - cess to the early potentially formed QGP ﬁreball , but only indirect measurements of the ﬁnal emitted hadrons or their decay products . Furthermore , the theoretical description of the collision dynamics involves many uncertain physical fac - tors that are not yet fully clear from theory or experimental comprehension . These uncertainties can interfere with dif - ferent ﬁnal physical observables in the experiment . Thus , from the limited and contaminated ( i . e . , heavily inﬂuenced by many uncertain factors ) measurements , a reliable pinning down for the physics of the produced extreme QCD matter is non - trivial and challenging . This seriously hampers the ex - traction of physical knowledge from the major efforts in the heavy ion collision programs . Novel , efﬁcient computational methods are urgently needed to address this challenge for fur - ther physics exploration . As a modern computational paradigm , machine learn - ing within AI has become increasingly promising in recent years for applications at the forefront of high - energy nuclear physics research . In general , machine learning algorithms can be used to automatically identify patterns and correla - tions in data , allowing knowledge to be extracted from data computationally and automatically . It can thus help to ex - tract meaningful information about the underlying physics or fundamental driving laws from the available data . In contrast to the traditional focus of machine learning , which is usually predictions based on pattern recognition from the collected data , the intersection of high energy nuclear physics and ma - chine learning is also concerned with the underlying patterns and causality for the purpose of uncertainty assessment and also physical interpretation , and thus new knowledge discov - ery . A recent collection of datasets from different areas of fundamental physics , including in particular high energy par - ticle physics and nuclear physics , with supervised machine learning studies is presented in Ref . [ 1 ] . For the purpose of physics identiﬁcation , the intersection of high energy nuclear physics and machine learning also goes beyond the mere application of existing learning algorithms to the data set accessible in the physics problem . Paying spe - cial attention to the physical constraints or required funda - mental laws or symmetries of the systems would help the ef - ﬁciency of machine learning in solving the speciﬁc physics problem . For example , when using regressive or genera - tive models to study quantum many - body systems or gen - eral quantum ﬁeld theory , implementing the symmetries of the system can greatly reduce the need for training data and improve the performance for recognition [ 2 ] . It is also worth mentioning that machine learning has been applied to many topics at low and intermediate energy HICs , e . g . , [ 3 – 9 ] where a recent mini - review can be found in Ref . [ 10 ] , as well as hadron physics , e . g . , [ 11 – 13 ] . In addition , machine learning can also be applied in the context of simulations , which play a key role in fundamental physics research as well as in a wide range of other scientiﬁc ﬁelds such as biology , chemistry , robotics , climate modelling , etc . In high - energy nuclear physics , for both experimental and theoretical studies , simulation is an important tool , start - ing from the understanding of the fundamental interactions involved , e . g . , in heavy ion collision dynamics and detector simulation , as well as in lattice quantum ﬁeld theory simu - lation . Simulations are used to model the behavior of nu - clear matter and its constituents , and the interactions that take place between them , which are usually highly complex with detailed use of many involved physical laws and equations or empirical phenomenological models . It has long been a major consumer of computing resources for high - statistics or high - resolution simulations of heavy ion collisions and the associ - ated detectors in high - energy nuclear physics . The collision dynamics simulation with extensive synthetic data is required to accurately interpret the experimental measurements , which is enormously computationally and memory intensive . Ma - chine learning can be used to improve the efﬁciency and de - scriptive power of these simulations to facilitate the physics discovery process . For example , it has been proposed to use machine learning to speed up the simulation of hydrodynam - ics , to optimize the parameters that go into the model sim - ulation , to make it more robust to uncertainties , or to solve the many - body problems directly by augmenting the conven - tional Monte Carlo simulation . In brief , machine learning is an effective tool that can be employed to tackle many challenges in high - energy nuclear physics . It can assist in analyzing large amounts of data from high energy nuclear physics , linking nuclear experiments to physics theory exploration effectively , optimizing simulations and calibrating models more efﬁciently , as well as developing new empirical and theoretical models . It is undeniable that machine learning technologies have the potential to make a signiﬁcant impact , even transforming the ﬁeld of high - energy nuclear physics . Therefore , it is essential to acknowledge and recognize the importance of this new paradigm in advancing the ﬁeld further . II . METHODOLOGY Machine learning ( ML ) can be classiﬁed in several ways . One way is to classify ML by its functionality , into clas - siﬁcation , regression , generation and dimensionality reduc - tion . The other way is to classify ML by the type of training data , into supervised learning , unsupervised learning , semi - supervised learning , self - supervised learning , active learning , and reinforcement learning . For example , supervised learn - ing requires data to be labelled in such a way that the ma - chine can be trained to build a mapping between the input and the labels . Unsupervised learning does not need labelled 3 data , it can learn patterns from data , assuming that the ma - chine will make self - consistent predictions on data that is perturbed or slightly augmented . Semi - supervised learning requires a small amount of labelled data along with a large amount of unlabelled data . Self - supervised learning works with speciﬁc data such as natural language or images that are sequential . It allows the machine to predict one part of the sequence from the other part . Active learning is a type of semi - supervised learning that has two pools of data , a small pool of labelled data and a large pool of unlabelled data . The machine is trained on the labelled data and validated on the unlabelled data . The performance of the simply trained ma - chine will be different for different samples from the unla - belled data pool . For example , the machine might be quite uncertain on one sample , predicting that the label of this sam - ple is A with 51 % probability and B with 49 % probability . This sample is assumed to be more difﬁcult and more impor - tant for the trained machine than simple samples where the machine’s predictions are quite certain . To be more data efﬁ - cient , this sample is labelled and moved from the unlabelled pool to the labelled pool for further training . Reinforcement learning uses data generated by interactions with the environ - ment . Based on the previous description , the loss function for su - pervised learning in the regression task can be written as l = | | y pred − y true | | ( 1 ) where y pred = f ( x , θ ) is the function represented by ma - chine learning models such as decision trees or deep neural networks , x is the input data and θ represents all trainable model parameters , y true is the label of the input data x . The | | · | | usually represents the l 1 norm , which gives the mean absolute error MAE , or the l 2 norm , which gives the root - mean - square error RMSE . For classiﬁcation , the cross - entropy loss is widely used . It is deﬁned as l = − K (cid:88) k = 1 p k log q k ( 2 ) where K is the number of possible categories of the input data x , p k = y true is the true label ( probability ) , q k = f ( x , θ ) is the network prediction . This loss is inspired by the KL divergence , which quantiﬁes the difference between two dis - tributions p and q , KL ( p | | q ) = K (cid:88) k = 1 p k log p k q k ( 3 ) = (cid:88) k p k log p k − p k log q k ( 4 ) = − H ( p ) + H ( p , q ) ( 5 ) where H ( p ) is the entropy of the distribution p and the cross entropy H ( p , q ) = − (cid:80) k p k log q k quantiﬁes the average number of bits required to encode the distribution p using the model q . In binary classiﬁcation , the cross entropy is reduced to l = − 1 m m (cid:88) i = 1 [ p i log q i + ( 1 − p i ) log ( 1 − q i ) ] . ( 6 ) where p i is the true label of the i ’th sample whose value is 0 or 1 . q i is the network prediction using the sigmoid activation function in the last layer to ensure 0 < q i < 1 . m is the number of samples in each minibatch . If the true label is p i = 0 , only the 2nd part contributes to the loss function . For multi - categorical classiﬁcation , the loss function is also the cross - entropy loss , with the activation function in the last layer replaced by the softmax activation , Softmax ( z i ) = e z i (cid:80) Kk = 1 e z k ( 7 ) For unsupervised learning , the loss function can generally be written as l = | | manipulate 1 ( x ) − manipulate 2 ( x ) | | ( 8 ) where manipulate 1 , 2 represents 2 manipulations on the same data . For example , in clustering tasks , the manipulations on x are to compute the total distance of samples to multiple cen - tres . In image classiﬁcation tasks , the manipulations are to compute the network prediction over two different augmenta - tions of the same image , e . g . cropping or rotation . This loss is also called self - consistent loss . For semi - supervised learning , the loss function is the com - bination of the supervised loss and the unsupervised loss , l = l supervised + l unsupervised ( 9 ) For self - supervised learning , a widely used loss function is the reconstruction loss . For example , in computer vision , the reconstruction loss is deﬁned as the difference between the original image and the image reconstructed by a neural network from a masked image . l = | | x − f ( ( 1 − M ) · x ) | | ( 10 ) where x is the original image , M is the binary mask used to remove M = 0 pixels from an image , f is a neural network used to reconstruct the image . The same method can be used to reconstruct natural language , by predicting the next sen - tence or missing words in a sentence . The pre - trained network can be used in many downstream tasks such as classiﬁcation , regression or generation . In active learning , the loss function is basically the same as in supervised learning . The difference is that the trained network ranks samples from the unsupervised pool for anno - tation . So the key is to rank the samples . There are two main ways of doing this . One way is to rank according to the en - tropy of the predictions made by the pre - trained network , s = − (cid:88) i p i log p i ( 11 ) where p i is the predicted probability that the sample is in class i . The other way is to rank according to the diversity of the 4 training data set , by giving the highest rank to the sample that has the greatest distance from the training data . For reinforcement learning , the data is generated by sub - sequent interactions between the network policy and the en - vironment . The network receives an observation o t from the environment at time t , makes a decision and takes an action a t on the environment , the environment returns a new ob - servation o t + 1 , an immediate reward r t + 1 and a done signal . The data are thus { o t , a t , o t + 1 , r t + 1 , done } trajectories . The loss of reinforcement learning is similar to supervised learn - ing with data o t and true labels a t , r t + 1 . Optimisation The goal of machine learning is to minimize the loss of prediction on new data not used for training . In gradient - based models , this is achieved simply by stochastic gradient descent ( SGD ) and its variants , θ = θ − (cid:15) 1 m m (cid:88) i = 1 ∂l i ∂θ ( 12 ) where θ represents all the trainable parameters of the machine learning model , (cid:15) is a small positive number called the learn - ing rate , m is the size of the mini - batch . Updating θ with the negative gradient − (cid:15) 1 m (cid:80) mi = 1 ∂l i ∂θ helps to gradually re - duce the loss . This can be easily veriﬁed if there is only one trainable parameter θ and the loss is l = θ 2 , whose negative gradient is − 2 θ . All possible values of θ form a space called the parameter space . The initial value of θ is usually a bunch of random numbers . Updating θ using SGD is analogous to walking around the parameter space looking for the minimum value of the loss function . The loss function can be thought of as the potential surface whose negative gradients give the direction of acceleration (cid:126)a . In this way , simple SGD means that the po - sition θ in parameter space is updated using the acceleration . As a result , there are two major drawbacks to using native SGD . First , if the gradient is 0 , the optimisation stops imme - diately . Second , the network update is much faster along the direction where the gradient is large . These two drawbacks are partly solved using the momentum mechanism [ 14 ] and the adaptive learning rate [ 15 ] . In reinforcement learning , the goal is to maximize the accu - mulated rewards in the future . The optimization is a stochas - tic gradient ascent . In the popular policy gradient method , the parameters of the policy network are updated as follows , θ = θ + (cid:15)G t ∇ ln π ( a t | o t , θ ) ( 13 ) where G t = (cid:80) Tk = t + 1 γ k − t − 1 r k is the return representing the accumulated rewards in the future with a discounting factor γ < 1 . Auto differentiation The number of trainable parameters in a deep neural network is huge . In order to learn from the data , one has to compute the negative gradients of loss with respect to each of the millions or trillions of model parameters − ∂l ∂θ . This is intractable using ﬁnite difference or analytic differen - tiation . Finite difference has both truncation and round - off er - rors that cannot be controlled . Analytical differentiation will have exploding expressions for deep neural networks that are too complex to compute efﬁciently . In deep learning , the neg - ative gradient is mainly computed using auto differentiation ( auto - diff ) , which is computationally efﬁcient but has analyt - ical precision . Auto - diff has a forward mode and a backward mode . If the deep neural network is a R 1 → R n mapping , a forward pass gives derivatives of all output variables y i with respect to the input variable x . On the other hand , if the network is a R n → R 1 mapping , each forward pass returns only the derivative of the output variable y on one of the input vari - ables x i . In the SGD algorithm , the backward mode is much more efﬁcient because the mapping from θ to the loss is a R n → R 1 mapping . In the following , the forward autodiff is brieﬂy explained . In forward mode , auto - diff is implemented by introducing a dual number for each variable , x → x + ˙ x d ( 14 ) y → y + ˙ y d ( 15 ) where x and y are two variables that require gradients , ˙ x and ˙ y are the derivatives of x and y with respect to some variable , as mentioned above , with ˙ x = 1 , ˙ y = 0 will give ∂l / ∂x in one pass of the forward mode , setting ˙ x = 0 , ˙ y = 1 will give ∂l / ∂y in another pass of the forward model . d is an inﬁnites - imal symbol satisfying d 2 = 0 , analogous to the imaginary symbol I 2 = − 1 . With this deﬁnition , the traditional out - put z of each operator will be a dual number z + ˙ z d whose coefﬁcient ˙ z is the derivative of z , as shown below , x + ˙ x d + y + ˙ y d = ( x + y ) + ( ˙ x + ˙ y ) d ( 16 ) x + ˙ x d − ( y + ˙ y d ) = ( x − y ) + ( ˙ x − ˙ y ) d ( 17 ) ( x + ˙ x d ) ∗ ( y + ˙ y d ) = x ∗ y + ( x ˙ y + y ˙ x ) d ( 18 ) ( x + ˙ x d ) / ( y + ˙ y d ) = ( x + ˙ x d ) ( y − ˙ y d ) y 2 − ˙ y 2 d 2 ( 19 ) = x y + y ˙ x − x ˙ y y 2 d ( 20 ) ( 21 ) The calculations of dual numbers can easily be extended to polynomial functions , P ( x + ˙ x d ) = P ( x ) + P (cid:48) ( x ) d ( 22 ) On the computer , more complex functions such as sin ( x ) , log x and e x can be approximated by polynomial functions . In principle , auto - diff works for these functions as well . In practice , these functions can be overloaded to produce output in the form of dual numbers , e . g . , sin ( x + ˙ x d ) → sin x + cos x ˙ x d . Because of the universal approximation capability of DNNs and the efﬁcient and accurate auto - diff , DNNs are widely used to represent solutions of ordinary differential equations ( ODE ) and partial differential equations ( PDE ) that require gradients . In this way , many physical problems are translated into optimisation problems . This method is com - monly referred to as Physically - Informed Neural Network ( PINN ) . Compared to traditional numerical solutions , PINN 5 InputLayer Convolu / onalLayer PoolingLayer FullyConnectedLayer OutputLayer Fig . 1 . ( Color online ) Convolutional neural networks . is mesh - free , works for very high dimensions , is easy to im - plement , especially for multi - scale and multi - physics prob - lems . convolutional neural networks Convolutional Neural Net - works ( CNN ) are distinguished from other neural networks by their superior performance on image , speech , or audio signal inputs . A naive CNN consists of three main types of layers , i . e . , convolutional layer , pooling layer , and fully - connected layer , as shown in Fig . 1 . The convolutional layer is the core building block of a CNN . The term convolution refers to the convolutional operation between the input fea - tures and the ﬁlters ( or kernels ) . In the mathematical view , a convolution operation is a special kind of linear operation where two functions are multiplied to produce a third function that expresses how the shape of one function is modiﬁed by the other . In the ML view , the convolution layer uses the ﬁl - ters to extract the features from the input data , and combines the extracted features as the output . In a well - trained con - volutional layer , a ﬁlter is only sensitive to one speciﬁc type of feature . Usually , there are many ﬁlters in a convolutional layer , to satisfy the complex input features . After the convo - lutional operation , a Rectiﬁed Linear Unit ( ReLU ) is usually chosen as the active function , which introduces nonlinearity into the neural network . After the convolutional layer , a pooling layer is applied to reduce the number of parameters , also known as downsam - pling . There are two main types of pooling , that are max pooling and average pooling . Max pooling selects the maxi - mum value to be the output , and the average pooling uses the average of the pixels covered by the pooling kernel . The fully connected layer is used to map the features extracted by the previous layers to the ﬁnal output . The convolutional layers can be stacked to make the neu - ral network go deeper . Earlier layers will break down the complex features from the input data to be individual simple features . As the progresses go through the next convolutional layers , the ﬁlters begin to capture larger elements or shapes of the features . With the ability to extract complex features , the CNN architecture became a foundation of modern computer vision . However , when the neural networks go deep , the vanish - ing gradient problem becomes very serious . To overcome this problem in CNN architectures , many complex neural net - works have been developed , such as AlexNet , VGGNet , In - ceptionNet , GoogLeNet , ResNet . Recurrent neural networks Recurrent Neural Networks ( RNN ) are distinguished from other neural networks by their h V W U Y X Unfold h t - 1 W U V h t W U V h t + 1 W U V V … … X t - 1 Y t - 1 Y t Y t + 1 X t X t + 1 Fig . 2 . ( Color online ) Recurrent neural networks . superior performance on sequence or time - series data . Fig . 2 shows the structure of a basic RNN , where U denotes the weights for the connection of the input layer to the hidden layer , V denotes the weights for the connection of the hidden layer to the hidden layer , W denotes the weights for the con - nection of the hidden layer to the output layer . Using self con - nection by weights V , RNN take information from previous inputs to inﬂuence the current input and output . This feature is often referred to as ’memory’ , which makes the RNN good at processing sequential data . The loss function L of all time steps is deﬁned based on the loss at each time step as follows : L ( ˆ Y , Y ) = T (cid:88) t = 1 L ( ˆ Y t , Y t ) . ( 23 ) The RNN uses the Backpropagation Through Time ( BPTT ) algorithm to determine the gradients . The error is backprop - agated from the last time step to the ﬁrst time step . At time step T , the derivative of the loss L with respect to the weight matrix W is expressed as follows : ∂ L ( T ) ∂W = T (cid:88) t = 1 ∂ L ( t ) ∂W ( 24 ) RNNs also suffer from the problem of gradients vanishing and exploding . To deal with the gradient problems , some variant networks have been developed , such as Long Short - Term Memory Networks ( LSTM ) and Gated Recurrent Units ( GRU ) . Point Cloud Network The ﬁnal state particles from heavy ion collisions form a point cloud in momentum space . This data must be manipulated to use CNN and RNN , as these net - works were originally designed for images and natural lan - guage . For example , to use CNN , density estimation ( his - togram ) is usually used to convert the particle cloud into im - ages . However , it does not work well for a few particles in 3 - dimensional space because the particles are dilute and the resolution is poor . To use RNN , the particle cloud must be sorted to 1 dimension , which can only keep the local informa - tion in 1d . The point cloud network is designed to preserve the permutation symmetry of a set of particles . Shown in ﬁgure . 3 is a simple demonstration of a point cloud network . The input to the network is a set of parti - cles in momentum space with their 4 - momentum , mass and other quantum numbers . A fully connected neural network or Multi - Layer Perceptron ( MLP ) is applied to a particle 6 Fig . 3 . ( Colour online ) A simple demonstration of the point cloud network . to transform its m input features into 128 features in high - dimensional latent space . This MLP is shared by all parti - cles in the cloud , which is also called a 1DCNN . This step preserves the permutation symmetry between all particles . Then , global max pooling ( GMP ) or global average pooling ( GAP ) is applied to these latent features of all particles , to ex - tract the global information of this particle cloud . The GMP and GAP extract the boundaries of the input particle cloud in high - dimensional latent space , which learn the multi - particle correlation for the ﬁnal decision . This extracted global infor - mation ( 128 features ) is further fed to another MLP for the ﬁnal decision . The output neuron has a value in the range ( 0 , 1 ) and uses 0 . 5 as the decision boundary . The network shown in Fig . 3 is used to classify nuclear phase transitions [ 16 ] . Some point cloud network applies a Euclidean rotation to the point cloud to preserve rotational symmetry , i . e . , the network should make self - consistent pre - diction if the point cloud is rotated globally [ 17 ] . Other vari - ants use k nearest neighbors in spatial or momentum space to extract the high dimensional latent features of each parti - cle , to keep more local correlation . The k nearest neighbors of each particle can be calculated in feature space to capture long range multiple particle correlation , because particle that are close in feature space might be far apart in spatial or mo - mentum space . This technique is called dynamical edge con - volution and was used to look for self similarity between par - ticles in momentum space , which is associated to critical phe - nomenon that may happen in heavy ion collisions [ 18 ] . The dynamical edge convolution is one kind of message passing neural network that is also called graph neural network . Generative Modelling In unsupervised learning , genera - tive modelling is a class of techniques when it’s related to probability distribution learning . In the sense of tasks , gener - ally machine learning can be categorized into discriminative modelling and generative modelling . With probabilistic per - spectives , discriminative modelling such as pattern recogni - tion aim at learning a conditional probability , p ( y | x ) , which can be used to predict for a given input object ( x ) its associ - ated properties or class identities ( y ) , while the goal of gen - erative modelling is to capture the joint distribution , p ( x , y ) , from which one can generate new data points following the same statistics as the training set . In machine learning com - munity the generative modelling have shown great successes in numerous applications including image synthesis , inpaint - ing , super - resolution , text - to - image translation , speech gen - eration , chat robotics . Actually , quite a lot of the genera - tive models were developed with profound inﬂuence from and into physics . In science there are also lots of direct applica - tions e . g . , computational ﬂuid simulation , drug molecule de - sign , anomaly detection , many - body physics study and also lattice ﬁeld conﬁguration generation for QCD , etc . The central purpose of generative modelling is to get the ability to sample data ( ˜ x ) from the same distribution of the training set p d ( x ) . Most of the generative modes construct parametric ( explicit or implicit ) models p θ ( x ) to approach the desired data distribution . From information theory , the Kullback - Leibler ( KL ) divergence ( in Eq . ( 3 ) ) provides an ob - jective for this task , which measures the dissimilarity between the model and data distributions . Per Jensen inequality , the KL divergence is non - negative , and will be zero only when the two distributions match exactly . The minimization of the KL divergence under given observational data for the system with collected training set , D = { x } , is equivalent to the min - imization of the negative log - likelihood ( NLL ) , L = 1 | D | (cid:88) x ∈D log p θ ( x ) , ( 25 ) 7 thus the maximum likelihood estimation ( MLE ) . In the following , we will review shortly several repre - sentative and popular deep generative models , including the variational autoencoder ( VAE ) , generative adversarial net - works ( GAN ) , autoregressive modelling and normalizing ﬂows ( NF ) . Variational autoencoder , VAE [ 19 ] , introduces a latent vari - able z to facilitate the generation process , thus constructs a trainable conditional probability p θ ( x | z ) ( called the decoder or generator , usually modelled by neural network ) . The la - tent variable is per generation convenience assumed to fol - low an easy - to - be - sampled prior distribution , p ( z ) such as the multivariate Gaussian distribution . The introduction of la - tent variable however makes the data generation distribution ( thus the likelihood ) intractable since the needed marginal - ization , p θ ( x ) = (cid:82) p θ ( x | z ) p ( z ) dz . There out , the posterior distribution for the latent variable is intractable as well since p ( z | x ) = p θ ( x | z ) p ( z ) / p θ ( x ) . VAE employs a variational inference approach to approximately perform the maximum likelihood ( MLE ) on the training data . Speciﬁcally , an en - coder model q φ ( z | x ) ( also modelled by a neural network ) is introduced to approach the real posterior p ( z | x ) , and the KL divergence D KL ( q φ ( z | x ) | | p ( z | x ) naturally provide the training objective which further derived as a variational lower bound ( also known as evidence lower bound , ELBO , as the cornerstone for VAE ) to the likelihood : L = E q φ ( z | x ) [ log p θ ( x | z ) + log p ( z ) − log q φ ( z | x ) ] ( 26 ) ≤ log p θ ( x ) , ( 27 ) As another latent variable generative model , the genera - tive adversarial network , GAN [ 20 ] , is developed to train the generator through adversarial strategy . Intuitively , the GAN framework constructs two non - liner differentiable functions ( represented both by adaptive neural network per dimension - ality requesting ) : one called generator G ( z ) mapping latent variable z to the target data manifold ˜ x = G ( z ) , which gives an implicit synthesized data distribution p G ( x ) when the la - tent variable is supposed to follow a prior latent distribution p ( z ) , e . g . , multivariate uniform or Gaussian , and the goal would be training the generator to push p G ( x ) approaching the target distribution p true ( x ) ; the other is called discrimina - tor D ( x ) which maps the data manifold to one single scalar representing the fake - vs . - true distinguish result of the dis - criminator on the input data . For vanilla GAN it’s designed as a binary classiﬁer for the discriminator , that for real data it’s trained to output D ( x ) = 1 while for generated one it’s trained to output D ( ˜ x ) = 0 . The generator and discriminator will be trained alternatively to improve their abilities in com - peting against each other , this can be achieved by mimicking a two - players min - max game , thus train the discriminator to better distinguishing the real data from the generated ones , meanwhile train the generator to cheat the discriminator for recognizing them as “real” ones . It’s proved mathematically that the adversarial training of GAN is equivalent to minimizing the JS divergence , JS ( p real | | p G ) = 1 2 ( KL ( p real | | p mix ) + KL ( p G | | p mix ) ) , ( 28 ) with p mix = ( p real + p G ) / 2 . Thus , the GAN belongs to implicit MLE based generative model . The optimally trained GAN is derived to converge approaching the Nash equilib - rium state , where the generator excels in synthesizing sam - ples that the discriminator can not differentiate anymore from the real data , so the generator induced distribution indeed captured the real data distribution per training . This tech - nique has been utilized in various scientiﬁc contexts , like in condensed matter physics [ 21 , 22 ] , particle physics [ 23 , 24 ] , cosmology [ 25 , 26 ] , also in QFT study with lattice simula - tion [ 27 , 28 ] . There are also explicit MLE based generative models , which are with close relation to statistical physics . Among them , the simplest one is autoregressive model [ 29 ] . Basi - cally , it invokes the probability chain rule to decompose the full probability into products of a series of conditionals , p θ ( x ) = N (cid:89) i p θ ( x i | x 1 , x 2 , . . . , x i − 1 ) , ( 29 ) which is used as the generative model distribution to approach the desired data distribution . Speciﬁcally , one can use neural networks to parameterize each of the conditional components involved in the above . Then these neural networks as a whole actually can be viewed as one single general neural network ( could take fully connected or CNN or RNN architecture ) with masked weight parameter matrix ( e . g . , triangular with matrix for simple fully connected case ) as a result of respect - ing the autoregressive properties speciﬁed by the Eq . 29 . Be - ing termed as PixelCNN [ 30 ] or PexelRNN [ 31 ] respectively , the employment of convolutional layer or recurrent layer for treating structured systems in autoregressive modelling can further respect the spatial or temporal translational invariance of the system . It also achieved state - of - the - art performances in speech synthesis with such autoregressive networks termed as WaveNet [ 32 ] . With the above autoregressive represen - tation as parametric generative model , the MLE can be ex - plicitly performed to optimize the p θ ( x ) for approaching the targeted data distribution p real ( x ) , which as derived is mini - mizing the forward KL divergence KL ( p real | | p θ ) . This idea is also applied in many - body physics for statistical mechanics solving and general continuous system study [ 33 ] . With combination of both latent variable model and ex - plicit maximum likelihood estimation , the normalizing ﬂow ( NF ) [ 34 – 36 ] is developed . Basically , NF introduces bijec - tive afﬁne transformations to map a simple latent space vari - able z to the complex data manifold sample x = g ( z ) . Note that the bijectivity requires the transformation to be with the same dimensionality in input and output . This renders the us - age of change of variable theorem to estimate the likelihood explicitly , p θ ( x ) = p ( z ) | det ( ∂z ∂x ) | , ( 30 ) with the determinant of Jacobian for the ( inverse ) transfor - mation needed . Then , after the MLE training , the parameter - ized transformation just serves as a generator for new sample generation x = g ( z ) . To simplify the evaluation of needed 8 Jacobian determinant in Eq . 30 , special network structure are adopted , e . g . , those holding triangular Jacobian matrix as used in Real NVP . Such ﬂow - based generative models have been implemented in lattice QFT studies [ 37 – 39 ] and shown very promising indicator for further QCD study in the past few years . Recently , such ﬂow based model was also gen - eralized into Fourier frequency space and use in generating Feynman paths for quantum physics [ 40 ] . Principal Component Analysis In machine learning , Prin - cipal Component Analysis ( PCA ) is a statistical technique that transforms a set of correlated variables into independent variables through orthogonal transformations . The principal components , associated with the obtained main eigenvectors ( or non - negligible singular values ) reveal the most represen - tative conﬁgurations of the data . As one of the unsupervised learning techniques , PCA implements the Singular Value De - composition ( SVD ) on a real matrix [ 41 ] : M = XΣZ = VZ ( 31 ) where M is a matrix of a size of N × m , X and Z are two orthogonal matrices of the size of N × N and m × m and Σ is a diagonal matrix with the singular values arranged in descending order . Then , the i th row of the matrix M ( i ) can be expressed as : M ( i ) = m (cid:88) j = 1 x ( i ) j σ j z j = m (cid:88) j = 1 ˜ v ( i ) j z j ≈ k (cid:88) j = 1 ˜ v ( i ) j z j ( i ) = 1 , . . . , N ( 32 ) where ˜ v ( i ) j is the corresponding coefﬁcient of z j for the i th row . In the last step , there is a cut on the indices , k since PCA focuses only on the most important components . Due to its strong power in data mining , PCA has been widely used in various research areas of physics . For recent progress in Heavy Ion Collisions , please see Sec . VII in this review . III . INITIAL CONDITION In the traditional view , nuclear structure manifests its sig - niﬁcance only at low energy since the high energy nucleus - nucleus collisions are a violent process in which the whole nucleus will be disassembling . However , many recent progress has demonstrated that the initial nuclear structure information is very important for understanding the ﬁnal ob - servables in high energy heavy - ion collisions . One of the ex - amples is collective ﬂows , e . g . , elliptic ﬂow and triangular ﬂow , in which the initial participant shape and nucleon den - sity distribution as well as their initial state ﬂuctuations are relevant . In particular , collision geometry , neutron skin , de - formation , and α - clustering structure are very important to inﬂuence the ﬁnal observable . A mini - review can be found in a chapter in the handbook of nuclear physics by Ma and Zhang [ 42 ] . Machine learning provides a powerful tool for discriminating such initial structure information . In this sec - tion , we will discuss such applications . Impact parameter estimation : The impact parameter b de - scribes the distance between the centers of the two colliding nuclei in the classical view , which is a crucial quantity de - termining the initial geometry of a collision . In experiments , the impact parameter is not directly measurable and usually estimated from the multiplicity of ﬁnal - state particles in track detectors or the energy deposited in calorimeters . Machine learning approaches are proposed to determine the impact pa - rameters from the ﬁnal - state particles , and show better perfor - mance than conventional methods . Ref . [ 43 ] proposes to use ANN and CNN to reconstruct the impact parameters from the energy spectra of ﬁnal - state charged hadrons of heavy ion col - lisions at √ s NN = 7 . 7 GeV to 200 GeV , which are simulated with the AMPT model . Both the ANN and CNN can recon - struct the impact parameters with a mean absolute error of about 0 . 4 fm . When the input feature is from a larger pseu - dorapidity window , the CNN shows a higher prediction accu - racy than the ANN . Ref . [ 44 ] reports the performance of CNN and LightGBM in reconstructing the impact parameter from the heavy ion collisions at √ s NN = 0 . 2 GeV to 1 GeV , which are simulated with the UrQMD model . The input features are constructed from the proton spectra in transverse momentum and rapidity . The average difference between the true impact parameter and the estimated one can be less than 0 . 1 fm . The LightGBM shows a better performance than the CNN . A model - independent Bayesian inference method to re - construct the impact parameter distributions is proposed in Ref . [ 45 ] . The impact parameter distributions are inferred from model - independent data . This method is based on Bayes’ theorem , P ( b | X ) = P ( b ) P ( X | b ) / P ( X ) , ( 33 ) P ( X ) is the probability of the observable that can be mea - sured in the experiment . P ( X | b ) is the probability density distribution of X for a given impact parameter b . Fluctuation is taken into account by assuming P ( X | b ) to be a Gaussian or gamma distribution , which can be determined by ﬁtting the data with the formula P ( X ) = (cid:82) P ( X | b ) P ( b ) db . P ( X ) can be a multidimensional form . In the Ref . [ 45 ] two observ - ables are used , X = { M , p tott } , where M is the multiplicity of the charged particles and p tott is the total transverse mo - mentum of the light particles . End - to - end centrality estimation for CBM : The Com - pressed Baryonic Matter ( CBM ) detector is currently under construction for FAIR at GSI , which will study the properties of strongly compressed nuclear matter via heavy ion colli - sions with beam energies ranging from 2 to 10 AGeV . A characteristic of the CBM experiment is its very high event rate and trigger rate , which will produce a huge amount of raw data per second in real - time and pose a challenge for on - line event characterization and storage . To address the on - line event characterization , it’s essential to be able to work on the direct output of the detector , which has an inherent point cloud structure – a collection of points as an unordered list with particles or tracks’ attributes recorded . One important property of the point cloud is that they as a whole should be invariant under permutation . PointNet structure [ 46 ] is spe - cially developed to respect this order invariance . Accordingly , 9 0 20 40 60 80 Centrality ( % ) − 0 . 6 − 0 . 4 − 0 . 2 0 . 0 0 . 2 0 . 4 M e a n e rr o r ( f m ) Polyfit M - hits S - hits MS - tracks HT - combi Fig . 4 . ( Color online ) Taken from Ref . [ 47 ] . Mean error in pre - dictions as a function of centrality . Dataset Test2 is used in which peripheral events are more likely to occur . The track multiplicity is used for the centrality binning . The points at 90 % centrality are re - sults from events with no tracks reconstructed . Therefore , the Polyﬁt and MS - Tracks model do not have a data point at 90 % centrality . to heavy ion collisions , the PointNet - based models serve as a natural way to perform real - time physics analysis on the de - tector output directly . Refs . [ 17 , 47 ] proposed to use PointNet based models to construct event - by - event impact parameter determination for CBM experiment using direct output from the detector , thus serving as an end - to - end centrality estimator . The supervised learning strategy is taken for this regression task , where the training data is prepared from UrQMD followed by CBM - Root detector simulation to obtain the detector output which are hits or tracks of the particles . A PointNet based model is constructed and trained to capture the inverse mapping be - tween the detector output and the impact parameter informa - tion . It’s shown that the PointNet - based model can give ac - curate event - by - event impact parameter determination using hits of charged particles in different detector planes and / or the tracks reconstructed from these hits , showing superior perfor - mance than a baseline model using charged track multiplicity as input inside a polynomial ﬁt . In terms of both the precision and accuracy sector , the PointNet - based models outperform the baseline . While the baseline model gives similar resolu - tion ( relative precision ) as to the PointNet - based model in the semi - central collision region , it gives a much more ﬂuctuating and poorer accuracy for impact parameters ranging from 3 to 16 fm , indicated by the mean of the prediction error for im - pact parameter . This trend also becomes more evident when it goes to realistic event distribution ( i . e . , ∼ bdb ) , as shown by Fig . 4 for the mean prediction error . Considering the nat - ural parallelizibility and high speed , PointNet based model paves the way for real - time end - to - end event characterization for heavy ion collisions studies . Nuclear deformation estimation : The momentum distribu - tion of ﬁnal state hadrons is sensitive to nuclear shape de - formation . For example , due to the different collision geom - etry , the elliptic ﬂow as a function of charge multiplicity is ⊗ ⊙ Pb + Pb ⊗ ⊙ ⊗⊙ ( a ) body - body aligned ( b ) body - body crossed ( c ) tip - tip ( d ) tip - body U + U ( e ) Impact parameter determined Fig . 5 . ( Color online ) Collision geometry for Pb + Pb and U + U colli - sions . quite different for Pb + Pb and U + U collisions . As shown in Fig . 5 , the 208 Pb is a double magic nucleus with an almost perfectly spherical shape , whose collision patterns depend only on the impact parameter b . While the shape of 238 U is close to a watermelon , whose collision patterns are much more complex than Pb + Pb collisions . For example , the U + U collisions have body - body aligned , body - body crossed , tip - tip and tip - body collisions . Different collision patterns corre - spond to different charge multiplicity and elliptic ﬂow . Both the fully overlapped body - body aligned and the central tip - tip collisions correspond to most central collisions with high charge multiplicity , but their elliptic ﬂows are quite different . This kind of difference will lead to a much larger variance in the elliptic ﬂow for most central U + U collisions , compared to high - multiplicity Pb + Pb collisions . In principle , the complex collision patterns will lead to many differences in the ellip - tic ﬂow versus charged multiplicity diagram . Deep learning is expected to identify these differences and make a predic - tion of the nuclear shape deformation parameters using these patterns . It is demonstrated that using nuclei with different deforma - tion parameters β 2 and β 4 , high - energy heavy ion collisions can be simulated using the Trento Monte Carlo model to ob - tain the event - by - event total initial entropy ( which is propor - tional to the ﬁnal charged multiplicity ) and the corresponding geometric eccentricity ( which is approximately proportional to the elliptic ﬂow ) . A deep residual neural network is trained to predict β 2 and β 4 using the 2D images of total entropy versus eccentricity [ 48 ] . It shows that the network is success - ful in predicting the absolute values of β 2 and β 4 , but fails to predict their signs using the information provided . Using the Class Activation Map ( CAM ) method to map the last con - volutional layer onto the input image , the authors found two regions in the image that are important for decision - making . One is the most central collision region , which is most sensi - tive to the variance of the elliptical ﬂow . Recently , Bayesian Inference with Gaussian Process emu - lator is used for reconstructing the nuclear structure including deformation parameters based on heavy ion collisions mea - surements [ 49 ] . As a ﬁrst step exploratory study , the collision observables ( charged multiplicities N ch , elliptic ﬂow v 2 , tri - 10 angular ﬂow v 3 and mean transverse momentum (cid:104) p T (cid:105) ) are estimated with Monte Carlo Glauber model calculation ( to - tal energy E , elliptic eccentricity (cid:15) 2 , triangular eccentricity (cid:15) 3 and energy density d ⊥ ) , which is argued to be able to give reasonable estimator for the ratio of observables in isobaric collision systems since cancellation of dynamics’ uncertain - ties [ 50 ] . Under this setup , nuclear structure reconstruction based upon both the single collision system and contrast iso - baric collision system measurements are discussed . For sin - gle collision systems it’s found that the Woods - Saxon param - eters of nuclei can be precisely inferred from ﬁnal state ob - servables estimated with ( P , (cid:15) 2 , (cid:15) 3 , d ⊥ ) . For isobar collision systems , the simultaneous inference on the two set of nu - clear structures fails with only ratio of those ﬁnal observables , while the further provision of single collision system’s mul - tiplicity distribution makes high precision nuclear structure reconstruction possible . Additionally , the ratio of radial ﬂow is found to be redundant in the presence of the ratio of elliptic ﬂow and vise versa . α - clustering structure : Clustering structure is an exotic phenomenon in nuclei , it usually takes place in light nuclei [ 51 ] . In nuclear collisions for light clustering nucleus against a heavy - ion , the clustering structure could make the ﬁnal state particles anisotropically distributed [ 42 , 52 , 53 ] . It is crucial to determine how to extract the quantitative information about the clustering from the ﬁnal observables . In the 12 C / 16 O + 197 Au collisions at relativistic energies , a machine learning method is used to retrieve evidence of the cluster structures from the azimuthal angle and transverse momentum distribu - tions of charged pions [ 54 ] . In this work , a Bayesian Convo - lutional Neural Network ( BCNN ) is used . Except for an in - put layer and an output layer , the hidden layers consist of four convolutional layers and three fully connected layers . The pa - rameters of the three fully connected layers are sampled from learned distributions that trained by Bayesian inference . A two - dimensional histogram of azimuthal angle versus trans - verse momentum is designed as input . Considering the de - tection efﬁciency in experiments , charged pions with rapidity from - 1 to 1 and transverse momentum from 0 to 2 GeV / c are selected . The whole data set consists of 1 . 6 × 10 6 histograms with 64 × 64 bins ( pixels ) , with different labels to indicate different conﬁgurations . The typical spectra of 4000 merged events are shown in Fig . 6 . Even with merging , the samples of different conﬁgurations are still barely distinguishable to the naked eye . The number of merged events is denoted as NEvent , which is taken to be 1000 , 2000 , and 4000 . The learning curves are shown in Fig . 7 . As more events are merged , the event - by - event ﬂuctuations are reduced and the network is able to learn the features of the ﬁnal state to pre - dict the initial conﬁguration . For 12 C with NEvent = 4000 and 16 O with NEvent = 2000 , the validation accuracy reaches 95 % and 97 % , respectively , and for 16 O with NEvent = 4000 it is even 99 % . For the cluster phenomenon , it is extremely difﬁcult to ex - tract signals from the ﬁnal particles because ﬂuctuations play such an important role in relativistic heavy ion collisions . By averaging over multiple events , the BCNN model is able to learn the features with promising performance . Fig . 6 . ( Color online ) Taken from Ref . [ 54 ] . Two - dimensional az - imuthal angle vs transverse momentum distributions of charged pi - ons for non - clustered ( Up ) and clustered ( Down ) 12 C from AMPT - generated 12 C + 197 Au collision event at √ S NN = 200 GeV . Neutron skin estimation The distribution of neutrons is im - portant in determining the thickness of the neutron skin , the symmetry energy of the nucleus , the QCD EoS of dense nu - clear matter , as well as some astrophysical observables such as the mass - radius relation of neutron stars and the gravita - tional wave emitted during neutron star mergers . However , extracting the distribution of neutrons inside the nucleus is extremely difﬁcult . The distribution of neutrons inside the nucleus is different from protons . The proton distribution is much easier to measure than the neutron distribution because the former is equivalent to the charge distribution , while the latter is associated with the weak charge distribution . The neutron skin , which is the difference between the root - mean - square radius of neutrons and protons , provides a way to de - termine the neutron ( weak charge ) distribution in the nucleus . 11 Fig . 7 . ( Color online ) Taken from Ref . [ 54 ] . Validation accuracy during the training process for colliding systems 12 C / 16 O + 197 Au with NEvent = 1000 , 2000 , and 4000 . PREX2 measured the parity - violating asymmetry by scatter - ing longitudinally polarized electrons on Pb208 to obtain a neutron skin thickness of about R n − R p = 0 . 283 ± 0 . 071 fm [ 55 ] . The neutron skin is used as a constraint in the cal - culation of the positive and negative correlations between the symmetry energy and the slop parameter at saturation den - sity . With this constraint , the Bayesian analysis achieves a compromise between the “conﬂicting” data that lead to the famous “PREXII puzzle” and the “soft Tin puzzle” [ 56 , 57 ] . There are many efforts to determine the neutron skin thick - ness and the symmetry energy at low energy [ 58 ] , such as the charge - exchange spin - dipole excitation [ 59 ] , the super - nova neutrinos [ 60 ] , nuclear fragmentation reactions [ 61 ] , and parity - violating electron scattering [ 55 , 62 ] . For high - energy heavy ion collisions , it is proposed that the isobar ratios of the charge multiplicities of the mean trans - verse momentum and the net charge multiplicities between 9644 Ru + 9644 Ru and 9640 Zr + 9640 Zr . can be used to precisely deter - mine the nucleon skin and the symmetry energy . The authors claim that the high - energy isobar collisions can signiﬁcantly improve the result of the the traditional low energy method [ 63 ] . In another study , the yields of spectator protons and neutrons at the forward velocity of ultra - central collisions are proposed to be good probes of the neutron skin , sensitive to the neutron skin of 208 Pb but insensitive to other parameters during the collision [ 64 ] . An even cleaner probe is to measure the free spectator neutron yield ratios between 9644 Ru + 9644 Ru and 96 40 Zr + 96 40 Zr , in ultra - central collisions [ 65 ] . A lot of data have already been collected from high - energy heavy ion collisions . There may be a data - driven way to reuse these data to determine neutron distribution and neutron skin thickness . It has been tested in [ 66 ] that nucleons sampled from nuclei with different neutron skin types can be classiﬁed with reasonable accuracy using deep CNN and point cloud networks . However , once the nucleus is involved in heavy ion collisions , it is almost impossible to distinguish the neu - tron skin types of the colliding nucleus using the momentum distribution of the ﬁnal state hadrons . For this particular task , the signal is weak in minimum bias collisions and deep neu - ral networks fail to solve this difﬁcult inverse problem . A new machine - learning method is needed to search for small and weak signals in data with large statistical ﬂuctuations . IV . BULK MATTER Shear and bulk viscosity Shear and bulk viscosity are im - portant properties that strongly inﬂuence the dynamical ex - pansion of QGP and the momentum distribution of ﬁnal state hadrons as shown in relativistic ﬂuid dynamics simulations [ 67 – 69 ] . In solving the inverse problem of HIC it was found that the effects of viscosity are entangled with the initial ther - malisation time , the equation of state of QGP and the phase transition between QGP and HRG . Determining the shear and bulk viscosity of hot nuclear matter thus becomes a notori - ously difﬁcult problem . Bayesian analysis plays an important role in determining the temperature dependence of the shear viscosity over the entropy density ratio η / s ( T ) as well as the bulk viscosity over the entropy density ratio ζ / s ( T ) [ 70 – 72 ] . Suppose all parameters in the theoretical model of HIC form a set { θ } , all experimental data from RHIC and LHC form another set { D } , then the posterior distribution of the model parameters is given by P ( θ i | D ) = P ( D | θ i ) P ( θ i ) P ( D ) = P ( D | θ i ) P ( θ i ) (cid:80) j P ( D | θ j ) P ( θ j ) ( 34 ) where P ( D | θ i ) is the likelihood between experimental data D and model output using parameter combinations θ i , P ( θ i ) is the a priori distribution of θ i , which may be our belief based on past experience or physical considerations , the de - nominator P ( D ) = (cid:80) j P ( D | θ j ) P ( θ j ) is a normalisation factor called evidence . Note that P ( D ) is computationally too expensive to compute because it requires the theoretical model to traverse the entire parameter space . Fortunately , in Bayesian analysis , the normalisation factor is not really needed because the Markov Chain Monte Carlo ( MCMC ) method is able to sample from the following unnormalised distributions P ( θ i | D ) ∝ P ( D | θ i ) P ( θ i ) ( 35 ) The ﬁnal output of the Bayesian analysis is a large number of different combinations of model parameters , sampled from the above unnormalised posterior distribution function . Do - ing a density estimation for each parameter , e . g . , the slope of η / s over T c , gives a distribution ( or histogram ) of the slope parameter . This distribution has its maximum value , the loca - tion of which corresponds to the MAP estimate . It also has a 12 variance that corresponds to the uncertainty in the slope pa - rameter , which comes from the experimental data , the prior distribution and the likelihood function . It is thus clear that the extracted model parameters are well constrained when their posterior distribution has a narrow peak . To estimate the temperature dependence of shear and bulk viscosity , two parameterised functions based on physical a priori are required . In the Nature paper [ 71 ] the shear and bulk viscosity are parameterised as , ( η / s ) ( T ) = ( η / s ) min + ( η / s ) slope ( T − T c ) (cid:18) T T c (cid:19) ( η / s ) crv ( 36 ) ( ζ / s ) ( T ) = ( ζ / s ) max 1 + (cid:16) T − ( ζ / s ) T peak ( ζ / s ) width (cid:17) 2 ( 37 ) where ( η / s ) min and ( ζ / s ) max are the minimum / maximum shear and bulk viscosity values to be determined , T c = 154 MeV is the QCD transition temperature representing the lo - cation of the minimum in η / s ( T ) , ( ζ / s ) T peak is the location of the maximum bulk viscosity to be determined . Other pa - rameters to be determined are the slope ( η / s ) slope and the curvature of the shear viscosity ( η / s ) crv , and the width of the bulk viscosity peak ( ζ / s ) width . Without considering other parameters , these 6 parameters form a 6 - dimensional parameter space . The above Bayes formulae are used to walk in this space , with the trajec - tories forming a set of parameter combinations . This is equivalent to importance sampling using the posterior dis - tribution of these 6 parameters . Density estimation shows that the distribution of ( η / s ) min is approximately normal , whose mean and variance give a quantitative estimate of ( η / s ) min = 0 . 085 + 0 . 026 − 0 . 025 . An anti - correlation is observed be - tween ( ζ / s ) max and ( ζ / s ) width , indicating that it is the in - tegral of ( ζ / s ) ( T ) that matters , not its speciﬁc form . The analysis also shows that the experimental data used have no constraining power on the parameters ( η / s ) crv and ( ζ / s ) T peak , as there are no obvious peaks in the posterior distributions of these 2 parameters . Nuclear temperature The difﬁculties in studying the ﬁnite temperature properties of nuclear matter arise mainly from the preparation of a ﬁnite temperature nuclear system and the determination of its temperature . Heavy - ion collisions pro - vide a possible venue for studying the ﬁnite temperature prop - erties of nuclear matter . During the reaction , a transient ex - cited system is formed , which can generally be regarded as a ( near ) equilibrium state , since the evolution of its constituent nucleons is sufﬁciently short compared to the global evolu - tion . Its temperature can be obtained from e . g . energy spectra by moving source ﬁtting , excited state populations , ( double ) isotope ratios , or quadruple momentum ﬂuctuations , etc . For a brief review , see Ref . [ 73 ] . For a reliable thermometer of HICs , we require it to be insensitive to both the collective effects and the secondary decay of unstable nuclei after the system disintegrates , which is generally difﬁcult to achieve . Moreover , because of the difﬁculty in verifying the accuracy of the apparent temperature obtained by these thermometers , it is not a trivial task to propose different ways of determining the apparent temperature and thus provide more opportunities for cross - checking . Using machine learning techniques , the charge multiplic - ity distribution can be used to determine the apparent tem - perature of HICs at intermediate to low energies [ 74 ] . Usu - ally , the fragment charge distributions show typical changes of the hot nuclei disassembly mechanism with temperature , i . e . , from evaporation mechanism at lower temperature ( e . g . , T model = 2 MeV ) , to multifragmentation at intermediate tem - perature ( e . g . , T model = 8 MeV ) , till vaporization at higher temperature ( e . g . , T model = 17 MeV ) . A relation between the source temperature T model and M c ( Z cf ) can then be established by a DNN that transforms the complex relation into a nonlinear map through its neu - rons . This relation can be used to determine the apparent temperature of a particular transient state during HICs at intermediate - to - low energies , such as the 103 Pd + 9 Be frag - mentation reaction , via their ﬁnal - state M c ( Z cf ) . With the ﬁnal state charge multiplicity distribution simulated with the IQMD , its apparent temperature was obtained by the trained DNN . With the T ap determined by DNN , the caloric curve , i . e . , the apparent temperature as a function of the excitation en - ergy per nucleon E ∗ / A of the HICs , was also examined . Fig . 8 ( a ) shows the caloric curve of the 103 Pd + 9 Be reaction from the IQMD simulation with the apparent temperature de - termined by DNN using M c ( Z cf ) of the reaction . As shown in the ﬁgure , the increase of T ap slows down when E ∗ / A reaches to about 8 MeV . Traditionally , this characteristic be - havior of the caloric curve is explained by the fact that , as the excitation energy increases , the system is driven into a spin - odal region , in which part of the excitation energy begins to transfer to latent heat . The T ap at the maximum of the spe - ciﬁc heat capacity of the system ˜ c ≡ d ( E ∗ / A ) dT ap is called the limit temperature T lim . The caloric curve in Fig . 8 ( a ) leads to T lim = 6 . 4 MeV ( through a polynomial ﬁt of the ˜ c , red dashed line in the inset of Fig . 8 ( a ) ) . This value of T lim fol - lows the general trend of the Natowitz’s limiting - temperature dependence on the system size [ 75 ] , and thus indicates the va - lidity of determining the apparent temperature through charge multiplicity distribution presented in this article . Nuclear liquid gas phase transition As mentioned above , machine - learning techniques can be used to study nuclear temperature and then the caloric curve , which is of particu - lar interest in reaction dynamics . A lot of probes by analyz - ing sophisticatedly the information of the reaction products have been proposed to recognize the liquid - gas phase tran - sition of nuclei [ 75 , 77 – 84 ] . Since the nucleus is an uncon - trollable system , its liquid - gas phase transition is realized by tracing the effect of the spinodal instability on the reaction dynamics , e . g . , by measuring the properties of the interme - diate mass fragments ( with charge number greater than 3 ) . In a recent work , the averaged charge multiplicity distribu - tion (cid:104) M c (cid:105) ( Z ) of the quasi - projectile ( QP ) fragments in the reactions of 40 Ar on 27 Al and 48 Ti at 47 MeV / nucleon has been used to study the nuclear liquid - gas phase transition by 13 3 6 9 15 12 0 2 4 6 8 10 12 E ex / A ( MeV ) T a p ( M e V ) E ex / A ( MeV ) T a p ( M e V ) Fig . 8 . ( Color online ) Upper panel : Taken from Ref . [ 74 ] . Caloric curve of the reaction 103 Pd + 9 Be . Black open squares represent the result based on the IQMD model , with T ap determined by DNN using M c ( Z cf ) . The blue dashed line is its polynomial ﬁt . The inset shows the speciﬁc heat capacity ˜ c derived from the ﬁtted formula . Lower panel : Taken from Ref . [ 76 ] . Scatter plot of the appar - ent temperature versus the excitation energy per nucleon . The red dashed line represents (cid:104) T ap (cid:105) as a function of E ex / A . The horizontal and vertical cyan dotted lines represent the limit temperature and an analogical characteristic value of E ex / A obtained by the confusion scheme , respectively ( see below ) . the autoencoder method and a confusion scheme [ 76 ] . The QP fragments are supposed to come from the excited projec - tile nucleus , which can largely avoid the effect of dynamical evolution . They can be obtained by a three - source ( i . e . , a QP source , an intermediate velocity source and a quasi - target source ) reconstruction method [ 82 ] . Fig . 8 ( b ) shows the scat - ter plot of T ap versus the system’s excitation energy E ex / A , i . e . , the caloric curve , of the events with Z QP = 12 from the experimental data . The event - by - event charge - weighted charge multiplicity distribution of QP fragments ZM c ( Z ) from the experiment are used as the input to train the autoencoder network . The network consists of two main parts , the encoder part encodes the input event - by - event ZM c ( Z ) into a latent variable , and the decoder part decodes the latent variable into ZM (cid:48) c ( Z ) . The neural network is trained to recover the encoded in - formation as best as possible , i . e . , the network is trained to minimize the difference between ZM c ( Z ) and ZM (cid:48) c ( Z ) . Fig . 9 shows the averaged M c ( Z ) in three typical E ex / A bins ( dashed lines ) . For the test QP events , the reconstructed M (cid:48) c ( Z ) are averaged and compared with the original M c ( Z ) in Fig . 9 . Once the autoencoder network is trained , each QP event is mapped to the latent variable . The latent variable as a function of T ap and E ex / A shows a sigmoid pattern , indi - cating that the trained autoencoder network treats the low and high temperature ( or low and high excitation energy ) regions differently . The area in the midst of the two phases represents those liquid - gas coexistence events that enter the spinodal re - gion and are affected by the spinodal instability . A confusion scheme combining the supervised and unsu - pervised learning has been adopted to obtain the limit tem - perature of the nuclear liquid - gas phase transition . In the con - fusion scheme , the neural network is trained with data that are deliberately labelled incorrectly according to a proposed critical point , and the phase transition properties can be de - duced from the performance curve , i . e . , the total test accu - racy as a function of the proposed critical point , of the neural network [ 85 ] . The total test accuracy reaches its minimum at T (cid:48) ap ≈ T lim . The limiting temperature through the confu - sion scheme is 9 . 24 ± 0 . 04 MeV , which is consistent with the 9 . 0 ± 0 . 4 MeV obtained from the traditional analysis of caloric curve [ 86 ] . Crossover or ﬁrst order phase transition In general , as mentioned in the Introduction , the challenge being associ - ated with high energy nuclear collision studies essentially can be viewed as an inverse problem . That is , assuming that all related physical factors ( e . g . , initial condition / ﬂuc - tuations , QGP bulk properties , transport coefﬁcients , freeze - out parameter , hadronic interactions , etc . ) are given , then well - established theoretical models ( e . g . , relativistic viscous hydrodynamics with hadronic transport simulation ) can be adopted to simulate the heavy ion collision process to give their ﬁnal state observables , such a forward process is well understood . However , given instead only limited measure - ments on ﬁnal state of heavy ion collisions , it’s unclear how to disentangle those different inﬂuencing physical factors to decode back those corresponding early time dynamics . For high energy heavy ion collisions , two strategies exist in tack - ling this inverse problem with the help of statistical methods and machine learning , one is Bayesian inference with the task of parameter estimation for calibrating the chosen model ( e . g . , in Ref . [ 87 ] and those introduced in above texts ) , the other one is supervised machine learning to capture directly the inverse mapping from the ﬁnal state to the corresponding physics of interest . In Ref . [ 88 ] it is proposed to use deep convolutional neural network to capture the direct inverse mapping from the ﬁnal state information to the cared early time happened QCD tran - sition types . This is inspired by the success of image recog - nition in computer vision , that although the inverse mapping might be very implicit , but it’s possible to use deep neural networks to decode and represent it in the sense of big data supervisedly . The needed training data can be prepared from 14 0 2 4 6 8 10 12 10 - 2 10 - 1 10 0 10 1 10 2 0 2 4 6 8 10 12 10 - 2 10 - 1 10 0 10 1 10 2 0 2 4 6 8 10 12 10 - 2 10 - 1 10 0 10 1 10 2 · M c Ò ( Z ) Z Experiment Autoencoder reconstruction Experiment Autoencoder reconstruction Z Experiment Autoencoder reconstruction Z ( a ) ( b ) ( c ) Fig . 9 . ( Color online ) Taken from Ref . [ 76 ] . The averaged charge multiplicity distribution (cid:104) M c (cid:105) ( Z ) of the QP fragments . The average is taken for different E ex / A bins , left panel for low excitation ( 0 . 9 MeV - 2 . 8 MeV ) , middle panel for intermediate excitation ( 5 . 3 MeV - 5 . 4 MeV ) , and right panel for high excitation ( 8 . 1 MeV - 13 . 0 MeV ) . The dashed curves represent (cid:104) M c (cid:105) ( Z ) from the NIMROD experiment , while the circles from the autoencoder network reconstruction (cid:104) M (cid:48) c (cid:105) ( Z ) . Each E ex / A bin contains 500 test events . Fig . 10 . A schematic plot for QCD transition classiﬁcation with HICs ﬁnal particle spectra . well - established model simulation for heavy ion collisions , such as the state of the art 3 + 1 - dimensional viscous hydrody - namics [ 89 , 90 ] , where diversity can be introduced in by vary - ing these different physical factors ( i . e . , parameters within the simulation ) . As an exploratory study a binary classiﬁcation task is targeted , where the Deep CNN is trained to identify the QCD transition type embedded within the collision dynamics to be crossover or ﬁrst order solely based upon the ﬁnal pion spectra ρ ( p T , φ ) , as shown in Fig . 10 . Basically , the EoS of the hot and dense matter enters the hydrodynamic simulations as a crucial ingredient , embedded in which the nature of QCD transition ( ﬁrst order or crossover ) can affect strongly the hy - drodynamic evolution due to the shape of pressure gradient . As the input to the deep CNN , ﬁnal charged pion’s spectra at mid - rapidity are obtained from the Cooper - Frye formula in each hydrodynamic simulation , ρ ( p T , φ ) = dN i dY p T dp T dφ = g i (cid:90) σ p µ dσ µ f i ( p · u ) ( 38 ) with N i the particle number density , Y the rapidity , g i the degeneracy , dσ µ the freeze - out hypersurface element and f i the thermal distribution . The training dataset of ρ ( p T , φ ) is generated from event - by - event hydrodynamic package CLVisc [ 89 ] with ﬂuctuating AMPT initial conditions , with which supervised learning using CNN is performed for bi - nary classiﬁcation in identifying the QCD transition types . Shown in Fig . 11 is the space - time evolution histories of QGP expansion starting from the same initial condition with different conditions , in relativistic hydrodynamic simulations using CLVisc . For EOSQ with a ﬁrst order phase transition , the pressure gradient is zero in the mixed phase . Multiple ridge structures are formed with a ﬁrst order phase transition in the EoS because the expansion of QGP is mainly driven by the pressure gradient and the acceleration is 0 in the mixed phase . On the other hand , the expansion histories are quite different when the shear viscosity is not 0 . Different evolu - tion histories lead to different ﬁnal state particle spectra in momentum space . To verify the robustness of the trained Deep CNN in this QCD EoS recognition task , the testing set was simulated from a different hydrodynamics package , or with different initial ﬂuctuating conditions ( IP - Glasma or MC - Glauber ) and dif - ferent η / s parameters . The conventional observables like el - liptic ﬂows v 2 and integrated particle spectrum are shown to be incapable of distinguishing the two QCD transition classes on these testing set , while the trained deep CNN gives on average 95 % classiﬁcation accuracy which is robust against other contamination factors , especially from initial ﬂuctua - tions and shear viscosity . For comparison , the classiﬁcation accuracy from traditional machine learning algorithms such as decision tree , random forest , support vector machine or gradient boosting is to the best approximately 80 % . The good performance of the trained deep CNN ﬁrst indicate that im - print of the early time transition dynamics is not fully washed 15 E O S L η / s = 0 . 0 τ = 0 . 4 fm τ = 1 . 9 fm τ = 3 . 7 fm τ = 6 . 7 fm E O S Q η / s = 0 . 0 E O S L η / s = 0 . 08 E O S Q η / s = 0 . 08 Fig . 11 . The evolution history of quark gluon plasma simulated by relativistic hydrodynamic model CLVisc , starting from the same ini - tial condition with four different parameter combinations . From top to down , each row represents four snapshots at different time , using different combinations of equation of state and shear viscosity over entropy density ratio , where EOSL represents lattice QCD equaiton of state with a crossover transition between QGP and hadron reso - nance gas , EOSQ represents an equation of state with a ﬁrst order phase transition between QGP and hadron resonance gas . out by the collision evolution , and is still encoded inside the ﬁnal state information . And , the inverse mapping from ﬁ - nal state observables to the QCD transition information can be well captured by the deep CNN from supervised train - ing strategy , thus enabling discriminative and traceable en - coder for the dynamical information of QCD transition . In this way , the constructed deep CNN act as an “EoS - meter " to efﬁciently bridge the heavy ion collision experiments to QCD bulk matter physics . This study paves a path to the success of experimental study on QCD EoS and the search for the criti - cal end point of QCD phase diagram . It should be noted that , this study didn’t consider the afterburner hadronic cascade ef - fects , thus the conclusion about the direct inverse mapping is made on the level of pure hydrodynamic evolution . Later this strategy was further deepened in a series of works for more realistic scenarios , like to take into account the afterburner hadronic cascade via incorporating UrQMD following the hydrodynamics evolution [ 91 , 92 ] , to consider non - equilibrium dynamics of phase transition’s inﬂuence e . g . , spinodal decomposition [ 16 , 93 ] or Langevian dynamics [ 94 ] , to further also include the more realistic experimental detec - tor effects through detector simulation with Hits or Tracks as input [ 47 , 95 ] , perform unsupervised outlier detection in heavy ion collisions [ 96 ] and identifying nuclear symmetry energy [ 97 ] . Speciﬁcally , in Ref . [ 95 ] it was shown that us - ing just the detector output directly , PointNet models can be used to classify collision events simulated by an EoS asso - ciated with a ﬁrst order phase transition from an EoS with a crossover transition . The PointNet models take the recon - structed tracks from the CBM detector ( simulated with CBM - Root ) following by the hybrid - UrQMD events . It achieves a binary classiﬁcation accuracy around 96 % when trained on collision events for impact parameter ranging from 0 to 7 fm . When the model training set was shrunk to mid - central region with b = 0 ∼ 3 fm , the model performance increased with an accuracy about 99 % . A combination of training set from both peripheral and mid - central collisions also works and result in a classiﬁer being able to identify the phase transition type across different centralities meanwhile not compromising the accuracy for central region . Active learning for QCD EoS First principle calculations using Lattice QCD provide the equation of state of hot nu - clear matter at high temperature and zero baryon chemical potential . Because of fermion’s sign problem , Lattice QCD fails to compute the nuclear EoS at ﬁnite µ B at present . Us - ing Taylor expansion , it is possible to get the nuclear EoS at small µ B that is close to zero , approximately . The BEST collaboration constructed one nuclear EoS with critical end point by mapping the 3D Ising model with the Tylor expan - sion result . However , the model contains 4 free parameters whose values determine the size and location of the critical end point . Some combinations of these parameters lead to unphysical EoS , e . g . , acausal or unstable . Supervised learning can help to mapping out unphysical re - gions of parameter combinations . However , labelling is com - putationally expansive in this task . For thermaldynamic sta - bility , one has to check the positivity of energy density , pres - sure , entropy density , baryon density , second order baryon susceptibility χ B 2 and the heat capacity ( ∂S / ∂T ) n B , as well as the causality condition , 0 ≤ c 2 s ≤ 1 ( 39 ) where c s is the speed of sound of hot nuclear matter . The authors use active learning to ﬁnd the most informative parameter combinations before labelling them [ 98 ] . In active learning , the network is ﬁrst trained using a small amount of labelled data . Then the trained network is employed to make predictions on all samples from a large unsupervised pool . If the network is uncertain about one parameter combination , e . g . , it predicts that this group of parameter combination will lead to a EoS that is unphysical with probability 51 % , then this sample lives on the decision boundary and should be quite informative and important for the network . Labelling this sample will improve the performance of the network more than labelling easy samples . The newly labelled sample will be moved out from the pool and will take part in supervised learning later . Deep learning accelearate Relativistic Hydrodynamics Relativistic Hydrodynamics is a powerful tool to simulate the QGP expansion and study the ﬂow observables in Relativistic Heavy Ion Collisions at RHIC and the LHC energies [ 101 – 106 ] . For ideal hydrodynamics with zero net charge densities , it solves the transport equations of the energy momentum ten - 16 5 0 5 y ( f m ) 2 = 0 . 188 3 = 0 . 210 4 = 0 . 080 ( a1 ) Ed ( Initial ) 5 0 5 y ( f m ) 2 = 0 . 496 3 = 0 . 175 4 = 0 . 173 ( b1 ) 5 0 5 y ( f m ) 2 = 0 . 188 3 = 0 . 210 4 = 0 . 080 ( c1 ) 5 0 5 y ( f m ) 2 = 0 . 610 3 = 0 . 193 4 = 0 . 222 ( d1 ) 5 0 5 y ( f m ) 2 = 0 . 099 3 = 0 . 119 4 = 0 . 137 ( e1 ) 5 0 5 x ( fm ) 5 0 5 y ( f m ) 2 = 0 . 168 3 = 0 . 078 4 = 0 . 103 ( f1 ) 10 70 130 ( a2 ) 2 = 0 . 178 3 = 0 . 150 4 = 0 . 053 ed×1 . 0 Ed ( VISH2 + 1 ) ( a3 ) 2 = 0 . 179 3 = 0 . 149 4 = 0 . 054 ed×1 . 0 Ed ( sU - net ) ( b2 ) 2 = 0 . 418 3 = 0 . 132 4 = 0 . 200 ed×1 . 0 ( b3 ) 2 = 0 . 425 3 = 0 . 135 4 = 0 . 201 ed×1 . 0 ( c2 ) 2 = 0 . 134 3 = 0 . 030 4 = 0 . 042 ed×3 . 0 ( c3 ) 2 = 0 . 130 3 = 0 . 037 4 = 0 . 039 ed×3 . 0 ( d2 ) 2 = 0 . 255 3 = 0 . 095 4 = 0 . 113 ed×3 . 0 ( d3 ) 2 = 0 . 280 3 = 0 . 099 4 = 0 . 123 ed×3 . 0 ( e2 ) 2 = 0 . 021 3 = 0 . 026 4 = 0 . 072 ed×6 . 0 ( e3 ) 2 = 0 . 023 3 = 0 . 014 4 = 0 . 075 ed×6 . 0 5 0 5 x ( fm ) ( f2 ) 2 = 0 . 032 3 = 0 . 027 4 = 0 . 032 ed×6 . 0 5 0 5 x ( fm ) ( f3 ) 2 = 0 . 036 3 = 0 . 029 4 = 0 . 053 ed×6 . 0 10 30 50 ( a4 ) MC - Glauber Vx ( VISH2 + 1 ) ( a5 ) 0 = 2 . 0fm / c Vx ( sU - net ) ( b4 ) TRENTo ( b5 ) 0 = 2 . 0fm / c ( c4 ) MC - Glauber ( c5 ) 0 = 4 . 0fm / c ( d4 ) AMPT ( d5 ) 0 = 4 . 0fm / c ( e4 ) MC - Glauber ( e5 ) 0 = 6 . 0fm / c 5 0 5 x ( fm ) ( f4 ) MC - KLN 5 0 5 x ( fm ) ( f5 ) 0 = 6 . 0fm / c 0 . 9 0 . 6 0 . 3 0 . 0 0 . 3 0 . 6 0 . 9 ( a6 ) Vy ( VISH2 + 1 ) ( a7 ) Vy ( sU - net ) ( b6 ) ( b7 ) ( c6 ) ( c7 ) ( d6 ) ( d7 ) ( e6 ) ( e7 ) 5 0 5 x ( fm ) ( f6 ) 5 0 5 x ( fm ) ( f7 ) Fig . 12 . Energy density and ﬂow velocity proﬁles , predicted by sU - Net and calculated from VISH2 + 1 for six test initial proﬁles of MC - Glauber , MC - KLN , AMPT and TRENTo [ 99 , 100 ] . sor : ∂ µ T µν = 0 ( 40 ) where T µν = ( e + p ) u µ u ν − pg µν , e is the energy density , p is the pressure , and u µ is the four velocity . For traditional sim - ulations , hydrodynamics numerically solve these transport equations with some particular Algorithm such as SHASTA , LCPFCT , which translate the initial conditions to ﬁnal state proﬁles through its non - linear evolutions [ 103 , 106 – 108 ] . In the recent work [ 99 , 100 ] , a deep neural network , called stacked U - net ( sU - net ) , is designed and trained to learn the initial and ﬁnal state mapping from the nonlinear hydrody - namic evolution . The constructed sU - net belongs to the ar - chitecture of encoder - decoder network , which contains four blocks of U - net with residual connections between them . For each U - net , there are three convolutional and deconvolutional layers with Leaky ReLU and softplus activation functions em - ployed for the inner and output layers . By concatenating the feature maps along the channel dimension , the output of the ﬁrst two convolution layers is fed into the last two deconvo - lution layers . For more details , please refer to [ 99 , 100 ] . The training and test data ( the proﬁles of initial and ﬁ - nal energy momentum tensor T ττ , T τx , T τy ) are generated from VISH2 + 1 hydrodynamics [ 109 , 110 ] with zero viscos - ity , zero net baryon density and longitudinal boost invariance . In more detail , sU - net is trained with 10000 initial and ﬁ - nal proﬁles from VISH2 + 1 with MC - Glauber initial condi - tions [ 111 , 112 ] , and then tested for its predictive power us - ing the proﬁles of four different types of initial conditions , including MC - Glauber [ 111 , 112 ] , MC - KLN [ 112 , 113 ] , AMPT [ 89 , 114 , 115 ] and TRENTo [ 116 ] . Fig . 12 shows the ﬁnal energy density and ﬂow velocity predicted by sU - net , to - gether with a comparison with the hydrodynamic results . It can be seen that the trained sU - net can capture the magni - tudes and structures of both energy density and ﬂow velocity . In particular , panels ( b ) , ( d ) and ( f ) show that the network , trained with data sets from MC - Glauber initial conditions , is also capable of predicting the ﬁnal proﬁles of other types of initial conditions . Ref . [ 99 , 100 ] also calculated the eccen - tricity coefﬁcients which evaluate the deformation and inho - 17 mogeneity of a large number of the energy density proﬁles , which demonstrated that the predictions from sU - net almost overlap with the results from VISH2 + 1 . Compared with the 10 ∼ 20 minutes simulation time of VISH2 + 1 on a traditional CPU , sU - net takes several seconds to directly generate the ﬁnal proﬁles for different types of ini - tial conditions on one P40 GPU , which greatly accelerates the traditional hydrodynamic simulations . However , the de - signed and trained sU - net in Ref . [ 99 , 100 ] mainly focuses on mimicking the 2 + 1 - dimensional hydrodynamic evolution with ﬁxed evolution time . For more realistic implementation , it is also important to explore the possibilities of mapping the initial proﬁles to the ﬁnal proﬁles of the particles emitted on the freeze - out surface of the relativistic heavy ion collisions . V . IN - MEDIUM EFFECTS spectral function reconstruction Accessing Real - time properties of QCD ( or a many - body system in general ) remain a notoriously difﬁcult problem , because the non - perturbative computations , such as lattice ﬁeld simulations or functional methods , usually operate in Euclidean space - time ( after a Wick rotation t → it ≡ τ ) and thus can only provide Eu - clidean correlators ( i . e . , in imaginary time ) . Then the analytic continuation of these discrete noisy data is often ill - posed . But quantitatively understanding the real - time dynamics de - termined by the Minkowski correlator is important and in - teresting in itself , for example for understanding scattering processes , transport or non - equilibrium phenomena that hap - pen in heavy ion collisions . The Minkowski correlator is usu - ally accessed from the Euclidean correlator via spectral re - construction . The associated ill - posed problem can be cast as a Fredholm equation of the ﬁrst kind , g ( t ) = (cid:90) b a K ( t , s ) ρ ( s ) ds , ( 41 ) with the goal of retrieving the function ρ ( s ) given the ker - nel function K ( t , s ) but limited information about g ( t ) . It has been well shown that the required inverse transform be - comes ill - conditioned once only a ﬁnite set of data points with non - vanishing uncertainty are available for g ( t ) . In the con - text of QFT , one could simply approach this problem via the Kaellen - Lehmann spectral representation of the correlators , thus taking the kernel function to be K ( t , s ) = s ( s 2 + t 2 ) − 1 π − 1 , ( 42 ) where the ρ ( s ) involved are usually called spectral functions . The task of reconstructing the spectral function from the cor - relator measurements ( from the lattice calculation ) needs to be regularized to make sense of the inverse problem involved . Over the past few decades , many different regularization tech - niques have been explored for this ill - conditioned inverse problem , such as Tikhonov regularization , maximum entropy methods , Bayesian inference techniques . Recently , deep learning based strategies have also been ex - plored to tackle spectral reconstruction , which can be mainly categorized into two schemes : data - driven supervised learn - ing approaches and unsupervised learning based approaches . The ﬁrst application of domain - knowledge - free deep learning methods to this ill - conditioned spectral reconstruction ( also called analytic continuation ) appears in Ref . [ 117 ] in the con - text of general quantum many - body physics , which shows the good performance of deep neural networks via super - vised training in the cases of a Mott - Hubbard insulator and also metallic spectrum . In particular , the convolutional neu - ral network was found to achieve better reconstruction than the fully connected network , with performance superior to the MEM , one of the most widely used conventional methods . In Ref . [ 118 ] the authors adopted a similar strategy , but also in - troduced Principal Component Analysis ( PCA ) to reduce the dimensionality of the QMC simulated imaginary time corre - lation function of the position operator for a harmonic oscil - lator linearly coupled to an ideal heat bath . Ref . [ 119 ] also from a data - driven perspective , pushed a similar strategy to spectral function reconstruction into the QFT context and considered the Källen – Lehmann ( KL ) spec - tral representation as the accessible propagator , G ( p ) = (cid:82) ∞ 0 dωπ ωρ ( ω ) ω 2 + p 2 , which basically takes the kernel in the Fred - holm equation as the Källen – Lehmann ( KL ) kernel . For the dummy spectral functions , the superposition of Breit - Wigner peaks was used , based on the perturbative one - loop QFT - derived parameterisation ρ BW ( ω ) = 4 A Γ ω / ( ( M 2 + Γ 2 − ω 2 ) 2 + 4Γ 2 ω 2 ) . Two types of deep neural networks have been studied , both with the noisy propagator as input , but with dif - ferent outputs : one estimates the parameters ( e . g . Γ i , M i for the collection of Breit - Wigner peaks ) of the spectral function ( denoted PaNet ) , the other tries to reconstruct directly the dis - cretized data points of the spectral function ( denoted PoNet ) . As another non - parametric representation , Gaussian Pro - cesses ( GP ) are used in the reconstruction of the 2 + 1 ﬂavor QCD ghost and gluon spectral function in Ref . [ 121 ] . In gen - eral , the GP can deﬁne a probability distribution over fami - lies of functions , typically characterized by the chosen kernel function . In Ref . [ 121 ] the GP is assumed to describe the spectral function , ρ ( ω ) ∼ GP ( µ ( ω ) , C ( ω , ω (cid:48) ) ) , ( 43 ) with µ ( ω ) the mean function often set to zeros , and C ( ω , ω (cid:48) ) the covariance dictated by the kernel function used , to which a common standard choice is the radial basis function ( RBF ) kernel , C ( ω , ω (cid:48) ) = σ C e − ( ω − ω (cid:48) ) 2 2 l 2 , ( 44 ) including tunable hyperparameters σ C for overall magnitude and l a length scale . Then this GP represented prior can be plugged into the Bayesian inference procedure with lat - tice data for the ghost dressing function and gluon propaga - tor for constructing the likelihood . Ref . [ 121 ] also speciﬁ - cally extended the lattice data , e . g . , for ghost they extended the dressing function into the deep IR and also constrain the low - frequency behavior by spectral DSE results [ 122 ] , and 18 0 5 10 0 . 0 0 . 5 1 . 0 ( ) = 10 3 0 5 10 0 . 0 0 . 5 1 . 0 = 10 4 0 5 10 0 . 0 0 . 5 1 . 0 = 10 5 0 5 10 0 . 00 0 . 25 0 . 50 0 . 75 ( ) 0 5 10 0 . 00 0 . 25 0 . 50 0 . 75 0 5 10 0 . 00 0 . 25 0 . 50 0 . 75 Ground Truth MEM NN NN - P2P Fig . 13 . The spectral functions reconstructed from MEM , NN and NN - P2P , under different Gaussian noises added to the propagator data with N p = 25 , and N ω = 500 for the spectral . Taken from Ref . [ 120 ] . for gluon it’s extended into the UV with previous fRG compu - tation results [ 123 ] . This reduces the variance in the solution space and enhanced the stability compared to the inference without such extensions . It’s shown that while approximately fulﬁlling the Oehme - Zimmermann superconvengence ( OZS ) condition for gluon , the reconstruction with GP regression in this work accurately reproduces the lattice data within the un - certainties with deviations for gluon propagator stronger in some region than for the ghost dressing function . For the spectral function , the reconstruction shows similar peak struc - ture to previous fRG reconstruction of the Yang - Mills propa - gator [ 123 ] . In Ref . [ 120 , 124 , 125 ] , the authors devised an unsuper - vised approach based on deep neural network ( DNN ) repre - sentation for the spectral function together with automatic dif - ferentiation ( AD ) to reconstruct the spectral , which does not need training data preparation for supervision ( Note that sim - ilar DNN based inverse problem solving strategy within AD framework is also used to reconstruct Neutron Star EoS from astrophysical observables [ 126 , 127 ] and inferring parton dis - tribution function of pion in lattice QCD studies [ 128 ] ) . The introduced DNN representation can preserve the smoothness of the spectral function automatically , thus naturally help to regularize the degeneracy issue in this inverse problem , be - cause it’s analyzed in Ref . [ 125 ] that the degeneracy is related to the null - modes of the investigated kernel function , which will usually induce oscillation for the reconstructed spectral function . Speciﬁcally , the DNN represented spectral , (cid:126)ρ = [ ρ 1 , ρ 2 , . . . , ρ N ω ] , can be converted into the propagator under the discretization scheme as D ( p ) = (cid:80) N ω i K ( p , ω i ) ρ i ∆ ω . Then the loss function over the propagator as compared to lattice data , L = (cid:80) N p i ( D i − D ( p i ) ) 2 / σ i , can be evaluated and provide guide for the tuning over DNN represented spec - tral . Taking gradient - based algorithms , the derivative of the loss with respect to network parameters can be derived as ∇ θ L = (cid:88) j , k K ( p j , ω k ) ∂ L ∂D ( p j ) ∇ θ ρ k . ( 45 ) with ∇ θ ρ k computed easily under standard backward propa - gation for the network . For the DNN representation of spectral , two different schemes were investigated in this work : one used the mul - tiple outputs of an L - layers neural network to represent in list format the spectral function ( denoted as NN ) , and the other directly use a feedforward neural network as parame - terization ( denoted as NN - P2P ) over the spectral as a func - tion of frequency , ρ ( ω ) . For the training , the Adam opti - mizer is adopted , and the L 2 regularization is set in the warm - up beginning stage under an annealing strategy until a small enough regularization strength value ( set as smaller than 10 − 8 in the calculation ) , this can relax the regularization to obtain a hyperparameter independent inference results . For the di - rect NN list representation , a quenched implementation of smoothness condition λ s (cid:80) N ω i = 1 ( ρ i − ρ i − 1 ) 2 is also performed with λ s decreased from 10 − 2 to 0 . Such newly devised unsu - pervised spectral reconstruction method got approved for the uniqueness of the solutions with manifestation both analyti - cally and numerically [ 125 ] . As Fig . 13 shows , on superposed Breit - Wigner peaks this method was demonstrated to outper - form the traditional MEM method , especially on multi - peaks spectra with large measurement noise situations . 19 0 5 10 0 . 0 0 . 5 ρ ε = 0 . 001 ground truth GaussianRBF 0 5 10 0 . 0 0 . 5 ε = 0 . 0001 ground truth GaussianRBF 0 5 10 0 . 0 0 . 5 ε = 0 . 00001 ground truth GaussianRBF 0 5 10 0 . 0 0 . 5 ρ ε = 0 . 001 ground truth TSVD 0 5 10 0 . 0 0 . 5 ε = 0 . 0001 ground truth TSVD 0 5 10 0 . 0 0 . 5 ε = 0 . 00001 ground truth TSVD 0 5 10 0 . 0 0 . 5 ρ ε = 0 . 001 ground truth Tikhonov 0 5 10 0 . 0 0 . 5 ε = 0 . 0001 ground truth Tikhonov 0 5 10 0 . 0 0 . 5 ε = 0 . 00001 ground truth Tikhonov 0 5 10 ω 0 . 0 0 . 5 ρ ε = 0 . 001 ground truth MEM 0 5 10 ω 0 . 0 0 . 5 ε = 0 . 0001 ground truth MEM 0 5 10 ω 0 . 0 0 . 5 ε = 0 . 00001 ground truth MEM Fig . 14 . The constructed spectral functions obtained from RBFN , TSVD , Tikhonov and MEM , using the correlation data generated by the mock SPF of mixing two Breit - Wigner distributions . For left to right panels , different Gaussian noises are added to the correlation data with (cid:15) = 0 . 001 , 0 . 0001 and 0 . 00001 [ 129 ] . Beside further Gaussian - like and Lorentian - like spec - tral reconstruction tests , the newly devised framework in Ref . [ 120 , 124 ] was also validated in two physics motivated tests , one is for non - positive deﬁnite spectral reconstruction , which is beyond classical MEM applicability scope but of - ten encountered for spectral functions related to conﬁnement phenomenon to e . g . , gluons and ghosts , or thermal excitations with long - range correlation in strongly coupled system ; the other one is for hadron spectral function encoded in temper - ature dependent thermal correlator with lattice QCD noise - level noises . On both of these two further physical cases this proposed DNN and automatic differentiation based method with NN representation works consistently well while tradi - tional MEM based methods loses the peak information or fail in confronting the non - positiveness . The spectral function can also be reconstructed from ﬁnite correlation data through implementing the radial basis func - tions networks ( RBFN ) , which is a multilayered perceptron model based on the radial basis functions ( RBF ) [ 133 , 134 ] . RBFN has been widely used in feature extraction , classiﬁ - cation , regression and so on [ 135 – 138 ] . In Ref . [ 129 ] , the spectral function ρ ( ω ) was ﬁrst approximately described by a linear combination of RBFs : ρ ( ω ) = N (cid:88) j = 1 w j φ ( ω − m j ) , ( 46 ) where φ is the active RBF with an adjustable weight w j and an adjustable center m j , which can take a Gaussian form φ ( r ) = e − r 2 2 a 2 or a MQ form φ ( r ) = ( r 2 + a 2 ) 12 , etc . Here , a is the shape parameter , which is adjustable and essential for the regularization . Then , the inverse mapping problem of 20 T ( MeV ) M ( T ) - M ( 0 ) ( M e V ) - 100 0 100 200 3S - 100 - 50050100 2P - 100 - 50 0 2S - 60 - 40 - 200 1P 150 200 250 300 - 30 - 20 - 10010 1S box : Lattice open : HTL solid : DNN T ( MeV ) Γ ( M e V ) 0 200 400 600 nS 150 200 250 300 0 200 400 nP - 6 - 5 - 4 - 3 - 2 - 1 V R ( G e V ) 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 0 . 0 0 . 5 1 . 0 1 . 5 r ( fm ) - V I ( G e V ) 151173199251334 MeV 0 Fig . 15 . Piciture taken from Ref . [ 130 ] . Left and Middle : In - medium mass shifts with respect to the vacuum mass ( left ) and the thermal widths ( right ) of different bottomonium states obtained from ﬁts to LQCD results of Ref . [ 131 ] ( lines and shaded bands ) using weak - coupling motivated functional forms [ 132 ] ( open symbols ) and DNN based optimization ( solid symbols ) . The points are shifted horizontally for better visualization . Υ ( 1 S ) , χ b 0 ( 1 P ) , Υ ( 2 S ) , χ b 0 ( 2 P ) and Υ ( 3 S ) states are represented by red circles , orange pluses , green squares , blue crosses and purple diamonds , respectively . Right : The DNN reconstructed real ( top ) and imaginary ( bottom ) parts of the heavy quark potential at temperatures T = 0 ( black ) , 151 ( purple ) , 173 ( blue ) , 199 ( green ) , 251 ( orange ) , and 334 MeV ( red ) . The uncertainty bands represent the 68 % ( 1 σ ) conﬁdent region . constructing the spectral function is transformed into calcu - lating the linear weights of RBF , which naturally enables a smooth and continuous reconstruction . To calculate these parameters in Eq . ( 46 ) , Ref . [ 129 ] con - structed a neutral network called radial basis function net - works ( RBFN ) , which is a three - layers feed - forward neural network with the active RBFs built in the hidden layer . Af - ter discrete the spectral function , Eq . ( 46 ) is transformed into a matrix form [ ρ ] = [ Φ ] [ W ] . Then , the correlation func - tions in Euclidean space with the integral spectral represen - tation G ( τ , T ) = (cid:82) ∞ 0 dω 2 π ρ ( ω , T ) K ( ω , τ , T ) becomes a ma - trix form : G i = M (cid:88) j = 1 N (cid:88) k = 1 K ij Φ jk w k ≡ M (cid:88) k = 1 ˜ K ik w k , i = 1 . . . (cid:98) N ( 47 ) where ˜ K is a (cid:98) N × M matrix associated with the integration kernel , (cid:98) N is the number of data points for the correlation function G i . Note that the spectral function ρ ( ω i ) has been discretized into N parts with m i = ω i , i = 1 . . . N and M is set to M = N = 500 . To obtain w j , one could further im - plement the TSVD method or a deep neural network [ 129 ] . Compared with other machine learning approach based on the supervised learning [ 119 , 139 , 140 ] , this method can be rapidly trained and free from the over - ﬁtting problem . Fig . 14 shows a comparison of the constructed spectral functions from RBFN , TSVD , Tikhonov and MEM , using the correlation data generated by a mock SPF . Such mock SPF is obtained by mixing two Breit - Wigner distributions : ρ Mock ( ω ) = ρ BW ( A 1 , Γ 1 , M 1 , ω ) + ρ BW ( A 2 , Γ 2 , M 2 , ω ) with ρ BW ( A i , Γ i , M i , ω ) = 4 A i Γ i ω ( M 2 i + Γ 2 i − ω 2 ) 2 + 4Γ 2 i ω 2 . The pa - rameters for the mock SPF in Fig . 14 are set to A 1 = 0 . 8 , M 1 = 2 , Γ 1 = 0 . 5 ; A 2 = 1 , M 2 = 5 , Γ 2 = 0 . 5 . Here , one generates 30 discrete correlation data using the Euclidean correlation functions of the mock SPF , together with a noise added , G noise ( τ i ) = G ( τ i ) + noise . Compared with the results from traditional methods , RBFN provides a better description of the spectra functions , especially for the low frequency part . It also almost repro - duces the ﬁrst peak of the mock SPF using the correlation data with smaller noise (cid:15) = 0 . 00001 . In contrast , Tikhonov , TSVD and MEM present some oscillation behavior at low frequency . For such a task of extracting the transport coef - ﬁcients from the Kubo relation , an improved reconstruction of the spectra functions at low frequency is especially impor - tant . Although RBFN fails to reconstruct the second peak of the mock SPF , it is the only method that reduces the oscilla - tion at the low frequency , compared with the three commonly used ones . Ref . [ 129 ] also compared the Gaussian and MQ RBF used in the network and found that the Gaussian RBF gives better construction of the SPF , including the location and the width of the peak . With the mock data generated from the spectral function of energy momentum tensor , Ref . [ 129 ] also demonstrated that the RBFN method gives a precise and stable extraction of the transport coefﬁcients . in - medium heavy quark potential As an important probe 21 for the properties of the created QGP in heavy ion collisions , heavy quarkonium ( the bound state of heavy quark and its anti - quark ) is intensively measured in experiments and stud - ies theoretically [ 141 , 142 ] , the investigation and calcula - tion to which requires understanding to the in - medium heavy quark interaction . Basically , the heavy quarkonium provide a calibrated QCD force , since in vacuum the simple Cor - nell potential can reproduce well the spectroscopy of heavy quarkonium , and when we put the bound state into QCD medium the color screening effects would naturally happen and weaken the interactions between the heavy quarks , be - yond which , a non - vanishing imaginary part manifested as thermal width is argued to show up according to both the one - loop hard thermal loop ( HTL ) perturbative QCD calcu - lations [ 143 , 144 ] and the recent effective ﬁeld theory stud - ies e . g . , pNRQCD [ 145 , 146 ] . However , the non perturbative treatment like lattice QCD is necessary because it’s difﬁcult to get satisfactory description of the strong interaction dic - tated in - medium heavy quarkonium solely from perturbative calculations . From those EFTs studies , it’s also shown that a potential based picture can provide good approximation to the quarkonium , under which the Schrödinger - type equation can be employed to study the spectroscopy of the bound state . Re - cently , the lattice QCD studies released quantiﬁcation of the in - medium spectrum – mass shift and thermal widths of Bot - tomonium ( b ¯ b ) up to 3S and 2P states in QGP [ 131 ] , which was found can not be reproduced by the one - loop HTL mo - tivated functional form of the heavy quark in - medium poten - tial , V R ( T , r ) and V I ( T , r ) . Note that the mass shift may give inﬂuence to quarkonium production in HICs [ 147 ] . In Ref . [ 130 ] the authors devised a model - independent DNN - based method to reconstruct the temperature and inter - quark distance dependent in - medium heavy quark potential based upon the lattce QCD results mentioned above for Bot - tomonium . Since the universal approximation theorem , the DNN is introduced to parameterize the potential in an un - biased yet ﬂexible enough fashion . The DNN represented heavy quark potential is coupled to the Schrödinger equa - tion solving process to be converted into complex valued en - ergy eigenvalues E n , which are related to the bound state in - medium mass and thermal width through Re [ E n ] = m n − 2 m b and Im [ E n ] = − Γ n . By comparing to the lattice QCD “measurements” , the corresponding χ 2 provide the loss func - tion for optimizing the parameters of the potential - DNN , L = 1 2 (cid:88) T , n ( m T , n − m LQCDT , n δm LQCDT , n ) 2 + ( Γ T , n − Γ LQCDT , n δ Γ LQCDT , n ) 2 , ( 48 ) with T ∈ { 0 , 151 , 173 , 199 , 251 , 334 } MeV and n ∈ { 1 S , 2 S , 3 S , 1 P , 2 P } according to the lattice QCD evalua - tion conditions . Gradient descent with back propagation tech - niques can be applied for the DNN optimization here , of which the gradient is estimated efﬁciently from perturbative analysis on the Schrödinger equation with respect to perturba - tive change of the potential and just arrived in the Hellman - Feynman theorem . Furthermore , the uncertainty of the re - constructed potential is quantiﬁed by invoking Bayesian in - ference , thus evaluating the posterior distribution of the DNN parameters . With the outlined approach , Ref . [ 130 ] achieved nice agreement with the lattice QCD results on masses and thermal widths of Bottomonium simultaneously , see left and middle panel of Fig . 15 . Meanwhile , the temperature and dis - tance dependent heavy quark potential is also obtained as dis - played in the right panel of Fig . 15 . Clearly the color screen - ing effect emerged for the reconstruction with ﬂatter structure appearing in V R ( T , r ) with increasing temperature at large distance , but the temperature dependence is found to be mild compared to perturbative analysis based results in the same temperature range considered . On the other hand , the imagi - nary part , V I ( T , r ) , shows signiﬁcant growth both with tem - perature and distance , and also shows greater magnitude than one - loop HTL motivated results . Deep Learning Quasi Particle Mass The equation of state of hadron resonance gas in the QCD phase diagram can be calculated using simple statistical formula with the following partition function , ln Z ( T ) = (cid:88) i ln Z hi ( T ) ( 49 ) where Z hi ( T ) is the partition function for one of the several hundred hadrons in HRG , assuming that there is no interac - tion between different hadrons . The calculated EoS agrees with lattice QCD calculations . It is not possible to get the lattice QCD EoS for QGP using the same formula , as quarks and gluons interact with each other and form a many body quantum system . However , if one assumes that the quarks and gluons are non - interacting quasi particles whose masses depend on the local temperature , it is able to reproduce Lat - tice QCD EoS using the following simple statistical formula , ln Z ( T ) = ln Z g ( T ) + (cid:88) i ln Z q i ( T ) ln Z g ( T ) = − d g V 2 π 2 (cid:90) ∞ 0 p 2 dp ln (cid:20) 1 − exp (cid:18) − 1 T (cid:113) p 2 + m 2 g ( T ) (cid:19)(cid:21) ln Z q i ( T ) = + d q i V 2 π 2 (cid:90) ∞ 0 p 2 dp ln (cid:20) 1 + exp (cid:18) − 1 T (cid:113) p 2 + m 2 q i ( T ) (cid:19)(cid:21) ( 50 ) where Z g is the partition function of quasi gluons , Z q i repre - sent the partition function of quasi quarks , d g and d q i are the spin and color degeneracy for gluons and quarks respectively , p is the magnitude of momentum , T is the local temperature . Gluons , up , down and strange quarks are taken into account in this calculation . It is assumed that the temperature quasi par - ticle masses m u / d ( T ) are the same for up and down quarks , but different for gluons m g ( T ) and strange quarks m s ( T ) . As a result , there will be 3 variational functions whose forms are unknown and to be determined by mapping the following 22 resulted EoS to Lattice QCD EoS , P ( T ) = T (cid:18) ∂ ln Z ( T ) ∂V (cid:19) T (cid:15) ( T ) = T 2 V (cid:18) ∂ ln Z ( T ) ∂T (cid:19) V . ( 51 ) Several deep residual neural networks are constructed to represent the variational functions m u / d ( T ) , m s ( T ) and m g ( T ) . The mass functions of these quasi partons are used in Eq . 50 to compute the partition function . The resulted par - tition function is further used in Eq . 51 to compute the pres - sure and energy density as a function of temperature . Notice that in this procedure there are both numerical integration and differentiation . The integration is implemented using Gauss quadrature in the language of tensorﬂow , while the differen - tiation is given by auto - differentiation . The loss function is designed as , loss = | s dnn − s lattice | 2 + | ∆ dnn − ∆ lattice | 2 + L constrain ( 52 ) where s = ( (cid:15) + P ) / T is the entropy density , ∆ = ( (cid:15) − 3 P ) / T 4 is the trace anomaly . The L constrain contains physical constraints at high temperature region whose the - oretical function form is given by HTL calculations . The learned quasi partons reproduce Lattice QCD EoS . Using these mass functions , the authors calculated η / s ( T ) and found that its minimum located around 1 . 25 T c [ 148 ] . VI . HARD PROBE Energetic partons lose energy as they pass through the hot quark gluon plasma . This process is quantiﬁed by the jet transport coefﬁcient ˆ q , which is deﬁned as the transverse momentum broadening squared per unit length [ 149 – 153 ] . The temperature dependent jet transport coefﬁcient for heavy quarks is extracted using Bayesian analysis , using the D - meson v 2 and R AA data from different experiments [ 154 ] . Bayesian inference extracted the jet energy loss distributions , showing that the observed jet quenching is dominated by only a few out - of - cone scatterings [ 155 ] . The JETSCAPE col - laboration extracted ˆ q with a multi - stage jet evolution ap - proach model [ 156 ] . These studies typically use parametrized forms for the unknown ˆ q ( T ) function . An information ﬁeld is proposed to provide non - parametric functions for global Bayesian inference in order to to avoid long - range correla - tions and human biases [ 157 , 158 ] . Deep learning has been widely used in high energy parti - cle physics to analyse the substructures of jets and to to clas - sify jets using the momentum of ﬁnal state hadrons in jets [ 159 , 160 ] . In heavy ion collisions , deep learning is used not only to classify quark and gluon jets , but also to study the jet energy loss , the medium response and the initial jet produc - tion positions [ 161 – 163 ] . Constraining the initial jet production positions will allow more detailed and differential studies of jet quenching . For example , one task in HIC is to search for Mach cones in QGP produced by the supersonic parton jet . The difﬁculty is that the jets are produced at different locations in the initial state and travel in different directions in the QGP . As a result , the shape of the Mach cone depends on the path length and is distorted by the local radial ﬂow and temperature gradient . Predicting jet production positions using deep learning will help to select jet events whose Mach cones have a similar shape , thus enhancing the signal of the Mach cones in the ﬁnal state hadron distribution . In these studies , the training data are usually generated by jet transport models [ 164 , 165 ] , e . g . , in the linear Boltzmann transport model ( LBT ) , the jet parton loses energy through elastic scattering with thermal partons in QGP and inelas - tic gluon radiation . This process is described by a linearised Boltzmann equation , shown below , p a · ∂f a = (cid:90) (cid:89) i = b , c , d d 3 p i 2 E i ( 2 π ) 3 γ b 2 ( f c f d − f a f b ) | M ab → cd | 2 ( 53 ) × S 2 ( ˆ s , ˆ t , ˆ u ) ( 2 π ) 4 δ 4 ( p a + p b − p c − p d ) + inelastic . ( 54 ) where f a / c are the distribution functions of the jet partons before and after scattering in the forward process , f b / d = 1 / (cid:104) e p · uT ± 1 (cid:105) are the Fermi - Dirac and Bose - Einstein distri - butions for thermal quarks and gluons , respectively , in QGP . On the right hand side , f c f d corresponds to the gain term and − f a f b to the loss term during elastic scattering , whose amplitude squares | M ab → cd | 2 from leading order perturba - tive QCD calculations . The γ b is the colour and spin de - generacy of the thermal parton b and the term ˆ S 2 = θ ( ˆ s > 2 µ 2 D ) θ ( − ˆ s + µ 2 D ≤ ˆ t ≤ − µ 2 D ) is used to regularise the collinear divergence . The inelastic part comes from the gluon radiation described by higher - twist calculations [ ] . The lost energy is deposited in QGP as source terms of the relativistic hydrodynamic equations , ∇ µ T µν = J ν ( 55 ) where T µν is the local energy momentum tensor of QGP and J ν is the source term . In practice , if the energy deposited on the recoiled thermal parton exceeds 2 GeV , it will be taken out and put into the LBT . This leaves a negative jet source in QGP . If the deposited energy is less than 2 GeV , this corre - sponds to a positive jet source . Recoiled partons in the LBT do not interact with each other , which explains why the LBT solves a linearised Boltzmann equation . Recently , the LBT has been extended to the QLBT , which treats quarks and glu - ons as quasi - partons to constrain various transport parameters [ 166 ] . The initial jet production positions are sampled from the distribution of hard scattering , which is proportional to the distribution of the number of binary collisions . The initial entropy density distribution is provided by the Trento Monte Carlo model , from which the initial T µν can be calculated . Simultaneously solving Eq . 54 and Eq . 55 provides both the 23 jet energy loss and the medium response in each simulation . Typically 10 ∼ 100 thousand jet events are required to pre - dict the initial jet production positions . Of course , the more training data the better , if there are sufﬁcient computing re - sources . One might ask whether there is a type of deep neural net - work that is better suited to studying jet energy loss and pre - dicting jet production positions . In practice , convolutional neural networks ( CNNs ) , point cloud neural networks , and graph neural networks have all been used in different projects . Typically , the performance of different neural network archi - tectures is tested on these candidates and the one that works best for the speciﬁc task is selected . The simplest yet most powerful CNN should be the ﬁrst to be tried in jet shape and jet energy loss studies . To capture the full information in jets , a point cloud network and a message passing neural network can be used . VII . OBSERVABLES IN HIC PCA for ﬂow analysis In relativistic heavy - ion collisions , the collective ﬂow provides important information about the properties of the QGP and its initial state ﬂuctuations [ 101 – 106 ] . The ﬂow observables are generally deﬁned by a Fourier decomposition of the produced particle distribution in mo - mentum space , such as : d N d ϕ = 1 2 π ∞ (cid:88) −∞ (cid:126)V n e − inϕ = 1 2 π ( 1 + 2 ∞ (cid:88) n = 1 v n e − in ( ϕ − Ψ n ) ) ( 56 ) where (cid:126)V n = v n e in Ψ n is the ﬂow - vector of order n , v n is ﬂow harmonics of order n and Ψ n is the corresponding event plane angle . The ﬂow coefﬁcients can also be obtained from the two - particle correlations associated with a Fourier decompo - sition : (cid:28) dN pairs dp 1 dp 2 (cid:29) ∝ 1 + 2 ∞ (cid:88) n = 1 V n ∆ ( p T1 , p T2 ) cos ( n ∆ φ ) ( 57 ) where V n ∆ ( p T1 , p T2 ) is a symmetric covariance matrix and ∆ φ = φ a − φ b is the relative azimuthal angle between two emitted particles . Under the assumption of ﬂow factoriza - tion , V n ∆ ( p T1 , p T2 ) is related to the ﬂow harmonics v n ( p T ) by : V n ∆ ( p T1 , p T2 ) ≈ v n ( p T1 ) v n ( p T2 ) [ 167 ] ( For other ﬂow methods or ﬂow measurements , see [ 103 , 106 , 168 – 170 ] ) . Recently , one of the machine learning techniques , called the Principal Component Analysis ( PCA ) based on the Sin - gular Value Decomposition ( SVD ) , has been implemented to study the collective ﬂow in relativistic heavy - ion colli - sions . For the 2 - particle correlations with the Fourier ex - pansion [ 172 – 175 ] , the event - by - event ﬂow ﬂuctuations have been investigated by PCA , which reveals the substructures of the ﬂow ﬂuctuations [ 172 – 174 ] . In more detail , with PCA , V n ∆ ( p T1 , p T2 ) can be expressed as [ 173 ] : V n ∆ ( p T1 , p T2 ) = (cid:80) α v ( α ) n ( p T1 ) v ( α ) n ( p T2 ) , ( 58 ) with (cid:82) dp T w 2 ( p T ) v ( α ) n ( p T ) v ( β ) n ( p T ) = λ α δ αβ ( 59 ) 2 0 2 0 . 2 0 . 0 0 . 2 d N / d z 1 / z 2 2 0 2 z 3 / z 4 2 0 2 z 5 / z 6 2 0 2 0 . 2 0 . 0 0 . 2 d N / d z 7 / z 8 2 0 2 z 9 / z 10 2 0 2 z 11 / z 12 Fig . 16 . The PCA eigenvectors z j for the ﬁnal state matrix of particle distributions , generated from VISH2 + 1 hydrodynamics in 2 . 76 A TeV Pb + Pb collisions at 10 % - 20 % centrality [ 171 ] . 1 5 10 15 20 j 0 . 0 0 . 1 0 . 2 j v 0 2 v 0 3 v 0 4 v 0 1 v 0 5 v 0 6 Fig . 17 . The singular values of PCA for the ﬁnal state matrix of particle distributions in Pb + Pb collisions at 10 - 20 % centrality [ 171 ] . where v ( α ) n ( p T ) are the eigenvectors of the two - particle co - variance matrix , w ( p T ) is the weight for the particle . α = 1 denotes the leading modes , α = 2 denotes the subleading modes and α = 3 denotes the subsubleading modes , and so on . It was found that the leading modes correspond to the traditional ﬂow harmonics and the sub - leading modes lead to the breakdown of the ﬂow factorization . Using hydrodynamic simulations , Ref [ 173 , 174 ] demonstrated a linear relationship V ( α ) n ∝ E ( α ) n for the leading , subleading and subsubleading modes . In Ref . [ 175 ] , PCA was implemented to study the mode coupling between ﬂow harmonics , which found some hidden mode - mixing patterns that had not been discovered before . Recently , the CMS collaboration extracted the sub - leading ﬂow modes for Pb + Pb and p + Pb collisions at the LHC , which showed a qualitative agreement between exper - imental measurements and the theoretical calculations [ 176 ] . Using AMPT and HIJING simulations , Ref . [ 177 ] showed that the PCA modes depend on the choice of the p T range and the particle weight w . In addition , the leading modes are affected by the non - ﬂow effects , and the mixing between the non - ﬂow and leading ﬂow modes leads to fake sublead - ing modes . Therefore , it is important to carefully handle with the non - ﬂow effects and the choice of weight and phase 24 0 . 0 0 . 1 v 2 0 . 0 0 . 1 v 0 2 ×1 0 . 0 0 . 1 v 3 v 0 3 ×2 0 . 0 0 . 1 v 4 v 0 4 ×4 0 . 0 0 . 1 v 5 v 0 5 ×10 0 . 0 0 . 1 v 6 v 0 6 ×15 Fig . 18 . A comparison between the event - by - event ﬂow harmonics v (cid:48) n from PCA and v n from the Fourier expansion in Pb + Pb collisions at 10 - 20 % centrality [ 171 ] . space when implementing PCA to extract the subleading ﬂow modes in both experimental and theoretical sides . These above PCA studies of collective ﬂow [ 172 – 177 ] are all based on the correlation data obtained with a Fourier ex - pansion . Recently , PCA has been applied directly to the sin - gle particle distributions dN / dϕ without any prior treatment with a Fourier transform , which aimed to explore whether PCA could directly discover ﬂow without the guidance from human - beings [ 171 ] . More speciﬁcally , with a PCA matrix multiplication , the i th row of a particle distribution matrix with N events , generated from VISH2 + 1 hydrodynamics , can be expressed as : dN / dϕ ( i ) = m (cid:88) j = 1 x ( i ) j σ j z j = m (cid:88) j = 1 ˜ v ( i ) j z j ≈ k (cid:88) j = 1 ˜ v ( i ) j z j ( i ) = 1 , . . . , N ( 60 ) Here , ( i ) = 1 , 2 , . . . , N is the index of the event . j is the in - dex for the azimuthal angle where the total azimuthal angle [ − π , π ] is divided into m bins in order to count the number of particles in each bin . After the Singular Value Decomposi - tion ( SVD ) , dN / dϕ ( i ) can be expressed by a linear combina - tion of the eigenvectors z j with the corresponding coefﬁcient ˜ v ( i ) j ( where j = 1 , 2 , . . . , m ) , and σ i is the diagonal elements ( singular values ) of the particle distribution matrix , which is arranged in a descending order . In the spirit of PCA , in the last step , a cut is made at the indices k to focus only on the most important components . Fig . 16 and Fig . 17 show the ﬁrst 12 eigenvectors z j and the ﬁrst 20 singular values σ j of the PCA in descending order for the ﬁnal state matrix constructed from 2000 dN / dϕ distributions with the azimuthal angle [ − π , π ] equally di - vided into 50 bins . Such dN / dϕ distributions are generated from the VISH2 + 1 hydrodynamics with event - by - event ﬂuctuating TRENTo initial conditions for 2 . 76 A TeV Pb + Pb collisions at 10 % - 20 % centrality . Fig . 16 shows that the PCA eigenvectors are similar to the traditional Fourier bases . For example , the 1 st and 2 nd eigenvectors are close to sin ( 2 ϕ ) and cos ( 2 ϕ ) and the 3 rd and 4 th eigenvectors are close to sin ( 3 ϕ ) and cos ( 3 ϕ ) . The corresponding singular values in Fig . 17 are also arranged in pairs , which correspond to the real and imaginary parts of the anisotropic ﬂow . It was found that , for n ≤ 6 , the values of these PCA ﬂow harmonics are very close to those of the traditional event averaged ﬂow harmonics obtained from the Fourier expansion , but not exactly the same . Fig . 18 compares the event - by - event ﬂow harmonics obtained from PCA and from the traditional Fourier expansion . It shows that the elliptic ﬂow with n = 2 and the triangular ﬂow with n = 3 from both methods are in good agreement . However , for higher ﬂow harmonics with n ≥ 4 , the PCA and Fourier expansion ones differ signiﬁ - cantly from each other due to the mode mixing effects . With these PCA ﬂow harmonics v (cid:48) n , Ref . [ 171 ] also calculates the symmetric cumulants SC (cid:48) ( m , n ) . Except for SC (cid:48) ( 2 , 3 ) , these PCA symmetric cumulants largely decrease compared with the traditional Fourier ones , due to the largely increased linearity between the PCA ﬂow harmonics and the initial eccentricities . These results indicate that PCA could deﬁne the collective ﬂow on its own basis . Compared with the traditional ones obtained from the Fourier decomposition , the PCA method reduces the mode coupling effects between different ﬂow harmonics [ 171 ] . CME detection In the presence of magnetic ﬁeld , chiral magnetic effect ( CME ) phenomenon can occur when the sys - tem shows chiral imbalance thus the number of left - handed and right - handed particles differs . Basically , a current of elec - tric charge ( known as chiral magnetic current ) can be induced to ﬂow along the direction of the magnetic ﬁeld . It’s been proposed to use Chiral magnetic effect to reveal the vacuum structure of QCD . In heavy ion collisions , a strong magnetic ﬁeld can be created from the motion of the colliding ions , also it’s predicted that in the formed hot and dense QGP the topo - logical ﬂuctuations of gluon ﬁelds may cause chiral imbal - ance for quarks , accordingly the CME may take place which can manifest as a separation of electric charge along the mag - netic ﬁeld direction . However , several challenges hinder the detection of CME in heavy ion collisions , among which the chief difﬁculty is to disentangle the CME signal from other possible sources of charge separation e . g . , elliptic ﬂow , the global polarization and other background noises though mul - tiple observables are proposed . Despite the challenges , there’s a long - term and continu - ing interest for the search for CME in heavy ion collisions 25 since its general importance to QCD . Recently in Ref . [ 178 ] it’s proposed to use deep learning to construct an end - to - end CME - meter , which can efﬁciently analyze the ﬁnal - state hadronic spectrum as a whole in the sense of big data with deep convolutional neural network to disclose the ﬁnger - prints of CME . In performing supervised learning , the train - ing set is prepared from the string melting a multiphase trans - port ( AMPT ) model with CME implemented under a global charge separation ( CS ) scheme . Basically , the CME events are generative by switching the y - components of momenta of a fraction of downward moving light quark , and it’s cor - responding anti - quarks with upward moving direction . The fraction deﬁnes the CS fraction , f which separates the events to “no CS” ( label as “0” ) class for those with f = 0 % and “CS” class ( label as “1” ) for those with f > 0 % . Each event is represented as two - dimensional transverse momentum and azimuthal angle spectra of charged pions in the ﬁnal state , ρ π ( p T , φ ) . Then deep CNN is trained to perform binary clas - siﬁcation on the labeled events with the spectra to be the in - put . See Fig . 19 about the architecture of the devised deep CNN for CME - meter construction . Fig . 19 . Taken from [ 178 ] . The convolutional neural network archi - tecture with π + and π − spectra ρ ± ( p T , φ ) as input . As seen from Fig . 19 the output of the network has two nodes , with each of them being naturally interpreted as the probability resulting from the network decision in recogniz - ing any given input spectrum as CME ( P 1 ) or non - CME ( P 0 = 1 − P 1 ) events . The training set contains multiple collision beam energies and centralities for diversity consid - eration . The pion spectra is obtained by averaging over 100 events with the same collision condition to reduce the ﬂuctu - ations , which also reduces the backgrounds and thus should be considered as prerequisite for realistic application in ex - periment . For the training , different levels of CS fraction is used , and it’s found that the classﬁction validation accuracy for using smaller CS fraction training events is less than the model trained with higher CS fraction events . This shows that larger CS fraction can be identiﬁed easier , which is natural . Despite the different induced discernibility , the trained deep CNNs all showed robust performance against varying colli - sion centrality and energy . One can conclude that at least un - der the AMPT modelling level the CS signals can survive into the ﬁnal state of the collision dynamics at different collision conditions , which can be recognized by the deep CNN - based CME - meter . Table 1 . The results of the ( 0 % + 10 % ) model on the isobaric collision systems ( Ru + Ru and Zr + Zr at 200 GeV ) . Centrality 0 - 10 % 10 - 20 % 20 - 30 % 30 - 40 % 40 - 50 % 50 - 60 % R iso 9 . 95 % 12 . 99 % 8 . 13 % 13 . 84 % 19 . 67 % 10 . 47 % Note that the network is trained just on Au + Au collision systems , while the extrapolation to other collision system is showed to work successfully . Speciﬁcally , the obtained CME - meter is applied to isobaric collisions of 9640 Zr + 9640 Zr and 9644 Ru + 9644 Ru , which is proposed for the search of CME es - pecially . Since the Ru contains more protons to induce a larger magnetic ﬁeld than Zr , it’s expected that there would be a larger CS signal in Ru + Ru collisions . To reveal this dif - ference and also the distinguishable difference of the CME - meter on the two isobaric collision systems from P Ru 1 > P Zr 1 , the R iso is evaluated which well verify the developed CNN based CME - meter , R iso = 2 × (cid:104) logit ( P Ru 1 ) (cid:105) − (cid:104) logit ( P Zr 1 ) (cid:105) (cid:104) logit ( P Ru 1 ) (cid:105) + (cid:104) logit ( P Zr 1 ) (cid:105) , ( 61 ) where the function logit ( x ) = log [ x / ( 1 − x ) ] is used to re - store the derivative at saturation region of the activation in NN last layer , SoftMax . From Tab . 1 on R iso it’s seen that the trained CME - meter gets well veriﬁed beyond the training collision system , which indicate its robust capture of general CME signal in the collisions . The CME - meter is also validated on a different model simulation , anomalous - viscous ﬂuid dynamics ( AVFD ) . P 1 shows consistent positive correlation along with increasing N 5 / S which controls the CME strength , while the contam - ination from local charge conservation ( LCC ) up to 30 % didn’t augment the performance of the CME - meter on the testing events from AVFD . To reveal the underlying account for the trained CME - meter , in Ref . [ 178 ] the comparison be - tween network output P 1 and γ - correlator is also investigated , where the γ - correlator as conventional CME probe can mea - sure the event - by - event two - particle azimuthal correlation of charged hadrons . It’s shown that on averaged events , both the CME signal and the background from δγ ( difference be - tween correlations within same charged particles and correla - tion within opposite charged particles ) get suppressed . Being differently , the CME - meter output P 1 works well in classify - ing CS and no - CS classes on the averaged events . The direct implementation of this trained CME - meter into real experiments would require ﬁrstly reconstructing the reac - tion plane of each collision event to form the averaged events as input for the meter . In general the reaction plane recon - struction can be reached by measuring correlations of ﬁnal state particles which inevitably contains ﬁnite resolution and 26 background effects . It’s shown that even on restricted event plane reconstruction , the trained CME - meter still can recog - nize the CS signals . To render the deployment of the trained CME - meter on single event measurements , Ref . [ 178 ] also proposed a hypothesis test perspective . Fig . 20 . DeepDream map for the ( 0 % + 10 % ) model [ 178 ] . Another way to interpret the trained deep learning al - gorithm is the DeepDream method , which was used in Ref . [ 178 ] to reconstruct the network most responding in - put pion spectrum , manifesting the “CME pattern” that the CNN - based CME - meter essentially captured for its further CME signal recognition . The key idea is to perform varia - tional tuning on the input pion spectrum with the trained and frozen network to maximize its output ( i . e . , pushing P 1 → 1 ) , driven by the gradient δP 1 ( ρ π ( p T , φ ) ) / δρ π ( ∂ T , φ ) . The re - sultant “CME pattern” from the trained network is displayed in Fig . 20 , from which the charge conservation and a clear dipole structure appears , both being CME related features . VIII . SUMMARY AND OUTLOOK Summary As a modern computational paradigm , artiﬁcial intelligence ( AI ) especially the machine - and deep - learning techniques are nowadays opening up a wealth of applications and new possibilities to the forefront of scientiﬁc studies . Due to its special ability in recognizing patterns and structures hid - den in complex data , these learning algorithms based strate - gies make it feasible for physics exploration with big data or a smart computation mindset . In the context of high energy nu - clear physics revolving around heavy ion collision programs to understand nuclear matter properties under different condi - tions , different research topics are beneﬁting as well from the incorporation of these techniques . We present the recent progresses in the area of heavy ion collisions in this mini - review article , including initial states physics inference , QCD matter transport and bulk properties study , thermal medium modiﬁcations for parton or hadrons as well as physical observables recognition in HICs . For methodology , we ﬁrst overview different loss functions l used in supervised learning , un - supervised learning , semi - supervised learning , self - supervised learning and active learn - ing . During training , the negative gradients − ∂l∂θ are used to optimize the network in SGD - like algorithms . Auto differen - tiation is employed to compute the derivatives of the loss with regard to model parameters θ , efﬁciently . As auto - diff has an - alytical precision for the variational function represented by the neural network , it has been widely used in physics in - formed neural network to solve ODEs and PDEs . We then in - troduced the widely used neural network architectures , such as MLP , CNN , RNN and Point Cloud Network . Afterward , we explained the generative models such as Auto Encoder , GAN , ﬂow model and diffusion models in details . These models are widely used in Lattice QCD to generate ﬁeld con - ﬁgurations . For the initial condition , machine learning has been widely used to determine the centrality classes ( impact parameter ) using the ﬁnal state hadrons in momentum space , to extract the initial nuclear structures such as the nuclear deformation , the α clustering as well as the neutron skin . In general , it seems easier to extract the nuclear deformation than the α clustering and neutron skin in current literature . For bulk matter , Bayesian parameter estimation has been successfully used to determine the temperature dependent shear and bulk viscosity of QGP . Unsupervised auto encoder is used to reconstruct the charged multiplicity distributions , which helps to determine the source temperature and the tem - perature of nuclear liquid gas phase transition . Deep CNN , point cloud network and event averaging techniques are em - ployed to classify the crossover and ﬁrst order phase tran - sition regions in QCD phase diagram , using data generated with relativistic hydrodynamic models and hadronic transport models . Active learning is used to mapping out thermody - namically unstable regions near the critical endpoint in the QCD phase diagram . For hydrodynamic evolution , a well - designed network , called sUnet , could capture the non - linear mapping between initial and ﬁnal proﬁles with sufﬁcient pre - cision , which is also much faster than the traditional hydro - dynamic simulations . For QGP in - medium effects , we ﬁrst reported some of the recent machine learning based spectral function reconstruc - tion , which is a notorious ill - posed inverse problem . Both supervised and unsupervised methods have been discussed for the inference of spectral out of Euclidean correlator mea - surements from Monte Carlo simulation ( e . g . , lattice study ) . Then the in - medium heavy quark interaction inference based upon in - medium heavy quarkonium spectroscopy is intro - duced , where a novel DNN representation integrated inside the forward problem - solving pipeline with automatic differ - entiation approach is proposed . This strategy is also used then for in - medium quasi particle effective model construc - tion from the lattice QCD EoS . For hard probes , Bayesian analysis is widely used to extract the temperature dependent jet ( or heavy quark ) transport co - efﬁcient ˆ q ( T ) and the jet energy loss distributions . Recently , deep learning assisted jet tomography is developed to locate the initial jet production positions . This is important for the study of jet substructures and the medium response . Using this technique , it was observed that the signal of jet induced 27 Mach cones is ampliﬁed by selecting jet events . For the observables , the Principal Component Analysis ( PCA ) has been implemented to study the collective ﬂow in relativistic heavy ion collisions . It revealed the substruc - tures of the ﬂow ﬂuctuations , which can potentially be im - plemented to extract the subleading modes of ﬂow with the efforts from both experimental and theoretical sides . After applied directly to the single particle distributions , PCA could directly discover ﬂow with a basis similar to the Fourier ex - pansion ones , which also greatly reduces the mode coupling between different ﬂow harmonics . Outlook In spite of many impressive progresses , the inter - play between high energy nuclear physics and machine learn - ing is still inducing hectic evolution . Yet , many questions and challenges exist and deserve further exploration . Besides the previously mentioned applications of ML into heavy ion collisions , several foreseeable topics could be explored with ML paradigm as well , e . g . , for critical end point searching , in eRHIC and EIC regime [ 179 ] , spin polarization study , the upcoming FAIR program , nuclear structure inference , HIAF experiment in China , etc . About the future prospects on this ﬁeld of applying machine learning techniques for heavy ion collision physics study , since it’s fast evolving and still un - der developing , we here throw out several questions those we think deserve future efforts , to get the ball rolling to advance our ﬁeld : • Can ML deﬁne us more efﬁcient " observables” to pin down the desired physics ? • Could the algorithm dig out new physical knowledge to advance our understanding of nuclear matter from the data ? • How to make those ML algorithms to be confronted with realistic experiment ? if it’s possible to do on - line analysis ? How to access experimental raw data to test neural network pretrained with model simulations ? • If it’s possible to speed up heavy ion collision dynam - ical simulations for confronting high statistic measure - ments or in Bayesian inference ? • How to combine Bayesian inference strategy with ma - chine learning to advance our ﬁeld and better connect experiment to theory ? • How we fully take symmetries into the analysis us - ing ML ? e . g . , Lorentz Group Equivariant Autoencoders [ 180 ] , other symmetries ? How would dimensionality analysis ( constraints ) be incorporated properly inside the ML methods consistently ? It’s also worthy to think about how we expand more po - tentially useful approaches from other ﬁelds’ development , e . g . , from particle physics , condensed matter physics , astro - physics , or even other scientiﬁc disciplines , and how the com - munity to better organize with joint efforts , like taking deeper look into the direction and maximize the potential of these novel computational techniques to really advance the ﬁeld of HENP . ACKNOWLEDGMENTS This work was supported in part by the National Nat - ural Science Foundation of China under contract Nos . 11890714 and 12147101 ( Ma ) , 12075098 ( Pang ) , 12247107 and 12075007 ( Song ) , and the Germany BMBF under the ErUM - Data project ( Zhou ) , and Guangdong Major Project of Basic and Applied Basic Research No . 2020B0301030008 ( Ma ) . [ 1 ] L . Benato et al . , Comput . Softw . Big Sci . 6 , 9 ( 2022 ) , arXiv : 2107 . 00656 [ cs . LG ] . [ 2 ] M . Favoni , A . Ipp , D . I . Müller , and D . Schuh , Phys . Rev . Lett . 128 , 032003 ( 2022 ) , arXiv : 2012 . 12901 [ hep - lat ] . [ 3 ] Z . - P . Gao , Y . - J . Wang , H . - L . Lü , Q . - F . Li , C . - W . Shen , and L . Liu , Nucl . Sci . Tech . 32 , 109 ( 2021 ) . [ 4 ] C . Xie , K . Ni , K . Han , and S . Wang , Science China Physics , Mechanics & Astronomy 64 , 261011 ( 2021 ) . [ 5 ] X . - C . Ming , H . - F . Zhang , R . - R . Xu , X . - D . Sun , Y . Tian , and Z . - G . Ge , Nucl . Sci . Tech . 33 , 48 ( 2022 ) . [ 6 ] X . Wu , Y . Lu , and P . Zhao , Physics Letters B 834 , 137394 ( 2022 ) . [ 7 ] X . - Z . Li , Q . - X . Zhang , H . - Y . Tan , Z . - Q . Cheng , L . - Q . Ge , G . - Q . Zeng , and W . - C . Lai , Nucl . Sci . Tech . 32 , 143 ( 2021 ) . [ 8 ] Z . Gao , Y . Wang , Q . Li , and L . Liu , Science China Physics , Mechanics & Astronomy 52 , 252010 ( 2022 ) . [ 9 ] P . Li , J . Bai , Z . Niu , and Y . Niu , Science China Physics , Me - chanics & Astronomy 52 , 252006 ( 2022 ) . [ 10 ] W . - B . He , Q . - F . Li , Y . - G . Ma , Z . - M . Niu , J . - C . Pei , and Y . - X . Zhang , arXiv : 2301 . 06396 [ nucl - th ] . [ 11 ] L . Ng , L . Bibrzycki , J . Nys , C . Fernandez - Ramirez , A . Pil - loni , V . Mathieu , A . J . Rasmusson , and A . P . Szczepaniak ( Joint Physics Analysis Center , JPAC ) , Phys . Rev . D 105 , L091501 ( 2022 ) , arXiv : 2110 . 13742 [ hep - ph ] . [ 12 ] Z . Zhang , R . Ma , J . Hu , and Q . Wang , Chin . Phys . Lett . 39 , 111201 ( 2022 ) . [ 13 ] K . Desai , B . Nachman , and J . Thaler , Phys . Rev . D 105 , 096031 ( 2022 ) . [ 14 ] G . Goh , Distill ( 2017 ) , 10 . 23915 / distill . 00006 . [ 15 ] D . P . Kingma and J . Ba , CoRR abs / 1412 . 6980 ( 2014 ) . [ 16 ] J . Steinheimer , L . Pang , K . Zhou , V . Koch , J . Randrup , and H . Stoecker , JHEP 12 , 122 ( 2019 ) , arXiv : 1906 . 06562 [ nucl - th ] . [ 17 ] M . Omana Kuttan , J . Steinheimer , K . Zhou , A . Redelbach , and H . Stoecker , Particles 4 , 47 ( 2021 ) . [ 18 ] Y . - G . Huang , L . - G . Pang , X . Luo , and X . - N . Wang , ( 2021 ) , arXiv : 2107 . 11828 [ nucl - th ] . [ 19 ] D . P . Kingma and M . Welling , in 2nd International Confer - ence on Learning Representations , ICLR 2014 , Banff , AB , Canada , April 14 - 16 , 2014 , Conference Track Proceedings ( 2014 ) http : / / arxiv . org / abs / 1312 . 6114v10 . [ 20 ] I . Goodfellow , J . Pouget - Abadie , M . Mirza , B . Xu , D . Warde - 28 Farley , S . Ozair , A . Courville , and Y . Bengio , in Advances in Neural Information Processing Systems , Vol . 27 , edited by Z . Ghahramani , M . Welling , C . Cortes , N . Lawrence , and K . Weinberger ( Curran Associates , Inc . , 2014 ) . [ 21 ] L . Mosser , O . Dubrule , and M . J . Blunt , Phys . Rev . E 96 , 043309 ( 2017 ) , arXiv : 1704 . 03225 [ cs . CV ] . [ 22 ] K . Mills and I . Tamblyn , Phys . Rev . E 97 , 032119 ( 2018 ) , arXiv : 1706 . 09779 [ cond - mat . mtrl - sci ] . [ 23 ] L . de Oliveira , M . Paganini , and B . Nachman , Comput . Softw . Big Sci . 1 , 4 ( 2017 ) , arXiv : 1701 . 05927 [ stat . ML ] . [ 24 ] M . Paganini , L . de Oliveira , and B . Nachman , Phys . Rev . Lett . 120 , 042003 ( 2018 ) , arXiv : 1705 . 02355 [ hep - ex ] . [ 25 ] S . Ravanbakhsh , F . Lanusse , R . Mandelbaum , J . Schneider , and B . Poczos , ( 2016 ) , arXiv : 1609 . 05796 [ astro - ph . IM ] . [ 26 ] M . Mustafa , D . Bard , W . Bhimji , Z . Luki´c , R . Al - Rfou , and J . M . Kratochvil , Computational Astrophysics and Cosmol - ogy 6 , 1 ( 2019 ) , arXiv : 1706 . 02390 [ astro - ph . IM ] . [ 27 ] K . Zhou , G . Endr˝odi , L . - G . Pang , and H . Stöcker , Phys . Rev . D 100 , 011501 ( 2019 ) , arXiv : 1810 . 12879 [ hep - lat ] . [ 28 ] J . M . Pawlowski and J . M . Urban , Mach . Learn . Sci . Tech . 1 , 045011 ( 2020 ) , arXiv : 1811 . 03533 [ hep - lat ] . [ 29 ] M . Germain , K . Gregor , I . Murray , and H . Larochelle , arXiv e - prints , arXiv : 1502 . 03509 ( 2015 ) , arXiv : 1502 . 03509 [ cs . LG ] . [ 30 ] A . v . d . Oord , N . Kalchbrenner , O . Vinyals , L . Espeholt , A . Graves , and K . Kavukcuoglu , in Proceedings of the 30th International Conference on Neural Information Processing Systems , NIPS’16 ( Curran Associates Inc . , Red Hook , NY , USA , 2016 ) p . 4797 – 4805 . [ 31 ] A . Van Den Oord , N . Kalchbrenner , and K . Kavukcuoglu , in Proceedings of the 33rd International Conference on In - ternational Conference on Machine Learning - Volume 48 , ICML’16 ( JMLR . org , 2016 ) p . 1747 – 1756 . [ 32 ] A . v . d . Oord , S . Dieleman , H . Zen , K . Simonyan , O . Vinyals , A . Graves , N . Kalchbrenner , A . Senior , and K . Kavukcuoglu , “Wavenet : A generative model for raw audio , ” ( 2016 ) , cite arxiv : 1609 . 03499 . [ 33 ] L . Wang , Y . Jiang , L . He , and K . Zhou , Chin . Phys . Lett . 39 , 120502 ( 2022 ) , arXiv : 2005 . 04857 [ cond - mat . dis - nn ] . [ 34 ] L . Dinh , D . Krueger , and Y . Bengio , arXiv e - prints , arXiv : 1410 . 8516 ( 2014 ) , arXiv : 1410 . 8516 [ cs . LG ] . [ 35 ] D . Jimenez Rezende and S . Mohamed , arXiv e - prints , arXiv : 1505 . 05770 ( 2015 ) , arXiv : 1505 . 05770 [ stat . ML ] . [ 36 ] L . Dinh , J . Sohl - Dickstein , and S . Bengio , arXiv e - prints , arXiv : 1605 . 08803 ( 2016 ) , arXiv : 1605 . 08803 [ cs . LG ] . [ 37 ] M . S . Albergo , G . Kanwar , and P . E . Shanahan , Phys . Rev . D 100 , 034515 ( 2019 ) , arXiv : 1904 . 12072 [ hep - lat ] . [ 38 ] G . Kanwar , M . S . Albergo , D . Boyda , K . Cranmer , D . C . Hackett , S . Racanière , D . J . Rezende , and P . E . Shanahan , Phys . Rev . Lett . 125 , 121601 ( 2020 ) , arXiv : 2003 . 06413 [ hep - lat ] . [ 39 ] D . Boyda , G . Kanwar , S . Racanière , D . J . Rezende , M . S . Albergo , K . Cranmer , D . C . Hackett , and P . E . Shanahan , Phys . Rev . D 103 , 074504 ( 2021 ) , arXiv : 2008 . 05456 [ hep - lat ] . [ 40 ] S . Chen , O . Savchuk , S . Zheng , B . Chen , H . Stoecker , L . Wang , and K . Zhou , ( 2022 ) , arXiv : 2211 . 03470 [ hep - lat ] . [ 41 ] J . Shlens , CoRR abs / 1404 . 1100 ( 2014 ) . [ 42 ] Y . - G . Ma and S . Zhang , In : Tanihata , I . , Toki , H . , Kajino , T . ( eds ) Handbook of Nuclear Physics . Springer , Singapore . , 1 . [ 43 ] P . Xiang , Y . - S . Zhao , and X . - G . Huang , Chin . Phys . C 46 , 074110 ( 2022 ) , arXiv : 2112 . 03824 [ hep - ph ] . [ 44 ] F . Li , Y . Wang , H . Lü , P . Li , Q . Li , and F . Liu , J . Phys . G 47 , 115104 ( 2020 ) , arXiv : 2008 . 11540 [ nucl - th ] . [ 45 ] L . Li , X . Chen , Y . Cui , Z . Li , and Y . Zhang , ( 2022 ) , arXiv : 2201 . 12586 [ nucl - th ] . [ 46 ] R . Q . Charles , H . Su , M . Kaichun , and L . J . Guibas , in 2017 IEEE Conference on Computer Vision and Pattern Recogni - tion ( CVPR ) ( 2017 ) pp . 77 – 85 . [ 47 ] M . Omana Kuttan , J . Steinheimer , K . Zhou , A . Redel - bach , and H . Stoecker , Phys . Lett . B 811 , 135872 ( 2020 ) , arXiv : 2009 . 01584 [ hep - ph ] . [ 48 ] L . - G . Pang , K . Zhou , and X . - N . Wang , ( 2019 ) , arXiv : 1906 . 06429 [ nucl - th ] . [ 49 ] Y . - L . Cheng , S . Shi , Y . - G . Ma , H . Stöcker , and K . Zhou , ( 2023 ) , arXiv : 2301 . 03910 [ nucl - th ] . [ 50 ] C . Zhang and J . Jia , Phys . Rev . Lett . 128 , 022301 ( 2022 ) , arXiv : 2109 . 01631 [ nucl - th ] . [ 51 ] W . B . He , Y . G . Ma , X . G . Cao , X . Z . Cai , and G . Q . Zhang , Phys . Rev . Lett . 113 , 032506 ( 2014 ) . [ 52 ] C . - Z . Shi and Y . - G . Ma , Nucl . Sci . Tech . 32 , 66 ( 2021 ) , arXiv : 2109 . 09938 [ nucl - th ] . [ 53 ] S . Zhang , Y . G . Ma , J . H . Chen , W . B . He , and C . Zhong , Phys . Rev . C 95 , 064904 ( 2017 ) . [ 54 ] J . He , W . - B . He , Y . - G . Ma , and S . Zhang , Phys . Rev . C 104 , 044902 ( 2021 ) , arXiv : 2109 . 06277 [ hep - ph ] . [ 55 ] D . Adhikari et al . ( PREX ) , Phys . Rev . Lett . 126 , 172502 ( 2021 ) , arXiv : 2102 . 10767 [ nucl - ex ] . [ 56 ] J . Xu ( 2023 ) arXiv : 2301 . 07884 [ nucl - th ] . [ 57 ] J . Xu , W . - J . Xie , and B . - A . Li , Phys . Rev . C 102 , 044316 ( 2020 ) , arXiv : 2007 . 07669 [ nucl - th ] . [ 58 ] M . B . Tsang et al . , Phys . Rev . C 86 , 015803 ( 2012 ) , arXiv : 1204 . 0466 [ nucl - ex ] . [ 59 ] S . - H . Cheng , J . Wen , L . - G . Cao , and F . - S . Zhang , Chin . Phys . C 47 , 024102 ( 2023 ) . [ 60 ] X . - R . Huang and L . - W . Chen , Phys . Rev . D 106 , 123034 ( 2022 ) , arXiv : 2210 . 04534 [ nucl - th ] . [ 61 ] E . A . Teixeira , T . Aumann , C . A . Bertulani , and B . V . Carl - son , Eur . Phys . J . A 58 , 205 ( 2022 ) , arXiv : 2208 . 13300 [ nucl - th ] . [ 62 ] D . Androic et al . ( Qweak ) , Phys . Rev . Lett . 128 , 132501 ( 2022 ) , arXiv : 2112 . 15412 [ nucl - ex ] . [ 63 ] H . - j . Xu , in 20th International Conference on Strangeness in Quark Matter 2022 ( 2023 ) arXiv : 2301 . 08303 [ nucl - th ] . [ 64 ] N . Kozyrev , A . Svetlichnyi , R . Nepeivoda , and I . Pshenich - nov , Eur . Phys . J . A 58 , 184 ( 2022 ) , arXiv : 2204 . 07189 [ nucl - th ] . [ 65 ] L . - M . Liu , C . - J . Zhang , J . Zhou , J . Xu , J . Jia , and G . - X . Peng , Phys . Lett . B 834 , 137441 ( 2022 ) , arXiv : 2203 . 09924 [ nucl - th ] . [ 66 ] Y . - J . Huang , L . - G . Pang , and X . - N . Wang , Science China Physics , Mechanics & Astronomy 52 , 252011 ( 2022 ) . [ 67 ] U . W . Heinz , H . Song , and A . K . Chaudhuri , Phys . Rev . C 73 , 034904 ( 2006 ) , arXiv : nucl - th / 0510014 . [ 68 ] P . Romatschke and U . Romatschke , Phys . Rev . Lett . 99 , 172301 ( 2007 ) , arXiv : 0706 . 1522 [ nucl - th ] . [ 69 ] D . Teaney , Phys . Rev . C 68 , 034913 ( 2003 ) , arXiv : nucl - th / 0301099 . [ 70 ] J . E . Bernhard , J . S . Moreland , S . A . Bass , J . Liu , and U . Heinz , Phys . Rev . C 94 , 024907 ( 2016 ) , arXiv : 1605 . 03954 [ nucl - th ] . [ 71 ] J . E . Bernhard , J . S . Moreland , and S . A . Bass , Nature Phys . 15 , 1113 ( 2019 ) . [ 72 ] Z . Yang and L . - W . Chen , ( 2022 ) , arXiv : 2207 . 13534 [ nucl - th ] . [ 73 ] A . Kelic , J . Natowitz , and K . - H . Schmidt , Eur . Phys . J . A 30 , 203 ( 2006 ) . 29 [ 74 ] Y . D . Song , R . Wang , Y . - G . M . Ma , X . - G . Deng , and H . - L . Liu , Phys . Lett . B 814 , 136084 ( 2021 ) . [ 75 ] J . B . Natowitz , R . Wada , K . Hagel , T . Keutgen , M . Murray , A . Makeev , L . Qin , P . Smith , and C . Hamilton , Phys . Rev . C 65 , 034618 ( 2002 ) . [ 76 ] R . Wang , Y . - G . Ma , R . Wada , L . - W . Chen , W . - B . He , H . - L . Liu , and K . - J . Sun , Phys . Rev . Research 2 , 043202 ( 2020 ) . [ 77 ] B . Borderie and J . Frankland , Prog . Part . Nucl . Phys . 105 , 82 ( 2019 ) . [ 78 ] J . Pochodzalla , T . Möhlenkamp , T . Rubehn , et al . , Phys . Rev . Lett . 75 , 1040 ( 1995 ) . [ 79 ] Y . G . Ma , A . Siwek , J . Péter , et al . , Phys . Lett . B 390 , 41 ( 1997 ) . [ 80 ] Y . G . Ma , Phys . Rev . Lett . 83 , 3617 ( 1999 ) . [ 81 ] J . B . Natowitz , K . Hagel , Y . G . Ma , et al . , Phys . Rev . Lett 89 , 212701 ( 2002 ) . [ 82 ] Y . G . Ma , J . B . Natowitz , R . Wada , et al . , Phys . Rev . C 71 , 054606 ( 2005 ) . [ 83 ] X . G . Deng , P . Danielewicz , Y . G . Ma , H . Lin , and Y . X . Zhang , Phys . Rev . C 105 , 064613 ( 2022 ) . [ 84 ] C . Liu , X . G . Deng , and Y . G . Ma , Nucl . Sci . Tech . 33 , 52 ( 2022 ) . [ 85 ] E . P . L . van Nieuwenburg , Y . - H . Liu , and S . D . Huber , Nat . Phys . 13 , 435 ( 2017 ) . [ 86 ] R . Wada , W . Lin , P . Ren , H . Zheng , X . Liu , M . Huang , K . Yang , and K . Hagel , Phys . Rev . C 99 , 024616 ( 2019 ) . [ 87 ] M . Omana Kuttan , J . Steinheimer , K . Zhou , and H . Stöcker , ( 2022 ) , arXiv : 2211 . 11670 [ nucl - th ] . [ 88 ] L . - G . Pang , K . Zhou , N . Su , H . Petersen , H . Stöcker , and X . - N . Wang , Nature Commun . 9 , 210 ( 2018 ) , arXiv : 1612 . 04262 [ hep - ph ] . [ 89 ] L . Pang , Q . Wang , and X . - N . Wang , Phys . Rev . C 86 , 024911 ( 2012 ) , arXiv : 1205 . 5019 [ nucl - th ] . [ 90 ] C . Shen , Z . Qiu , H . Song , J . Bernhard , S . Bass , and U . Heinz , Comput . Phys . Commun . 199 , 61 ( 2016 ) , arXiv : 1409 . 8164 [ nucl - th ] . [ 91 ] Y . - L . Du , K . Zhou , J . Steinheimer , L . - G . Pang , A . Motor - nenko , H . - S . Zong , X . - N . Wang , and H . Stöcker , Eur . Phys . J . C 80 , 516 ( 2020 ) , arXiv : 1910 . 11530 [ hep - ph ] . [ 92 ] Y . - L . Du , K . Zhou , J . Steinheimer , L . - G . Pang , A . Motor - nenko , H . - S . Zong , X . - N . Wang , and H . Stöcker , Nucl . Phys . A 1005 , 121891 ( 2021 ) , arXiv : 2009 . 03059 [ nucl - th ] . [ 93 ] J . Steinheimer , L . - G . Pang , K . Zhou , V . Koch , J . Randrup , and H . Stoecker , Nucl . Phys . A 1005 , 121867 ( 2021 ) . [ 94 ] L . Jiang , L . Wang , and K . Zhou , Phys . Rev . D 103 , 116023 ( 2021 ) , arXiv : 2103 . 04090 [ nucl - th ] . [ 95 ] M . Omana Kuttan , K . Zhou , J . Steinheimer , A . Redelbach , and H . Stoecker , JHEP 21 , 184 ( 2020 ) , arXiv : 2107 . 05590 [ hep - ph ] . [ 96 ] P . Thaprasop , K . Zhou , J . Steinheimer , and C . Herold , Phys . Scripta 96 , 064003 ( 2021 ) , arXiv : 2007 . 15830 [ hep - ex ] . [ 97 ] Y . Wang , F . Li , Q . Li , H . Lü , and K . Zhou , Phys . Lett . B 822 , 136669 ( 2021 ) , arXiv : 2107 . 11012 [ nucl - th ] . [ 98 ] D . Mroczek , M . Hjorth - Jensen , J . Noronha - Hostler , P . Parotto , C . Ratti , and R . Vilalta , ( 2022 ) , arXiv : 2203 . 13876 [ nucl - th ] . [ 99 ] H . Huang , B . Xiao , Z . Liu , Z . Wu , Y . Mu , and H . Song , Phys . Rev . Res . 3 , 023256 ( 2021 ) , arXiv : 1801 . 03334 [ nucl - th ] . [ 100 ] H . Huang , B . Xiao , H . Xiong , Z . Wu , Y . Mu , and H . Song , Nucl . Phys . A 982 , 927 ( 2019 ) , arXiv : 1807 . 05728 [ nucl - th ] . [ 101 ] D . A . Teaney , “Viscous Hydrodynamics and the Quark Gluon Plasma , ” in Quark - gluon plasma 4 , edited by R . C . Hwa and X . - N . Wang ( 2010 ) pp . 207 – 266 , arXiv : 0905 . 2433 [ nucl - th ] . [ 102 ] P . Romatschke , Int . J . Mod . Phys . E 19 , 1 ( 2010 ) , arXiv : 0902 . 3663 [ hep - ph ] . [ 103 ] U . Heinz and R . Snellings , Ann . Rev . Nucl . Part . Sci . 63 , 123 ( 2013 ) , arXiv : 1301 . 2826 [ nucl - th ] . [ 104 ] C . Gale , S . Jeon , and B . Schenke , Int . J . Mod . Phys . A 28 , 1340011 ( 2013 ) , arXiv : 1301 . 5893 [ nucl - th ] . [ 105 ] H . Song , Pramana 84 , 703 ( 2015 ) , arXiv : 1401 . 0079 [ nucl - th ] . [ 106 ] H . Song , Y . Zhou , and K . Gajdosova , Nucl . Sci . Tech . 28 , 99 ( 2017 ) , arXiv : 1703 . 00670 [ nucl - th ] . [ 107 ] P . F . Kolb and U . W . Heinz , , 634 ( 2003 ) , arXiv : nucl - th / 0305084 . [ 108 ] H . Song , Causal Viscous Hydrodynamics for Relativistic Heavy Ion Collisions , Other thesis ( 2009 ) , arXiv : 0908 . 3656 [ nucl - th ] . [ 109 ] H . Song and U . W . Heinz , Phys . Rev . C 77 , 064901 ( 2008 ) , arXiv : 0712 . 3715 [ nucl - th ] . [ 110 ] H . Song and U . W . Heinz , Phys . Lett . B 658 , 279 ( 2008 ) , arXiv : 0709 . 0742 [ nucl - th ] . [ 111 ] M . L . Miller , K . Reygers , S . J . Sanders , and P . Stein - berg , Ann . Rev . Nucl . Part . Sci . 57 , 205 ( 2007 ) , arXiv : nucl - ex / 0701025 . [ 112 ] T . Hirano and Y . Nara , Phys . Rev . C 79 , 064904 ( 2009 ) , arXiv : 0904 . 4080 [ nucl - th ] . [ 113 ] H . J . Drescher and Y . Nara , Phys . Rev . C 75 , 034905 ( 2007 ) , arXiv : nucl - th / 0611017 . [ 114 ] H . - j . Xu , Z . Li , and H . Song , Phys . Rev . C 93 , 064905 ( 2016 ) , arXiv : 1602 . 02029 [ nucl - th ] . [ 115 ] W . Zhao , H . - j . Xu , and H . Song , Eur . Phys . J . C 77 , 645 ( 2017 ) , arXiv : 1703 . 10792 [ nucl - th ] . [ 116 ] J . S . Moreland , J . E . Bernhard , and S . A . Bass , Phys . Rev . C 92 , 011901 ( 2015 ) , arXiv : 1412 . 4708 [ nucl - th ] . [ 117 ] H . Yoon , J . - H . Sim , and M . J . Han , Phys . Rev . B 98 , 245101 ( 2018 ) , arXiv : 1806 . 03841 [ cond - mat . str - el ] . [ 118 ] R . Fournier , L . Wang , O . V . Yazyev , and Q . Wu , Phys . Rev . Lett . 124 , 056401 ( 2020 ) . [ 119 ] L . Kades , J . M . Pawlowski , A . Rothkopf , M . Scherzer , J . M . Urban , S . J . Wetzel , N . Wink , and F . P . G . Ziegler , Phys . Rev . D 102 , 096001 ( 2020 ) , arXiv : 1905 . 04305 [ physics . comp - ph ] . [ 120 ] L . Wang , S . Shi , and K . Zhou , Phys . Rev . D 106 , L051502 ( 2022 ) , arXiv : 2111 . 14760 [ hep - ph ] . [ 121 ] J . Horak , J . M . Pawlowski , J . Rodríguez - Quintero , J . Turn - wald , J . M . Urban , N . Wink , and S . Zafeiropoulos , Phys . Rev . D 105 , 036014 ( 2022 ) , arXiv : 2107 . 13464 [ hep - ph ] . [ 122 ] J . Horak , J . Papavassiliou , J . M . Pawlowski , and N . Wink , Phys . Rev . D 104 ( 2021 ) , 10 . 1103 / PhysRevD . 104 . 074017 , arXiv : 2103 . 16175 [ hep - th ] . [ 123 ] A . K . Cyrol , J . M . Pawlowski , A . Rothkopf , and N . Wink , SciPost Phys . 5 , 065 ( 2018 ) , arXiv : 1804 . 00945 [ hep - ph ] . [ 124 ] L . Wang , S . Shi , and K . Zhou , in 35th Conference on Neu - ral Information Processing Systems ( 2021 ) arXiv : 2112 . 06206 [ physics . comp - ph ] . [ 125 ] S . Shi , L . Wang , and K . Zhou , Comput . Phys . Commun . 282 , 108547 ( 2023 ) , arXiv : 2201 . 02564 [ hep - ph ] . [ 126 ] S . Soma , L . Wang , S . Shi , H . Stöcker , and K . Zhou , JCAP 08 , 071 ( 2022 ) , arXiv : 2201 . 01756 [ hep - ph ] . [ 127 ] S . Soma , L . Wang , S . Shi , H . Stöcker , and K . Zhou , ( 2022 ) , arXiv : 2209 . 08883 [ astro - ph . HE ] . [ 128 ] X . Gao , A . D . Hanlon , N . Karthik , S . Mukherjee , P . Petreczky , P . Scior , S . Shi , S . Syritsyn , Y . Zhao , and K . Zhou , Phys . Rev . D 106 , 114510 ( 2022 ) , arXiv : 2208 . 02297 [ hep - lat ] . [ 129 ] M . Zhou , F . Gao , J . Chao , Y . - X . Liu , and H . Song , Phys . Rev . D 104 , 076011 ( 2021 ) , arXiv : 2106 . 08168 [ hep - ph ] . [ 130 ] S . Shi , K . Zhou , J . Zhao , S . Mukherjee , and P . Zhuang , Phys . 30 Rev . D 105 , 014017 ( 2022 ) , arXiv : 2105 . 07862 [ hep - ph ] . [ 131 ] R . Larsen , S . Meinel , S . Mukherjee , and P . Petreczky , Phys . Lett . B 800 , 135119 ( 2020 ) , arXiv : 1910 . 07374 [ hep - lat ] . [ 132 ] D . Lafferty and A . Rothkopf , Phys . Rev . D 101 , 056010 ( 2020 ) , arXiv : 1906 . 00035 [ hep - ph ] . [ 133 ] D . S . Broomhead and D . Lowe , Radial basis functions , multi - variable functional interpolation and adaptive networks , Tech . Rep . ( Royal Signals and Radar Establishment Malvern ( United Kingdom ) , 1988 ) . [ 134 ] F . Schwenker , H . A . Kestler , and G . Palm , Neural networks 14 , 439 ( 2001 ) . [ 135 ] L . Beheim , A . Zitouni , F . Belloir , and C . d . M . de la Housse , WSEAS Transactions on Systems , 467 ( 2004 ) . [ 136 ] J . Wang and G . Liu , International Journal for Numerical Methods in Engineering 54 , 1623 ( 2002 ) . [ 137 ] J . C . Carr , W . R . Fright , and R . K . Beatson , IEEE transactions on medical imaging 16 , 96 ( 1997 ) . [ 138 ] W . Chen , X . Han , G . Li , C . Chen , J . Xing , Y . Zhao , and H . Li , arXiv preprint arXiv : 1812 . 04302 ( 2018 ) . [ 139 ] H . Yoon , J . - H . Sim , and M . J . Han , Phys . Rev . B 98 , 245101 ( 2018 ) . [ 140 ] R . Fournier , L . Wang , O . V . Yazyev , and Q . Wu , Phys . Rev . Lett . 124 , 056401 ( 2020 ) . [ 141 ] K . Zhou , N . Xu , Z . Xu , and P . Zhuang , Phys . Rev . C 89 , 054911 ( 2014 ) , arXiv : 1401 . 5845 [ nucl - th ] . [ 142 ] J . Zhao , K . Zhou , S . Chen , and P . Zhuang , Prog . Part . Nucl . Phys . 114 , 103801 ( 2020 ) , arXiv : 2005 . 08277 [ nucl - th ] . [ 143 ] M . Laine , O . Philipsen , P . Romatschke , and M . Tassler , JHEP 03 , 054 ( 2007 ) , arXiv : hep - ph / 0611300 . [ 144 ] A . Beraudo , J . P . Blaizot , and C . Ratti , Nucl . Phys . A 806 , 312 ( 2008 ) , arXiv : 0712 . 4394 [ nucl - th ] . [ 145 ] N . Brambilla , J . Ghiglieri , A . Vairo , and P . Petreczky , Phys . Rev . D 78 , 014017 ( 2008 ) , arXiv : 0804 . 0993 [ hep - ph ] . [ 146 ] N . Brambilla , M . A . Escobedo , J . Ghiglieri , J . Soto , and A . Vairo , JHEP 09 , 038 ( 2010 ) , arXiv : 1007 . 4156 [ hep - ph ] . [ 147 ] B . Chen , K . Zhou , and P . Zhuang , Phys . Rev . C 86 , 034906 ( 2012 ) , arXiv : 1202 . 3523 [ nucl - th ] . [ 148 ] F . - P . Li , H . - L . Lü , L . - G . Pang , and G . - Y . Qin , ( 2022 ) , arXiv : 2211 . 07994 [ hep - ph ] . [ 149 ] R . Baier , Y . L . Dokshitzer , A . H . Mueller , S . Peigne , and D . Schiff , Nucl . Phys . B 483 , 291 ( 1997 ) , arXiv : hep - ph / 9607355 . [ 150 ] R . Baier , Y . L . Dokshitzer , A . H . Mueller , S . Peigne , and D . Schiff , Nucl . Phys . B 484 , 265 ( 1997 ) , arXiv : hep - ph / 9608322 . [ 151 ] M . Gyulassy and X . - n . Wang , Nucl . Phys . B 420 , 583 ( 1994 ) , arXiv : nucl - th / 9306003 . [ 152 ] X . - f . Guo and X . - N . Wang , Phys . Rev . Lett . 85 , 3591 ( 2000 ) , arXiv : hep - ph / 0005044 . [ 153 ] U . A . Wiedemann , Nucl . Phys . B 588 , 303 ( 2000 ) , arXiv : hep - ph / 0005129 . [ 154 ] Y . Xu , J . E . Bernhard , S . A . Bass , M . Nahrgang , and S . Cao , Phys . Rev . C 97 , 014907 ( 2018 ) , arXiv : 1710 . 00807 [ nucl - th ] . [ 155 ] Y . He , L . - G . Pang , and X . - N . Wang , Phys . Rev . Lett . 122 , 252302 ( 2019 ) , arXiv : 1808 . 05310 [ hep - ph ] . [ 156 ] R . Soltz ( Jetscape ) , PoS HardProbes2018 , 048 ( 2019 ) . [ 157 ] M . Xie , W . Ke , H . Zhang , and X . - N . Wang , ( 2022 ) , arXiv : 2206 . 01340 [ hep - ph ] . [ 158 ] M . Xie , W . Ke , H . Zhang , and X . - N . Wang , ( 2022 ) , arXiv : 2208 . 14419 [ hep - ph ] . [ 159 ] M . Feickert and B . Nachman , ( 2021 ) , arXiv : 2102 . 02770 [ hep - ph ] . [ 160 ] K . T . Yi - Lun DU , Daniel PABLOS , Science China Physics , Mechanics & Astronomy 52 , 252017 ( 2022 ) . [ 161 ] Y . - L . Du , D . Pablos , and K . Tywoniuk , JHEP 21 , 206 ( 2020 ) , arXiv : 2012 . 07797 [ hep - ph ] . [ 162 ] Y . - L . Du , D . Pablos , and K . Tywoniuk , Phys . Rev . Lett . 128 , 012301 ( 2022 ) , arXiv : 2106 . 11271 [ hep - ph ] . [ 163 ] Z . Yang , Y . He , W . Chen , W . - Y . Ke , L . - G . Pang , and X . - N . Wang , ( 2022 ) , arXiv : 2206 . 02393 [ nucl - th ] . [ 164 ] Y . He , T . Luo , X . - N . Wang , and Y . Zhu , Phys . Rev . C 91 , 054908 ( 2015 ) , [ Erratum : Phys . Rev . C 97 , 019902 ( 2018 ) ] , arXiv : 1503 . 03313 [ nucl - th ] . [ 165 ] S . Cao et al . ( JETSCAPE ) , Phys . Rev . C 96 , 024909 ( 2017 ) , arXiv : 1705 . 00050 [ nucl - th ] . [ 166 ] F . - L . Liu , W . - J . Xing , X . - Y . Wu , G . - Y . Qin , S . Cao , and X . - N . Wang , Eur . Phys . J . C 82 , 350 ( 2022 ) , arXiv : 2107 . 11713 [ hep - ph ] . [ 167 ] F . G . Gardim , F . Grassi , M . Luzum , and J . - Y . Ollitrault , Phys . Rev . C 87 , 031901 ( 2013 ) , arXiv : 1211 . 0989 [ nucl - th ] . [ 168 ] S . A . Voloshin , A . M . Poskanzer , and R . Snellings , Landolt - Bornstein 23 , 293 ( 2010 ) , arXiv : 0809 . 2949 [ nucl - ex ] . [ 169 ] R . Snellings , New J . Phys . 13 , 055008 ( 2011 ) , arXiv : 1102 . 3010 [ nucl - ex ] . [ 170 ] J . Jia , J . Phys . G 41 , 124003 ( 2014 ) , arXiv : 1407 . 6057 [ nucl - ex ] . [ 171 ] Z . Liu , W . Zhao , and H . Song , Eur . Phys . J . C 79 , 870 ( 2019 ) , arXiv : 1903 . 09833 [ nucl - th ] . [ 172 ] R . S . Bhalerao , J . - Y . Ollitrault , S . Pal , and D . Teaney , Phys . Rev . Lett . 114 , 152301 ( 2015 ) , arXiv : 1410 . 7739 [ nucl - th ] . [ 173 ] A . Mazeliauskas and D . Teaney , Phys . Rev . C 91 , 044902 ( 2015 ) , arXiv : 1501 . 03138 [ nucl - th ] . [ 174 ] A . Mazeliauskas and D . Teaney , Phys . Rev . C 93 , 024913 ( 2016 ) , arXiv : 1509 . 07492 [ nucl - th ] . [ 175 ] P . Bozek , Phys . Rev . C 97 , 034905 ( 2018 ) , arXiv : 1711 . 07773 [ nucl - th ] . [ 176 ] A . M . Sirunyan et al . ( CMS ) , Phys . Rev . C 96 , 064902 ( 2017 ) , arXiv : 1708 . 07113 [ nucl - ex ] . [ 177 ] Z . Liu , A . Behera , H . Song , and J . Jia , Phys . Rev . C 102 , 024911 ( 2020 ) , arXiv : 2002 . 06061 [ nucl - ex ] . [ 178 ] Y . - S . Zhao , L . Wang , K . Zhou , and X . - G . Huang , Phys . Rev . C 106 , L051901 ( 2022 ) , arXiv : 2105 . 13761 [ hep - ph ] . [ 179 ] K . Lee , J . Mulligan , M . Plosko´n , F . Ringer , and F . Yuan , ( 2022 ) , arXiv : 2210 . 06450 [ hep - ph ] . [ 180 ] Z . Hao , R . Kansal , J . Duarte , and N . Chernyavskaya , ( 2022 ) , arXiv : 2212 . 07347 [ hep - ex ] .