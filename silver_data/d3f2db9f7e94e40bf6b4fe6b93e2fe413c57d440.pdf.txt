MANAGEMENT SCIENCE Articles in Advance , pp . 1 – 21 ISSN 0025 - 1909 ( print ) (cid:151) ISSN 1526 - 5501 ( online ) http : / / dx . doi . org / 10 . 1287 / mnsc . 1120 . 1698 ©2013 INFORMS Organizational Decision Making : An Information Aggregation View Felipe A . Csaszar Stephen M . Ross School of Business , University of Michigan , Ann Arbor , Michigan 48109 , fcsaszar @ umich . edu J . P . Eggers Stern School of Business , New York University , New York , New York 10012 , jeggers @ stern . nyu . edu W e study four information aggregation structures commonly used by organizations to evaluate opportuni - ties : individual decision making , delegation to experts , majority voting , and averaging of opinions . Using a formal mathematical model , we investigate how the performance of each of these structures is contingent upon the breadth of knowledge within the ﬁrm and changes in the environment . Our model builds on work in the Carnegie tradition and in the group and behavioral decision - making literatures . We use the model to explore when delegation is preferable to other structures , such as voting and averaging . Our model shows that dele - gation is the most effective structure when there is diversity of expertise , when accurate delegation is possible , and when there is a good ﬁt between the ﬁrm’s knowledge and the knowledge required by the environment . Otherwise , depending on the knowledge breadth of the ﬁrm , voting or averaging may be the most effective structure . Finally , we use our model to shed light on which structures are more robust to radical environmental change and when crowd - based decision making may outperform delegation . Key words : organizational structure ; decision making ; knowledge ; environmental change History : Received January 19 , 2011 ; accepted December 3 , 2012 , by Jesper Sørensen , organizations . Published online in Articles in Advance . 1 . Introduction Important organizational decisions are likely to be made by groups rather than by individuals ( Tindale et al . 2003 , p . 381 ) . Such groups might include top management teams , boards of directors , or ﬁnance committees . Given that different group members may often have different opinions , how individual opin - ions are aggregated into a group - level decision has important implications for organizational action and performance . Yet selecting the right method to aggre - gate opinions can be challenging , as there are many aggregation methods and which one is the best may be contingent on multiple factors . The following example illustrates some of these challenges . Imagine the case of three founders of a startup com - pany deciding on whether or not to acquire a com - petitor . How should their different opinions on the value of the target be aggregated ? Should they vote on whether or not to acquire ? Should they delegate the decision to one member ? Should they average their differing valuations and make a decision based on that average ? Would the answer depend on the expertise of the decision makers ( e . g . , if they were three MBAs versus if they were an accountant , an MBA , and a scientist ) ? Would the answer depend on the degree of environmental uncertainty ( as when , e . g . , the target produces a promising but unproved technology versus a commodity ) ? Problems such as these are common in settings ranging from small startups to large governments . Example settings include movie studio executives or corporate ﬁnance committees deciding on which projects to fund , venture capitalists and mutual fund managers choosing which assets to buy , and boards of directors and top management teams deciding on strategic actions . In general , information aggregation pervades organizational decision making . Despite this pervasiveness in organizations , with few exceptions ( e . g . , Knudsen and Levinthal 2007 , Fang et al . 2010 , Csaszar 2012b ) , organizations lit - erature has devoted comparatively little attention to information aggregation issues . Information aggrega - tion ( also known as the structure of decision making ) was proclaimed by the Carnegie tradition as one of its central concerns ( Cyert and March 1963 , pp . 19 – 22 ) . In fact , Simon ( 1947 / 1997 , pp . 18 – 19 ) deﬁnes the basic construct of “organization” in terms of information aggregation : “the pattern of communications and rela - tions among a group of human beings , including the processes for making and implementing decisions . ” Despite this deep concern with structure and infor - mation aggregation , a recent paper by Gavetti et al . ( 2007 , p . 528 ) notes that this is one of the “forgotten 1 C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Published online ahead of print April 4 , 2013 Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View 2 Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS pillars” of the Carnegie tradition . Other literatures that share an information processing sensibility— such as the resource allocation process ( Bower 1970 ) and organization design ( Thompson 1967 , Burton and Obel 2004 ) —have also discussed the lack of research on the topic ( Miller et al . 2009 ) . Our central question is this : What is the most appropriate decision - making structure for the eval - uation of potential alternatives given an organiza - tion’s environment and the expertise of its members ? To rigorously address this question , we develop a parsimonious model of information aggregation in organizations . Our model allows us to investigate questions of organizational design in a manner simi - lar to recent work by Knudsen and Levinthal ( 2007 ) and Csaszar ( 2012a ) . Our modeling approach draws on and extends information aggregation models from psychology and economics . Previous models in these literatures have analyzed ( among other issues ) the robustness of dif - ferent structures ( Hastie and Kameda 2005 ) , opti - mal processes for weighting opinions ( Ben - Yashar and Nitzan 1997 ) , and when simple heuristics are capa - ble of making near - optimal decisions ( Gigerenzer and Goldstein 1996 ) . We contribute to these literatures by introducing to existing information aggregation mod - els a few key organizational elements . These include assessing the robustness of our focal decision - making structures—delegation to experts , majority voting , and averaging of opinions—to changes in the envi - ronment and the expertise of its members . Additional contingencies include errors in delegation , errors in assessing expertise , and updating processes by deci - sion makers . Our approach captures the knowledge of the ﬁrm and the knowledge required by the envi - ronment through modeling individuals’ perceptions , knowledge , and the environment as a stochastic pro - cess . This approach allows us to extend models in behavioral and group decision making , leading to novel organizational results . Our model also provides a starting point for investigating further questions of aggregation in organizations . Three novel ﬁndings emerge from this analysis . First , the interconnected nature of structure , knowl - edge , and environment means that each decision - making structure can be the optimal choice under the appropriate set of contingencies . Second , organi - zational performance depends in a nontrivial way on how the opinions of individual members are aggre - gated . This is true even for means of aggregation that are ostensibly similar , such as averaging esti - mates and allowing individuals to vote . Third , the delegation of decision - making authority can be sur - prisingly effective , speciﬁcally when individual deci - sion makers have signiﬁcantly different knowledge , when projects can be correctly assigned to suitable individuals , and when there is a good ﬁt between the knowledge of the ﬁrm and the knowledge required by the environment . These ﬁndings provide insights into the prevalence of delegation as a means of organiza - tional decision making ; the conditions under which other , more egalitarian , structures can be effective within a ﬁrm ; and the relative performance of differ - ent information aggregation mechanisms available to organizations . 2 . Theoretical Motivation The main question explored in this paper—“What is the most appropriate decision - making structure for the evaluation of potential alternatives given an organization’s environment and the expertise of its members ? ”—parallels a number of questions that have been explored in different literatures on infor - mation aggregation . In this section we review these literatures and discuss how our approach differs from previous studies . 2 . 1 . Antecedents of Our Work The study of different information aggregation rules has a long history . In fact , the study of the main rules explored in this paper—delegation , voting , and averaging—can be traced back at least to Aristotle ( c . 330 BCE / 1984 ) , Condorcet ( 1785 ) , and Laplace ( 1814 / 1995 ) , respectively . 1 Because devising effective aggregation rules is relevant in many applications , research on this topic is vast and spans multiple dis - ciplines ( Grofman and Owen 1986 , p . xi ) . Therefore , the following review is focused on prior research with close similarity to the current study . This review is biased toward models of aggregation rules that are carried out by fallible decision makers who share a common goal . We group these literatures into four clusters : Carnegie tradition , behavioral decision mak - ing , group decision making , and economics . Carnegie Tradition . The Carnegie tradition is con - cerned with how decisions are made within organi - zations . As mentioned in the introduction , issues of information aggregation are central to this tradition . In recent years , several models of organizational deci - sion making have emerged from this tradition , very much in the spirit of classic work ( e . g . , Cohen et al . 1972 , March 1991 ) . 1 Aristotle observed that groups can outperform individuals because , in a group , “some understand one part , and some another , and among them they understand the whole” ( Aristotle c . 330 BCE / 1984 , Book III . 11 ) , implicitly pointing to a delegation pro - cess inside the group . Centuries later , Condorcet ( 1785 ) proved that majority voting would tend to select the right choice as the num - ber of voters approached inﬁnity , assuming uncorrelated voters with a modicum of screening ability . A few years later , Laplace ( 1814 / 1995 , Chap . IX ) suggested that averaging individual opinions could be used to synthesize a highly accurate opinion . C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS 3 To understand the previous research and how the current work relates to it , it is useful to examine the three main stages of organizational decision making : ( i ) search , ( ii ) evaluation , and ( iii ) implementation of alternatives ( Mintzberg et al . 1976 , Schwenk 1984 ) . Interestingly , most of the models of organizational decision making done in the Carnegie tradition have focused on stage ( i ) by using NK and other simula - tion methods ( e . g . , Seshadri and Shapira 2003 , Rivkin and Siggelkow 2003 , Siggelkow and Rivkin 2005 ) . Because this paper models how a group of decision makers evaluate alternatives , it falls squarely into the less researched stage ( ii ) of the previous taxonomy . Among the few studies in the Carnegie tradition that have modeled aspects of the evaluation stage , there is work about the effect of different organizational struc - tures on errors of omission and commission ( Knudsen and Levinthal 2007 , Christensen and Knudsen 2010 , Csaszar 2012b ) and on exploration and exploitation ( Fang et al . 2010 , Csaszar 2012a ) . Behavioral Decision Making . This literature is concerned with the performance of different decision - making heuristics . A prevalent ﬁnding in this litera - ture is that several heuristics , that is , rules that do not take into account all of the available data or whose internal parameters do not stem from an optimization process , can perform at similar levels compared with optimal decision rules in many realistic settings . Some of the heuristics that have been shown to exhibit this high performance are unit weights ( Einhorn and Hog - arth 1975 ) , improper linear models ( Dawes 1979 ) , and take - the - best ( Gigerenzer and Goldstein 1996 ) and similar lexicographic rules ( e . g . , Deterministic Elimi - nation by Aspects ; Hogarth and Karelaia 2005 ) . Similar to the structures in our model , the previous heuristics seek to choose the best among a set of alter - natives ; each alternative is described by a set of char - acteristics or cues . If one interprets the judgments of the individuals in our model as decision - making cues , two similarities emerge between our model and the aforementioned heuristics . First , averaging of judg - ments is akin to unit weights ( i . e . , the judgments of the experts are totaled using equal weights ) . Second , delegation to the most suitable expert is akin to take - the - best , as only the most relevant cue is used ( i . e . , the judgment of the most suitable expert ) . A more general similarity relates to one of the goals of our model : to understand what decision rules work bet - ter in which environments . This goal was highlighted early in the behavioral decision literature ( Brunswik 1952 , Simon 1956 ) and has more recently motivated a rich stream of literature on adaptive decision mak - ing ( Payne et al . 1993 , Hogarth and Karelaia 2007 , Soll and Larrick 2009 ) . Group Decision Making . Group decision - making lit - erature has modeled different rules that groups can use to aggregate opinions . For instance , Hastie and Kameda ( 2005 ) , similar to earlier work by Einhorn et al . ( 1977 ) and Hogarth ( 1978 ) , study the perfor - mance of nine aggregation rules . The main aggrega - tion rules studied in the current paper—delegation , voting , and averaging—are among the ones explored by Hastie and Kameda ( 2005 ) ( they call these rules best - member , majority / plurality , and average win - ner , respectively ) . They test the performance of these decision - making rules in an environment deﬁned by a linear combination of the cue values . In this environ - ment , averaging is the best performer , because it nat - urally captures the linearity of the environment . They also ﬁnd that voting performs similarly to averaging , and that voting usually outperforms delegation . In general , they ﬁnd that rules that combine individual scores ( such as averaging ) tend to outperform rules that do not combine individual scores ( such as delega - tion and voting ) . More recently , Kameda et al . ( 2011 ) have shown that majority voting can effectively pool information even if the group members are self inter - ested and some of them decide to shirk their duty to vote . Soll and Larrick ( 2009 ) also develop a model that explores some of the same structures as the current paper . In particular , they use their model to provide a normative framework to choose between averag - ing and delegation , and then perform experiments to determine how people deviate from this framework . Two of the parameters in their framework ( disper - sion in judges’ accuracies and identiﬁability of the most expert judge ) are similar to elements in our model ( namely , the relative accuracy of the experts and delegation errors , which are introduced in § § 4 . 1 and 4 . 3 , respectively ) . Larrick and Soll ( 2006 ) show that , in two - member groups , averaging performs at least as well as delegating to a random member . Yet despite this , people often distrust averaging . Newell and Shanks ( 2003 ) use experiments to show when people rely on a single cue versus multiple cues , and thus examine when people use delegation versus rules that use more information . For surveys of the group decision - making literature , see Tindale et al . ( 2003 ) and Laughlin ( 2011 ) . Economics . Economics has also studied models of information aggregation . Research here has studied , among other issues , the efﬁciency of decentralized , hierarchical structures ( Radner 1993 ) ; compared hier - archy and polyarchy as means of aggregating dichoto - mous choices ( Sah and Stiglitz 1986 ) ; and devised optimal weights for committee decision making ( Ben - Yashar and Nitzan 1997 ) . 2 2 A weighted averaging rule is optimal given an optimal set of weights . Yet computing these weights requires assumptions that are not particularly plausible from a behavioral standpoint . In § 4 . 4 C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View 4 Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS Voting models are also common in social choice theory , however , these models are different than the information aggregation models discussed in this paper : In the models we have reviewed , individuals share a utility function ( e . g . , they are all trying to make the best choice for a given ﬁrm ) . Differences of opinion arise because of errors in perception or rea - soning . In contrast , models of social choice assume that different individuals have different utility func - tions ( e . g . , individuals have different preferences and like different political candidates ) , which leads to the well - known impossibility theorem ( Arrow 1951 ) , that is , the inability to aggregate individual preferences into group - level preferences that satisfy a reasonable set of fairness criteria . 2 . 2 . How This Study Differs from Previous Studies Although the reviewed literatures have provided many valuable insights on the properties of differ - ent aggregation methods , these insights are difﬁcult to apply directly to organizational settings because organizational performance depends heavily on two contingencies—individuals’ knowledge and environ - mental change—that have not been accounted for in previous information aggregation models . This study extends previous information aggregation models by infusing these organizational contingencies . The ratio - nale for including both of these contingencies is dis - cussed below . First , our model considers individuals who are het - erogeneous with respect to their knowledge or exper - tise . In contrast , previous group decision - making research has conceptualized heterogeneity in terms of ability ( Grofman et al . 1983 , Kanazawa 1998 , Sorkin et al . 2001 , Soll and Larrick 2009 ) , where a high ability agent produces , on average , superior esti - mates of the project under consideration than low ability agents . On the other hand , differences in knowledge imply that a given individual may be inferior to another at assessing Project A , but supe - rior at assessing Project B . In the context of organi - zations , differences in knowledge are fundamental . Specialization and division of labor typically lead to decision - making teams involving members with dif - ferent expertises ( Bower 1970 , Hambrick and Mason 1984 ) . For instance , top management teams often include individuals with ﬁnancial , marketing , and operations expertise , who are not equally capable of assessing the quality of , for example , a new manufac - turing process . Second , our model considers decision making in both stable and changing environments . In contrast , we discuss these assumptions and use the optimal rule as a bench - mark against which to compare the performance of the structures studied in this paper . previous group decision models have usually stud - ied stable environments . In fact , Hastie and Kameda ( 2005 , p . 506 ) state , “how various group decision rules will perform in unstable environments is an open question . ” This gap may be a result of the fact that the organizations typically studied in group deci - sion making are formed on demand to solve a given problem in a given environment , and need not per - sist , as ﬁrms do , over longer periods of time that entail changes in the environment . Yet in the con - text of organizations , environmental change plays a central role . This is shown by the vast and diverse literatures on topics such as disruptive technological change ( Tushman and Anderson 1986 ) , organizational ecology ( Hannan and Freeman 1977 ) , and strategic ﬁt ( Siggelkow 2001 ) . Including both contingencies allows our model to shed light on when it is better to use which structure in terms of contingencies that are central to organi - zations . It also leads to a number of new results on the relative advantages of each structure . For instance , Hastie and Kameda ( 2005 ) found that averaging out - performed voting , and voting outperformed delega - tion . In our model , each one of these three structures can be the best performer , depending on the value of the contingencies we study . To further understand decision making in organizations , later in the paper we incorporate additional contingencies into the anal - ysis ( such as changing the number of decision mak - ers , including correlation among the individuals , and allowing for delegation errors ) . In sum , our approach extends previous models of information aggregation to account for distinctly organizational characteristics . We believe that introducing these elements allows our model to address important questions faced by organizations . 3 . Model The aim of our model is to study how organizational performance depends on organizational structure , the expertise of its members , and the organization’s external environment . We start by providing a brief overview of the model before describing each of its components in detail . The decision - making structures we analyze are in charge of screening a stream of projects—that is , approving good projects and rejecting the rest . Screen - ing projects or opportunities is a common task in many settings , including formal organizations such as movie studios where executives evaluate multi - million dollar projects ( we discussed other examples in § 1 ) and informal organizations such as moun - taineers deciding whether today is the best day for a summit attempt . The structures we study differ in how they aggregate the opinions of their mem - bers to produce organization - level decisions about the C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS 5 projects under review . For the sake of results com - parability and model parsimony , most of our analy - ses compare three structures with three members each ( we call these structures Delegation , Voting , and Aver - aging ) . For benchmarking purposes , we also analyze an organization with a single decision maker ( a struc - ture that we call Individual ) . Later in the paper we analyze variations of these structures and increase the number of decision makers . The projects screened by the organizations in our model are described by a type and a quality . The project type represents the domain of knowledge , or expertise , involved in accurately assessing the project ( e . g . , in the venture capital context , project types could correspond to semiconductors , software , biotechnology , etc . ) . Project quality represents how much value the project will create if implemented . Project quality is noisily perceived by individuals . The greater the distance between the individual’s expertise and the type of project being evaluated , the greater the noise in that individual’s perception of the project’s quality ( e . g . , a software expert will be more accurate at determining the value of a software startup than of a semiconductor startup ) . Organiza - tional performance is deﬁned as the sum of the quali - ties of the projects approved by the ﬁrm , divided by the total number of projects considered . 3 . 1 . Projects We deﬁne a project as a tuple (cid:52)q(cid:49)t(cid:53) , where q denotes the project’s quality and t its type . We interpret q as value , in terms of ( appropriately discounted ) rev - enues minus costs ; thus we say a project is good only if q > 0 . For simplicity , we assume that the dis - count rate is correctly set , that ﬁrms do not face liq - uidity constraints , and that there are no interactions among projects . Under these assumptions , ﬁrms max - imize their performance by accepting good projects and rejecting bad ones . The type ( t ) of the project is a real number that denotes the speciﬁc type of knowl - edge required to properly assess the project’s value . The actual value of t is relevant only with respect to the expertise of the individual decision makers , as discussed in § 3 . 2 . We deﬁne the project environment as the range of projects that a ﬁrm faces . We view this environment as a set of problems exogenously posed to the organi - zation . This view is consistent with multiple research traditions ( Hannan and Freeman 1977 , Carley and Lin 1997 ) . Thus , our model captures the evaluation of alternatives , not their generation ( Knudsen and Levinthal 2007 ) . Because projects are described by two parameters , the environment is deﬁned by the ranges that both q and t can take . We assume that projects are uniformly distributed in the rectangular interval deﬁned by (cid:54)q(cid:49) ¯ q(cid:55) and (cid:54)t(cid:49) ¯ t(cid:55) . 3 Continuing with our example , these bounds could reﬂect the range of projects a venture capital ﬁrm faces in terms of quality ( e . g . , values from − $ 100 million to $ 100 million ) and types ( e . g . , from 0 to 10 , where 0 represents hardware projects , 10 represents software projects , and interme - diate numbers represent mixtures between these two extremes ) . 4 3 . 2 . Individuals As with projects , we model individuals as having a type , which we call an expertise . Heterogeneity in expertise ( as discussed in § 2 . 2 ) is largely absent from the group decision - making literature , but is a common assumption in literatures on , for example , top management teams ( Hambrick and Mason 1984 ) and organizational learning ( Liang et al . 1995 ) . For any given project , the difference between a project’s type and the expertise of the individual assessing that project affects the level of noise in the individ - ual’s perception of project quality . For example , if a software expert is asked to evaluate a project that involves mostly software , her perception will likely be more accurate than the perception of a hardware expert . We model noisy perception as a signal plus noise , where the signal is the actual quality of the project ( q ) and the standard deviation of the noise is proportional to the distance between project type and individual expertise . 5 Mathematically , if an individual of expertise e is asked to assess the quality of project (cid:52)q(cid:49)t(cid:53) , she will perceive the quality of this project as q (cid:48) = q + ˜ n(cid:49) where ˜ n ∼ N(cid:52) 0 (cid:49) (cid:151) t − e (cid:151) (cid:53)(cid:48) ( 1 ) We thus denote the perceived quality as q (cid:48) . 6 Model - ing perception as signal plus noise is consistent with prior models ( e . g . , Einhorn et al . 1977 , Knudsen and Levinthal 2007 ) and with empirical work on manage - rial perceptions ( e . g . , Mezias and Starbuck 2003 ) . 3 In Appendix B we consider an alternative probability distribution of project types . 4 An alternative interpretation of (cid:54)t(cid:49) ¯ t(cid:55) is in terms of a ﬁltering mechanism ; that is , the ﬁrm ignores the projects that fall outside that range . 5 Our assumption that the project type and individual expertise are real numbers can be interpreted in a nonrestrictive manner ( à la Hotelling 1929 ) , as simply meaning that it is possible to compute a distance between a given project type and a given individual exper - tise . In other words , we make no assumptions about the dimen - sionality of the knowledge space and assume only that it is possible to compute distances in that space . 6 The noise term in Equation ( 1 ) implies that if expertise equals project type , perception is completely accurate . One might argue that this standard of perfection is too high , even for an expert with t = e . It is possible to relax this assumption by adding a baseline error rate , so that no individual can achieve perfect perception . This does not result in any qualitative change to our results , as we discuss in § 4 . 4 and Appendix B . C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View 6 Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS 3 . 3 . Structures In the spirit of Simon’s ( 1947 / 1997 , pp . 18 – 19 ) deﬁ - nition of organization as “the pattern of communica - tions and relations among a group of human beings , including the processes for making and implement - ing decisions , ” we model organizational structure as the mechanism that aggregates individual percep - tions into a group - level decision about each project reviewed . Although there are an unlimited number of poten - tial decision - making structures to study , we focus on four in the main part of this paper because they represent structures commonly used in real - world organizations . 7 These four structures are graphically summarized in Figure 1 . Each of these structures ( except for the ﬁrst ) consists of three individuals with expertise levels e L , e M , and e H such that e L ≤ e M ≤ e H . To ease presentation of results and simplify our dis - cussion of the homogeneity or heterogeneity of exper - tise , we express the three expertise levels in terms of two parameters , e M ( the expertise of the intermediate individual ) and (cid:130) ( the breadth of knowledge in the organization ) such that e L = e M − (cid:130) and e H = e M + (cid:130) . We discuss an alternative distribution of expertise in Appendix B . The following paragraphs describe how each of the four structures operates . Individual ( Figure 1 ( a ) ) . This is the simplest struc - ture ; it involves only one manager , with expertise e M , who is in charge of either accepting or rejecting projects . If he perceives a project to have a positive quality ( i . e . , q (cid:48) > 0 ) , he approves the project ; other - wise , he rejects it . We use this as a benchmark against which the other , more complex , structures are com - pared . This structure represents the settings where decisions are made by a single individual . Examples include many small and entrepreneurial ﬁrms as well as ﬁrms with a particularly powerful manager . This structure can also be characterized as highly central - ized , as authority is closely held by a single decision maker ( Huber and McDaniel 1986 , p . 581 ) . Delegation ( Figure 1 ( b ) ) . In this structure , projects are delegated to the manager whose expertise is clos - est to the type of the project being screened ( e . g . , if a project type t = 4 is assessed by a structure with experts e L = 0 , e M = 5 , and e H = 10 , the project would be assigned to e M ) . The assigned expert then accepts the project if she perceives it as a good one , or rejects it . We initially assume that the project is assigned to the manager whose expertise is closest to the project’s type . Later we relax that assumption by exploring the case of imperfect delegation , which may occur 7 In § 4 . 4 and Appendix C , we discuss a number of other structures , namely , structures in which individuals’ errors are correlated , struc - tures that optimally weight individuals’ judgments , and structures where individuals’ update their judgments after taking into account their level of competence . when the determination of project types or individ - ual expertise is subject to error . Delegation is the most decentralized structure . It represents settings such as engineering and consulting ﬁrms , where the evalu - ation of a project is usually assigned to the partner with the most relevant experience , or movie studios where a producer or executive might specialize in a certain genre and review all scripts that are pitched in that genre . Voting ( Figure 1 ( c ) ) . Under this structure , each of the three managers evaluates the project and the orga - nization makes a decision based on the vote of the majority ( i . e . , if two or more of the individuals per - ceive the project to have a positive quality then the project is approved ; otherwise , it is rejected ) . This structure represents boards of directors , partnerships such as venture capital ﬁrms , and egalitarian top management teams . Averaging ( Figure 1 ( d ) ) . This structure is similar to Voting ; however , instead of counting votes , the struc - ture averages the perceptions of the three individuals . If the average of the three perceived qualities is pos - itive , then the organization accepts the project ; oth - erwise , it is rejected . This structure could represent cases where committees try to ﬁne - tune their deci - sions to more fully incorporate the assessments of members . For example , if two managers have a mildly negative view of a project but the third manager has an extremely positive perception of it , this structure would yield acceptance of the project . One question this research tries to answer is why this structure , which arguably takes into account more information than Voting ( i . e . , continuous perceptions rather than yes / no votes ) , is not observed more often within organizations . It is , however , often used outside busi - ness organizations in attempts to harness the “wis - dom of crowds . ” Examples include averaging scores from movie critics ( e . g . , metacritic . com ) and creating a consensus earnings forecast based on estimates from multiple ﬁnancial analysts ( e . g . , First Call ) . 3 . 4 . Performance We deﬁne organizational performance as the quality of the projects accepted by a given structure using a given set of experts under a given external envi - ronment . We use the deﬁnitions presented previously to mathematically derive the organizational perfor - mance of each structure . Here we show how to derive this metric for the Individual structure ; the metrics for the other structures can be similarly derived and are summarized in Appendix A . Given Equation ( 1 ) , the probability that an individ - ual with expertise e approves a project of quality q and type t is (cid:16) (cid:52) approving a given project (cid:53) = S ind (cid:52)q(cid:49)t(cid:51)e(cid:53) = 1 − (cid:234)(cid:52) 0 (cid:49) (cid:151) t − e (cid:151) (cid:53) (cid:12)(cid:12) − q (cid:49) ( 2 ) C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS 7 Figure 1 Graphical Description of Each of the Structures Analyzed e M Reject Accept q (cid:1) > 0 q (cid:1) ≤ 0 ( a ) Individual ( b ) Delegation ( c ) Voting q H (cid:1) > 0 q M (cid:1) > 0 q L (cid:1) > 0 q H (cid:1) ≤ 0 q M (cid:1) ≤ 0 Reject e H e M e L Accept A ss i gn m e n t otherwise q L (cid:1) ≤ 0 t > e H + e M 2 t < e M + e L 2 ( d ) Averaging q (cid:1)(cid:1) > 0 q (cid:1)(cid:1) ≤ 0 Accept Reject e H e M e L 13 ( q L (cid:1) + q M (cid:1) + q H (cid:1) ) q (cid:1)(cid:1) = q H (cid:1) q H (cid:1) q M (cid:1) q M (cid:1) q L (cid:1) q L (cid:1) (cid:1) ≥ 2 Accept Reject e H e M e L (cid:1) < 2 (cid:1) = i ∈ { L , M , H } [ q i (cid:1) > 0 ] (cid:1) Note . The input to each structure is a project (cid:52)q(cid:49)t(cid:53) . where (cid:234)(cid:52)(cid:140)(cid:49)(cid:145)(cid:53) (cid:12)(cid:12) x is the cumulative distribution func - tion of an N(cid:52)(cid:140)(cid:49)(cid:145)(cid:53) evaluated at x . 8 In agreement with the previous literature ( e . g . , Sah and Stiglitz 1986 ) , we call this probability the “screening function of the organization . ” The previous probability is deﬁned for a single (cid:52)q(cid:49)t(cid:53) project . Hence , to obtain a performance met - ric for an environment of several projects , we com - pute expected values over the range of q - values and t - values that deﬁne that environment , that is , Ɛ (cid:52) quality of approved projects (cid:53) = 1 ¯ t − t 1 ¯ q − q (cid:90) ¯ t t (cid:90) ¯ q q qS ind (cid:52)q(cid:49)t(cid:51)e(cid:53) d q d t(cid:48) ( 3 ) We call this expected value the organization’s per - formance in a given environment , as it corresponds to the expected accrued quality per screened project . Simply put , performance is equivalent to the total values of the accepted projects ( i . e . , ﬁrm proﬁts ) normalized by the number of screened projects ( so that performance is comparable across organizations screening different numbers of projects ) . Because Equations ( 2 ) and ( 3 ) cannot be solved analytically , we analyze the model numerically . 8 See Csaszar ( 2012a ) for details . 4 . Results In this section we use the model to explore the rela - tionship between organizational structure , individual expertise , and the external environment . We do this in two stages . First , we study the effect of employee expertise ( i . e . , the “knowledge of the organization” ) on the performance of each structure for a given envi - ronment . Studying ﬁrms that differ with regard to the expertise of their employees serves to clarify the per - formance effects of knowledge diversity . Second , we investigate how changes in the project environment ( i . e . , the type of projects that constitute the stream of projects ) affect the performance of each structure . Studying the effect of different project environments can shed light on how robust a structure is when faced with an unexpected change in the environment ( e . g . , a radical technological change ) . We use graphical plots to intuitively and precisely convey the main results of the analysis . Each plot shows how performance ( on the y - axis ) varies as a function of knowledge breadth ( (cid:130) , on the x - axis ) for a ﬁxed range of project types (cid:54)t(cid:49) ¯ t(cid:55) and project qual - ities (cid:54)q(cid:49) ¯ q(cid:55) . To compare organizations within a given environment we look at one plot ; to compare across environments we look at two or more plots . To explore the model in a way that is amenable to analysis yet representative of its behavior under a C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View 8 Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS broad range of realistic conditions , we present results for a carefully chosen set of scenarios . We focus on these scenarios after extensively exploring the model and verifying that the presented results ( a ) capture the fundamentals of the model’s behavior and ( b ) are qualitatively robust with respect to the exact value of the inputs ( robustness checks are described in § 4 . 4 and Appendix B ) . In other words , analyzing the model under these values enables the reader to extrapolate results to other plausible scenarios . In general , two regularities allow us to keep the number of scenarios presented to a minimum . First , the effect of varying the range of project qualities (cid:54)q(cid:49) ¯ q(cid:55) is straightforward : As the quality of the port - folio increases , the performance of all the struc - tures increases monotonically and without affecting the performance ranking of the structures . Within the limit ( when project qualities become extremely positive or extremely negative ) , all structures per - form identically ( accepting or rejecting all projects , respectively ) . Thus , to understand the behavior of the model , it is enough to study it under one range of qualities ( in the ensuing analyses we use q ∼ U(cid:54) − 5 (cid:49) 5 (cid:55) ) . The second regularity is that , because the noise in individual perception is a function of the dif - ference between project type and individual exper - tise ( by Equation ( 1 ) , which sets noise proportional to (cid:151) t − e (cid:151) ) , to understand the behavior of the model it is enough to vary one of the elements of the differ - ence ( t or e ) while keeping the other ﬁxed ; in most of our analyses , we vary the expertise while keeping the range of project types ﬁxed . Within each plotted scenario ( deﬁned by a range of project types t and qualities q ) , we explore the behav - ior of the different structures while varying the degree of expertise heterogeneity within each structure . As mentioned previously , in most of our analyses the respective expertise of the three managers employed by Delegation , Voting , and Averaging are parameter - ized by the expertise e M of the middle manager and knowledge breadth (cid:130) , such that e L = e M − (cid:130) and e H = e M + (cid:130) . In the ﬁrst scenario analyzed , we set the exper - tise of the middle manager at the exact center of the range of project types ( i . e . , e M = (cid:52) ¯ t − t(cid:53) / 2 ) . This choice of e M minimizes the expected perception errors of that manager . This assumption is relaxed later , but we consider this choice of e M a reasonable starting point because mechanisms such as competition or learning could allow ﬁrms to discover that this is an optimal position for e M . We vary knowledge breadth ( (cid:130) ) from 0 ( i . e . , all individuals are identical ) to the value at which the three experts are maximally different but still remain within the range of the project types ( e . g . , if t ∼ U(cid:54) 0 (cid:49) 10 (cid:55) and e M = 5 , then (cid:130) max = 5 since this value leads to e L = 0 and e H = 10 ) . We structure the analysis as follows . First , we famil - iarize the reader with the inner workings of each structure by focusing on a base case . This helps uncover the basic mechanisms that relate knowledge breadth ( (cid:130) ) to performance for each structure . Second , we explore the effect of shifting the range of project types while keeping the expertise of the individuals ﬁxed . Third , we explore what happens to the perfor - mance of Delegation when we relax the assumption of perfect assignment of projects . Finally , we describe the robustness of the ﬁndings and important model extensions . More general implications of the model are discussed in § 5 . 4 . 1 . Base Case Figure 2 shows the performance of the four struc - tures as a function of knowledge breadth (cid:130) in a sin - gle environment ( q ∼ U(cid:54) − 5 (cid:49) 5 (cid:55) and t ∼ U(cid:54) 0 (cid:49) 10 (cid:55) ) , and under the assumption that the middle manager is at the middle of the project types being considered ( e M = 5 ) . To get a sense of the values of performance in the y - axis , note that a random decision maker would get a performance of 0 , while an omniscient one ( i . e . , someone approving all the good projects and rejecting all the bad ) would get a performance of 1 . 25 . 9 A ﬁrst look at this ﬁgure reveals several nontrivial relationships , including noticeable differ - ences between Averaging and Voting in addition to the strong performance of Delegation as knowledge breadth increases . The following paragraphs delve into the mechanisms explaining these and other dif - ferences in performance across structures . A baseline observation from Figure 2 , which also holds for all subsequent ﬁgures , is that the perfor - mance of the Individual structure is ﬂat with respect to knowledge breadth . This follows directly from the deﬁnition of this structure , in which (cid:130) does not play any role ; this structure involves only one individual , whose expertise is e M . Because this structure is so simple ( in terms of both structure and performance ) and because it serves as a building block of all the other structures , it makes a natural benchmark against which to compare the performance of the other struc - tures . This ﬁgure shows that the Individual struc - ture is the poorest performer ( with one exception in the lower right of the ﬁgure , which we discuss later ) . Thus , in our model , going from organizations of one individual to organizations of three individu - als is almost always associated with a positive effect on performance . 4 . 1 . 1 . Averaging vs . Voting . Perhaps the most striking observation from Figure 2 is that , although Voting and Averaging are intuitively similar , the 9 From Equation ( 3 ) , the performance of the omniscient decision maker is (cid:52) 1 / (cid:52) 10 − 0 (cid:53)(cid:53)(cid:52) 1 / (cid:52) 5 − (cid:52) − 5 (cid:53)(cid:53)(cid:53) (cid:82) 10 0 (cid:82) 5 − 5 q (cid:28) (cid:54)q > 0 (cid:55) d q d t = 1 (cid:48) 25 . C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS 9 Figure 2 Performance of the Four Structures in the Base Case 0 1 2 3 4 5 0 . 90 0 . 95 1 . 00 1 . 05 1 . 10 1 . 15 1 . 20 Knowledge breadth ( (cid:1) ) P e rf o r m a n ce IndividualDelegationVotingAveraging e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] relationship between performance and knowledge breadth in these two structures is quite different . The performance of Averaging peaks when its managers are the most similar ( i . e . , with no knowledge breadth , or (cid:130) = 0 ) , and it decreases rapidly as knowledge breadth increases . In contrast , Voting is less sensitive to knowledge breadth and exhibits a nonmonotonic behavior : Performance peaks at a moderate level of (cid:130) before declining slightly . The performance differ - ence between Voting and Averaging is particularly interesting in that everybody we informally surveyed beforehand believed that any difference between the performance of the two structures would be to the advantage of Averaging , which takes more infor - mation into account ( Averaging combines individual perceptions whereas Voting combines votes , which are nothing more than discretized versions of those individual perceptions ) . Moreover , this pro - Averaging argument seemed consistent with the well - known concept of applied statistics that running a statistical procedure ( e . g . , a regression ) on continuous variables , rather than on binary versions of them , produces bet - ter estimates . But these a priori arguments proved to be deceptive . The logic explaining the counterintuitive differ - ence between Voting and Averaging is that Averaging overweights perceptions of the less suitably informed individuals . For example , if experts e L = 1 , e M = 5 , and e H = 9 review a project of type t = 7 and quality q = 1 , then experts e M and e H would likely perceive the project with the least noise , putting their estimates very close to q = 1 . Meanwhile , expert e L would be as likely to have a q (cid:48) of − 5 as + 7 ( i . e . , q ± (cid:151) t − e (cid:151) = 1 ± 6 ) ; the decision to accept or reject the project would highly depend on the speciﬁc opinion of e L . Thus , under Averaging , the perception of the least suited individual can throw the organization - level decision completely off balance . In contrast , under Voting , no matter how mistaken the perception of any individ - ual , her effect on the ﬁnal decision is capped : She can only affect the vote count by one vote . The only instance where Averaging surpasses Vot - ing is when knowledge breadth is close to 0 ( i . e . , (cid:130) < 1 in Figure 2 ) . In this case , all the individuals are simi - larly qualiﬁed to evaluate the project , so no individual has undue power to throw the averaging process off balance . When individuals are identical and unequal noise ceases to be an issue , the additional information carried in the continuous signals of Averaging ( versus the discrete signals of Voting ) is beneﬁcial . In other words , when (cid:130) is low , the continuous signals used by Averaging do not come at a signiﬁcant cost and the informational advantage of these signals becomes observable . 10 An intriguing implication of the pos - sibility that Averaging may underperform Voting is that groups should not necessarily use all the infor - mation at their disposal . Sometimes , less information is better ( i . e . , discrete votes instead of full - ﬂedged perceptions ) . An application of the previous comparison is that if the organization designer is unsure about the actual expertise of the organization’s members then he may be better off choosing Voting rather than Averaging . In the opposite case , an organization designer who can perfectly choose the expertise of its members may be better off employing members of identical expertise and using Averaging rather than Voting . An instance of this advantageous use of Averaging could be in Olympic competitions , where scores are usu - ally computed by averaging the scores of the judges ( rather than by counting votes ) ; as arguably , the orga - nizers of the games can choose many judges that have similar expertise ( e . g . , ex - Olympians ) . In most other settings , conspicuously in business and govern - ment , where perfect calibration to a well - deﬁned task is unlikely to occur or hard to assess , the organiza - tion designer might better use Voting over Averaging . In sum , Voting structures are robust to miscalibrated members . This may explain why voting is so preva - lent in settings that face a variety of environments and high member turnover , such as boards of direc - tors and legislative bodies . 4 . 1 . 2 . Delegation . We now explore the behavior of Delegation , whose performance ( as shown in Fig - ure 2 ) also exhibits a distinct path , in this case , an inverted - U shape . We describe the performance of 10 In statistical terms , continuous signals have an informational advantage over discrete signals because they allow errors to be cancelled out more efﬁciently or , equivalently , because they have a greater asymptotic accuracy ( Casella and Berger 2002 , p . 470 ) . C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View 10 Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS Delegation in three steps . Initially , under knowledge homogeneity ( (cid:130) = 0 ) , Delegation performs identically to the Individual structure because any individual receiving a delegated project is identical to the only member of the Individual structure . As (cid:130) increases , so does the performance of Delegation . As the deci - sion makers become spread over a larger portion of the knowledge range , it becomes more likely that the expertise of the manager receiving the delegated project is close to the type of the project . Finally , the relationship between performance and (cid:130) peaks at an intermediate level of (cid:130) . To understand why the maxi - mal performance of Delegation occurs there , imagine three goalkeepers jointly trying to cover a goal that is 6 meters wide . Simple arithmetic shows that , if the goalkeepers position themselves 0 , 3 , and 6 meters from the left post , then , in the worst - case scenario , a goalkeeper will be 1 . 5 meters from the ball when it crosses the goal line . However , if the goalkeepers locate at positions 1 , 3 , and 5 , then one of them will be at most 1 meter from the ball . A similar reasoning explains why , under Delegation , the optimal knowl - edge breadth is less than the maximal (cid:130) ( i . e . , the value of (cid:130) that would locate experts at the corners of the range of project types ) . 11 In Figure 2 , Delegation becomes the best perform - ing structure at around (cid:130) = 1 (cid:48) 3 . One may wonder if this level of knowledge heterogeneity is not so high that , in reality , no one would plausibly consider using egalitarian rules such as averaging or voting after that level of (cid:130) ( because as (cid:130) increases , the least suitable individual becomes patently farther and farther off from the project being evaluated ) . This plausibility condition can be formally analyzed by comparing the accuracy of the best and worst expert at the point where Delegation becomes the superior structure . Fol - lowing Soll and Larrick ( 2009 ) , we measure this rela - tive accuracy by computing the mean absolute error ( MAE ) of the worst expert versus the MAE of the best expert . At the crossover point in Figure 2 , the MAE of the least suitable expert is about 1 . 6 times larger than the MAE of the best expert , which seems a plausible ratio to encounter in a real organization . 12 4 . 1 . 3 . Comparing Structures . An interesting per - spective emerges from reading Figure 2 normatively : Consistent with the main tenet of contingency the - ory ( Lawrence and Lorsch 1967 ) , there is no single 11 The performance of Delegation is highly contingent on the abil - ity to assign the project to the proper expert , an issue we explore in § 4 . 3 . 12 For projects uniformly distributed in the range (cid:54)t(cid:49) ¯ t(cid:55) × (cid:54)q(cid:49) ¯ q(cid:55) and structures with three experts , the ratio of the MAEs corresponds to MAE Worst MAE Best = (cid:82) ¯ t t max (cid:56) (cid:151) t − (cid:52)e M − (cid:130)(cid:53) (cid:151) (cid:49) (cid:151) t − e M (cid:151) (cid:49) (cid:151) t − (cid:52)e M + (cid:130)(cid:53) (cid:151) (cid:57) d t (cid:82) ¯ t t min (cid:56) (cid:151) t − (cid:52)e M − (cid:130)(cid:53) (cid:151) (cid:49) (cid:151) t − e M (cid:151) (cid:49) (cid:151) t − (cid:52)e M + (cid:130)(cid:53) (cid:151) (cid:57) d t (cid:48) best organizational structure . In fact , Delegation , Vot - ing , and Averaging each can be the best performer depending on the distribution of knowledge breadth ( (cid:130) ) . With perfect information about the knowledge breadth of the organization , one should Average ( when (cid:130) is low ) or Delegate ( otherwise ) . Thus , if (cid:130) is known , Voting is dominated by the combination of Averaging and Delegation . Yet with imperfect infor - mation about the knowledge breadth of the organiza - tion one might prefer to use Voting as it has lower variance , and in some regions , a higher mean than randomly choosing between Averaging and Delega - tion . 13 In other words , Averaging performs best when (cid:130) is low , Delegation performs best when (cid:130) is high , and Voting may be the best choice when (cid:130) is uncer - tain ( which could happen if the organization designer is uncertain about the individuals’ expertise or the projects’ types ) . We further discuss across - structure comparisons in § 5 . 1 . 4 . 2 . Changing Environment Here we explore how these structures perform when the environment changes . Speciﬁcally , we explore the case of a shift in the type of projects reviewed by the ﬁrm while the expertise of its members remains ﬁxed . This type of change is akin to the effect of a radical new technology , such as the shift from ana - log to digital photography ( Tripsas and Gavetti 2000 ) . For example , at Polaroid during this transformation , expertise related to hiring software engineers became more important , while expertise related to hiring chemists became less so . At the same time , because of organizational inertia and the unexpected nature of the change , an incumbent ﬁrm such as Polaroid could not adapt its expertise instantly and so had only its previous set of experts , who had been selected based on the older , analog environment . The analy - ses in this section shed light on what structures and levels of knowledge breadth can better cope with rad - ical environmental changes . In other words , we now study how structure and knowledge breadth can pro - tect ﬁrms that cannot instantly change their managers every time the environment varies . The panels in Figure 3 show four snapshots of per - formance as the environment gradually varies from the base case ( t ∼ U(cid:54) 0 (cid:49) 10 (cid:55) in the ﬁrst panel ) to a rad - ically different environment ( t ∼ U(cid:54) 15 (cid:49) 25 (cid:55) in the last panel ) . A ﬁrst observation from this ﬁgure is that the overall performance of the structures ( i . e . , the ranges on the y - axes ) decreases as we move from panel ( a ) to panel ( d ) . This happens because , as we advance through the panels , project types begin to drift away from the expertise of the ﬁrm , which remains ﬁxed 13 We thank an anonymous reviewer for highlighting this last point . C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS 11 Figure 3 Four Snapshots of Performance as the Range of Project Types Changes ( from t ∼ U(cid:54) 0 (cid:49) 10 (cid:55) in Panel ( a ) to t ∼ U(cid:54) 15 (cid:49) 25 (cid:55) in Panel ( d ) ) 0 1 2 3 4 5 0 . 90 0 . 95 1 . 00 1 . 05 1 . 10 1 . 15 1 . 20 P e rf o r m a n ce P e rf o r m a n ce P e rf o r m a n ce ( a ) e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] ( b ) e M = 5 t ~ U [ 5 , 15 ] q ~ U [ – 5 , 5 ] ( c ) e M = 5 t ~ U [ 10 , 20 ] q ~ U [ – 5 , 5 ] ( d ) e M = 5 t ~ U [ 15 , 25 ] q ~ U [ – 5 , 5 ] 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 . 70 0 . 75 0 . 80 0 . 85 0 . 90 0 . 95 1 . 00 1 . 05 0 . 35 0 . 40 0 . 45 0 . 50 0 . 55 0 . 60 0 . 65 0 . 70 P e rf o r m a n ce 0 . 25 0 . 30 0 . 35 Individual Delegation Voting Averaging Knowledge breadth ( (cid:1) ) Knowledge breadth ( (cid:1) ) Knowledge breadth ( (cid:1) ) Knowledge breadth ( (cid:1) ) ( at e L = 5 − (cid:130) , e M = 5 , and e H = 5 + (cid:130) ) . For exam - ple , in the last panel , experts centered around e M = 5 must screen projects with types between 15 and 25 , which leads to large evaluation errors and a conse - quent effect on performance . Another observation from Figure 3 is that , at each step , Averaging increasingly trumps Delegation as the best performer at low and middle levels of expertise breadth ( i . e . , in the ﬁrst panel , Averaging dominates until (cid:130) = 1 , whereas in the last panel , Averaging dom - inates over the entire range of (cid:130) ) . The rationale is that , as the environment shifts farther away from the ﬁrms’ decision makers , the decision makers’ errors in evaluating projects become increasingly similar in their distributions ( when the shift is extreme , the three decision makers become similarly incompetent in their ability to assess the projects under consid - eration ) . This convergence improves the relative per - formance of Averaging over Delegation , as averaging similarly uninformed opinions cancels out some of the error , whereas delegating to a single uninformed individual does not . Hence , a group could make com - paratively good decisions even if none of its members has the appropriate expertise to make a good decision individually . The explanation for the effect of environmental change on Delegation is quite different . Although Del - egation loses some terrain against Averaging ( and Voting ) in the ﬁrst three panels of Figure 3 , Dele - gation remains the top performer when knowledge breadth ( (cid:130) ) is high . This is because , when (cid:130) is high , the expert who receives the delegated project is better C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View 12 Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS equipped to deal with the new project than is the uneven mix of experts who participate in Averaging or Voting . To make the argument clearer , suppose that experts e L = 0 , e M = 5 , and e H = 10 are evaluating a project of type t = 12 , which is slightly above the level of e H ’s expertise . Delegation would send the project to e H , whose noisy perception would have a stan - dard deviation of 2 ( = (cid:151) 10 − 12 (cid:151) ) . Averaging or Voting would additionally use the opinions of the two other experts , whose noisy signals have a much higher stan - dard deviation ( of (cid:151) 0 − 12 (cid:151) = 12 and (cid:151) 5 − 12 (cid:151) = 7 ) and would thus throw off balance the relatively accurate perception of the other individual rather than cancel out its error . Thus , Delegation performs quite well in a changing environment as long as one of the deci - sion makers possesses expertise that is within or near the range of project types under consideration . As the expertise of the closest decision maker ( e H ) becomes less helpful in the new environment , the performance of Delegation falls below that of Averaging . One implication of Figure 3 is that Delegation com - bined with high breadth of expertise is an organiza - tion design that is robust to unexpected shifts in the range of project types , except at the most extreme lev - els of change ( i . e . , Figure 3 ( d ) ) . Observe that under the extreme change scenario , none of the ﬁrm’s original experts ( e between 0 and 10 ) is of much use in the new space ( t between 15 and 25 ) . However , a scenario like this is only descriptive of the most extreme environ - mental changes . Most changes in the real world prob - ably do not reach these levels . For instance , even after the shift to digital photography , Polaroid’s competen - cies in camera design and lens technology were still relevant . Therefore , one empirical implication of our model is that Delegation ( when knowledge breadth is high ) exhibits comparatively higher performance dur - ing most periods of environmental change . 4 . 3 . Imperfect Delegation Delegation has an important difference with respect to the other structures : It incorporates a ﬁrst stage dur - ing which the project is assigned to a single decision maker . So far , the model has assumed that projects are perfectly assigned to the person whose expertise is closest to the type of the project received . Yet in real life this assignment process can be imperfect . Errors in assignment can occur , for example , if project type or individual expertise are hard to assess , if there is no a reliable process for matching projects to experts , if there are political reasons for not assigning projects to the right experts , or if the right expert is unavailable . Studying the sensitivity of Delegation to errors of assignment is particularly relevant because our analy - ses so far have identiﬁed a wide range of cases where Delegation is the top - performing structure . To account for possible assignment imperfections in Delegation , we introduce a new parameter , r , Figure 4 Performance of Delegation Under Imperfect Assignment , Where Parameter r Denotes the Error Rate 0 1 2 3 4 5 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 P e rf o r m a n ce r = 0 r = 0 . 2 r = 0 . 4 r = 0 . 6 r = 0 . 8 r = 1 e M = 5 t ~ U [ 0 , 10 ] q ~ U [ 5 , 5 ] Knowledge breadth ( (cid:1) ) which captures error in the assignment process . When r = 0 , projects are always assigned to the best expert ; when r = 1 projects are randomly assigned among the three experts . Intermediate values of r interpolate between these two extremes . More formally , param - eter r affects the delegation process as follows : With probability r / 3 , the project is erroneously assigned to any of the two least suitable experts ; otherwise ( with probability 1 − 2 r / 3 ) the incoming project is assigned to the best suited expert . A mathematical deﬁnition of imperfect delegation is provided in Appendix A . Figure 4 plots the performance of Delegation for different values of r under the base case environ - ment ( t ∼ U(cid:54) 0 (cid:49) 10 (cid:55) ) . 14 The main observation from this ﬁgure is that Delegation is quite sensitive to assign - ment errors . For example , if the error rate rises from 0 % to 20 % ( compare lines r = 0 and r = 0 (cid:48) 2 in Fig - ure 4 ) , then the peak performance of Delegation falls from roughly 1 . 20 to 1 . 12 . Comparing Figures 2 and 4 shows that when r is slightly above 0 . 2 , Delega - tion falls below the peaks for Averaging ( for low (cid:130) ) and Voting ( for high and medium (cid:130) ) . An error rate slightly above 0 . 6 would render Delegation the worst - performing of the structures ( i . e . , even worse than the Individual structure for high levels of (cid:130) ) . Delegation’s sensitivity to assignment errors is an important char - acteristic to keep in mind when considering manage - rial implications . 4 . 4 . Robustness Checks and Extensions We studied the robustness of our ﬁndings with respect to a broad range of parameter values and 14 The results for other environments are qualitatively equivalent . C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS 13 model speciﬁcations . These tests included ( a ) setting individuals’ expertise in a random ( rather than sym - metric ) fashion , ( b ) adding a base error rate to the individual perceptions , ( c ) scaling the individual’s errors , and ( d ) changing the probability distributions of the project types . In general , our results are quali - tatively robust to changes in all of these assumptions . A representative set of the robustness tests is shown in Appendix B . We also studied three extensions to our model , which provide interesting results and illustrate how our model could be used as a platform to explore additional issues of information aggregation in orga - nizations . We include these analyses as extensions , not as part of the main results . Each of them could be analyzed in more detail , but such detailed analysis is outside the scope of this initial paper . Nevertheless , we believe that there is value in brieﬂy describing each of these three modeling extensions and the main results that emerge from them . Figures and additional details are given in Appendix C . The ﬁrst extension we consider is the effect of cor - related errors among the experts , an analysis in the spirit of Clemen and Winkler ( 1985 ) , which repre - sents a of phenomena such as groupthink ( Janis 1972 ) or herding ( Bikhchandani et al . 1992 ) . Results of this analysis ( see Figure C . 1 in Appendix C ) show that the performance of both Averaging and Voting declines rapidly as the errors of the decision makers become more correlated . This suggests that error correlation among experts can harm the performance of Voting and Averaging , similar to the way that assignment errors can harm the performance of Delegation . The second extension is to place optimal weights on the experts’ opinions in Voting and Averaging , an analysis in the spirit of Ben - Yashar and Nitzan ( 1997 ) . Calculating these optimal weights is compu - tationally complex and requires precise information on the incoming project and individuals’ expertise . It is not , therefore , particularly plausible from a behav - ioral point of view . Still , comparing the performance of the simple structures against optimal benchmarks is informative . The results of this analysis ( see Fig - ure C . 2 in Appendix C ) show that optimal Averaging dominates all the other structures for all levels of (cid:130) . This is reasonable , as Delegation is a particular case of optimal Averaging ( i . e . , put all the weight in the best expert ) . In § 4 . 1 we had shown that the combination of Averaging and Delegation dominated performance in the base case . More interestingly , the performance of optimal Averaging is never more than 5 % above the performance of the best among the simple versions of Averaging and Delegation studied in this paper . This is in line with classic results on the power of sim - ple heuristics ( e . g . , Einhorn and Hogarth 1975 , Dawes 1979 , Gigerenzer and Goldstein 1996 ) . The last extension considers individuals who do not report their true opinions , but an adjusted version of them . This adjustment process captures the idea that an individual may realize that she is not competent to analyze a given project and thus may decide to correct her estimate by reporting one that is closer to the typ - ical project that the organization receives . We model the correction phase by updating the individual’s per - ception , q (cid:48) , into a convex combination between the perception and the mean project , that is , q (cid:48) corrected = f (cid:52) · (cid:53)q (cid:48) + (cid:52) 1 − f (cid:52) · (cid:53)(cid:53)(cid:52)q + ¯ q(cid:53) / 2 . We interpret the function f (cid:52) · (cid:53) as self - conﬁdence ( i . e . , when f (cid:52) · (cid:53) = 1 the indi - vidual only takes into account her own perception ) . This is akin to the parameter weight on self , ws , in Soll and Larrick ( 2009 ) . We consider two shapes for the function f (cid:52) · (cid:53) : ( a ) one that decreases monotonically as the expert moves farther away from the incom - ing project type ( i . e . , f (cid:52) · (cid:53) decreases with (cid:151) t − e (cid:151) ) , and ( b ) one that is U - shaped with respect to (cid:151) t − e (cid:151) ( e . g . , a competent expert reports what she perceives , a half - competent expert reports an average between what she perceives and the average project , and an incom - petent expert does not know she is incompetent and simply reports what she perceives ) . Alternative ( a ) is consistent with Bayesian decision making ; alternative ( b ) is consistent with empirical research on percep - tions of competence ( Kruger and Dunning 1999 ) . The results for this extension ( see Figure C . 3 in Appendix C ) show that having individuals that update their estimates affects Averaging the most . For alternative ( a ) , the performance of Averaging improves , as the potentially damaging opinions of the least informed individuals are now made less extreme . For alternative ( b ) , the performance of Aver - aging falls drastically when knowledge breadth ( (cid:130) ) is high . This is reasonable , as the uninformed individu - als are unaware of their incompetence ; their unﬁltered opinions throw Averaging off balance . For both alter - natives , the performance of Voting remains robust , and Delegation remains the best performer at higher levels of (cid:130) . In sum , how individuals behave when they are incompetent affects structures differently . Voting remains robust , while the relative performance of Averaging can vary wildly . 5 . Discussion This study introduces a mathematical model involv - ing organizational structure , individual expertise , and the external environment . The model extends pre - vious models of information aggregation by infus - ing organizational elements such as heterogeneous knowledge and a changing external environment . We use the model to compare the performance of four decision - making structures : an Individual deci - sion maker , a Delegation process , a Voting body , C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View 14 Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS and an Averaging body . The results demonstrate that structure , expertise , and environment are tightly inter - connected in their effect on organizational perfor - mance . Below we discuss practical uses of the model , as well as connections between our work and crowd - based decision making and organizational adaptation . 5 . 1 . Managerial Application An application of the model consists in recommend - ing a decision - making structure based on given values for the parameters . Decision trees are a concise way to organize model recommendations . Figure 5 shows one such decision tree , which encapsulates the struc - ture comparison for the base case ( § 4 . 1 ) and recom - mends the most appropriate structure , assuming that the environment is static and that knowledge breadth is given ( i . e . , managers of the organization cannot be replaced or retrained ) . The decision tree takes into account four conditions ( depicted as boxes ) , which we describe brieﬂy , starting from the top . The ﬁrst con - dition is a cost – beneﬁt : If the cost of employing three decision makers does not compensate for its beneﬁts , then the Individual structure is preferable ( this argu - ment assumes that three decision makers are costlier than one ) . The second and third conditions relate to knowledge breadth ( (cid:130) ) : If (cid:130) is unknown ( as when expertise is difﬁcult or costly to assess ) , Voting offers a robust performance ; if (cid:130) is low , Averaging offers the best performance ; and if (cid:130) is high , the prefer - able structure will depend on the likelihood of dele - gation errors . If delegation errors are likely , Voting is Figure 5 Decision Tree That Recommends the Best Structure as a Function of Knowledge Breadth (cid:52)(cid:130)(cid:53) and Delegation Errors (cid:52)r(cid:53) , and Assuming a Static Environment Yes Individual Delegation Voting Averaging No No Yes Voting Yes No Yes No Are delegation errors likely to occur ? Is cost an issue ? Is (cid:1) known ? Is (cid:1) low ? a good choice ; otherwise , Delegation should be used . The ﬁgure shows that each of the four structures can be a viable choice under the appropriate set of con - ditions ( i . e . , the four structures appear as terminal nodes of the tree ) . This underscores the contingent nature of organizational design ( Lawrence and Lorsch 1967 , Nickerson and Zenger 2004 ) . Similar decision trees can be constructed for the three remaining combinations that stem from consid - ering knowledge breadth as given or controllable and the environment as stable or changing as well as for the extensions mentioned in § 4 . 4 . We do not present these trees here for the sake of brevity ( in any case , they can be inferred from the Results section and Appendix C ) . Construction of the model implies some impor - tant boundary conditions on the results . First , in the spirit of Hotelling ( 1929 ) , we assume that it is pos - sible to measure a unidimensional distance between project type and individual expertise . Theoretically ( as explained in Footnote 5 ) , this assumption is not very restrictive . In practice , it could be that the dis - tance between e and t is hard to measure accurately , leading to noisy assignments as described in the con - text of imperfect Delegation ( § 4 . 3 ) . Second , our model assumes no interactions between projects , in terms of capacity constraints , trade - offs , or complementarities . The presence of signiﬁcant interactions might increase the appeal of centralized structures ( Siggelkow and Rivkin 2005 ) . Third , our model does not incorporate the cost of decision makers . Cost considerations could affect the overall level of the performance curves , and thus make some structures unproﬁtable . Fourth , we have focused on the process of alternative evaluation only ; the model says nothing about how structure affects the performance of other stages of organiza - tional decision making . 5 . 2 . Delegation or the “Wisdom of Crowds” ? The wisdom of crowds ( Surowiecki 2004 , Kameda et al . 2011 ) and related concepts such as prediction markets ( Wolfers and Zitzewitz 2004 ) have received signiﬁcant coverage in practitioner literature . The general tone of this coverage is that choices made by crowds are superior to the results of traditional , delegative decision making in ﬁrms . For example , a recent article in the Wall Street Journal ( Murray 2010 ) postulates that crowd decision making would cause “the end of management . ” Yet , casual observation indicates that delegation is probably the most preva - lent form of decision making in ﬁrms . Are ﬁrms wrong to choose delegation over crowds ? Our model can shed light on when delegation is superior to crowd - based decision making and vice versa . To compare Delegation with a crowd , we cre - ated Voting and Averaging models with increasingly larger numbers of decision makers to determine at C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS 15 Figure 6 Performance of Voting and Averaging as a Function of the Number of Decision Makers (cid:52)N(cid:53) 0 1 2 3 4 5 0 . 95 1 . 00 1 . 05 1 . 10 1 . 15 1 . 20 P e rf o r m a n ce Voting ( N = 3 ) Voting ( N = 5 ) Voting ( N = 7 ) Voting ( N = 9 ) Delegation ( N = 3 ) 0 1 2 3 4 5 0 . 90 0 . 95 1 . 00 1 . 05 1 . 10 1 . 15 1 . 20 P e rf o r m a n ce Averaging ( N = 3 ) Averaging ( N = 5 ) Averaging ( N = 7 ) Averaging ( N = 15 ) Delegation ( N = 3 ) e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] Knowledge breadth ( (cid:1) ) Knowledge breadth ( (cid:1) ) what point these models dominate Delegation . Fig - ure 6 shows that , in the base case scenario , Voting with nine individuals dominates Delegation , while Averaging requires ﬁfteen individuals . The numbers in this analysis should not be read literally ; they depend on a number of assumptions ( i . e . , we used the base case assumptions , placed the N experts evenly over the knowledge range , and assumed uncorrelated errors ) . In real life , although the number of individ - uals in a crowd may be much larger than nine or ﬁfteen , their knowledge breadth may be narrow or their errors correlated ( as discussed in § 4 . 4 and in Appendix C . 1 ) , thus acting equivalently to a small group and greatly reducing the overall performance of the crowd . Also , in many situations accessing the crowd may be impractical for cost or conﬁdentiality reasons . Still , the main message of this analysis is sug - gestive : A well chosen crowd can surpass the perfor - mance of Delegation . Thus , this model adds to the conversation ( e . g . , Hastie and Kameda 2005 , Kameda et al . 2011 ) about distinguishing between the wisdom ( Surowiecki 2004 ) and the madness ( Mackay 1852 ) of crowds . Another way of thinking about crowds versus del - egation is in terms of “crowdlike” decision mak - ing within the ﬁrm . Averaging and Voting are more crowdlike than Delegation , as they take into account the opinion of more individuals . They also aggre - gate this information in a more egalitarian way . In our models with three decision makers , Delegation outperforms crowdlike structures ( Voting or Averag - ing ) when the projects assessed by the ﬁrm are well matched by the expertise of the delegated individual ( in terms of the model , when (cid:130) is moderate or high ) , when signiﬁcant environmental shocks are unlikely to make existing ﬁrm expertise obsolete , and when assignment errors are relatively rare ( r is low ) . Other - wise , one of the crowdlike structures is preferable . We conjecture that the conditions under which Delegation outperforms the other structures have been preva - lent throughout business history , so that ( in the sense of organizational evolution ) Delegation has become the default decision structure for ﬁrms . A further managerial implication is that crowdlike mechanisms should be considered in settings that deviate from the conditions above . A related question is why , when ﬁrms use a crowd - like mechanism , it is usually by means of Voting and only rarely by means of Averaging . Prior work has suggested that the Averaging structure can be costly : The process of arriving at a speciﬁc number can be more time - consuming and cognitively demand - ing than Voting’s thumbs up – thumbs down approach or Delegation’s division of labor ( Hastie and Kameda 2005 ) . Similarly , the value of some potential projects may be difﬁcult to distill down to a single value , mak - ing Averaging impractical . Our model suggests two additional reasons that most ﬁrms make little use of Averaging structures . First , it may be challenging to ensure that all decision makers are similar in terms of expertise ( i . e . , (cid:130) is close to 0 in Figure 2 ) , either because doing so is costly or because organizations ( and their members ) will change over time . Second , Averaging is more susceptible than Voting to agency concerns , because Averaging would overweight an outlier opin - ion even if that opinion were based on personal pref - erences rather than on facts . In contrast , Voting is less susceptible to this bias because the negative effect of a dishonest individual is capped at one vote . 5 . 3 . Structure and Environmental Change A long - standing question in management is how ﬁrms can use organizational structure to cope with C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View 16 Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS environmental change . Some of the early structures that have been suggested in this regard are Organic ( Woodward 1965 ) , Prospector ( Miles and Snow 1978 ) , and Adhocracy ( Mintzberg 1979 ) . Our work sug - gests that there is no single correct structure to address environmental change and that the proper choice depends on internal and external contingen - cies , namely , the knowledge of the organization’s members and the knowledge required by the envi - ronment . Speciﬁcally , for a ﬁrm in which knowledge remains relevant in the changing environment , Del - egation makes the most sense . This ﬁnding aligns with work on technological change , which suggests that ﬁrms could adapt more successfully by plac - ing authority in the hands of , for example , scientists with directly relevant knowledge , instead of execu - tives who may be wedded to existing business mod - els ( Christensen and Bower 1996 , Tripsas and Gavetti 2000 , Eggers and Kaplan 2009 ) . However , if the ﬁrm does not have the knowledge relevant to the new environment ( either because the individuals’ expertise is extremely homogeneous or because the environ - mental shift is radical ) , it would be more advanta - geous for the ﬁrm to use Voting or Averaging . 5 . 4 . Future Work Our research can be extended in several directions . Possible avenues for empirical work include ( a ) test - ing the model’s predictions ( e . g . , test the hypotheses implicit in the decision tree of Figure 5 ) ; ( b ) devel - oping ethnographic studies to better understand how real organizations use and misuse the mechanisms proposed here ; and ( c ) measuring the effect sizes asso - ciated with the model’s parameters , as well as the variance explained by information aggregation versus other ﬁrm - and industry - level characteristics . Possible avenues for theoretical work include ( a ) pre - dicting other performance metrics , such as measures of variance or risk , and errors of omission and com - mission ; ( b ) incorporating other processes into the model , such as incentives and learning ; and ( c ) com - paring the performance of a broader set of structures , e . g . , organizations , with dynamic decision rules or complex network structures . We believe that models of information aggregation , such as the one presented in the current study , hold great promise for the study of organizations because they are fully aligned with the main tenets of behavioral theory of the ﬁrm , i . e . , that an organization’s main task is to process infor - mation and that organizational structure determines how information is aggregated . 5 . 5 . Conclusion Our study is a step toward better understand - ing of the links between information aggregation and the process of organizational decision mak - ing . We uncover complex interdependencies between organizational structure , individual expertise , and environmental change . From a practical standpoint , our results shed light on which structure to use when . Some of the questions we have addressed are why Delegation is a common structure for organizations in more stable environments while voting is more common under changing environments or member - ships ; how structure can be used to cope with rad - ical environmental change ; and when organizational decision making is better off using less ( rather than more ) information from its members . From a theoret - ical standpoint , we address several calls to enrich our understanding of organizations by describing a causal mechanism that links the structure of organizational decision making to organizational performance . Over - all , our results underscore the critical role of informa - tion aggregation on organizations . Acknowledgments For helpful comments , the authors thank Natalia Karelaia , Dan Levinthal , Gabriel Natividad , Narayan Pant , David Ross , José Santos , Melissa Schilling , Gus Stuart , Timothy Van Zandt , and Peter Zemsky , as well as the associate edi - tor and three anonymous referees . The authors also thank the seminar participants at INSEAD , New York University , the University of Michigan , the Tuck School of Business at Dartmouth , the Washington University in St . Louis , the European School of Management and Technology , Theoreti - cal Organizational Models Society Meeting at Barcelona , the Atlanta Competitive Advantage Conference , the Academy of Management Conference , and the Utah – Brigham Young University Winter Conference . The authors thank Naveen Naidu Narisetty for his contributions as a research assistant . All errors remain the authors’ own . Appendix A . Screening Function of Each Structure The respective screening functions for each of the four struc - tures ( plus imperfect delegation ) are detailed here ( these equations are the equivalents of Equation ( 2 ) ) . The per - formance of each structure can be computed by evaluat - ing Equation ( 3 ) with the corresponding screening function . A detailed log that includes the derivation of the metrics is available from the authors , as is simulation code that veri - ﬁes that these derivations are correct . A . 1 . Individual S ind (cid:52)q(cid:49)t(cid:51)e(cid:53) = 1 − (cid:234)(cid:52) 0 (cid:49) (cid:151) t − e (cid:151) (cid:53) (cid:151) − q (cid:48) A . 2 . Delegation S del (cid:52)q(cid:49)t(cid:51)e L (cid:49)e M (cid:49)e H (cid:53) =   S ind (cid:52)q(cid:49)t(cid:51)e L (cid:53) if t < (cid:52)e M + e L (cid:53) / 2 (cid:49) S ind (cid:52)q(cid:49)t(cid:51)e M (cid:53) if (cid:52)e M + e L (cid:53) / 2 ≤ t ≤ (cid:52)e M + e H (cid:53) / 2 (cid:49) S ind (cid:52)q(cid:49)t(cid:51)e H (cid:53) if t > (cid:52)e M + e H (cid:53) / 2 (cid:48) A . 2 (cid:48) . Imperfect Delegation S del (cid:48) (cid:52)q(cid:49)t(cid:49)r(cid:51)e L (cid:49)e M (cid:49)e H (cid:53) =   S ind (cid:52)q(cid:49)t(cid:51)e 1 (cid:53) with (cid:16) (cid:52) 1 − 2 r / 3 (cid:53)(cid:49) S ind (cid:52)q(cid:49)t(cid:51)e 2 (cid:53) with (cid:16) (cid:52)r / 3 (cid:53)(cid:49) S ind (cid:52)q(cid:49)t(cid:51)e 3 (cid:53) with (cid:16) (cid:52)r / 3 (cid:53)(cid:49) C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS 17 where e 1 is the ﬁrst - best expert given project t ( i . e . , the num - ber among e L , e M , e H that is closest to t ) ; e 2 is the second - best expert ; and e 3 is the worst expert . A . 3 . Voting S vot (cid:52)q(cid:49)t(cid:51)e L (cid:49)e M (cid:49)e H (cid:53) = (cid:16) (cid:52)A(cid:49)A(cid:49)R(cid:53) + (cid:16) (cid:52)A(cid:49)R(cid:49)A(cid:53) + (cid:16) (cid:52)R(cid:49)A(cid:49)A(cid:53) + (cid:16) (cid:52)A(cid:49)A(cid:49)A(cid:53)(cid:49) where (cid:16) (cid:52)v L (cid:49)v M (cid:49)v H (cid:53) = (cid:89) i ∈ (cid:56)L(cid:49)M(cid:49)H(cid:57) (cid:20)(cid:26) S ind (cid:52)q(cid:49)t(cid:51)e i (cid:53) if v i = A ( individual i accepts ) (cid:49) 1 − S ind (cid:52)q(cid:49)t(cid:51)e i (cid:53) if v i = R ( individual i rejects ) (cid:21) (cid:48) Figure B . 1 Randomly Locating the Decision Makers Within the Range (cid:54)e M − (cid:130)(cid:49)e M + (cid:130)(cid:55) 0 1 2 3 4 5 0 . 95 1 . 00 1 . 05 1 . 10 1 . 15 P e rf o r m a n ce Knowledge breadth ( (cid:1) ) t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] Individual Delegation Voting Averaging Figure B . 2 Adding a Base Error Rate (cid:52)k(cid:53) to the Individuals’ Perceptions Knowledge breadth ( (cid:1) ) k = 0 . 5 e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] k = 1 . 0 e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] Individual Delegation Voting Averaging 0 1 2 3 4 5 Knowledge breadth ( (cid:1) ) 0 1 2 3 4 5 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 P e rf o r m a n ce P e rf o r m a n ce A . 4 . Averaging S avg (cid:52)q(cid:49)t(cid:51)e L (cid:49)e M (cid:49)e H (cid:53) = 1 − (cid:234) (cid:16) 0 (cid:49) 13 (cid:112) (cid:52)t − e L (cid:53) 2 + (cid:52)t − e M (cid:53) 2 + (cid:52)t − e H (cid:53) 2 (cid:17)(cid:12)(cid:12)(cid:12) − q (cid:48) Appendix B . Robustness Checks This appendix presents and discusses ﬁgures correspond - ing to the robustness checks mentioned in § 4 . 4 . The overall conclusion is that the results presented in the body of the paper are qualitatively robust under a broad range of model speciﬁcations . B . 1 . Randomly Locating the Experts Figure B . 1 plots the performance of structures in which the experts are not located symmetrically ( previously , e M − e L = e H − e M = (cid:130) ) . In this check , the position of each of the three individuals is randomly picked from the range (cid:54)e M − (cid:130)(cid:49)e M + (cid:130)(cid:55) ( where e M is no longer the position of the middle expert and instead just the middle of the ﬁrm’s expertise range ) . Comparing Figure B . 1 to the base case ( Figure 2 ) shows that the ordering of the lines remains unchanged . The performance of the Individual structure is now decreasing in (cid:130) because its expertise is no longer ide - ally positioned at e M . B . 2 . Adding a Base Error Rate to Individual Perceptions Equation ( 1 ) assumes that the standard deviation of the individuals’ perceptions is proportional to (cid:151) t − e (cid:151) . In this robustness check we explore the case of individuals whose standard deviation does not reach 0 even if their exper - tise perfectly matches the type of the project screened . We therefore add a constant k to the standard deviation of the individuals’ perceptions ( i . e . , the noise in Equation ( 1 ) becomes ˜ n ∼ N(cid:52) 0 (cid:49) (cid:151) t − e (cid:151) + k ) ) . Figure B . 2 plots the base case when k = 0 (cid:48) 5 and k = 1 ( left and right panels , respec - tively ) . The ﬁgure shows that , although overall performance C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View 18 Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS Figure B . 3 Scaling the Individuals’ Noise Level by Factor s s = 1 . 0 e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] Knowledge breadth ( (cid:1) ) 0 1 2 3 4 5 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 P e rf o r m a n ce s = 1 . 2 e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] Knowledge breadth ( (cid:1) ) 0 1 2 3 4 5 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 P e rf o r m a n ce s = 0 . 8 e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] Knowledge breadth ( (cid:1) ) 0 1 2 3 4 5 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 P e rf o r m a n ce IndividualDelegationVotingAveraging Figure B . 4 Performance of the Four Structures When Project Type Is Distributed According to t ∼ N(cid:52)(cid:140) t (cid:49)(cid:145) t (cid:53) e M = 5 (cid:1) t = 5 (cid:2) t = 3 q ~ U [ – 5 , 5 ] 0 1 2 3 4 5 Knowledge breadth ( (cid:3) ) 0 . 9 1 . 0 1 . 1 1 . 2 P e rf o r m a n ce e M = 5 (cid:1) t = 5 (cid:2) t = 4 q ~ U [ – 5 , 5 ] 0 1 2 3 4 5 Knowledge breadth ( (cid:3) ) 0 . 9 1 . 0 1 . 1 1 . 2 P e rf o r m a n ce e M = 5 (cid:1) t = 5 (cid:2) t = 2 q ~ U [ – 5 , 5 ] 0 1 2 3 4 5 Knowledge breadth ( (cid:3) ) 0 . 9 1 . 0 1 . 1 1 . 2 P e rf o r m a n ce IndividualDelegationVotingAveraging decreases ( compare left and right panels ) , the ordering relationship among the different structures’ performance remains unchanged . B . 3 . Scaling the Error in Individual Perceptions In this robustness check we explore how the results change if the magnitude of the errors is scaled by a factor s ( i . e . , the noise in Equation ( 1 ) becomes ˜ n ∼ N(cid:52) 0 (cid:49)s (cid:151) t − e (cid:151) (cid:53) . Figure B . 3 shows that the performance of all structures decreases when s increases . The relative ordering of the different structures , however , remains unchanged . B . 4 . Normally Distributed Project Types In the base case we assumed that the projects screened by the ﬁrm are uniformly distributed over (cid:54)t(cid:49) ¯ t(cid:55) . In this robust - ness check we explore a normal distribution of projects . Fig - ure B . 4 plots the base case but under t ∼ N(cid:52)(cid:140) t (cid:49)(cid:145) t (cid:53) rather than t ∼ U(cid:54)t(cid:49) ¯ t(cid:55) . Across the three panels , we assume (cid:140) t = 5 , and (cid:145) t = 2 , 3 , and 4 , respectively . Again , the relative order - ing of the different structures remains unchanged . Appendix C . Extensions This appendix provides the mathematical deﬁnitions and ﬁgures corresponding to the modeling extensions discussed in § 4 . 4 . C . 1 . Correlated Errors We modeled correlated errors by drawing individuals’ noises from a multivariate normal in which all the cross - correlations are set to (cid:144) . Mathematically , this is implemented by drawing ˜ n L , ˜ n M , ˜ n H from a multivariate normal with (cid:204) =   0 00   and covariance matrix (cid:232) = D (cid:208) D , where correlation matrix (cid:208) =   1 (cid:144) (cid:144) (cid:144) 1 (cid:144) (cid:144) (cid:144) 1   (cid:49) and standard deviations matrix D =   (cid:151) t − e L (cid:151) 0 0 0 (cid:151) t − e M (cid:151) 0 0 0 (cid:151) t − e H (cid:151)   (cid:48) To analyze the effect of correlated errors , each of the panels in Figure C . 1 uses the same parameters as in the base case ( e M = 5 , q ∼ U(cid:54) − 5 (cid:49) 5 (cid:55) , t ∼ U(cid:54) 0 (cid:49) 10 (cid:55) ) while varying parameter (cid:144) from 0 ( no correlation ) to 1 ( perfect correlation ) in 0 . 2 increments . C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS 19 Figure C . 1 Performance of the Four Structures as the Correlation (cid:52)(cid:144)(cid:53) of the Perceptual Errors Varies from 0 ( in the First Panel ) to 1 ( in the Last Panel ) Individual Delegation Voting Averaging 0 1 2 3 4 5 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 0 1 2 3 4 5 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 P e rf o r m a n ce P e rf o r m a n ce P e rf o r m a n ce P e rf o r m a n ce P e rf o r m a n ce P e rf o r m a n ce 0 1 2 3 4 5 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 0 1 2 3 4 5 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 0 1 2 3 4 5 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 0 1 2 3 4 5 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 ( a ) (cid:1) = 0 . 0 e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] ( b ) (cid:1) = 0 . 2 e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] ( c ) (cid:1) = 0 . 4 e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] ( d ) (cid:1) = 0 . 6 e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] ( e ) (cid:1) = 0 . 8 e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] ( f ) (cid:1) = 1 . 0 e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] Knowledge breadth ( (cid:2) ) Knowledge breadth ( (cid:2) ) Knowledge breadth ( (cid:2) ) Knowledge breadth ( (cid:2) ) Knowledge breadth ( (cid:2) ) Knowledge breadth ( (cid:2) ) C . 2 . Optimally Weighted Structures To create optimally weighted versions of Voting and Aver - aging in the spirit of Ben - Yashar and Nitzan ( 1997 ) , we mathematically speciﬁed the structures and let a numeri - cal algorithm ﬁnd the optimal parameters ( as a function of (cid:130) and the environment (cid:54)q(cid:49) ¯ q(cid:55) , (cid:54)t(cid:49) ¯ t(cid:55) ) . Note that the closed - form derivations of Ben - Yashar and Nitzan ( 1997 ) cannot be directly used in our model , as they only apply to Voting and do not take into account project types ( i . e . , parameters q and t , which are essential to our model ) . We speciﬁed the optimal structures as follows : • Optimally Weighted Voting only accepts a project if w L (cid:28) (cid:54)q (cid:48) L > (cid:146)(cid:55) + w M (cid:28) (cid:54)q (cid:48) M > (cid:146)(cid:55) + w H (cid:28) (cid:54)q (cid:48) H > (cid:146)(cid:55)(cid:53) > 12 , and • Optimally Weighted Averaging only accepts a project if w L q (cid:48) L + w M q (cid:48) M + w H q (cid:48) H > (cid:146) , where w L , w M , w H represent the optimal weights for each individual , and (cid:146) is the optimal approval threshold . Opti - mally Weighted Voting is deﬁned in this fashion in Ben - Yashar and Nitzan ( 1997 , p . 181 ) ; Optimally Weighted Averaging is a natural translation of this idea to the realm of Averaging . Figure C . 2 compares Voting and Averaging to their optimal counterparts as well as to Delegation . C . 3 . Regression to the Mean of Reported Judgments In this extension individuals “correct” their judgements before reporting them . The corrected perception is deﬁned as q (cid:48) corrected = f(cid:52) · (cid:53)q (cid:48) + (cid:52) 1 − f(cid:52) · (cid:53)(cid:53)(cid:52)q + ¯ q(cid:53) / 2 , where 0 ≤ f(cid:52) · (cid:53) ≤ 1 represents a self - conﬁdence function . We explored two dif - ferent speciﬁcations for f(cid:52) · (cid:53) : • Monotonically decreasing . Under this function , if an expert receives a project that perfectly matches her type ( i . e . , Figure C . 2 Performance of Voting and Averaging With and Without Optimal Weights 0 1 2 3 4 5 0 . 90 0 . 95 1 . 00 1 . 05 1 . 10 1 . 15 1 . 20 P e rf o r m a n ce DelegationVotingVoting with optimal weights AveragingAveraging with optimal weights e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] Knowledge breadth ( (cid:1) ) C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View 20 Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS Figure C . 3 Performance of the Structures Under Individuals that Correct Their Judgments According to Self - Conﬁdence 0 1 2 3 4 5 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 P e rf o r m a n ce Knowledge breadth ( (cid:1) ) 0 1 2 3 4 5 0 . 8 0 . 9 1 . 0 1 . 1 1 . 2 P e rf o r m a n ce Knowledge breadth ( (cid:1) ) f = f L e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] f = f U e M = 5 t ~ U [ 0 , 10 ] q ~ U [ – 5 , 5 ] IndividualDelegation Voting Averaging Notes . Left : Self - conﬁdence is linearly related to accuracy . Right : Self - conﬁdence is U - shaped with respect to accuracy . t = e ) , her self - conﬁdence is 1 . As the difference between t and e increases , self - conﬁdence linearly decreases . At the extreme , when the difference between t and e is maximal ( i . e . , the expert receives the project that it is farther away in the project range ) , self - conﬁdence becomes 0 . Mathemat - ically , this function is deﬁned as f L (cid:52)t(cid:49)e(cid:53) = min (cid:52) 0 (cid:49) 1 −(cid:151) t − e (cid:151) / (cid:133) max (cid:53)(cid:49) where (cid:133) max = max (cid:56) (cid:151)¯ t − e (cid:151) (cid:49) (cid:151) t − e (cid:151) (cid:57)(cid:48) • U - shaped . To model the U - shaped self - conﬁdence function we created a “Mexican hat” – like function ( ) , which achieves a maximum when t = e , minima at some intermediate points t = e ± (cid:139)(cid:52) ¯ t − t(cid:53) , and from there on increases until reaching a plateau level when t = e ± (cid:52) ¯ t − t(cid:53) . This function is controlled by four parameters : y 1 ( level of maximum ) , y 2 ( level of the minima ) , y 3 ( plateau level ) , and (cid:139) ( position of the minima as a fraction of the range of possible project types ) . Mathematically , this function is deﬁned as f U (cid:52)t(cid:49)e(cid:53) =     y 3 + y 2 − y 3 (cid:52)e − (cid:139)(cid:52) ¯ t − t(cid:53)(cid:53) − (cid:52)e − (cid:52) ¯ t − t(cid:53)(cid:53)(cid:52)t − (cid:52)e − (cid:52) ¯ t − t(cid:53)(cid:53)(cid:53) if e − (cid:52) ¯ t − t(cid:53) ≤ t < e − (cid:139)(cid:52) ¯ t − t(cid:53)(cid:49) y 2 + y 1 − y 2 e − (cid:52)e − (cid:139)(cid:52) ¯ t − t(cid:53)(cid:53)(cid:52)t − (cid:52)e − (cid:139)(cid:52) ¯ t − t(cid:53)(cid:53)(cid:53) if e − (cid:139)(cid:52) ¯ t − t(cid:53) ≤ t < e(cid:49) y 1 + y 2 − y 1 (cid:52)e + (cid:139)(cid:52) ¯ t − t(cid:53)(cid:53) − e(cid:52)t − e(cid:53) if e ≤ t < e + (cid:139)(cid:52) ¯ t − t(cid:53)(cid:49) y 2 + y 3 − y 2 (cid:52)e + (cid:52) ¯ t − t(cid:53)(cid:53) − (cid:52)e + (cid:139)(cid:52) ¯ t − t(cid:53)(cid:53)(cid:52)t − (cid:52)e + (cid:139)(cid:52) ¯ t − t(cid:53)(cid:53)(cid:53) if e + (cid:139)(cid:52) ¯ t − t(cid:53) ≤ t < e + (cid:52) ¯ t − t(cid:53)(cid:49) y 3 otherwise (cid:48) To study these adjustment processes , the panels in Fig - ure C . 3 plot the performance of the four main structures , under f L and f U ( this last one , assuming (cid:139) = 0 (cid:48) 25 , y 1 = 1 , y 2 = 0 , and y 3 = 0 (cid:48) 8 ) , using the same assumptions as in the base case ( e M = 5 , q ∼ U(cid:54) − 5 (cid:49) 5 (cid:55) , t ∼ U(cid:54) 0 (cid:49) 10 (cid:55) ) . References Aristotle ( c . 330 BCE / 1984 ) Politics . Barnes J , ed . Complete Works of Aristotle : The Revised Oxford Translation , Vol . 2 ( Princeton Uni - versity Press , Princeton , NJ ) . Arrow KJ ( 1951 ) Social Choice and Individual Values ( Yale University Press , New Haven , CT ) . Ben - Yashar RC , Nitzan SI ( 1997 ) The optimal decision rule for ﬁxed - size committees in dichotomous choice situations : The general result . Internat . Econom . Rev . 38 ( 1 ) : 175 – 186 . Bikhchandani S , Hirshleifer D , Welch I ( 1992 ) A theory of fads , fash - ion , custom , and cultural - change as informational cascades . J . Political Econom . 100 ( 5 ) : 992 – 1026 . Bower JL ( 1970 ) Managing the Resource Allocation Process : A Study of Corporate Planning and Investment ( Division of Research , Grad - uate School of Business Administration , Harvard University , Boston ) . Brunswik E ( 1952 ) The Conceptual Framework of Psychology ( Univer - sity of Chicago Press , Chicago ) . Burton RM , Obel B ( 2004 ) Strategic Organizational Diagnosis and Design : The Dynamics of Fit , 3rd ed . ( Kluwer , Boston ) . Carley KM , Lin ZA ( 1997 ) A theoretical study of organizational performance under information distortion . Management Sci . 43 ( 7 ) : 976 – 997 . Casella G , Berger RL ( 2002 ) Statistical Inference , 2nd ed . ( Duxbury , Paciﬁc Grove , CA ) . Christensen CM , Bower JL ( 1996 ) Customer power , strategic invest - ment , and the failure of leading ﬁrms . Strategic Management J . 17 ( 3 ) : 197 – 218 . Christensen M , Knudsen T ( 2010 ) Design of decision making - organizations . Management Science 56 ( 1 ) : 71 – 89 . Clemen RT , Winkler RL ( 1985 ) Limits for the precision and value of information from dependent sources . Oper . Res . 33 ( 2 ) : 427 – 442 . Cohen MD , March JG , Olsen JP ( 1972 ) A garbage can model of organizational choice . Admin . Sci . Quart . 17 : 1 – 25 . Condorcet M de ( 1785 ) Essai sur l’application de l’analyse à la prob - abilité des décisions rendues à la pluralité des voix . Imprimerie Royale , 164 – 165 . C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g . Csaszar and Eggers : Organizational Decision Making : An Information Aggregation View Management Science , Articles in Advance , pp . 1 – 21 , ©2013 INFORMS 21 Csaszar FA ( 2012a ) An efﬁcient frontier in organization design : Organizational structure as a determinant of exploration and exploitation . Organ . Sci . , ePub ahead of print November 5 , http : / / dx . doi . org / 10 . 1287 / orsc . 1120 . 0784 . Csaszar FA ( 2012b ) Organizational structure as a determinant of performance : Evidence from mutual funds . Strategic Manage - ment J . 33 ( 6 ) : 611 – 632 . Cyert RM , March JG ( 1963 ) A Behavioral Theory of the Firm ( Prentice - Hall , Englewood Cliffs , NJ ) . Dawes RM ( 1979 ) The robust beauty of improper linear - models in decision - making . Amer . Psychologist 34 ( 7 ) : 571 – 582 . Eggers JP , Kaplan S ( 2009 ) Cognition and renewal : Comparing CEO and organizational effects on incumbent adaptation to techni - cal change . Organ . Sci . 20 ( 2 ) : 461 – 477 . Einhorn HJ , Hogarth RM ( 1975 ) Unit weighting schemes for deci - sion making . Organ . Behav . Human Performance 13 ( 2 ) : 171 – 192 . Einhorn HJ , Hogarth RM , Klempner E ( 1977 ) Quality of group judgment . Psychol . Bull . 84 ( 1 ) : 158 – 172 . Fang C , Lee J , Schilling MA ( 2010 ) Balancing exploration and exploitation through structural design : The isolation of sub - groups and organizational learning . Organ . Sci . 21 ( 3 ) : 625 – 642 . Gavetti G , Levinthal DA , Ocasio W ( 2007 ) Neo - Carnegie : The Carnegie School’s past , present , and reconstructing for the future . Organ . Sci . 18 ( 3 ) : 523 – 536 . GigerenzerG , GoldsteinDG ( 1996 ) Reasoningthefastandfrugalway : Models of bounded rationality . Psychol . Rev . 103 ( 4 ) : 650 – 669 . Grofman B , Owen G , eds . ( 1986 ) Information Pooling and Group Deci - sion Making ( JAI Press , Greenwich , CT ) . Grofman B , Owen G , Feld SL ( 1983 ) Thirteen theorems in search of the truth . Theory and Decision 15 ( 3 ) : 261 – 278 . Hambrick DC , Mason PA ( 1984 ) Upper echelons : The organiza - tion as a reﬂection of its top managers . Acad . Management Rev . 9 ( 2 ) : 193 – 206 . Hannan MT , Freeman J ( 1977 ) The population ecology of organiza - tions . Amer . J . Sociol . 82 ( 5 ) : 929 – 964 . Hastie R , Kameda T ( 2005 ) The robust beauty of majority rules in group decisions . Psychol . Rev . 112 ( 2 ) : 494 – 508 . Hogarth RM ( 1978 ) A note on aggregating opinions . Organ . Behav . Human Performance 21 ( 1 ) : 40 – 46 . Hogarth RM , Karelaia N ( 2005 ) Simple models for multiattribute choice with many alternatives : When it does and does not pay to face trade - offs with binary attributes . Management Sci . 51 ( 12 ) : 1860 – 1872 . Hogarth RM , Karelaia N ( 2007 ) Heuristic and linear models of judgment : Matching rules and environments . Psychol . Rev . 114 ( 3 ) : 733 – 758 . Hotelling H ( 1929 ) Stability in competition . Econom . J . 39 ( 153 ) : 41 – 57 . Huber GP , McDaniel RR ( 1986 ) The decision - making paradigm of organizational design . Management Sci . 32 ( 5 ) : 572 – 589 . Janis IL ( 1972 ) Victims of Groupthink : A Psychological Study of Foreign - Policy Decisions and Fiascoes ( Houghton Mifﬂin , Boston ) . Kameda T , Tsukasaki T , Hastie R , Berg N ( 2011 ) Democracy under uncertainty : The wisdom of crowds and the free - rider problem in group decision making . Psychol . Rev . 118 ( 1 ) : 76 – 96 . Kanazawa S ( 1998 ) A brief note on a further reﬁnement of the Con - dorcet jury theorem for heterogeneous groups . Math . Soc . Sci . 35 ( 1 ) : 69 – 73 . Knudsen T , Levinthal DA ( 2007 ) Two faces of search : Alternative generation and alternative evaluation . Organ . Sci . 18 ( 1 ) : 39 – 54 . Kruger J , Dunning D ( 1999 ) Unskilled and unaware of it : How difﬁ - culties in recognizing one’s own incompetence lead to inﬂated self - assessments . J . Personality Soc . Psychol . 77 ( 6 ) : 1121 – 1134 . Laplace PS ( 1814 / 1995 ) Philosophical Essay on Probabilities ( translated by Dale A ) ( Springer - Verlag , New York ) . Larrick RP , Soll JB ( 2006 ) Intuitions about combining opinions : Misappreciation of the averaging principle . Management Sci . 52 ( 1 ) : 111 – 127 . Laughlin PR ( 2011 ) Group Problem Solving ( Princeton University Press , Princeton , NJ ) . Lawrence PR , Lorsch JW ( 1967 ) Organization and Environment : Managing Differentiation and Integration ( Division of Research , Graduate School of Business Administration , Harvard Univer - sity , Boston ) . Liang DW , Moreland R , Argote L ( 1995 ) Group versus individual training and group - performance : The mediating role of trans - active memory . Personality Soc . Psychol . Bull . 21 ( 4 ) : 384 – 393 . Mackay C ( 1852 ) Memoirs of Extraordinary Popular Delusions and the Madness of Crowds ( Richard Bentley , London ) . March JG ( 1991 ) Exploration and exploitation in organizational learning . Organ . Sci . 2 ( 1 ) : 71 – 87 . Mezias JM , Starbuck WH ( 2003 ) Studying the accuracy of man - agers’ perceptions : A research odyssey . British J . Management 14 ( 1 ) : 3 – 17 . Miles RE , Snow CC ( 1978 ) Organizational Strategy , Structure , and Process ( McGraw - Hill , New York ) . Miller D , Greenwood R , Prakash R ( 2009 ) What happened to orga - nization theory ? J . Management Inquiry 18 ( 4 ) : 273 – 279 . Mintzberg H ( 1979 ) The Structuring of Organizations ( Prentice - Hall , Englewood Cliffs , NJ ) . Mintzberg H , Raisinghani D , Theoret A ( 1976 ) Structure of unstruc - tured decision - processes . Admin . Sci . Quart . 21 ( 2 ) : 246 – 275 . Murray A ( 2010 ) The end of management . Wall Street Journal ( August 21 ) W3 . Newell BR , Shanks DR ( 2003 ) Take the best or look at the rest ? fac - tors inﬂuencing “one - reason” decision making . J . Experimental Psychol . : Learning , Memory , Cognition 29 ( 1 ) : 53 – 65 . Nickerson JA , Zenger TR ( 2004 ) A knowledge - based theory of the ﬁrm : The problem - solving perspective . Organ . Sci . 15 ( 6 ) : 617 – 632 . Payne JW , Bettman JR , Johnson EJ ( 1993 ) The Adaptive Decision Maker ( Cambridge University Press , Cambridge , UK ) . Radner R ( 1993 ) The organization of decentralized information pro - cessing . Econometrica 61 ( 5 ) : 1109 – 1146 . Rivkin JW , Siggelkow N ( 2003 ) Balancing search and stability : Inter - dependencies among elements of organizational design . Man - agement Sci . 49 ( 3 ) : 290 – 311 . Sah RK , Stiglitz JE ( 1986 ) The architecture of economic systems : Hierarchies and polyarchies . Amer . Econom . Rev . 76 ( 4 ) : 716 – 727 . Schwenk CR ( 1984 ) Cognitive simpliﬁcation processes in strategic decision - making . Strategic Management J . 5 ( 2 ) : 111 – 128 . Seshadri S , Shapira Z ( 2003 ) The ﬂow of ideas and timing of evalu - ation as determinants of knowledge creation . Indust . Corporate Change 12 ( 5 ) : 1099 – 1124 . Siggelkow N ( 2001 ) Change in the presence of ﬁt : The rise , the fall , and the renaissance of Liz Claiborne . Acad . Management J . 44 ( 4 ) : 838 – 857 . Siggelkow N , Rivkin JW ( 2005 ) Speed and search : Designing organi - zations for turbulence and complexity . Organ . Sci . 16 ( 2 ) : 101 – 122 . Simon HA ( 1947 / 1997 ) Administrative Behavior , 4th ed . ( Free Press , New York ) . Simon HA ( 1956 ) Rational choice and the structure of the environ - ment . Psychol . Rev . 63 ( 2 ) : 129 – 138 . Soll JB , Larrick RP ( 2009 ) Strategies for revising judgment : How ( and how well ) people use others’ opinions . J . Experimental Psy - chol . : Learning , Memory , Cognition 35 ( 3 ) : 780 – 805 . Sorkin RD , Hays CJ , West R ( 2001 ) Signal - detection analysis of group decision making . Psychol . Rev . 108 ( 1 ) : 183 – 203 . Surowiecki J ( 2004 ) The Wisdom of Crowds ( Doubleday , New York ) . Thompson J ( 1967 ) Organizations in Action : Social Science Bases in Administrative Theory ( McGraw - Hill , New York ) . Tindale RS , Kameda T , Hinsz VB ( 2003 ) Group decision making . Hogg MA , Cooper J , eds . The SAGE Handbook of Social Psychol - ogy ( Sage , London ) , 381 – 406 . Tripsas M , Gavetti G ( 2000 ) Capabilities , cognition , and iner - tia : Evidence from digital imaging . Strategic Management J . 21 ( 10 – 11 ) : 1147 – 1161 . Tushman ML , Anderson P ( 1986 ) Technological discontinuities and organizational environments . Admin . Sci . Quart . 31 ( 3 ) : 439 – 465 . Wolfers J , Zitzewitz E ( 2004 ) Prediction markets . J . Econom . Perspec - tives 18 ( 2 ) : 107 – 126 . Woodward J ( 1965 ) Industrial Organization : Theory and Practice ( Oxford University Press , London ) . C op y r i gh t : I N F O R M S ho l d s c op y r i g h t t o t h i s A r t i c l e s i n A d v an c e v e r s i on , w h i c h i s m ade a v a il ab l e t o s ub sc r i be r s . T he ﬁ l e m a y no t be po s t ed on an y o t he r w eb s i t e , i n c l ud i ng t he au t ho r ’ s s i t e . P l ea s e s end an y que s t i on s r ega r d i ng t h i s po li cy t o pe r m i ss i on s @ i n f o r m s . o r g .