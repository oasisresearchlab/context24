Self - Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks Blake Bordelon & Cengiz Pehlevan John Paulson School of Engineering and Applied Sciences , Center for Brain Science Harvard University Cambridge MA , 02138 blake _ bordelon @ g . harvard . edu , cpehlevan @ g . harvard . edu Abstract We analyze feature learning in inﬁnite - width neural networks trained with gradient ﬂow through a self - consistent dynamical ﬁeld theory . We construct a collection of deterministic dynamical order parameters which are inner - product kernels for hidden unit activations and gradients in each layer at pairs of time points , providing a reduced description of network activity through training . These kernel order parameters collectively deﬁne the hidden layer activation distribution , the evolution of the neural tangent kernel , and consequently output predictions . We show that the ﬁeld theory derivation recovers the recursive stochastic process of inﬁnite - width feature learning networks obtained from Yang & Hu with Tensor Programs [ 1 ] . For deep linear networks , these kernels satisfy a set of algebraic matrix equations . For nonlinear networks , we provide an alternating sampling procedure to self - consistently solve for the kernel order parameters . We provide comparisons of the self - consistent solution to various approximation schemes including the static NTK approximation , gradient independence assumption , and leading order perturbation theory , showing that each of these approximations can break down in regimes where general self - consistent solutions still provide an accurate description . Lastly , we provide experiments in more realistic settings which demonstrate that the loss and kernel dynamics of CNNs at ﬁxed feature learning strength is preserved across different widths on a CIFAR classiﬁcation task . 1 Introduction Deep learning has emerged as a successful paradigm for solving challenging machine learning and computational problems across a variety of domains [ 2 , 3 ] . However , theoretical understanding of the training and generalization of modern deep learning methods lags behind current practice . Ideally , a theory of deep learning would be analytically tractable , efﬁciently computable , capable of predicting network performance and internal features that the network learns , and interpretable through a reduced description involving desirably initialization - independent quantities . Several recent theoretical advances have fruitfully considered the idealization of wide neural networks , where the number of hidden units in each layer is taken to be large . Under certain parameterization , Bayesian neural networks and gradient descent trained networks converge to gaussian processes ( NNGPs ) [ 4 – 6 ] and neural tangent kernel ( NTK ) machines [ 7 – 9 ] in their respective inﬁnite - width limits . These limits provide both analytic tractability as well as detailed training and generalization analysis [ 10 – 17 ] . However , in this limit , with these parameterizations , data representations are ﬁxed and do not adapt to data , termed the lazy regime of NN training , to contrast it from the rich regime where NNs signiﬁcantly alter their internal features while ﬁtting the data [ 18 , 19 ] . The fact that the representation of data is ﬁxed renders these kernel - based theories incapable of explaining feature 36th Conference on Neural Information Processing Systems ( NeurIPS 2022 ) . a r X i v : 2205 . 09653v3 [ s t a t . M L ] 4 O c t 2022 learning , an ingredient which is crucial to the success of deep learning in practice [ 20 , 21 ] . Thus , alternative theories capable of modeling feature learning dynamics are needed . Recently developed alternative parameterizations such as the mean ﬁeld [ 22 ] and the µP [ 1 ] param - eterizations allow feature learning in inﬁnite - width NNs trained with gradient descent . Using the Tensor Programs framework , Yang & Hu identiﬁed a stochastic process that describes the evolution of preactivation features in inﬁnite - width µP NNs [ 1 ] . In this work , we study an equivalent parame - terization to µP with self - consistent dynamical mean ﬁeld theory ( DMFT ) and recover the stochastic process description of inﬁnite NNs using this alternative technique . In the same large width scaling , we include a scalar parameter γ 0 that allows smooth interpolation between lazy and rich behavior [ 18 ] . We provide a new computational procedure to sample this stochastic process and demonstrate its predictive power for wide NNs . Our novel contributions in this paper are the following : 1 . We develop a path integral formulation of gradient ﬂow dynamics in inﬁnite - width networks in the feature learning regime . Our parameterization includes a scalar parameter γ 0 to allow interpolation between rich and lazy regimes and comparison to perturbative methods . 2 . Using a stationary action argument , we identify a set of saddle point equations that the kernels satisfy at inﬁnite - width , relating the stochastic processes that deﬁne hidden activation evolution to the kernels and vice versa . We show that our saddle point equations recover at γ 0 = 1 , from an alternative method , the same stochastic process obtained previously with Tensor Programs [ 1 ] . 3 . We develop a polynomial - time numerical procedure to solve the saddle point equations for deep networks . In numerical experiments , we demonstrate that solutions to these self - consistency equations are predictive of network training at a variety of feature learning strengths , widths and depths . We provide comparisons of our theory to various approximate methods , such as perturbation theory . 1 . 1 Related Works A natural extension to the lazy NTK / NNGP limit that allows the study of feature learning is to calculate ﬁnite width corrections to the inﬁnite - width limit . Finite width corrections to Bayesian inference in wide networks have been obtained with various perturbative [ 23 – 29 ] and self - consistent techniques [ 30 – 33 ] . In the gradient descent based setting , leading order corrections to the NTK dynamics have been analyzed to study ﬁnite width effects [ 34 – 36 , 27 ] . These methods give approximate corrections which are accurate provided the strength of feature learning is small . In very rich feature learning regimes , however , the leading order corrections can give incorrect predictions [ 37 , 38 ] . Another approach to study feature learning is to alter NN parameterization in gradient - based learning to allow signiﬁcant feature evolution even at inﬁnite - width , the mean ﬁeld limit [ 22 , 39 ] . Works on mean ﬁeld NNs have yielded formal loss convergence results [ 40 , 41 ] and shown equivalences of gradient ﬂow dynamics to a partial differential equation ( PDE ) [ 42 – 44 ] . Our results are most closely related to a set of recent works which studied inﬁnite - width NNs trained with gradient descent ( GD ) using the Tensor Programs ( TP ) framework [ 1 ] . We show that our discrete time ﬁeld theory at unit feature learning strength γ 0 = 1 recovers the stochastic process which was derived from TP . The stochastic process derived from TP has provided insights into practical issues in NN training such as hyper - parameter search [ 45 ] . Computing the exact inﬁnite - width limit of GD has exponential time requirements [ 1 ] , which we show can be circumvented with an alternating sampling procedure . A projected variant of GD training has provided an inﬁnite - width theory that could be scaled to realistic datasets like CIFAR - 10 [ 46 ] . Inspired by Chizat and Bach’s work on mechanisms of lazy and rich training [ 18 ] , our theory interpolates between lazy and rich behavior in the mean ﬁeld limit for varying γ 0 and allows comparison of DMFT to perturbative analysis near small γ 0 . Further , our derivation of a DMFT action allows the possibility of pursuing ﬁnite width effects . Our theory is inspired by self - consistent dynamical mean ﬁeld theory ( DMFT ) from statistical physics [ 47 – 53 ] . This framework has been utilized in the theory of random recurrent networks [ 54 – 59 ] , tensor PCA [ 60 , 61 ] , phase retrieval [ 62 ] , and high - dimensional linear classiﬁers [ 63 – 66 ] , but has yet to be developed for deep feature learning . By developing a self - consistent DMFT of deep NNs , we gain insight into how features evolve in the rich regime of network training , while retaining many pleasant analytic properties of the inﬁnite - width limit . 2 2 Problem Setup and Deﬁnitions Our theory applies to inﬁnite - width networks , both fully - connected and convolutional . For notational ease we will relegate convolutional results to later sections . For input x µ ∈ R D , we deﬁne the hidden pre - activation vectors h (cid:96) ∈ R N for layers (cid:96) ∈ { 1 , . . . , L } as f µ = 1 γ √ N w L · φ ( h Lµ ) , h (cid:96) + 1 µ = 1 √ N W (cid:96) φ ( h (cid:96)µ ) , h 1 µ = 1 √ D W 0 x µ , ( 1 ) where θ = Vec { W 0 , . . . , w L } are the trainable parameters of the network and φ is a twice differ - entiable activation function . Inspired by previous works on the mechanisms of lazy gradient based training , the parameter γ will control the laziness or richness of the training dynamics [ 18 , 19 , 1 , 42 ] . Each of the trainable parameters are initialized as Gaussian random variables with unit variance W (cid:96)ij ∼ N ( 0 , 1 ) . They evolve under gradient ﬂow ddt θ = − γ 2 ∇ θ L . The choice of learning rate γ 2 causes ddt L | t = 0 to be independent of γ . To characterize the evolution of weights , we introduce backpropaga - tion variables g (cid:96)µ = γ √ N ∂f µ ∂ h (cid:96)µ = ˙ φ ( h (cid:96)µ ) (cid:12) z (cid:96)µ , where z (cid:96)µ = 1 √ N W (cid:96) (cid:62) g (cid:96) + 1 µ is the pre - gradient signal . The relevant dynamical objects to characterize feature learning are feature and gradient kernels for each hidden layer (cid:96) ∈ { 1 , . . . , L } , deﬁned as Φ (cid:96)µα ( t , s ) = 1 N φ ( h (cid:96)µ ( t ) ) · φ ( h (cid:96)α ( s ) ) , G (cid:96)µα ( t , s ) = 1 N g (cid:96)µ ( t ) · g (cid:96)α ( s ) . ( 2 ) From the kernels { Φ (cid:96) , G (cid:96) } L(cid:96) = 1 , we can compute the Neural Tangent Kernel K NTKµα ( t , s ) = ∇ θ f µ ( t ) · ∇ θ f α ( s ) = (cid:80) L(cid:96) = 0 G (cid:96) + 1 µα ( t , s ) Φ (cid:96)µα ( t , s ) , [ 7 ] and the dynamics of the network function f µ d dtf µ ( t ) = P (cid:88) α = 1 K NTKµα ( t , t ) ∆ α ( t ) , ∆ µ ( t ) = − ∂ ∂f µ L | f µ ( t ) , ( 3 ) where we deﬁne base cases G L + 1 µα ( t , s ) = 1 , Φ 0 µα ( t , s ) = K xµα = 1 D x µ · x α . We note that the above formula holds for any data point µ which may or may not be in the set of P training examples . The above expressions demonstrate that knowledge of the temporal trajectory of the NTK on the t = s diagonal gives the temporal trajectory of the network predictions f µ ( t ) . Following prior works on inﬁnite - width networks [ 22 , 1 , 40 , 19 ] , we study the mean ﬁeld limit N , γ → ∞ , γ 0 = γ √ N = O N ( 1 ) ( 4 ) As we demonstrate in the Appendix D and N , this is the only N - scaling which allows feature learning as N → ∞ . The γ 0 = 0 limit recovers the static NTK limit [ 7 ] . We discuss other scalings and parameterizations in Appendix N , relating our work to the µP - parameterization and TP analysis of [ 1 ] , showing they have identical feature dynamics in the inﬁnite - width limit . We also analyze the effect of different hidden layer widths and initialization variances in the Appendix D . 8 . We focus on equal widths and NTK parameterization ( as in eq . ( 1 ) ) in the main text to reduce complexity . 3 Self - consistent DMFT Next , we derive our self - consistent DMFT in a limit where t , P = O N ( 1 ) . Our goal is to build a description of training dynamics purely based on representations , and independent of weights . Studying feature learning at inﬁnite - width enjoys several analytical properties : • The kernel order parameters Φ (cid:96) , G (cid:96) concentrate over random initializations but are dynamical , allowing ﬂexible adaptation of features to the task structure . • In each layer (cid:96) , each neuron’s preactivation h (cid:96)i and pregradient z (cid:96)i become i . i . d . draws from a distribution characterized by a set of order parameters { Φ (cid:96) , G (cid:96) , A (cid:96) , B (cid:96) } . • The kernels are deﬁned as self - consistent averages ( denoted by (cid:104)(cid:105) ) over this distribution of neurons in each layer Φ (cid:96)µα ( t , s ) = (cid:10) φ ( h (cid:96)µ ( t ) ) φ ( h (cid:96)α ( s ) ) (cid:11) and G (cid:96)µα ( t , s ) = (cid:10) g (cid:96)µ ( t ) g (cid:96)α ( s ) (cid:11) . The next section derives these facts from a path - integral formulation of gradient ﬂow dynamics . 3 3 . 1 Path Integral Construction Gradient ﬂow after a random initialization of weights deﬁnes a high dimensional stochastic process over initalizations for variables { h , g } . Therefore , we will utilize DMFT formalism to obtain a re - duced description of network activity during training . For a simpliﬁed derivation of the DMFT for the two - layer ( L = 1 ) case , see D . 2 . Generally , we separate the contribution on each forward / backward pass between the initial condition and gradient updates to weight matrix W (cid:96) , deﬁning new stochastic variables χ (cid:96) , ξ (cid:96) ∈ R N as χ (cid:96) + 1 µ ( t ) = 1 √ N W (cid:96) ( 0 ) φ ( h (cid:96)µ ( t ) ) , ξ (cid:96)µ ( t ) = 1 √ N W (cid:96) ( 0 ) (cid:62) g (cid:96) + 1 µ ( t ) . ( 5 ) We let Z represent the moment generating functional ( MGF ) for these stochastic ﬁelds Z [ { j (cid:96) , v (cid:96) } ] = (cid:42) exp  (cid:88) (cid:96) , µ (cid:90) ∞ 0 dt (cid:2) j (cid:96)µ ( t ) · χ (cid:96)µ ( t ) + v (cid:96)µ ( t ) · ξ (cid:96)µ ( t ) (cid:3)(cid:43) { W 0 ( 0 ) , . . . w L ( 0 ) } , which requires , by construction the normalization condition Z [ { 0 , 0 } ] = 1 . We enforce our deﬁnition of χ , ξ using an integral representation of the delta - function . Thus for each sample µ ∈ [ P ] and each time t ∈ R + , we multiply Z by 1 = (cid:90) R N (cid:90) R N d χ (cid:96) + 1 µ ( t ) d ˆ χ (cid:96) + 1 µ ( t ) ( 2 π ) N exp (cid:18) i ˆ χ (cid:96) + 1 µ ( t ) · (cid:20) χ (cid:96) + 1 µ ( t ) − 1 √ N W (cid:96) ( 0 ) φ ( h (cid:96)µ ( t ) ) (cid:21)(cid:19) , ( 6 ) for χ and the respective expression for ξ . After making such substitutions , we perform integration over initial Gaussian weight matrices to arrive at an integral expression for Z , which we derive in the appendix D . 4 . We show that Z can be described by set of order - parameters { Φ (cid:96) , ˆΦ (cid:96) , G (cid:96) , ˆ G (cid:96) , A (cid:96) , B (cid:96) } Z [ { j (cid:96) , v (cid:96) } ] ∝ (cid:90) (cid:89) (cid:96)µαts d Φ (cid:96)µα ( t , s ) d ˆΦ (cid:96)µα ( t , s ) dG (cid:96)µα ( t , s ) d ˆ G (cid:96)µα ( t , s ) dA (cid:96)µα ( t , s ) dB (cid:96)µα ( t , s ) ( 7 ) × exp (cid:16) NS [ { Φ , ˆΦ , G , ˆ G , A , B , j , v } ] (cid:17) , S = (cid:88) (cid:96)µα (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds (cid:104) Φ (cid:96)µα ( t , s ) ˆΦ (cid:96)µα ( t , s ) + G (cid:96)µα ( t , s ) ˆ G (cid:96)µα ( t , s ) − A (cid:96)µα ( t , s ) B (cid:96)µα ( t , s ) (cid:105) + ln Z [ { Φ , ˆΦ , G , ˆ G , A , B , j , v } ] , ( 8 ) where S is the DMFT action and Z is a single - site MGF , which deﬁnes the distribution of ﬁelds { χ (cid:96) , ξ (cid:96) } over the neural population in each layer . The kernels A and B are related to the correlations between feedforward and feedback signals in the network . We provide a detailed formula for Z in the Appendix D . 4 and show that it factorizes over different layers Z = (cid:81) L(cid:96) = 1 Z (cid:96) . 3 . 2 Deriving the DMFT Equations from the Path Integral Saddle Point As N → ∞ , the moment - generating function Z is exponentially dominated by the saddle point of S . The equations that deﬁne this saddle point also deﬁne our DMFT . We thus identify the kernels that render S locally stationary ( δS = 0 ) . The most important equations are those which deﬁne { Φ (cid:96) , G (cid:96) } δS δ ˆΦ (cid:96)µα ( t , s ) = Φ (cid:96)µα ( t , s ) + 1 Z δ Z δ ˆΦ (cid:96)µα ( t , s ) = Φ (cid:96)µα ( t , s ) − (cid:10) φ ( h (cid:96)µ ( t ) ) φ ( h (cid:96)α ( s ) ) (cid:11) = 0 , δS δ ˆ G (cid:96)µα ( t , s ) = G (cid:96)µα ( t , s ) + 1 Z δ Z δ ˆ G (cid:96)µα ( t , s ) = G (cid:96)µα ( t , s ) − (cid:10) g (cid:96)µ ( t ) g (cid:96)α ( s ) (cid:11) = 0 , ( 9 ) where (cid:104)(cid:105) denotes an average over the stochastic process induced by Z , which is deﬁned below { u (cid:96)µ ( t ) } µ ∈ [ P ] , t ∈ R + ∼ GP ( 0 , Φ (cid:96) − 1 ) , { r (cid:96)µ ( t ) } µ ∈ [ P ] , t ∈ R + ∼ GP ( 0 , G (cid:96) + 1 ) , h (cid:96)µ ( t ) = u (cid:96)µ ( t ) + γ 0 (cid:90) t 0 ds P (cid:88) α = 1 (cid:2) A (cid:96) − 1 µα ( t , s ) + ∆ α ( s ) Φ (cid:96) − 1 µα ( t , s ) (cid:3) z (cid:96)α ( s ) ˙ φ ( h (cid:96)α ( s ) ) , z (cid:96)µ ( t ) = r (cid:96)µ ( t ) + γ 0 (cid:90) t 0 ds P (cid:88) α = 1 (cid:2) B (cid:96)µα ( t , s ) + ∆ α ( s ) G (cid:96) + 1 µα ( t , s ) (cid:3) φ ( h (cid:96)α ( s ) ) , ( 10 ) 4 where we deﬁne base cases Φ 0 µα ( t , s ) = K xµα and G L + 1 µα ( t , s ) = 1 , A 0 = B L = 0 . We see that the ﬁelds { h (cid:96) , z (cid:96) } , which represent the single site preactivations and pre - gradients , are implicit functionals of the mean - zero Gaussian processes { u (cid:96) , r (cid:96) } which have covariances (cid:10) u (cid:96)µ ( t ) u (cid:96)α ( s ) (cid:11) = Φ (cid:96) − 1 µα ( t , s ) and (cid:10) r (cid:96)µ ( t ) r (cid:96)α ( s ) (cid:11) = G (cid:96) + 1 µα ( t , s ) . The other saddle point equations give A (cid:96)µα ( t , s ) = γ − 1 0 (cid:28) δφ ( h (cid:96)µ ( t ) ) δr (cid:96)α ( s ) (cid:29) , B (cid:96)µα ( t , s ) = γ − 1 0 (cid:28) δg (cid:96) + 1 µ ( t ) δu (cid:96) + 1 α ( s ) (cid:29) which arise due to coupling between the feedforward and feedback signals . We note that , in the lazy limit γ 0 → 0 , the ﬁelds approach Gaussian processes h (cid:96) → u (cid:96) , z (cid:96) → r (cid:96) . Lastly , the ﬁnal saddle point equations δSδ Φ (cid:96) = 0 , δSδG (cid:96) = 0 imply that ˆΦ (cid:96) = ˆ G (cid:96) = 0 . The full set of equations that deﬁne the DMFT are given in D . 7 . This theory is easily extended to more general architectures such as networks with varying widths by layer ( App . D . 8 ) , trainable bias parameter ( App . H ) , multiple ( but O N ( 1 ) ) output channels ( App . I ) , convolutional architectures ( App . G ) , networks trained with weight decay ( App . J ) , Langevin sampling ( App . K ) and momentum ( App . L ) , discrete time training ( App . M ) . In Appendix N , we discuss parameterizations which give equivalent feature and predictor dynamics and show our derived stochastic process is equivalent to the µP scheme of Yang & Hu [ 1 ] . 4 Solving the Self - Consistent DMFT The saddle point equations obtained from the ﬁeld theory discussed in the previous section must be solved self - consistently . By this we mean that , given knowledge of the kernels , we can characterize the distribution of { h (cid:96) , z (cid:96) } , and given the distribution of { h (cid:96) , z (cid:96) } , we can compute the kernels [ 67 , 64 ] . In the Appendix B , we provide Algorithm 1 , a numerical procedure based on this idea to efﬁciently solve for the kernels with an alternating Monte - Carlo strategy . The output of the algorithm are the dynamical kernels Φ (cid:96)µα ( t , s ) , G (cid:96)µα ( t , s ) , A (cid:96)µα ( t , s ) , B (cid:96)µα ( t , s ) , from which any network observable can be computed as we discuss in Appendix D . We provide an example of the solution to the saddle point equations compared to training a ﬁnite NN in Figure 1 . We plot Φ (cid:96) , G (cid:96) at the end of training and the sample - trace of these kernels through time . Additionally , we compare the kernels of ﬁnite width N network to the DMFT predicted kernels using a cosine - similarity alignment metric A ( Φ DMFT , Φ NN ) = Tr Φ DMFT Φ NN | Φ DMFT | | Φ NN | . Additional examples are in Appendix Figures 6 and Figure 7 . 4 . 1 Deep Linear Networks : Closed Form Self - Consistent Equations Deep linear networks ( φ ( h ) = h ) are of theoretical interest since they are simpler to analyze than nonlinear networks but preserve non - trivial training dynamics and feature learning [ 68 – 72 , 25 , 32 , 23 ] . In a deep linear network , we can simplify our saddle point equations to algebraic formulas that close in terms of the kernels H (cid:96)µα ( t , s ) = (cid:10) h (cid:96)µ ( t ) h (cid:96)α ( s ) (cid:11) , G (cid:96) ( t , s ) = (cid:10) g (cid:96) ( t ) g (cid:96) ( s ) (cid:11) [ 1 ] . This is a signiﬁcant simpliﬁcation since it allows solution of the saddle point equations without a sampling procedure . To describe the result , we ﬁrst introduce a vectorization notation h (cid:96) = Vec { h (cid:96)µ ( t ) } µ ∈ [ P ] , t ∈ R + . Like - wise we convert kernels H (cid:96) = Mat { H (cid:96) µα ( t , s ) } µ , α ∈ [ P ] , t , s ∈ R + into matrices . The inner product under this vectorization is deﬁned as a · b = (cid:82) ∞ 0 dt (cid:80) Pµ = 1 a µ ( t ) b µ ( t ) . In a practical computa - tional implementation , the theory would be evaluated on a grid of T time points with discrete time gradient descent , so these kernels H (cid:96) ∈ R PT × PT would indeed be matrices of the appropriate size . The ﬁelds h (cid:96) , g (cid:96) are linear functionals of independent Gaussian processes u (cid:96) , r (cid:96) , giving ( I − γ 20 C (cid:96) D (cid:96) ) h (cid:96) = u (cid:96) + γ 0 C (cid:96) r (cid:96) , ( I − γ 20 D (cid:96) C (cid:96) ) g (cid:96) = r (cid:96) + γ 0 D (cid:96) u (cid:96) . The matrices C (cid:96) and D (cid:96) are causal integral operators which depend on { A (cid:96) − 1 , H (cid:96) − 1 } and { B (cid:96) , G (cid:96) + 1 } respectively which we deﬁne in Appendix F . The saddle point equations which deﬁne the kernels are H (cid:96) = (cid:10) h (cid:96) h (cid:96) (cid:62) (cid:11) = ( I − γ 20 C (cid:96) D (cid:96) ) − 1 [ H (cid:96) − 1 + γ 20 C (cid:96) G (cid:96) + 1 C (cid:96) (cid:62) ] (cid:2) ( I − γ 20 C (cid:96) D (cid:96) ) − 1 (cid:3) (cid:62) G (cid:96) = (cid:10) g (cid:96) g (cid:96) (cid:62) (cid:11) = (cid:0) I − γ 20 D (cid:96) C (cid:96) (cid:1) − 1 (cid:2) G (cid:96) + 1 + γ 20 D (cid:96) H (cid:96) − 1 D (cid:96) (cid:62) (cid:3) (cid:104)(cid:0) I − γ 20 D (cid:96) C (cid:96) (cid:1) − 1 (cid:105) (cid:62) . ( 11 ) Examples of the predictions obtained by solving these systems of equations are provided in Figure 2 . We see that these DMFT equations describe kernel evolution for networks of a variety of depths and that the change in each layer’s kernel increases with the depth of the network . Unlike many prior results [ 68 – 71 ] , our DMFT does not require any restrictions on the structure of the input data but hold for any K x , y . However , for whitened data K x = I we show in 5 0 20 40 60 80 100 t 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 t NN 0 = 0 NN 0 = 1 . 0 DMFT ( a ) Lazy vs Rich Loss Dynamics 6 4 2 0 2 4 6 h 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 I n i t i a l D e n s i t y = 1 = 2 = 3 ( b ) Initial Preactivation Density 6 4 2 0 2 4 6 h 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 F i n a l D e n s i t y = 1 = 2 = 3 ( c ) Final Preactivation Density E x p t = 1 T h e o r y = 2 = 3 ( d ) Final Φ (cid:96) Kernels γ 0 = 1 E x p t ( t , s ) = 1 D M F T ( t , s ) = 2 = 3 ( e ) Φ (cid:96) Dynamics γ 0 = 1 . 0 10 1 10 2 10 3 N 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 A ( D M F T , NN ) = 1 = 2 = 3 ( f ) Φ (cid:96) Convergence to DMFT E x p t G = 1 T h e o r y G = 2 = 3 ( g ) Final G (cid:96) kernels γ 0 = 1 . 0 E x p t G ( t , s ) = 1 D M F T G ( t , s ) = 2 = 3 ( h ) G (cid:96) Dynamics γ 0 = 1 . 0 10 1 10 2 10 3 N 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 A ( G D M F T , G NN ) = 1 = 2 = 3 ( i ) G (cid:96) Convergence to DMFT Figure 1 : Neural network feature learning dynamics is captured by self - consistent dynamical mean ﬁeld theory ( DMFT ) . ( a ) Training loss curves on a subsample of P = 10 CIFAR - 10 training points in a depth 4 ( L = 3 , N = 2500 ) tanh network ( φ ( h ) = tanh ( h ) ) trained with MSE . Increasing γ 0 accelerates training . ( b ) - ( c ) The distribution of preactivations at the beginning and end of training matches predictions of the DMFT . ( d ) The ﬁnal Φ (cid:96) ( at t = 100 ) kernel order parameters match the ﬁnite width network . ( e ) The temporal dynamics of the sample - traced kernels (cid:80) µ Φ (cid:96)µµ ( t , s ) matches experiment and reveals rich dynamics across layers . ( f ) The alignment A ( Φ (cid:96)DMFT , Φ (cid:96)NN ) , deﬁned as cosine similarity , of the kernel Φ (cid:96)µα ( t , s ) predicted by theory ( DMFT ) and width N networks for different N but ﬁxed γ 0 = γ / √ N . Errorbars show standard deviation computed over 10 repeats . Around N ∼ 500 DMFT begins to show near perfect agreement with the NN . ( g ) - ( i ) The same plots but for the gradient kernel G (cid:96) . Whereas ﬁnite width effects for Φ (cid:96) are larger at later layers (cid:96) since variance accumulates on the forward pass , ﬂuctuations in G (cid:96) are large in early layers . Appendix F . 1 . 1 , F . 2 that our DMFT learning curves interpolate between NTK dynamics and the sigmoidal trajectories of prior works [ 68 , 69 ] as γ 0 is increased . For example , in the two layer ( L = 1 ) linear network with K x = I , the dynamics of the error norm ∆ ( t ) = | | ∆ ( t ) | | takes the form ∂∂t ∆ ( t ) = − 2 (cid:112) 1 + γ 20 ( y − ∆ ( t ) ) 2 ∆ ( t ) where y = | | y | | . These dynamics give the linear convergence rate of the NTK if γ 0 → 0 but approaches logistic dynamics of [ 69 ] as γ 0 → ∞ . Further , H ( t ) = (cid:10) h 1 ( t ) h 1 ( t ) (cid:62) (cid:11) ∈ R P × P only grows in the yy (cid:62) direction with H y ( t ) = 1 y 2 y (cid:62) H ( t ) y = (cid:112) 1 + γ 20 ( y − ∆ ( t ) ) 2 . At the end of training H ( t ) → I + 1 y 2 [ (cid:112) 1 + γ 20 y 2 − 1 ] yy (cid:62) , recovering the rank one spike which was recently obtained in the small initialization limit [ 73 ] . We show this one dimensional system in Figure 8 . 4 . 2 Feature Learning with L2 Regularization As we show in Appendix J , the DMFT can be extended to networks trained with weight decay d θ dt = − γ 2 ∇ θ L − λ θ . If neural network is homogenous in its parameters so that f ( c θ ) = c κ f ( θ ) ( examples include networks with linear , ReLU , quadratic activations ) , then the ﬁnal network predictor 6 0 20 40 60 80 100 t 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 t L = 2 L = 3 L = 4 L = 5 DMFT ( a ) Deep Linear Loss Dynamics E x p t . H 1 T h e o r y H 2 H 3 H 4 H 5 ( b ) Predicted vs Experimental Final H (cid:96) Kernels 2 . 0 2 . 5 3 . 0 3 . 5 4 . 0 4 . 5 5 . 0 L 0 . 75 0 . 80 0 . 85 0 . 90 A ( H ( 0 , 0 ) , H ( T , T ) ) NNDMFT ( c ) L - Dependent Kernel Movement G ( t , s ) = 1 H ( t , s ) = 2 = 3 = 4 = 5 ( d ) L = 5 DMFT Temporal Kernels Figure 2 : Deep linear network with the full DMFT . ( a ) The train loss for NNs of varying L . ( b ) For a L = 5 , N = 1000 NN , the kernels H (cid:96) at the end of training compared to DMFT theory on P = 20 datapoints . ( c ) The average displacement of feature kernels for different depth networks at same γ 0 value . For equal values of γ 0 , deeper networks exhibit larger changes to their features , manifested in lower alignment with their initial t = 0 kernels H . ( d ) The solution to the temporal components of the G (cid:96) ( t , s ) and (cid:80) µ H (cid:96)µµ ( t , s ) kernels obtained from the self - consistent equations . is a kernel regressor with the ﬁnal NTK lim t →∞ f ( x , t ) = k ( x ) (cid:62) [ K + λκ I ] − 1 y where K ( x , x (cid:48) ) is the ﬁnal - NTK , [ k ( x ) ] µ = K ( x , x µ ) and [ K ] µα = K ( x µ , x α ) . We note that the effective regularization λκ increases with depth L . In NTK parameterization , weight decay in inﬁnite width homogenous networks gives a trivial ﬁxed point K ( x , x (cid:48) ) → 0 and consequently a zero predictor f → 0 [ 74 ] . However , as we show in Figure 3 , increasing feature learning γ 0 can prevent convergence to the trivial ﬁxed point , allowing a non - zero ﬁxed point for K , f even at inﬁnite width . The kernel and function dynamics can be predicted with DMFT . The ﬁxed point is a nontrivial function of the hyperparameters λ , κ , L , γ 0 . 0 100 200 300 400 500 t 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 ( t ) 0 = 0 . 25 0 = 0 . 50 0 = 0 . 75 0 = 1 . 00 ( a ) Loss for varying γ 0 E x p t 0 = 0 . 25 D M F T 0 = 0 . 50 0 = 0 . 75 0 = 1 . 00 ( b ) Final Φ Kernels Figure 3 : Width N = 1000 ReLU networks trained with L2 regularization have nontrivial ﬁxed point in DMFT limit ( γ 0 > 0 ) . ( a ) Training loss dynamics for a L = 1 ReLU network with λ = 1 . In γ 0 → 0 limit the ﬁxed point is trivial f = K = 0 . The ﬁnal loss is a decreasing function of γ 0 . ( b ) The ﬁnal kernel is more aligned with target with increasing γ 0 . Networks with homogenous activations enjoy a representer theorem at inﬁnite - width as we show in Appendix J . 7 5 Approximation Schemes We now compare our exact DMFT with approximations of prior works , providing an explanation of when these approximations give accurate predictions and when they break down . 5 . 1 Gradient Independence Ansatz We can study the accuracy of the ansatz A (cid:96) = B (cid:96) = 0 , which is equivalent to treating the weight matrices W (cid:96) ( 0 ) and W (cid:96) ( 0 ) (cid:62) which appear in forward and backward passes respectively as indepen - dent Gaussian matrices . This assumption was utilized in prior works on signal propagation in deep networks in the lazy regime [ 75 – 79 ] . A consequence of this approximation is the Gaussianity and statistical independence of χ (cid:96) and ξ (cid:96) ( conditional on { Φ (cid:96) , G (cid:96) } ) in each layer as we show in Appendix O . This ansatz works very well near γ 0 ≈ 0 ( the static kernel regime ) since d h d r , d z d u ∼ O ( γ 0 ) or around initialization t ≈ 0 but begins to fail at larger values of γ 0 , t ( Figure 4 ) . 5 . 2 Perturbation theory in γ 0 at inﬁnite - width In the γ 0 → 0 limit , we recover static kernels , giving linear dynamics identical to the NTK limit [ 7 ] . Corrections to this lazy limit can be extracted at small but ﬁnite γ 0 . This is conceptually similar to recent works which consider perturbation series for the NTK in powers of 1 / N [ 35 , 27 , 28 ] ( though not identical , see Appendix P . 7 for ﬁnite N effects ) . We expand all observables q ( γ 0 ) in a power series in γ 0 , giving q ( γ 0 ) = q ( 0 ) + γ 0 q ( 1 ) + γ 20 q ( 2 ) + . . . and compute corrections up to O ( γ 20 ) . We show that the O ( γ 0 ) and O ( γ 30 ) corrections to kernels vanish , giving leading order expansions of the form Φ = Φ 0 + γ 20 Φ 2 + O ( γ 40 ) and G = G 0 + γ 20 G 2 + O ( γ 40 ) ( see Appendix P . 2 ) . Further , we show that the NTK has relative change at leading order which scales linearly with depth | ∆ K NTK | / | K NTK , 0 | ∼ O γ 0 , L ( Lγ 20 ) = O N , γ , L ( γ 2 LN ) , which is consistent with ﬁnite width effective ﬁeld theory at γ = O N ( 1 ) [ 26 – 28 ] ( Appendix P . 6 ) . Further , at the leading order correction , all temporal dependencies are controlled by P ( P + 1 ) functions v α ( t ) = (cid:82) t 0 ds ∆ 0 α ( s ) and v αβ ( t ) = (cid:82) t 0 ds ∆ 0 α ( s ) (cid:82) s 0 ds (cid:48) ∆ 0 β ( s (cid:48) ) , which is consistent with those derived for ﬁnite width NNs using a truncation of the Neural Tangent Hierarchy [ 34 , 35 , 27 ] . To lighten notation , we focus our main text comparison of our non - perturbative DMFT to perturbation theory in the deep linear case . Full perturbation theory is in Appendix P . 2 . Using the timescales derived in the previous section , we ﬁnd that the leading order correction to the kernels in inﬁnite - width deep linear network have the form K NTKµν ( t , s ) = ( L + 1 ) K xµν + γ 20 L ( L + 1 ) 2 K xµν (cid:88) αβ K xαβ [ v αβ ( t ) + v βα ( s ) + v α ( t ) v β ( s ) ] + γ 20 L ( L + 1 ) 2  (cid:88) αβ K xµα K xνβ [ v αβ ( t ) + v βα ( s ) ] + (cid:88) αβ K xµα K xνβ v α ( t ) v β ( s )   + O ( γ 40 ) . ( 12 ) We see that the relative change in the NTK | K NTK − K NTK ( 0 ) | / | K NTK ( 0 ) | ∼ O ( γ 20 L ) = O ( γ 2 L / N ) , so that large depth L networks exhibit more signiﬁcant kernel evolution , which agrees with other perturbative studies [ 35 , 27 , 25 ] as well as the non - perturbative results in Figure 2 . However at large γ 0 and large L , this theory begins to break down as we show in Figure 4 . The DMFT formalism can also be used to extract leading corrections to observables at large but ﬁnite width N as we explore in P . 7 . When deviating from inﬁnite width , the kernels are no longer deter - ministic over network initializations . The key observation is that the DMFT action S deﬁnes a Gibbs measure over the space of kernel order parameters k = Vec { Φ (cid:96) , G (cid:96) , A (cid:96) , B (cid:96) } with probability den - sity 1 Z exp ( NS [ k ] ) where Z is a normalization constant . Near inﬁnite width , any observable average (cid:104) O ( k ) (cid:105) = 1 Z (cid:82) d k exp ( NS [ k ] ) O ( k ) is dominated by order parameters within a 1 √ N neighborhood of k ∗ . As a consequence , a perturbative series for (cid:104) O ( k ) (cid:105) can be obtained from simple averages over Gaussian ﬂuctuations in the kernels k ∼ N ( k ∗ , − 1 N [ ∇ 2 S [ k ∗ ] ] − 1 ) [ 29 ] . The components for ∇ 2 S [ k ∗ ] include four point correlations of ﬁelds computed over the DMFT distribution . 8 0 20 40 60 80 100 t 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 ( t ) NN DMFT NTK Pert . Gr . Ind . ( a ) Loss dynamics E x p t . H 1 D M F T N T K P e r t . G r . I n d . H 2 H 3 H 4 H 5 ( b ) Final H (cid:96) Kernels γ 0 = 1 . 5 E x p t . H 1 ( t , s ) D M F T N T K P e r t . G r . I n d . H 2 ( t , s ) H 3 ( t , s ) H 4 ( t , s ) H 5 ( t , s ) ( c ) H (cid:96) Kernel Dynamics γ 0 = 1 . 5 0 . 25 0 . 50 0 . 75 1 . 00 1 . 25 1 . 50 0 0 . 80 0 . 85 0 . 90 0 . 95 1 . 00 A ( H , H NN ) DMFT NTK Pert . Gr . Ind . ( d ) Theory H (cid:96) vs NN with N = 1000 Figure 4 : Comparison of DMFT to various approximation schemes in a L = 5 hidden layer , width N = 1000 linear network with γ 0 = 1 . 0 and P = 100 . ( a ) The loss for the various approximations do not track the true trajectory induced by gradient descent in the large γ 0 regime . ( b ) - ( c ) The feature kernels H (cid:96)µα ( t , s ) across each of the L = 5 hidden layers for each of the theories is compared to a width 1000 neural network . Again , we plot the sample - traced dynamics (cid:80) µµ H (cid:96)µµ ( t , s ) . ( d ) The alignment of H (cid:96) compared to the ﬁnite NN A ( H (cid:96) , H (cid:96)NN ) averaged across (cid:96) ∈ { 1 , . . . , 5 } for varying γ . The predictions of all of these theories coincide in the γ 0 = 0 limit but begin to deviate in the feature learning regime . Only the non - perturbative DMFT is accurate over a wide range of γ 0 . 6 Feature Learning Dynamics is Preserved at Fixed γ 0 Our DMFT suggests that for networks sufﬁciently wide for their kernels to concentrate , the dynamics of loss and kernels should be invariant under the rescaling N → RN , γ → γ / √ R , which keeps γ 0 ﬁxed . To evaluate how well this idea holds in a realistic deep learning problem , we trained CNNs of varying channel counts N on two - class CIFAR classiﬁcation [ 80 ] . We tracked the dynamics of the loss and the last layer Φ L kernel . The results are provided in Figure 5 . We see that dynamics are largely independent of rescaling as predicted . Further , as expected , larger γ 0 leads to larger changes in kernel norm and faster alignment to the target function y , as was also found in [ 81 ] . Consequently , the higher γ 0 networks train more rapidly . The trend is consistent for width N = 250 and N = 500 . More details about the experiment can be found in Appendix C . 2 . 7 Discussion We provided a unifying DMFT derivation of feature dynamics in inﬁnite networks trained with gradient based optimization . Our theory interpolates between lazy inﬁnite - width behavior of a static NTK in γ 0 → 0 and rich feature learning . At γ 0 = 1 , our DMFT construction agrees with the stochastic process derived previously with the Tensor Programs framework [ 1 ] . Our saddle point equations give self - consistency conditions which relate the stochastic ﬁelds to the kernels . These equations are exactly solveable in deep linear networks and can be efﬁciently solved with a numerical method in the nonlinear case . Comparisons with other approximation schemes show that DMFT can be accurate at a much wider range of γ 0 . We believe our framework could be a useful perspective for future theoretical analyses of feature learning and generalization in wide networks . 9 0 20 40 60 80 100 t 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 T e s t L o ss N = 250 , 0 = 1 N = 500 , 0 = 1 0 = 5 0 = 10 ( a ) Test MSE 0 20 40 60 80 100 t 0 . 50 0 . 55 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 T e s t A cc u r a c y ( b ) Classiﬁcation Error 0 20 40 60 80 100 t 0 . 020 0 . 025 0 . 030 0 . 035 0 . 040 0 . 045 0 . 050 0 . 055 0 . 060 A li g n m e n t ( c ) A ( Φ L , yy (cid:62) ) Dynamics Figure 5 : The dynamics of a depth 5 ( L = 4 hidden ) CNNs trained on ﬁrst two classes of CIFAR ( boat vs plane ) exhibit consistency for different channel counts N ∈ { 250 , 500 } for ﬁxed γ 0 = γ / √ N . ( a ) We plot the test loss ( MSE ) and ( b ) test classiﬁcation error . Networks with higher γ 0 train more rapidly . Time is measured in every 100 update steps . ( c ) The dynamics of the last layer feature kernel Φ L , shown as alignment to the target function . As predicted by the DMFT , higher γ 0 corresponds to more active kernel evolution , evidenced by larger change in the alignment . Though our DMFT is quite general in regards to the data and architecture , the technique is not entirely rigorous and relies on heuristic physics techniques . Our theory holds in the T , P = O N ( 1 ) and may break down otherwise ; other asymptotic regimes ( such as P / N , T / log ( N ) = O N ( 1 ) , etc ) may exhibit phenomena relevant to deep learning practice [ 32 , 82 ] . The computational requirements of our method , while smaller than the exponential time complexity for exact solution [ 1 ] , are still signiﬁcant for large PT . In Table 1 , we compare the time taken for various theories to compute the feature kernels throughout T steps of gradient descent . For a width N network , computation of each forward pass on all P data points takes O ( PN 2 ) computations . The static NTK requires computation of O ( P 2 ) entries in the kernel which do not need to be recomputed . However , the DMFT requires matrix multiplications on PT × PT matrices giving a O ( P 3 T 3 ) time scaling . Future work could aim to improve the computational overhead of the algorithm , by considering data averaged theories [ 64 ] or one pass SGD [ 1 ] . Alternative projected versions of gradient descent have also enabled much better computational scaling in evaluation of the theoretical predictions [ 46 ] , allowing evaluation on full CIFAR - 10 . Requirements Width - N NN Static NTK Perturbative Full DMFT Memory for Kernels O ( N 2 ) O ( P 2 ) O ( P 4 T ) O ( P 2 T 2 ) Time for Kernels O ( PN 2 T ) O ( P 2 ) O ( P 4 T ) O ( P 3 T 3 ) Time for Final Outputs O ( PN 2 T ) O ( P 3 ) O ( P 4 ) O ( P 3 T 3 ) Table 1 : Computational requirements to compute kernel dynamics and trained network predictions on P points in a depth N neural network on a grid of T time points trained with P data points for various theories . DMFT is faster and less memory intensive than a width N network only if N (cid:29) PT . It is more computationally efﬁcient to compute full DMFT kernels than leading order perturbation theory when T (cid:28) √ P . The expensive scaling with both samples and time are the cost of a full - batch non - perturbative theory of gradient based feature learning dynamics . Acknowledgments and Disclosure of Funding This work was supported by NSF grant DMS - 2134157 and an award from the Harvard Data Science Initiative Competitive Research Fund . BB acknowledges additional support from the NSF - Simons Center for Mathematical and Statistical Analysis of Biology at Harvard ( award # 1764269 ) and the Harvard Q - Bio Initiative . BB thanks Jacob Zavatone - Veth , Alex Atanasov , Abdulkadir Canatar , and Ben Ruben for comments on this manuscript as well as Greg Yang , Boris Hanin , Yasaman Bahri , and Jascha Sohl - Dickstein for useful discussions . 10 References [ 1 ] Greg Yang and Edward J Hu . Tensor programs iv : Feature learning in inﬁnite - width neural networks . In International Conference on Machine Learning , pages 11727 – 11737 . PMLR , 2021 . [ 2 ] Ian Goodfellow , Yoshua Bengio , and Aaron Courville . Deep learning . MIT press , 2016 . [ 3 ] Yann LeCun , Yoshua Bengio , and Geoffrey Hinton . Deep learning . nature , 521 ( 7553 ) : 436 – 444 , 2015 . [ 4 ] Radford M Neal . Bayesian learning for neural networks , volume 118 . Springer Science & Business Media , 2012 . [ 5 ] Jaehoon Lee , Jascha Sohl - dickstein , Jeffrey Pennington , Roman Novak , Sam Schoenholz , and Yasaman Bahri . Deep neural networks as gaussian processes . In International Conference on Learning Representations , 2018 . [ 6 ] Alexander G . de G . Matthews , Jiri Hron , Mark Rowland , Richard E . Turner , and Zoubin Ghahramani . Gaussian process behaviour in wide deep neural networks . In International Conference on Learning Representations , 2018 . [ 7 ] Arthur Jacot , Franck Gabriel , and Clement Hongler . Neural tangent kernel : Convergence and generalization in neural networks . In S . Bengio , H . Wallach , H . Larochelle , K . Grauman , N . Cesa - Bianchi , and R . Garnett , editors , Advances in Neural Information Processing Systems , volume 31 , pages 8571 – 8580 . Curran Associates , Inc . , 2018 . [ 8 ] Jaehoon Lee , Lechao Xiao , Samuel Schoenholz , Yasaman Bahri , Roman Novak , Jascha Sohl - Dickstein , and Jeffrey Pennington . Wide neural networks of any depth evolve as linear models under gradient descent . Advances in neural information processing systems , 32 , 2019 . [ 9 ] Sanjeev Arora , Simon S Du , Wei Hu , Zhiyuan Li , Russ R Salakhutdinov , and Ruosong Wang . On exact computation with an inﬁnitely wide neural net . Advances in Neural Information Processing Systems , 32 , 2019 . [ 10 ] Simon Du , Jason Lee , Haochuan Li , Liwei Wang , and Xiyu Zhai . Gradient descent ﬁnds global minima of deep neural networks . In International conference on machine learning , pages 1675 – 1685 . PMLR , 2019 . [ 11 ] B . Bordelon , A . Canatar , and C . Pehlevan . Spectrum dependent learning curves in kernel regression and wide neural networks . International Conference of Machine Learning , 2020 . [ 12 ] Abdulkadir Canatar , Blake Bordelon , and Cengiz Pehlevan . Spectral bias and task - model alignment explain generalization in kernel regression and inﬁnitely wide neural networks . Nature communications , 12 ( 1 ) : 1 – 12 , 2021 . [ 13 ] Omry Cohen , Or Malka , and Zohar Ringel . Learning curves for overparametrized deep neural networks : A ﬁeld theory perspective . Physical Review Research , 3 ( 2 ) : 023034 , 2021 . [ 14 ] Arthur Jacot , Berﬁn Simsek , Francesco Spadaro , Clément Hongler , and Franck Gabriel . Kernel alignment risk estimator : Risk prediction from training data . Advances in Neural Information Processing Systems , 33 : 15568 – 15578 , 2020 . [ 15 ] Bruno Loureiro , Cedric Gerbelot , Hugo Cui , Sebastian Goldt , Florent Krzakala , Marc Mezard , and Lenka Zdeborova . Learning curves of generic features maps for realistic datasets with a teacher - student model . In A . Beygelzimer , Y . Dauphin , P . Liang , and J . Wortman Vaughan , editors , Advances in Neural Information Processing Systems , 2021 . [ 16 ] James B Simon , Madeline Dickens , and Michael R DeWeese . Neural tangent kernel eigenvalues accurately predict generalization . arXiv preprint arXiv : 2110 . 03922 , 2021 . [ 17 ] Zeyuan Allen - Zhu , Yuanzhi Li , and Zhao Song . A convergence theory for deep learning via over - parameterization . In International Conference on Machine Learning , pages 242 – 252 . PMLR , 2019 . 11 [ 18 ] Lenaic Chizat , Edouard Oyallon , and Francis Bach . On lazy training in differentiable program - ming . Advances in Neural Information Processing Systems , 32 , 2019 . [ 19 ] Mario Geiger , Stefano Spigler , Arthur Jacot , and Matthieu Wyart . Disentangling feature and lazy training in deep neural networks . Journal of Statistical Mechanics : Theory and Experiment , 2020 ( 11 ) : 113301 , 2020 . [ 20 ] Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , et al . Language models are few - shot learners . Advances in neural information processing systems , 33 : 1877 – 1901 , 2020 . [ 21 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770 – 778 , 2016 . [ 22 ] Song Mei , Andrea Montanari , and Phan - Minh Nguyen . A mean ﬁeld view of the landscape of two - layer neural networks . Proceedings of the National Academy of Sciences , 115 ( 33 ) : E7665 – E7671 , 2018 . [ 23 ] Laurence Aitchison . Why bigger is not always better : on ﬁnite and inﬁnite neural networks . In International Conference on Machine Learning , pages 156 – 164 . PMLR , 2020 . [ 24 ] Sho Yaida . Non - gaussian processes and neural networks at ﬁnite widths . In Mathematical and Scientiﬁc Machine Learning , pages 165 – 192 . PMLR , 2020 . [ 25 ] Jacob Zavatone - Veth , Abdulkadir Canatar , Ben Ruben , and Cengiz Pehlevan . Asymptotics of representation learning in ﬁnite bayesian neural networks . Advances in Neural Information Processing Systems , 34 , 2021 . [ 26 ] Gadi Naveh , Oded Ben David , Haim Sompolinsky , and Zohar Ringel . Predicting the outputs of ﬁnite deep neural networks trained with noisy gradients . Physical Review E , 104 ( 6 ) : 064301 , 2021 . [ 27 ] Daniel A Roberts , Sho Yaida , and Boris Hanin . The principles of deep learning theory . arXiv preprint arXiv : 2106 . 10165 , 2021 . [ 28 ] Boris Hanin . Correlation functions in random fully connected neural networks at ﬁnite width . arXiv preprint arXiv : 2204 . 01058 , 2022 . [ 29 ] Kai Segadlo , Bastian Epping , Alexander van Meegen , David Dahmen , Michael Krämer , and Moritz Helias . Uniﬁed ﬁeld theory for deep and recurrent neural networks , 2021 . [ 30 ] Gadi Naveh and Zohar Ringel . A self consistent theory of gaussian processes captures feature learning effects in ﬁnite cnns . Advances in Neural Information Processing Systems , 34 , 2021 . [ 31 ] Inbar Seroussi and Zohar Ringel . Separation of scales and a thermodynamic description of feature learning in some cnns . arXiv preprint arXiv : 2112 . 15383 , 2021 . [ 32 ] Qianyi Li and Haim Sompolinsky . Statistical mechanics of deep linear neural networks : The backpropagating kernel renormalization . Physical Review X , 11 ( 3 ) : 031059 , 2021 . [ 33 ] Jacob A Zavatone - Veth and Cengiz Pehlevan . Depth induces scale - averaging in overparame - terized linear bayesian neural networks . 55th Asilomar Conference on Signals , Systems , and Computers , 2021 . [ 34 ] Jiaoyang Huang and Horng - Tzer Yau . Dynamics of deep neural networks and neural tangent hierarchy . In International conference on machine learning , pages 4542 – 4551 . PMLR , 2020 . [ 35 ] Ethan Dyer and Guy Gur - Ari . Asymptotics of wide networks from feynman diagrams . arXiv preprint arXiv : 1909 . 11304 , 2019 . [ 36 ] Anders Andreassen and Ethan Dyer . Asymptotics of wide convolutional neural networks . arXiv preprint arXiv : 2008 . 08675 , 2020 . 12 [ 37 ] Jacob A Zavatone - Veth , William L Tong , and Cengiz Pehlevan . Contrasting random and learned features in deep bayesian linear regression . arXiv preprint arXiv : 2203 . 00573 , 2022 . [ 38 ] Aitor Lewkowycz , Yasaman Bahri , Ethan Dyer , Jascha Sohl - Dickstein , and Guy Gur - Ari . The large learning rate phase of deep learning : the catapult mechanism . arXiv preprint arXiv : 2003 . 02218 , 2020 . [ 39 ] Dyego Araújo , Roberto I Oliveira , and Daniel Yukimura . A mean - ﬁeld limit for certain deep neural networks . arXiv preprint arXiv : 1906 . 00193 , 2019 . [ 40 ] Lenaic Chizat and Francis Bach . On the global convergence of gradient descent for over - parameterized models using optimal transport . Advances in neural information processing systems , 31 , 2018 . [ 41 ] Grant M Rotskoff and Eric Vanden - Eijnden . Trainability and accuracy of neural networks : An interacting particle system approach . arXiv preprint arXiv : 1805 . 00915 , 2018 . [ 42 ] Song Mei , Theodor Misiakiewicz , and Andrea Montanari . Mean - ﬁeld theory of two - layers neural networks : dimension - free bounds and kernel limit . In Conference on Learning Theory , pages 2388 – 2464 . PMLR , 2019 . [ 43 ] Phan - Minh Nguyen . Mean ﬁeld limit of the learning dynamics of multilayer neural networks . arXiv preprint arXiv : 1902 . 02880 , 2019 . [ 44 ] Cong Fang , Jason Lee , Pengkun Yang , and Tong Zhang . Modeling from features : a mean - ﬁeld framework for over - parameterized deep neural networks . In Conference on learning theory , pages 1887 – 1936 . PMLR , 2021 . [ 45 ] Greg Yang , Edward Hu , Igor Babuschkin , Szymon Sidor , Xiaodong Liu , David Farhi , Nick Ryder , Jakub Pachocki , Weizhu Chen , and Jianfeng Gao . Tuning large neural networks via zero - shot hyperparameter transfer . Advances in Neural Information Processing Systems , 34 , 2021 . [ 46 ] Greg Yang , Michael Santacroce , and Edward J Hu . Efﬁcient computation of deep nonlinear inﬁnite - width neural networks that learn features . In International Conference on Learning Representations , 2022 . [ 47 ] Paul Cecil Martin , ED Siggia , and HA Rose . Statistical dynamics of classical systems . Physical Review A , 8 ( 1 ) : 423 , 1973 . [ 48 ] C De Dominicis . Dynamics as a substitute for replicas in systems with quenched random impurities . Physical Review B , 18 ( 9 ) : 4913 , 1978 . [ 49 ] Haim Sompolinsky and Annette Zippelius . Dynamic theory of the spin - glass phase . Physical Review Letters , 47 ( 5 ) : 359 , 1981 . [ 50 ] Haim Sompolinsky and Annette Zippelius . Relaxational dynamics of the edwards - anderson model and the mean - ﬁeld theory of spin - glasses . Physical Review B , 25 ( 11 ) : 6860 , 1982 . [ 51 ] G Ben Arous and Alice Guionnet . Large deviations for langevin spin glass dynamics . Probability Theory and Related Fields , 102 ( 4 ) : 455 – 509 , 1995 . [ 52 ] G Ben Arous and Alice Guionnet . Symmetric langevin spin glass dynamics . The Annals of Probability , 25 ( 3 ) : 1367 – 1422 , 1997 . [ 53 ] Gérard Ben Arous , Amir Dembo , and Alice Guionnet . Cugliandolo - kurchan equations for dynamics of spin - glasses . Probability theory and related ﬁelds , 136 ( 4 ) : 619 – 660 , 2006 . [ 54 ] A Crisanti and H Sompolinsky . Path integral approach to random neural networks . Physical Review E , 98 ( 6 ) : 062120 , 2018 . [ 55 ] Haim Sompolinsky , Andrea Crisanti , and Hans - Jurgen Sommers . Chaos in random neural networks . Physical review letters , 61 ( 3 ) : 259 , 1988 . 13 [ 56 ] Moritz Helias and David Dahmen . Statistical Field Theory for Neural Networks . Springer International Publishing , 2020 . [ 57 ] Lutz Molgedey , J Schuchhardt , and Heinz G Schuster . Suppressing chaos in neural networks by noise . Physical review letters , 69 ( 26 ) : 3717 , 1992 . [ 58 ] M Samuelides and Bruno Cessac . Random recurrent neural networks dynamics . The European Physical Journal Special Topics , 142 ( 1 ) : 89 – 122 , 2007 . [ 59 ] Kanaka Rajan , LF Abbott , and Haim Sompolinsky . Stimulus - dependent suppression of chaos in recurrent neural networks . Physical review e , 82 ( 1 ) : 011903 , 2010 . [ 60 ] Stefano Sarao Mannelli , Florent Krzakala , Pierfrancesco Urbani , and Lenka Zdeborova . Passed & spurious : Descent algorithms and local minima in spiked matrix - tensor models . In interna - tional conference on machine learning , pages 4333 – 4342 . PMLR , 2019 . [ 61 ] Stefano Sarao Mannelli , Giulio Biroli , Chiara Cammarota , Florent Krzakala , Pierfrancesco Urbani , and Lenka Zdeborová . Marvels and pitfalls of the langevin algorithm in noisy high - dimensional inference . Physical Review X , 10 ( 1 ) : 011057 , 2020 . [ 62 ] Francesca Mignacco , Pierfrancesco Urbani , and Lenka Zdeborová . Stochasticity helps to navigate rough landscapes : comparing gradient - descent - based algorithms in the phase retrieval problem . Machine Learning : Science and Technology , 2 ( 3 ) : 035029 , 2021 . [ 63 ] Elisabeth Agoritsas , Giulio Biroli , Pierfrancesco Urbani , and Francesco Zamponi . Out - of - equilibrium dynamical mean - ﬁeld equations for the perceptron model . Journal of Physics A : Mathematical and Theoretical , 51 ( 8 ) : 085002 , 2018 . [ 64 ] Francesca Mignacco , Florent Krzakala , Pierfrancesco Urbani , and Lenka Zdeborová . Dynamical mean - ﬁeld theory for stochastic gradient descent in gaussian mixture classiﬁcation . Advances in Neural Information Processing Systems , 33 : 9540 – 9550 , 2020 . [ 65 ] Michael Celentano , Chen Cheng , and Andrea Montanari . The high - dimensional asymptotics of ﬁrst order methods with random data . arXiv preprint arXiv : 2112 . 07572 , 2021 . [ 66 ] Francesca Mignacco and Pierfrancesco Urbani . The effective noise of stochastic gradient descent . arXiv preprint arXiv : 2112 . 10852 , 2021 . [ 67 ] Alessandro Manacorda , Grégory Schehr , and Francesco Zamponi . Numerical solution of the dynamical mean ﬁeld theory of inﬁnite - dimensional equilibrium liquids . The Journal of chemical physics , 152 ( 16 ) : 164506 , 2020 . [ 68 ] Kenji Fukumizu . Dynamics of batch learning in multilayer neural networks . In International Conference on Artiﬁcial Neural Networks , pages 189 – 194 . Springer , 1998 . [ 69 ] Andrew M Saxe , James L McClelland , and Surya Ganguli . Exact solutions to the nonlinear dynamics of learning in deep linear neural networks . arXiv preprint arXiv : 1312 . 6120 , 2013 . [ 70 ] Sanjeev Arora , Nadav Cohen , Noah Golowich , and Wei Hu . A convergence analysis of gradient descent for deep linear neural networks . In International Conference on Learning Representations , 2019 . [ 71 ] Madhu S Advani , Andrew M Saxe , and Haim Sompolinsky . High - dimensional dynamics of generalization error in neural networks . Neural Networks , 132 : 428 – 446 , 2020 . [ 72 ] Arthur Jacot , François Ged , Franck Gabriel , Berﬁn ¸Sim¸sek , and Clément Hongler . Deep linear networks dynamics : Low - rank biases induced by initialization scale and l2 regularization . arXiv preprint arXiv : 2106 . 15933 , 2021 . [ 73 ] Alexander Atanasov , Blake Bordelon , and Cengiz Pehlevan . Neural networks as kernel learners : The silent alignment effect . In International Conference on Learning Representations , 2022 . [ 74 ] Aitor Lewkowycz and Guy Gur - Ari . On the training dynamics of deep networks with l _ 2 regularization . Advances in Neural Information Processing Systems , 33 : 4790 – 4799 , 2020 . 14 [ 75 ] Ben Poole , Subhaneil Lahiri , Maithra Raghu , Jascha Sohl - Dickstein , and Surya Ganguli . Exponential expressivity in deep neural networks through transient chaos . Advances in neural information processing systems , 29 , 2016 . [ 76 ] Samuel S Schoenholz , Justin Gilmer , Surya Ganguli , and Jascha Sohl - Dickstein . Deep informa - tion propagation . International Conference of Learning Representations , 2017 . [ 77 ] Greg Yang and Samuel Schoenholz . Mean ﬁeld residual networks : On the edge of chaos . Advances in neural information processing systems , 30 , 2017 . [ 78 ] Greg Yang . Scaling limits of wide neural networks with weight sharing : Gaussian pro - cess behavior , gradient independence , and neural tangent kernel derivation . arXiv preprint arXiv : 1902 . 04760 , 2019 . [ 79 ] Greg Yang and Etai Littwin . Tensor programs iib : Architectural universality of neural tangent kernel training dynamics . In International Conference on Machine Learning , pages 11762 – 11772 . PMLR , 2021 . [ 80 ] Alex Krizhevsky , Geoffrey Hinton , et al . Learning multiple layers of features from tiny images . 2009 . [ 81 ] Haozhe Shan and Blake Bordelon . A theory of neural tangent kernel alignment and its inﬂuence on training , 2021 . [ 82 ] Stéphane d’Ascoli , Maria Reﬁnetti , and Giulio Biroli . Optimal learning rate schedules in high - dimensional non - convex optimization problems , 2022 . [ 83 ] James Bradbury , Roy Frostig , Peter Hawkins , Matthew James Johnson , Chris Leary , Dougal Maclaurin , George Necula , Adam Paszke , Jake VanderPlas , Skye Wanderman - Milne , and Qiao Zhang . JAX : composable transformations of Python + NumPy programs , 2018 . [ 84 ] Juha Honkonen . Ito and stratonovich calculuses in stochastic ﬁeld theory . arXiv preprint arXiv : 1102 . 1581 , 2011 . [ 85 ] Crispin W Gardiner et al . Handbook of stochastic methods , volume 3 . springer Berlin , 1985 . [ 86 ] Carl M Bender and Steven Orszag . Advanced mathematical methods for scientists and engineers I : Asymptotic methods and perturbation theory , volume 1 . Springer Science & Business Media , 1999 . [ 87 ] John Hubbard . Calculation of partition functions . Physical Review Letters , 3 ( 2 ) : 77 , 1959 . [ 88 ] Charles Stein . A bound for the error in the normal approximation to the distribution of a sum of dependent random variables . In Proceedings of the sixth Berkeley symposium on mathematical statistics and probability , volume 2 : Probability theory , volume 6 , pages 583 – 603 . University of California Press , 1972 . [ 89 ] Roman Novak , Lechao Xiao , Jaehoon Lee , Yasaman Bahri , Greg Yang , Jiri Hron , Daniel A Abolaﬁa , Jeffrey Pennington , and Jascha Sohl - Dickstein . Bayesian deep convolutional networks with many channels are gaussian processes . arXiv preprint arXiv : 1810 . 05148 , 2018 . [ 90 ] Greg Yang . Wide feedforward or recurrent neural networks of any architecture are gaussian processes . Advances in Neural Information Processing Systems , 32 , 2019 . [ 91 ] Adam X . Yang , Maxime Robeyns , Edward Milsom , Nandi Schoots , and Laurence Aitchison . A theory of representation learning in deep neural networks gives a deep generalisation of kernel methods , 2021 . [ 92 ] Yurii E Nesterov . A method for solving the convex programming problem with convergence rate o ( 1 / kˆ 2 ) . In Dokl . akad . nauk Sssr , volume 269 , pages 543 – 547 , 1983 . [ 93 ] Yurii Nesterov and Boris T Polyak . Cubic regularization of newton method and its global performance . Mathematical Programming , 108 ( 1 ) : 177 – 205 , 2006 . [ 94 ] Gabriel Goh . Why momentum really works . Distill , 2017 . 15 [ 95 ] Michael Muehlebach and Michael I Jordan . Optimization with momentum : Dynamical , control - theoretic , and symplectic perspectives . Journal of Machine Learning Research , 22 ( 73 ) : 1 – 50 , 2021 . [ 96 ] Mehran Kardar . Statistical physics of ﬁelds . Cambridge University Press , 2007 . Checklist 1 . For all authors . . . ( a ) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope ? [ Yes ] As described in the abstract and introduction , we provide a dynamical ﬁeld theory of deep networks based on kernel evolution . ( b ) Did you describe the limitations of your work ? [ Yes ] We have an explicit limitations as the last paragraph of the paper in Section 7 . ( c ) Did you discuss any potential negative societal impacts of your work ? [ N / A ] This work is theoretical and is very unlikely to present negative social impacts . ( d ) Have you read the ethics review guidelines and ensured that your paper conforms to them ? [ Yes ] 2 . If you are including theoretical results . . . ( a ) Did you state the full set of assumptions of all theoretical results ? [ Yes ] We describe that our theory holds for NN architectures in the inﬁnite - width N → ∞ limit . ( b ) Did you include complete proofs of all theoretical results ? [ Yes ] All claims made in the main text are supported by derivations in the Appendix . 3 . If you ran experiments . . . ( a ) Did you include the code , data , and instructions needed to reproduce the main experimental re - sults ( either in the supplemental material or as a URL ) ? [ Yes ] Code to reproduce experimental results is provided in the supplementary material . ( b ) Did you specify all the training details ( e . g . , data splits , hyperparameters , how they were chosen ) ? [ Yes ] We provide details of all experiments in C . ( c ) Did you report error bars ( e . g . , with respect to the random seed after running experiments multiple times ) ? [ Yes ] We provided errorbars in the alignment scores of DMFT as a function of width N in Figure 1 . All other runs were over a single wide network , where performance is predicted to concentrate over initialization . ( d ) Did you include the total amount of compute and the type of resources used ( e . g . , type of GPUs , internal cluster , or cloud provider ) ? [ Yes ] We mention our GPU usage in C . 2 . 4 . If you are using existing assets ( e . g . , code , data , models ) or curating / releasing new assets . . . ( a ) If your work uses existing assets , did you cite the creators ? [ Yes ] We cited the creators of Jax , Neural Tangents , and CIFAR - 10 . ( b ) Did you mention the license of the assets ? [ N / A ] These are all open source provided they are appropriately credited in academic research . ( c ) Did you include any new assets either in the supplemental material or as a URL ? [ N / A ] ( d ) Did you discuss whether and how consent was obtained from people whose data you’re using / curating ? [ N / A ] ( e ) Did you discuss whether the data you are using / curating contains personally identiﬁable information or offensive content ? [ N / A ] 5 . If you used crowdsourcing or conducted research with human subjects . . . ( a ) Did you include the full text of instructions given to participants and screenshots , if applica - ble ? [ N / A ] ( b ) Did you describe any potential participant risks , with links to Institutional Review Board ( IRB ) approvals , if applicable ? [ N / A ] ( c ) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation ? [ N / A ] 16 Appendix A Additional Figures 0 50 100 150 200 t 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 t 0 = 0 . 2 0 = 1 . 0 0 = 2 . 0 0 = 3 . 0 ( a ) Loss Dynamics 5 0 5 h 10 4 10 3 10 2 10 1 10 0 p ( h ) = 0 . 2 = 3 . 0 DMFT ( b ) Final h Distribution 10 0 10 z 10 4 10 3 10 2 10 1 10 0 p ( z ) = 0 . 2 = 3 . 0 DMFT ( c ) Final z Distribution T h e o r y 1 E x p t . 1 0 = 0 . 2 0 = 1 . 0 0 = 2 . 0 0 = 3 . 0 ( d ) Final Φ 1 Kernels T h e o r y G 1 E x p t . G 1 0 = 0 . 2 0 = 1 . 0 0 = 2 . 0 0 = 3 . 0 ( e ) Final G 1 Kernels Figure 6 : Self - consistent DFT reproduces two layer ( L = 1 hidden layer , width N = 2000 ) ReLU NN’s preactivation density , loss dynamics and learned kernel . ( a ) The loss is obtained by taking saddle point results for Φ , G and calculating the NTK’s dynamics . The γ 0 → 0 limit is governed by a static NTK , while the γ 0 > 0 network exhibits kernel evolution and accelerated training . ( b ) We plot the preactivation h distribution for neurons in the hidden layer of the trained NN against the theoretical densities deﬁned by Z [ Φ , G ] . For small γ 0 , the ﬁnal distribution is approximately Gaussian , but becomes non - Gaussian , asymmetric , and heavy tailed for large γ 0 . The DMFT estimate of the distribution is noisy due to ﬁnite sampling error . ( c ) The pre - gradient distribution p ( z ) in the trained network has larger ﬁnal variance for large γ 0 . ( d ) - ( e ) The ﬁnal Φ , G are accurately predicted by the ﬁeld theory and exhibit a block structure that increases with γ 0 due to feature learning . 17 0 20 40 60 80 100 t 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 t NN 0 = 0 NN 0 = 1 . 0 DMFT ( a ) Lazy vs Rich Loss Dynamics T h e o r y E x p t . 1 2 3 ( b ) Final Φ Kernels γ 0 = 1 T h e o r y E x p t . G 1 G 2 G 3 ( c ) Final G Kernels , γ 0 = 1 T r ( t , s ) = 1 T r G ( t , s ) = 2 = 3 ( d ) Φ , G Temporal Dynamics γ 0 = 1 Figure 7 : Self - consistent DFT reproduces loss dynamics , and kernels through time in a L = 3 tanh network . ( a ) The loss when training on synthetic data is obtained by taking saddle point results for Φ , G and calculating the NTK’s dynamics . The γ 0 → 0 limit is governed by a static NTK , while the γ 0 > 0 network exhibits kernel evolution and accelerated training . Solid lines are a N = 2000 NN and dashed lines are from solving DMFT equations . ( b ) - ( c ) The ﬁnal learned kernels Φ ( b ) and G ( c ) are accurately predicted by the ﬁeld theory and exhibits block structure due to clustering by class identity . ( d ) The temporal components of Φ , G reveals nontrivial dynamical structure . 0 50 100 150 200 t 0 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 ( t ) 0 = 0 . 5 0 = 1 . 0 0 = 5 . 0 0 = 10 . 0 0 = 50 . 0 ( a ) Two Layer Error Dynamics 0 50 100 150 200 t 0 10 0 10 1 y H ( t ) y 0 = 0 . 5 0 = 1 . 0 0 = 5 . 0 0 = 10 . 0 0 = 50 . 0 ( b ) Projection on Target Figure 8 : The error and kernel dynamics obtained by solving a one dimensional ODE system for a depth 2 linear network . ( a ) The ∆ ( t ) error dynamics from F . 1 . 1 allows one to solve for H ( t ) by solving a one dimensional ODE at each value of γ 0 . The learning curves interpolate between exponential convergence at small γ 0 and logistic sigmoidal trajectories at large γ 0 . ( b ) The projection of the kernel H ( t ) along the task relevant subspace y ∈ R P . 18 0 20 40 60 80 100 t 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 t L = 2 L = 3 L = 4 L = 5 ( a ) Grad . Independence DMFT E x p t . H 1 G r a d . I n d e p . H 2 H 3 H 4 H 5 ( b ) Grad . Independence Predicted Feature Kernels Figure 9 : Gradient independence fails to characterize feature learning dynamics in networks with L > 1 and large γ 0 . ( a ) Loss curves for deep linear networks predicted under gradient independence ansatz for γ 0 = 1 . 5 . ( b ) The predicted and experimental feature kernels H (cid:96) for the L = 5 hidden layer network demonstrate that gradient independence underestimates the size of kernel adaptation . 10 2 10 3 10 4 t 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 T e s t L o ss N = 250 , 0 = 1 N = 500 , 0 = 1 N = 250 , 0 = 5 N = 500 , 0 = 5 N = 250 , 0 = 10 N = 500 , 0 = 10 N = 250 , 0 = 20 N = 500 , 0 = 20 N = 250 , 0 = 30 N = 500 , 0 = 30 ( a ) Test MSE Loss 10 2 10 3 10 4 t 0 . 50 0 . 55 0 . 60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 T e s t A cc N = 250 , 0 = 1 N = 500 , 0 = 1 N = 250 , 0 = 5 N = 500 , 0 = 5 N = 250 , 0 = 10 N = 500 , 0 = 10 N = 250 , 0 = 20 N = 500 , 0 = 20 N = 250 , 0 = 30 N = 500 , 0 = 30 ( b ) Test Classiﬁcation Accuracy 10 2 10 3 10 4 t 0 . 000 0 . 005 0 . 010 0 . 015 0 . 020 0 . 025 0 . 030 0 . 035 A li g n m e n t N = 250 , 0 = 1 N = 500 , 0 = 1 N = 250 , 0 = 5 N = 500 , 0 = 5 N = 250 , 0 = 10 N = 500 , 0 = 10 N = 250 , 0 = 20 N = 500 , 0 = 20 N = 250 , 0 = 30 N = 500 , 0 = 30 ( c ) A ( Φ L , yy (cid:62) ) Dynamics Figure 10 : Repeating the experiment of Figure 5 with depth 7 ( L = 6 hidden layer ) CNN trained on two class CIFAR over a wide range of γ 0 with N ∈ { 250 , 500 } . We ﬁnd consistent agreement of loss and prediction dynamics across widths but ﬁnite size effects become more signiﬁcant when computing feature kernels of deeper layers . We note that , while higher γ 0 is associated with faster convergence , the ﬁnal test accuracy for this model is roughly insensitive to choice of γ 0 . B Algorithmic Implementation The alternating sample - and - solve procedure we developed and describe below for nonlinear networks is based on numerical recipes used in the dynamical mean ﬁeld simulations in computational physics [ 67 ] . The basic principle is to leverage the fact that , conditional on kernels , we can easily draw 19 samples { u (cid:96)µ ( t ) , r (cid:96)µ ( t ) } from their appropriate GPs . From these sampled ﬁelds , we can identify the kernel order parameters by simple estimation of the appropriate moments . Algorithm 1 : Alternating Monte - Carlo Solution to Saddle Point Equations Data : K x , y , Initial Guesses { Φ (cid:96) , G (cid:96) } L(cid:96) = 1 , { A (cid:96) , B (cid:96) } L − 1 (cid:96) = 1 , Sample count S , Update Speed β Result : Final Kernels { Φ (cid:96) , G (cid:96) } L(cid:96) = 1 , { A (cid:96) , B (cid:96) } L − 1 (cid:96) = 1 , Network predictions through training f µ ( t ) 1 Φ 0 = K x ⊗ 11 (cid:62) , G L + 1 = 11 (cid:62) ; 2 while Kernels Not Converged do 3 From { Φ (cid:96) , G (cid:96) } compute K NTK ( t , t ) and solve ddt f µ ( t ) = (cid:80) α ∆ α ( t ) K NTKµα ( t , t ) ; 4 (cid:96) = 1 ; 5 while (cid:96) < L + 1 do 6 Draw S samples { u (cid:96)µ , n ( t ) } S n = 1 ∼ GP ( 0 , Φ (cid:96) − 1 ) , { r (cid:96)µ , n ( t ) } S n = 1 ∼ GP ( 0 , G (cid:96) + 1 ) ; 7 Solve equation ( 10 ) for each sample to get { h (cid:96)µ , n ( t ) , z (cid:96)µ , n ( t ) } S n = 1 ; 8 Compute new Φ (cid:96) , G (cid:96) estimates : 9 ˜Φ (cid:96)µα ( t , s ) = 1 S (cid:80) n ∈ [ S ] φ ( h (cid:96)µ , n ( t ) ) φ ( h (cid:96)α , n ( s ) ) , ˜ G (cid:96)µα ( t , s ) = 1 S (cid:80) n ∈ [ S ] g (cid:96)µ , n ( t ) g (cid:96)α , n ( s ) ; 10 Solve for Jacobians on each sample ∂φ ( h (cid:96)n ) ∂ r (cid:96) (cid:62) n , ∂ g (cid:96)n ∂ u (cid:96) (cid:62) n ; 11 Compute new A (cid:96) , B (cid:96) − 1 estimates : 12 ˜ A (cid:96) = 1 S (cid:80) n ∈ [ S ] ∂φ ( h (cid:96)n ) ∂ r (cid:96) (cid:62) n , ˜ B (cid:96) − 1 = 1 S (cid:80) n ∈ [ S ] ∂ g (cid:96)n ∂ u (cid:96) (cid:62) n ; 13 (cid:96) ← (cid:96) + 1 ; 14 end 15 (cid:96) = 1 ; 16 while (cid:96) < L + 1 do 17 Update feature kernels : Φ (cid:96) ← ( 1 − β ) Φ (cid:96) + β ˜ Φ (cid:96) , G (cid:96) ← ( 1 − β ) G (cid:96) + β ˜ G (cid:96) ; 18 if (cid:96) < L then 19 Update A (cid:96) ← ( 1 − β ) A (cid:96) + β ˜ A (cid:96) , B (cid:96) ← ( 1 − β ) B (cid:96) + β ˜ B (cid:96) 20 end 21 (cid:96) ← (cid:96) + 1 22 end 23 end 24 return { Φ (cid:96) , G (cid:96) } L(cid:96) = 1 , { A (cid:96) , B (cid:96) } L − 1 (cid:96) = 1 , { f µ ( t ) } Pµ = 1 The parameter β controls recency weighting of the samples obtained at each iteration . If β = 1 , then the rank of the kernel estimates is limited to the number of samples S used in a single iteration , but with β < 1 smaller sample sizes S can be used to still obtain accurate results . We used β = 0 . 6 in our deep network experiments . Convergence is usually achieved in around ∼ 15 steps for a depth 4 ( L = 3 hidden layer ) network such as the one in Figure 1 and 7 . C Experimental Details All NN training was performed with Jax gradient descent optimizer [ 83 ] with ﬁxed learning rate . C . 1 MLP Experiments For the MLP experiments , we performed full batch gradient descent . Networks were initialized with Gaussian weights with unit standard deviation W (cid:96)ij ∼ N ( 0 , 1 ) . The learning rate was chosen as η 0 γ 2 = η 0 γ 20 N for a network of width N . The hidden features h (cid:96)µ ( t ) ∈ R N were stored throughout training and used to compute the kernels Φ (cid:96)µα ( t , s ) = 1 N φ ( h (cid:96)µ ( t ) ) · φ ( h (cid:96)α ( s ) ) . These experiments can be reproduced with provided jupyter notebooks . C . 2 CNN Experiments on CIFAR - 10 We deﬁne a depth L CNN model with ReLU activations and stride 1 , which is implemented as a pytree of parameters in JAX [ 83 ] . We apply global average pooling in the ﬁnal layer before a dense readout layer . The code to initialize and evaluate the model is provided below . 20 1 from jax import random , lax 2 import jax . numpy as jnp 3 4 # L : number of hidden layers , N : width 5 def initialize _ cnn ( L , N , seed = 0 ) : 6 key = random . PRNGKey ( seed ) 7 params = [ ] # creates list of L + 1 weights 8 params + = [ random . normal ( key , ( 3 , 3 , 3 , N ) ) ] # HWIO 9 for l in range ( L - 1 ) : 10 key , _ = random . split ( key ) 11 params + = [ random . normal ( key , ( 3 , 3 , N , N ) ) ] 12 params + = [ random . normal ( key , ( N , ) ) ] 13 return params 14 15 dn = lax . conv _ dimension _ numbers ( ( 1 , 3 , 3 , 3 ) , ( 3 , 3 , 3 , 1 ) , ( ’NHWC’ , ’HWIO’ , ’ NHWC’ ) ) # defines which axis used for convolution 16 nonlin _ fn = lambda h : ( h > 0 . 0 ) * h # ReLU activation 17 def cnn ( params , X ) : 18 L = len ( params ) - 1 # number of hidden layers 19 N = params [ 0 ] . shape [ - 1 ] # width 20 h = lax . conv _ general _ dilated ( X , params [ 0 ] , ( 1 , 1 ) , ’SAME’ , ( 1 , 1 ) , ( 1 , 1 ) , dn ) # h1 21 phi = nonlin _ fn ( h ) # phi ( h1 ) 22 for i in range ( 1 , L - 1 ) : 23 h = 1 / jnp . sqrt ( N ) * lax . conv _ general _ dilated ( phi , params [ i ] , ( 1 , 1 ) , ’SAME’ , ( 1 , 1 ) , ( 1 , 1 ) , dn ) # recurrence for h 24 phi = nonlin _ fn ( h ) # phi ( h ) 25 phi = phi . mean ( axis = ( 1 , 2 ) ) # global average pooling 26 w = params [ - 1 ] 27 f = 1 / N * phi @ w # Mean - field parameterization 28 return f After constructing a CNN model , we train using MSE loss with base learning rate η 0 = 2 . 0 × 10 − 4 , batch size 250 . The learning rate passed to the optimizer is thus η = η 0 γ 2 = η 0 γ 20 N . We optimize the loss function which is scaled appropriately as (cid:96) ( γ − 1 0 f , y ) . Throughout training , we compute the last layer’s embedding φ ( h L ) on the test set to calculate the alignment A ( Φ L , yy (cid:62) ) . Training was performed on 4 NVIDIA GPUs . Training a L = 3 network of width 500 takes roughly 1 hour . D Derivation of Self - Consistent Dynamical Field Theory In this section , we introduce the dynamical ﬁeld theory setup and saddle point equations . The path integral theory we develop is based on the Martin - Siggia - Rose - De Dominicis - Janssen ( MSRDJ ) framework [ 47 ] , of which a useful review for random recurent networks can be found here [ 54 ] . Sim - ilar computations can be found in recent works which consider typical behavior in high dimensional classiﬁcation on random data [ 63 , 64 ] . D . 1 Deep Network Field Deﬁnitions and Scaling As discussed in the main text , we consider the following wide network architecture parameterzied by trainable weights θ = Vec { W 0 , W 1 , . . . w L } , giving network output f µ deﬁned as f µ = 1 γ h L + 1 µ , h L + 1 µ = 1 √ N w L · φ ( h Lµ ) h (cid:96) + 1 µ = 1 √ N W (cid:96) φ ( h (cid:96)µ ) , h 1 µ = 1 √ D W 0 x µ ( 13 ) 21 Using gradient ﬂow with learning rate η on cost L = (cid:80) µ (cid:96) ( f µ , y µ ) for loss function , we introduce functions ∆ µ = − ∂ L ∂f µ and η for learning rate , gradient ﬂow induces the following dynamics d θ dt = η γ (cid:88) µ ∆ µ ∂h L + 1 µ ∂ θ , ∂f µ ∂t = η γ 2 (cid:88) α ∆ α K NTKµα , K NTKµα = ∂h L + 1 µ ∂ θ · ∂h L + 1 α ∂ θ ( 14 ) Since K NTK is O γ ( 1 ) at initialization , it is clear that to have O γ ( 1 ) evolution of the network output at initialization we need η = γ 2 . With this scaling , we have the following d θ dt = γ (cid:88) µ ∆ µ ∂h L + 1 µ ∂ θ , ∂f µ ∂t = (cid:88) α ∆ α K NTKµα ( 15 ) Now , to build a valid ﬁeld theory , we want to express everything in terms of features h (cid:96)µ rather than parameters θ and we will deﬁne the following gradient features g (cid:96)µ = √ N ∂h L + 1 µ ∂ h (cid:96)µ which admit the recursion and base case g (cid:96)µ = √ N ∂h L + 1 µ ∂ h (cid:96)µ = (cid:32) ∂ h (cid:96) + 1 µ ∂ h (cid:96)µ (cid:33) (cid:62) (cid:32) √ N ∂h L + 1 µ ∂ h (cid:96) + 1 µ (cid:33) = ˙ φ ( h (cid:96)µ ) (cid:12) z (cid:96)µ , z (cid:96)µ = 1 √ N W (cid:96) (cid:62) g (cid:96) + 1 µ g Lµ = ˙ φ ( h Lµ ) (cid:12) w L ( 16 ) We deﬁne the pre - gradient ﬁeld z (cid:96)µ = 1 √ N W (cid:96) (cid:62) g (cid:96) + 1 µ so that g (cid:96)µ = ˙ φ ( h (cid:96)µ ) (cid:12) z (cid:96)µ ( t ) . From these quantities , we can derive the gradients with respect to parameters ∂h L + 1 µ ∂ W (cid:96) = N (cid:88) i = 1 ∂h L + 1 µ ∂h (cid:96) + 1 µ , i ∂h (cid:96) + 1 µ , i ∂ W (cid:96) = 1 N g (cid:96) + 1 µ φ ( h (cid:96)µ ) (cid:62) ( 17 ) which allows us to compute the NTK in terms of these features K NTKµα = 1 N φ ( h Lµ ) · φ ( h Lα ) + L − 1 (cid:88) (cid:96) = 1 (cid:32) g (cid:96) + 1 µ · g (cid:96) + 1 α N (cid:33) (cid:32) φ ( h (cid:96)µ ) · φ ( h (cid:96)α ) N (cid:33) + g 1 µ · g 1 α N K xµα ( 18 ) where K xµα = x µ · x α D is the input Grammian . We see that the NTK can be built out of the following primitive kernels Φ (cid:96)µν = 1 N φ ( h (cid:96)µ ) · φ ( h (cid:96)ν ) , G (cid:96)µν = 1 N g (cid:96)µ · g (cid:96)ν ( 19 ) We utilize the parameter space dynamics to express W (cid:96) in terms of the { g , h } ﬁelds W (cid:96) ( t ) = W (cid:96) ( 0 ) + γ N (cid:90) t 0 ds (cid:88) µ ∆ α ( s ) g (cid:96) + 1 µ ( s ) φ ( h (cid:96)µ ( s ) ) (cid:62) ( 20 ) Using the ﬁeld recurrences h (cid:96) + 1 µ ( t ) = 1 √ N W (cid:96) ( t ) φ ( h (cid:96)µ ( t ) ) we can derive the following recursive dynamics for the features h (cid:96) + 1 µ ( t ) = χ (cid:96) + 1 µ ( t ) + γ √ N (cid:90) t 0 ds (cid:88) ν ∆ ν g (cid:96) + 1 ν ( t ) Φ (cid:96)νµ ( s , t ) z (cid:96)µ ( t ) = ξ (cid:96)µ ( t ) + γ √ N (cid:90) t 0 ds (cid:88) ν ∆ ν ( s ) φ ( h (cid:96)ν ( s ) ) G (cid:96) + 1 νµ ( s , t ) , g (cid:96)µ ( t ) = ˙ φ ( h (cid:96)µ ( t ) ) (cid:12) z (cid:96)µ ( t ) ∂f µ ∂t = (cid:88) α ∆ α ( t ) (cid:34) Φ Lµα ( t , t ) + L − 1 (cid:88) (cid:96) = 1 G (cid:96) + 1 µα ( t , t ) Φ (cid:96)µα ( t , t ) + G 1 µα ( t , t ) K xµα (cid:35) ( 21 ) where we introduced the following random ﬁelds χ (cid:96)µ ( t ) , ξ (cid:96)µ ( t ) which involve the random initial conditions χ (cid:96) µ ( t ) = 1 √ N W (cid:96) ( 0 ) φ ( h (cid:96) µ ( t ) ) , ξ (cid:96) µ ( t ) = 1 √ N W (cid:96) ( 0 ) (cid:62) g (cid:96) + 1 µ ( t ) ( 22 ) We observe that the dynamics of the hidden features is controlled by the factor γ √ N . If γ = O N ( 1 ) then we recover static NTK in the limit as N → ∞ . However , if γ = O N ( √ N ) then we obtain O N ( 1 ) evolution of our features and we reach a new rich regime . We choose the scaling γ = γ 0 √ N for our ﬁeld theory so that γ 0 > 0 will give a feature learning network . 22 D . 2 Warmup : DMFT for One Hidden Layer NN In this section , we provide a warmup problem of a L = 1 hidden layer network which allows us to illustrate the mechanics of the MSRDJ formalism . A more detailed computation can be found in the next section . Though many of the interesting dynamical aspects of the deep network case are missing in the two layer case , our aim is to show a simple application of the ideas . The ﬁelds of interest are χ µ = 1 √ D W 0 ( 0 ) x µ and ξ = w 1 ( 0 ) . Unlike the deeper L ≥ 2 case , both of these ﬁelds are time invariant since x µ does not vary in time . These random ﬁelds provide initial conditions for the preactivation and pre - gradient ﬁelds h µ ( t ) , z ( t ) ∈ R N , which evolve according to h µ ( t ) = χ µ + γ 0 (cid:90) t 0 ds (cid:88) α [ z ( s ) (cid:12) ˙ φ ( h α ( s ) ) ] K xµα ∆ α ( s ) z ( t ) = ξ + γ 0 (cid:90) t 0 ds (cid:88) α φ ( h α ( s ) ) ∆ α ( s ) . ( 23 ) where the network predictions evolve as ∂∂t f µ ( t ) = (cid:80) α [ Φ µα ( t , t ) + G µα ( t , t ) K xµα ] ∆ α ( t ) for kernels Φ µα ( t , t ) = 1 N φ ( h µ ( t ) ) · φ ( h α ( t ) ) and G µα ( t , t ) = 1 N g µ ( t ) · g α ( t ) . At ﬁnite N , the kernels Φ , G will depend on the random initial conditions χ , ξ , leading to a predictor f µ which varies over initializations . If we can establish that the kernels Φ , G concentrate at inﬁnite - width N → ∞ , then ∆ µ are deterministic . We now study the moment generating function for the ﬁelds Z [ { j µ } µ ∈ [ P ] , v ] = (cid:42) exp (cid:32)(cid:88) µ j µ · χ µ + ξ · v (cid:33)(cid:43) θ 0 . ( 24 ) To perform the average over θ 0 = { W 0 ( 0 ) , w 1 ( 0 ) } , we enforce the deﬁnition of χ µ , ξ with delta functions 1 = (cid:90) d χ µ δ (cid:18) χ µ − 1 √ N W 0 ( 0 ) x µ (cid:19) = (cid:90) d χ µ d ˆ χ µ ( 2 π ) N exp (cid:18) i ˆ χ µ · (cid:18) χ µ − 1 √ D W 0 ( 0 ) x µ (cid:19)(cid:19) 1 = (cid:90) d ξ δ (cid:0) ξ − w 1 ( 0 ) (cid:1) = (cid:90) d ξ d ˆ ξ ( 2 π ) N exp (cid:16) i ˆ ξ · (cid:0) ξ − w 1 ( 0 ) (cid:1)(cid:17) . ( 25 ) Though this step may seem redundant in this example , it will be very helpful in the deep network case , so we pursue it for illustration . After mulitplying by these factors of unity and performing the Gaussian integrals , we obtain Z = (cid:90) (cid:89) µ d χ d ˆ χ ( 2 π ) N d ξ d ˆ ξ ( 2 π ) N exp (cid:32) − 1 2 (cid:88) µα ˆ χ µ · ˆ χ α K xµα + (cid:88) µ χ µ · ( i ˆ χ µ + j µ ) − 1 2 | ˆ ξ | 2 + ξ · ( i ˆ ξ + v ) (cid:33) ( 26 ) We now aim enforce the deﬁnitions of the kernel order parameters with delta functions 1 = N (cid:90) d Φ µα ( t , s ) δ ( N Φ µα ( t , s ) − φ ( h µ ( t ) ) · φ ( h α ( s ) ) ) = (cid:90) d Φ µα ( t , s ) d ˆΦ µα ( t , s ) 2 πiN − 1 exp (cid:16) N ˆΦ µα ( t , s ) ( N Φ µα ( t , s ) − φ ( h µ ( t ) ) · φ ( h α ( s ) ) ) (cid:17) 1 = N (cid:90) dG µα ( t , s ) δ ( NG µα ( t , s ) − g µ ( t ) · g α ( s ) ) = (cid:90) dG µα ( t , s ) d ˆ G µα ( t , s ) 2 πiN − 1 exp (cid:16) N ˆ G µα ( t , s ) ( NG µα ( t , s ) − g µ ( t ) · g α ( s ) ) (cid:17) , ( 27 ) where the ﬁelds h µ ( t ) , g µ ( t ) are regarded as functions of { χ µ } µ , ξ ( see Equation ( 23 ) ) and the ˆΦ , ˆ G integrals run over the imaginary axis ( − i ∞ , i ∞ ) . After this step , we can write Z ∝ (cid:90) (cid:89) µαts d Φ µα ( t , s ) d ˆΦ µα ( t , s ) dG µα ( t , s ) d ˆ G µα ( t , s ) exp (cid:16) NS [ Φ , ˆΦ , G , ˆ G ] (cid:17) ( 28 ) 23 where the DMFT action S [ Φ , ˆΦ , G , ˆ G ] is O N ( 1 ) and has the form S [ Φ , ˆΦ , G , ˆ G ] = (cid:88) µα (cid:90) dtds [ Φ µα ( t , s ) ˆΦ µα ( t , s ) + G µα ( t , s ) ˆ G µα ( t , s ) ] + 1 N N (cid:88) i = 1 ln Z [ j i , v i ] . ( 29 ) The single site moment generating function Z [ j , v ] arises from the factorization of the integrals over N different ﬁelds in the hidden layer and takes the form Z [ j , v ] = (cid:90) (cid:89) µ dχ µ d ˆ χ µ 2 π dξd ˆ ξ 2 π exp (cid:32) − 1 2 (cid:88) µα ˆ χ µ ˆ χ α K xµα + ( j µ + i ˆ χ µ ) χ µ − 1 2 ˆ ξ 2 + ( v + i ˆ ξ ) ξ (cid:33) × exp (cid:32) − (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds (cid:88) µα [ ˆΦ µα ( t , s ) φ ( h µ ( t ) ) φ ( h α ( s ) ) + ˆ G µα ( t , s ) g µ ( t ) g α ( s ) ] (cid:33) ( 30 ) where , again we must regard h µ ( t ) , g µ ( t ) as functions of χ , ξ . The variables in the above are no longer vectors in R N but rather are scalars . We can write Z [ j , v ] = (cid:82) (cid:81) µ dχ µ d ˆ χ µ dξd ˆ ξ exp (cid:16) −H [ { χ µ , ˆ χ µ } , ξ , ˆ ξ , j , v ] (cid:17) where H is the logarithm of the integrand above . Since the full MGF takes the form Z ∝ (cid:82) d Φ d ˆΦ dGd ˆ G exp (cid:16) NS [ Φ , ˆΦ , G , ˆ G ] (cid:17) , characterization of the N → ∞ limit requires one to identify the saddle point of S , where δS = 0 for any variation of these 4 order parameters . δS δ Φ µα ( t , s ) = ˆΦ µα ( t , s ) = 0 , δS δ ˆΦ µα ( t , s ) = Φ µα ( t , s ) − 1 N N (cid:88) i = 1 (cid:104) φ ( h µ ( t ) ) φ ( h α ( s ) ) (cid:105) i = 0 δS δG µα ( t , s ) = ˆ G µα ( t , s ) = 0 , δS δ ˆ G µα ( t , s ) = G µα ( t , s ) − 1 N N (cid:88) i = 1 (cid:104) g µ ( t ) g α ( s ) (cid:105) i = 0 ( 31 ) where the i - th single site average (cid:104)(cid:105) i of an observable O ( χ , ˆ χ , ξ , ˆ ξ ) is deﬁned as (cid:68) O ( χ , ˆ χ , ξ , ˆ ξ ) (cid:69) i = 1 Z [ j i , v i ] (cid:90) (cid:89) µ dχ µ d ˆ χ µ dξd ˆ ξ exp (cid:16) −H [ { χ µ , ˆ χ µ } , ξ , ˆ ξ , j i , v i ] (cid:17) O ( χ , ˆ χ , ξ , ˆ ξ ) ( 32 ) Since ˆΦ = ˆ G = 0 the single site MGF reveals that the initial ﬁelds are independent Gaussians { χ µ } ∼ N ( 0 , K x ) and ξ ∼ N ( 0 , 1 ) . At zero source j , v → 0 , all single site averages (cid:104)(cid:105) i are equivalent and we may merely write Φ µα ( t , s ) = (cid:104) φ ( h µ ( t ) ) φ ( h α ( s ) ) (cid:105) , G µα ( t , s ) = (cid:104) g µ ( t ) g α ( s ) (cid:105) , where (cid:104)(cid:105) is the average over the single site distributions for j , v → 0 . D . 2 . 1 Final L = 1 DMFT equations Putting all of the saddle point equations together , we arrive at the following DMFT { χ µ } µ ∈ [ P ] ∼ N ( 0 , K x ) , ξ ∼ N ( 0 , 1 ) h µ ( t ) = χ µ + γ 0 (cid:90) t 0 ds (cid:88) α [ z ( s ) ˙ φ ( h α ( s ) ) ] K xµα ∆ α ( s ) , z ( t ) = ξ + γ 0 (cid:90) t 0 ds (cid:88) α φ ( h α ( s ) ) ∆ α ( s ) Φ µα ( t , s ) = (cid:104) φ ( h µ ( t ) ) φ ( h α ( s ) ) (cid:105) , G µα ( t , s ) = (cid:104) g µ ( t ) g α ( s ) (cid:105) = (cid:68) z ( t ) z ( s ) ˙ φ ( h µ ( t ) ) ˙ φ ( h α ( s ) ) (cid:69) ∂f µ ∂t = (cid:88) α [ Φ µα ( t , t ) + G µα ( t , t ) K xµα ] ∆ α ( t ) ( 33 ) We see that for L = 1 networks , it sufﬁces to solve for the kernels on the time - time diagonal . Further in this two layer case χ , ξ are independent and do not vary in time . These facts will not hold in general for L ≥ 2 networks , which requires a more intricate analysis as we show in the next section . 24 D . 3 Path Integral Formulation for Deep Networks As discussed in the main text , we study the distribution over ﬁelds by computing the moment generating functional for the stochastic processes { χ (cid:96) , ξ (cid:96) } L(cid:96) = 1 Z [ { j (cid:96) , v (cid:96) } ] = (cid:42) exp  (cid:88) (cid:96) , µ (cid:90) ∞ 0 dt (cid:2) j (cid:96)µ ( t ) · χ (cid:96)µ ( t ) + v (cid:96)µ ( t ) · ξ (cid:96)µ ( t ) (cid:3)(cid:43) θ 0 = Vec { W 0 ( 0 ) , . . . w L ( 0 ) } ( 34 ) Moments of these stochastic ﬁelds can be computed through differentiation of Z near zero - source (cid:10) χ (cid:96) 1 µ 1 ( t 1 ) . . . χ (cid:96) n µ n ( t n ) ξ (cid:96) 1 µ 1 ( t 1 ) . . . ξ (cid:96) m µ m ( t m ) (cid:11) = δ δj (cid:96) 1 µ 1 ( t 1 ) . . . δ δj (cid:96) n µ n ( t n ) δ δv (cid:96) 1 µ 1 ( t 1 ) . . . δ δv (cid:96) m µ m ( t m ) Z [ { j (cid:96) , v (cid:96) } ] | j = v = 0 . ( 35 ) To perform the average over the initial parameters , we enforce the deﬁnition of the ﬁelds χ (cid:96) + 1 ( t ) = 1 √ N W (cid:96) ( 0 ) φ ( h (cid:96)µ ( t ) ) , ξ (cid:96)µ ( t ) = 1 √ N W (cid:96) ( 0 ) (cid:62) g (cid:96) + 1 µ ( t ) , by inserting the following terms in the def - inition of Z [ { j , v } ] so we may more easily perform the average over weights θ 0 . We enforce these deﬁnitions with an integral representation of the Dirac - Delta function 1 = (cid:82) R dx δ ( x ) = 12 π (cid:82) R dx (cid:82) R d ˆ x exp ( ix ˆ x ) . We note that we are implicitly working in the Ito scheme , where factors of Jacobian determinants are equal to one [ 54 , 84 , 85 ] ( we note that h (cid:96)µ ( t ) does not causally depend on χ (cid:96) + 1 µ ( t ) and g (cid:96)µ ( t ) does not causally depend on ξ (cid:96) ( t ) ) . Applying this to ﬁelds χ , ξ , we have 1 = (cid:90) R N (cid:90) R N d χ 1 µ ( t ) d ˆ χ 1 µ ( t ) ( 2 π ) N exp (cid:18) i ˆ χ 1 µ ( t ) · (cid:20) χ 1 µ ( t ) − 1 √ D W (cid:96) ( 0 ) x µ (cid:21)(cid:19) 1 = (cid:90) R N (cid:90) R N d χ (cid:96) + 1 µ ( t ) d ˆ χ (cid:96) + 1 µ ( t ) ( 2 π ) N exp (cid:18) i ˆ χ (cid:96) + 1 µ ( t ) · (cid:20) χ (cid:96) + 1 µ ( t ) − 1 √ N W (cid:96) ( 0 ) φ ( h (cid:96)µ ( t ) ) (cid:21)(cid:19) , (cid:96) ∈ { 1 , . . . , L − 1 } 1 = (cid:90) R N (cid:90) R N d ξ Lµ ( t ) d ˆ ξ Lµ ( t ) ( 2 π ) N exp (cid:16) i ˆ ξ Lµ ( t ) · (cid:2) ξ Lµ ( t ) − w L ( 0 ) (cid:3)(cid:17) 1 = (cid:90) R N (cid:90) R N d ξ (cid:96)µ ( t ) d ˆ ξ (cid:96)µ ( t ) ( 2 π ) N exp (cid:18) i ˆ ξ (cid:96)µ ( t ) · (cid:20) ξ (cid:96)µ ( t ) − 1 √ N W (cid:96) ( 0 ) (cid:62) g (cid:96)µ ( t ) (cid:21)(cid:19) , (cid:96) ∈ { 1 , . . . , L − 1 } ( 36 ) where { h (cid:96) , g (cid:96) } are understood to be stochastic processes which are causally determined by the { χ (cid:96) , ξ (cid:96) } ﬁelds , in the sense that h (cid:96) ( t ) only depends on χ (cid:96) ( s ) for s < t . We thus have an expression of the form Z [ { j (cid:96) , v (cid:96) } ] = (cid:90) (cid:89) (cid:96)µt d χ (cid:96) + 1 µ ( t ) d ˆ χ (cid:96) + 1 µ ( t ) ( 2 π ) N (cid:89) (cid:96)µt d ξ (cid:96)µ ( t ) d ˆ ξ (cid:96)µ ( t ) ( 2 π ) N exp   (cid:88) (cid:96) , µ (cid:90) ∞ 0 dt (cid:2) j (cid:96)µ ( t ) · χ (cid:96)µ ( t ) + v (cid:96)µ ( t ) · ξ (cid:96)µ ( t ) (cid:3)   × L − 1 (cid:89) (cid:96) = 1 (cid:42) exp (cid:32) − i √ N (cid:88) µ (cid:90) ∞ 0 dt (cid:104) ˆ χ (cid:96) + 1 µ ( t ) (cid:62) W (cid:96) ( 0 ) φ ( h (cid:96)µ ( t ) ) + g (cid:96) + 1 µ ( t ) (cid:62) W (cid:96) ( 0 ) ˆ ξ (cid:96)µ ( t ) (cid:105)(cid:33)(cid:43) W (cid:96) ( 0 ) × (cid:42) exp (cid:32) − i √ D (cid:88) µ (cid:90) ∞ 0 dt ˆ χ 1 µ ( t ) (cid:62) W 0 ( 0 ) x µ (cid:33)(cid:43) W 0 ( 0 ) × (cid:42) exp (cid:32) − i (cid:88) µ (cid:90) ∞ 0 ˆ ξ Lµ ( t ) · w L ( 0 ) (cid:33)(cid:43) w L ( 0 ) × L (cid:89) (cid:96) = 1 exp (cid:32) i (cid:88) µ (cid:90) ∞ 0 dt (cid:104) ˆ χ (cid:96)µ ( t ) · χ (cid:96)µ ( t ) + ˆ ξ (cid:96)µ ( t ) · ξ (cid:96)µ ( t ) (cid:105)(cid:33) ( 37 ) Since W (cid:96) ( 0 ) are all Gaussian random variables , these averages can be performed quite easily yielding 25 (cid:42) exp (cid:32) − i √ D (cid:88) µ (cid:90) ∞ 0 dt ˆ χ 1 µ ( t ) (cid:62) W 0 ( 0 ) x µ (cid:33)(cid:43) W 0 ( 0 ) = exp (cid:32) − 1 2 (cid:90) ∞ 0 (cid:90) ∞ 0 dtds (cid:88) µα ˆ χ 1 µ ( t ) · ˆ χ 1 α ( s ) K xµα (cid:33) (cid:42) exp (cid:32) − i (cid:88) µ (cid:90) ∞ 0 ˆ ξ Lµ ( t ) · w L ( 0 ) (cid:33)(cid:43) w L ( 0 ) = exp (cid:32) − 1 2 (cid:88) µα (cid:90) ∞ 0 (cid:90) ∞ 0 dtds ˆ ξ Lµ ( t ) · ˆ ξ Lα ( s ) (cid:33) (cid:42) exp (cid:32) − i √ N (cid:88) µ (cid:90) ∞ 0 dt (cid:104) ˆ χ (cid:96) + 1 µ ( t ) (cid:62) W (cid:96) ( 0 ) φ ( h (cid:96)µ ( t ) ) + g (cid:96) + 1 µ ( t ) (cid:62) W (cid:96) ( 0 ) ˆ ξ (cid:96)µ ( t ) (cid:105)(cid:33)(cid:43) W (cid:96) ( 0 ) = exp (cid:32) − 1 2 N (cid:88) µα (cid:90) ∞ 0 (cid:90) ∞ 0 dtds (cid:104) ˆ χ (cid:96) + 1 µ ( t ) · ˆ χ (cid:96) + 1 µ ( t ) φ ( h (cid:96)µ ( t ) ) · φ ( h (cid:96)α ( s ) ) + ˆ ξ (cid:96)µ ( t ) · ˆ ξ (cid:96)α ( s ) g (cid:96) + 1 µ ( t ) · g (cid:96) + 1 α ( s ) (cid:105)(cid:33) × exp (cid:32) − 1 N (cid:88) µα (cid:90) ∞ 0 (cid:90) ∞ 0 dtds ˆ χ (cid:96) + 1 µ ( t ) · g (cid:96) + 1 α ( s ) φ ( h (cid:96)µ ( t ) ) · ˆ ξ (cid:96)α ( s ) (cid:33) ( 38 ) D . 4 Order Parameters and Action Deﬁnition We deﬁne the following order parameters which we will show concentrate in the N → ∞ limit Φ (cid:96)µ , α ( t , s ) = 1 N φ ( h (cid:96)µ ( t ) ) · φ ( h (cid:96)α ( s ) ) , G (cid:96)µα ( t , s ) = 1 N g (cid:96)µ ( t ) · g (cid:96)α ( s ) , A (cid:96)µα ( t , s ) = − i N φ ( h (cid:96)µ ( t ) ) · ˆ ξ (cid:96)α ( s ) . ( 39 ) The NTK only depends on { Φ (cid:96) , G (cid:96) } so from these order parameters , we can compute the function evolution . The parameter A (cid:96) arises from the coupling of the ﬁelds across a single layer’s initial weight matrix W (cid:96) ( 0 ) . We can again enforce these deﬁnitions with integral representations of the Dirac - delta function . For each pair of samples µ , α and each pair of times t , s , we multiply by 1 = (cid:90) (cid:90) d Φ (cid:96)µα ( t , s ) d ˆΦ (cid:96)µα ( t , s ) 2 πiN − 1 exp (cid:16) N Φ (cid:96)µα ( t , s ) ˆΦ (cid:96)µα ( t , s ) − ˆΦ (cid:96)µα ( t , s ) φ ( h (cid:96)µ ( t ) ) · φ ( h (cid:96)α ( s ) ) (cid:17) (cid:96) ∈ { 1 , . . . , L } 1 = (cid:90) (cid:90) dG µα ( t , s ) d ˆ G µα ( t , s ) 2 πiN − 1 exp (cid:16) NG (cid:96)µα ( t , s ) ˆ G (cid:96)µα ( t , s ) − ˆ G (cid:96)µα ( t , s ) g (cid:96)µ ( t ) · g (cid:96)α ( s ) (cid:17) , (cid:96) ∈ { 1 , . . . , L } 1 = (cid:90) (cid:90) dA (cid:96) µα ( t , s ) dB (cid:96) µα ( t , s ) 2 πiN − 1 exp (cid:16) − NA (cid:96)µα ( t , s ) B (cid:96)µα ( t , s ) − iB (cid:96)µα ( t , s ) φ ( h (cid:96)µ ( t ) ) · ˆ ξ (cid:96)α ( s ) ) (cid:17) , (cid:96) ∈ { 1 , . . . , L − 1 } ( 40 ) After introducing these order parameters into the deﬁnition of the partition function , we have a factorization of the integrals over each of the N sites in each hidden layer . This gives the following partition function Z = (cid:90) (cid:89) (cid:96) , µα , ts d Φ (cid:96)µα ( t , s ) d ˆΦ (cid:96)µα ( t , s ) 2 πiN − 1 dG µα ( t , s ) d ˆ G µα ( t , s ) 2 πiN − 1 dA (cid:96)µα ( t , s ) dB (cid:96)µα ( t , s ) 2 πiN − 1 exp (cid:16) NS [ { Φ , ˆΦ , G , ˆ G , A , B } ] (cid:17) S [ { Φ , ˆΦ , G , ˆ G , A , B } ] = (cid:88) (cid:96)µα (cid:90) ∞ 0 (cid:90) ∞ 0 dtds (cid:104) Φ (cid:96)µα ( t , s ) ˆΦ (cid:96)µα ( t , s ) + G (cid:96)µα ( t , s ) ˆ G (cid:96)µα ( t , s ) − A (cid:96)µα ( t , s ) B (cid:96)µα ( t , s ) (cid:105) + ln Z [ { Φ , ˆΦ , G , ˆ G , A , B , j , v } ] ( 41 ) 26 We thus see that the action S consists of inner - products between order parameters { Φ , G , A } and their duals { ˆΦ , ˆ G , B } as well as a single site MGF Z [ { Φ , ˆΦ , G , ˆ G , A , B , j , v } ] , which is deﬁned as Z = (cid:90) (cid:89) (cid:96)µt d ˆ χ (cid:96)µ ( t ) dχ (cid:96)µ ( t ) 2 π d ˆ ξ (cid:96)µ ( t ) dξ (cid:96)µ ( t ) 2 π exp  (cid:88) (cid:96)µ (cid:90) ∞ 0 dt (cid:104)(cid:0) j (cid:96)µ ( t ) + i ˆ χ (cid:96)µ ( t ) (cid:1) χ (cid:96)µ ( t ) + (cid:16) v (cid:96)µ ( t ) + i ˆ ξ (cid:96)µ ( t ) (cid:17) ξ (cid:96)µ ( t ) (cid:105) × exp (cid:32) − 1 2 (cid:88) µα (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds ˆ χ 1 µ ( t ) ˆ χ 1 α ( s ) K xµα − 1 2 (cid:88) µα (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds ˆ ξ Lµ ( t ) ˆ ξ Lα ( s ) (cid:33) × exp (cid:32) − 1 2 L − 1 (cid:88) (cid:96) = 1 (cid:88) µα (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds (cid:104) ˆ χ (cid:96) + 1 µ ( t ) ˆ χ (cid:96) + 1 α ( s ) Φ (cid:96)µα ( t , s ) + ˆ ξ (cid:96)µ ( t ) ˆ ξ (cid:96)α ( s ) G (cid:96) + 1 µα ( t , s ) (cid:105)(cid:33) × exp (cid:32) − L (cid:88) (cid:96) = 1 (cid:88) µα (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds (cid:104) φ ( h (cid:96)µ ( t ) ) φ ( h (cid:96)α ( s ) ) ˆΦ (cid:96)µα ( t , s ) + g (cid:96)µ ( t ) g (cid:96)α ( s ) ˆ G (cid:96)µα ( t , s ) (cid:105)(cid:33) × exp (cid:32) − i L (cid:88) (cid:96) = 1 (cid:88) µα (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds (cid:104) φ ( h (cid:96)µ ( t ) ) ˆ ξ (cid:96)α ( s ) B (cid:96)µα ( t , s ) + ˆ χ (cid:96) + 1 µ ( t ) g (cid:96) + 1 α ( s ) A (cid:96)µα ( t , s ) (cid:105)(cid:33) ( 42 ) D . 5 Saddle Point Equations Since the integrand in the moment generating function Z takes the form e NS [ { Φ , ˆΦ , G , ˆ G , A , B } ] , the N → ∞ limit can be obtained from saddle point integration , also known as the method of steepest descent [ 86 ] . This consists in ﬁnding order parameters { Φ , ˆΦ , G , ˆ G , A , B } which render the action S locally stationary . Concretely , this leads to the following saddle point equations . δS δ ˆΦ (cid:96)µα ( t , s ) = Φ (cid:96)µα ( t , s ) + 1 Z δ Z δ ˆΦ (cid:96)µα ( t , s ) = Φ (cid:96)µα ( t , s ) − (cid:10) φ ( h (cid:96)µ ( t ) ) φ ( h (cid:96)α ( s ) ) (cid:11) = 0 δS δ Φ (cid:96)µα ( t , s ) = ˆΦ (cid:96)µα ( t , s ) + 1 Z δ Z δ Φ (cid:96)µα ( t , s ) = ˆΦ (cid:96)µα ( t , s ) − 1 2 (cid:10) ˆ χ (cid:96) + 1 µ ( t ) ˆ χ (cid:96) + 1 α ( s ) (cid:11) = 0 δS δ ˆ G (cid:96)µα ( t , s ) = G (cid:96)µα ( t , s ) + 1 Z δ Z δ ˆ G (cid:96)µα ( t , s ) = G (cid:96)µα ( t , s ) − (cid:10) g (cid:96)µ ( t ) g (cid:96)α ( s ) (cid:11) = 0 δS δG (cid:96)µα ( t , s ) = ˆ G (cid:96)µα ( t , s ) + 1 Z δ Z δG (cid:96)µα ( t , s ) = ˆ G (cid:96)µα ( t , s ) − 1 2 (cid:10) ˆ g (cid:96)µ ( t ) ˆ g (cid:96)α ( s ) (cid:11) = 0 δS δA (cid:96)µα ( t , s ) = − B (cid:96)µα ( t , s ) + 1 Z δ Z δA (cid:96)µα ( t , s ) = − B (cid:96)µα ( t , s ) − i (cid:10) ˆ χ (cid:96) + 1 µ ( t ) g (cid:96) + 1 α ( s ) (cid:11) = 0 δS δB (cid:96)µα ( t , s ) = − A (cid:96)µα ( t , s ) + 1 Z δ Z δB (cid:96)µα ( t , s ) = − A (cid:96)µα ( t , s ) − i (cid:68) φ ( h (cid:96)µ ( t ) ) ˆ ξ (cid:96)α ( s ) (cid:69) = 0 ( 43 ) We use the notation (cid:104)(cid:105) to denote an average over the self - consistent distribution on ﬁelds in - duced by the single - site moment generating function Z at the saddle point . Concretely if Z = (cid:82) dχdξd ˆ χd ˆ ξ exp (cid:16) −H [ χ , ξ , ˆ χ , ˆ ξ ] (cid:17) then the single - site self - consistent average of observable O ( [ χ , ξ , ˆ χ , ˆ ξ ] ) is deﬁned as (cid:68) O ( [ χ , ξ , ˆ χ , ˆ ξ ] ) (cid:69) = 1 Z (cid:90) dχdξd ˆ χd ˆ ξ O ( [ χ , ξ , ˆ χ , ˆ ξ ] ) exp (cid:16) −H [ χ , ξ , ˆ χ , ˆ ξ ] (cid:17) ( 44 ) To calculate the averages of the dual variables such as (cid:10) ˆ χ (cid:96) + 1 ˆ χ (cid:96) + 1 (cid:11) , it will be convenient to work with vector and matrix notation . We let χ (cid:96) = Vec { χ (cid:96)µ ( t ) } µ ∈ [ P ] , t ∈ R + represent the vectorization of the stochastic process over different samples and times and deﬁne the dot product between two of these vectors as a · b = (cid:80) Pµ = 1 (cid:82) ∞ 0 dt a µ ( t ) b µ ( t ) . We also apply this procedure on the kernels so that Φ = Mat { Φ µα ( t , s ) } µα ∈ [ P ] , t , s ∈ R + . Matrix vector products take the form [ Ab ] µ , t = (cid:82) ∞ 0 ds (cid:80) α A µα ( t , s ) b α ( s ) . We can obtain the behavior of (cid:10) ˆ χ (cid:96) + 1 µ ˆ χ (cid:96) + 1 (cid:62) µ (cid:11) in terms of primal ﬁelds { χ , ξ , h , z } by insertion of a dummy source u into the effective partition function . 27 (cid:10) ˆ χ (cid:96) + 1 ˆ χ (cid:96) + 1 (cid:11) = − ∂ 2 ∂ u ∂ u (cid:62) (cid:10) exp (cid:0) i u · ˆ χ (cid:96) + 1 (cid:1)(cid:11) | u = 0 = − 1 Z ∂ 2 ∂ u ∂ u (cid:62) (cid:90) d χ (cid:96) + 1 . . . exp (cid:18) − 1 2 (cid:0) χ (cid:96) + 1 + u − A (cid:96) g (cid:96) + 1 (cid:1) (cid:62) [ Φ (cid:96) ] − 1 (cid:0) χ (cid:96) + 1 + u − A (cid:96) g (cid:96) + 1 (cid:1) − . . . (cid:19) = [ Φ (cid:96) ] − 1 − [ Φ (cid:96) ] − 1 (cid:68)(cid:0) χ (cid:96) + 1 − A (cid:96) g (cid:96) + 1 (cid:1) (cid:0) χ (cid:96) + 1 − A (cid:96) g (cid:96) + 1 (cid:1) (cid:62) (cid:69) (cid:2) Φ (cid:96) (cid:3) − 1 ( 45 ) Similarly , we can obtain the equation for (cid:68) ˆ ξ (cid:96) ˆ ξ (cid:96) (cid:62) (cid:69) by inserting a dummy source r and differentiating near zero source (cid:68) ˆ ξ (cid:96) ˆ ξ (cid:96) (cid:69) = − ∂ 2 ∂ r ∂ r (cid:62) (cid:68) exp (cid:16) i ˆ r · ˆ ξ (cid:96) (cid:17)(cid:69) | r = 0 = [ G (cid:96) + 1 ] − 1 − [ G (cid:96) + 1 ] − 1 (cid:10) ( ξ (cid:96) − B (cid:96) (cid:62) φ (cid:96) ) ( ξ (cid:96) − B (cid:96) (cid:62) φ (cid:96) ) (cid:62) (cid:11) [ G (cid:96) + 1 ] − 1 ( 46 ) As we will demonstrate in the next subsection , these correlators must vanish . Lastly , we can calculate the remaining correlators in terms of primal variables − i (cid:10) ˆ χ (cid:96) + 1 g (cid:96) + 1 , (cid:62) (cid:11) = ∂ ∂ u (cid:10) exp (cid:0) − i ˆ u · ˆ χ (cid:96) + 1 (cid:1) g (cid:96) + 1 (cid:62) (cid:11) = [ Φ (cid:96) ] − 1 (cid:10) ( χ (cid:96) + 1 − A (cid:96) g (cid:96) + 1 ) g (cid:96) + 1 (cid:62) (cid:11) − i (cid:68) φ ( h (cid:96) ) ˆ ξ (cid:96) (cid:62) (cid:69) = ∂ ∂ r (cid:62) (cid:68) φ ( h ) exp (cid:16) − i r · ˆ ξ (cid:96) (cid:17)(cid:69) = (cid:10) φ ( h (cid:96) ) ( ξ (cid:96) − B (cid:96) (cid:62) φ ( h (cid:96) ) ) (cid:11) [ G (cid:96) + 1 ] − 1 ( 47 ) D . 6 Single Site Stochastic Process : Hubbard Trick To get a better sense of this distribution , we can now simplify the quadratic forms appearing in Z using the Hubbard trick [ 87 ] , which merely relates a Gaussian function to its Fourier transform . exp (cid:18) − 1 2 x (cid:62) Ax (cid:19) = (cid:90) R d d u ( 2 π ) d / 2 √ det A exp (cid:18) − 1 2 u (cid:62) A − 1 u − i u · x (cid:19) = (cid:104) exp ( − i u · x ) (cid:105) u ∼N ( 0 , A ) ( 48 ) Applying this to the quadratic forms in the single - site MGF Z , we get exp (cid:32) − 1 2 (cid:88) µα (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds ˆ χ 1 µ ( t ) ˆ χ 1 α ( s ) K xµα (cid:33) = (cid:42) exp (cid:32) − i (cid:88) µ (cid:90) ∞ 0 dt u 1 µ ( t ) ˆ χ (cid:96) + 1 µ ( t ) (cid:33)(cid:43) { u 1 } ∼GP ( 0 , K x ⊗ 11 (cid:62) ) exp (cid:32) − 1 2 (cid:88) µα (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds ˆ χ (cid:96) + 1 µ ( t ) ˆ χ (cid:96) + 1 α ( s ) Φ (cid:96)µα ( t , s ) (cid:33) = (cid:42) exp (cid:32) − i (cid:88) µ (cid:90) ∞ 0 dt u (cid:96) + 1 µ ( t ) ˆ χ (cid:96) + 1 µ ( t ) (cid:33)(cid:43) { u (cid:96) } ∼GP ( 0 , Φ (cid:96) ) exp (cid:32) − 1 2 (cid:88) µα (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds ˆ ξ (cid:96)µ ( t ) ˆ ξ (cid:96)α ( s ) G (cid:96) + 1 µα ( t , s ) (cid:33) = (cid:42) exp (cid:32) − i (cid:88) µ (cid:90) ∞ 0 dt r (cid:96)µ ( t ) ˆ ξ (cid:96)µ ( t ) (cid:33)(cid:43) { r (cid:96) } ∼GP ( 0 , G (cid:96) + 1 ) exp (cid:32) − 1 2 (cid:88) µα (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds ˆ ξ Lµ ( t ) ˆ ξ Lα ( s ) (cid:33) = (cid:42) exp (cid:32) − i (cid:88) µ (cid:90) ∞ 0 dt r Lµ ( t ) ˆ ξ (cid:96)µ ( t ) (cid:33)(cid:43) { r L } ∼GP ( 0 , 11 (cid:62) ) ( 49 ) Next , we integrate over all ˆ χ (cid:96) , ˆ ξ (cid:96) variables which yield Dirac - delta functions (cid:90) (cid:89) µt d ˆ χ (cid:96)µ ( t ) 2 π exp (cid:0) i ˆ χ (cid:96) · (cid:2) χ (cid:96) − u (cid:96) − A (cid:96) − 1 g (cid:96) (cid:3)(cid:1) = δ (cid:0) χ (cid:96) − u (cid:96) − A (cid:96) − 1 g (cid:96) (cid:1) (cid:90) (cid:89) µt d ˆ ξ (cid:96) µ ( t ) 2 π exp (cid:16) i ˆ ξ (cid:96) · (cid:2) ξ (cid:96) − r (cid:96) − B (cid:96) (cid:62) φ ( h (cid:96) ) (cid:3)(cid:17) = δ (cid:0) ξ (cid:96) − r (cid:96) − B (cid:96) (cid:62) φ ( h (cid:96) ) (cid:1) ( 50 ) To remedy the notational asymmetry , we redeﬁne B (cid:96) as its transpose B (cid:96) → B (cid:96) (cid:62) . The presence of these delta - functions in the MGF Z indicate the constraints u (cid:96) = χ (cid:96) − A (cid:96) − 1 g (cid:96) and r (cid:96) = 28 ξ (cid:96) − B (cid:96) φ ( h (cid:96) ) . We can thus return to the ˆΦ and ˆ G saddle point equations and verify that these order parameters vanish ˆ Φ (cid:96) = − 1 2 (cid:10) ˆ χ (cid:96) + 1 ˆ χ (cid:96) + 1 (cid:62) (cid:11) = 1 2 [ Φ (cid:96) ] − 1 (cid:68)(cid:0) χ (cid:96) + 1 − A (cid:96) g (cid:96) + 1 (cid:1) (cid:0) χ (cid:96) + 1 − A (cid:96) g (cid:96) + 1 (cid:1) (cid:62) (cid:69) (cid:2) Φ (cid:96) (cid:3) − 1 − 1 2 [ Φ (cid:96) ] − 1 = 1 2 [ Φ (cid:96) ] − 1 (cid:10) u (cid:96) + 1 u (cid:96) + 1 (cid:62) (cid:11) (cid:2) Φ (cid:96) (cid:3) − 1 − 1 2 [ Φ (cid:96) ] − 1 = 0 , ( 51 ) since (cid:10) u (cid:96) + 1 u (cid:96) + 1 (cid:62) (cid:11) = Φ (cid:96) . Following an identical argument , ˆ G (cid:96) = 0 . After this simpliﬁcation , the single site MGF takes the form Z [ { j (cid:96) , v (cid:96) } ] = (cid:42)(cid:90) (cid:89) (cid:96) d χ (cid:96) d ξ (cid:96) δ (cid:0) χ (cid:96) − u (cid:96) − A (cid:96) − 1 g (cid:96) (cid:1) δ (cid:0) ξ (cid:96) − r (cid:96) − B (cid:96) φ ( h (cid:96) ) (cid:1) exp (cid:0) i j (cid:96) · χ (cid:96) + i v (cid:96) · ξ (cid:96) (cid:1)(cid:43) { u (cid:96) , r (cid:96) } ( 52 ) The interpretation is thus that u (cid:96) , r (cid:96) are sampled independently from their respective Gaussian processes and the ﬁelds χ (cid:96) and ξ (cid:96) are determined in terms of u (cid:96) , r (cid:96) , h (cid:96) , g (cid:96) . This means that we can apply Stein’s Lemma ( integration by parts ) [ 88 ] to simplify the last two saddle point equations A (cid:96) = (cid:10) φ ( h (cid:96) ) r (cid:96) (cid:62) (cid:11) [ G (cid:96) + 1 ] − 1 = (cid:28) ∂φ ( h (cid:96) ) ∂ r (cid:96) (cid:62) (cid:29) , B (cid:96) = (cid:10) g (cid:96) + 1 u (cid:96) + 1 (cid:11) [ Φ (cid:96) ] − 1 = (cid:28) ∂ g (cid:96) + 1 ∂ u (cid:96) + 1 (cid:62) (cid:29) ( 53 ) D . 7 Final DMFT Equations We can now close this stochastic process in terms of preactivations h (cid:96) and pre - gradients z (cid:96) . To match the formulas provided in the main text , we rescale A (cid:96) → A (cid:96) / γ 0 = O γ 0 ( 1 ) and B (cid:96) → B (cid:96) / γ 0 = O γ 0 ( 1 ) , which makes it clear that the non - Gaussian corrections to the h (cid:96)µ ( t ) , z (cid:96)µ ( t ) ﬁelds are O ( γ 0 ) . After this rescaling , we have the following complete DMFT equations . h (cid:96)µ ( t ) = χ (cid:96)µ ( t ) + γ 0 (cid:90) t 0 ds (cid:88) α ∆ α ( s ) Φ (cid:96) − 1 µα ( t , s ) z α ( s ) ˙ φ ( h (cid:96)α ( s ) ) = u (cid:96)µ ( t ) + γ 0 (cid:90) t 0 ds (cid:88) α (cid:2) A (cid:96) − 1 µα ( t , s ) + ∆ α ( s ) Φ (cid:96) − 1 µα ( t , s ) (cid:3) ˙ φ ( h (cid:96)α ( s ) ) z (cid:96)α ( s ) z (cid:96)µ ( t ) = ξ (cid:96)µ ( t ) + γ 0 (cid:90) t 0 ds (cid:88) α ∆ α ( s ) G (cid:96) + 1 µα ( t , s ) φ ( h (cid:96)α ( s ) ) = r (cid:96)µ ( t ) + γ 0 (cid:90) t 0 (cid:88) α (cid:2) B (cid:96)µα ( t , s ) + ∆ α ( s ) G (cid:96) + 1 µα ( t , s ) (cid:3) φ ( h (cid:96)α ( s ) ) Φ (cid:96)µα ( t , s ) = (cid:10) φ ( h (cid:96)µ ( t ) ) φ ( h (cid:96)α ( s ) ) (cid:11) , G (cid:96)µα ( t , s ) = (cid:10) g (cid:96)µ ( t ) g (cid:96)α ( s ) (cid:11) A (cid:96)µα ( t , s ) = γ − 1 0 (cid:42) δφ ( h (cid:96)µ ( t ) ) δr (cid:96)α ( s ) (cid:43) , B (cid:96)µα ( t , s ) = γ − 1 0 (cid:42) δg (cid:96) + 1 µ ( t ) δu (cid:96) + 1 α ( s ) (cid:43) The base cases in the above equations are that A 0 = B L = 0 and Φ 0 µα ( t , s ) = K xµα and G L + 1 µα ( t , s ) = 1 . From the above self - consistent equations , one obtains the NTK dynamics and consequently the output predictions of the network with ∂f µ ∂t = (cid:80) α ∆ α ( t ) (cid:2)(cid:80) (cid:96) G (cid:96) + 1 µα ( t , t ) Φ (cid:96)µα ( t , t ) (cid:3) . D . 8 Varying Network Widths and Initialization Scales In this section , we relax the assumption of network widths being equal while taking all widths to inﬁnity at a ﬁxed ratio . This will allow us to analyze the inﬂuence of bottlenecks on the dynamics . We let N (cid:96) = a (cid:96) N represent the width of layer (cid:96) . Without loss of generality , we can choose that N L = N and proceed by deﬁning order parameters in the usual way Φ (cid:96)µα ( t , s ) = 1 N (cid:96) φ ( h (cid:96)µ ( t ) ) · φ ( h (cid:96)α ( s ) ) , G (cid:96)µα ( t , s ) = 1 N (cid:96) g (cid:96)µ ( t ) · g (cid:96)α ( s ) ( 55 ) 29 Since N L = N , the variable g L = √ N L ∂h L + 1 ∂ h L = w L (cid:12) ˙ φ ( h L ) = O N , γ ( 1 ) as desired . We extend this deﬁnition to each layer as before g (cid:96) = √ N (cid:96) ∂h L + 1 ∂ h (cid:96) which again satisﬁes the recursion g (cid:96)µ ( t ) = z (cid:96)µ ( t ) (cid:12) ˙ φ ( h (cid:96)µ ( t ) ) , z (cid:96)µ ( t ) = 1 √ N (cid:96) + 1 W (cid:96) ( t ) (cid:62) g (cid:96) + 1 µ ( t ) ( 56 ) Now , we need to calculate the dynamics on weights W (cid:96) d dt W (cid:96) = γ 2 (cid:88) µ ∆ µ ∂f µ ∂ W (cid:96) = γ 2 (cid:88) µ ∆ µ ∂f µ ∂ h (cid:96) + 1 µ · ∂ h (cid:96) + 1 µ ∂ W (cid:96) = γ √ N (cid:96) √ N (cid:96) + 1 (cid:88) µ ∆ µ g (cid:96) + 1 µ φ ( h (cid:96)µ ) (cid:62) ( 57 ) Using our deﬁnition of the kernels and the h , z ﬁelds h (cid:96)µ ( t ) = χ (cid:96)µ ( t ) + γ √ N (cid:96) (cid:88) α (cid:90) t 0 ds ∆ α ( s ) g (cid:96)α ( s ) Φ (cid:96) − 1 µα ( t , s ) z (cid:96)µ ( t ) = ξ (cid:96)µ ( t ) + γ √ N (cid:96) (cid:88) α (cid:90) t 0 ds ∆ α ( s ) φ ( h (cid:96)α ( s ) ) G (cid:96) + 1 µα ( t , s ) ( 58 ) We also ﬁnd the usual formula for the NTK K NTKµα = γ 2 (cid:88) (cid:96) Tr (cid:20) ∂f µ ∂ W (cid:96) (cid:21) (cid:62) ∂f α ∂ W (cid:96) = Φ Lµα + L − 1 (cid:88) (cid:96) = 1 G (cid:96) + 1 µα Φ (cid:96)µα + G 1 µα K xµα ( 59 ) Now , as before , we need to consider the distribution of χ , ξ ﬁelds . We assume W (cid:96)ij ( 0 ) ∼ N ( 0 , σ 2 (cid:96) ) . This requires computing integrals like (cid:42) exp (cid:32) i (cid:88) µ (cid:90) ∞ 0 dt (cid:104) ˆ χ (cid:96) + 1 µ ( t ) (cid:62) W (cid:96) ( 0 ) φ ( h (cid:96)µ ( t ) ) / √ N (cid:96) + g (cid:96) + 1 µ ( t ) (cid:62) W (cid:96) ( 0 ) ˆ ξ (cid:96)µ ( t ) / √ N (cid:96) + 1 (cid:105)(cid:33)(cid:43) W (cid:96) ( 0 ) = exp (cid:32) − σ 2 (cid:96) 2 (cid:88) µα (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds (cid:104) ˆ χ (cid:96) + 1 µ ( t ) · ˆ χ (cid:96) + 1 µ ( t ) Φ (cid:96)µα ( t , s ) + ˆ ξ (cid:96)µ ( t ) · ˆ ξ (cid:96)µ ( t ) G (cid:96) + 1 µα ( t , s ) (cid:105)(cid:33) × exp (cid:32) − iσ 2 (cid:96) (cid:114) a (cid:96) a (cid:96) + 1 (cid:88) µα (cid:90) ∞ 0 dt (cid:90) ∞ 0 dsA (cid:96)µα ( t , s ) χ (cid:96) + 1 µ ( t ) · g (cid:96) + 1 α ( s ) (cid:33) ( 60 ) where A (cid:96)µα ( t , s ) = − iN (cid:96) φ ( h (cid:96)µ ( t ) ) · ˆ ξ (cid:96)α ( s ) . The action thus takes the form S = (cid:88) (cid:96) a (cid:96) Tr (cid:104) ˆΦ (cid:96) (cid:62) Φ (cid:96) + G (cid:96) (cid:62) ˆ G (cid:96) − A (cid:96) (cid:62) B (cid:96) (cid:105) + (cid:88) (cid:96) a (cid:96) ln Z (cid:96) ( 61 ) where the zero - source MGF for layer (cid:96) has the form Z (cid:96) = (cid:90) (cid:89) µt dχ (cid:96)µ ( t ) d ˆ χ (cid:96)µ ( t ) 2 π dξ (cid:96)µ ( t ) d ˆ ξ (cid:96) µ ( t ) 2 π exp (cid:16) − φ ( h (cid:96) ) (cid:62) ˆΦ (cid:96) φ ( h (cid:96) ) − g (cid:96) (cid:62) ˆ G (cid:96) g (cid:96) + i χ (cid:96) · ˆ χ (cid:96) + i ξ (cid:96) · ˆ ξ (cid:96) (cid:17) exp (cid:18) − σ 2 (cid:96) − 1 2 ˆ χ (cid:96) Φ (cid:96) − 1 ˆ χ (cid:96) − σ 2 (cid:96) 2 ˆ ξ (cid:96) G (cid:96) + 1 ˆ ξ (cid:96) (cid:19) exp (cid:18) − iσ 2 (cid:96) − 1 (cid:114) a (cid:96) − 1 a (cid:96) ˆ χ (cid:96) A (cid:96) − 1 g (cid:96) − iφ ( h (cid:96) ) (cid:62) B (cid:96) ˆ ξ (cid:96) (cid:19) ( 62 ) The saddle point equations give Φ (cid:96) = (cid:10) φ ( h (cid:96) ) φ ( h (cid:96) ) (cid:62) (cid:11) , G (cid:96) = (cid:10) g (cid:96) g (cid:96) (cid:62) (cid:11) A (cid:96) = − i (cid:68) φ ( h (cid:96) ) ˆ ξ (cid:96) (cid:62) (cid:69) = (cid:28) ∂φ ( h (cid:96) ) ∂ r (cid:96) (cid:62) (cid:29) a (cid:96) B (cid:96) = − ia (cid:96) + 1 σ 2 (cid:96) (cid:114) a (cid:96) a (cid:96) + 1 (cid:10) ˆ χ (cid:96) + 1 g (cid:96) + 1 , (cid:62) (cid:11) = ⇒ B (cid:96) = σ 2 (cid:96) (cid:114) a (cid:96) + 1 a (cid:96) (cid:28) ∂ g (cid:96) + 1 (cid:62) ∂ u (cid:96) + 1 (cid:29) ( 63 ) 30 where u (cid:96) ∼ GP ( 0 , σ 2 (cid:96) − 1 Φ (cid:96) − 1 ) , r (cid:96) ∼ GP ( 0 , σ 2 (cid:96) G (cid:96) + 1 ) . We redeﬁne B (cid:96) → 1 σ 2 (cid:96) (cid:113) a (cid:96) a (cid:96) + 1 B (cid:96) . To take the N → ∞ limit of the ﬁeld dynamics , again use γ 0 = γ / √ N = O N ( 1 ) . The ﬁeld equations take the form h (cid:96)µ ( t ) = u (cid:96)µ ( t ) + (cid:90) ∞ 0 P (cid:88) α = 1 (cid:20) σ 2 (cid:96) − 1 (cid:114) a (cid:96) − 1 a (cid:96) A (cid:96) − 1 µα ( t , s ) + γ 0 √ a (cid:96) Θ ( t − s ) Φ (cid:96) − 1 µα ( t , s ) (cid:21) ˙ φ ( h (cid:96)α ( s ) ) z (cid:96)α ( s ) z (cid:96)µ ( t ) = r (cid:96)µ ( t ) + (cid:90) ∞ 0 P (cid:88) α = 1 (cid:20) σ 2 (cid:96) (cid:114) a (cid:96) + 1 a (cid:96) B (cid:96)µα ( t , s ) + γ 0 √ a (cid:96) Θ ( t − s ) G (cid:96) + 1 µα ( t , s ) (cid:21) φ ( h (cid:96)α ( s ) ) ( 64 ) We thus ﬁnd that the evolution of the scalar ﬁelds in a given layer is set by the parameter γ 0 / √ a (cid:96) , indicating that relatively wider layers evolve less and contribute less of a change to the overall NTK . This deﬁnition for A (cid:96) , B (cid:96) is non - ideal to extract intuition about bottlenecks since A (cid:96) − 1 ∼ O (cid:16) γ 0 √ a (cid:96) − 1 (cid:17) and B (cid:96) ∼ O (cid:16) γ 0 √ a (cid:96) + 1 (cid:17) . To remedy this , we redeﬁne ˜ A (cid:96) = √ a (cid:96) γ 0 A (cid:96) , ˜ B (cid:96) = √ a (cid:96) + 1 γ 0 B (cid:96) . With this choice , we have h (cid:96)µ ( t ) = u (cid:96)µ ( t ) + γ 0 √ a (cid:96) (cid:90) ∞ 0 P (cid:88) α = 1 (cid:104) σ 2 (cid:96) − 1 ˜ A (cid:96) − 1 µα ( t , s ) + Θ ( t − s ) Φ (cid:96) − 1 µα ( t , s ) (cid:105) ˙ φ ( h (cid:96)α ( s ) ) z (cid:96)α ( s ) z (cid:96)µ ( t ) = r (cid:96)µ ( t ) + γ 0 √ a (cid:96) (cid:90) ∞ 0 P (cid:88) α = 1 (cid:104) σ 2 (cid:96) ˜ B (cid:96)µα ( t , s ) + Θ ( t − s ) G (cid:96) + 1 µα ( t , s ) (cid:105) φ ( h (cid:96)α ( s ) ) ( 65 ) where ˜ A (cid:96) − 1 , ˜ B (cid:96) do not have a leading order scaling with a (cid:96) − 1 or a (cid:96) + 1 respectively . Under this change of variables , it is now apparent that a very wide layer (cid:96) , where γ 0 √ a (cid:96) (cid:28) 1 is small , the ﬁelds h (cid:96) , z (cid:96) become well approximated by the Gaussian processes u (cid:96) , r (cid:96) , albeit with evolving covariances Φ (cid:96) − 1 , G (cid:96) + 1 respectively . In a realistic CNN architecture where the number of channels increases across layers , this result would predict that more feature learning and deviations from Gaussianity to occur in the early layers and the later layers to be well approximated as Gaussian ﬁelds u (cid:96) , r (cid:96) with temporally evolving covariances for (cid:96) ∼ L . We leave evaluation of this prediction to future work . E Two Layer Networks In a two layer network , there are no A or B order parameters , so the ﬁelds χ 1 and ξ 1 are always independent . Further , χ 1 and ξ 1 are both constant throughout training dynamics . Thus we can obtain differential rather than integral equations for the stochastic ﬁelds h 1 , z 1 which are ∂ ∂th 1 µ ( t ) = γ 0 P (cid:88) α = 1 ∆ α ( t ) K xµα ˙ φ ( h 1 α ( t ) ) z 1 ( t ) , ∂ ∂tz 1 ( t ) = γ 0 P (cid:88) α = 1 ∆ α ( t ) φ ( h 1 α ( t ) ) Φ 1 µα ( t ) = (cid:10) φ ( h 1 µ ( t ) ) φ ( h 1 α ( t ) ) (cid:11) , G 1 µα ( t ) = (cid:68) z ( t ) 2 ˙ φ ( h 1 µ ( t ) ) ˙ φ ( h 1 α ( t ) ) (cid:69) ∂ ∂t ∆ µ ( t ) = − P (cid:88) α = 1 (cid:2) G 1 µα ( t ) K xµα + Φ 1 µα ( t ) (cid:3) ∆ α ( t ) ( 66 ) where the average is taken over the random initial conditions h 1 ( 0 ) ∼ N ( 0 , K x ) and z 1 ( 0 ) ∼ N ( 0 , 11 (cid:62) ) . An example of the two layer theory for a ReLU network can be found in Appendix Figure 6 . In this two layer setting , a drift PDE can be obtained for the joint density of preactivations 31 and feedback ﬁelds p ( h , z ; t ) ∂ ∂tp ( h , z , t ) = − p ( h , z , t ) z ( t ) (cid:88) µ ∆ µ ( t ) K xµµ ¨ φ ( h µ ( t ) ) − γ 0 (cid:88) µα K xµα ∆ α ˙ φ ( h α ( t ) ) z ( t ) ∂p ( h , z , t ) ∂h µ − γ 0 (cid:88) µα ∆ α φ ( h α ) ∂p ( h , z , t ) ∂z µ ∂ ∂t ∆ µ ( t ) = − P (cid:88) α = 1 (cid:2) G 1 µα ( t ) K xµα + Φ 1 µα ( t ) (cid:3) ∆ α ( t ) Φ 1 µα ( t ) = (cid:10) φ ( h 1 µ ( t ) ) φ ( h 1 α ( t ) ) (cid:11) , G 1 µα ( t ) = (cid:68) z 1 ( t ) 2 ˙ φ ( h 1 µ ( t ) ) ˙ φ ( h 1 α ( t ) ) (cid:69) , ( 67 ) which is a zero - diffusion feature space version of the PDE derived in the original two layer mean ﬁeld limit of neural networks [ 22 , 42 , 43 ] . F Deep Linear Networks In the deep linear case , the g (cid:96) µ ( t ) ﬁelds are independent of sample index µ . We introduce the kernel H (cid:96)µα ( t , s ) = (cid:10) h (cid:96)µ ( t ) h (cid:96)α ( s ) (cid:11) . The ﬁeld equations are h (cid:96)µ ( t ) = u (cid:96)µ ( t ) + γ 0 (cid:90) ∞ 0 P (cid:88) α = 1 (cid:2) A (cid:96) − 1 µα ( t , s ) + Θ ( t − s ) H (cid:96) − 1 µα ( t , s ) (cid:3) ∆ α ( s ) g (cid:96) ( s ) g (cid:96) ( t ) = r (cid:96) ( t ) + γ 0 (cid:90) ∞ 0 P (cid:88) α = 1 [ B (cid:96)α ( t , s ) + γ 0 Θ ( t − s ) G (cid:96) + 1 ( t , s ) ] ∆ α ( s ) h (cid:96)α ( s ) ( 68 ) Or in vector notation h (cid:96) = u (cid:96) + γ 0 C (cid:96) g (cid:96) and g (cid:96) = r (cid:96) + γ 0 D (cid:96) h (cid:96) where C (cid:96)µ ( t , s ) = P (cid:88) α = 1 [ A (cid:96) − 1 µα ( t , s ) + Θ ( t − s ) H (cid:96) − 1 µα ( t , s ) ] ∆ α ( s ) , D (cid:96)µ ( t , s ) = [ B (cid:96)µ ( t , s ) + Θ ( t − s ) G (cid:96) + 1 ( t , s ) ] ∆ µ ( s ) ( 69 ) Using the formulas which deﬁne the ﬁelds , we have h (cid:96) = u (cid:96) + γ 0 C (cid:96) r (cid:96) + γ 20 C (cid:96) D (cid:96) h (cid:96) = ⇒ h (cid:96) = ( I − γ 20 C (cid:96) D (cid:96) ) − 1 [ u (cid:96) + γ 0 C (cid:96) r (cid:96) ] g (cid:96) = r (cid:96) + γ 0 D (cid:96) u (cid:96) + γ 20 D (cid:96) C (cid:96) g (cid:96) = ⇒ g (cid:96) = ( I − γ 20 D (cid:96) C (cid:96) ) − 1 [ r (cid:96) + γ 0 D (cid:96) u (cid:96) ] ( 70 ) The saddle point equations can thus be written as H (cid:96) = (cid:10) h (cid:96) h (cid:96) (cid:62) (cid:11) = ( I − γ 20 C (cid:96) D (cid:96) ) − 1 [ H (cid:96) − 1 + γ 20 C (cid:96) G (cid:96) + 1 C (cid:96) (cid:62) ] (cid:2) ( I − γ 20 C (cid:96) D (cid:96) ) (cid:62) (cid:3) − 1 G (cid:96) = (cid:10) g (cid:96) g (cid:96) (cid:62) (cid:11) = (cid:0) I − γ 20 D (cid:96) C (cid:96) (cid:1) − 1 (cid:2) G (cid:96) + 1 + γ 20 D (cid:96) H (cid:96) − 1 D (cid:96) (cid:62) (cid:3) (cid:104)(cid:0) I − γ 20 D (cid:96) C (cid:96) (cid:1) (cid:62) (cid:105) − 1 A (cid:96) = ( I − γ 20 C (cid:96) D (cid:96) ) − 1 C (cid:96) , B (cid:96) − 1 = ( I − γ 20 D (cid:96) C (cid:96) ) − 1 D (cid:96) ( 71 ) We solve these equations by repeatedly updating H (cid:96) , G (cid:96) , using Equation ( 71 ) and the current estimate of C (cid:96) , D (cid:96) . We then use the new H (cid:96) , G (cid:96) to recompute K NTK and ∆ ( t ) , calculating C (cid:96) , D (cid:96) and then recomputing H (cid:96) , G (cid:96) . This procedure usually converges in ∼ 5 − 10 steps . F . 1 Two Layer Linear Network As we saw in Appendix E , the ﬁeld dynamics simplify considerably in the two layer case , allowing description of all ﬁelds in terms of differential equations . In a two layer linear network , we let h ( t ) ∈ R P represent the hidden activation ﬁeld and g ( t ) ∈ R represent the gradient ∂ ∂t h ( t ) = γ 0 g ( t ) K x ∆ ( t ) , ∂ ∂tg ( t ) = γ 0 ∆ ( t ) · h ( t ) ( 72 ) 32 The kernels H ( t ) = (cid:10) h ( t ) h ( t ) (cid:62) (cid:11) and G ( t ) = (cid:10) g ( t ) 2 (cid:11) thus evolve as ∂ ∂t H ( t ) = γ 0 K x ∆ (cid:10) g ( t ) h ( t ) (cid:62) (cid:11) + γ 0 (cid:104) g ( t ) h ( t ) (cid:105) ∆ (cid:62) K x ∂ ∂tG ( t ) = 2 γ 0 (cid:104) g ( t ) h ( t ) (cid:105) · ∆ ( t ) ( 73 ) It is easy to verify that the network predictions on the P training points are f ( t ) = y − ∆ ( t ) = 1 γ 0 (cid:104) g ( t ) h ( t ) (cid:105) ∈ R P . Thus the dynamics of H ( t ) , G ( t ) and ∆ ( t ) close ∂ ∂t H ( t ) = γ 20 K x ∆ ( y − ∆ ) (cid:62) + γ 20 ( y − ∆ ) ∆ (cid:62) K x ∂ ∂tG ( t ) = 2 γ 20 ( y − ∆ ) · ∆ ( t ) ∂ ∂t ∆ ( t ) = − [ H ( t ) + G ( t ) K x ] ∆ ( t ) ( 74 ) where the initial conditions are H ( 0 ) = I , G ( 0 ) = 1 and ∆ ( 0 ) = y . These equations hold for any choice of data K x , y . F . 1 . 1 Whitened Data in Two Layer Linear For input data which is whitened where K x = I , then the dynamics can be simpliﬁed even further , recovering the sigmoidal curves very similar to those obtained under a special initialization [ 68 , 69 , 71 , 73 ] . In this case we note that the error signal always evolves in the y direction , ∆ ( t ) = ∆ ( t ) y | y | , and that H only evolves in a rank one direction yy (cid:62) direction as well . Let 1 | y | 2 y (cid:62) H ( t ) y = H y ( t ) . Let y = | y | represent the norm of the target vector , then the relevant scalar dynamics are ∂ ∂tH y ( t ) = 2 γ 20 ∆ ( t ) ( y − ∆ ( t ) ) , ∂ ∂tG ( t ) = 2 γ 20 ∆ ( t ) ( y − ∆ ( t ) ) ∂ ∂t ∆ ( t ) = − [ H y ( t ) + G ( t ) ] ∆ ( t ) ( 75 ) Now note that , at initialization H y ( 0 ) = G ( 0 ) = 1 and that ∂∂t H y ( t ) = ∂∂t G ( t ) . Thus , we have an automatic balancing condition H y ( t ) = G ( t ) for all t ∈ R + and the dynamics reduce to two variables ∂ ∂tH y ( t ) = 2 γ 20 ∆ ( t ) ( y − ∆ ( t ) ) , ∂ ∂t ∆ ( t ) = − 2 H y ( t ) ∆ ( t ) ( 76 ) We note that this system obeys a conservation law which constrains ( H y , y − ∆ ) to a hyperbola 1 2 ∂ ∂t (cid:2) H 2 y − γ 20 ( y − ∆ ( t ) ) 2 (cid:3) = 2 γ 20 H y ∆ ( y − ∆ ) − 2 γ 20 H y ∆ ( y − ∆ ) = 0 ( 77 ) This conservation law implies that H y ( 0 ) 2 = 1 = lim t →∞ H y ( t ) 2 − γ 20 y 2 or that the ﬁnal kernel has the form lim t →∞ H ( t ) = 1 y 2 [ (cid:112) 1 + γ 2 0 y 2 − 1 ] yy (cid:62) + I . The result that the ﬁnal kernel becomes a rank one spike in the direction of the target function was also obtained in ﬁnite width networks in the limit of small initialization [ 73 ] and also from a normative toy model of feature learning [ 81 ] . We can use the conservation law above 1 = H y ( t ) 2 − γ 20 ( ∆ ( t ) − y ) 2 to simplify the dynamics to a one dimensional system ∂ ∂t ∆ ( t ) = − 2 (cid:113) 1 + γ 20 ( ∆ ( t ) − y ) 2 ∆ ( t ) = ⇒ ∂ ∂tf = 2 (cid:113) 1 + γ 20 f 2 ( y − f ) ( 78 ) where f = y − ∆ . We see that increasing γ 0 provides strict acceleration in the learning dynamics , illustrating the training beneﬁts of feature evolution . Since this system is separable , we can solve for the time it takes for the network output norm to reach output level f 2 t = (cid:90) f 0 ds ( y − s ) (cid:112) 1 + γ 20 s 2 = 1 (cid:112) 1 + γ 20 y 2 tanh − 1 (cid:32) 1 + γ 20 yf (cid:112) 1 + γ 20 y 2 (cid:112) 1 + γ 20 f 2 (cid:33) − 1 (cid:112) 1 + γ 20 y 2 tanh − 1 (cid:32) 1 (cid:112) 1 + γ 20 y 2 (cid:33) ( 79 ) 33 The NTK limit can be obtained by taking γ 0 → 0 which gives ∂ ∂t ∆ ( t ) ∼ − 2∆ ( t ) = ⇒ ∆ ( t ) ∼ e − 2 t ( 80 ) which recovers the usual convergence rate of a linear model . The right hand side of Equation ( 79 ) has a perturbation series in γ 20 which converges in the disk γ 0 < 1 y . The other limit of interest is the γ 0 → ∞ limit where d dt ∆ ( t ) ∼ − 2 γ 0 ( y − ∆ ( t ) ) ∆ ( t ) ( 81 ) which recovers the logistic growth observed in the initialization scheme of prior works [ 68 , 69 ] . The timescale τ required to learn is only τ ∼ 1 γ 0 (cid:28) 1 , which is much smaller than the O γ 0 ( 1 ) time to learn predicted from the small γ 0 expansion . We note that the above leading order asymptotic behavior at large γ 0 considers the DMFT initial condition ∆ ( 0 ) = y as an unstable ﬁxed point . For realistic learning curves , one would need to stipulate some alternative initial condition such as ∆ = y − (cid:15) for some small (cid:15) > 0 in order to have nontrivial leading order dynamics . F . 2 Deep Linear Whitened Data In this section , we examine the role of depth when linear networks are trained on whitened data . As in the two layer case , all hidden kernels H (cid:96) ( t , s ) need only be tracked in the one dimensional task relevant subspace along the vector y . We let ∆ ( t ) = 1 y y · ∆ ( t ) and let h y ( t ) = 1 y h (cid:96) ( t ) · y . We have h (cid:96)y ( t ) = u (cid:96)y ( t ) + γ 0 (cid:90) ∞ 0 ds C (cid:96) ( t , s ) g (cid:96) ( s ) , C (cid:96) ( t , s ) = A (cid:96) − 1 y ( t , s ) + Θ ( t − s ) H (cid:96) − 1 y ( t , s ) ∆ ( s ) g (cid:96) ( t ) = r (cid:96) ( t ) + γ 0 (cid:90) ∞ 0 ds D (cid:96) ( t , s ) h (cid:96)y ( s ) , D (cid:96) ( t , s ) = B (cid:96) − 1 y ( t , s ) + Θ ( t − s ) G (cid:96) + 1 ( t , s ) ∆ ( s ) ( 82 ) Lastly we have the simple evolution equation for the scalar error ∆ ( t ) ∂ ∆ ( t ) ∂t = − L (cid:88) (cid:96) = 0 G (cid:96) + 1 ( t , t ) H (cid:96)y ( t , t ) ∆ ( t ) = ⇒ ∆ ( t ) = exp (cid:32) − (cid:90) t 0 ds L (cid:88) (cid:96) = 0 G (cid:96) + 1 ( s , s ) H (cid:96)y ( s , s ) (cid:33) y ( 83 ) Vectorizing we ﬁnd the following equations for the time × time matrix order parameters h (cid:96) = u (cid:96) + γ 0 C (cid:96) g (cid:96) , g (cid:96) = r (cid:96) + γ 0 D (cid:96) h (cid:96) , we can solve for the response functions A (cid:96) = (cid:0) I − γ 20 C (cid:96) D (cid:96) (cid:1) − 1 C (cid:96) and B (cid:96) = (cid:0) I − γ 20 D (cid:96) C (cid:96) (cid:1) − 1 D (cid:96) . This formulation has the advantage that it no longer has any sample - size dependence : arbitrary sample sizes can be considered with no computational cost . G Convolutional Networks with Inﬁnite Channels The DMFT described in this work can be extended to CNNs with inﬁnitely many channels , much in the same way that inﬁnite CNNs have a well deﬁned kernel limit [ 89 , 90 ] . We let W (cid:96)ij , a represent the value of the ﬁlter at spatial displacement a from the center of the ﬁlter , which maps relates activity at channel j of layer (cid:96) to channel i of layer (cid:96) + 1 . The ﬁelds h (cid:96)µ , i , a are deﬁned recursively as h (cid:96) + 1 µ , i , a = 1 √ N N (cid:88) j = 1 (cid:88) b ∈S (cid:96) W (cid:96)ij , b φ ( h (cid:96)µ , j , a + b ) , i ∈ { 1 , . . . , N } ( 84 ) where S (cid:96) is the spatial receptive ﬁeld at layer (cid:96) . For example , a ( 2 k + 1 ) × ( 2 k + 1 ) convolution will have S (cid:96) = { ( i , j ) ∈ Z 2 : − k ≤ i ≤ k , − k ≤ j ≤ k } . The output function is obtained from the last layer is deﬁned as f µ = 1 γ 0 N (cid:80) Ni = 1 w Li , a φ ( h Lµ , i , a ) . The gradient ﬁelds have the same deﬁnition as before g (cid:96) µ , a = γ 0 N ∂f µ ∂ h (cid:96)µ , a , which as before enjoy the following recursion from the chain rule g (cid:96)µ , a = γ 0 N (cid:88) b ∂f µ ∂ h (cid:96) + 1 µ , b · ∂ h (cid:96) + 1 µ , b ∂ h (cid:96)µ , a = ˙ φ ( h (cid:96)µ , a ) (cid:12)   1 √ N N (cid:88) j = 1 (cid:88) b ∈S (cid:96) W (cid:96) (cid:62) b g (cid:96) + 1 µ , a − b   ( 85 ) 34 The dynamics of each set of ﬁlters { W (cid:96) b } can therefore be written in terms of the features h (cid:96) a , g (cid:96) a d dt W (cid:96) b = γ 0 √ N (cid:88) µ , a ∆ µ g (cid:96) + 1 µ , a φ ( h (cid:96)µ , a + b ) (cid:62) . ( 86 ) The feature space description of the forward and backward pass relations is h (cid:96) + 1 µ , a ( t ) = χ (cid:96) + 1 µ , a ( t ) + γ 0 (cid:90) t 0 ds (cid:88) α b , c ∆ α ( s ) Φ (cid:96)µα , a + b , c + b ( t , s ) g (cid:96) + 1 α , c ( s ) z (cid:96)µ , a ( t ) = ξ (cid:96)µ a ( t ) + γ 0 (cid:90) t 0 ds (cid:88) α b , c ∆ α ( s ) G (cid:96) + 1 µα , a − b , c − b ( t , s ) φ ( h (cid:96)α , c ) ( 87 ) where χ (cid:96) + 1 µ , a ( t ) = 1 √ N W (cid:96) ( 0 ) φ ( h (cid:96)µ a ( t ) ) . The order parameters for this network architecture are Φ (cid:96)µα , ab ( t , s ) = 1 N φ ( h (cid:96)µ a ( t ) ) · φ ( h (cid:96)α b ( s ) ) , G (cid:96)µα , ab ( t , s ) = 1 N g (cid:96)µ a ( t ) · g (cid:96)α b ( s ) ( 88 ) These two order parameters per layer collectively deﬁne the neural tangent kernel . Following the computation in D , we obtain the following ﬁeld theory in the N → ∞ limit : { u (cid:96)µ a ( t ) } ∼ GP ( 0 , Φ (cid:96) − 1 ) , { r (cid:96)µ a ( t ) } ∼ GP ( 0 , G (cid:96) + 1 ) h (cid:96)µ a ( t ) = u (cid:96)µ a ( t ) + γ 0 (cid:90) t 0 ds (cid:88) α , b A (cid:96) − 1 µα , ab ( t , s ) ˙ φ ( h (cid:96)α b ( s ) ) z (cid:96)α b ( s ) + γ 0 (cid:90) t 0 ds (cid:88) α b , c ∆ α ( s ) Φ (cid:96) − 1 µα , a + b , c + b ˙ φ ( h (cid:96)α c ( s ) ) z (cid:96)α c ( s ) z (cid:96)µ a ( t ) = r (cid:96)µ a ( t ) + γ 0 (cid:90) ∞ 0 ds (cid:88) α , b B (cid:96)µα , ab ( t , s ) φ ( h (cid:96)α b ( s ) ) + γ 0 (cid:90) t 0 ds (cid:88) α b , c ∆ α ( s ) G (cid:96) + 1 µα , a − b , c − b φ ( h (cid:96)α c ( s ) ) Φ (cid:96)µα , ab ( t , s ) = (cid:10) φ ( h (cid:96)µ a ( t ) ) φ ( h (cid:96)α b ( s ) ) (cid:11) , G (cid:96)µα , ab ( t , s ) = (cid:10) g (cid:96)µ a ( t ) g (cid:96)α b ( s ) (cid:11) A (cid:96)µα , ab ( t , s ) = 1 γ 0 (cid:42) δφ ( h (cid:96)µ a ( t ) ) δr (cid:96)α b ( s ) (cid:43) , B (cid:96)µα , ab ( t , s ) = 1 γ 0 (cid:42) δg (cid:96) + 1 µ a ( t ) δu (cid:96) + 1 α b ( s ) (cid:43) ( 89 ) We see that this ﬁeld theory essentially multiples the number of sample indices by the number of spatial indices P → P | S | . Thus the time complexity of evaluation of this theory scales very poorly as O ( P 3 | S | 3 T 3 ) , rendering DMFT solutions very computationally intensive . H Trainable Bias Parameter If we include a bias b (cid:96) ( t ) ∈ R N in our trainable model , so that h (cid:96) + 1 µ ( t ) = 1 √ N W (cid:96) ( t ) φ ( h (cid:96)µ ( t ) ) + b (cid:96) ( t ) ( 90 ) then the dynamics on b (cid:96) ( t ) induced by gradient ﬂow is d dt b (cid:96) ( t ) = γ 2 (cid:88) α ∆ α ( t ) ∂f α ∂b (cid:96) = γ √ N (cid:88) α ∆ α ( t ) g (cid:96) + 1 α ( t ) = γ 0 (cid:88) α ∆ α ( t ) g (cid:96)α ( t ) ( 91 ) Assuming that b (cid:96)i ( 0 ) ∼ N ( 0 , 1 ) , the dynamics of the DMFT becomes { u (cid:96) } ∼ GP ( 0 , Φ (cid:96) − 1 + 11 (cid:62) ) , { r (cid:96) } ∼ GP ( 0 , G (cid:96) + 1 ) h (cid:96)µ ( t ) = u (cid:96)µ ( t ) + γ 0 (cid:90) ∞ 0 ds (cid:88) α [ A (cid:96) − 1 µα ( t , s ) + Θ ( t − s ) ∆ α ( s ) Φ (cid:96) − 1 µα ( t , s ) ] g (cid:96)α ( s ) + γ 0 (cid:90) t 0 ds (cid:88) α ∆ α ( s ) g (cid:96)α ( s ) z (cid:96)µ ( t ) = r (cid:96)µ ( t ) + γ 0 (cid:90) ∞ 0 ds (cid:88) α [ B (cid:96)µα ( t , s ) + Θ ( t − s ) ∆ α ( s ) G (cid:96) + 1 µα ( t , s ) ] φ ( h (cid:96)α ( s ) ) ( 92 ) 35 I Multiple Output Channels We now consider network outputs on C = O N ( 1 ) classes . The prediction for a data point µ ∈ [ P ] at time t ∈ R + is f µ ( t ) ∈ R C . As before , we deﬁne the error signal as ∆ µ = − ∂∂ f µ (cid:96) ( f µ , y µ ) ∈ R C . For any pair of data points µ , α the NTK is a C × C matrix K NTKµα ∈ R C × C with entries K NTKµα , cc (cid:48) = ∂f c ( x µ ) ∂ θ · ∂f c (cid:48) ( x α ) ∂ θ . From these matrices , we can compute the evolution of the predictions in the network . d dt f µ = P (cid:88) α = 1 K NTKµα ∆ α ( 93 ) In this case , we have matrices for the backprop features g (cid:96) = γ √ N ∂ f (cid:62) ∂ h (cid:96) ∈ R N × C . These satisfy the usual recursion g (cid:96) = γ √ N ∂ f (cid:62) ∂ h (cid:96) = γ √ N (cid:18) ∂ h (cid:96) + 1 ∂ h (cid:96) (cid:19) (cid:62) ∂ f (cid:62) ∂ h (cid:96) + 1 = (cid:104) ˙ φ ( h (cid:96) ) 1 (cid:62) (cid:105) (cid:12) (cid:20) 1 √ N W (cid:96) (cid:62) g (cid:96) + 1 (cid:21) ( 94 ) We can now compute the NTK for samples µ , α K NTKµα = (cid:88) (cid:96) ∂ f ( x µ ) ∂ W (cid:96) · ∂ f ( x α ) ∂ W (cid:96) = Φ Lµα I + L − 1 (cid:88) (cid:96) = 1 G (cid:96) + 1 µα Φ (cid:96)µα + G 1 µα K xµα ( 95 ) where G (cid:96)µα = 1 N g (cid:96) (cid:62) µ g (cid:96)α ∈ R C × C and Φ (cid:96)µα = 1 N φ ( h (cid:96)µ ) · φ ( h (cid:96)α ) ∈ R . Next we introduce kernels A (cid:96)µα ( t , s ) ∈ R C and B (cid:96)µα ( t , s ) ∈ R C which are deﬁned in the usual way . The corresponding ﬁeld theory has the form h (cid:96)µ ( t ) = χ (cid:96)µ ( t ) + γ 0 (cid:90) ∞ 0 ds P (cid:88) α = 1 (cid:2) A (cid:96) − 1 µα ( t , s ) + Θ ( t − s ) ∆ α ( s ) Φ (cid:96) − 1 µα ( t , s ) (cid:3) · g (cid:96)α ( s ) ∈ R z (cid:96)µ ( t ) = ξ (cid:96)µ ( t ) + γ 0 (cid:90) ∞ 0 ds P (cid:88) α = 1 (cid:2) B (cid:96)µα ( t , s ) + Θ ( t − s ) G (cid:96) + 1 µα ∆ α ( s ) (cid:3) φ ( h (cid:96)µ ( t ) ) ∈ R C g (cid:96)µ ( t ) = ˙ φ ( h (cid:96)µ ( t ) ) z (cid:96)µ ( t ) ∈ R C ( 96 ) From these ﬁelds , the saddle point equations deﬁne the kernels as Φ (cid:96)µα ( t , s ) = (cid:10) φ ( h (cid:96)µ ( t ) ) φ ( h (cid:96)α ( s ) ) (cid:11) ∈ R , G (cid:96)µα ( t , s ) = (cid:10) g (cid:96)µ ( t ) g (cid:96)α ( s ) (cid:62) (cid:11) ∈ R C × C A (cid:96)µα ( t , s ) = 1 γ 0 (cid:42) δφ ( h (cid:96) µ ( t ) ) δ r (cid:96)α ( s ) (cid:43) ∈ R C , B (cid:96)µα ( t , s ) = 1 γ 0 (cid:42) δ g (cid:96) µ ( t ) δu (cid:96)α ( s ) (cid:43) ∈ R C . ( 97 ) This allows studying the multi - class structure of learned representations . J Weight Decay in Deep Homogenous Networks If we train with weight decay , ddt θ = − γ 2 ∇ θ L − λ θ , in a κ - degree homogenous network ( f ( c θ ) = c κ f ( θ ) ) , then the prediction dynamics satisfy d dtf ( x , t ) = (cid:88) α ∆ α ( t ) K NTK µα ( x , x α , t ) − λκf ( x , t ) , This holds by the following identity ∂∂c f ( c θ ) = ∂∂c c κ f ( θ ) , which when evaluated at c = 1 gives ∂∂ θ f ( θ ) · θ = κf ( θ ) . This identity was utilized in a prior work which studied L2 regularization in the lazy regime [ 74 ] . For a L - hidden layer ReLU network φ ( h ) = max ( 0 , h ) , the degree is κ = L + 1 , 36 while rectiﬁed power law nonlinearities φ ( h ) = max ( 0 , h ) q give degrees κ = q L + 1 − 1 q − 1 . We note that the ﬁxed point of the function dynamics above gives a representer theorem with the ﬁnal NTK f ( x ) = k ( x ) (cid:62) [ K + λκ I ] − 1 y ( 98 ) where [ k ( x ) ] µ = lim t →∞ K ( x , x µ , t ) and K µα = lim t →∞ K ( x µ , x α , t ) . The prior work of Lewkowycz et al [ 74 ] considered NTK parameterization γ 0 = 0 . In this limit , the kernel ( and consequently output function ) decay to zero at large time , but if γ 0 > 0 , then the network converges to a nontrivial ﬁxed point as t → ∞ . In the DMFT limit we can determine the ﬁnal kernel by solving the following ﬁeld dynamics h (cid:96)µ ( t ) = e − λt χ (cid:96)µ ( t ) + γ 0 (cid:90) t 0 ds e − λ ( t − s ) P (cid:88) α = 1 ∆ α ( s ) g (cid:96)α ( s ) Φ (cid:96) − 1 µα ( t , s ) z (cid:96)µ ( t ) = e − λt ξ (cid:96)µ ( t ) + γ 0 (cid:90) t 0 ds e − λ ( t − s ) P (cid:88) α = 1 ∆ α ( s ) φ ( h (cid:96)α ( s ) ) G (cid:96) + 1 µα ( t , s ) . ( 99 ) We see that the contribution from initial conditions is exponentially suppressed at large time t while the second term contributes most when the system has equilibrated . We provide an example of the weight decay DMFT showing its validity in a two layer ReLU network in Figure 3 . K Bayesian / Langevin Trained Mean Field Networks Rather than studying exact gradient ﬂow , many works have considered Langevin dynamics ( gradient ﬂow with white noise process on the weights ) of neural network training [ 25 , 32 , 91 , 30 , 31 ] . This setting is of special theoretical interest since the distribution of parameters converges at long times to a Gibbs equilibrium distribution which has a Bayesian interpretation [ 4 , 5 , 91 ] . The relevant Langevin equation for our mean ﬁeld gradient ﬂow is d θ ( t ) = − γ 2 ∇ L ( θ ( t ) ) dt − λβ − 1 θ ( t ) dt + (cid:112) 2 β − 1 d (cid:15) ( t ) , ( 100 ) where λ is a ridge penalty which controls the scale of parameters , and d (cid:15) ( t ) is a Brownian motion term which has covariance structure (cid:10) d (cid:15) ( t ) d (cid:15) ( t (cid:48) ) (cid:62) (cid:11) = δ ( t − t (cid:48) ) I . The parameter β , known as the inverse temperature controls the scale of the random Gaussian noise injected into this stochastic process . The dynamical treatment of the β → ∞ limit will coincide with our usual DMFT while the β (cid:28) ∞ will exhibit a nontrivial balance between the usual DMFT feature updates and the random Langevin noise . At late times , such a system will equilibrate to its Gibbs distribution . K . 1 Dynamical Analysis In this section we analyze the dynamical mean ﬁeld theory for these Langevin dynamics . First we note that the effect of regularization can be handled with a simple integrating factor d [ W (cid:96) ( t ) e λtβ ] = e λβ t (cid:34) γ 0 √ N (cid:88) µ ∆ µ ( t ) g (cid:96) + 1 µ ( t ) φ ( h (cid:96) µ ( t ) ) (cid:62) (cid:35) dt + (cid:112) 2 β − 1 e λtβ d (cid:15) (cid:96) ( t ) . ( 101 ) where d (cid:15) ( t ) ∈ R N × N is the Gaussian noise for layer (cid:96) at time t . It is straightforward to verify by Ito’s lemma that , under mean ﬁeld parameterization , the ﬂuctuations in f (cid:48) s dynamics due to Brownian motion are ∂f∂ θ · d (cid:15) ( t ) ∼ O ( N − 1 / 2 ) and are thus negligible in the N → ∞ limit . Thus the evolution of the network function takes the form ∂f µ ( t ) ∂t = (cid:88) α ∆ α ( t ) K µα ( t , t ) − λβ − 1 θ ( t ) · ∇ θ f µ ( t ) + 1 β Tr ∇ 2 θ f µ ( t ) We can express both of these parameter contractions in feature space provided we introduce the new features r (cid:96)i , µ ( t ) = ∂g (cid:96)i , µ ∂h (cid:96)i , µ which are necessary to compute Hessian terms like ∂ 2 f ∂W (cid:96)ij ∂W (cid:96)ij = N − 3 / 2 ∂ ∂W (cid:96)ij [ g (cid:96) + 1 i φ ( h (cid:96) j ) ] = N − 2 r i φ ( h (cid:96) j ) 2 in each layer . This gives the following evolution ∂f µ ( t ) ∂t = (cid:88) α ∆ α ( t ) K µα ( t , t ) − λβ − 1 (cid:88) (cid:96) (cid:10) z (cid:96)µ ( t ) φ ( h (cid:96)µ ( t ) ) (cid:11) + β − 1 (cid:88) (cid:96) (cid:10) r (cid:96) + 1 µ ( t ) (cid:11) (cid:10) φ ( h (cid:96)µ ( t ) ) 2 (cid:11) ( 102 ) 37 As before , we compute the next layer ﬁeld h (cid:96) + 1 in terms of χ (cid:96) + 1 and z (cid:96) in terms of ξ (cid:96) h (cid:96) + 1 µ ( t ) = e − λβ t χ (cid:96) + 1 µ ( t ) + (cid:90) t 0 e − λβ ( t − s ) (cid:34) dsγ 0 N (cid:88) α ∆ α ( s ) g (cid:96) + 1 α ( s ) φ ( h (cid:96)α ( s ) ) (cid:62) + (cid:114) 2 βN d (cid:15) (cid:96) ( s ) (cid:35) φ ( h (cid:96)µ ( t ) ) z (cid:96) + 1 µ ( t ) = e − λβ t ξ (cid:96) + 1 µ ( t ) + (cid:90) t 0 e − λβ ( t − s ) (cid:34) dsγ 0 N (cid:88) α ∆ α ( s ) g (cid:96) + 1 α ( s ) φ ( h (cid:96)α ( s ) ) (cid:62) + (cid:114) 2 βN d (cid:15) (cid:96) ( s ) (cid:35) (cid:62) g (cid:96) + 1 µ ( t ) The dependence on the initial condition through χ , ξ is suppressed at long times due the regularization factor e − λβ t , while the Brownian motion and gradient updates will survive in the t → ∞ limit . In addition to the usual { χ (cid:96) , ξ (cid:96) } ﬁelds which arise from the initial condition , we see that h (cid:96) ( t ) , z (cid:96) ( t ) also depend on the following ﬁelds which arise from the integrated Brownian motion χ (cid:15) , (cid:96)µ ( t ) = (cid:114) 2 βN (cid:90) ∞ 0 ds e − λβ ( t − s ) Θ ( t − s ) d (cid:15) (cid:96) ( s ) φ ( h (cid:96)µ ( t ) ) ξ (cid:15) , (cid:96)µ ( t ) = (cid:114) 2 βN (cid:90) ∞ 0 ds e − λβ ( t − s ) Θ ( t − s ) d (cid:15) (cid:96) ( s ) (cid:62) g (cid:96) + 1 µ ( t ) ( 103 ) Our aim is now to compute the moment generating function for the { χ , ξ , χ (cid:15) , ξ (cid:15) } ﬁelds which causally determine { h , z } . This MGF has the form Z = (cid:42) exp  (cid:88) (cid:96)µ (cid:90) ∞ 0 (cid:2) j (cid:96)µ ( t ) · χ (cid:96)µ ( t ) + v (cid:96)µ ( t ) · ξ (cid:96)µ ( t ) + j (cid:15) , (cid:96)µ ( t ) · χ (cid:15) , (cid:96)µ ( t ) + v (cid:15) , (cid:96)µ ( t ) · ξ (cid:15)(cid:96)µ ( t ) (cid:3)(cid:43) θ 0 , (cid:15) ( t ) ( 104 ) We insert Dirac - delta functions in the usual way to enforce the deﬁnitions of χ , ξ , χ (cid:15) , ξ (cid:15) and then average over θ 0 , (cid:15) ( t ) . These averages can be performed separately with the θ 0 average giving the identical terms as derived in previous sections . We focus on the average over Brownian disorder ln (cid:42) exp (cid:32) i √ 2 β − 1 / 2 N − 1 / 2 (cid:88) µ (cid:90) ∞ 0 dt Tr (cid:104) ˆ χ (cid:15) , (cid:96) + 1 µ ( t ) φ ( h (cid:96)µ ( t ) ) (cid:62) + g (cid:96) + 1 µ ( t ) ˆ ξ (cid:15) , (cid:96)µ ( t ) (cid:62) (cid:105) (cid:90) e − λβ ( t − s ) Θ ( t − s ) d (cid:15) ( s ) (cid:33)(cid:43) (cid:15) ( t ) = − 1 βN (cid:90) ∞ 0 ds (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:90) dt Θ ( t − s ) e − λβ ( t − s ) (cid:88) µ (cid:104) ˆ χ (cid:15) , (cid:96) + 1 µ ( t ) φ ( h (cid:96)µ ( t ) ) (cid:62) + g (cid:96) + 1 µ ( t ) ˆ ξ (cid:15) , (cid:96)µ ( t ) (cid:62) (cid:105)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2 = − 1 β (cid:90) ∞ 0 ds (cid:90) ∞ 0 dt (cid:90) ∞ 0 dt (cid:48) Θ ( t − s ) Θ ( t (cid:48) − s ) e − λβ ( t − s + t (cid:48) − s ) × (cid:88) µα (cid:104) ˆ χ (cid:15) , (cid:96) + 1 µ ( t ) · ˆ χ (cid:15) , (cid:96) + 1 α ( t (cid:48) ) Φ (cid:96)µα ( t , t (cid:48) ) + ˆ ξ (cid:15) , (cid:96)µ ( t ) · ˆ ξ (cid:15) , (cid:96)α ( t (cid:48) ) G (cid:96) + 1 µα ( t , t (cid:48) ) + 2 i ˆ χ (cid:15) , (cid:96) + 1 µ ( t ) · g (cid:96) + 1 α ( t (cid:48) ) A (cid:15) , (cid:96)µα ( t , t (cid:48) ) (cid:105) = − 1 2 λ (cid:90) ∞ 0 dt (cid:90) ∞ 0 dt (cid:48) exp (cid:18) − λ β ( t + t (cid:48) ) (cid:19) (cid:104) e 2 λβ min { t , t (cid:48) } − 1 (cid:105) × (cid:88) µα (cid:104) ˆ χ (cid:15) , (cid:96) + 1 µ ( t ) · ˆ χ (cid:15) , (cid:96) + 1 α ( t (cid:48) ) Φ (cid:96)µα ( t , t (cid:48) ) + ˆ ξ (cid:15) , (cid:96)µ ( t ) · ˆ ξ (cid:15) , (cid:96)α ( t (cid:48) ) G (cid:96) + 1 µα ( t , t (cid:48) ) + 2 i ˆ χ (cid:15) , (cid:96) + 1 µ ( t ) · g (cid:96) + 1 α ( t (cid:48) ) A (cid:15) , (cid:96)µα ( t , t (cid:48) ) (cid:105) ( 105 ) where we introduced the order parameter iA (cid:15) , (cid:96)µα ( t , t (cid:48) ) = 1 N φ ( h (cid:96)µ ( t ) ) · ˆ ξ (cid:15) , (cid:96)α ( s ) . We will use the shorthand for the temporal prefactor in the above C λ , β ( t , t (cid:48) ) = 1 λ exp (cid:16) − λβ ( t + t (cid:48) ) (cid:17) (cid:104) e 2 λβ min { t , t (cid:48) } − 1 (cid:105) ∼ t , t (cid:48) →∞ 1 λ exp (cid:16) − λβ | t − t (cid:48) | (cid:17) . We insert a Lagrange multiplier B (cid:15) , (cid:96) to enforce the deﬁnition of A (cid:15) , (cid:96) . After Z ∝ (cid:90) d Φ (cid:96)µα ( t , s ) d ˆΦ (cid:96)µα ( t , s ) dG (cid:96)µα ( t , s ) d ˆ G (cid:96)µα ( t , s ) dA (cid:96)µα ( t , s ) dB (cid:96)µα ( t , s ) dA (cid:15)(cid:96)µα ( t , s ) dB (cid:15)(cid:96)µα ( t , s ) × exp (cid:16) NS [ Φ , ˆΦ , G , ˆ G , A , B , A (cid:15) , B (cid:15) ] (cid:17) ( 106 ) 38 The order parameters can be determined by the saddle point equations . These equations for Φ , ˆΦ , G , ˆ G , A , B are the same as before . The new equations are δS δA (cid:15) , (cid:96)µα ( t , s ) = − B (cid:15) , (cid:96)µα ( t , s ) − iC λ , β ( t , s ) (cid:10) ˆ χ (cid:15) , (cid:96) + 1 µ ( t ) g (cid:96) + 1 α ( s ) (cid:11) = 0 δS δB (cid:15) , (cid:96)µα ( t , s ) = − A (cid:15) , (cid:96)µα ( t , s ) − iC λ , β ( t , s ) (cid:68) φ ( h (cid:96)µ ( t ) ) ˆ ξ (cid:15) , (cid:96)α ( s ) (cid:69) = 0 ( 107 ) Using the fact that Φ (cid:96) , G (cid:96) concentrate , we can use the Hubbard trick to linearize the quadratic terms in ˆ χ (cid:15) and ˆ ξ (cid:15) . exp (cid:32) − 1 2 (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds C λ , β ( t , s ) (cid:88) µα ˆ χ (cid:15) , (cid:96) + 1 µ ( t ) ˆ χ (cid:15) , (cid:96) + 1 α ( s ) Φ (cid:96)µα ( t , s ) (cid:33) = (cid:42) exp (cid:32) − i (cid:88) µ (cid:90) ∞ 0 dt u (cid:15) , (cid:96) + 1 µ ( t ) ˆ χ (cid:15) , (cid:96) + 1 µ ( t ) (cid:33)(cid:43) u (cid:15) , (cid:96) + 1 µ ( t ) ∼GP ( 0 , C (cid:12) Φ (cid:96) ) ( 108 ) exp (cid:32) − 1 2 (cid:90) ∞ 0 dt (cid:90) ∞ 0 ds C λ , β ( t , s ) (cid:88) µα ˆ ξ (cid:15) , (cid:96)µ ( t ) ˆ ξ (cid:15) , (cid:96)α ( s ) G (cid:96) + 1 µα ( t , s ) (cid:33) = (cid:42) exp (cid:32) − i (cid:88) µ (cid:90) ∞ 0 dt r (cid:15) , (cid:96)µ ( t ) ˆ ξ (cid:15) , (cid:96)µ ( t ) (cid:33)(cid:43) r (cid:15) , (cid:96)µ ( t ) ∼GP ( 0 , C (cid:12) G (cid:96) + 1 ) ( 109 ) Using the vectorization notation , we ﬁnd the interpretation that χ (cid:15) , (cid:96) and ξ (cid:15) , (cid:96) decouple as χ (cid:15) , (cid:96) + 1 = u (cid:15) , (cid:96) + 1 + A (cid:15) , (cid:96) + 1 g (cid:96) + 1 , ξ (cid:15) , (cid:96) = r (cid:15) , (cid:96) + B (cid:15) , (cid:96) (cid:62) φ ( h (cid:96) ) ( 110 ) A (cid:15) , (cid:96) + 1 = C λ , β (cid:12) (cid:28) ∂φ ( h (cid:96) ) ∂ r (cid:15) , (cid:96) (cid:29) , B (cid:15)(cid:96) = C λ , β (cid:12) (cid:28) ∂ g (cid:96) + 1 ∂ u (cid:96) + 1 (cid:29) (cid:62) ( 111 ) As before , we make the substitutions B → γ − 1 0 B (cid:62) and A → γ − 1 0 A and arrive at the ﬁnal DMFT equations { u (cid:96)µ ( t ) } ∼ GP ( 0 , Φ (cid:96) − 1 ) , { r (cid:96)µ ( t ) } ∼ GP ( 0 , G (cid:96) + 1 ) { u (cid:15) , (cid:96)µ ( t ) } ∼ GP ( 0 , C λ , β (cid:12) Φ (cid:96) − 1 ) , { r (cid:15) , (cid:96)µ ( t ) } ∼ GP ( 0 , C λ , β (cid:12) G (cid:96) + 1 ) h (cid:96)µ ( t ) = e − λβ t (cid:34) u (cid:96)µ ( t ) + γ 0 (cid:90) t 0 ds (cid:88) α A (cid:96) − 1 µα ( t , s ) g (cid:96)α ( s ) (cid:35) + u (cid:15) , (cid:96)µ ( t ) + γ 0 (cid:90) t 0 ds (cid:88) α (cid:104) A (cid:15) , (cid:96) − 1 µα ( t , s ) + e − λβ ( t − s ) ∆ α ( s ) Φ (cid:96) − 1 µα ( t , s ) (cid:105) g (cid:96)α ( s ) z (cid:96)µ ( t ) = e − λβ t (cid:34) r (cid:96)µ ( t ) + γ 0 (cid:90) t 0 ds (cid:88) α B (cid:96)µα ( t , s ) g (cid:96)α ( s ) (cid:35) + r (cid:15) , (cid:96)µ ( t ) + γ 0 (cid:90) t 0 ds (cid:88) α (cid:104) B (cid:15) , (cid:96)µα ( t , s ) + e − λβ ( t − s ) ∆ α ( s ) G (cid:96) + 1 µα ( t , s ) (cid:105) φ ( h (cid:96)α ( s ) ) ( 112 ) where the kernels are deﬁned in the usual way . As expected , the contributions from the initial conditions χ (cid:96) , ξ (cid:96) are exponentially suppressed at late time whereas the contributions from the Brownian disorder χ (cid:15) , (cid:96) , ξ (cid:15) , (cid:96) persist at late time . K . 2 Weak Feature Learning , Long Time Limit In the weak feature learning γ 0 → 0 and long time t → ∞ limit , the preactivation ﬁelds equilibrate to Gaussian processes h (cid:96)µ ( t ) ∼ u (cid:15) , (cid:96)µ ( t ) , z (cid:96)µ ( t ) ∼ r (cid:15) , (cid:96)α ( t ) , which have respective covariances H (cid:96)µα ( t , s ) = (cid:10) h (cid:96)µ ( t ) h (cid:96)α ( s ) (cid:11) = C λ , β ( t , s ) Φ (cid:96) − 1 µα ( t , s ) , Z (cid:96)µα ( t , s ) = (cid:10) z (cid:96)µ ( t ) z (cid:96)α ( s ) (cid:11) = C λ , β ( t , s ) G (cid:96) + 1 µα ( t , s ) . In this 39 long time limit , the feature kernels will be time translation invariant eg Φ (cid:96)µα ( t , s ) = Φ (cid:96)µα ( | t − s | ) . Letting τ = | t − s | and C λ , β ( τ ) = 1 λ exp (cid:16) − λβ τ (cid:17) , we have the following recurrence for H (cid:96) , Φ (cid:96) H 1 µα ( τ ) = C λ , β ( τ ) K xµα , Φ 1 µα ( τ ) = (cid:104) φ ( h ) φ ( h (cid:48) ) (cid:105) h , h (cid:48) ∼N ( 0 , H 1 ) , H 1 = (cid:20) H 1 µµ ( 0 ) H 1 µα ( τ ) H 1 µα ( τ ) H 1 αα ( 0 ) (cid:21) H (cid:96) + 1 µα ( τ ) = C λ , β ( τ ) Φ (cid:96)µα ( τ ) , Φ (cid:96) + 1 µα ( t , s ) = (cid:104) φ ( h ) φ ( h (cid:48) ) (cid:105) h , h (cid:48) ∼N ( 0 , H (cid:96) + 1 ) H (cid:96) + 1 = (cid:20) H (cid:96) + 1 µµ ( 0 ) H (cid:96) + 1 µα ( τ ) H (cid:96) + 1 µα ( τ ) H (cid:96) + 1 αα ( 0 ) (cid:21) ( 113 ) Similarly , we can obtain Z (cid:96) and G (cid:96) in a backward pass recursion Z Lµα ( τ ) = C λ , β ( τ ) , G Lµα ( τ ) = ˙Φ Lµα ( τ ) Z Lµα ( τ ) , ˙Φ Lµα ( τ ) = (cid:68) ˙ φ ( h ) ˙ φ ( h (cid:48) ) (cid:69) h , h (cid:48) ∼N ( 0 , H L ) Z (cid:96)µα ( τ ) = C λ , β ( τ ) G (cid:96) + 1 µα ( τ ) , G (cid:96)µα ( τ ) = ˙Φ (cid:96)µα ( τ ) Z (cid:96)µα ( τ ) , ˙Φ (cid:96)µα ( τ ) = (cid:68) ˙ φ ( h ) ˙ φ ( h (cid:48) ) (cid:69) h , h (cid:48) ∼N ( 0 , H (cid:96) ) ( 114 ) On the temporal diagonal τ = 0 , these equations give the usual recursions used to compute the NNGP kernels at initialization [ 5 ] , though with initialization variance C λ , β ( 0 ) = λ − 1 , set by the weight decay term in the Langevin dynamics . This indicates that the long time Langevin dynamics at γ 0 → 0 simply rescales the Gaussian weight variance based on λ . It would be interesting to explore ﬂuctuation dissipation relationships at ﬁnite γ 0 within this framework which we leave to future work . K . 3 Equilibrium Analysis The Langevin dynamics at ﬁnite N converges ( possibly in a time extensive in N ) to an equilibrium distribution with several interesting properties , as was recently studied by Aitchison et al [ 91 ] and implicitly by Seroussi et al [ 31 ] in a large sample size limit . This setting differs from the previous section where ﬁrst N → ∞ limit is taken , followed by a t → ∞ limit in the DMFT . This , section , on the other hand studies for any N , the t → ∞ limiting equilibrium distribution . This equilibrated distribution is then analyzed in the N → ∞ limit . The relationship between these two orders of limits remains an open problem . The equilibrium distribution over parameters p ( θ | D ) ∝ exp (cid:0) − βγ 2 L ( θ ) − λ 2 | θ | 2 (cid:1) can be viewed as a Bayes posterior with log - likelihood − βγ 2 L ( θ ) and a Gaussian prior with scale λ − 1 / 2 . In the mean ﬁeld limit with γ = √ Nγ 0 , we can express the density over pre - activations h (cid:96) and the output predictions f . This gives p ( f | D ) ∝ exp (cid:32) − Nγ 20 β (cid:88) µ (cid:96) ( f µ , y µ ) (cid:33) × (cid:90) d h (cid:96)µ (cid:42)(cid:89) µ δ (cid:18) f µ − 1 Nγ 0 w L · φ ( h Lµ ) (cid:19) (cid:89) µ(cid:96) δ (cid:18) h (cid:96) + 1 µ − 1 √ N W (cid:96) φ ( h (cid:96)µ ) (cid:19)(cid:43) θ ∼N ( 0 , λ − 1 I ) ∝ (cid:90) (cid:89) µ d ˆ f µ (cid:89) (cid:96)µα d Φ (cid:96)µα d ˆΦ (cid:96)µα exp (cid:32) − Nγ 20 β (cid:88) µ (cid:96) ( f µ , y µ ) − Nγ 0 (cid:88) µ ˆ f µ f µ + N 2 λ (cid:88) µα ˆ f µ Φ Lµα ˆ f α (cid:33) exp   N 2 (cid:88) (cid:96)µα Φ (cid:96)µα ˆΦ (cid:96)µα + N (cid:88) (cid:96) ln Z [ Φ (cid:96) − 1 , ˆΦ (cid:96) ]   Z [ Φ (cid:96) − 1 , ˆΦ (cid:96) ] = (cid:90) (cid:89) µ dh µ d ˆ h µ 2 π exp (cid:32) − 1 2 λ (cid:88) µα ˆ h µ Φ (cid:96) − 1 µα ˆ h α − 1 2 (cid:88) µα φ ( h µ ) ˆΦ (cid:96)µα φ ( h α ) + i (cid:88) µ ˆ h µ h µ (cid:33) ( 115 ) We see that p ( f | D ) ∝ (cid:82) d Φ d ˆΦ exp (cid:16) NS [ Φ , ˆ Φ ] (cid:17) where S = − γ 20 β (cid:88) µ (cid:96) ( f µ , y µ ) − γ 0 (cid:88) µ ˆ f µ f µ + 1 2 λ (cid:88) µα ˆ f µ Φ Lµα ˆ f α + 1 2 (cid:88) (cid:96)µα Φ (cid:96)µα ˆΦ (cid:96)µα + (cid:88) (cid:96) ln Z [ Φ (cid:96) − 1 , ˆΦ (cid:96) ] ( 116 ) 40 Thus the predictions f µ become non - random in this N → ∞ limit and can be determined from the saddle point equations as in [ 91 ] . Again , letting ∆ µ = − ∂∂f µ (cid:96) ( f µ , y µ ) , we ﬁnd ∂S ∂f µ = γ 0 ˆ f µ − γ 20 β ∆ µ = 0 , ∂S ∂ ˆ f µ = − γ 0 f µ + 1 λ (cid:88) α Φ Lµα ˆ f α = 0 ∂S ∂ Φ Lµα = 1 2 λ ˆ f µ ˆ f α + 1 2 ˆΦ Lµα = 0 , ∂S ∂ ˆΦ Lµα = 1 2Φ (cid:96)µα − 1 2 (cid:10) φ ( h Lµ ) φ ( h Lα ) (cid:11) = 0 ∂S ∂ Φ (cid:96)µα = − 1 2 (cid:68) ˆ h (cid:96) + 1 µ ˆ h (cid:96) + 1 α (cid:69) + 1 2 ˆΦ (cid:96)µα = 0 , ∂S ∂ ˆΦ (cid:96)µα = 1 2Φ (cid:96)µα − 1 2 (cid:10) φ ( h (cid:96)µ ) φ ( h (cid:96)α ) (cid:11) = 0 ( 117 ) which implies that f µ at the ﬁxed point satisﬁes the following equations f µ = β λ (cid:88) α Φ Lµα ∆ α , ∆ α = − ∂(cid:96) ( f α , y α ) ∂f α . ( 118 ) The last layer’s dual kernel has the form ˆΦ Lµα = − γ 20 β 2 2 λ ∆ µ ∆ α , which we see vanishes as feature learning strength is taken to zero γ 0 → 0 , while for non - negligible γ 0 , we see that the last layer features are non - Gaussian . We thus see that the moment generating function for the last layer ﬁeld has the form Z [ Φ L − 1 , ˆΦ L ] = (cid:90) (cid:89) µ dh µ d ˆ h µ 2 π exp   − 1 2 λ (cid:88) µα ˆ h µ ˆ h α Φ L − 1 µα + γ 20 β 2 2 λ (cid:34)(cid:88) µ ∆ µ φ ( h µ ) (cid:35) 2 + i (cid:88) µ ˆ h µ h µ   ( 119 ) In the γ 0 → 0 limit , the non - Gaussian component of this density vanishes . Now that we have this form , we can compute Φ L conditional on Φ L − 1 . Next , we calculate ˆΦ L − 1 µα = (cid:68) ˆ h Lµ ˆ h Lα (cid:69) , giving ˆΦ L − 1 = λ [ Φ L − 1 ] − 1 − λ 2 [ Φ L − 1 ] − 1 (cid:10) h L h L (cid:62) (cid:11) [ Φ L − 1 ] − 1 ( 120 ) Again , we note that in the γ 0 → 0 limit , since (cid:10) h L h L (cid:11) ∼ λ − 1 Φ L − 1 , so that ˆ Φ L − 1 = 0 , implying that the h L − 1 ﬁelds are also Gaussian in this γ 0 → 0 limit . For arbitrary γ 0 , this recursive argument can be completed going backwards using Φ (cid:96) = (cid:10) φ ( h (cid:96) ) φ ( h (cid:96) ) (cid:62) (cid:11) , ˆΦ (cid:96) − 1 = λ [ Φ (cid:96) − 1 ] − 1 − λ 2 [ Φ (cid:96) − 1 ] − 1 (cid:10) h (cid:96) h (cid:96) (cid:62) (cid:11) [ Φ (cid:96) − 1 ] − 1 ( 121 ) For deep linear networks , the distributions are all Gaussian , allowing one to close algebraically , the saddle point equations for Φ , ˆΦ [ 91 ] . L Momentum Dynamics Standard gradient descent often converges slowly and requires careful tuning of learning rate . Momen - tum , in contrast can , be stable under a wider range of learning rates and can beneﬁt from acceleration on certain problems [ 92 – 95 ] . In this section we show that our ﬁeld theory is still valid when training with momentum ; simply altering the ﬁeld deﬁnitions appropriately gives the inﬁnite - width feature learning behavior . Momentum uses a low - pass ﬁltered version of the gradients to update the weights . A continuous limt of momentum dynamics on the trainable parameters { W (cid:96) } would give the following differential equations . ∂ ∂t W (cid:96) ( t ) = Q (cid:96) ( t ) τ d dt Q (cid:96) ( t ) = − Q (cid:96) + γ N (cid:88) µ ∆ µ ( t ) g (cid:96) + 1 µ ( t ) φ ( h (cid:96)µ ( t ) ) (cid:62) ( 122 ) We write the expression this way so that the small time constant τ → 0 limit corresponds to classic gradient descent . Integrating out the Q (cid:96) ( t ) variable , this gives the following weight dynamics W (cid:96) ( t ) = W (cid:96) ( 0 ) + γ Nτ (cid:90) t 0 dt (cid:48) (cid:90) t (cid:48) 0 dt (cid:48)(cid:48) e − ( t (cid:48) − t (cid:48)(cid:48) ) / τ (cid:88) µ ∆ µ ( t (cid:48)(cid:48) ) g (cid:96) + 1 µ ( t (cid:48)(cid:48) ) φ ( h (cid:96)µ ( t (cid:48)(cid:48) ) ) (cid:62) ( 123 ) 41 which implies the following ﬁeld evolution h (cid:96) + 1 µ ( t ) = χ (cid:96) + 1 µ ( t ) + γ 0 τ (cid:90) t 0 dt (cid:48) (cid:90) t (cid:48) 0 dt (cid:48)(cid:48) e − ( t (cid:48) − t (cid:48)(cid:48) ) / τ (cid:88) α ∆ α ( t (cid:48)(cid:48) ) g (cid:96) + 1 α ( t (cid:48)(cid:48) ) Φ (cid:96)µα ( t , t (cid:48)(cid:48) ) z (cid:96)µ ( t ) = ξ (cid:96)µ ( t ) + γ 0 τ (cid:90) t 0 dt (cid:48) (cid:90) t (cid:48) 0 dt (cid:48)(cid:48) e − ( t (cid:48) − t (cid:48)(cid:48) ) / τ (cid:88) α dt (cid:48)(cid:48) ∆ α ( t (cid:48)(cid:48) ) φ ( h (cid:96)α ( t (cid:48)(cid:48) ) ) G (cid:96) + 1 µα ( t , t (cid:48)(cid:48) ) ( 124 ) We see that in the τ → 0 limit , the t (cid:48)(cid:48) integral is dominated by the contribution at t (cid:48)(cid:48) ∼ t (cid:48) recovering usual gradient descent dynamics . For τ (cid:29) 0 , we see that the integral accumulates additional contributions from the past values of ﬁelds and kernels . M Discrete Time Our model can also be accommodated in discrete time , though we lose the NTK as a key player in the theory ( note that ddt f µ = df µ dθ · dθdt = (cid:80) α ∆ α K NTKµα requires a continuous time limit of the gradient descent dynamics ) . For a discrete time analysis we let t ∈ N and deﬁne our network function as f µ ( t ) = 1 Nγ 0 w L ( t ) · φ ( h Lµ ( t ) ) = 1 Nγ 0 (cid:34) w L ( 0 ) + γ 0 t − 1 (cid:88) s = 0 (cid:88) α ∆ α ( s ) φ ( h Lα ( s ) ) (cid:35) · φ ( h Lµ ( t ) ) = 1 Nγ 0 w L ( 0 ) · φ ( h Lµ ( t ) ) + (cid:88) α (cid:88) s < t ∆ α ( s ) Φ Lµα ( t , s ) ( 125 ) We treat f µ ( t ) as a potentially random variable and insert 1 = (cid:90) d ˆ f µ ( t ) df µ ( t ) 2 πN − 1 exp (cid:32) i ˆ f µ ( t ) (cid:34) Nf µ ( t ) − 1 γ 0 w L ( 0 ) · φ ( h L ( t ) ) − N (cid:88) α (cid:88) s < t ∆ α ( s ) Φ Lµα ( t , s ) (cid:35)(cid:33) ( 126 ) Noting that w L ( 0 ) is involved in the deﬁnition of both f µ ( t ) and ξ Lµ ( t ) , we see that the average over w L ( 0 ) now takes the form (cid:42) exp (cid:32) i (cid:88) µt [ ˆ ξ Lµ ( t ) + γ − 1 0 ˆ f µ ( t ) φ ( h Lµ ( t ) ) ] · w L ( 0 ) (cid:33)(cid:43) w L ( 0 ) = exp (cid:32) − 1 2 (cid:88) µtαs ˆ ξ Lµ ( t ) · ˆ ξ Lα ( s ) (cid:33) exp (cid:32) − N 2 γ 20 (cid:88) µαts ˆ f µ ( t ) ˆ f α ( s ) Φ Lµα ( t , s ) (cid:33) exp (cid:32) − 1 γ 0 (cid:88) µαts ˆ f µ ( t ) φ ( h Lµ ( t ) ) · ˆ ξ Lα ( s ) (cid:33) ( 127 ) We extend our deﬁnition as before iA Lµα ( t , s ) = 1 Nγ 0 φ ( h Lµ ( t ) ) · ξ Lα ( s ) . Proceeding with the calcula - tion as usual , we ﬁnd that Z ∝ (cid:90) df µ ( t ) d ˆ f µ ( t ) d Φ (cid:96) . . . dB (cid:96) exp (cid:16) NS [ { f , ˆ f , Φ (cid:96) , ˆΦ (cid:96) , . . . , A (cid:96) , B (cid:96) } ] (cid:17) S = i (cid:88) µt ˆ f µ ( t ) f µ ( t ) − 1 2 γ 2 0 (cid:88) µαts ˆ f µ ( t ) ˆ f α ( s ) Φ L µα ( t , s ) − i (cid:88) µαts ˆ f µ ( t ) A L µα ( t , s ) − i (cid:88) µtαs ˆ f µ ( t ) [ Θ ( t − s ) ∆ α ( s ) Φ L µα ( t , s ) ] + (cid:88) (cid:96)µαts [ Φ (cid:96)µα ˆΦ (cid:96)µα ( t , s ) + G (cid:96)µα ( t , s ) ˆ G µα ( t , s ) − A (cid:96)µα ( t , s ) B (cid:96)µα ( t , s ) ] + ln Z [ { Φ (cid:96) , ˆΦ (cid:96) , . . . , A (cid:96) , B (cid:96) } ] ( 128 ) 42 The saddle point equations can now be analyzed . In addition to the usual order parameters , we note that f , ˆ f also generate saddle point equations ∂S ∂f µ ( t ) = i ˆ f µ ( t ) = 0 ∂S ∂i ˆ f µ ( t ) = f µ ( t ) + 1 γ 20 (cid:88) αs Φ Lµα ( t , s ) ( i ˆ f α ( s ) ) − (cid:88) αs A Lµα ( t , s ) − (cid:88) αs Θ ( t − s ) ∆ α ( s ) Φ Lµα ( t , s ) ( 129 ) We also obtain saddle point equations for the new A L , B L order parameters . ∂S ∂A Lµα ( t , s ) = − B Lµα ( t , s ) − i ˆ f µ ( t ) = 0 ( 130 ) ∂S ∂B Lµα ( t , s ) = − A Lµα ( t , s ) + iγ − 1 0 (cid:68) φ ( h Lµ ( t ) ) ˆ ξ Lα ( s ) (cid:69) = 0 ( 131 ) which implies B Lµα ( t , s ) = 0 and A L = γ − 1 0 (cid:28) φ ( h Lµ ( t ) ) ∂r Lα ( s ) (cid:29) . This gives the following DMFT f µ ( t ) = (cid:88) s < t (cid:88) α Φ Lµα ( t , s ) ∆ α ( s ) + (cid:88) αs A Lµα ( t , s ) u (cid:96) ∼ N ( 0 , Φ (cid:96) − 1 ) , r (cid:96) ∼ N ( 0 , G (cid:96) + 1 ) h (cid:96)µ ( t ) = u (cid:96)µ ( t ) + γ 0 (cid:88) αs [ A (cid:96) − 1 µα ( t , s ) + Θ ( t − s ) ∆ α ( s ) Φ (cid:96) − 1 µα ( t , s ) ] g (cid:96)α ( s ) z (cid:96)µ ( t ) = r (cid:96)µ ( t ) + γ 0 (cid:88) αs [ B (cid:96)µα ( t , s ) + Θ ( t − s ) ∆ α ( s ) G (cid:96) + 1 µα ( t , s ) ] φ ( h (cid:96)α ( s ) ) Φ (cid:96)µα ( t , s ) = (cid:10) φ ( h (cid:96)µ ( t ) ) φ ( h (cid:96)α ( s ) ) (cid:11) , G (cid:96)µα ( t , s ) = (cid:10) g (cid:96)µ ( t ) g (cid:96)α ( s ) (cid:11) A (cid:96)µα ( t , s ) = γ − 1 0 (cid:42) ∂φ ( h (cid:96)µ ( t ) ) ∂u (cid:96)α ( s ) (cid:43) , B (cid:96)µα ( t , s ) = γ − 1 0 (cid:42) ∂g (cid:96) + 1 µ ( t ) ∂r (cid:96) + 1 α ( s ) (cid:43) . ( 132 ) We leave it to future work to verify that a continuous time limit of the above DMFT recovers function evolution governed by the NTK . N Equivalent Parameterizations In this section , we show the equivalence of our parameterization scheme with many alternatives including the µP parameterization of Yang [ 1 ] . We also compare the derived stochastic processes obtained with DMFT and Tensor Programs in Appendix Section N . 6 . Following Yang we use a modiﬁed variant of abc parameterization ( we assume one which deﬁnes the features h (cid:96) + 1 = N − a (cid:96) W (cid:96) φ ( h (cid:96) ) with W (cid:96)ij ∼ N ( 0 , N − b (cid:96) ) and η = γ 2 N − c ) . Lastly , we will take γ = γ 0 N d and ﬁnd that only d = 12 will allow feature learning h 1 = D − a 0 W 0 x µ , W 0 ij ∼ N ( 0 , D − b 0 ) h (cid:96) + 1 = N − a (cid:96) W (cid:96) φ ( h (cid:96) ) , W (cid:96)ij ∼ N ( 0 , N − b ) f = 1 γ h L + 1 , h L + 1 = N − a L w L · φ ( h L ) , w Li ∼ N ( 0 , N − b ) γ = γ 0 N d , γ 0 = O N ( 1 ) ( 133 ) We will now derive constraints on ( a , b , c , d ) which give desired large width behavior . We will identify a one - dimensional family of parameterizations which satisfy three desiderata of network training 1 . ﬁnite preactivations , 2 . learning in ﬁnite time , 3 . feature learning . 43 N . 1 Predictions Evolve in O N ( 1 ) time As before we let the NTK be the matrix which deﬁnes network prediction dynamics ∂ t f µ = (cid:80) α K NTKµα ∆ α . We demand that this matrix be O N ( 1 ) so that the network predictions have O N ( 1 ) evolution K NTKµα = γ 2 N − c (cid:88) (cid:96) ∂f µ ∂ W (cid:96) · ∂f α ∂ W (cid:96) = N − c (cid:34) φ ( h Lµ ) · φ ( h Lα ) N 2 a L + (cid:88) (cid:96) ∂h L + 1 µ ∂ h (cid:96) + 1 µ · ∂h L + 1 α ∂ h (cid:96) + 1 α φ ( h (cid:96)µ ) · φ ( h (cid:96)α ) N 2 a (cid:96) + ∂h L + 1 µ ∂ h 1 µ · ∂h L + 1 α ∂ h 1 α x µ · x α D 2 a 0 (cid:35) = N − c (cid:34) N 1 − 2 a L Φ Lµα + (cid:88) (cid:96) N 1 − 2 a (cid:96) ∂h L + 1 µ ∂ h (cid:96) + 1 µ · ∂h L + 1 α ∂ h (cid:96) + 1 α Φ (cid:96)µα + D 1 − 2 a 0 ∂h L + 1 µ ∂ h 1 µ · ∂h L + 1 α ∂ h 1 α K x (cid:35) ( 134 ) where we used the usual deﬁnition of the kernels Φ (cid:96) = 1 N φ ( h (cid:96) ) · φ ( h (cid:96) ) which are O N ( 1 ) provided each neuron’s preactivation h (cid:96)i = O N ( 1 ) . We see that the choice a (cid:96) = 12 recovers the parameterization discussed in Appendix D . Further to have O N ( 1 ) evolution of the output predictions f µ we need K NTK = O N ( 1 ) . Now , to enforce the O N ( 1 ) evolution of predictions we demand N 1 − c − 2 a (cid:96) = O N ( 1 ) , (cid:96) ∈ { 1 , . . . , L } N − c D 1 − 2 a 0 = O N ( 1 ) ( 135 ) If , on the other hand , we take D ∼ O N ( N ) , then this simply demands the constraint that c = 2 a (cid:96) − 1 for all (cid:96) ∈ { 0 , . . . , L } . N . 2 Fields Are O N ( 1 ) Having ﬁelds which are O N ( 1 ) can be ensured at initialization provided that (cid:10) h (cid:96) + 1 i h (cid:96) + 1 j (cid:11) = N − 2 a (cid:96) (cid:88) k , k (cid:48) (cid:10) W (cid:96)ik ( 0 ) W (cid:96)jk (cid:48) ( 0 ) (cid:11) φ ( h (cid:96)k ) φ ( h (cid:96)k (cid:48) ) = δ ij N 1 − 2 a (cid:96) − b (cid:96) Φ (cid:96) = O N ( 1 ) ( 136 ) which implies that 2 a (cid:96) + b (cid:96) = 1 . Again we see that a (cid:96) = 12 , b (cid:96) = 0 works , but this is not the only possible scaling . Alternatively standard parameterization a (cid:96) = 0 , b (cid:96) = 1 will also preserve the O N ( 1 ) scale of the features . We next need to analyze the scale of the feature gradients ∂h L + 1 ∂ h (cid:96) . We start with the last layer ∂h (cid:96) + 1 ∂ h L = N − a L w L (cid:12) ˙ φ ( h L ) = ⇒ ∂h (cid:96) + 1 ∂ h L · ∂h (cid:96) + 1 ∂ h L = O N ( N 1 − 2 a (cid:96) − b (cid:96) ) ( 137 ) Since we already demanded that 2 a L + b L = 1 , this inner product will be O N ( 1 ) . Now we will see whether it remains O N ( 1 ) under its recursion ∂h L + 1 ∂ h (cid:96) = (cid:18) ∂ h (cid:96) + 1 ∂ h (cid:96) (cid:19) (cid:62) ∂h L + 1 ∂ h (cid:96) + 1 = ˙ φ ( h (cid:96) ) (cid:12) (cid:20) N − a (cid:96) W (cid:96) ( 0 ) (cid:62) ∂h L + 1 ∂ h (cid:96) (cid:21) ( 138 ) Now , letting g (cid:96) = √ N ∂h L + 1 ∂ h (cid:96) and z (cid:96) = N − a (cid:96) W (cid:96) ( 0 ) (cid:62) g (cid:96) + 1 we have (cid:104) z i z j (cid:105) = δ ij N 1 − 2 a (cid:96) − b (cid:96) G (cid:96) + 1 ( 139 ) which is indeed O N ( 1 ) as desired provided that 2 a + b = 1 . 44 N . 3 O N ( 1 ) Feature Evolution Now , we desire that the ﬁelds h i , z i all evolve by an O N ( 1 ) amount during network training , which is equivalent to stable feature learning . The update equation for W (cid:96) and h (cid:96) give d dt W (cid:96) = γN − c − a (cid:96) (cid:88) µ ∆ µ ∂h L + 1 ∂ h (cid:96) + 1 µ φ (cid:96) (cid:62) µ = γ 0 N d − c − a (cid:96) − 12 (cid:88) µ ∆ µ g (cid:96) + 1 µ φ (cid:96) (cid:62) µ h (cid:96) + 1 µ ( t ) = χ (cid:96) + 1 µ ( t ) + γ 0 N d − c − 2 a (cid:96) + 12 (cid:88) α (cid:90) t 0 ds ∆ α ( s ) g (cid:96) + 1 α ( s ) Φ (cid:96)µα ( t , s ) ( 140 ) where we used γ = γ 0 N d . This equation implies that d − c − 2 a (cid:96) + 12 = 0 is necessary and sufﬁcient for O N ( 1 ) feature evolution . N . 4 Putting Constraints Together We now let γ = γ 0 N d . We see that the set of parameterizations which yield O ( 1 ) feature evolution are those for which 1 . Features h , z are O N ( 1 ) = ⇒ 2 a (cid:96) + b (cid:96) = 1 2 . Outputs predictions evolve in O N ( 1 ) time = ⇒ c + 2 a (cid:96) = 1 3 . Features h , z have O N ( 1 ) evolution = ⇒ d = c + 2 a (cid:96) − 12 = 12 . We see that the parameterization discussed in Appendix D satisﬁes these with d = 12 , a (cid:96) = 12 , b (cid:96) = 0 , c = 0 . The quite general requirement for feature learning that d = 12 indicates that γ = γ 0 √ N for any choice of a (cid:96) , b (cid:96) , c . The set of parameterizations which meet these three requirements is one dimensional with d = 12 , and ( a (cid:96) , b (cid:96) , c (cid:96) ) ∈ { ( a (cid:96) , 1 − 2 a (cid:96) , 1 − 2 a (cid:96) ) : a (cid:96) ∈ R } . However , in the next section , we show that if one demands O N ( 1 ) learning rate , then the parameterization is unique and is the µP parameterization of Yang and Hu [ 1 ] . N . 5 O N ( 1 ) Learning Rate We are also interested in a parameterization for which we can have O ( 1 ) learning rate which are those for which γ 2 N − c = O N ( N 2 d − c ) = O N ( 1 ) = ⇒ c = 2 d = 1 . Under this constraint , a (cid:96) = 0 and b (cid:96) = 1 , which corresponds to standard parameterization , modiﬁed by γ = γ 0 √ N in the last layer . In a computational algorithm , the learning rate would be η = γ 2 N − c = γ 20 . This is equivalent to the µP parameterization of Yang and Hu [ 1 ] . N . 6 Equivalence of DMFT at γ 0 = 1 and Tensor Programs derived Stochastic Process Now that we have established that the parameterization we consider here ( modiﬁed NTK parame - terization ) is equivalent to µP , ( modiﬁed standard parameterization ) , we will now demonstrate that the stochastic process which we obtained through a stationary action principle applied to our DMFT action S is equivalent to the stochastic process derived from the Tensor Programs framework of Yang [ 90 , 1 ] . Using the notation from Appendix H of Yang and Hu [ 1 ] , they give the following evolution equations for the preactivations in a hidden layer in one pass SGD Z h t = ˆ Z Wx t + ˙ Z Wx t − t − 1 (cid:88) s = 0 χ s Z dh s E [ Z x s Z x t ] Z dx t = ˆ Z W (cid:62) dh t + ˙ Z W (cid:62) dh t − t − 1 (cid:88) s = 0 χ s Z x s E [ Z dh t Z dh s ] ( 141 ) where ˆ Z Wx t is mean zero Gaussian variable with covariance E [ Z x t Z x s ] and ˆ Z W (cid:62) dh t is mean zero Gaussian with covariance E [ Z dh t Z dh s ] . We can switch to the notation of this work by mak - ing the substitutions Z h t → h ( t ) , ˆ Z Wx t → u ( t ) , χ s → − ∆ ( s ) , ˙ Z Wx → (cid:80) s ∆ ( s ) A ( t , s ) and E [ Z x s Z x t ] → Φ ( t , s ) , and so on . A summary of the full set of notational substitutions between this work and TP are summarized in Table 2 . After these substitutions are made , we see that the equations above match the one - pass SGD version 45 DMFT h ( t ) χ ( t ) g ( t ) ξ ( t ) Φ (cid:96) ( t , s ) G (cid:96) ( t , s ) A (cid:96) ( t , s ) , B (cid:96) ( t , s ) ∆ ( t ) TP Z h t Z Wx t Z dx t Z W (cid:62) dh t E [ Z x t Z x s ] E [ Z dh t Z dh s ] θ ts − χ t Table 2 : A dictionary relating the notation of the Tensor Programs ( TP ) framework [ 1 ] and this work . of the DMFT Equations in Appendix M . A similar identiﬁcation can be made for the backward pass . This shows that both Tensor Programs and DMFT , though alternative derivations , give identical descriptions of the stochastic processes induced by random initializations + GD in inﬁnite neural networks . O Gradient Independence The gradient independence approximation treats the random initial weight matrix W (cid:96) ( 0 ) as a independently sampled Gaussian matrix when used in the backward pass . We let this second matrix be ˜ W (cid:96) ( 0 ) . As before , we have χ (cid:96) + 1 = 1 √ N W (cid:96) ( 0 ) φ ( h (cid:96) ) , however we now deﬁne ξ (cid:96) = 1 √ N ˜ W (cid:96) ( 0 ) (cid:62) g (cid:96) + 1 . Now , when computing the moment generating function Z , the integrals over W (cid:96) ( 0 ) and ˜ W (cid:96) ( 0 ) factorize (cid:42) exp (cid:32) i √ N (cid:90) ∞ 0 dt (cid:34)(cid:88) µ ˆ χ (cid:96) + 1 ( t ) W (cid:96) ( 0 ) φ ( h (cid:96)µ ( t ) ) + g (cid:96) + 1 µ ( t ) (cid:62) ˜ W (cid:96) ( 0 ) ξ (cid:96)µ ( t ) (cid:35)(cid:33)(cid:43) = exp (cid:32) − 1 2 (cid:88) µα (cid:90) ∞ 0 dt (cid:48) (cid:90) ∞ 0 ds (cid:48) (cid:104) ˆ χ (cid:96) + 1 µ ( t ) · ˆ χ (cid:96) + 1 α ( s ) Φ (cid:96)µα ( t , s ) + ˆ ξ (cid:96)µ ( t ) · ˆ ξ (cid:96)α ( s ) G (cid:96) + 1 µα ( t , s ) (cid:105)(cid:33) . ( 142 ) We see that in this ﬁeld theory , the ﬁelds χ , ξ are all independent Gaussian processes { χ (cid:96) + 1 µ ( t ) } ∼ GP ( 0 , Φ (cid:96) ) and { ξ (cid:96)µ ( t ) } ∼ GP ( 0 , G (cid:96) + 1 ) . This corresponds to making the assumption that A (cid:96) = B (cid:96) = 0 so that χ = u and ξ = r within the full DMFT . P Perturbation Theory P . 1 Small γ 0 Expansion In this section we analyze the leading corrections in a small γ 0 expansion of our DMFT theory . All ﬁelds are expanded in power series in γ 0 . h (cid:96)µ ( t ) − u (cid:96)µ ( t ) = ∞ (cid:88) n = 1 γ n 0 h (cid:96) , ( n ) µ ( t ) z (cid:96)µ ( t ) − r (cid:96)µ ( t ) = ∞ (cid:88) n = 1 γ n 0 z (cid:96) , ( n ) µ ( t ) ( 143 ) Our goal is to calculate all corrections to the kernels up to O ( γ 30 ) to show that the leading correction is O ( γ 20 ) and the subleading correction is O ( γ 40 ) . It will again be convenient to utilize the vector notation deﬁned in D . We note that unlike other works on perturbation theory in wide networks , we do not attempt to characterize ﬂuctuation effects in the kernels due to ﬁnite width , but rather operate in a regime where the kernels are concentrating and their variance is negligible . For a more thorough discussion of perturbative ﬁeld theory in ﬁnite width networks , see [ 27 , 28 , 35 ] . P . 1 . 1 Linear Network The kernels in deep linear networks can be expanded in powers of γ 20 giving a leading order correction of size O ( γ 20 ) and can be computed explicitly from the closed saddle point equations . We use the symmetrizer { X , Y } sym = XY + Y (cid:62) X (cid:62) as shorthand . The leading order behavior of 46 C (cid:96) ∼ C ( 0 ) + O ( γ 20 ) , D (cid:96) ∼ D ( 0 ) + O ( γ 20 ) , H (cid:96) , 0 = H ( 0 ) = K x ⊗ 11 (cid:62) , G (cid:96) , ( 0 ) = G ( 0 ) = 11 (cid:62) is independent of layer index so we ﬁnd the following leading order corrections H (cid:96) ∼ H ( 0 ) + (cid:96)γ 20 (cid:16) { C ( 0 ) D ( 0 ) , H ( 0 ) } sym + C ( 0 ) 11 (cid:62) C ( 0 ) (cid:62) (cid:17) + O ( γ 40 ) G (cid:96) ∼ 11 (cid:62) + ( L + 1 − (cid:96) ) γ 20 ( { D ( 0 ) C ( 0 ) , 11 (cid:62) } sym + D ( 0 ) H ( 0 ) [ D ( 0 ) ] (cid:62) ) + O ( γ 40 ) K NTK ∼ L H 0 + γ 20 L ( L + 1 ) 2 (cid:16) { C ( 0 ) D ( 0 ) , K x } sym + C ( 0 ) 11 (cid:62) C ( 0 ) (cid:62) (cid:17) + γ 20 L ( L + 1 ) 2 K x ⊗ ( { D ( 0 ) C ( 0 ) , 11 (cid:62) } sym + D ( 0 ) H ( 0 ) [ D ( 0 ) ] (cid:62) ) + O ( γ 40 ) ( 144 ) Note that [ C 0 g ] µt = (cid:82) t 0 dt (cid:48) (cid:80) β H 0 µβ ( t , t (cid:48) ) ∆ β ( t (cid:48) ) g ( t (cid:48) ) = (cid:80) β K xµβ (cid:82) t 0 dt (cid:48) ∆ β ( t (cid:48) ) g ( t (cid:48) ) and note that [ Dh ] t = (cid:82) t 0 dt (cid:48) G 0 ( t , t (cid:48) ) (cid:80) α ∆ α ( t (cid:48) ) h α ( t (cid:48) ) = (cid:80) α (cid:82) t 0 dt (cid:48) ∆ α ( t (cid:48) ) h α ( t (cid:48) ) . H (cid:96)µν ( t , s ) = K xµν + (cid:96)γ 20 (cid:88) αβ K xµα K xνβ (cid:90) t 0 dt (cid:48) ∆ α ( t (cid:48) ) (cid:90) t (cid:48) 0 dt (cid:48)(cid:48) ∆ β ( t (cid:48)(cid:48) ) + ( ( µ , t ) ↔ ( ν , s ) ) + (cid:96)γ 20 (cid:88) αβ K xµα K xνβ (cid:20)(cid:90) t 0 dt (cid:48) ∆ α ( t (cid:48) ) (cid:21) (cid:20)(cid:90) s 0 ds (cid:48) ∆ β ( s (cid:48) ) (cid:21) G (cid:96) ( t , s ) = 1 + γ 20 ( L + 1 − (cid:96) ) (cid:88) αβ K xαβ (cid:90) t 0 dt (cid:48) ∆ α ( t (cid:48) ) (cid:90) t (cid:48) 0 dt (cid:48)(cid:48) ∆ β ( t (cid:48)(cid:48) ) + ( t ↔ s ) + γ 20 ( L + 1 − (cid:96) ) (cid:88) αβ K xµα (cid:20)(cid:90) t 0 dt (cid:48) ∆ α ( t (cid:48) ) (cid:21) (cid:20)(cid:90) s 0 ds (cid:48) ∆ α ( s (cid:48) ) (cid:21) ( 145 ) We can simplify the notation by introducing functions v α ( t ) = (cid:82) t 0 ∆ α ( t (cid:48) ) and v αβ ( t ) = (cid:82) t 0 dt (cid:48) ∆ α ( t (cid:48) ) (cid:82) t (cid:48) 0 dt (cid:48)(cid:48) ∆ β ( t (cid:48)(cid:48) ) . H (cid:96)µν ( t , s ) = K xµν + (cid:96)γ 20 (cid:88) αβ K xµα K xνβ [ v αβ ( t ) + v βα ( s ) ] + (cid:96)γ 20 (cid:88) αβ K xµα K xνβ v α ( t ) v β ( s ) G (cid:96) ( t , s ) = 1 + γ 20 ( L + 1 − (cid:96) ) (cid:88) αβ K xαβ [ v αβ ( t ) + v βα ( s ) + v α ( t ) v β ( s ) ] ( 146 ) Using the fact that K NTKµα ( t , s ) = L (cid:88) (cid:96) = 0 G (cid:96) + 1 ( t , s ) H (cid:96)µα ( t , s ) ∼ ( L + 1 ) K xµα + γ 20 L (cid:88) (cid:96) = 1 H (cid:96) , 2 µα ( t , s ) + γ 20 L (cid:88) (cid:96) = 1 G (cid:96) , 2 ( t , s ) K xµα + O ( γ 40 ) ( 147 ) and utilizing the identity (cid:80) L(cid:96) = 1 (cid:96) = 12 L ( L + 1 ) , we recover the result provided in the main text . P . 2 Nonlinear Perturbation Theory We start with the formula which implicitly deﬁnes h , z h (cid:96) = u (cid:96) + γ 0 C (cid:96) [ ˙ φ ( h (cid:96) ) (cid:12) z (cid:96) ] , z (cid:96) = r (cid:96) + γ 0 D (cid:96) φ ( h (cid:96) ) ( 148 ) 47 We proceed under the assumption of a power series in γ 0 h (cid:96) − u (cid:96) = γ 0 h (cid:96) , 1 + γ 20 h (cid:96) , 2 + . . . z (cid:96) − r (cid:96) = γ 0 z (cid:96) , 1 + γ 20 z (cid:96) , 2 + . . . Φ (cid:96) − Φ (cid:96) , 0 = γ 0 Φ (cid:96) , 1 + γ 20 Φ (cid:96) , 2 + . . . G (cid:96) − G (cid:96) , 0 = γ 0 G (cid:96) , 1 + γ 20 G (cid:96) , 2 + . . . C (cid:96) − C (cid:96) , 0 = γ 0 C (cid:96) , 1 + γ 20 C (cid:96) , 2 + . . . D (cid:96) − D (cid:96) , 0 = γ 0 D (cid:96) , 1 + γ 20 D (cid:96) , 2 + . . . ( 149 ) Expanding both sides of the implicit equation for z (cid:96) we have γ 0 z (cid:96) , 1 + γ 20 z (cid:96) , 2 + . . . = γ 0 D (cid:96) , 0 φ ( u (cid:96) ) + γ 20 (cid:104) D (cid:96) , 0 ˙ φ ( u ) (cid:12) h (cid:96) , 1 + D (cid:96) , 1 φ ( u ) (cid:105) + γ 30 (cid:104) D (cid:96) , 0 ˙ φ ( u ) (cid:12) h (cid:96) , 2 + D (cid:96) , 0 ¨ φ ( u ) (cid:12) [ h (cid:96) , 1 ] 2 + D (cid:96) , 1 ˙ φ ( u ) (cid:12) h (cid:96) , 1 + D (cid:96) , 2 φ ( u ) (cid:105) + O ( γ 40 ) ( 150 ) Performing a similar exercise for h (cid:96) , we get the following ﬁrst three leading terms for z (cid:96) , h (cid:96) , we ﬁnd z (cid:96) , 1 = D (cid:96) , 0 φ ( u ) z (cid:96) , 2 = D (cid:96) , 0 ˙ φ ( u ) (cid:12) h (cid:96) , 1 + D (cid:96) , 1 φ ( u ) z (cid:96) , 3 = D (cid:96) , 0 (cid:20) 1 2 ¨ φ ( u ) (cid:12) [ h (cid:96) , 1 ] 2 + ˙ φ ( u ) (cid:12) h (cid:96) , 2 (cid:21) + D (cid:96) , 1 [ ˙ φ ( u ) (cid:12) h (cid:96) , 1 ] + D (cid:96) , 2 φ ( u ) h (cid:96) , 1 = C (cid:96) , 0 g (cid:96) , 0 = C (cid:96) , 0 [ ˙ φ ( u ) (cid:12) r ] h (cid:96) , 2 = C (cid:96) , 1 g (cid:96) , 1 + C (cid:96) , 0 g (cid:96) , 2 = C (cid:96) , 0 (cid:104) ˙ φ ( u ) z (cid:96) , 1 + ¨ φ ( u ) h (cid:96) , 1 r (cid:105) + C (cid:96) , 1 (cid:20) ˙ φ ( u ) z (cid:96) , 2 + ¨ φ ( u ) h (cid:96) , 1 z (cid:96) , 1 + 1 2 . . . φ ( u ) [ h (cid:96) , 1 ] 2 r + ¨ φ ( u ) h (cid:96) , 2 r (cid:21) h (cid:96) , 3 = C (cid:96) , 0 g (cid:96) , 2 + C (cid:96) , 1 g (cid:96) , 1 + C (cid:96) , 2 g (cid:96) , 0 = C (cid:96) , 0 (cid:20) ˙ φ ( u ) z (cid:96) , 2 + ¨ φ ( u ) h (cid:96) , 1 z (cid:96) , 1 + ¨ φ ( u ) h (cid:96) , 2 r + 1 2 . . . φ ( u ) [ h (cid:96) , 1 ] 2 r (cid:21) + C (cid:96) , 1 (cid:104) ˙ φ ( u ) z (cid:96) , 1 + ¨ φ ( u ) h (cid:96) , 1 r (cid:105) + C (cid:96) , 2 (cid:104) ˙ φ ( u ) r (cid:105) ( 151 ) As will become apparent soon , it is crucially important to identify the dependence of each of these terms on r . We note that z (cid:96) , 1 does not depend on r and h (cid:96) , 1 is linear in r . In the next section , we use this fact to show that Φ (cid:96) , 1 = 0 and G (cid:96) , 1 = 0 . These conditions imply that C (cid:96) , 0 and D (cid:96) , 1 = 0 . As a consequence , z (cid:96) , 2 is linear in r and h (cid:96) , 2 only contains even powers of r . Lastly , this implies that z (cid:96) , 3 only contains even powers of r and h (cid:96) , 3 contains only odd powers of r . 48 P . 2 . 1 Leading Corrections to Φ 1 Kernel is O ( γ 20 ) We start in the ﬁrst layer where u 1 ∼ GP ( 0 , K x ⊗ 11 (cid:62) ) ( note that this is O γ 0 ( 1 ) ) and compute the expansion of Φ 1 in γ 0 Φ 1 = (cid:10) φ ( h 1 ) φ ( h 1 ) (cid:62) (cid:11) = (cid:10) φ ( u 1 ) φ ( u 1 ) (cid:62) (cid:11) + γ 0 (cid:68)(cid:104) ˙ φ ( u 1 ) h 1 , 1 (cid:105) φ ( u 1 ) (cid:62) (cid:69) + γ 0 (cid:28) φ ( u 1 ) (cid:104) ˙ φ ( u 1 ) h 1 , 1 (cid:105) (cid:62) (cid:29) + γ 20 (cid:28)(cid:104) ˙ φ ( u 1 ) h 1 , 1 (cid:105) (cid:104) ˙ φ ( u 1 ) h 1 , 1 (cid:105) (cid:62) (cid:29) + γ 20 2 (cid:68)(cid:104) ¨ φ ( u 1 ) h 1 , 2 (cid:105) φ ( u 1 ) (cid:69) + γ 20 2 (cid:68)(cid:104) ¨ φ ( u 1 ) h 1 , 2 (cid:105) φ ( u 1 ) (cid:69) + γ 30 (cid:28)(cid:20) ˙ φ ( u 1 ) h 1 , 3 + ¨ φ ( u ) h 1 , 1 h 1 , 2 + 1 6 . . . φ ( u ) ( h 1 , 1 ) 3 (cid:21) φ ( u ) (cid:62) (cid:29) + γ 30 (cid:42) φ ( u ) (cid:20) ˙ φ ( u 1 ) h 1 , 3 + ¨ φ ( u ) h 1 , 1 h 1 , 2 + 1 6 . . . φ ( u ) ( h 1 , 1 ) 3 (cid:21) (cid:62) (cid:43) + γ 30 (cid:28)(cid:20) ˙ φ ( u ) h 1 , 2 + 1 2 ¨ φ ( u ) ( h 1 , 1 ) 2 (cid:21) (cid:104) ˙ φ ( u ) h 1 , 1 (cid:105) (cid:62) (cid:29) + γ 30 (cid:42)(cid:104) ˙ φ ( u ) h 1 , 1 (cid:105) (cid:20) ˙ φ ( u ) h 1 , 2 + 1 2 ¨ φ ( u ) ( h 1 , 1 ) 2 (cid:21) (cid:62) (cid:43) + O ( γ 40 ) ( 152 ) where powers and multiplications of vectors are taken elementwise . Now , note that , as promised , the terms linear in γ 0 vanish since h 1 , 1 is linear the Gaussian random variable r 1 , which is a mean zero and independent of u 1 so an average like (cid:10) r 1 F ( u 1 ) (cid:11) = (cid:10) r 1 , 0 (cid:11) (cid:10) F ( u 1 ) (cid:11) = 0 must vanish for any function F . Thus we see that Φ (cid:96) ’s leading correction is O ( γ 20 ) . We also obtain , by a similar argument , that the cubic O ( γ 30 ) term vanishes . To see this , note that h 1 , 3 only contains odd powers of r 1 . Next , h 1 , 1 h 1 , 2 contains only odd powers of r , and ( h 1 , 1 ) 3 is cubic in r . Since all odd moments of a mean - zero Gaussian vanish , all averages of these terms over r annihilate , causing the γ 30 terms to vanish . Thus Φ 1 = Φ 1 , 0 + γ 20 Φ 1 , 2 + O ( γ 40 ) . P . 3 Forward Pass Induction for Φ (cid:96) We now assume the inductive hypothesis that for some (cid:96) ∈ { 1 , . . . , L − 1 } that Φ (cid:96) = Φ (cid:96) , 0 + γ 20 Φ (cid:96) , 2 + O ( γ 40 ) ( 153 ) and we will show that this will imply that the next layer must have a similar expansion Φ (cid:96) + 1 = Φ (cid:96) + 1 , 0 + γ 20 Φ (cid:96) + 1 , 2 + O ( γ 40 ) . First , we note that u (cid:96) + 1 ∼ GP ( 0 , Φ (cid:96) , 0 + γ 20 Φ (cid:96) , 2 + . . . ) . As before , we compute the leading terms in the expansion of Φ (cid:96) + 1 Φ (cid:96) + 1 = (cid:10) φ ( h (cid:96) + 1 ) φ ( h (cid:96) + 1 ) (cid:62) (cid:11) = (cid:10) φ ( u (cid:96) + 1 ) φ ( u (cid:96) + 1 ) (cid:11) + γ 20 (cid:28)(cid:104) ˙ φ ( u (cid:96) + 1 ) h (cid:96) + 1 , 1 (cid:105) (cid:104) ˙ φ ( u (cid:96) + 1 ) h (cid:96) + 1 , 1 (cid:105) (cid:62) (cid:29) + γ 20 2 (cid:68)(cid:104) ¨ φ ( u (cid:96) + 1 ) h (cid:96) + 1 , 2 (cid:105) φ ( u (cid:96) + 1 ) (cid:62) (cid:69) + γ 20 2 (cid:28) φ ( u (cid:96) + 1 ) (cid:104) ¨ φ ( u (cid:96) + 1 ) h (cid:96) + 1 , 2 (cid:105) (cid:62) (cid:29) + O ( γ 40 ) ( 154 ) where , as before the γ 0 and γ 30 terms vanish by the fact that odd moments of r (cid:96) + 1 vanish . Now , note that all averages are performed over u (cid:96) + 1 ∼ GP ( 0 , Φ (cid:96) , 0 + γ 20 Φ (cid:96) , 2 + . . . ) , which depends on the perturbed kernel of the previous layer . How can we calculate the contribution of the correction which is due to the previous layer’s kernel movement ? This can be obtained easily from the following identity . Let F ( u , r ) be an arbitrary observable which depends on Gaussian ﬁelds u and r which 49 have covariances Φ (cid:96) , 0 + γ 20 Φ (cid:96) , 2 + O ( γ 40 ) and G (cid:96) + 2 , 0 + γ 20 G (cid:96) + 2 , 2 + O ( γ 30 ) ( note this only requires that the linear in γ 0 terms of G vanish which is easy to verify ) . Then (cid:104) F ( u , r ) (cid:105) u , r = (cid:90) d k d u d v d r F ( u , r ) exp (cid:18) − 1 2 k (cid:62) [ Φ (cid:96) , 0 + γ 20 Φ (cid:96) , 2 + . . . ] k + i k · u (cid:19) exp (cid:18) − 1 2 v (cid:62) [ G (cid:96) + 2 , 0 + γ 20 G (cid:96) + 2 , 2 + . . . ] v + i v · r (cid:19) ( 155 ) ∼ (cid:104) F ( u , r ) (cid:105) u 0 r 0 + γ 20 2 Tr (cid:34) Φ (cid:96) − 1 , 2 (cid:28) ∂ 2 ∂ u ∂ u (cid:62) f ( u , r ) (cid:29) u 0 r 0 (cid:35) + γ 20 2 Tr (cid:34) G (cid:96) + 1 , 2 (cid:28) ∂ 2 ∂ r ∂ r (cid:62) f ( u , r ) (cid:29) u 0 r 0 (cid:35) + O ( γ 30 ) ( 156 ) where u 0 ∼ GP ( 0 , Φ (cid:96) , 0 ) , r 0 ∼ N ( 0 , G (cid:96) + 2 , 0 ) . Thus , the leading order behavior of Φ (cid:96) + 1 can easily be obtained in terms of averages over the original unperturbed covariances Φ (cid:96) + 1 = (cid:10) φ ( u 0 ) φ ( u 0 ) (cid:62) (cid:11) u 0 + γ 20 2 Tr (cid:34) Φ (cid:96) , 2 (cid:28) ∂ 2 ∂ u 0 ∂ u (cid:62) 0 φ ( u 0 ) φ ( u 0 ) (cid:62) (cid:29) u 0 (cid:35) + γ 20 2 ∂ 2 ∂γ 20 | γ 0 = 0 (cid:104) φ ( h ( u 0 , r 0 , γ 0 ) ) φ ( h ( u 0 , r 0 , γ 0 ) ) (cid:105) u 0 , r 0 + O ( γ 40 ) , ( 157 ) where the trace is taken against the Hessian indices and the indices on Φ (cid:96) , 2 . This gives us the desired result by induction that for all (cid:96) ∈ { 1 , . . . , L } , we have Φ (cid:96) = Φ (cid:96) , 0 + γ 20 Φ (cid:96) , 2 + O ( γ 40 ) . We see that Φ (cid:96) accumulates corrections from the previous layers’ corrections through the forward pass recursion . P . 4 Leading Corrections to G L Kernel is O ( γ 20 ) The analogous argument for G L now can be provided . First note that r L is independent of u L and of γ 0 . Thus we can ﬁnd that G L has no linear - in - γ 0 term in its expansion since G L , 1 = (cid:68) [ ˙ φ ( u L ) r L ] (cid:104) ˙ φ ( u L ) z L , 1 + ¨ φ ( u L ) h L , 1 r L (cid:105)(cid:69) + (cid:68) [ ˙ φ ( u L ) r L ] (cid:104) ˙ φ ( u L ) z L , 1 + ¨ φ ( u L ) h L , 1 r L (cid:105)(cid:69) = 0 each term contains only odd powers of r L and odd moments of Gaussian variables vanish . After much more work , one can verify that G L , 3 also must vanish since all terms contain odd powers of r . G L , 3 = (cid:10) g L , 3 g L , 0 (cid:62) (cid:11) + (cid:10) g L , 0 g L , 3 (cid:62) (cid:11) + (cid:10) g L , 2 g L , 1 (cid:62) (cid:11) + (cid:10) g L , 1 g L , 2 (cid:62) (cid:11) ( 158 ) First , note that g L , 0 is linear in r . Next , note that g L , 1 only depends on even powers of r since g L , 1 = ˙ φ ( u ) z L , 1 + ¨ φ ( u ) h L , 1 r . Next , we have g L , 2 = ˙ φ ( u ) z L , 2 + ¨ φ ( u ) [ h L , 2 r + h L , 1 z L , 1 ] + 1 2 . . . φ ( u ) [ h L , 1 ] 2 ( 159 ) which only depends on odd powers of r . Lastly , we have g L , 3 g L , 3 = ˙ φ ( u ) z L , 3 + ¨ φ ( u ) [ h L , 3 r + h L , 2 z L , 1 + h L , 1 z L , 2 ] + 1 2 . . . φ ( u ) [ 2 h L , 1 h L , 2 r + [ h L , 1 ] 2 z L , 1 ] + 1 6 φ ( 4 ) ( u ) [ h L , 1 ] 3 r ( 160 ) which we see only contains even powers of r . Thus g L , 3 g L , 0 will be odd in r . Looking at the expansion for G L , 3 , we see that all terms are odd in r and so the averages vanish under the Gaussian integrals . 50 P . 5 Backward Pass Recursion for G (cid:96) We can derive a similar recursion on the backward pass for G (cid:96) ’s leading order corrections . Using the same idea from the previous section , we ﬁnd the following expressions G (cid:96) = (cid:28)(cid:104) ˙ φ ( u 0 ) r 0 (cid:105) (cid:104) ˙ φ ( u 0 ) r 0 (cid:105) (cid:62) (cid:29) u 0 , r 0 + γ 20 2 (cid:68) ˙ φ ( u 0 ) ˙ φ ( u 0 ) (cid:69) u 0 (cid:12) G (cid:96) + 1 , 2 + γ 20 2 ∂ 2 ∂γ 20 | γ 0 = 0 (cid:28)(cid:104) ˙ φ ( h ( u 0 , r 0 , γ 0 ) ) r 0 (cid:105) (cid:104) ˙ φ ( h ( u 0 , r 0 , γ 0 ) ) r 0 (cid:105) (cid:62) (cid:29) u 0 , r 0 + O ( γ 40 ) This time , we see that G (cid:96) accumulates corrections from succeeding layers through the backward pass recursion . P . 6 Form of the Leading Corrections We can expand the h (cid:96) and z (cid:96) ﬁelds around u (cid:96) , 0 , r (cid:96) , 0 to ﬁnd the leading order corrections to each feature kernel Φ (cid:96) , 2 = 1 2 ∂ 2 ∂γ 2 0 | γ 0 = 0 (cid:10) φ ( h (cid:96) ( u 0 , r 0 , γ 0 ) ) φ ( h (cid:96) ( u 0 , r 0 , γ 0 ) ) (cid:62) (cid:11) u 0 , r 0 + 1 2 Tr (cid:34) Φ (cid:96) − 1 , 2 (cid:28) ∂ 2 ∂ u 0 ∂ u (cid:62) 0 (cid:2) φ ( u 0 ) φ ( u 0 ) (cid:62) (cid:3)(cid:29) u 0 (cid:35) ( 161 ) The ﬁrst term requires additional expansion to extract the corrections in γ 20 φ ( u + γ 0 C (cid:96) g (cid:96) ) ∼ φ ( u ) + γ 0 ˙ φ ( u ) (cid:12) [ C (cid:96) g (cid:96) ] + γ 20 2 ¨ φ ( u ) (cid:12) [ C (cid:96) g (cid:96) ] 2 ∼ φ ( u ) + γ 0 ˙ φ ( u ) (cid:12) [ C (cid:96) , 0 g (cid:96) , 0 ] + γ 20 ˙ φ ( u ) (cid:12) [ C (cid:96) , 0 g (cid:96) , 1 ] + γ 20 2 ¨ φ ( u ) (cid:12) [ C (cid:96) , 0 g (cid:96) , 0 ] 2 ˙ φ ( h (cid:96) ) (cid:12) z (cid:96) ∼ ˙ φ ( u ) (cid:12) r + γ 0 ¨ φ ( u ) (cid:12) [ C (cid:96) , 0 g (cid:96) , 0 ] (cid:12) r + γ 0 ˙ φ ( u ) (cid:12) [ D (cid:96) , 0 φ ( u ) ] + O ( γ 20 ) C (cid:96) , 0 µα ( t , s ) = A (cid:96) − 1 , 1 µα ( t , s ) + Θ ( t − s ) ∆ 0 α ( s ) Φ (cid:96) − 1 , 0 µα ( t , s ) D (cid:96) , 0 µα ( t , s ) = B (cid:96) , 1 µα ( t , s ) + Θ ( t − s ) ∆ 0 α ( s ) Φ (cid:96) − 1 , 0 µα ( t , s ) ( 162 ) where we used the fact that C (cid:96) , 1 = 0 which follows from the fact that Φ (cid:96) − 1 , 1 = 0 , and ∆ (cid:96) , 1 = 0 . Now , expanding out term by term Φ (cid:96) = Φ (cid:96) , 0 + γ 20 (cid:68) [ ˙ φ ( u ) (cid:12) ( C (cid:96) , 0 g (cid:96) , 0 ) ] [ ˙ φ ( u ) (cid:12) ( C (cid:96) , 0 g (cid:96) , 0 ) ] (cid:62) (cid:69) + γ 20 (cid:68)(cid:104) ˙ φ ( u ) (cid:12) ( C (cid:96) , 0 [ ¨ φ ( u ) (cid:12) [ C (cid:96) , 0 g (cid:96) , 0 ] (cid:12) r ] ) (cid:105) φ ( u ) (cid:62) (cid:69) + transpose + γ 20 (cid:68)(cid:104) ˙ φ ( u ) (cid:12) ( C (cid:96) , 0 [ ˙ φ ( u ) (cid:12) [ D (cid:96) , 0 φ ( u ) ] ] ) (cid:105) φ ( u ) (cid:62) (cid:69) + transpose + γ 20 2 (cid:68)(cid:104) ¨ φ ( u ) (cid:12) [ C (cid:96) , 0 g (cid:96) , 0 ] 2 (cid:105) φ ( u ) (cid:62) (cid:69) + transpose + γ 20 2 Tr (cid:34) Φ (cid:96) − 1 , 2 (cid:28) ∂ 2 ∂ u ∂ u (cid:62) (cid:2) φ ( u ) φ ( u ) (cid:62) (cid:3)(cid:29) u ∼GP ( 0 , Φ (cid:96) − 1 , 0 ) (cid:35) + O ( γ 40 ) ( 163 ) We see that the corrections for the Φ (cid:96) kernels accumulate on the forward pass through the ﬁnal term so Φ (cid:96) , 2 ∼ O ( (cid:96) ) . Now we will perform the same analysis for G (cid:96) . G (cid:96) = (cid:10) g (cid:96) ( u , r ) g (cid:96) ( u , r ) (cid:62) (cid:11) u ∼GP ( 0 , Φ (cid:96) − 1 , 0 ) r ∼GP ( 0 , G (cid:96) + 1 , 0 ) + γ 20 2 Tr (cid:34) G (cid:96) + 1 , 2 (cid:28) ∂ 2 ∂ r ∂ r (cid:62) (cid:104) ( ˙ φ ( u ) (cid:12) r ) ( ˙ φ ( u ) (cid:12) r ) (cid:62) (cid:105)(cid:29) u ∼GP ( 0 , Φ (cid:96) − 1 , 0 ) r ∼GP ( 0 , G (cid:96) + 1 , 0 ) (cid:35) + O ( γ 4 0 ) = (cid:10) g (cid:96) ( u , r ) g (cid:96) ( u , r ) (cid:62) (cid:11) u ∼GP ( 0 , Φ (cid:96) − 1 , 0 ) r ∼GP ( 0 , G (cid:96) + 1 , 0 ) + γ 20 2 G (cid:96) + 1 , 2 (cid:12) (cid:68) ˙ φ ( u ) ˙ φ ( u ) (cid:69) u ∼GP ( 0 , Φ (cid:96) − 1 , 0 ) + O ( γ 40 ) ( 164 ) 51 We see that , through the second term , the G (cid:96) kernels accumulate on the backward pass so that G (cid:96) , 2 ∼ O ( L + 1 − (cid:96) ) . As before the difﬁcult term is the ﬁrst expression which requires a full expansion of g (cid:96) to second order g (cid:96) ∼ ˙ φ ( u ) (cid:12) r + γ 0 ˙ φ ( u ) (cid:12) [ D (cid:96) , 0 φ ( u ) + γ 0 D (cid:96) , 0 ˙ φ ( u ) C (cid:96) , 0 g (cid:96) , 0 ] + γ 0 ¨ φ ( u ) [ C (cid:96) , 0 g (cid:96) , 0 + γ 0 C (cid:96) , 0 g (cid:96) , 1 ] (cid:12) r ( 165 ) From these terms we ﬁnd G (cid:96) = G (cid:96) , 0 + γ 20 (cid:68) [ ˙ φ ( u ) (cid:12) ( D (cid:96) , 0 φ ( u ) ) ] [ ˙ φ ( u ) (cid:12) ( D (cid:96) , 0 φ ( u ) ) ] (cid:62) (cid:69) + γ 20 (cid:68) [ ¨ φ ( u ) ( C (cid:96) , 0 g (cid:96) , 0 ) ] [ ¨ φ ( u ) ( C (cid:96) , 0 g (cid:96) , 0 ) ] (cid:62) (cid:69) + γ 20 (cid:68)(cid:104) ˙ φ ( u ) (cid:12) (cid:16) D (cid:96) , 0 ˙ φ ( u ) C (cid:96) , 0 g (cid:96) , 0 (cid:17)(cid:105) g (cid:96) , 0 (cid:69) + transpose + γ 20 (cid:68) [ ¨ φ ( u ) (cid:12) C (cid:96) , 0 ( ¨ φ ( u ) (cid:12) C (cid:96) , 0 g (cid:96) , 0 ) ] g (cid:96) , 0 (cid:69) + transpose + γ 20 2 G (cid:96) + 1 , 2 (cid:12) (cid:68) ˙ φ ( u ) ˙ φ ( u ) (cid:69) u ∼GP ( 0 , Φ (cid:96) − 1 , 0 ) + O ( γ 40 ) ( 166 ) Now the correction to the NTK has the form K NTK , 2 = Φ L , 2 + L − 1 (cid:88) (cid:96) = 1 G (cid:96) , 0 Φ (cid:96) , 2 + L − 1 (cid:88) (cid:96) = 1 G (cid:96) , 2 Φ (cid:96) , 0 + G 1 , 2 (cid:12) ( K x ⊗ 11 (cid:62) ) ( 167 ) Since each Φ (cid:96) , 2 , G L + 1 − (cid:96) , 2 ∼ O ( (cid:96) ) , each of the two sums from (cid:96) ∈ { 1 , . . . , L − 1 } gives a depth scaling of the form ∼ (cid:80) L − 1 (cid:96) = 1 (cid:96) = L ( L − 1 ) 2 . Since the original NTK has scale K NTK , 0 ∼ O ( L ) , the relative change in the kernel is | K 2 | | K 0 | = O ( γ 20 L ) . In a ﬁnite width N , network , our deﬁnition γ = γ 0 √ N would indicate that a width N network would have corrections of scale γ 20 L = γ 2 LN in the NTK regime where γ = O N ( 1 ) provided the network is sufﬁciently wide to disregard initialization dependent ﬂuctuations in the kernels . P . 7 Perturbation Theory in Width N ( Finite Size Corrections ) Finite size corrections to the DMFT can also be obtained within our ﬁeld theoretic framework . Let k = Vec { Φ (cid:96) , ˆ Φ (cid:96) , G (cid:96) , ˆ G (cid:96) , A (cid:96) , B (cid:96) } denote the collection of kernel order parameters of the DMFT . To simplify the subsequent discussion , we redeﬁne the DMFT action to be its negation S → − S . The DMFT action S [ k ] deﬁnes a Gibbs measure over order parameters k , where observables O ( k ) have averages which can be computed as (cid:104) O ( k ) (cid:105) = (cid:82) d k exp ( − NS [ k ] ) O ( k ) (cid:82) d k exp ( − NS [ k ] ) ( 168 ) The inﬁnite - width DMFT is characterized by the set of saddle point equations which are ∇ k S [ k ] | k = k ∗ = 0 . Let the saddle point be k ∗ . To identify corrections to the observable aver - age (cid:104) O ( k ) (cid:105) due to ﬁnite size , we now Taylor expand S around k ∗ S [ k ] = S [ k ∗ ] + 1 2 ( k − k ∗ ) ∇ 2 k S [ k ] | k = k ∗ ( k − k ∗ ) + 1 6 (cid:88) ijl ( k i − k ∗ i ) ( k j − k ∗ j ) ( k l − k ∗ l ) ∂ 3 S ∂k i ∂k j ∂k l + . . . ( 169 ) The linear component vanishes at the saddle point since ∇ k S [ k ] | k = k ∗ = 0 . Our observable average is thus (cid:104) O ( k ) (cid:105) = (cid:82) d k exp (cid:0) − N 2 ( k − k ∗ ) ∇ 2 S [ k ∗ ] ( k − k ∗ ) + . . . (cid:1) O ( k ) (cid:82) d k exp (cid:0) − N 2 ( k − k ∗ ) ∇ 2 S [ k ∗ ] ( k − k ∗ ) + . . . (cid:1) ( 170 ) = (cid:82) d δ exp (cid:0) − 12 δ ∇ 2 S [ k ∗ ] δ − U ( δ ) (cid:1) O ( k ∗ + N − 1 / 2 δ ) (cid:82) d δ exp (cid:0) − 12 δ ∇ 2 S [ k ∗ ] δ − U ( δ ) (cid:1) ( 171 ) 52 where we made the change of variables δ = √ N ( k − k ∗ ) . The function U ( δ ) contains all higher order terms ( cubic and higher ) in the Taylor expansion of NS [ k ] . Since the leading power in U is cubic in ( k − k ∗ ) = N − 1 / 2 δ , the leading behavior of this remainder is U = O ( N − 1 / 2 ) so it can be regarded as a perturbation to the Gibbs distribution . Taylor expanding the exponential exp (cid:0) − 12 δ ∇ 2 S [ k ∗ ] δ − U ( δ ) (cid:1) = exp (cid:0) − 12 δ ∇ 2 S [ k ∗ ] δ (cid:1) [ 1 − U + 12 U 2 + . . . ] in both numerator and denominator , we eliminate the presence of the higher order terms in the Gibbs measure . Lastly , we let (cid:104)·(cid:105) 0 represent an average over the unperturbed Gaussian potential δ ∼ N ( 0 , [ ∇ 2 S [ k ∗ ] ] − 1 ) . For notational simplicity , we let (cid:15) = N − 1 / 2 and obtain (cid:104) O ( k ) (cid:105) = (cid:104) O ( k ∗ + (cid:15) δ ) (cid:105) 0 − (cid:104) O ( k ∗ + (cid:15) δ ) U ( δ ) (cid:105) + 12 (cid:10) O ( k ∗ + (cid:15) δ ) U ( δ ) 2 (cid:11) 0 + . . . 1 − (cid:104) U ( δ ) (cid:105) 0 + 12 (cid:104) U ( δ ) 2 (cid:105) 0 + . . . = (cid:104) O ( k ∗ + (cid:15) δ ) (cid:105) 0 − [ (cid:104) O ( k ∗ + (cid:15) δ ) U ( δ ) (cid:105) − (cid:104) O ( k ∗ + (cid:15) δ ) (cid:105) (cid:104) U ( δ ) (cid:105) ] + 1 2 (cid:2)(cid:10) O ( k ∗ + (cid:15) δ ) U ( δ ) 2 (cid:11) − (cid:104) O ( k ∗ + (cid:15) δ ) (cid:105) (cid:10) U ( δ ) 2 (cid:11)(cid:3) − (cid:104) (cid:104) O ( k ∗ + (cid:15) δ ) U ( δ ) (cid:105) (cid:104) U ( δ ) (cid:105) 0 − (cid:104) O ( k ∗ + (cid:15) δ ) (cid:105) (cid:104) U ( δ ) (cid:105) 20 (cid:105) + . . . = ∞ (cid:88) n = 0 ( − 1 ) n n ! (cid:104) O ( k ∗ + (cid:15) δ ) U ( δ ) n (cid:105) c 0 ( 172 ) where (cid:104)(cid:105) c 0 represents a connected cumulant [ 96 ] . The ﬁrst two connected correlations have the form (cid:104) OU (cid:105) c 0 = (cid:104) OU (cid:105) 0 − (cid:104) O (cid:105) 0 (cid:104) U (cid:105) 0 (cid:10) OU 2 (cid:11) c 0 = (cid:10) OU 2 (cid:11) 0 − 2 (cid:104) OU (cid:105) 0 (cid:104) U (cid:105) 0 − (cid:104) O (cid:105) 0 (cid:10) U 2 (cid:11) 0 + 2 (cid:104) O (cid:105) 0 (cid:104) U (cid:105) 20 ( 173 ) If one is interested only in the leading order correction to the observable (cid:104) O ( k ) (cid:105) , this can be obtained with the following correction (cid:104) O ( k ) (cid:105) = O ( k ∗ ) + 1 2 N Tr (cid:2) ∇ 2 S [ k ∗ ] (cid:3) − 1 ∇ 2 k O ( k ∗ ) + 1 √ N ∇ k O ( k ∗ ) · (cid:104) δ U ( δ ) (cid:105) 0 + O ( N − 2 ) ( 174 ) Since U = O ( N − 1 / 2 ) both corrections are of order 1 / N . This analysis shows that the leading order correction of the kernel distributions is O ( N − 1 ) and can be approximated by performing averages over a Gaussian distribution for k determined by the saddle point solution k ∗ and covariance given by 1 N (cid:2) ∇ 2 k S [ k ] | k = k ∗ (cid:3) − 1 . We derive expressions for the components of this Hessian in P . 7 . 1 . These ﬂuctuations have standard deviation O ( N − 1 / 2 ) . This technique is a common approach to identifying ﬁnite size effects [ 56 ] and was recently employed in Bayesian inference setting for networks in the lazy regime [ 29 ] . Before computing the Hessian terms , we can compare ﬁnite size effects under NTK scaling γ = O N ( 1 ) , and the mean ﬁeld scaling γ = O ( √ N ) . Concretely , we are interested in the feature learning component of the kernel change which is O ( γ 2 N ) . Let (cid:104) ∆ k (cid:105) represent the change in the kernel through training , which we showed in P . 2 is of size O ( γ 20 ) = O (cid:16) γ 2 N (cid:17) . We will now deﬁne the signal to noise ratio of feature learning as SNR = (cid:104) ∆ k (cid:105) (cid:112) Var ( k ) = O (cid:18) γ 2 √ N (cid:19) ( 175 ) For NTK regime , this is vanishing as N → ∞ , while for the DMFT regime , this is goes as O ( √ N ) since kernel evolution is always O ( 1 ) but variance is O ( N − 1 ) . P . 7 . 1 DMFT Action Hessian We now compute the various blocks of the Hessian of the DMFT action −∇ 2 k S [ k ∗ ] . The various necessary derivatives are given below . We will utilize the the factorization of the single site MGF (cid:80) (cid:96) ln Z (cid:96) to simplify many of the expressions . Below we provide exhaustive expressions for each 53 of the relevant terms . These expressions are included for completeness but we did not yet attempt computing all of them as we did the saddle point equations which deﬁne k ∗ . First , we list the collection of Hessian terms which do not involve A , B below . − ∂ 2 S ∂ ˆΦ (cid:96)µν ( t , s ) ∂ ˆΦ (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = δ (cid:96)(cid:96) (cid:48) (cid:2)(cid:10) φ ( h (cid:96)µ ( t ) ) φ ( h (cid:96)ν ( s ) ) φ ( h (cid:96)α ( t (cid:48) ) ) φ ( h (cid:96)β ( s (cid:48) ) ) (cid:11) − Φ (cid:96)µν ( t , s ) Φ (cid:96)αβ ( t (cid:48) , s (cid:48) ) (cid:3) − ∂ 2 S ∂ Φ (cid:96)µν ( t , s ) ∂ Φ (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = 0 − ∂ 2 S ∂ ˆΦ (cid:96)µν ( t , s ) ∂ Φ (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = − δ (cid:96)(cid:96) (cid:48) δ µα δ νβ δ ( t − t (cid:48) ) δ ( s − s (cid:48) ) + ∂ ∂ Φ (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) Φ (cid:96)µν ( t , s ) − ∂ 2 S ∂ ˆ G (cid:96)µν ( t , s ) ∂ ˆ G (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = δ (cid:96) , (cid:96) (cid:48) (cid:2)(cid:10) g (cid:96)µ ( t ) g (cid:96)ν ( s ) g (cid:96)α ( t (cid:48) ) g (cid:96)β ( s (cid:48) ) (cid:11) − G (cid:96)µν ( t , s ) G (cid:96)αβ ( t (cid:48) , s (cid:48) ) (cid:3) − ∂ 2 S ∂G (cid:96)µν ( t , s ) ∂G (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = 0 − ∂ 2 S ∂ ˆ G (cid:96)µν ( t , s ) ∂G (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = − δ (cid:96)(cid:96) (cid:48) δ µα δ νβ δ ( t − t (cid:48) ) δ ( s − s (cid:48) ) + ∂ ∂G (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) G (cid:96)µν ( t , s ) − ∂ 2 S ∂ ˆΦ (cid:96)µν ( t , s ) ∂ ˆ G (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = δ (cid:96) , (cid:96) (cid:48) (cid:2)(cid:10) φ ( h (cid:96)µ ( t ) ) φ ( h (cid:96)ν ( s ) ) g (cid:96)α ( t (cid:48) ) g (cid:96)β ( s (cid:48) ) (cid:11) − Φ (cid:96)µν ( t , s ) G (cid:96)αβ ( t (cid:48) , s (cid:48) ) (cid:3) − ∂ 2 S ∂ ˆΦ (cid:96)µν ( t , s ) ∂G (cid:96)αβ ( t (cid:48) , s (cid:48) ) = ∂ ∂G (cid:96)αβ ( t (cid:48) , s (cid:48) ) Φ (cid:96)µν ( t , s ) − ∂ 2 S ∂ ˆ G (cid:96)µν ( t , s ) ∂ Φ (cid:96)αβ ( t (cid:48) , s (cid:48) ) = ∂ ∂ Φ (cid:96)αβ ( t (cid:48) , s (cid:48) ) G (cid:96)µν ( t , s ) − ∂ 2 S ∂G (cid:96)µν ( t , s ) ∂ Φ (cid:96)αβ ( t (cid:48) , s (cid:48) ) = 0 54 Now for the terms involving A (cid:96) , B (cid:96) , we ﬁnd the following expressions . − ∂ 2 S ∂A (cid:96)µν ( t , s ) ∂A (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = (cid:42) ∂ 2 ∂u (cid:96) + 1 ν ( s ) ∂u (cid:96) (cid:48) + 1 β ( s (cid:48) ) [ g (cid:96) + 1 µ ( t ) g (cid:96) (cid:48) + 1 α ( t (cid:48) ) ] (cid:43) − B (cid:96)µν ( t , s ) B (cid:96)αβ ( t (cid:48) , s (cid:48) ) − ∂ 2 S ∂B (cid:96)µν ( t , s ) ∂B (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = (cid:42) ∂ 2 ∂r (cid:96)ν ( s ) ∂r (cid:96) (cid:48) β ( s (cid:48) ) [ φ ( h (cid:96)µ ( t ) ) φ ( h (cid:96) (cid:48) α ( t (cid:48) ) ) ] (cid:43) − ∂ 2 S ∂A (cid:96)µν ( t , s ) ∂B (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = δ (cid:96)(cid:96) (cid:48) δ µα δ νβ δ ( t − t (cid:48) ) δ ( s − s (cid:48) ) + (cid:42) ∂ 2 ∂u (cid:96) + 1 ν ( s ) ∂r (cid:96) (cid:48) β ( s (cid:48) ) [ g (cid:96) + 1 µ ( t ) φ ( h (cid:96) (cid:48) α ( s ) ) ] (cid:43) − B (cid:96)µν ( t , s ) A (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) − ∂ 2 S ∂A (cid:96)µν ( t , s ) ∂ Φ (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = δ (cid:96) , (cid:96) (cid:48) (cid:42) ∂ 3 ∂u (cid:96) + 1 ν ( t ) ∂u (cid:96) + 1 α ( t (cid:48) ) ∂u (cid:96) + 1 β ( s (cid:48) ) g (cid:96) + 1 µ ( t ) (cid:43) − ∂ 2 S ∂A (cid:96)µν ( t , s ) ∂ ˆ Φ (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = δ (cid:96) + 1 , (cid:96) (cid:48) (cid:20)(cid:28) ∂ ∂u (cid:96) + 1 ν ( t ) [ g (cid:96) + 1 µ ( t ) φ ( h (cid:96) + 1 α ( t (cid:48) ) ) φ ( h (cid:96) + 1 β ( s (cid:48) ) ) ] (cid:29) − B (cid:96)µν ( t , s ) Φ (cid:96) + 1 αβ ( t (cid:48) , s (cid:48) ) (cid:21) − ∂ 2 S ∂A (cid:96)µν ( t , s ) ∂G (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = δ (cid:96) + 2 , (cid:96) (cid:48) (cid:42) ∂ 3 ∂u (cid:96) + 1 ν ( t ) ∂r (cid:96) + 1 α ( t (cid:48) ) ∂r (cid:96) + 1 β ( s (cid:48) ) g (cid:96) + 1 µ ( t ) (cid:43) − ∂ 2 S ∂A (cid:96)µν ( t , s ) ∂ ˆ G (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = δ (cid:96) + 1 , (cid:96) (cid:48) (cid:20)(cid:28) ∂ ∂u (cid:96) + 1 ν ( t ) [ g (cid:96) + 1 µ ( t ) g (cid:96) + 1 α ( t (cid:48) ) g (cid:96) + 1 β ( s (cid:48) ) ] (cid:29) − B (cid:96)µν ( t , s ) G (cid:96) + 1 αβ ( t (cid:48) , s (cid:48) ) (cid:21) − ∂ 2 S ∂B (cid:96)µν ( t , s ) ∂ Φ (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = δ (cid:96) − 1 , (cid:96) (cid:48) (cid:42) ∂ 3 ∂r (cid:96)ν ( t ) ∂u (cid:96)α ( t (cid:48) ) ∂u (cid:96)β ( s (cid:48) ) φ ( h (cid:96)µ ( t ) ) (cid:43) − ∂ 2 S ∂B (cid:96)µν ( t , s ) ∂ ˆΦ (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = δ (cid:96) , (cid:96) (cid:48) (cid:20)(cid:28) ∂ ∂r (cid:96)ν ( t ) [ φ ( h (cid:96)µ ( t ) ) φ ( h (cid:96)α ( t (cid:48) ) ) φ ( h (cid:96)β ( s (cid:48) ) ) ] (cid:29) − A (cid:96)µν ( t , s ) Φ (cid:96) ( t (cid:48) , s (cid:48) ) (cid:21) − ∂ 2 S ∂B (cid:96)µν ( t , s ) ∂G (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = δ (cid:96) + 1 , (cid:96) (cid:48) (cid:42) ∂ 3 ∂r (cid:96)ν ( t ) ∂r (cid:96)α ( t (cid:48) ) ∂r (cid:96)β ( s (cid:48) ) φ ( h (cid:96)µ ( t ) ) (cid:43) − ∂ 2 S ∂B (cid:96)µν ( t , s ) ∂ ˆ G (cid:96) (cid:48) αβ ( t (cid:48) , s (cid:48) ) = δ (cid:96) , (cid:96) (cid:48) (cid:20)(cid:28) ∂ ∂r (cid:96)ν ( t ) [ φ ( h (cid:96)µ ( t ) ) g (cid:96)α ( t (cid:48) ) g (cid:96)β ( s (cid:48) ) ] (cid:29) − A (cid:96)µν ( t , s ) G (cid:96)αβ ( t (cid:48) , s (cid:48) ) (cid:21) From these block matrices which comprise the Hessian , we can obtain the ﬁnite width covari - ance structure in our order parameters k = Vec { Φ (cid:96) , G (cid:96) , A (cid:96) , B (cid:96) } by computing the inverse C = (cid:0) −∇ 2 k S [ k ∗ ] (cid:1) − 1 . In this approximation scheme , we have k ∼ N ( k ∗ , 1 N (cid:0) −∇ 2 k S [ k ∗ ] (cid:1) − 1 ) . Many questions about this expansion remain including : can it be proven that these O ( N − 1 / 2 ) ﬂuctuations always lead to higher expected test loss ? 55