Inspiration through Observation : Demonstrating the Inﬂuence of Automatically Generated Text on Creative Writing Melissa Roemmele Language Weaver ( RWS Group ) Los Angeles , CA , USA mroemmele @ sdl . com Abstract Getting machines to generate text perceived as creative is a long - pursued goal . A growing body of research directs this goal towards augmenting the creative writing abilities of hu - man authors . In this paper , we pursue this objective by an - alyzing how observing examples of automatically generated text inﬂuences writing . In particular , we examine a task re - ferred to as sentence inﬁlling , which involves transforming a list of words into a complete sentence . We emphasize “sto - riability” as a desirable feature of the resulting sentences , where “storiable” sentences are those that suggest a story a reader would be curious to hear about . Both humans and an automated system ( based on a neural language model ) per - formed this sentence inﬁlling task . In one setting , people wrote sentences on their own ; in a different setting , people observed the sentences produced by the model while writing their own sentences . Readers then assigned storiability pref - erences to the resulting sentences in a subsequent evaluation . We ﬁnd that human - authored sentences were judged as more storiable when authors observed the generated examples , and that storiability increased as authors derived more semantic content from the examples . This result gives evidence of an “inspiration through observation” paradigm for human - computer collaborative writing , through which human writ - ing can be enhanced by text generation models without di - rectly copying their output . 1 Introduction Creative text generation is a signiﬁcant focal point at the in - tersection between computational creativity and natural lan - guage processing research . The goal behind much of this re - search is to understand and simulate human creative writing abilities . There is also increasing interest in using this work to augment human creativity . This objective has become es - pecially visible given recent advancements in systems that can directly interface with human - authored text . Many existing creative text generation systems can be ap - plied to facilitate human authoring , even if they are not ex - plicitly presented in this way . The clarity of this use case can largely depend on how the system is evaluated . There is 1 All code associated with our model , dataset syn - thesis , and authoring experiments is available at github . com / roemmele / InSentive . The data resulting from the authoring experiments is also available upon request by contacting the authors . no standard design for such evaluations of beneﬁts to human authoring . Much work uses the convention of comparing generated output to human reference output for a given task , either by comparing the features of the text itself or compar - ing relative human judgments of it . Success by this standard is based on how well the system simulates human writing . One could theorize that the more a system writes like a hu - man , the more it will be able to help other humans write , but further empirical exploration of this is needed . Alter - natively , systems that explicitly aim to support human au - thoring are often evaluated in the context of interactive ap - plications where authors can elicit generated text . Here , the quality of the model can be evaluated according to authors’ interaction with the generated output . In this paper , we focus on an “inspiration through ob - servation” paradigm for human interaction with generated text . In many application settings for text generation , this human interaction is dynamic , with system output changing frequently in direct response to user choices . While discov - ering the best interaction paradigm is a critical objective of research on authoring support , here we minimize the role of user control over the generated text in order to assess the impact of merely observing the text . Authors see examples of generated text that fulﬁll a particular authoring objective , and they repeat the same task on their own . We compare human authoring outcomes in the absence and presence of these generated examples . This broad methodology could be applied to probe the ability of any system for aiding au - thoring , even systems that have not previously been assessed for this use case . Our exploration of this paradigm focuses on a particular authoring task , sentence inﬁlling , and a particular authoring objective , which we term storiability . Sentence inﬁlling in - volves expanding a list of words into a full sentence . In our version of this task , the sentences we elicit can be viewed as story excerpts . The construct of storiability is related to pre - viously discussed ideas such ‘storiness’ by Bailey ( 1999 ) , which pertains to the success of a story from a reader’s per - spective . We deﬁne storiability as the degree to which an ex - cerpt ( here , a single sentence ) alludes to an appealing story . Even though this is a broad deﬁnition , we operationalize it through speciﬁc instructions in our experiments . Through our experiments we ﬁnd that observing automatically gen - erated examples of our sentence inﬁlling task helps people a r X i v : 2107 . 04007v1 [ c s . C L ] 8 J u l 2021 better fulﬁll the storiability authoring objective . This pro - vides evidence for a general inspiration - through - observation framework by which generation systems can improve human authoring . Background As artiﬁcial intelligence has progressed , so has the develop - ment of Creativity Support Tools ( CSTs ) . CSTs are digital applications intended to augment human abilities in creative endeavors like visual and performance art , music , and writ - ing ( see Frich et al . ( 2019 ) for a review of several appli - cations ) . CSTs for writing in particular have been boosted by recent advances in natural language generation , making it possible for systems to interface with any unconstrained human - authored text . This includes ﬁgurative language like poetry ( Kantosalo , 2019 ) and metaphors ( Gero and Chilton , 2019 ) . Advances in story generation ( e . g . Fan , Lewis , and Dauphin , 2018 ; Martin , 2021 ) have been showcased by the increasing development of CSTs that support authoring in the narrative domain . One design pattern for these systems involves authors querying a generation model for a “sug - gestion” that can be integrated into their text ( Clark et al . , 2018 ; Khalifa , Barros , and Togelius , 2017 ; Manjavacas et al . , 2017 ; Roemmele and Gordon , 2018b ) . This enables analysis of what users choose to do with the generated text ( e . g . retaining or deleting it ) and how their choices are af - fected by the features of the text ( Akoury et al . , 2020 ; Roem - mele and Gordon , 2018a ; Clark and Smith , 2021 ) . Human - computer interaction studies have compared peo - ple’s writing with and without the use of AI - based tools , showing that these tools do change how people write . Exist - ing work has examined the effect of word and phrase predic - tions for content like image captions ( Arnold , Chauncey , and Gajos , 2020 ) , emails ( Buschek , Z ¨ urn , and Eiband , 2021 ) , and movie reviews ( Bhat , Agashe , and Joshi , 2021 ) . For more open - ended creative writing tasks , most research has focused on optimizing and assessing how much people favor the generated content . What is needed is more experimental comparison of how the use of CSTs changes the authoring outcome as perceived by readers . Mizrahi , Yardeni Seelig , and Shahaf ( 2020 ) recently pursued this for the speciﬁc task of creating neologisms ( i . e . new words ) . In their work , peo - ple wrote neologisms before and after observing automati - cally generated examples . The results showed that observ - ing these examples helped people produce better neologisms in terms of their perceived creativity . In this paper , we fol - low a similar approach to examine the intervening effect of generated examples for the sentence inﬁlling task . Sentence Inﬁlling We focus on the speciﬁc task of sentence inﬁlling to evaluate our hypotheses about authoring . Given a sequence of input words ( e . g . “he town rain” ) , which we refer to as a “prompt” , the inﬁlling task expands the sequence into a complete sen - tence by inserting additional words ( e . g . “he rode his bike to town in the pouring rain . ” ) . We created a dataset for this task and trained an automated model on it , as detailed below . Overview Text inﬁlling , alternatively known as expansion or elabora - tion , has recently attracted signiﬁcant attention for multiple types of corpora ( Donahue , Lee , and Liang , 2020 ; Fedus , Goodfellow , and Dai , 2018 ; Huang et al . , 2020 ; Shen et al . , 2020 ) . There are different conﬁgurations of this task based on the length of the text to be inﬁlled . For stories , some work has focused on inserting sentence - length sequences that con - nect passages ( Chandu , Dong , and Black , 2020 ; Ippolito et al . , 2019 ; Mori et al . , 2020 ) . A more constrained version of inﬁlling turns it into a cloze ( i . e . ﬁll - in - the - blank ) task where inﬁlled segments are single words or short phrases . Our inﬁlling model outputs a single sentence given a se - quence of words , but no assumptions are made about the number of words to inﬁll . This design is reﬂected in existing work applied to creative authoring support ( ¨Ozbal , Pighin , and Strapparava , 2013 ; Safovich and Azaria , 2020 ) , but it has yet to be examined how automatically inﬁlled sentences affect human performance of this task . Dataset We are not aware of any datasets that mirror the design of our particular inﬁlling task , by which sentences can be gen - erated from any arbitrary sequence of words . However , it is easy to simulate an inﬁlling dataset using existing cor - pora . Given that the task is framed in the context of story - telling , we obtained 10 , 000 English - language stories from a variety of genres in the BookCorpus ( Kobayashi , 2018 ) . We segmented each story into sentences 2 , ﬁltering sentences with less than ten words . To derive pairs of prompts and inﬁlled sentences , we randomly dropped between 60 - 100 % of words in each sentence . We required that the resulting ablated sentence consist of at least 50 % content words ( i . e . nouns , verbs , adjectives ) , since function words that con - vey little semantic meaning ( i . e . pronouns , prepositions , determiners ) are more frequent in text . The ablated sen - tences became the prompts used as the source inputs to the model , whereas the corresponding original sentences were the target inﬁlled outputs generated by the model . The mean number of words in the prompts and inﬁlled sentences was 4 . 86 and 19 . 19 , respectively . These pairs were divided into 34 , 172 , 128 training instances , 897 , 473 validation instances , and 894 , 484 test instances fully held - out during training . Model Design Our inﬁlling model 3 is a Transformer language model ( LM ) ( Vaswani et al . , 2017 ) , which is currently a popular architec - ture for many machine learning approaches to language gen - eration . Figure 1 broadly illustrates the model . Our scheme 2 All linguistic processing steps used to derive this dataset , in - cluding sentence segmentation , word tokenization , and part - of - speech tagging , were performed with the spaCy library : spacy . io 3 We used the Texar - PyTorch library for implementation : texar - pytorch . readthedocs . io . Additional hyperparameter settings in - cluded : maximum epochs = 100 , batch size = 32 , gradient accu - mulation over 8 steps , validation every 25 , 000 steps , early stopping after 25 consecutive rounds of no validation improvement , static learning rate = 0 . 001 , maximum gradient norm = 1 . 0 . for applying this architecture to inﬁlling is closely related to that described in Donahue , Lee , and Liang ( 2020 ) , with one main distinction . Their approach uses designated to - kens ( i . e . [ BLANK ] ) in the input sequences to indicate the position where text should be inﬁlled in the output . Alterna - tively , we only represent prompt words in the input , without any explicit signal for where text should be inﬁlled between prompt words . As in the cited work , we initialized the model with weights from pretrained GPT - 2 ( Radford et al . , 2019 ) as a means of embedding general knowledge of English text . GPT - 2 has been highlighted for its potential to generate cre - ative text ( See et al . , 2019 ; Dathathri et al . , 2020 ) . We used the “small” version of GPT - 2 ( 117M parameters ) and also the corresponding GPT - 2 tokenizer to represent all text as subword tokens . We concatenated each prompt and cor - responding inﬁlled sentence together as a single token se - quence , using designated tokens to signify the start ( { { ) and end ( } } ) of the prompt . To avoid memory errors , we set a limit on the size of the sequences by truncating prompts to the ﬁrst 25 subword tokens and target sentences to the ﬁrst 75 tokens . We then ﬁne - tuned the pretrained model for the inﬁlling task by training it on the dataset described above , using the maximum likelihood estimation loss function that is standard for training neural LMs . Our only variation from standard LM training was that we optimized using only the loss for the tokens in the target inﬁlled sentences , and did not compute the loss of the source prompt tokens . This sim - ulates an encoder - decoder scheme which decodes target text from the encoded source input ; here the LM functions as both an encoder and decoder , which signiﬁcantly reduces the number of parameters in the model . We monitored per - plexity on the validation items in order to end training when perplexity stopped improving . In inference mode , the model observes a prompt and generates an inﬁlled sentence through a standard LM decoding method . In particular , we autore - gressively sample from the LM probability distribution and append the resulting token to the sentence , until the end - of - sequence token ( i . e . [ EOS ] ) is generated . Figure 1 : General architecture of sentence inﬁlling model Authoring Experiment We next designed a human authoring task that integrates our trained inﬁlling model . To broadly summarize this process detailed in this section : we selected certain prompts from the test partition of our dataset and generated inﬁlled sen - tences for them . We then elicited human - authored inﬁlled sentences for these same prompts . People produced sen - tences in two conditions . In the ﬁrst , they simply wrote sen - tences for each prompt . In the second , they were shown the sentences generated by our model for the same prompts and wrote new sentences . We explain each of these steps below . Prompt Selection We selected prompts from the test set with exactly three words . This particular length value was picked based on intuition . Fewer words approximates unconstrained genera - tion rather than inﬁlling , while more words simulates a con - strained ﬁll - in - the - blank task . We excluded prompts derived from dialogue sentences ( i . e . those containing quotation marks ) . Dialogue can pose issues for sentence segmentation ( e . g . “he said . ” may be segmented as a separate sentence from its adjacent quote ) . We also excluded prompts contain - ing punctuation , numerical digits , named entities 4 , or word tokens not recognized in the DistilBERT ( described below ) tokenizer vocabulary . Finally , we excluded prompts with more than one function word ( e . g . pronouns , prepositions , determiners ) . By applying these constraints , we expected the prompts to give clear semantic cues for the inﬁlled sen - tences . The resulting selection consisted of 23 , 005 prompts . Since the process for deriving prompts involved random ablation of full sentences and the position of the ablated words varied , we theorized that even prompts of the same length require different degrees of inﬁlling to yield gram - matical sentences . For example , the prompt “his , body , re - lax” already resembles English syntax , and thus it would only take a single inﬁlling word to produce a grammatical sentence ( e . g . “His body could relax” ) . In contrast , it is possible but harder for native English speakers to ﬁnd a sin - gle inﬁlled word that could transform the prompt “peculiar , rob , more” into a grammatical sentence . Accordingly , we expected that the difﬁculty of the task would vary according to the degree of required inﬁlling for a prompt . We designed an approach for automatically scoring this difﬁculty . For each selected prompt , we scored the probability of each of its word tokens according to the Masked LM conﬁguration of DistilBERT 5 ( Sanh et al . , 2019 ) . A Masked LM is well - suited for this measure because it is speciﬁcally trained on a ﬁll - in - the - blank task to predict the likelihood of words ac - cording to their context . We used the average of the prompt token probabilities to represent the inverse difﬁculty ( i . e . easiness ) of a prompt . We theorized that high - probability prompts are easier in inﬁll since they are already probable sequences , whereas low - probability prompts require more inﬁlling to become probable . We assigned the difﬁculty la - bel “easy” to the 10 % highest - probability prompts and the label “hard” to 10 % lowest - probability prompts , yielding 2 , 301 prompts for each difﬁculty level . Generated Sentences for Prompts We then applied the trained model to produce inﬁlled sen - tences for the selected prompts . We generated ﬁve inﬁlled sentences per prompt , using the decoding method of nu - cleus ( top - p ) sampling with p = 0 . 7 , based on the parame - ters recommended by DeLucia et al . ( 2020 ) for generating narrative text . The generated output followed constraints 4 As with the data creation , this detection was done with spaCy . 5 We used the model interface provided by the HuggingFace transformers library : huggingface . co / transformers Figure 2 : Screenshot of authoring interface for a single prompt in the P OST stage . In the P RE stage , the example sentences are not visible . consistent with the human authoring instructions described below . In particular , the generated sentences had to con - tain all prompt words in the same order as they appeared in the prompt . Prompt words were allowed to be capital - ized in the sentence . Sentences had to consist of at least seven word tokens but no more than ﬁfty . We addition - ally restricted sentences with quotation marks and missing end - of - sentence punctuation ( i . e . by requiring the last char - acter to be non - alphanumeric ) , since this may signify the sequence is not a complete sentence or combines multiple sentences ( e . g . quoted dialogue ) . We ﬁltered sentences with adjacently repeated words ( this is a frequently observed is - sue with neural LMs ) . Finally , we promoted the diversity of the ﬁve sentence outputs for a given prompt by ﬁltering any sentence with 60 % or more of its words already appearing in previously generated sentences for that prompt . All of this criteria was satisﬁed by continually generating sentences for a prompt until it yielded ﬁve acceptable outputs . As a last step that we performed through manual review , we ﬁltered any items where the prompt or generated sentence contained profanity or offensive content . This was done to minimize potential risk of harm to participants in the experiment . The ﬁnal set consisted of 2 , 205 easy items and 2 , 189 hard items . Human Authoring Task We then conducted a human authoring task 6 utilizing the se - lected prompts and generated sentences . Participants were instructed that they would be shown a list of three words ( the prompt ) and would write two unique sentences con - taining those words . They were presented with some man - ually written examples of inﬁlled sentences . The instruc - 6 This was implemented as a ReactJS + Flask web application . tions emphasized that they should “try to write sentences that evoke a story someone would be curious to hear” , which activates the construct of storiability that we focus on in this work . The authors’ sentences were required to obey the same prompt token order , length , and end - of - sentence punctuation constraints as the model output , which we en - forced through the user interface . In the ﬁrst stage of the task ( the P RE stage ) , each author wrote two sentences for ﬁve prompts , which were randomly sampled from the “easy” and “hard” categories . In the second stage ( the P OST stage ) , authors were again shown the same ﬁve prompts and wrote an additional two unique sentences for each . This time , the ﬁve generated sentences were shown to them as examples they could reference while writing . Their sentences were re - quired to be different from the examples . Figure 2 shows an example screenshot of the interface for this exercise . The presence of the generated examples was the only vari - able that differed between the two stages . In both stages , af - ter submitting the sentences for a single prompt , participants were shown generated text passages ( described as “stories” ) that each began with the sentences they wrote . These pas - sages were generated by the original pretrained GPT - 2 7 ( not the inﬁlling model ) . Passages had a maximum length of 75 words , and only the ﬁrst k complete sentences within this limit were displayed . The instructions informed authors that writing more interesting sentences would yield more inter - esting stories . However , this component of the task was not an experimental variable , since it was not varied between the two stages . This feedback was simply intended to incen - tivize authors to write more storiable sentences . 7 Using the model interface provided by HuggingFace trans - formers ; generated using nucleus sampling with p = 0 . 7 Prompt Difﬁculty P RE Sentences P OST Sentences G EN Examples walkingandseeing easy 1 . The little chil - dren enjoyed walking through the zoo and seeing all the differ - ent animals . 2 . The boy’s favorite activity was walking to the marina and see - ing all of the boats in the water . 1 . After being re - leased from prison for a crime he didn’t commit , the old man was thoroughly enjoying walking through the city and seeing how the world had changed . 2 . The woman cried when she saw her little girl walking and seeing for the ﬁrst time after she got her new glasses . 1 . She felt the urge to cry , but she kept walking and seeing no sign of it . 2 . He was walking in front of the stove and he looked down on the ground seeing what was going on . 3 . We were walking in and were immediately upon seeing what the neighbors had in store . 4 . She was walking with a friend , and she just hap - pened to be seeing a man , a man , and he was going to kill her . 5 . She could hear men walking up and down the alley , and she didn’t know what they were doing , but she couldn’t deny seeing the resemblance . nosepushedsee hard 1 . The sled dogs nose was in the air as it pushed through the snow to see his owner . 2 . I held my nose and pushed the stinky garbage can to the curb to see if I can catch the garbage man in time . 1 . The dog , using his big nose , pushed the front door open to see if his owner was home . 2 . The boy held his nose to stiﬂe a sneeze but the involuntary re - ﬂex pushed his head forward , watering his eyes and making it hard for him to see . 1 . The man’s nose was being pushed up and down , and as he moved closer to the screen , the image started to dawn on him , and he was shocked to see his father lying on the ground , dying . 2 . He cleared his throat , the same way he had when he had slapped the back of his head and nose , then pushed himself away , but he was careful not to let her see his anger . 3 . When he saw his own nose in the white sordid mess , he pushed off his seat to see it for himself . 4 . He kissed her nose and pushed the sleeve of her shirt back to see what she was thinking . 5 . A stray nose - bleed might be pushed up , but I couldn’t see anything out of place . Table 1 : Examples of authoring blocks . Each block consists of sentences written by a single author before ( P RE ) and after ( P OST ) observing the generated ( G EN ) example sentences . We recruited participants for this task through Amazon Mechnical Turk 8 ( AMT ) , a crowdsourcing platform . 23 au - thors from majority native English - speaking countries were each paid $ 10 based on an estimated completion time of 45 minutes to 1 hour . The result was a dataset of authoring blocks , with each block consisting of a prompt shown to an author , their two sentences written before observing the gen - erated examples ( P RE ) , their two sentences written after the observing the generated examples ( P OST ) , and the ﬁve gen - erated examples they saw ( G EN ) . Examples of authoring blocks are shown in Table 1 . With each author responding to ﬁve unique prompts , this yielded 115 blocks . We ﬁltered six blocks where at least one sentence response ( P RE or P OST ) was revealed to actually consist of multiple sentences ( since this wasn’t straightforward to check through the interface during the task ) . This ultimately resulted in a set of 109 blocks to be used for evaluation , 53 for easy prompts and 56 for hard prompts . Evaluation of Authoring Experiment In line with the objective of the authoring task , we conducted a judgment task to evaluate readers’ perceived storiability of the sentences in the authoring blocks . This resembles story generation evaluations where people are asked which one of a set of stories they most prefer reading ( e . g . Fan , Lewis , and Dauphin , 2018 ) . For each of the 109 blocks , we gathered all unique combinations of the two P RE sen - tences , two P OST sentences , and the ﬁrst two of the ob - served G EN examples in that block , yielding 872 judgment groups ( 109 ∗ 2 ∗ 2 ∗ 2 = 872 ) . Thus , each judgment group consisted of a P RE , P OST , and G EN sentence aligned to the same prompt and author . We designed a questionnaire tar - geting the relative storiability of the sentences in each group . Raters were instructed to “imagine that each sentence [ in the judgment group ] is an excerpt from a story and pick the one that makes you most want to read that story” . Only the sen - tence text itself was shown , and the sentences in each group were randomly ordered . We recruited 16 participants from majority native English - speaking countries through AMT to 8 mturk . com Prompt Difﬁculty P RE Sentence P OST Sentence G EN Sentence feltmeetagain easy Jenna felt a spooky sense of deja vu and felt that she was about to meet a familiar stranger yet again . Bonnie felt a syrupy sentimen - tality and nostalgia and wanted to meet Raphael again . I felt so relieved to meet you again . regardsortsprevent hard He had no regard for his own safety , a maverick of sorts , which did nothing to help prevent him from oft getting injured . In regard to the message , there were all sorts of interpretations that could be made , so she asked for clariﬁcation to pre - vent misunderstandings . A lower regard may come to any type of treatment that may result in a delay of sorts in or - der to prevent future evidence of therapy . servantsearlylife easy It’s sad when people have ser - vants that have to wake up early and do everything for someone else without having a life of their own . They became servants at a very early age after having a difﬁcult life and losing their parents . But , yes , there were two ex - cellent servants from a very early age in the village , who could carry the life of an even younger man . hopingquestionsfew hard I was hoping I could ﬁnd the answer to my homework ques - tions , and after a few minutes I found them by doing a simple Google search . She pored her thoughts , fears , and dreams into her diary , hoping that by writing them down , she could answer the vexing questions of life that few people ever really understood . They were hoping to avoid an - swering any questions for a few days . quickly and joined easy There was a bird that quickly fell from the sky and joined with the ground . The car quickly entered the lane and joined with the trafﬁc . The nurse quickly packed up the case and joined him . arms awkwardly car hard The arms hung awkwardly out the window of the car . His arms ﬂung awkwardly as the police slammed him up against the car to cuff him . Sue wrapped her arms around his neck , pulled him awkwardly out of the car , and then pushed him down the long , steep driveway . Table 2 : Examples of judgment groups . The bolded sentence in each group was selected by both raters as the most storiable . rate subsets of 55 - 56 judgment groups , with each paid $ 5 for an estimated completion time of 25 - 30 minutes . There were two raters for each subset , yielding a total of 1 , 744 responses ( 848 for authoring blocks with easy prompts and 896 for hard ) . Examples of judgment groups are shown in Table 2 . In these examples the bolded sentence was picked by both of its raters as the most storiable among the group . For the results described below , we discuss judgments in terms of storiability preferences . In particular , each response is a single data point where the most storiable sentence se - lected by the rater was labeled as “Preferred” and the other sentences in the judgment group were labeled as “Not Pre - ferred” . All data points have equal weight in the analyses . Results Human versus Generated Storiability Table 3 shows the normalized distribution of storiability preferences across the P RE , P OST , and G EN sentences , along with their raw num - ber of “Preferred” votes . Note that if preferences were ran - domly distributed across these three sets , each would ap - proximate 0 . 33 ( one - third ) of the distribution . The numbers show that people notably preferred human - authored sen - tences ( both P RE and P OST ) to G EN sentences ( statistically signiﬁcant at p < 0 . 05 ) 9 . In contrast with human authoring , the inﬁlling model did not receive any explicit instructions about the storiability au - thoring objective . The model was simply trained to generate sentences that appeared in stories . We can guess that the training sentences observed by the model are not all equally likely to be perceived as storiable . It is possible that this is why raters favored human - authored sentences over the generated ones . However , even generated text designed to mimic human writing objectives often does not meet this standard ( e . g Lin et al . , 2020 ) , so the difference in prefer - ences is not simple to interpret . The focus of this particu - lar paper is not on comparing the relative quality of human and generated text , but on whether generated text can alter the quality of human writing . Thus , the rest of our analyses concentrate on this question . Prompt Difﬁculty Table 4 shows the effect of difﬁculty on the number of inﬁlling words people used to connect the prompt words . The human - authored sentences for the hard prompts had signiﬁcantly more inﬁlled words between 9 Statistical signiﬁcance for all analyses was determined by two - sample Monte Carlo permutation tests . Preferred P RE Preferred P OST Preferred G EN 0 . 356 ( 621 ) 0 . 365 ( 636 ) 0 . 279 ( 487 ) Table 3 : Distribution of storiability preferences prompt words compared with easy prompts ( p < 0 . 05 ) . This validates the expected difference between these conditions , suggesting that hard prompts required more authoring effort . Difﬁculty Inﬁlled Words easy 3 . 035 hard 4 . 317 Table 4 : Mean number of words between prompt words in human - authored sentences according to difﬁculty Prompt Difﬁculty and Storiability Table 5 shows the distribution of preferences for P RE and P OST sentences grouped by difﬁculty level . We found that P OST sentences had higher storiability than the P RE sentences , but only for hard prompts ( p < 0 . 05 ) . Thus , people were more likely to write storiable sentences for these prompts after observ - ing the G EN examples . The result for easy prompts showed a tendency towards the reverse pattern , but the difference in this case was not statistically signiﬁcant . Based on this result , we focus our subsequent analyses on the items asso - ciated with hard prompts . We return to some discussion of this interaction effect regarding difﬁculty in the next section . Difﬁculty Preferred P RE Preferred P OST easy 0 . 384 0 . 354 hard 0 . 329 0 . 375 Table 5 : Distribution of storiability preferences for human - authored sentences by difﬁculty Inﬂuence of Generated Examples The higher preference for the P OST sentences suggests that observing the G EN ex - amples had some impact on authors . One could consider other interpretations : for example , maybe authors were sim - ply better at the task in the P OST stage after a round of prac - tice in the P RE stage . To investigate this , we ﬁrst determined whether any inﬂuence of the G EN examples could be quanti - tatively detected in the P OST sentences . There are many dif - ferent features that could be used to quantify this inﬂuence . Here we focused on whether authors incorporated seman - tic content from the examples they observed . We assessed this using a quantitative measure of semantic similarity be - tween sentences based on vector representations given by a pretrained language model . Intuitively , pretrained LMs are expected to produce similar vector representations for sen - tences with a similar meaning . This representation should transcend the lexical level , so that even sentences with few words in common can have a high similarity score if their respective words in context are synonymous . We computed semantic similarity between the P RE and G EN sentences , and then separately between the P OST and G EN sentences . Since the G EN examples were not shown in the P RE condi - tion and thus could have no inﬂuence on the P RE sentences , any signiﬁcant difference in this measure between the P RE and P OST sentences can be attributed to authors observing the G EN examples . We computed the cosine vector similarity between sen - tences encoded with the DistilBERT 10 LM . For a given prompt , the similarity score for a human - authored sentence h is its maximum similarity over all G EN examples gs for that prompt , i . e . score ( h , gs ) = max g ∈ gs sim ( h , g ) . We select the maximum because there may be one G EN exam - ple in particular that most inﬂuences a given sentence . Table 6 shows the mean of this similarity measure for the P RE and P OST sentences . P OST sentences had higher simi - larity to G EN sentences ( p < 0 . 05 ) , conﬁrming that the G EN examples had semantic inﬂuence on the authors’ writing . Condition Similarity P RE 0 . 921 P OST 0 . 923 Table 6 : Similarity between human and generated sentences before ( P RE ) and after ( P OST ) observation of G EN examples Inﬂuence and Storiability After verifying that the differ - ence between the P RE and P OST conditions can be attributed to semantic inﬂuence from the G EN examples , we examined whether this inﬂuence was related to the higher storiability of the P OST sentences . Table 7 demonstrates that sentences preferred as more storiable were also more semantically in - ﬂuenced by the G EN examples , as indicated by the higher similarity scores for the Preferred sentences ( p < 0 . 05 ) . Thus , by incorporating some degree of content from the G EN examples , people tended to better fulﬁll the authoring objective . Table 8 gives some examples of judgment groups where semantic inﬂuence can be qualitatively observed in the P OST sentence . The G EN example with the most inﬂu - ence is shown ( i . e . the one most similar to the P OST sen - tence ) , and we comment on the subjective evidence of their similarity . These results encourage future opportunities for explaining the exact mechanism underlying semantic inﬂu - ence . We discuss this further in the next section . Judgment Similarity Not Preferred 0 . 922 Preferred 0 . 925 Table 7 : Similarity between P OST and G EN sentences ( i . e . degree of semantic inﬂuence ) according to storiability pref - erences 10 The same core model used for computing probability scores to determine prompt difﬁculty , as described earlier . Here , we use the raw hidden states of the model for feature representation instead of the Masked LM probability outputs . Prompt P RE Sentence P OST Sentence Inﬂuential G EN Example Description shoulderswavescolor My shoulders were aching but I was set on diving through the waves , the color of the water getting deeper the further out I went . Her new hair cut had the length to the shoulders , with waves of a bright pink color all the way down . His hair was cropped short , ﬂowing down his shoulders , but there were waves of the same color . Connectedpromptwordsviasemanticcategoryofhair therediecapacity The bouncer thought there was a chance people might die if there was a ﬁre be - cause the club was way over its capacity . There is no chance you’re not going to die , so you have to come to terms with that in some capacity . It seems , that there is a good chance that I will die in my capacity to forgive and to get on with my life . Used less literal sense of word “capacity” meantsaidstore The child yelled at her sis - ter not understanding what she meant when she said to her that she wanted some comics from the store . It meant a lot to me when she said she was going to the toy store to get me a game . It meant a lot to me , because I’d said I’d drop by the store . Used phrase “it meant a lot to me” spentwindhim After his run he stood by the beach , spent , as the wind whipped by him . She spent the day by the wa - ter , the wind whipping her hair , aching for him . She spent the rest of the day in the saddle , keeping the wind from blowing through her hair and reminding her of her promise to get him a hot bath . Used expanded form of phrase “spent the day” ( “spent the rest of the day” ) peculiar rob more I have a peculiar friend named Rob who always wants more excitement . I felt it was very peculiar that after talking to Rob for only about an hour , I wanted to know more about him . They felt a peculiar attrac - tion to Rob , but couldn’t af - ford to spend much more time together . Referred to curiosity about Rob Table 8 : Examples of P OST sentences demonstrating semantic inﬂuence , with subjective descriptions of how inﬂuence is seen . For reference , the P RE sentence without semantic inﬂuence is also shown . Discussion Observing automatically generated examples of sentence in - ﬁlling inﬂuenced authors to better perform this inﬁlling task on their own . Even though this is a contrived exercise differ - ent from conventional forms of creative writing , it still calls upon the same linguistic creativity . A related task is reﬂected in the real world through popular word games where people produce sentences given word constraints and players rate the interpretability and creativity of the resulting sentences ( e . g . Cooper and McNeill , 2005 ) . The task is also applica - ble to CSTs for writing : for example , a writer might want to brainstorm about potential connections between words they already have in mind , which could be facilitated by a model related to inﬁlling . In contrast to other research on CSTs , this paper focuses less on the interactive capabilities of such systems , like enabling author control over generated output , but our ﬁndings are still relevant to interactive applications . We chose to emphasize the authoring objective of stori - ability because of our focus on AI - augmented story writ - ing . Storiability is not a one - size - ﬁts - all metric for this re - search . The quality of a story can be judged on multiple dimensions that are often not consistently deﬁned across dif - ferent studies , as discussed in Celikyilmaz , Clark , and Gao ( 2020 ) . Evaluations tend to target both the sensibility of sto - ries ( e . g . grammaticality , coherence , plausibility ) and their more “creative” aspects ( e . g . interestingness , suspenseful - ness , humorousness ) . The notion of storiability is more re - lated to the latter group , but does not preclude other dimen - sions . For example , if a sentence contains grammatical er - rors , a person may not prefer to read the story associated with that sentence . By operationalizing storiability accord - ing to a speciﬁc question ( “which sentence makes you want to read more ? ” ) , we tried to elicit judgments that encompass many ways this objective can be achieved . Future research can examine more speciﬁc formulations of this question . An intriguing ﬁnding was the difference in outcomes ac - cording to prompt difﬁculty , such that only sentences for hard prompts displayed more storiability as an effect of ob - serving generated text , with no such pattern for easy items . This points to a broad direction for future work : to examine how the demands of the writing task itself affect authors’ in - teraction with an automated model . For instance , authors’ engagement with writing assistance tools varies at different times during a single writing session , as discussed in Huang , Huang , and Huang ( 2020 ) . This may be due to some parts of the text being harder to write than others , as hinted by the mediating effect of difﬁculty in our results . Interestingly , a follow - up analysis showed there were no signiﬁcant differ - ences in P OST similarity to G EN examples based on difﬁ - culty , meaning that the G EN sentences for easy prompts had just as much semantic inﬂuence as those for hard prompts . Thus , this inﬂuence was somehow not as helpful in promot - ing storiability in the easy case . One possibility is that au - thors were already good at producing storiable sentences for easy prompts in the P RE stage , so even when they were inﬂu - enced by the G EN examples , this inﬂuence did not addition - ally beneﬁt the P OST sentences . The hard prompts may have been more challenging , giving the G EN examples a larger opportunity to enhance the P OST sentences in this case . Be - cause our evaluation did not include pairwise comparisons between sentences for easy and hard prompts , it will require further research to better understand this ﬁnding . Our analysis of semantic inﬂuence conﬁrms authors de - rived certain content from the observed examples . More in - vestigation is needed to understand what type of content was most inﬂuential . Authors may have extracted speciﬁc words and phrases , as indicated by some of the examples in Table 8 , but they did not simply copy or mimic the examples at large ; if they had , there would not be a signiﬁcant difference in storiability between the P OST and G EN sentences as re - ported in Table 3 . One thought is that authors utilized an idea conveyed by a G EN sentence , but reformulated the sentence to repair inadequacies such as ill - formed , awkward , or vague wording . It is also possible that the G EN examples revealed a semantic dimension by which the prompt words were re - lated , one that authors did not initially consider in the P RE condition . The ﬁrst example in Table 8 might convey this : the G EN example connects the prompt words “shoulders” , “waves” , and “color” through the conceptual dimension of “hair” . Perhaps the example triggered the author to recall this particular concept unifying the prompt words , and they emulated it in the P OST sentence . One targeted metric for examining inﬂuence could focus speciﬁcally on modeling this activation of “latent” concepts . Our work quantiﬁed in - ﬂuence according to a single measure , but future work could attempt to narrow down the inﬂuence of speciﬁc linguistic features such as syntactic style ( e . g . relative proportion of nouns , verbs , prepositions , etc . ) , emotional tone ( e . g . joy - ful , sorrowful , fearful ) , and narrative perspective ( e . g . ref - erences to pronouns and proper nouns ) . Existing work has addressed this by examining the strategies authors develop for eliciting precise types of content from generation mod - els ; for example , by triggering the model at certain syntactic positions in a sentence ( Calderwood et al . , 2020 ) . We can use these analyses to guide future systems towards produc - ing content authors ﬁnd most helpful . Conclusion In this paper , we explore the question of how automatically generated text can inﬂuence human creative writing . We speciﬁcally assessed this question through the authoring task and objective of sentence inﬁlling and storiability , respec - tively . In accordance with a proposed inspiration - through - observation paradigm by which automated models provide helpful examples of how to fulﬁll the task , we found that ob - serving generated sentences enhanced reader - judged appeal of human - authored sentences . Our results provide empirical evidence that automated models can intervene in the writing process without necessarily replacing human effort . This in - vites further exploration of this paradigm for other authoring tasks and objectives . The outcome has the potential to tran - scend the standard of both human and computer authoring when each function independently . References Akoury , N . ; Wang , S . ; Whiting , J . ; Hood , S . ; Peng , N . ; and Iyyer , M . 2020 . STORIUM : A Dataset and Eval - uation Platform for Machine - in - the - Loop Story Genera - tion . In Proceedings of the 2020 Conference on Empiri - cal Methods in Natural Language Processing ( EMNLP ) , 6470 – 6484 . Association for Computational Linguistics . Arnold , K . C . ; Chauncey , K . ; and Gajos , K . Z . 2020 . Predic - tive text encourages predictable writing . In Proceedings of the 25th International Conference on Intelligent User Interfaces , 128 – 138 . Association for Computing Machin - ery . Bailey , P . 1999 . Searching for storiness : Story - generation from a reader’s perspective . In Working Notes of the Nar - rative Intelligence Symposium . Bhat , A . ; Agashe , S . ; and Joshi , A . 2021 . How do peo - ple interact with biased text prediction models while writ - ing ? In Proceedings of the First Workshop on Bridg - ing Human – Computer Interaction and Natural Language Processing , 116 – 121 . Association for Computational Linguistics . Buschek , D . ; Z ¨ urn , M . ; and Eiband , M . 2021 . The impact of multiple parallel phrase suggestions on email input and composition behaviour of native and non - native english writers . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . Association for Computing Machinery . Calderwood , A . ; Qiu , V . ; Gero , K . I . ; and Chilton , L . B . 2020 . How novelists use generative language models : An exploratory user study . In HAI - GEN + user2agent @ IUI . Celikyilmaz , A . ; Clark , E . ; and Gao , J . 2020 . Eval - uation of text generation : A survey . arXiv preprint arXiv : 2006 . 14799 . Chandu , K . R . ; Dong , R . - P . ; and Black , A . W . 2020 . Read - ing between the lines : Exploring inﬁlling in visual nar - ratives . In Proceedings of the 2020 Conference on Em - pirical Methods in Natural Language Processing , 1220 – 1229 . Association for Computational Linguistics . Clark , E . , and Smith , N . A . 2021 . Choose your own ad - venture : Paired suggestions in collaborative writing for evaluating story generation models . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics , 3566 – 3575 . Association for Computational Linguistics . Clark , E . ; Ross , A . S . ; Tan , C . ; Ji , Y . ; and Smith , N . A . 2018 . Creative writing with a machine in the loop : Case studies on slogans and stories . In 23rd International Conference on Intelligent User Interfaces , 329 – 340 . Association for Computing Machinery . Cooper , P . E . , and McNeill , D . 2005 . You’ve been sentenced ! https : / / boardgamegeek . com / boardgame / 20790 / youve - been - sentenced . Dathathri , S . ; Madotto , A . ; Lan , J . ; Hung , J . ; Frank , E . ; Molino , P . ; Yosinski , J . ; and Liu , R . 2020 . Plug and play language models : A simple approach to controlled text generation . In International Conference on Learning Representations . DeLucia , A . ; Mueller , A . ; Li , X . L . ; and Sedoc , J . 2020 . Decoding methods for neural narrative generation . arXiv preprint arXiv : 2010 . 07375 . Donahue , C . ; Lee , M . ; and Liang , P . 2020 . Enabling lan - guage models to ﬁll in the blanks . In Proceedings of the 58th Annual Meeting of the Association for Compu - tational Linguistics , 2492 – 2501 . Association for Compu - tational Linguistics . Fan , A . ; Lewis , M . ; and Dauphin , Y . 2018 . Hierarchical neural story generation . In Proceedings of the 56th An - nual Meeting of the Association for Computational Lin - guistics ( Volume 1 : Long Papers ) , 889 – 898 . Association for Computational Linguistics . Fedus , W . ; Goodfellow , I . ; and Dai , A . 2018 . MaskGAN : Better Text Generation via Filling in the . In International Conference on Learning Representations ( ICLR ) . Frich , J . ; MacDonald Vermeulen , L . ; Remy , C . ; Biskjaer , M . M . ; and Dalsgaard , P . 2019 . Mapping the landscape of creativity support tools in HCI . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems , 1 – 18 . Gero , K . I . , and Chilton , L . B . 2019 . Metaphoria : An algo - rithmic companion for metaphor creation . In Proceedings of the 2019 CHI Conference on Human Factors in Com - puting Systems , 1 – 12 . Association for Computing Ma - chinery . Huang , Y . ; Zhang , Y . ; Elachqar , O . ; and Cheng , Y . 2020 . INSET : Sentence inﬁlling with INter - SEntential trans - former . In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , 2502 – 2515 . Association for Computational Linguistics . Huang , C . - Y . ; Huang , S . - H . ; and Huang , T . - H . K . 2020 . Heteroglossia : In - situ story ideation with the crowd . In Proceedings of the 2020 CHI Conference on Human Fac - tors in Computing Systems . Ippolito , D . ; Grangier , D . ; Callison - Burch , C . ; and Eck , D . 2019 . Unsupervised hierarchical story inﬁlling . In Pro - ceedings of the First Workshop on Narrative Understand - ing . Kantosalo , A . 2019 . Human - Computer Co - Creativity : De - signing , Evaluating and Modelling Computational Col - laborators for Poetry Writing . Ph . D . Dissertation , Uni - versity of Helsinki . Khalifa , A . ; Barros , G . A . ; and Togelius , J . 2017 . Deept - ingle . In 8th International Conference on Computational Creativity . Kobayashi , S . 2018 . Homemade bookcorpus . https : / / github . com / BIGBALLON / cifar - 10 - cnn . Lin , B . Y . ; Zhou , W . ; Shen , M . ; Zhou , P . ; Bhagavatula , C . ; Choi , Y . ; and Ren , X . 2020 . CommonGen : A constrained text generation challenge for generative com - monsense reasoning . In Findings of the Association for Computational Linguistics : EMNLP 2020 . Manjavacas , E . ; Karsdorp , F . ; Burtenshaw , B . ; and Keste - mont , M . 2017 . Synthetic literature : Writing science ﬁc - tion in a co - creative process . In Proceedings of the Work - shop on Computational Creativity in Natural Language Generation . Martin , L . 2021 . Neurosymbolic Automated Story Genera - tion . Ph . D . Dissertation , Georgia Institute of Technology . Mizrahi , M . ; Yardeni Seelig , S . ; and Shahaf , D . 2020 . Com - ing to Terms : Automatic Formation of Neologisms in He - brew . In Findings of the Association for Computational Linguistics : EMNLP 2020 , 4918 – 4929 . Association for Computational Linguistics . Mori , Y . ; Yamane , H . ; Mukuta , Y . ; and Harada , T . 2020 . Finding and generating a missing part for story comple - tion . In Proceedings of the The 4th Joint SIGHUM Work - shop on Computational Linguistics for Cultural Heritage , Social Sciences , Humanities and Literature . ¨Ozbal , G . ; Pighin , D . ; and Strapparava , C . 2013 . BRAIN - SUP : Brainstorming support for creative sentence genera - tion . In Proceedings of the 51st Annual Meeting of the As - sociation for Computational Linguistics ( Volume 1 : Long Papers ) , 1446 – 1455 . Association for Computational Lin - guistics . Radford , A . ; Wu , J . ; Child , R . ; Luan , D . ; Amodei , D . ; and Sutskever , I . 2019 . Language models are unsupervised multitask learners . Roemmele , M . , and Gordon , A . 2018a . Linguistic features of helpfulness in automated support for creative writing . In Proceedings of the First Workshop on Storytelling . Roemmele , M . , and Gordon , A . S . 2018b . Automated Assis - tance for Creative Writing with an RNN Language Model . In Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion . Association for Computing Machinery . Safovich , Y . , and Azaria , A . 2020 . Fiction sentence expan - sion and enhancement via focused objective and novelty curve sampling . In 2020 IEEE 32nd International Confer - ence on Tools with Artiﬁcial Intelligence , 835 – 843 . IEEE . Sanh , V . ; Debut , L . ; Chaumond , J . ; and Wolf , T . 2019 . DistilBERT , a distilled version of BERT : smaller , faster , cheaper and lighter . arXiv preprint arXiv : 1910 . 01108 . See , A . ; Pappu , A . ; Saxena , R . ; Yerukola , A . ; and Manning , C . D . 2019 . Do massively pretrained language models make better storytellers ? In Proceedings of the 23rd Con - ference on Computational Natural Language Learning . Shen , T . ; Quach , V . ; Barzilay , R . ; and Jaakkola , T . 2020 . Blank language models . In Proceedings of the 2020 Con - ference on Empirical Methods in Natural Language Pro - cessing , 5186 – 5198 . Association for Computational Lin - guistics . Vaswani , A . ; Shazeer , N . ; Parmar , N . ; Uszkoreit , J . ; Jones , L . ; Gomez , A . N . ; Kaiser , Ł . ; and Polosukhin , I . 2017 . Attention is all you need . In Proceedings of the 31st In - ternational Conference on Neural Information Processing Systems .