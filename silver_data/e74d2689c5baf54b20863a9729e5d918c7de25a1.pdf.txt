Decision Support Towards a new framework for evaluating systemic problem structuring methods Gerald Midgley a , b , c , d , e , ⇑ , Robert Y . Cavana b , John Brocklesby b , Jeff L . Foote f , g , David R . R . Wood f , Annabel Ahuriri - Driscoll g a Centre for Systems Studies , Business School , University of Hull , Hull HU6 7RX , United Kingdom b Victoria Business School , Victoria University of Wellington , PO Box 600 , Wellington , New Zealand c School of Innovation , Design and Engineering , Mälardalen University , Sweden d School of Political and Social Sciences , University of Canterbury , New Zealand e School of Agriculture and Food Sciences , University of Queensland , Australia f Institute of Environmental Science and Research Ltd . , PO Box 29 - 181 , Christchurch , New Zealand g School of Health Sciences , University of Canterbury , Christchurch , Private Bag 4800 , New Zealand a r t i c l e i n f o Article history : Received 1 December 2011 Accepted 28 January 2013 Available online 9 February 2013 Keywords : Problem structuring methods Soft operational research Evaluation of methods Participative methods Systems methodology Systems thinking a b s t r a c t Operational researchers and social scientists often make signiﬁcant claims for the value of systemic prob - lem structuring and other participative methods . However , when they present evidence to support these claims , it is usually based on single case studies of inter vention . There have been very few attempts at evaluating across methods and across interventions undertaken by different people . This is because , in any local intervention , contextual factors , the skills of the researcher and the purposes being pursued by stakeholders affect the perceived success or failure of a method . The use of standard criteria for com - paring methods is therefore made problematic by the need to consider what is unique in each interven - tion . So , is it possible to develop a single evaluation approach that can support both locally meaningful evaluations and longer - term comparisons between methods ? This paper outlines a methodological framework for the evaluation of systemic problem structuring methods that seeks to do just this . (cid:2) 2013 Elsevier B . V . 1 . Introduction Participative methods facilitate the engagement of stakeholders and / or citizens in decision making to address complex organiza - tional , social , environmental or technolo gical issues . They are used by managemen t researchers and practitioners ( as well as other so - cial scientists ) in the context of interventions to stimulate deliber - ative dialogue and the developmen t of change proposal s ( Beierle and Cayford , 2002 ; Rowe and Frewer , 2004 ) . A subset of the general class of participative methods is problem structuring methods ( PSMs ) . A substanti al number of these have been develope d by operation al researchers over the past 50 years , although the term ‘problem structuring’ itself was only introduced into the operational research ( OR ) lexicon a couple of decades ago ( Rosenhead , 1989 , 2006 ; Rosenhead and Mingers , 2001 , 2004 ) . A distinguishing feature of PSMs , compared with many other partic - ipative methods developed by social scientists , is the use of models as ‘transitional objects’ to structure stakeholder engagem ent ( Eden and Sims , 1979 ; Eden and Ackermann , 2006 ) and provide a focus for dialogue ( Franco , 2006 ) . These models may use words , pictures and / or numbers to represent , for example , people’s understa ndings of a problematic situation ; the assumptions underpinning a partic - ular stakeholder perspective ; and / or the activities that might be needed to improve the situation . Usually , models are qualitative and are constructed collective ly in a workshop , but sometimes they are brought in by a facilitator based on previous inputs from participa nts and are used to orientate engagement : ‘‘the model . . . plays a key role in driving the process of negotiation towards agreement through discussion and the developmen t of a common understa nding’’ ( Franco , 2006 , p . 766 ) . However , a ‘common understa nding’ does not necessarily imply consensus or agreement across the board : it may be an agreed understand ing of the differ - ences between people’s perspectives and what accommodati ons are possible in the circumstanc es ( Checkland and Scholes , 1990 ) . Qualitative models have traditionally been produced on ﬂip charts using marker pens , but computer - medi ated modelling is increas - ing in popularit y , and this can facilitate remotely distribut ed and / or anonymous stakeholder participation , bringing advantag es compare d with face - to - face , pen and paper modelling ( Er and Ng , 1995 ; Fjermest ad , 2004 ; Fan et al . , 2007 ) . Some PSMs are explicitly systemic ( Jackson , 2000 ; Midgley , 2000 , 2003 ) . They not only seek to enhance mutual understanding between stakeholders , but they also support participa nts in under - taking ‘bigger picture’ analyses , which may cast new light on the 0377 - 2217 (cid:2) 2013 Elsevier B . V . http : / / dx . doi . org / 10 . 1016 / j . ejor . 2013 . 01 . 047 ⇑ Corresponding author at : Centre for Systems Studies , Business School , Univer - sity of Hull , Hull HU6 7RX , United Kingdom . Tel . : + 44 ( 0 ) 1482 463316 . E - mail address : g . r . midgley @ hull . ac . uk ( G . Midgley ) . European Journal of Operational Research 229 ( 2013 ) 143 – 154 Contents lists available at SciVerse ScienceDi rect European Journ al of Operational Research journal homepage : www . elsevier . com / locate / ejor Open access under CC BY - NC - ND license . Open access under CC BY - NC - ND license . issue and potential solutions . Notably , systemic PSMs are used to broaden the perspecti ves of participants in order to facilitate the emergence of new framings , strategies and actions . Typical ques - tions addressed by different systemic PSMs include : (cid:2) Whose viewpoints and what aspects of the issue should be included in analysis and decision making , and what should be excluded ? ( e . g . , Ulrich , 1994 ; Midgley , 2000 ) . (cid:2) What are people’s different perspecti ves on the issue , and what values and assumptions underpin these perspectives ? ( e . g . , Checkland and Scholes , 1990 ; Checkland and Poulter , 2006 ) . (cid:2) What interactions within and across organisation al , social and environmental phenomena could produce desirable or undesir - able outcomes ? ( e . g . , Vennix , 1996 ; Maani and Cavana , 2007 ) . We argue in this paper that a new framework is needed for the evaluation of systemic PSMs . However , given that so little has pre - viously been written on this subject , we also draw upon the wider literature about evaluating participa tive methods ( beyond problem structuring , systems thinking and OR ) . 2 . Evidence for the value of systemic problem structuring and other participative methods When claims are made for the success or failure of systemic problem structuring and other participativ e methods , the authors making those claims are usually required to justify them . Various reviews of the literature on the evaluation of participativ e methods suggest that most of the justiﬁcations provided by researchers are based on personal reﬂections alone ( Entwistle et al . , 1999 ; Connell , 2001 ; Rowe and Frewer , 2004 ; Sieber , 2006 ; White , 2006 ) . Clearly , many researchers are highly experienced , so their reﬂections should not be dismissed out of hand . Nevertheless , unless they think broadly and from different perspectives about the criteria they use to evaluate their participative interventions , they may miss evidence that does not ﬁt their current thinking about what is important ( Romm , 1996 ; Midgley , 2011 ) . We therefore suggest that there is a need for caution in accepting researcher reﬂections alone as reliable evidence of success or failure . Most researchers undertaking evaluations of participativ e methods beyond personal reﬂections tend to conduct post - intervention debrieﬁngs or interviews with project participants . These evaluations are often based on explicit criteria reﬂecting the researche r’s experience , a given theory , a literature review and / or stakeholder expectations generate d through a consultative exercise ( Beierle and Konisky , 2000 ; Rowe and Frewer , 2004 ) . In some cases , formal evaluation instruments have been developed and applied ( e . g . , Duram and Brown , 1999 ; Rowe et al . , 2004 ; Berry et al . , 200 6 ; Ro uwe tte , 201 1 ) . Al so a num be r of re se ar ch ers adv oca te triangulatio n across two or more evaluation methods , such as interviews , focus groups , participant observations , surveys , litera - ture reviews and document analyses ( Duram and Brown , 1999 ; Buysse et al . , 1999 ; Charnley and Engelbert , 2005 ; Rowe et al . , 2005 ; Cole , 2006 ; McGurk et al . , 2006 ; Franco , 2007 ; Rouwette , 2011 ) . What is clear from the literature , however , is that only a very small minority of studies ( e . g . , Valacich and Schwenk , 1995a ; Hal - vorsen , 2001 ; Rouwette et al . , 2011 ) seek to compare between methods or across case studies undertak en by different research - ers . A particular ly signiﬁcant study was undertak en by Beierle and Cayford ( 2002 ) , who quantitative ly compare d broad classes of methods using a standard set of variables applied to 239 case studies of public participatio n . They concluded that more intensive processes ( such as mediation workshops ) are better than less intensive processes ( such as public meetings ) at achieving a wide range of outcomes . We suggest that the use of systemic PSMs is relatively intensive compared with several of the other participa - tive processes investiga ted by Beierle and Cayford ( 2002 ) , so this gives us grounds to be cautiously optimistic . However , we cannot take this study as strong evidence because they did not speciﬁcally identify systemic PSMs as a category for comparis on with other participa tive approaches . Therefore , the overall picture is of many claims for the beneﬁts of a diverse array of systemic problem structuring and other partic - ipative methods , with varying degrees of evidence provided by researche rs to support these . Only a few studies have compared across methods , and even these have only been able to contrast broad classes of approach . The key question is : what kind of evaluation is both necessar y and possible ? We have already argued that researche r reﬂections alone can be problematic , but are there methodol ogical or practical reasons to prefer either locally focused evaluations ( possibly with some learning across case studies , when this is feasible ) or large - scale , quantitat ive comparisons between methods ? 2 . 1 . Different evaluation approaches Rowe and Frewer ( 2004 ) , reﬂecting on social science ap - proaches to evaluating participativ e methods , classify them into three types . First there are universal evaluations : i . e . , ones claiming to produce knowledge that is applicable across all types of partic - ipative method and intervention . According to Rowe and Frewer , to achieve universality , large - scale quantitative studies are needed . Neverthel ess , to make comparisons possible , only variables of gen - eral relevance across all methods and interventi ons can reasonabl y be assessed . Next there are local evaluations : comparing between a subgroup of methods or interventi on types . These require smaller scale studies and can incorporate more detailed questioning , as the variables to be examined may be relevant only to the subgroup of methods under study rather than to all possible methods . Some researche rs working on local evaluations advocate a quasi - experime ntal approach , either testing methods in the laboratory or in controlle d ﬁeld conditions . Rowe and Frewer ( 2004 ) call the third and ﬁnal type of evaluation , which the majority of research - ers use , speciﬁc . This means focusing on only one method or inter - vention . The advantage of this is that the evaluation can be made locally relevant , drawing ( for example ) on information about the unique expectations of stakeholders to establish evaluation crite - ria . Rowe and Frewer argue that , while it is difﬁcult ( for practical reasons ) to conduct truly universal evaluations , researchers should aim to achieve as much generality as possible , and should certainly do more than undertak e evaluations with only a speciﬁc remit be - cause generalising from these is highly problematic . White ( 2006 ) argues that very similar distinctio ns have been made in the OR and group decision support literatures , and prefer - ences for universality ( to a greater or lesser extent ) or speciﬁcity reﬂect the positivist and interpretivist paradigms respectively . Pos - itivists are said to argue for objective , quantitative , comparative studies that are capable of revealing the generalisab le advantages and disadvantag es of different methods , although ( like Rowe and Frewer , 2004 ) many are forced by the impracticality of undertak ing truly universal studies to resort to more local quasi - exp eriments in either the laboratory or the ﬁeld . Authors in this tradition include Nun ama ke r et al . ( 199 1 ) , Fj erm est ad and Hi ltz ( 199 8 ) , Pi ns onnea ult et al . ( 1999 ) , Fjermest ad ( 2004 ) and Joldersma and Roelofs ( 2004 ) . In contrast , interpretivists ( such as Eden , 1995 ; Eden and Ackermann , 1996 ; Shaw , 2003 ) argue that what matters most in an evaluation is what is achieved by the method in a given context , judged from the perspectives of stakeholders . It is therefore hardly surprisin g that most interpretivists are in favour of undertak ing speciﬁc ( single case study ) evaluations . See Connell ( 2001 ) , Bryant 144 G . Midgley et al . / European Journal of Operational Research 229 ( 2013 ) 143 – 154 and Darwin ( 2004 ) , Phahlamohlaka and Friend ( 2004 ) and Sørensen et al . ( 2004 ) for examples . Our own position on these debates is as follows . For both epis - temological and methodol ogical reasons , we do not accept that it is possible to generate universally applicable knowledge about meth - ods . Our epistemologi cal argument is that knowledge ( or under - standing ) is always linked to the purposes and values of those producing or using it , and is depende nt on the boundary judge - ments that they make ( Churchman , 1970 ; Ulrich , 1994 ; Alrøe , 2000 ; Midgley , 2000 ) . To claim that knowled ge about systemic PSMs ( or any other phenomeno n for that matter ) is universa l is to ignore the purposes , values and boundary judgeme nts that make the knowledge relevant and adequate for a particular con - text . This argument is consistent with the epistemol ogical assump - tions made by most of the creators of PSMs ( Jackson , 2006 ) . We also have two methodological arguments following from our epistemol ogical one . First , claiming universa lity for knowled ge about systemic PSMs would suggest that this knowledge will re - main stable over time . However , it is clear from the literature ( e . g . , Rosenhead and Mingers , 2004 ; Shaw et al . , 2006 ; Franco et al . , 2007 ) that new problem structuring methods are being pro - duced on a regular basis , indicating that people are learning from previous practice and are also having to respond to an ever increas - ing number of unique practical situations . Given that this is a dy - namic research environment , it would seem risky to assume that a standard set of variables will always be relevant . Undertaking a series of more limited comparisons between particular methods might be methodological ly wiser than trying to set up a ‘universal ’ study . Our second methodological argument , following Eden ( 1995 ) and others , is that only seeking knowledge about the supposed ly generic strengths and weakness es of methods ignores legitimate questions that can be asked about the effectiveness of those meth - ods in particular local circumstances . Given that operational researchers using systemic PSMs work most of the time in partic - ular contexts with unique features , it would only meet a small frac - tion of the need for evaluation if we were to ignore non - generic questions , and this would be unaccept able to local stakeholders wanting to know what will best meet their particular needs . There can also be problems with what Rowe and Frewer ( 2004 ) call ‘local’ evaluations ( comparing more limited sets of methods in smaller scale research projects ) . Some have called for ‘objective ’ lo - cal studies rather than the simple reporting of subjective impres - sions ( Pinsonneau lt et al . , 1999 ; Rowe and Frewer , 2004 ; Rowe et al . , 2005 ) . However , when the pursuit of objectivi ty involves a retreat into the laboratory to conduct controlled experiments ( e . g . , Valacich and Schwenk , 1995b ; Montazem i et al . , 1996 ; De la ney et al . , 19 97 ) , then the vali di ty of the com par is on of me th ods has been questioned due to the artiﬁciality of the situation ( Eden , 1995 ; Er and Ng , 1995 ; Shaw , 2003 ; White , 2006 ) . While we accept that laborator y experiments are valid when some technical ques - tions are being investigated , such as whether computer mediation enables the capture of more participa nts’ statements than use of a ﬂip chart ( Gallupe et al . , 1992 ; Fjermestad , 2004 ; Fan et al . , 2007 ) , we suggest that questions relating to the performance of methods in the context of stakeholder disagreem ent and conﬂict are another matter entirely . In the laboratory , ‘decisions’ made by participants have no longer term consequences , so participa nts are unlikely to think or behave in the same way as they do when faced with dis - agreements and potential outcomes that really matter to them . If quasi - exper iments are established in the ﬁeld instead of the labo - ratory , then this raises other problems : McAllister ( 1999 ) argues that it is unethical to use a control when dealing with real commu - nity issues , and Duignan and Casswell ( 1989 ) simply point to the impracticality of ﬁnding two situation s that are sufﬁciently alike to make a comparative study robust . In making criticisms of attempts to take a controlled or quasi - experime ntal approach , some authors have advanced alternatives . Kelly and Van Vlaender en ( 1995 ) , McKay ( 1998 ) , Jenkins and Bennett ( 1999 ) , De Vreede and Dickson ( 2000 ) , Gopal and Prasad ( 2000 ) and Allsop and Taket ( 2003 ) advocate ‘emergent’ methodol - ogies : i . e . , ones where criteria for evaluation emerge through engagem ent with stakeholder s . Eden ( 1995 ) makes the important point that most interventi ons are complex , and researchers can rarely anticipate everythin g that will become important , so the evaluation approach needs to be able to respond to the unexpected . However , does this mean that evaluations cannot legitimatel y generalise from single , speciﬁc case studies to other contexts that may be similar in at least some respects ? It is widely accepted that the ‘success’ or ‘failure’ of a method in any particular case results from use of the method - in - con text and cannot be attributed to the method alone ( Checkland and Scholes , 1990 ; Buysse et al . , 1999 ; McAlliste r , 1999 ; Murphy - Berm an et al . , 2000 ; Morgan , 2001 ; Margerum , 2002 ; Rowe and Frewer , 2000 , 2004 ; Branch and Bradbury , 2006 ; McGurk et al . , 2006 ; White , 2006 ; Warburton et al . , 2007 ) . Nevertheles s , several researchers claim that cross case study learning is possible , with two or more research teams reﬂect - ing on similarities and differences between cases ( e . g . , McAllister , 1999 ; Yearley , 2006 ; White , 2006 ) . Checkland ( 1981 ) argues that evaluating a systemic methodology depends on the long term accumulation of evidence from a diverse range of applications , giv - ing progressive ly more conﬁdence that the approach is useful across contexts : it is only through such an accumulation of evi - dence that the efﬁcacy ( does it work in the ways claimed ? ) , effec - tiveness ( is it the best approach for what is needed ? ) and efﬁciency ( are maximum beneﬁts gained at minimum cost ? ) of an approach can be reasonably assessed ( also see Checkland et al . , 1990 ; Zhang et al . , 1997 ) . 2 . 2 . A pragmatic step sideways It would appear from the literature that most researche rs accept the logic of interpretivism and are more inclined to undertak e spe - ciﬁc , locally meaningful evaluations ( and possibly learn across these ) than attempt comparis ons between methods using generic , quantitat ive measures ( Mingers and Rosenhead , 2004 ; White , 2006 ) . However , we have to ask whether this means that all forms of quantitative comparis on are redundant . White ( 2006 ) argues that the debate in the problem structuring research communi ty has become unhelpfully polarised , with many advocates on both sides taking ‘purist’ positions and spurning methods that could en - hance their own evaluation practices . He therefore proposes a more pragmatic line : identifying important research questions and asking what evaluation methods might answer these most effectively . We agree that this is a useful step sideways from the either / or debate , but we neverthe less suggest that identifying effective evaluation methods to address particular research ques - tions involves considering the practicalities of undertaking evalua - tions as well as the norms of what constitutes a valid or legitimate methodol ogy . A difﬁcult balance has to be struck between rigour and relevance ( Shaw , 1999 ) because if the former is unquestion - ingly prioritise d then there is good evidence that stakeholders will not co - opera te ( Rowe et al . , 2005 ) . Importantly , this balance has to be struck regardless of whether an emergent approach is being fol - lowed or whether a more traditional scientiﬁc study comparing methods is being undertak en . In sympathy with White’s ( 2006 ) pragmatic intent , we set out to propose an evaluation approach that supports locally meaning - ful evaluations and is capable of generating data for longer - term quantitat ive comparisons between methods without compromis - ing local relevance . The overall framework is based in the tradition G . Midgley et al . / European Journal of Operational Research 229 ( 2013 ) 143 – 154 145 of multi - method systemic intervention ( e . g . , Flood and Jackson , 1991 ; Jackson , 1991 , 2000 ; Flood and Romm , 1996 ; Mingers and Brocklesby , 1997 ; Mingers and Gill , 1997 ; Midgley , 2000 , 2003 ; Taket and White , 2000 ; Burns , 2007 ) , but instruments can be employed as part of the emergent evaluation of methods that enable data gathering for both immediate local and longer - term comparative use . Below , we outline the rationale for our frame - work . We then discuss early work in developing and testing a questionnair e that can be used in the context of it . 3 . A new evaluatio n framework Our evaluation framework is represented in Fig . 1 . An evalua - tion using it is primarily focused on the use of a particular method ( or set of methods ) in a context for particular purposes , giving rise to outcomes . The words in italics in the previous sentence represent what we regard as four necessar y foci to evaluative inquiry , and they need to be interrelated in the context of a speciﬁc reﬂection on the use of a method . Explorati on of these aspects may proceed in any direction around Fig . 1 , and may loop back and forth accord - ing to the needs of those involved in the evaluation . We note that it is possible to develop much more elaborate con - ceptual framewor ks than ours , with strong utility for research ( e . g . , Champion and Wilson , 2010 ) . However , if a framework is to be memorable in the context of practice , it needs to use relatively few high - level concepts organised in a visually appealing manner . Lower level concepts can be introduced under the higher level ones . Other authors have proposed similar , but not identical , frame - works to ours . Buysse et al . ( 1999 ) and McAllister ( 1999 ) advocate the explorati on of both purposes and context , but tend to take as given the nature of the method to be evaluated . Pinsonneault and Kraemer ( 1990 ) , Flood ( 1995 ) , McGurk et al . ( 2006 ) and Rouwette et al . ( 2009 ) ask researchers to reﬂect on the adequacy of their contextual analyses , their choices of methods or processes and their intervention outcome s . However , the purposes being pursued become implicit : whether or not these differ from the out - comes is not necessarily at issue . Warburton et al . ( 2007 ) propose reﬂection on context , purposes and methods , but they do not con - sider the implications of the researcher’s role in the situation . This is an important issue for us ( and is represented in Fig . 1 by the text in the lower parts of the four ellipses ) because our experience is that the researcher becomes an interactive part of the situation in which he or she is seeking to intervene using systemic PSMs ( also see Checkland , 1981 ) , and his or her identity and relationshi ps can signiﬁcantly affect the trajectory of an intervention ( Brocklesby , 1997 ; Mingers , 1997 ; Midgley et al . , 2007 ) . In our approach , when looking at a single case study , there is no pretence that it is possible to evaluate a method independently from the purposes it is put to , its outcomes and the context in which it is applied . Nevertheless , we can still inquire about the relationsh ips between the method , purposes , outcome s and context . Inquiry focused on an intervention can look , for example , at how satisfacto rily the method addressed given purposes ; what aspects of the context enabled or constrain ed its application ; and whether it gave rise to anticipated or unanticipated outcomes . Some fea - tures of the context - pur poses - methods - outcomes relationshi p may be apparent early on in an intervention , while others may only emerge as the inquiry unfolds . Hence the utility of an emergent ap - proach for the evaluation of methods , which remains open to new understa ndings as inquiry deepens ( e . g . , Kelly and Van Vlaenderen , 1995 ; Jenkins and Bennett , 1999 ; Gopal and Prasad , 2000 ; Allsop and Taket , 2003 ) . Below , we examine the four aspects of evaluation ( context , pur - poses , methods and outcomes ) in turn , explainin g why each of these is important to developing a rounded understanding of how a method has operated in a particular case study of practice . 3 . 1 . Context More has been written about context than the other aspects of evaluation , arguably because it is crucial to good practice to realise that the same method utilised by the same researcher can succeed Fig . 1 . Conceptual framework for the evaluation of systemic problem structuring methods . 146 G . Midgley et al . / European Journal of Operational Research 229 ( 2013 ) 143 – 154 or fail depending on the complexi ties and dynamics of the situa - tion ( e . g . , Checkland and Scholes , 1990 ; Nunamaker et al . , 1991 ; Buysse et al . , 1999 ; McAllister , 1999 ; Murphy - Ber man et al . , 2000 ; Rowe and Frewer , 2000 , 2004 ; Morgan , 2001 ; McGurk et al . , 2006 ; White , 2006 ; Warburton et al . , 2007 ; Champion and Wilson , 2010 ) . Relevant aspects of context identiﬁed by Jackson and Keys ( 1984 ) are the complexity of the issue being addressed using a sys - temic method and the relationshi ps between the participants . In contrast , Margerum ( 2002 ) identiﬁes potential contextu al inhibi - tors of effective participatio n : a low level of commitment by key decision makers ; parochia lism ( which can negatively affect inclu - siveness ) ; participa nts having inadequate skills and abilities ; oper - ational issues preventing the impleme ntation of ideas ; a lack of strategic thinking beyond the exercise at hand ; poor leadership ; and scarcity of resources . Ong ( 2000 ) discusses the facilitative ef - fects of strong social capital , and Alberts ( 2007 ) documents the negative effects of participant inexperie nce and ignorance of tech - nical issues . Branch and Bradbury ( 2006 ) claim that a key aspect of context is managerial attitude : especially the disclosure ( or not ) of relevant informat ion ; whether managers set agendas unilaterally or are open to power sharing ; whether or not there is mutual re - spect in relationships ; whether there is accountability to stake - holders ; and whether or not people believe that a transparent decision making process will be used following stakeholder partic - ipation . McCartt and Rohrbaugh ( 1995 ) argue that a key aspect of managerial attitude is openness to change , and participative meth - ods are often ineffective without it . Kelly and Van Vlaenderen ( 1995 ) and Brocklesby ( 2009 ) concentrate on stakeholder interac - tions , looking at how patterns of mistrust and miscommunicati on can become established and affect the use of participative meth - ods . Related to this is the identity of the researcher : Midgley et al . ( 2007 ) discuss how identity issues can make a signiﬁcant difference to the quality of relationships , and hence the success or failure of a method ( this is represented in Fig . 1 by the lower half of the ‘context’ ellipse ) . Champion and Wilson ( 2010 ) provide a particularly useful set of contextual variables to be considered , based on a literature review and feedback from practitioners : organisation al structure ; inﬂuence of the external environ - ment ; length of history of the problem in focus ; politics and personalitie s ; perceived impleme ntation difﬁculty ; and the level of experience of stakeholders . No doubt the list of possible aspects of context could be ex - tended indeﬁnitely ( Gopal and Prasad , 2000 ) , and different issues will be relevant in different situations , so we argue that it is more useful to give some methodological guidelines for exploring con - text in local situations than it is to provide a generic inventory of variables . We suggest that the following guidelines , derived from reﬂections on different systems paradigms ( as represented by Jack - son ( 1991 ) , and others ) , can all contribute in different ways to boundary critique ( the explorati on of different possible boundaries , or frames , for a contextual analysis ) : (cid:2) Underpinnin g different boundary judgements may be quite dif - ferent perspectives on the nature of the context ( Churchman , 1970 ) . Therefore , exploring diverse perspecti ves ( e . g . , as advo - cated by Checkland ( 1981 ) ) may lead to the identiﬁcation of alternative possible ways of bounding a contextual analysis ( Ulrich , 1994 ) . (cid:2) Establishing a boundary for analysis involves making a value judgement on what issues and stakeholder s are important or peripheral ( Ulrich , 1994 ) . Therefore , undertaking an explorati on of different stakeholder s’ values and priorities can be helpful . It is also useful to identify conﬂicts between people making different value judgements as well as processes of marginalisa - tion that may constrain stakeholder participation or make the discussion of some phenomena taboo ( e . g . , Midgley , 2000 ) . (cid:2) Identifying the presence of inﬂuential institutiona l or organisa - tional systems may be important . Any such system can have its own agenda , rationality and momentum that may come to dominate an intervention ( Douglas , 1986 ; Luhmann , 1986 ; Brocklesby , 2009 ) , yet organisational systems still have to inter - act with others , and tensions can result ( Paterson and Teubner , 1998 ) . Thus , an institutiona l analysis can be a useful aspect of boundary critique . (cid:2) There may be socio - econom ic and ecological systems providing resource s that can be used constructivel y by participants , or these systems may impose limits on what is achievab le without incurring negative side - effects ( Clayton and Radcliffe , 1996 ) . Economic issues may point to concerns about social justice , which ( if present ) could inﬂuence people’s perceptions of the effects of systemic PSMs : i . e . , the use of a particular method may be seen as supportive of just or unjust social relationships ( Jackson , 1991 , 2006 ) , so it can be useful to look at the effects of socio - eco nomic systems as part of boundary critique . Taking explicit account of ecological systems can also enhance bound - ary critique by challengi ng a tendency to uncritically resort to boundari es deﬁning exclusively human systems , thereby mar - ginalising the ecological ( Midgley , 1994 ) . Pettigrew ( 1987 ) notes that wider systemic ( e . g . , socio - economi c and ecological ) contexts not only inﬂuence perceptions of methods and pro - cesses , but also the content of participants ’ deliberations . (cid:2) Within and across ecological , economic , social and organisa - tional systems , there may be important causal pathways , and in particular feedback loops , that can point to systemic enablers of , or constrain ts on , an intervention ( e . g . , Forrester , 1969 ) . Bateson ( 1970 ) argues that it is important not to ‘cut’ relevant feedback loops , and again this is a good principle to inform boundary critique : when we see interconnec tions stretching beyond people’s usual understand ings of context we can ask whether it is important to widen the boundaries of analysis to account for these . Essentiall y then , a useful approach to exploring context may in - volve looking at different possible boundari es for analysis , concen - trating in particular on different stakeholder perspectives ; value judgeme nts around the inclusion or exclusion of issues and stake - holders ; processes of conﬂict and marginalisatio n ; ecological , eco - nomic , social and institutiona l / organisationa l systems that may act as enablers or constraints ; and causal relationship s and feedback processes within and across those systems . 3 . 2 . Purposes The second aspect of our evaluation framework is concerne d with exploring stakeholders’ purposes in engaging with an inter - vention . Purposes are closely linked with values and motivatio ns ( McAlliste r , 1999 ) , and they are important to an evaluation be - cause particular methods are likely to appear more or less useful depending on the purposes being pursued . Different methods are generally good for different things ( Flood and Jackson , 1991 ) , and it is the perceived ‘ﬁt’ between purpose and method that is impor - tant to evaluate : a disjuncti on may be responsible for an attribu - tion of failure . It is important to consider possible hidden agendas as well as explicitly articulated purposes . These may signiﬁcantly affect the trajector y of an intervention ( for instance through sabotage ) , and thereby the evaluation of the method used ( Ho , 1997 ) . It is also G . Midgley et al . / European Journal of Operational Research 229 ( 2013 ) 143 – 154 147 useful to look out for mismatches between articulated purposes and ones attributed by others ( both to individuals and organisa - tions ) because mismatches of this kind often signal mistrust or conﬂict that will be relevant to the performanc e and evaluation of methods ( Kelly and Van Vlaenderen , 1995 ) . Whether or not there is mistrust or conﬂict , there will often be multiple purposes at play . If people come to an intervention with different purposes for engaging , then it is likely that different evaluation criteria will be important to them ( McAllister , 1999 ; Murphy - Berm an et al . , 2000 ; Tuler et al . , 2005 ; Rowe and Frewer , 2004 ; Masozera et al . , 2006 ; White , 2006 ) . While Rowe and Frewer ( 2004 ) say that an appropriate response is to set aside the purposes and preferred criteria of diverse stakeholders in favour of a single criterion of ‘acceptability of the method to all parties’ , more nuanced ﬁndings will be generated by evaluating the method against multiple criteria of relevance to different stakeholders ( Murphy - Berm an et al . , 2000 ) . Note here that the purposes of the researche r should not be ex - cluded from consideration . There may be a good ‘ﬁt’ between stakeholder and researche r purposes , but there may also be dis - junctions . An example is when the researcher works in a university and brings a pre - deﬁned academic agenda into the intervention , which may inﬂuence how systemic PSMs are chosen and used . Even when an academic researcher makes a signiﬁcant effort to be responsive to stakeholders , there may still be mistrust stemming from expectations of divergent purposes ( Adams and McCullough , 2003 ) , and this may affect the evaluation of methods . 3 . 3 . Methods Earlier we mentioned that some authors have advocated taking account of the effects of stakeholder purposes and context , but they tend to take the nature of the method being evaluated for granted . It is important not to do this because different methods make different theoretical and methodological assumptions about ( amongst other things ) human relationships , knowled ge , values and the nature of the situation that is the focus of the intervention ( e . g . , Jackson , 1991 , 2000 ; Romm , 1996 ; Spash , 1997 ; Midgley , 2000 ) . In an evaluation , we need to be able to account for if and how these assumptions have shaped the unfolding of the intervention . There may also be elements of methods that people in some cultures ( or sub - cultu res ) will ﬁnd it easier to accept or work with than others . While culture may be conceive d as an aspect of the context , it may also be reﬂected in the construction of a method , which is why a number of methodol ogists working outside the Western tradition have sought to establish systems / OR and other approaches develope d from their own philosophic al and cultural perspectives ( e . g . , Smith , 1999 ; Zhu , 2000 ; Shen and Midgley , 2007 ) . Becoming aware of the cultural norms embodied in a meth - od may be important to understand ing its effects across cultural contexts . The process of application of a method is important as well , not just the method as formally constructed ( Keys , 1994 ) . For instance , the same basic method may be enacted in quite different ways depending on the preferences and skills of the researche r / facilita - tor and the demands of the situation at hand . Compare , for exam - ple , two signiﬁcantly different accounts of soft systems methodology ( SSM ) : Checkland and Scholes ( 1990 ) discuss how the methods from SSM should be utilised in a ﬂexible and iterative manner , while Li and Zheng ( 1995 ) insert some of the same meth - ods into a ‘general systems methodology’ . In the latter case , it is clear that the methods of SSM are to be applied in a linear se - quence . In many contexts , such a signiﬁcant difference in the pro - cess of application of the same set of methods is bound to impact upon the way that set will be perceived . Not only can the researcher’s preferenc es and approach be important , but also the extent of his or her skills and experience may inﬂuence whether the use of a method is perceived as suc - cessful or not . Mingers ( 1997 ) describes these as the ‘‘intellectual resource s’’ that the researcher brings into an interventi on , and it is important to be able to distingui sh whether problems encoun - tered in the use of a method derive from the limitations of the method itself or from the inadequate resources of the researcher . Conversel y , the evaluation may reveal that the researche r had exception al skills that were used to good effect in securing a suc - cessful outcome . 3 . 4 . Outcomes In addition to collecting information about the assumpti ons embedde d in , and the process of application of , the method being evaluated , it is most important to collect data on its outcomes . These can be viewed from the perspecti ves of those involved and affected ( usually participa nts in workshops , but others might be relevant too , depending on the context ) . This is the crux of the evaluation of methods . It is necessary to distinguish outcome criteria from process crite - ria ( Chess and Purcell , 1999 ; Rowe and Frewer , 2004 ) . Process cri - teria ( e . g . , did the process of applying the method give everyone a chance to speak , allow creative exploration , or enable a fair evalu - ation of options ? ) were discussed in the last section . In contrast , outcome criteria refer to whether , in a particular case , the method facilitate d the achievement of speciﬁc goals ( e . g . , the production of a plan or the generation of a common vision ) . The differenc e be - tween process and outcome criteria can get a little blurred when an explicit goal of an interventi on is , for instance , to facilitate par - ticipatory engagement . Nevertheles s , keeping the distinctio n expli - cit helps us avoid potentially major mistakes like focusing so much on process that we fail to notice that people’s purposes for the interventi on have not been achieved , or focusing so much on out - comes that we miss negative effects of the process on participants . Outcomes may also be longer term in nature , and these are not always predictable or easy to measure ( Duignan and Casswell , 1989 ) . Indeed , making a causal link between an intervention and an outcome that emerges , say , 10 years down the line is often ex - tremely difﬁcult . Long - term follow up studies are needed if some kinds of outcomes ( e . g . , those concerned with sustainability ) are to be properly assessed . The usual means of measuring many short - term outcomes of a method ( other than through personal reﬂections by the researcher ) is by gathering feedback from participants following workshops , often giving them questionnair es to ﬁll in as soon as the worksho p is complete ( e . g . , Duram and Brown , 1999 ; Berry et al . , 2006 ; Sykes and Goodwin , 2007 ; Rouwette , 2011 ) . This is an approach that we have found valuable in our own practice , and we have developed a questionnai re with some sections that are changeabl e from inter - vention to intervention to reﬂect speciﬁc local needs . Other sec - tions are relatively stable and are used repeatedly across a variety of local intervention contexts . Both types of section are use - ful for locally meaningful evaluations , but the latter ( stable ) sec - tions can also yield quantitat ive data for use in longer - term , cross - me thod comparisons . More information about our question - naire is provided below . 4 . Developing an evaluatio n questionnai re Our questionnaire is presented in Appendix A . Importantly , it is not the only tool needed for evaluating systemic PSMs : for instance , it cannot capture data on longer term outcomes . Nevertheles s , it can make a useful contribution by gathering the viewpoints of 148 G . Midgley et al . / European Journal of Operational Research 229 ( 2013 ) 143 – 154 participants on process and short - term outcome s immediately after their involvement in a workshop . The questionnaire has the following sections : 1 . A ﬁve - point scale for the quantitative assessment of usefulness , plus open questions about what people liked and disliked , and what could have been done different ly . Additional open ques - tions reﬂecting local contingencies can be added in here if and when required . 2 . Fourteen questions with ﬁve - point Likert scales enabling the quantitative evaluation of whether certain things have been achieved . Both process and outcome questions are included here , and this is a set of questions that is not tailored to partic - ular interventions ( except occasional words where it is neces - sary to mention that the workshop is focused on water , housing , health , policing , etc . ) . The process we went through to derive this set of questions is discussed below . 3 . Thirteen questions , again with ﬁve - point Likert scales , address - ing potential negative attributes of ( or things that can go wrong when using ) systemic problem structuring methods . Once again this is an unchangi ng set of questions , and our process for deriv - ing them is discussed below . 4 . A set of open ended questions asking respondents to assess the process from their own cultural viewpoints . These questions are usually worded generally so they are relevant to multiple cul - tural perspectives , but speciﬁc questions relating to particular cultures can be added if required ( for example , in New Zealand there often needs to be a speciﬁc focus on Ma ¯ori perspectives ) . 5 . Questions gathering basic demogra phic data ( stakeholder cate - gory , gender , age , etc . ) . 4 . 1 . The development process Our questionnaire was ﬁrst developed in the context of a re - search programme aiming to generate and evaluate new systemic problem structuring methods for use in promoting sustainable re - source use ( Winstanl ey et al . , 2005 ; Hepi et al . , 2008 ) . The adaptable parts of the questionnaire ( Sections 1 , 4 and 5 above ) were relatively straight forward to design , although they re - quired some iterative testing to get them right . The more difﬁcult task was to produce Sections 2 and 3 , which needed to yield data for meaningful use in both local evaluations and longer term com - parisons between methods . Because of the latter , the questions had to be reasonabl y generic . Other authors suggest a number of differ - ent ways of producing generic evaluation criteria , and these have been summarised by Beierle and Konisky ( 2000 ) and Rowe and Frewer ( 2004 ) . A combination of their thinking , plus an addition of our own , suggests that there are six distinct approaches : author - gene rated ( resulting from personal experience ) ; practice - based ( deriving from explicit reﬂections on case studies ) ; theory - based ( evaluating according to the expectati ons one would have if one agreed with a particular theory ) ; literature - based ( deriving from a review of other authors’ work ) ; expert - bas ed ( drawing on the views of an advisory panel ) ; and survey - bas ed ( ﬁnding out from potential participants , either through interview s or a mail survey , what their most widely held expectations are ) . Some authors have combined two or more of the above . We adopted an expert - and literature - based approach , with a couple of author - gene rated questions being added in as well . More details are provided below . We started with a key question : what do we want to measure ? One option was to focus only on criteria that we would expect to be meaningful for all systemic PSMs . This is the approach taken by Bjärås et al . ( 1991 ) and Beierle and Konisky ( 2000 ) . However , while it is useful to identify ‘common denominators’ and assess methods against these , this does not help in evaluating the unique attributes of methods that might make them complemen tary rather than competing . To evaluate these , it is important to look at the set of possible common and divergent attributes that a range of systemic PSMs might exhibit . We therefore set out to identify a number of methodol ogies and methods that could fairly represent the diversity of participative systems approaches . We established a panel of six internati onally known writers on systems thinking , all of whom suggested candi - date methods . We ended up with six systemic PSMs ( soft systems methodol ogy ; interactive planning ; causal loop diagrammin g ; via - ble system diagnosis ; critical systems heuristics ; and strategic assumpti on surfacing and testing ) that all claim to do different things . We then reviewed the literature on these , drawing out a set of attributes that could form the basis for questions to be asked of participants in workshops . We also asked the international panel to suggest their own evaluation criteria , and we added in a couple that were not apparent from the literature review but , in our expe - rience , were important . This list was then sent back to the panel for peer review , resulting in some amendments . We ended up with a set of questions for ﬁeld testing . We note that our questions align well with ﬁve high level crite - ria suggested by Hjortsø ( 2004 ) for the evaluation of PSMs , except that our questions go into much more detail . Hjortsø’s criteria are the extent to which the method is a good ﬁt for the context , and whether it supports ( i ) mutual understand ing ; ( ii ) stakeholder involvem ent in decision making ; ( iii ) the acceptance , transparency and accountability of decision making ; and ( iv ) the collabora tive managemen t of complexity and conﬂict . It is important to declare that we focused only on evaluation criteria relevant to systemic PSMs : we could not assume that cri - teria that have been used to assess other PSMs and participativ e methods would automatical ly be relevant . In making the decision to take this approach , we set aside another research opportunity that is available for others to pick up in future : looking at the lit - erature on group processes and focusing evaluations quite specif - ically on how systemic PSMs enhance these . While there are certainly questions about group process in the questionnaire , these reﬂect the variables that our expert panel and literature re - view suggested were most relevant to the success or otherwise of systemic PSMs : they are not based on wider reading on group process . It is generally accepted ( e . g . , Cavana et al . , 2001 ) that a ques - tionnaire to be employed in an experimental context should be tested for validity ( does it measure what we think it does ? ) and reliability ( does it give consisten t results ? ) . However , for an evalu - ation questionnai re to be employed in the ﬁeld outside the context of experimental studies , usability is just as important , if not more so ( Rowe and Frewer , 2004 ) . Usability means asking whether peo - ple are actually prepared to complete the questionnair e and do so in a sensible manner . Rowe and Frewer ( 2004 ) note that , because compromi ses have to be made in questionnai re design to ensure usability ( e . g . , the questions need to be answerable in 5 – 10 min - utes at the end of a gruelling day ) , usability is often inversely re - lated to validity and reliability ( both of which are enhanced by the generation of more rather than less data ) . This may be the case but , as Rowe et al . ( 2005 ) say , there is no point even beginning to consider validity and reliability if the instrument cannot be used in the ﬁrst place . We agree with Rowe et al . ( 2005 ) that assessing usability has to be a ﬁrst priority , although validity and reliability should not be ignored . At this point in our research , we have tested for usability but ( for reasons to be explained in Section 5 of this pa - per ) the more problemati c task of evaluating validity and reliability has not yet been undertaken . This will be the subject of future research . To check for usability , we ﬁeld tested the questionnair e in ﬁve different interventions , each time making small amendments in G . Midgley et al . / European Journal of Operational Research 229 ( 2013 ) 143 – 154 149 response to issues thrown up by the ways in which people ap - proached the questions : (cid:2) facilitating consultation with land owners and community interest groups as part of a feasibility study for the construction of a new water storage dam ( Winstanley et al . , 2005 ) . (cid:2) working with an Australian NGO and its stakeholders in explor - ing policy options to address the public injecting of illicit drugs ( Midgley et al . , 2005 ) ; (cid:2) facilitating workshops with the police and other stakeholder s in the New Zealand criminal justice system to look at ethical issues associated with anticipated future developmen ts of forensic DNA technologie s ( Baker et al . , 2006 ) ; (cid:2) reviewing the process used by the New Zealand Ministry of Research , Science and Technolo gy to develop ‘roadmap s’ for long - term investments in environment , energy , biotechnology and nanotechnolog y research ( Baker and Midgley , 2007 ) ; and (cid:2) developing a new collaborativ e evaluation approach in partner - ship with regional council staff responsib le for facilitating com - munity engagement in sustainability initiatives ( Hepi et al . , 2008 ) . We also tested the questionnaire on interventi ons undertaken by people other than ourselves : a public meeting and a stakeholder forum convened in two different areas of New Zealand to discuss water shortages . Following observations of participants completing the ﬁrst version of the questionnair e , it was judged to be over - long . We shortened it , but then in later iterations found that omitting some of the questions led to important gaps in our data sets . We ended up ﬁnding a compromise between comprehensive ness and brevity . On our ﬁrst iteration of ﬁeld testing , we also under - took a basic analysis to check that there were no counter - intuitive answers ( which might suggest the misinterpretat ion of a question ) ; that there was no tendency for people to tick the same point on all the scales ( indicating boredom or a lack of comprehens ion ) ; and that similar questions generated similar answers . All these checks proved satisfactory . Having undertaken this series of ﬁeld tests , we are now conﬁdent of the usability of our questionnaire . Nevertheless , we fully acknowledge that it would be possible to further test usability by interviewing partic - ipants on what they were thinking about when they answered the different questions . 4 . 2 . Interpreting data generated through use of the questionnair e Before closing this discussion of our questionnai re , it is impor - tant to note that the quantitative data generate d through it , on pro - cess and short - term outcomes within the context of a particular systemic intervention ( i . e . , a single case study ) , always has to be interpreted in relation to the other aspects of our framework : con - text , purposes , longer - te rm outcome s and researcher skills and preferences . Failure to undertake analyses of these aspects could result in attributions to the method of results that might have had other origins . It is important to note that qualitative informat ion about the purposes of stakeholders , the context , the assumptions embedded in the method and the skills and methodol ogical preferences of the researcher cannot easily be gathered using a standardise d instru - ment like a questionnai re . Therefore , the questionnaire data needs to be considered in a reﬂective worksho p covering all the relevant aspects of method , purposes , context and outcomes . Our normal practice is to bring the research team together with key stakehold - ers , and we use the concepts in our framework ( Fig . 1 ) to structure a dialogue , recording people’s viewpoin ts . By interpreti ng the ﬁnd - ings from the questionnaire in relation to a participative , ‘bigger picture’ , emergent analysis of the use of a method , it is possible to develop a more holistic and nuanced understanding of the per - formance of the method than if questionnair e data alone had been used . Table 1 gives some additional , generic , high - level questions for use in a reﬂective workshop with stakeholder s , going well beyond process and short - term outcome variables assessed through the questionnai re . However , we should note that these offer a guide - line only , as questioni ng needs to be tailored to the speciﬁc context of the intervention that has been undertak en , taking account of the knowled ge and perspectives of stakeholders . For example , it is un - likely that many stakeholder s will have knowled ge of the theoret - ical assumptions embedded in systemic PSMs , so this is something that needs to be considered by the researchers beforehand and then ( if relevant ) they can introduce informat ion about assump - tions into the workshop discussion . Also , in our experience , some stakeholder s are puzzled by questions asking whether a ‘method’ has had any effect ; if complete naivety about methods is antici - pated , the questioni ng can ask about the effects of ‘how the work - shop was run’ ( but then it’s important to ask about both speciﬁc modellin g activities and how the workshop was facilitated , so that the effects of the method and the process of applicati on can be distingui shed ) . Above , we have discussed how our framework can be employed in single case study evaluations of the use of a systemic PSM . How - ever , in making longer term comparis ons of methods using data from multiple case studies , we make the assumption that the more cases are included , the more likely it is that the effects of particular contexts , purposes , etc . , will be evened out . Therefore , the qualita - tive informat ion discussed in the workshops mentioned above , rel - evant primarily to single case studies , can mostly be set aside in favour of statistical analyses of the questionnai re ﬁndings . 5 . Strengths and limitatio ns of the evaluatio n framework and question naire As we see it , this new framework for the evaluation of systemic PSMs has two signiﬁcant strengths . First , by encouraging the explorati on of the context - purpo ses - methods - o utcomes relation - ship in a particular intervention , and by explicitly recognising that the researcher becomes part of the situation that he or she inter - venes in , our framewor k offers a more nuanced ( but still reason - ably parsimon ious ) set of concepts and guidelines to work with than many others in the literature . Second , it incorporate s a ques - tionnaire that can support both locally meaningful evaluations and longer - te rm comparisons between methods , thereby giving us the potential to move beyond the either / or debate that has character - ised the literature in recent years . Neverthel ess , it is important to clarify some of the framework’ s limitatio ns . In our view , the ﬁrst two of these are more or less inev - itable , and have to be managed as part of the evaluation process , while the ﬁnal four indicate the need for further research . Only the ﬁrst limitatio n concerns our framewor k as a whole : the rest re - late solely to the use of the questionnair e for longer - te rm compar - isons between methods : Within the context of a speciﬁc use of a method in a single interventi on , there is scope for the researcher to avoid unwelcome conclusio ns , for example by exaggerating the effect of an aspect of context that was outside his or her control , thereby missing short - falls in either the method or his or her own skill set . To help man - age this , three methodol ogical devices have been built into our framewor k to bring evidence of bad news to the attention of eval - uators , making avoidance more difﬁcult than it might be if the evaluato rs were basing their conclusions on personal reﬂections alone . First , the use of a questionnair e ensures that participant 150 G . Midgley et al . / European Journal of Operational Research 229 ( 2013 ) 143 – 154 voices are available . In particular , the answers to the open ended questions are likely to include the participants ’ own theories about shortcomin gs . Second , by offering guidelines for exploring the con - text that draw upon multiple paradigmati c perspectives , the risk of ‘paradigm blindness ’ ( interpreting the context in the same paradig - matic terms as the method , thereby missing insights that would be apparent from other perspectives ) is minimised ( also see Romm , 1996 ; Midgley , 2011 ) . Third , by explicitly focusing attention on the researcher’s identity , purposes , outcomes , skills and prefer - ences , the framework confronts evaluators with some of the ques - tions that they are most likely to want to avoid . If desired , and if feasible , researchers can go one step further to minimise avoidance by including participa nts on the evaluation team ( preferably ones that are themselv es open to the possibility of receiving bad news ) . The second limitatio n we are aware of , applying to longer term comparisons of methods using the questionnaire , comes from the observation that there is a strong movement advocating methodo - logical pluralism or ‘multi - method ology’ ( e . g . , Flood and Jackson , 1991 ; Jackson , 1991 , 2000 ; Flood and Romm , 1996 ; Mingers and Gill , 1997 ; Midgley , 2000 ) . At its most ﬂexible , a pluralist practice may involve the integration of several previously distinct methods into a new whole , perhaps also incorporating the design of novel elements ( Midgley , 2000 ) . It will be much easier to compare stan - dard sets of methods ( e . g . , those associated with discrete systems methodologi es ) than it will be to compare mixed methods , drawn from different methodologies , that have not been widely applied . The irony here is that the more ﬂexible and responsive that sys - temic problem structuring becomes , the more difﬁcult it will be to evaluate methods over the longer term in a manner that can control for contextual effects . We certainly would not want to see our desire for improved evaluations of methods to result in the stultiﬁcation of pluralist practice . Rather , we suggest that it may be wiser to accept that this limitation will restrict what can be asked of longer term comparis ons between methods , but it will not make them redundant . It will still be possible to compare the sets of methods associate d with well known and widely applied methodol ogies , giving us evidence of their strengths and weak - nesses in relation to the set of attributes that a representat ive range of methods possesses . It will also be possible to compare plu - ralist practice in general with the use of particular discrete ap - proaches . Finally , some mixed methods , if applied in several applicati ons , can also be compared with other sets of methods . There are a number of relatively popular mixes in the literature that will no doubt qualify for evaluation . When comparisons be - tween mixed methods using the questionnaire data look like they will be unreliable because the sample size is too small , it should neverthe less still be possible to facilitate cross - case study learning , where possible bringing together two or more research teams to reﬂect on their practice using our framewor k ( Fig . 1 ) . The third limitation is that we have not yet tested the question - naire for validity and reliability . Rowe et al . ( 2005 ) discuss the sub - stantial difﬁculties in doing this in the ﬁeld because participa nts are often reluctant to ﬁll in two or more questionnaires asking sim - ilar things ( the usual approach to testing for validity being to com - pare with another questionnair e constructed for similar purposes ) . Indeed , in this case , testing for validity will be difﬁcult because there are so few instruments available in the public domain ( e . g . , Halvorse n , 2001 ) , and those that exist are geared to evaluating forms of public participation other than the use of systemic PSMs . Also , checking reliability is even more troublesom e than a validity test because it involves getting participants to ﬁll in the same ques - tionnaire on two separate occasions . Generally speaking , the re - searcher only has access to participants on the day of a worksho p . Our intention is to do some validity and reliability Table 1 Generic questions for adaptation and use in reﬂective workshops bringing together researchers and stakeholders . a Aspect of the framework ( Fig . 1 ) Questions Context What key perspectives , values and assumptions were participants bringing in , and how did these affect discussion ? Were there signiﬁcant processes of marginalization or exclusion of people and / or issues ? What organizations , institutions , economic conditions and ecological factors inﬂuenced the perspectives that people came in with ? Did people feel enabled or constrained by wider systems , and what effects did this have ? Researcher identity How was the researcher seen by themselves and others , and why ? Purposes What openly expressed and hidden purposes did different people have for participating ? Which purposes were met and not met , and what were the effects ? Were there conﬂicting purposes ( or people thinking others had hidden agendas ) , and what were the effects ? Researcher purposes What purposes did the researcher have ? Was there any conﬂict between the participants’ and researcher’s purposes ? Did the participants trust the expressed purposes of the researcher ? Methods What theoretical assumptions made by the methods might have been inﬂuential ? What cultural norms did the methods assume , and how did these relate to the culture ( s ) of the participants ? Did the process facilitate effective participation ? Did the process help people to think systemically ? ( Different theoretical understandings of what it means to think systemically , such as appreciating other perspectives and getting a ‘bigger picture’ understanding , have informed some of the questions in the questionnaire ) Researcher skills and preferences What preferences ( for methodologies , methods and processes of application ) did the researcher have , and what were the effects ? What other skills , resources and competencies did the researcher bring in ( or not ) , and what were the effects ? Outcomes What plans , actions or changes were achieved ? Have longer - term outcomes been achieved , and can these be linked back to the use of the method ? ( This can only be asked in the context of a longer - term follow up ) How do the outcomes relate to people’s purposes ? What outcomes ( positive and negative ) were anticipated or unanticipated ? Researcher outcomes What outcomes were achieved by the researcher ? What was the ﬁt between the researcher’s outcomes and the outcomes for the participants and those experiencing wider effects ? a Information from the questionnaire results can inform answers to some of these questions . G . Midgley et al . / European Journal of Operational Research 229 ( 2013 ) 143 – 154 151 testing in due course when a good compara tive instrument can be identiﬁed and the testing can be added to an interventi on without difﬁculty . The fourth limitatio n we have identiﬁed concerns the inability of standard metrics , such as those to be found in sections two and three of our questionnair e , to pick up novelty : they can only evaluate against already established criteria . This is arguably one of the most signiﬁcant limitations in terms of conducting longer - term research based on multiple case studies : it appears that , after around 20 years of relative stability in the number of systemic PSMs that are widely used in practice , systems / OR practitioners are now producing a new generation of methodologies and meth - ods ( Rosenhead and Mingers , 2004 ; Shaw et al . , 2006 ; Franco et al . , 2007 ) , and it is important that the questionnaire does not go out of date . Our solution to this problem , which will need to be enacted as part of a longer term internati onal research program , will be to undertake a review of the questionnaire after a set period of data collection . This period will need to be long enough to allow sufﬁ - cient data to be gathered on the application of well established ap - proaches . Periodic reviews of the questionnaire followed by new data collection should enable a balance to be struck between sta - bility ( to facilitate robust comparisons ) and change ( to keep the longer term comparisons open to novelty ) . The ﬁfth limitation is that our questionnaire does not currently allow the comparison of systemic PSMs and non - particip ative modelling methods . Although we would ideally like to extend our research to include the latter , it may not be feasible to inte - grate questions about both types of method into a single instru - ment . Our ﬁeld testing suggests that we have already hit the upper limit for the number of questions people are willing to an - swer , so feasibility would depend on reducing the number of ques - tions about systemic PSMs in order to allow others to be included . The sixth and ﬁnal limitation we face is that no one group of researchers will be able to collect sufﬁcient data on its own to en - able the robust , longer term comparison of methods . International collaboratio n will therefore be essential , and we have made a start in moving towards this by establishi ng collaborative arrangements with over 80 systems / OR practitioners in 22 countries who are willing to test our evaluation framework and questionnaire in practice . 6 . Conclusions In this paper we have offered a new framewor k for evaluating systemic problem structuring methods , focusing on the context - purposes - meth ods - outcom es relationship . This framework can be used in an emergent mode , and it asks researchers to view them - selves as active contributors to the success or failure of a meth - od - in - conte xt . We have also reported on the development of a questionnair e to gather data from participants that can be of use in reﬂecting on the strengths and weaknesses of methods . The same data may be useful for both evaluations of methods in single case studies and longer term comparisons between methods using information from multiple cases . However , undertaking longer term comparisons will require a new , international research pro - gram , which is currently under development . Acknowled gements We would like to acknowledge the support of colleagues at the Institute of Environm ental Science and Research ( ESR ) Ltd . , New Zealand , who helped to ﬁeld test the questionnair e in the context of their projects : Virginia Baker , Jan Gregor , Wendy Gregory , Maria Hepi , Miria Lange , Johanna Veth and Ann Winstanley . We would also like to recognise the contributi on made by our internati onal panel of experts on systemic methods : John Brocklesby ( Victoria Universit y of Wellington , New Zealand ) , José Córdoba ( Royal Hollo - way University , UK ) , Amanda Gregory ( University of Hull , UK ) , John Mingers ( University of Kent , UK ) , Leroy White ( University of Bris - tol , UK ) and Jennifer Wilby ( University of Hull , UK ) . Furthermore , we would like to express our appreciation for the comments and feedback received following presentations at the annual confer - ences of the International Society for the Systems Sciences ( Tokyo , Japan , 2007 ) ; the Australia and New Zealand Systems Society ( Perth , Australia , 2008 ) ; and the Hellenic Society for Systemic Studies ( Athens , Greece , 2011 ) . Finally , we wish to acknowledge funding from various sources that enabled the research reported in this paper : the Foundation for Research , Science and Technolo gy , New Zealand ( Contracts C03X0304 and C03X0305 ) ; the Colonial Foundati on Trust , Australia ; and the Ministry for Research , Science and Technology , New Zealand . However , the interpretations and opinions expresse d in this paper are the authors’ own . Appendi x A . Supplementar y material The evaluation questionnaire associate d with this article can be found , in the online version , at http : / / dx . doi . org / 10 . 1016 / j . ejor . 2013 . 01 . 0 47 . References Adams , R . , McCullough , A . , 2003 . The urban practitioner and participation in research within a streetwork context . Community , Work & Family 6 , 269 – 287 . Alberts , D . J . , 2007 . Stakeholders or subject matter experts , who should be consulted ? Energy Policy 35 , 2336 – 2346 . Allsop , J . , Taket , A . , 2003 . Evaluating user involvement in primary healthcare . International Journal of Healthcare Technology & Management 5 , 34 – 44 . Alrøe , H . F . , 2000 . Science as systems learning : some reﬂections on the cognitive and communicational aspects of science . Cybernetics and Human Knowing 7 , 57 – 78 . Baker , V . , Gregory , W . , Midgley , G . , Veth , J . , 2006 . Ethical Implications and Social Impacts of Forensic DNA Technologies and Applications : Summary Report . Institute of Environmental Science and Research ( ESR ) Ltd . , Christchurch . Baker , V . , Midgley , G . , 2007 . Review of the MoRST Roadmaps Exercise : Final Report . Conﬁdential ESR Client Report . Institute of Environmental Science and Research ( ESR ) Ltd . , Wellington . Bateson , G . , 1970 . Form , substance , and difference . General Semantics Bulletin 37 , 5 – 13 . Beierle , T . C . , Cayford , J . , 2002 . Democracy in Practice : Public Participation in Environmental Decisions . RFF Press , Washington , DC . Beierle , T . C . , Konisky , D . M . , 2000 . Values , conﬂict , and trust in participatory environmental planning . Journal of Policy Analysis and Management 19 , 587 – 602 . Berry , H . , Bowman , S . R . , Hernandez , R . , Pratt , C . , 2006 . Evaluation tool for community development coalitions . Journal of Extension 44 , http : / / www . joe . org / joe / 2006december / tt2 . shtml ( accessed : 30 . 03 . 07 ) . Bjärås , G . , Haglund , B . J . A . , Rifkin , S . B . , 1991 . A new approach to community participation assessment . Health Promotion International 6 , 199 – 206 . Branch , K . M . , Bradbury , J . A . , 2006 . Comparison of DOE and Army advisory boards : application of a conceptual framework for evaluating public participation in environmental risk decision making . Policy Studies Journal 34 , 723 – 753 . Brocklesby , J . , 1997 . Becoming multimethodology literate : an assessment of the cognitive difﬁculties of working across paradigms . In : Mingers , J . , Gill , A . ( Eds . ) , Multimethodology : The Theory and Practice of Combining Management Science Methodologies . Wiley , Chichester . Brocklesby , J . , 2009 . Ethics beyond the model : how social dynamics can interfere with ethical practice in operational research / management science . Omega , the International Journal of Management Science 37 , 1073 – 1082 . Bryant , J . W . , Darwin , J . A . , 2004 . Exploring inter - organisational relationships in the health service : an immersive drama approach . European Journal of Operational Research 152 , 655 – 666 . Burns , D . , 2007 . Systemic Action Research : A Strategy for Whole System Change . Policy Press , Bristol . Buysse , V . , Wesley , P . , Skinner , D . , 1999 . Community development approaches for early intervention . Topics in Early Childhood Special Education 19 , 236 – 243 . Cavana , R . Y . , Delahaye , B . L . , Sekaran , U . , 2001 . Applied Business Research : Qualitative and Quantitative Methods . Wiley , Brisbane . Champion , D . , Wilson , J . M . , 2010 . The impact of contingency factors on validation of problem structuring methods . Journal of the Operational Research Society 61 , 1420 – 1431 . Charnley , S . , Engelbert , B . , 2005 . Evaluating public participation in environmental decision - making : EPA’s superfund community involvement program . Journal of Environmental Management 77 , 165 – 182 . 152 G . Midgley et al . / European Journal of Operational Research 229 ( 2013 ) 143 – 154 Checkland , P . , 1981 . Systems Thinking . Systems Practice . Wiley , Chichester . Checkland , P . B . , Forbes , P . , Martin , S . , 1990 . Techniques in soft systems practice . Part 3 : Monitoring and control in conceptual models and in evaluation studies . Journal of Applied Systems Analysis 17 , 29 – 37 . Checkland , P . , Scholes , J . , 1990 . Soft Systems Methodology in Action . Wiley , Chichester . Checkland , P . , Poulter , J . , 2006 . Learning for Action . Wiley , Chichester . Chess , C . , Purcell , K . , 1999 . Public participation and the environment : do we know what works ? Environmental Science & Technology 33 , 2685 – 2692 . Churchman , C . W . , 1970 . Operations research as a profession . Management Science 17 , B37 – 53 . Clayton , A . M . H . , Radcliffe , N . J . , 1996 . Sustainability : A Systems Approach . Earthscan , London . Cole , M . , 2006 . Evaluating the impact of community appraisals : some lessons from South - West England . Policy & Politics 34 , 51 – 68 . Connell , N . A . D . , 2001 . Evaluating soft OR : some reﬂections on an apparently ‘unsuccessful’ implementation using a soft systems methodology ( SSM ) based approach . Journal of the Operational Research Society 52 , 150 – 160 . Delaney , M . M . , Foroughi , A . , Perkins , W . C . , 1997 . An empirical study of the efﬁcacy of a computerized negotiation support system ( NSS ) . Decision Support Systems 20 , 185 – 197 . De Vreede , G . , Dickson , G . , 2000 . Using GSS to design organizational processes and information systems : An action research study on collaborative business engineering . Group Decision and Negotiation 9 , 161 – 183 . Douglas , M . , 1986 . How Institutions Think . Routledge and Kegan Paul , London . Duignan , P . , Casswell , S . , 1989 . Evaluating community development programs for health promotion : problems illustrated by a New Zealand example . Community Health Studies 13 , 74 – 81 . Duram , L . A . , Brown , K . G . , 1999 . Assessing public participation in U . S . watershed planning initiatives . Society & Natural Resources 12 , 455 – 467 . Eden , C . , 1995 . On evaluating the performance of ‘wide - band’ GDSS . European Journal of Operational Research 81 , 302 – 311 . Eden , C . , Ackermann , F . , 1996 . ‘‘Horses for courses’’ : a stakeholder approach to the evaluation of GDSSs . Group Decision and Negotiation 5 , 501 – 519 . Eden , C . , Ackermann , F . , 2006 . Where next for problem structuring methods . Journal of the Operational Research Society 57 , 766 – 768 . Eden , C . , Sims , D . , 1979 . On the nature of problems in consulting practice . Omega 7 , 119 – 127 . Entwistle , V . , Buchan , H . , Coulter , A . , Jadad , A . , 1999 . Towards constructive innovation and rigorous evaluation : a new series on methods for promoting and evaluating participation . Health Expectations 2 , 75 – 77 . Er , M . C . , Ng , A . C . , 1995 . The anonymity and proximity factors in group decision support systems . Decision Support Systems 14 , 75 – 83 . Fan , S . , Shen , Q . , Lin , G . , 2007 . Comparative study of idea generation between traditional value management workshops and GDSS - supported workshops . Journal of Construction Engineering and Management 133 , 816 – 825 . Fjermestad , J . , 2004 . An analysis of communication mode in group support systems research . Decision Support Systems 37 , 239 – 263 . Fjermestad , J . , Hiltz , S . , 1998 . An assessment of group support systems experimental research : methodology and results . Journal of Management Information Systems 15 , 7 – 149 . Flood , R . L . , 1995 . Solving Problem Solving . Wiley , Chichester . Flood , R . L . , Jackson , M . C . ( Eds . ) , 1991 . Critical Systems Thinking : Directed Readings . Wiley , Chichester . Flood , R . L . , Romm , N . R . A . ( Eds . ) , 1996 . Critical Systems Thinking : Current Research and Practice . Plenum , New York . Forrester , J . W . , 1969 . Principles of Systems . Wright - Allen Press , Cambridge , MA . Franco , L . A . , 2006 . Forms of conversation and problem structuring methods : a conceptual development . Journal of the Operational Research Society 57 , 813 – 821 . Franco , L . A . , 2007 . Assessing the impact of problem structuring methods in multi - organizational settings : an empirical investigation . Journal of the Operational Research Society 58 , 760 – 768 . Franco , L . A . , Shaw , D . , Westcombe , M . , 2007 . Taking problem structuring methods forward . Journal of the Operational Research Society 58 , 545 – 546 . Gallupe , R . B . , Dennis , A . R . , Cooper , W . H . , Valacich , J . S . , Bastianutti , L . M . , Nunamaker , J . F . , 1992 . Electronic brainstorming and group size . Academy of Management Journal 35 , 350 – 369 . Gopal , A . , Prasad , P . , 2000 . Understanding GDSS in symbolic context : shifting the focus from technology to interaction . MIS Quarterly 24 , 509 – 546 . Halvorsen , K . E . , 2001 . Assessing public participation techniques for comfort , convenience , satisfaction , and deliberation . Environmental Management 28 , 179 – 186 . Hepi , M . , Foote , J . , Ahuriri - Driscoll , A . , 2008 . Guidelines for Developing Resource Care Evaluation Criteria and Methods . Institute of Environmental Science and Research ( ESR ) Ltd . , Christchurch . Hjortsø , C . N . , 2004 . Enhancing public participation in natural resource management using soft OR : an application of strategic option development and analysis in tactical forest planning . European Journal of Operational Research 152 , 667 – 683 . Ho , C . H . , 1997 . A Critical Process for the Evaluation of Methodology . Ph . D . Thesis , University of Hull . Jackson , M . C . , 1991 . Systems Methodology for the Management Sciences . Plenum , New York . Jackson , M . C . , 2000 . Systems Approaches to Management . Plenum / Kluwer , New York . Jackson , M . C . , 2006 . Beyond problem structuring methods : reinventing the future of OR / MS . Journal of the Operational Research Society 57 , 868 – 878 . Jackson , M . C . , Keys , P . , 1984 . Towards a system of systems methodologies . Journal of the Operational Research Society 35 , 473 – 486 . Jenkins , N . T . , Bennett , M . I . J . , 1999 . Toward an empowerment zone evaluation . Economic Development Quarterly 13 , 23 – 28 . Joldersma , C . , Roelofs , E . , 2004 . The impact of soft OR - methods on problem structuring . European Journal of Operational Research 152 , 696 – 708 . Kelly , K . , Van Vlaenderen , H . , 1995 . Evaluating participation processes in community development . Evaluation & Program Planning 18 , 371 – 383 . Keys , P . , 1994 . Understanding the Process of Operational Research : Collected Readings . Wiley , Chichester . Li , X . , Zheng , H . , 1995 . Study on general systems methodology . In : Midgley , G . , Wilby , J . ( Eds . ) , Systems Methodology : Possibilities for Cross - Cultural Learning and Integration . Centre for Systems Studies , Hull . Luhmann , N . , 1986 . Ecological Communication . University of Chicago Press , Chicago . Maani , K . E . , Cavana , R . Y . , 2007 . Systems Thinking , System Dynamics , second ed . Pearson New Zealand , Auckland . Margerum , R . D . , 2002 . Collaborative planning : building consensus and building a distinct model for practice . Journal of Planning Education & Research 21 , 237 – 253 . Masozera , M . K . , Alavalapati , J . R . R . , Jacobson , S . K . , Shrestha , R . K . , 2006 . Assessing the suitability of community - based management for the Nyungwe forest reserve , Rwanda . Forest Policy & Economics 8 , 206 – 216 . McAllister , K . , 1999 . Understanding Participation : Monitoring and Evaluating Process , Outputs and Outcomes . Working Paper 2 , IDRC , Ottawa . McCartt , A . T . , Rohrbaugh , J . , 1995 . Managerial openness to change and the introduction of GDSS : explaining initial success and failure in decision conferencing . Organization Science 6 , 569 – 584 . McGurk , B . , Sinclair , A . J . , Diduck , A . , 2006 . An assessment of stakeholder advisory committees in forest management : case studies from Manitoba , Canada . Society & Natural Resources 19 , 809 – 826 . McKay , J . , 1998 . Using cognitive mapping to achieve shared understanding in information requirements determination . Australian Computer Journal 30 , 139 – 145 . Midgley , G . , 1994 . Ecology and the poverty of humanism : a critical systems perspective . Systems Research 11 , 67 – 76 . Midgley , G . , 2000 . Systemic Intervention : Philosophy , Methodology , and Practice . Plenum / Kluwer , New York . Midgley , G . ( Ed . ) , 2003 . Systems Thinking , vols . I to IV . Sage , London . Midgley , G . , 2011 . Theoretical pluralism in systemic action research . Systemic Practice and Action Research 24 , 1 – 15 . Midgley , G . , Ahuriri - Driscoll , A . , Baker , V . , Foote , J . , Hepi , M . , Taimona , H . , Rogers - Koroheke , M . , Gregor , J . , Gregory , W . , Lange , M . , Veth , J . , Winstanley , A . , Wood , D . , 2007 . Practitioner identity in systemic intervention : reﬂections on the promotion of environmental health through Ma ¯ori community development . Systems Research and Behavioral Science 24 , 233 – 247 . Midgley , G . , Winstanley , A . , Gregory , W . , Foote , J . , 2005 . Scoping the Potential Uses of Systems Thinking in Developing Policy on Illicit Drugs . Drug Policy Modelling Project Research Memorandum # 13 . Turning Point , Melbourne . Mingers , J . C . , 1997 . Towards critical pluralism . In : Mingers , J . , Gill , A . ( Eds . ) , Multimethodology : The Theory and Practice of Combining Management Science Methodologies . Wiley , Chichester . Mingers , J . , Brocklesby , J . , 1997 . Multimethodology : towards a framework for mixing methodologies . Omega , the International Journal of Management Science , 25 , 489 – 509 . Mingers , J . , Gill , A . ( Eds . ) , 1997 . Multimethodology : The Theory and Practice of Combining Management Science Methodologies . Wiley , Chichester . Mingers , J . , Rosenhead , J . , 2004 . Problem structuring methods in action . European Journal of Operational Research 152 , 530 – 554 . Montazemi , A . R . , Wang , F . , Nainar , S . M . K . , Bart , C . K . , 1996 . On the effectiveness of decisional guidance . Decision Support Systems 18 , 181 – 198 . Morgan , L . M . , 2001 . Community participation in health : perpetual allure , persistent challenge . Health Policy & Planning 16 , 221 – 230 . Murphy - Berman , V . , Schnoes , C . , Chambers , J . M . , 2000 . An early stage evaluation model for assessing the effectiveness of comprehensive community initiatives : three case studies in Nebraska . Evaluation & Program Planning 23 , 157 – 163 . Nunamaker , J . F . , Dennis , A . R . , Valacich , J . S . , Vogel , D . R . , George , J . F . , 1991 . Electronic meeting systems to support group work . Communications of the ACM 34 , 43 – 61 . Ong , B . N . , 2000 . Assessing the context for partnerships between communities and the National Health Service in England . Critical Public Health 10 , 343 – 351 . Paterson , J . , Teubner , G . , 1998 . Changing maps : empirical legal autopoiesis . Social and Legal Studies 7 , 451 – 486 . Pettigrew , A . M . , 1987 . Context and action in the transformation of the ﬁrm . Journal of Management Studies 24 , 649 – 670 . Phahlamohlaka , J . , Friend , J . , 2004 . Community planning for rural education in South Africa . European Journal of Operational Research 152 , 684 – 695 . Pinsonneault , A . , Barki , H . , Gallupe , R . B . , Hoppen , M . , 1999 . Electronic brainstorm : the illusion of productivity . Information Systems Research 10 , 110 – 132 . Pinsonneault , A . , Kraemer , K . L . , 1990 . The effects of electronic meetings on group processes and outcomes : an assessment of the empirical research . European Journal of Operational Research 46 , 143 – 161 . G . Midgley et al . / European Journal of Operational Research 229 ( 2013 ) 143 – 154 153 Romm , N . R . A . , 1996 . Inquiry - and - intervention in systems planning : probing methodological rationalities . World Futures 47 , 25 – 36 . Rosenhead , J . , 1989 . Rational Analysis for a Problematic World . Wiley , Chichester . Rosenhead , J . , 2006 . Past , present and future of problem structuring methods . Journal of the Operational Research Society 57 , 759 – 765 . Rosenhead , J . , Mingers , J . , 2001 . Rational Analysis for a Problematic World Revisited . Wiley , Chichester . Rosenhead , J . , Mingers , J . , 2004 . Problem structuring methods in action . European Journal of Operational Research 152 , 530 – 554 . Rouwette , E . A . J . A . , 2011 . Facilitated modelling in strategy development : measuring the impact on communication , consensus and commitment . Journal of the Operational Research Society 62 , 879 – 887 . Rouwette , E . , Bastings , I . , Blokker , H . , 2011 . A comparison of group model building and strategic options development and analysis . Group Decision and Negotiation 20 , 781 – 803 . Rouwette , E . A . J . A . , Vennix , J . A . M . , Felling , A . J . A . , 2009 . On evaluating the performance of problem structuring methods : an attempt at formulating a conceptual model . Group Decision and Negotiation 18 , 567 – 587 . Rowe , G . , Frewer , L . J . , 2000 . Public participation methods : a framework for evaluation . Science , Technology & Human Values 25 , 3 – 29 . Rowe , G . , Frewer , L . J . , 2004 . Evaluating public participation exercises : a research agenda . Science , Technology & Human Values 29 , 512 – 556 . Rowe , G . , Horlick - Jones , T . , Walls , J . , Pidgeon , N . , 2005 . Difﬁculties in evaluating public engagement initiatives : reﬂections on an evaluation of the UK GM Nation ? public debate about transgenic crops . Public Understanding of Science 14 , 331 – 352 . Rowe , G . , Marsh , R . , Frewer , L . J . , 2004 . Evaluation of a deliberative conference . Science , Technology & Human Values 29 , 88 – 121 . Shaw , D . , 2003 . Evaluating electronic workshops through analysing the ‘brainstormed’ ideas . Journal of the Operational Research Society 54 , 692 – 705 . Shaw , D . , Franco , A . , Westcombe , M . , 2006 . Problem structuring methods : new directions in a problematic world . Journal of the Operational Research Society 57 , 757 – 758 . Shaw , I . , 1999 . Qualitative Evaluation . Sage , London . Shen , C - Y . , Midgley , G . , 2007 . Toward a buddhist systems methodology 1 : comparisons between buddhism and systems theory . Systemic Practice and Action Research 20 , 167 – 194 . Sieber , R . , 2006 . Public participation geographic information systems : a literature review and framework . Annals of the Association of American Geographers 96 , 491 – 507 . Smith , L . T . ( Ed . ) , 1999 . Decolonizing Methodologies : Research and Indigenous Peoples . Zed Books , London . Sørensen , L . , Vidal , R . , Engström , E . , 2004 . Using soft OR in a small company – the case of Kirby . European Journal of Operational Research 152 , 555 – 570 . Spash , C . L . , 1997 . Ethics and environmental attitudes with implications for economic valuation . Journal of Environmental Management 50 , 403 – 416 . Sykes , C . , Goodwin , W . , 2007 . Assessing patient , carer and public involvement in health care . Quality in Primary Care 15 , 45 – 52 . Taket , A . , White , L . , 2000 . Partnership and Participation : Decision - making in the Multi - Agency Setting . Wiley , Chichester . Tuler , S . , Webler , T . , Finson , R . , 2005 . Competing perspectives on public involvement : planning for risk characterization and risk communication about radiological contamination from a national laboratory . Health , Risk & Society 7 , 247 – 266 . Ulrich , W . , 1994 . Critical Heuristics of Social Planning : A New Approach to Practical Philosophy . Wiley , Chichester . Valacich , J . S . , Schwenk , C . , 1995a . Devil’s advocacy and dialectical inquiry effects on face - to - face and computer - mediated group decision making . Organizational Behavior and Human Decision Processes 63 , 158 – 173 . Valacich , J . S . , Schwenk , C . , 1995b . Structuring conﬂictin individual , face - to - face , and computer - mediated group decision making : carping versus objective devil’s advocacy . Decision Sciences 26 , 369 – 393 . Vennix , J . A . M . , 1996 . Group Model Building . Wiley , Chichester . Warburton , D . , Wilson , R . , Rainbow , E . , 2007 . Making a Difference : A Guide to Evaluating Public Participation in Central Government . Involve , London , http : / / www . involve . org . uk / evaluation ( accessed : 30 . 05 . 07 ) . White , L . , 2006 . Evaluating problem - structuring methods : developing an approach to show the value and effectiveness of PSMs . Journal of the Operational Research Society 57 , 842 – 855 . Winstanley , A . , Baker , V . , Foote , J . , Gregor , J . , Gregory , W . , Hepi , M . , Midgley , G . , 2005 . Water in the Waimea Basin : Community Values and Water Management Options . Institute of Environmental Science and Research ( ESR ) Ltd . , Christchurch . Yearley , S . , 2006 . Bridging the science - policy divide in urban air - quality management : evaluating ways to make models more robust through public engagement . Environment and Planning C 24 , 701 – 714 . Zhang , J . , Smith , R . , Watson , R . B . , 1997 . Towards computer support of the soft systems methodology : an evaluation of the functionality and usability of an SSM toolkit . European Journal of Information Systems 6 , 129 – 139 . Zhu , Z . , 2000 . Dealing with a differentiated whole : the philosophy of the WSR approach . Systemic Practice and Action Research 13 , 21 – 57 . 154 G . Midgley et al . / European Journal of Operational Research 229 ( 2013 ) 143 – 154