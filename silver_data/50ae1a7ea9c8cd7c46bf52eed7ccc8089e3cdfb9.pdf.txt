Complex Momentum for Optimization in Games Jonathan Lorraine 1 , 2 David Acuna 1 , 2 , 3 Paul Vicol 1 , 2 David Duvenaud 1 , 2 University of Toronto 1 Vector Institute 2 NVIDIA 3 { lorraine , davidj , pvicol , duvenaud } @ cs . toronto . edu Abstract We generalize gradient descent with momentum for optimization in differentiable games to have complex - valued momentum . We give theoretical motivation for our method by proving convergence on bilinear zero - sum games for simultaneous and alternating updates . Our method gives real - valued parameter updates , making it a drop - in replacement for standard optimizers . We empirically demonstrate that complex - valued momentum can improve convergence in realistic adversarial games—like generative adversarial networks—by showing we can ﬁnd better solutions with an almost identical computational cost . We also show a practical generalization to a complex - valued Adam variant , which we use to train BigGAN to better inception scores on CIFAR - 10 . 1 Introduction Gradient - based optimization has been critical for the success of machine learning , updating a single set of parameters to minimize a single loss . A growing number of applications require learning in games , which generalize single - objective optimization . Common examples are GANs [ 1 ] , actor - critic models [ 2 ] , curriculum learning [ 3 – 5 ] , hyperparameter optimization [ 6 – 9 ] , adversarial examples [ 10 , 11 ] , learning models [ 12 – 14 ] , domain adversarial adaptation [ 15 ] , neural architecture search [ 16 , 17 ] , and meta - learning [ 18 , 19 ] . Games consist of multiple players , each with parameters and objectives . We often want solutions where no player gains from changing their strategy unilaterally , e . g . , Nash equilibria [ 20 ] or Stack - elberg equilibria [ 21 ] . Classical gradient - based learning often fails to ﬁnd these equilibria due to rotational dynamics [ 22 ] . Numerous saddle point ﬁnding algorithms for zero - sum games have been proposed [ 23 , 24 ] . [ 25 ] generalizes GD with momentum to games , showing we can use a negative momentum to converge if the eigenvalues of the Jacobian of the gradient vector ﬁeld have a large imaginary part . We use the terminology in [ 25 ] and say ( purely ) cooperative or adversarial games for games with ( purely ) real or imaginary eigenvalues . Setups like GANs are not purely adversarial , but rather have both purely cooperative and adversarial eigenspaces – i . e . , eigenspaces with purely real or imaginary eigenvalues . In cooperative eigenspaces , the players do not interfere with each other . We want solutions that converge with simultaneous and alternating updates in purely adversarial games – a setup where existing momentum methods fail . Also , we want solutions that are robust to different mixtures of adversarial and cooperative eigenspaces , because this depends on the games eigendecomposition which can be intractable . To solve this we unify and generalize existing momen - tum methods [ 26 , 25 ] to recurrently linked momentum – a setup with multiple recurrently linked momentum buffers with potentially negative coefﬁcients shown in Figure 2c . We show that selecting two of these recurrently linked buffers with appropriate momentum coef - ﬁcients can be interpreted as the real and imaginary parts of a single complex buffer and complex momentum coefﬁcient – see Figure 2d . This setup ( a ) allows us to converge in adversarial games with simultaneous updates , ( b ) only introduces one new optimizer parameter – the phase or arg of our momentum , ( c ) allows us to gain intuitions via complex analysis , ( d ) is trivial to implement in libraries supporting complex arithmetic , and ( e ) robustly converges for different eigenspace mixtures . Intuitively , our complex buffer stores historical gradient information , oscillating between adding or subtracting at a frequency dictated by the momentum coefﬁcient . Classical momentum only adds gradients , and negative momentum changes between adding or subtracting each iteration , while we oscillate at an arbitrary ( ﬁxed ) frequency – see Figure 4a . This reduces rotational dynamics during training by canceling out opposing updates . a r X i v : 2102 . 08431v2 [ c s . L G ] 1 J un 2021 Contributions • We provide generalizations and variants of classical [ 27 – 29 ] , negative [ 25 , 30 ] , and aggre - gated [ 26 ] momentum for learning in differentiable games . • We show our methods converges on adversarial games – including bilinear zero - sum games and the Dirac - GAN – with simultaneous and alternating updates . • We illustrate a robustness during optimization , converging faster and over a larger range of mixtures of cooperative and adversarial games than existing ﬁrst - order methods . • We give a practical extension of our method to a complex - valued Adam [ 31 ] variant , which we use to train a BigGAN [ 32 ] on CIFAR - 10 , improving [ 32 ] ’s inception scores . 2 Background Actual JAX implementation : changes in green mass = . 8 + . 3j def momentum ( step _ size , mass ) : . . . def update ( i , g , state ) : x , velocity = state velocity = mass * velocity + g x = x - jnp . real ( step _ size ( i ) * velocity ) return x , velocity . . . Figure 1 : How to modify JAX’s SGD with momentum here to use complex momentum . The only changes are in green . jnp . real gets the real part of step _ size times the momentum buffer ( called velocity here ) . We use a complex mass for our method in this case β “ | β | exp p i arg p β qq “ 0 . 9 exp p i π { 8 q « . 8 ` . 3 i . Appendix Table 2 summarizes our nota - tion . Consider the optimization problem : θ ˚ : “ arg min θ L p θ q ( 1 ) We can ﬁnd local minima of loss L using ( stochastic ) gradient descent with step size α . We denote the loss gradient at param - eters θ j by g j : “ g p θ j q : “ ∇ θ L p θ q | θ j . θ j ` 1 “ θ j ´ α g j ( SGD ) Momentum can generalize SGD . For ex - ample , Polyak’s Heavy Ball [ 27 ] : θ j ` 1 “ θ j ´ α g j ` β p θ j ´ θ j ´ 1 q ( 2 ) Which can be equivalently written with momentum buffer µ j “ p θ j ´ θ j ´ 1 q { α . µ j ` 1 “ β µ j ´ g j , θ j ` 1 “ θ j ` α µ j ` 1 ( SGDm ) We can also generalize SGDm to aggregated momentum [ 26 ] , shown in Appendix Algorithm 3 . 2 . 1 Game Formulations Another class of problems is learning in games , which includes problems like generative adversarial networks ( GANs ) [ 1 ] . We focus on 2 - player games —with players denoted by A and B —where each player minimizes their loss L A , L B with their parameters θ A P R d A , θ B P R d B . Solutions to 2 - player games – which are assumed unique for simplicity – can be deﬁned as : θ ˚ A : “ arg min θ A L A p θ A , θ ˚ B q , θ ˚ B : “ arg min θ B L B p θ ˚ A , θ B q ( 3 ) In deep learning , losses are non - convex with many parameters , so we often focus on ﬁnding local solutions . If we have a player ordering , then we have a Stackelberg game . For example , in GANs , the generator is the leader , and the discriminator is the follower . In hyperparameter optimization , the hyperparameters are the leader , and the network parameters are the follower . If θ ˚ B p θ A q denotes player B ’s best - response function , then Stackelberg game solutions can be deﬁned as : θ A ˚ : “ arg min θ A L A p θ A , θ ˚ B p θ A qq , θ ˚ B p θ A q : “ arg min θ B L B p θ A , θ B q ( 4 ) If L A and L B are differentiable in θ A and θ B we say the game is differentiable . We may be able to approximately ﬁnd θ ˚ A efﬁciently if we can do SGD on : L ˚ A p θ A q : “ L A p θ A , θ ˚ B p θ A qq ( 5 ) Unfortunately , SGD would require computing d L ˚ A { d θ A , which often requires d θ ˚ B { d θ A , but θ ˚ B p θ A q and its Jacobian are typically intractable . A common optimization algorithm to analyze for ﬁnding solu - tions is simultaneous SGD ( SimSGD ) – sometimes called gradient descent ascent for zero - sum games – where g jA : “ g A p θ jA , θ jB q and g jB : “ g B p θ jA , θ jB q are estimators for ∇ θ A L A | θ jA , θ jB and ∇ θ B L B | θ jA , θ jB : θ j ` 1 A “ θ jA ´ α g jA , θ j ` 1 B “ θ jB ´ α g jB ( SimSGD ) We simplify notation with the concatenated or joint - parameters ω : “r θ A , θ B sP R d and the joint - gradient vector ﬁeld ˆ g : R d Ñ R d , which at the j th iteration is the joint - gradient denoted : ˆ g j : “ ˆ g p ω j q : “ r g A p ω j q , g B p ω j qs “ r g jA , g jB s ( 6 ) We extend to n - player games by treating ω and ˆ g as concatenations of the players’ parameters and loss gradients , allowing for a concise expression of the SimSGD update with momentum ( SimSGDm ) : µ j ` 1 “ β µ j ´ ˆ g j , ω j ` 1 “ ω j ` α µ j ` 1 ( SimSGDm ) 2 [ 25 ] show classical momentum choices of β P r 0 , 1 q do not improve solution speed over SimSGD in some games , while negative momentum helps if the Jacobian of the joint - gradient vector ﬁeld ∇ ω ˆ g has complex eigenvalues . Thus , for purely adversarial games with imaginary eigenvalues , any non - negative momentum and step size will not converge . For cooperative games – i . e . , minimization – ∇ ω ˆ g has strictly real eigenvalues because it is a losses Hessian , so classical momentum works well . 2 . 2 Limitations of Existing Methods Higher - order : Methods using higher - order gradients are often harder to parallelize across GPUs , [ 33 ] , get attracted to bad saddle points [ 34 ] , require estimators for inverse Hessians [ 35 , 36 ] , are complicated to implement , have numerous optimizer parameters , and can be more expensive in iteration and memory cost [ 37 , 36 , 35 , 38 – 40 ] . Instead , we focus on ﬁrst - order methods . First - order : Some ﬁrst - order methods such as extragradient [ 41 ] require a second , costly , gradient evaluation per step . Similarly , methods alternating player updates are bottlenecked by waiting until after the ﬁrst player’s gradient is used to evaluate the second player’s gradient . But , many deep learning setups can parallelize computation of both players’ gradients , making alternating updates effectively cost another gradient evaluation . We want a method which updates with the effective cost of one gradient evaluation . Also , simultaneous updates are a standard choice in some settings [ 15 ] . Robust convergence : We want our method to converge in purely adversarial game’s with simulta - neous updates – a setup where existing momentum methods fail [ 25 ] . Furthermore , computing a games eigendecomposition is often infeasibly expensive , so we want methods that robustly converge over different mixtures of adversarial and cooperative eigenspaces . We are particularly interested in eigenspace mixtures that that are relevant during GAN training – see Figure 7 and Appendix Figure 9 . 2 . 3 Coming up with our Method Combining existing methods : Given the preceding limitations , we would like a robust ﬁrst - order method using a single , simultaneous gradient evaluation . We looked at combining aggregated [ 26 ] with negative [ 25 ] momentum by allowing negative coefﬁcients , because these methods are ﬁrst - order and use a single gradient evaluation – see Figure 2b . Also , aggregated momentum provides robustness during optimization by converging quickly on problems with wide range of conditioning , while negative momentum works in adversarial setups . We hoped to combine their beneﬁts , gaining robustness to different mixtures of adversarial and cooperative eigenspaces . However , with this setup we could not ﬁnd solutions that converge with simultaneous updates in purely adversarial games . Generalize to allow solutions : We generalized the setup to allow recurrent connections between momentum buffers , with potentially negative coefﬁcients – see Figure 2c and Appendix Algorithm 4 . There are optimizer parameters so this converges with simultaneous updates in purely adversarial games , while being ﬁrst - order with a single gradient evaluation – see Corollary 1 . However , in general , this setup could introduce many optimizer parameters , have unintuitive behavior , and not be amenable to analysis . So , we choose a special case of this method to help solve these problems . gradient µ update α β ( a ) Classical [ 27 ] gradient µ p 1 q . . . µ p n q update α p 1 q β p 1 q α p n q β p n q ( b ) Aggregated [ 26 ] gradient µ p 1 q µ p 2 q update β p 1 , 2 q α p 1 q β p 1 , 1 q β p 2 , 1 q α p 2 q β p 2 , 2 q ( c ) Recurrently linked ( new ) gradient (cid:60) p µ ) (cid:61) p µ ) update (cid:61) p β q α (cid:60) p β q ´ (cid:61) p β q (cid:60) p β q ( d ) Complex ( ours ) Figure 2 : We show computational diagrams for momentum variants simultaneously updating all players parameters , which update the momentum buffers µ at iteration j ` 1 with coefﬁcient β via µ j ` 1 “p β µ j ´ gradient q . Our parameter update is a linear combination of the momentum buffers weighted by step sizes α . ( a ) Classical momentum [ 27 , 29 ] , with a single buffer and coefﬁcient β P r 0 , 1 q . ( b ) Aggregated momentum [ 26 ] which adds multiple buffers with different coefﬁcients . ( c ) Recurrently linked momentum , which adds cross - buffer coefﬁcients and updates the buffers with µ j ` 1 p k q “p ř l β p l , k q µ j p l q ´ gradient q . We allow β p l , k q to be negative like negative momentum [ 25 ] for solutions with simultaneous updates in adversarial games . ( d ) Complex momentum is a special case of recurrently linked momentum with two buffers and β p 1 , 1 q “ β p 2 , 2 q “ (cid:60) p β q , β p 1 , 2 q “´ β p 2 , 1 q “ (cid:61) p β q . Analyzing other recurrently linked momentum setups is an open problem . 3 A simple solution : With two momentum buffers and correctly chosen recurrent weights , we can interpret our buffers as the real and imaginary part of one complex buffer – see Figure 2d . This method is ( a ) capable of converging in purely adversarial games with simultaneous updates – Corollary 1 , ( b ) only introduces one new optimizer parameter – the phase of the momentum coefﬁcient , ( c ) is tractable to analyze and have intuitions for with Euler’s formula – ex . , Eq . ( 8 ) , ( d ) is trivial to implement in libraries supporting complex arithmetic – see Figure 1 , and ( e ) can be robust to games with different mixtures of cooperative and adversarial eigenspaces – see Figure 5 . 3 Complex Momentum We describe our proposed method , where the momentum coefﬁcient β P C , step size α P R , momentum buffer µ P C d , and player parameters ω P R d . The simultaneous ( or Jacobi ) update is : µ j ` 1 “ β µ j ´ ˆ g j , ω j ` 1 “ ω j ` (cid:60) p α µ j ` 1 q ( SimCM ) There are many ways to get a real - valued update from µ P C , but we only consider updates equivalent to classical momentum when β P R . Speciﬁcally , we simply update the parameters using the real component of the momentum : (cid:60) p µ q . Algorithm 1 ( SimCM ) Momentum 1 : β , α P C , µ P C d , ω 0 P R d 2 : for i “ 1 . . . N do 3 : µ j ` 1 “ β µ j ´ ˆ g j 4 : ω j ` 1 “ ω j ` (cid:60) p α µ j ` 1 q return ω N We show the SimCM update in Algorithm 1 and visualize it in Figure 2d . We also show the alternating ( or Gauss - Seidel ) update , which is common for GAN training : µ j ` 1 A “ β µ jA ´ ˆ g A p ω j q , θ j ` 1 A “ θ jA ` (cid:60) p α µ j ` 1 A q ( AltCM ) µ j ` 1 B “ β µ jB ´ ˆ g B p θ j ` 1 A , θ jB q , θ j ` 1 B “ θ jB ` (cid:60) p α µ j ` 1 B q Generalizing negative momentum : Consider the negative momentum from [ 25 ] : ω j ` 1 “ ω j ´ α ˆ g j ` β p ω j ´ ω j ´ 1 q . Expanding ( SimCM ) with µ j “ p ω j ´ ω j ´ 1 q { α for real momentum shows the negative momentum method of [ 25 ] is a special case of our method : ω j ` 1 “ ω j ` (cid:60) p α p β p ω j ´ ω j ´ 1 q { α ´ ˆ g j qq “ ω j ´ α ˆ g j ` β p ω j ´ ω j ´ 1 q ( 7 ) 3 . 1 Dynamics of Complex Momentum For simplicity , we assume Numpy - style [ 43 ] component - wise broadcasting for operations like taking the real - part (cid:60) p z q of vector z “ r z 1 , . . . , z n s P C n , with proofs in the Appendix . Expanding the buffer updates with the polar components of β gives intuition for complex momentum : µ j ` 1 “ β µ j ´ ˆ g j ðñ µ j ` 1 “ β p β p¨ ¨ ¨ q ´ ˆ g j ´ 1 q ´ ˆ g j ðñ µ j ` 1 “´ k “ j ÿ k “ 0 β k ˆ g j ´ k ðñ (cid:60) p µ j ` 1 q“´ k “ j ÿ k “ 0 | β | k cos p k arg p β qq ˆ g j ´ k , (cid:61) p µ j ` 1 q“´ k “ j ÿ k “ 0 | β | k sin p k arg p β qq ˆ g j ´ k ( 8 ) The ﬁnal line is simply by Euler’s formula ( 26 ) . From ( 8 ) we can see β controls the momentum buffer µ by having | β | dictate prior gradient decay rates , while arg p β q controls oscillation frequency between adding and subtracting prior gradients , which we visualize in Figure 4a . D i s c r i m i n a t o r Generator N o r m o f j o i n t - g r a d i e n t } ˆ g } 10 0 10 1 10 2 10 3 10 4 Iteration 10 8 10 6 10 4 10 2 10 0 D i s t a n c e t o O p t i m u m arg ( ) = 0 ( classical ) arg ( ) = 8 ( ours ) theory D i s t a n ce t o op ti m u m Iterations Figure 3 : Complex momentum helps correct rotational dynamics when training a Dirac - GAN [ 42 ] . Left : Parameter trajectories with step size α “ 0 . 1 and momentum β “ 0 . 9 exp p i π { 8 q . We include the classical , real and positive momentum which diverges for any step size . Right : The distance from optimum , which has a linear convergence rate matching our prediction with Theorem 1 and ( 14 ) . 4 0 10 20 30 40 50 Iteration j 1 0 1 ( ) d e p e n d a n c e o n g j a t i t e r a t i o n 50 arg ( ) = 0 ( classical ) arg ( ) = 8 ( ours ) arg ( ) = ( negative ) D e p e nd e n ce on g r a d i e n t k Iteration k Figure 4 ( a ) : We show the real part of our mo - mentum buffer – which dictates the parameter update – at the 50 th iteration (cid:60) p µ 50 q depen - dence on past gradients ˆ g k for k “ 1 . . . 50 . The momentum magnitude is ﬁxed to | β | “ 0 . 9 as in Figure 3 . Euler’s formula is used in ( 8 ) to for ﬁnding dependence or coefﬁcient of ˆ g k via (cid:60) p µ 50 q “ ´ ř k “ 50 k “ 0 | β | k cos p k arg p β qq ˆ g j ´ k . Complex momentum allows smooth changes in the buffers dependence on past gradients . M o m e n t u m ph a s e a r g p β q Momentum magnitude | β | N u m b e r o f s t e p s t o c onv e r g e Figure 4 ( b ) : How many steps simultaneous complex momen - tum on a Dirac - GAN takes for a set solution distance . We ﬁx step size α “ 0 . 1 as in Figure 3 , while varying the phase and magnitude of our momentum β “ | β | exp p i arg p β qq . There is a red star at the optima , dashed red lines at real β , and a dashed magenta line for simultaneous gradient descent . There are no real - valued β that converge for this – or any – α with simultaneous updates [ 25 ] . Appendix Figure 8 compares this with alternating updates ( AltCM ) . Expanding the parameter updates with the Cartesian components of α and β is key for Theorem 1 , which characterizes the convergence rate : µ j ` 1 “ β µ j ´ ˆ g j ðñ (cid:60) p µ j ` 1 q“ (cid:60) p β q (cid:60) p µ j q´ (cid:61) p β q (cid:61) p µ j q´ (cid:60) p ˆ g j q , (cid:61) p µ j ` 1 q“ (cid:61) p β q (cid:60) p µ j q ` (cid:60) p β q (cid:61) p µ j q ( 9 ) ω j ` 1 “ ω j ` (cid:60) p α µ j ` 1 q ðñ ω j ` 1 “ ω j ´ α ˆ g j ` (cid:60) p αβ q (cid:60) p µ j q´ (cid:61) p αβ q (cid:61) p µ j q ( 10 ) So , we can write the next iterate with a ﬁxed - point operator : r (cid:60) p µ j ` 1 q , (cid:61) p µ j ` 1 q , ω j ` 1 s“ F α , β pr (cid:60) p µ j q , (cid:61) p µ j q , ω j sq ( 11 ) ( 9 ) and ( 10 ) allow us to write the Jacobian of F α , β which can be used to bound convergence rates near ﬁxed points , which we name the Jacobian of the augmented dynamics of buffer µ and joint - parameters ω and denote with : R : “ ∇ r µ , ω s F α , β “ « (cid:60) p β q I ´ (cid:61) p β q I ´ ∇ ω ˆ g (cid:61) p β q I (cid:60) p β q I 0 (cid:60) p αβ q I ´ (cid:61) p αβ q I I ´ α ∇ ω ˆ g ﬀ ( 12 ) So , for quadratic losses our parameters evolve via : r (cid:60) p µ j ` 1 q , (cid:61) p µ j ` 1 q , ω j ` 1 s J “ R r (cid:60) p µ j q , (cid:61) p µ j q , ω j s J ( 13 ) We can bound convergence rates by looking at the spectrum of R with Theorem 1 . Theorem 1 ( Consequence of Prop . 4 . 4 . 1 [ 44 ] ) . Convergence rate of complex momentum : If the spectral radius ρ p R q “ ρ p ∇ r µ , ω s F α , β q ă 1 , then , for r µ , ω s in a neighborhood of r µ ˚ , ω ˚ s , the distance of r µ j , ω j s to the stationary point r µ ˚ , ω ˚ s converges at a linear rate O pp ρ p R q ` (cid:15) q j q , @ (cid:15) ą 0 . Here , linear convergence means lim j Ñ8 } ω j ` 1 ´ ω ˚ } { } ω j ´ ω ˚ } P p 0 , 1 q , where ω ˚ is a ﬁxed point . We should select optimization parameters α , β so that the augmented dynamics spectral radius Sp p R p α , β qq ă 1 —with the dependence on α and β now explicit . We may want to express Sp p R p α , β qq in terms of the spectrum Sp p ∇ ω ˆ g q , as in Theorem 3 in [ 25 ] : f p Sp p ∇ ω ˆ g q , α , β q“ Sp p R p α , β qq ( 14 ) We provide a Mathematica command in Appendix A . 2 for a cubic polynomial p characterizing f with coefﬁcients that are functions of α , β & λ P Sp p ∇ ω ˆ g q , whose roots are eigenvalues of R , which we use in subsequent results . [ 45 , 26 ] mention that in practice we do not know the condition number , eigenvalues – or the mixture of cooperative and adversarial eigenspaces – of a set of functions that we are optimizing , so we try to design algorithms which work over a large range . Sharing this motivation , we consider convergence behavior on games ranging from purely adversarial to cooperative . In Section 4 . 2 at every non - real β we could select α and | β | so Algorithm 1 converges . We deﬁne almost - positive to mean arg p β q“ (cid:15) for small (cid:15) , and show there are almost - positive β which converge . Corollary 1 ( Convergence of Complex Momentum ) . There exist α P R , β P C so Algorithm 1 converges for bilinear zero - sum games . More - so , for small (cid:15) ( we show for (cid:15) “ π 16 ) , if arg p β q “ (cid:15) ( i . e . , almost - positive ) or arg p β q “ π ´ (cid:15) ( i . e . , almost - negative ) , then we can select α , | β | to converge . 5 Why show this ? Our result complements [ 25 ] who show that for all real α , β Algorithm 1 does not converge . We include the proof for bilinear zero - sum games , but the result generalizes to some games that are purely adversarial near ﬁxed points , like Dirac GANs [ 34 ] . The result’s second part shows evidence there is a sense in which the only β that do not converge are real ( with simultaneous updates on purely adversarial games ) . It also suggests a form of robustness , because almost - positive β can approach acceleration in cooperative eigenspaces , while converging in adversarial eigenspaces , so almost - positive β may be desirable when we have games with an uncertain or variable mixtures of real and imaginary eigenvalues like GANs . Sections 4 . 2 , 4 . 3 , and 4 . 4 investigate this further . 3 . 2 What about Acceleration ? With classical momentum , ﬁnding the step size α and momentum β to optimize the convergence rate tractable if 0 ă l ď L and Sp p ∇ ω ˆ g qPr l , L s d [ 46 ] – i . e . , we have an l - strongly convex and L - Lipschitz loss . The conditioning κ “ L { l can characterize problem difﬁculty . Gradient descent with an appropriate α can achieve a convergence rate of κ ´ 1 κ ` 1 , but using momentum with appropriate p α ˚ , β ˚ q can achieve an accelerated rate of ρ ˚ “ ? κ ´ 1 ? κ ` 1 . However , there is no consensus for constraining Sp p ∇ ω ˆ g q in games for tractable and useful results . Candidate constraints include monotonic vector ﬁelds generalizing notions of convexity , or vector ﬁelds with bounded eigenvalue norms capturing a kind of sensitivity [ 47 ] . Figure 7 shows Sp p ∇ ω ˆ g q for a GAN – we can attribute some eigenvectors to a single player’s parameters . The discriminator can be responsible for the largest and smallest norm eigenvalues , suggesting we may beneﬁt from varying α and β for each player as done in Section 4 . 4 . 3 . 3 Implementing Complex Momentum Complex momentum is trivial to implement with libraries supporting complex arithmetic like JAX [ 48 ] or Pytorch [ 49 ] . Given an SGD implementation , we often only need to change a few lines of code – see Figure 1 . Also , ( 9 ) and ( 10 ) can be easily used to implement Algorithm 1 in a library without complex arithmetic . More sophisticated optimizers like Adam can trivially support complex optimizer parameters with real - valued updates , which we explore in Section 4 . 4 . 3 . 4 Scope and Limitations For some games , we need higher than ﬁrst - order information to converge – ex . , pure - response games [ 7 ] – because the ﬁrst - order information for a player is identically zero . So , momentum methods only using ﬁrst - order info will not converge in general . However , we can combine methods with second - order information and momentum algorithms [ 7 , 9 ] . Complex momentum’s computational cost is almost identical to classical and negative momentum , except we now have a buffer with twice as many real parameters . We require one more optimization hyperparameter than classical momentum , which we provide an initial guess for in Section 4 . 5 . 4 Experiments We investigate complex momentum’s performance in training GANs and games with different mixtures of cooperative and adversarial eigenspaces , showing improvements over standard baselines . Code for experiments will be available on publication , with reproducibility details in Appendix C . Overview : We start with a purely adversarial Dirac - GAN and zero - sum games , which have known solutions ω ˚ “ p θ ˚ A , θ ˚ B q and spectrums Sp p ∇ ω ˆ g q , so we can assess convergence rates . Next , we evaluate GANs generating 2 D distributions , because they are simple enough to train with a plain , alternating SGD . Finally , we look at scaling to larger - scale GANs on images which have brittle optimization , and require optimizers like Adam . Complex momentum provides beneﬁts in each setup . We only compare to ﬁrst - order optimization methods , despite there being various second - order methods due limitations discussed in Section 2 . 2 . 4 . 1 Optimization in Purely Adversarial Games Here , we consider the optimizing the Dirac - GAN objective , which is surprisingly hard and where many classical optimization methods fail , because Sp p ∇ ω ˆ g q is imaginary near solutions : min x max y ´ log p 1 ` exp p´ xy qq ´ log p 2 q ( 15 ) Figure 3 empirically veriﬁes convergence rates given by Theorem 1 with ( 14 ) , by showing the optimization trajectories with simultaneous updates . Figure 4b investigates how the components of the momentum β affect convergence rates with simultaneous updates and a ﬁxed step size . The best β was almost - positive ( i . e . , arg p β q“ (cid:15) for small (cid:15) ) . We repeat this experiment with alternating updates in Appendix Figure 8 , which are standard in GAN training . There , almost - positive momentum is best ( but negative momentum also converges ) , and the beneﬁt of alternating updates can depend on if we can parallelize player gradient evaluations . 6 (cid:61) p Sp p R qq arg p β q “ 0 arg p β q “ π 4 The real component of the spectrum of the augmented learning dynamics (cid:60) p Sp p R qq arg p β q “ π 2 arg p β q “ 3 π 4 arg p β q “ π | β | Figure 6 : The spectrum of the augmented learning dynamics R is shown , whose spectral norm is the convergence rate in Theorem 1 . Each image is a different momentum phase arg p β q for a range of α , | β | P r 0 , 1 s . The opacity of an eigenvalue ( eig ) is the step size α and the color corresponds to momentum magnitude | β | . A red unit circle shows where all eigs must lie to converge for a ﬁxed α , β . If the max eig norm ă 1 , we draw a green circle whose radius is our convergence rate and a green star at the associated eig . Notably , at every non - real β we can select α , | β | for convergence . The eigs are symmetric over the x - axis , and eigs near (cid:60) p λ q“ 1 dictate convergence rate . Eigs near the center are due to state augmentation , have small magnitudes , and do not impact convergence rate . Simultaneous gradient descent corresponds to the magenta values where | β | “ 0 . 4 . 2 How Adversarialness Affects Convergence Rates Here , we compare optimization with ﬁrst - order methods for purely adversarial , cooperative , and mixed games . We use the following game , allowing us to easily interpolate between these regimes : min x max y x J p γA q y ` x J pp I ´ γ q B 1 q x ´ y J pp I ´ γ q B 2 q y ( 16 ) 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 10 1 10 2 10 3 arg ( ) = 0 ( classical ) arg ( ) = 8 ( ours ) arg ( ) = 2 ( ours ) arg ( ) = ( negative ) EG OG GDA # g r a d . e v a l . t o c onv e r g e Max adversarialness γ max Figure 5 : We compare ﬁrst - order methods con - vergence rates on the game in ( 16 ) , with A “ B 1 “ B 2 diagonal and entries linearly spaced in r 1 { 4 , 4 s . We interpolate from purely cooperative to a mixture of purely cooperative and adversar - ial eigenspaces in Sp p ∇ ω ˆ g q by making γ diagonal with γ j „ U r 0 , γ max s , inducing j th eigenvalue pair to have arg p λ j q « ˘ γ j π 2 . So , γ max controls the largest possible eigenvalue arg or max adversarial - ness . Every method generalizes gradient descent - ascent ( GDA ) by adding an optimizer parameter , tuned via grid search . Positive momentum and neg - ative momentum do not converge if there are purely adversarial eigenspaces ( i . e . , γ max “ 1 ) . Almost - positive momentum arg p β q“ (cid:15) ą 0 like π { 8 allows us to approach the acceleration of positive momen - tum if sufﬁciently cooperative ( i . e . , γ max ă 0 . 5 ) , while still converging if there are purely adversarial eigenspaces ( i . e . , γ max “ 1 ) . Tuning arg p β q with complex momentum performs competitively with extragradient ( EG ) , optimistic gradient ( OG ) for any adversarialness – ex . , arg p β q“ π { 2 does well if there are purely adversarial eigenspaces ( i . e . , γ max “ 1 ) . If γ “ I the game is purely adversarial , while if the γ “ 0 the game is purely cooperative . Figure 6 explores Sp p R q in purely adversarial games for a range of α , β , generalizing Figure 4 in [ 25 ] . At every non - real β —i . e . , arg p β q‰ π or 0 —we could select α , | β | that converge . Figure 5 compares ﬁrst - order algorithms as we interpolate from the purely cooperative games ( i . e . , minimization ) to mixtures of purely ad - versarial and cooperative eigenspaces , because this setup range can occur during GAN training – see Figure 7 . Our baselines are simultaneous SGD ( or gradient descent - ascent ( GDA ) ) , ex - tragradient ( EG ) [ 41 ] , optimistic gradient ( OG ) [ 50 – 52 ] , and momentum variants . We added extrapolation parameters for EG and OG so they are competitive with momentum – see Appendix Section C . 3 . We show how many gradient evaluations for a set solution distance , and EG costs two evaluations per update . We optimize convergence rates for each game and method by grid search , as is common for opti - mization parameters in deep learning . Takeaway : In the cooperative regime – i . e . , γ max ă . 5 or max λ P Sp p ∇ ω ˆ g q | arg p λ q | ă π { 4 – the best method is classical , positive momen - tum , otherwise we beneﬁt from a method for learning in games . If we have purely adver - sarial eigenspaces then GDA , positive and neg - ative momentum fail to converge , while EG , OG , and complex momentum can converge . In games like GANs , our eigendecomposition is infeasible to compute and changes during train - ing – see Appendix Figure 9 – so we want an optimizer that converges robustly . Choosing any non - real momentum β allows robust convergence for every eigenspace mixture . More so , almost - positive momentum β allows us to approach acceleration when cooperative , while still converging if there are purely adversarial eigenspaces . 7 4 . 3 Training GANs on 2 D Distributions Here , we investigate improving GAN training using alternating gradient descent updates with complex momentum . We look at alternating updates , because they are standard in GAN training [ 1 , 32 , 53 ] . It is not clear how EG and OG generalize to alternating updates , so we use positive and negative momentum as our baselines . We train to generate a 2 D mixture of Gaussians , because more complicated distribution require more complicated optimizers than SGD . Figure 1 shows all changes necessary to use the JAX momentum optimizer for our updates , with full details in Appendix C . 4 . We evaluate the log - likelihood of GAN samples under the mixture as an imperfect proxy for matching . Appendix Figure 10 shows heatmaps for tuning arg p β q and | β | with select step sizes . Takeaway : The best momentum was found at the almost - positive β « 0 . 7 exp p i π { 8 q with step size α “ 0 . 03 , and for each α we tested a broad range of non - real β outperformed any real β . This suggests we may be able to often improve GAN training with alternating updates and complex momentum . 4 . 4 Training BigGAN with a Complex Adam Here , we investigate improving larger - scale GAN training with complex momentum . However , larger - scale GANs train with more complicated optimizers than gradient descent – like Adam [ 31 ] – and have notoriously brittle optimization . We look at training BigGAN [ 32 ] on CIFAR - 10 [ 54 ] , but were unable to succeed with optimizers other than [ 32 ] - supplied setups , due to brittle optimization . So , we attempted to change procedure minimally by taking [ 32 ] - supplied code here which was trained with Adam , and making the β 1 parameter – analogous to momentum – complex . The modiﬁed complex Adam is shown in Algorithm 2 , where the momentum bias correction is removed to better match our theory . It is an open question on how to best carry over the design of Adam ( or other optimizers ) to the complex setting . Training each BigGAN took 10 hours on an NVIDIA T4 GPU , so Figure 12a and Table 1 took about 1000 and 600 GPU hours respectively . Figure 12a shows a grid search over arg p β 1 q and | β 1 | for a BigGAN trained with Algorithm 2 . We only changed β 1 for the discriminator’s optimizer . Takeaway : The best momentum was at the almost - positive β 1 « 0 . 8 exp p i π { 8 q , whose samples are in Appendix Figure 11b . We tested the best momentum value over 10 seeds against the author - provided baseline in Appendix Figure 12b , with the results summarized in Table 1 . [ 32 ] reported a single inception score ( IS ) on CIFAR - 10 of 9 . 22 , but the best we could reproduce over the seeds with the provided PyTorch code and settings was 9 . 10 . Complex momentum improves the best IS found with 9 . 25 p ` . 15 over author code , ` . 03 author reported q . We trained a real momentum | β 1 | “ 0 . 8 to see if the improvement was solely from tuning the momentum magnitude . This occasionally failed to train and decreased the best IS over re - runs , showing we beneﬁt from a non - zero arg p β 1 q . 4 . 5 A Practical Initial Guess for Optimizer Parameter arg p β q Here , we propose a practical initial guess for our new hyperparameter arg p β q . Corollary 1 shows we can use almost - real momentum coefﬁcients ( i . e . , arg p β q is close to 0 ) . Figure 5 shows almost - positive β approach acceleration in cooperative eigenspaces , while converging in all eigenspaces . Figure 7 shows GANs can have both cooperative and adversarial eigenspaces . Figures 10 and 12a do a grid search over arg p β q for GANs , ﬁnding that almost - positive arg p β q « π { 8 works in both cases . Also , by minimally changing arg p β q from 0 to a small (cid:15) , we can minimally change other hyperparameters in our model , which is useful to adapt existing , brittle setups like in GANs . Based on this , we propose an initial guess of arg p β q “ (cid:15) for a small (cid:15) ą 0 , where (cid:15) “ π { 8 worked in our GAN experiments . Algorithm 2 Complex Adam variant without momentum bias - correction 1 : β 1 P C , β 2 Pr 0 , 1 q 2 : α P R ` , (cid:15) P R ` 3 : for j “ 1 . . . N do 4 : µ j ` 1 “ β 1 µ j ´ g j 5 : v j ` 1 “ β 2 v j ` p 1 ´ β 2 qp g j q 2 6 : ˆ v j ` 1 “ v j ` 1 1 ´p β 2 q j 7 : ω j ` 1 “ ω j ` α (cid:60) p µ j q ? ˆ v j ` 1 ` (cid:15) return ω N CIFAR - 10 BigGAN Best IS for 10 seeds Discriminator β 1 Min Max 0 – [ 32 ] ’s default 8 . 9 9 . 1 0 . 8 exp p i π { 8 q – ours 8 . 96 p ` . 06 q 9 . 25 p ` . 15 q 0 . 8 3 . 12 p´ 5 . 78 q 9 . 05 p´ 0 . 05 q Table 1 : We display the best inception scores ( IS ) found over 10 runs for training BigGAN on CIFAR - 10 with various optimizer settings . We use a complex Adam variant outlined in Algorithm 2 , where we only tuned β 1 for the discriminator . The best parameters found in Figure 12a were β 1 “ 0 . 8 exp p i π { 8 q , which improved the min and max IS from our runs of the BigGAN authors base - line , which was the SoTA optimizer in this setting to best of our knowledge . We tested β 1 “ 0 . 8 to see if the gain was solely from tuning | β 1 | , which occasionally failed and decreased the best IS . 8 5 Related Work - π - π 2 0 π 2 π P h a s e o f e i g e nv a l u e a r g p λ q Spectrum of Jacobian of joint - grad Sp p ∇ ω ˆ g j q for GAN Log - magnitude of eigenvalue log p | λ | q d i s c . un s u r e g e n . D o e s e i g e nv ec t o r po i n t a t a p l a y e r ? Figure 7 : A log - polar coordinate visualization reveals struc - ture in the spectrum for a GAN at the end of the training on a 2 D mixture of Gaussians with a 1 - layer ( disc ) riminator and ( gen ) erator , so the joint - parameters ω P R 723 . It is difﬁcult to see structure by graphing the Cartesian ( i . e . , (cid:60) and (cid:61) ) parts of eigenvalues , because they span orders of magnitude , while being positive and negative . Appendix Figure 9 shows the spectrum through training . There is a mixture of many cooperative ( i . e . , real or arg p λ q « 0 , ˘ π ) and some adversarial ( i . e . , imaginary or arg p λ q « ˘ π 2 ) eigenvalues , so – contrary to what the name may suggest – generative adversarial networks are not purely adversarial . We may beneﬁt from optimizers leveraging this structure like complex momentum . Eigenvalues are colored if the associated eigenvector is mostly in one player’s part of the joint - parameter space – see Appendix Figure 9 for details on this . Many eigen - vectors lie mostly in the the space of ( or point at ) a one player . The structure of the set of eigenvalues for the disc . ( green ) is different than the gen . ( red ) , but further investiga - tion of this is an open problem . Notably , this may motivate separate optimizer choices for each player as in Section 4 . 4 . Accelerated ﬁrst - order methods : A broad body of work exists using momentum - type methods [ 27 , 28 , 55 , 56 ] , with a recent focus on deep learn - ing [ 29 , 57 – 60 ] . But , these works focus on momentum for minimization as op - posed to in games . Learning in games : Various works ap - proximate response - gradients - some by differentiating through optimiza - tion [ 61 , 34 , 62 ] . Multiple works try to leverage game eigenstructure during optimization [ 63 – 65 , 39 , 66 , 67 ] . First - order methods in games : In some games , we can get away with using only ﬁrst - order methods – [ 68 – 72 , 47 , 73 , 74 ] discuss when and how these methods work . [ 25 ] is the closest work to ours , showing a negative mo - mentum can help in some games . [ 30 ] note the suboptimality of negative mo - mentum in a class of games . [ 75 , 76 ] investigate acceleration in some games . Bilinear zero - sum games : [ 77 ] study the convergence of gradient methods in bilinear zero - sum games . Their analy - sis extends [ 25 ] , showing that we can achieve faster convergence by having separate step sizes and momentum for each player or tuning the extragradi - ent step size . [ 78 ] provide convergence guarantees for games satisfying a sufﬁ - ciently bilinear condition . Learning in GANs : Various works try to make GAN training easier with meth - ods leveraging the game structure [ 79 – 81 , 53 , 82 ] . [ 83 ] approximate the discriminator’s response function by differentiating through optimization . [ 34 ] ﬁnd solutions by minimizing the norm of the players’ updates . Both of these methods and various others [ 84 – 86 ] require higher - order information . [ 52 , 87 , 88 ] look at ﬁrst - order methods . [ 42 ] explore problems for GAN training convergence and [ 22 ] show that GANs have signiﬁcant rotations affecting learning . 6 Conclusion In this paper we provided a generalization of existing momentum methods for learning in differentiable games by allowing a complex - valued momentum with real - valued updates . We showed that our method robustly converges in games with a different range of mixtures of cooperative and adversarial eigenspaces than current ﬁrst - order methods . We also presented a practical generalization of our method to the Adam optimizer , which we used to improve BigGAN training . More generally , we highlight and lay groundwork for investigating optimizers which work well with various mixtures of cooperative and competitive dynamics in games . Societal Impact Our main contribution in this work is methodological – speciﬁcally , a scalable algorithm for optimiz - ing in games . Since our focus is on improving optimization methods , we do not expect there to be direct negative societal impacts from this contribution . 9 Acknowledgements Resources used in preparing this research were provided , in part , by the Province of Ontario , the Government of Canada through CIFAR , and companies sponsoring the Vector Institute . Paul Vicol was supported by an NSERC PGS - D Scholarship . We thank Guodong Zhang , Guojun Zhang , James Lucas , Romina Abachi , Jonah Phillion , Will Grathwohl , Jakob Foerster , Murat Erdogdu , Ken Jackson , and Ioannis Mitliagkis for feedback and helpful discussion . References [ 1 ] Ian Goodfellow , Jean Pouget - Abadie , Mehdi Mirza , Bing Xu , David Warde - Farley , Sherjil Ozair , Aaron Courville , and Yoshua Bengio . Generative adversarial nets . In Advances in Neural Information Processing Systems , pages 2672 – 2680 , 2014 . [ Cited on pages 1 , 2 , 8 , and 15 ] [ 2 ] David Pfau and Oriol Vinyals . Connecting generative adversarial networks and actor - critic methods . arXiv preprint arXiv : 1610 . 01945 , 2016 . [ Cited on page 1 ] [ 3 ] Bowen Baker , Ingmar Kanitscheider , Todor Markov , Yi Wu , Glenn Powell , Bob McGrew , and Igor Mordatch . Emergent tool use from multi - agent autocurricula . In International Conference on Learning Representations , 2019 . [ Cited on page 1 ] [ 4 ] David Balduzzi , Marta Garnelo , Yoram Bachrach , Wojciech Czarnecki , Julien Perolat , Max Jaderberg , and Thore Graepel . Open - ended learning in symmetric zero - sum games . In International Conference on Machine Learning , pages 434 – 443 . PMLR , 2019 . [ Not cited . ] [ 5 ] Sainbayar Sukhbaatar , Zeming Lin , Ilya Kostrikov , Gabriel Synnaeve , Arthur Szlam , and Rob Fergus . Intrinsic motivation and automatic curricula via asymmetric self - play . In International Conference on Learning Representations , 2018 . [ Cited on page 1 ] [ 6 ] Jonathan Lorraine and David Duvenaud . Stochastic hyperparameter optimization through hypernetworks . arXiv preprint arXiv : 1802 . 09419 , 2018 . [ Cited on page 1 ] [ 7 ] Jonathan Lorraine , Paul Vicol , and David Duvenaud . Optimizing millions of hyperparameters by implicit differentiation . In International Conference on Artiﬁcial Intelligence and Statistics , pages 1540 – 1552 . PMLR , 2020 . [ Cited on page 6 ] [ 8 ] Matthew MacKay , Paul Vicol , Jon Lorraine , David Duvenaud , and Roger Grosse . Self - tuning networks : Bilevel optimization of hyperparameters using structured best - response functions . In International Confer - ence on Learning Representations ( ICLR ) , 2019 . [ Not cited . ] [ 9 ] Aniruddh Raghu , Maithra Raghu , Simon Kornblith , David Duvenaud , and Geoffrey Hinton . Teaching with commentaries . arXiv preprint arXiv : 2011 . 03037 , 2020 . [ Cited on pages 1 and 6 ] [ 10 ] Avishek Joey Bose , Gauthier Gidel , Hugo Berrard , Andre Cianﬂone , Pascal Vincent , Simon Lacoste - Julien , and William L Hamilton . Adversarial example games . arXiv preprint arXiv : 2007 . 00720 , 2020 . [ Cited on page 1 ] [ 11 ] Xiaoyong Yuan , Pan He , Qile Zhu , and Xiaolin Li . Adversarial examples : Attacks and defenses for deep learning . IEEE Transactions on Neural Networks and Learning Systems , 30 ( 9 ) : 2805 – 2824 , 2019 . [ Cited on page 1 ] [ 12 ] Aravind Rajeswaran , Igor Mordatch , and Vikash Kumar . A game theoretic framework for model based reinforcement learning . arXiv preprint arXiv : 2004 . 07804 , 2020 . [ Cited on page 1 ] [ 13 ] Romina Abachi , Mohammad Ghavamzadeh , and Amir - massoud Farahmand . Policy - aware model learning for policy gradient methods . arXiv preprint arXiv : 2003 . 00030 , 2020 . [ Not cited . ] [ 14 ] Pierre - Luc Bacon , Florian Schäfer , Clement Gehring , Animashree Anandkumar , and Emma Brunskill . A Lagrangian method for inverse problems in reinforcement learning . lis . csail . mit . edu / pubs , 2019 . [ Cited on page 1 ] [ 15 ] David Acuna , Guojun Zhang , Marc T Law , and Sanja Fidler . f - domain - adversarial learning : Theory and algorithms for unsupervised domain adaptation with neural networks , 2021 . URL https : / / openreview . net / forum ? id = WqXAKcwfZtI . [ Cited on pages 1 and 3 ] [ 16 ] Will Grathwohl , Elliot Creager , Seyed Kamyar Seyed Ghasemipour , and Richard Zemel . Gradient - based optimization of neural network architecture . 2018 . [ Cited on page 1 ] [ 17 ] George Adam and Jonathan Lorraine . Understanding neural architecture search techniques . arXiv preprint arXiv : 1904 . 00438 , 2019 . [ Cited on page 1 ] [ 18 ] Mengye Ren , Eleni Triantaﬁllou , Sachin Ravi , Jake Snell , Kevin Swersky , Joshua B Tenenbaum , Hugo Larochelle , and Richard S Zemel . Meta - learning for semi - supervised few - shot classiﬁcation . arXiv preprint arXiv : 1803 . 00676 , 2018 . [ Cited on page 1 ] 10 [ 19 ] Mengye Ren , Eleni Triantaﬁllou , Kuan - Chieh Wang , James Lucas , Jake Snell , Xaq Pitkow , Andreas S Tolias , and Richard Zemel . Flexible few - shot learning with contextual similarity . arXiv preprint arXiv : 2012 . 05895 , 2020 . [ Cited on page 1 ] [ 20 ] Oskar Morgenstern and John Von Neumann . Theory of Games and Economic Behavior . Princeton University Press , 1953 . [ Cited on page 1 ] [ 21 ] Heinrich Von Stackelberg . Market Structure and Equilibrium . Springer Science & Business Media , 2010 . [ Cited on page 1 ] [ 22 ] Hugo Berard , Gauthier Gidel , Amjad Almahairi , Pascal Vincent , and Simon Lacoste - Julien . A closer look at the optimization landscapes of generative adversarial networks . In International Conference on Learning Representations , 2019 . [ Cited on pages 1 and 9 ] [ 23 ] Kenneth Joseph Arrow , Hirofumi Azawa , Leonid Hurwicz , and Hirofumi Uzawa . Studies in Linear and Non - Linear Programming , volume 2 . Stanford University Press , 1958 . [ Cited on page 1 ] [ 24 ] Yoav Freund and Robert E Schapire . Adaptive game playing using multiplicative weights . Games and Economic Behavior , 29 ( 1 - 2 ) : 79 – 103 , 1999 . [ Cited on page 1 ] [ 25 ] Gauthier Gidel , Reyhane Askari Hemmat , Mohammad Pezeshki , Rémi Le Priol , Gabriel Huang , Simon Lacoste - Julien , and Ioannis Mitliagkas . Negative momentum for improved game dynamics . In The 22nd International Conference on Artiﬁcial Intelligence and Statistics , pages 1802 – 1811 . PMLR , 2019 . [ Cited on pages 1 , 2 , 3 , 4 , 5 , 6 , 7 , 9 , 18 , 20 , 21 , and 23 ] [ 26 ] James Lucas , Shengyang Sun , Richard Zemel , and Roger Grosse . Aggregated momentum : Stability through passive damping . In International Conference on Learning Representations , 2018 . [ Cited on pages 1 , 2 , 3 , 5 , and 20 ] [ 27 ] Boris T Polyak . Some methods of speeding up the convergence of iteration methods . USSR Computational Mathematics and Mathematical Physics , 4 ( 5 ) : 1 – 17 , 1964 . [ Cited on pages 2 , 3 , 9 , and 17 ] [ 28 ] Yurii E Nesterov . A method for solving the convex programming problem with convergence rate o ( 1 / kˆ 2 ) . In Dokl . Akad . Nauk SSSR , volume 269 , pages 543 – 547 , 1983 . [ Cited on page 9 ] [ 29 ] Ilya Sutskever , James Martens , George Dahl , and Geoffrey Hinton . On the importance of initialization and momentum in deep learning . In International Conference on Machine Learning , pages 1139 – 1147 , 2013 . [ Cited on pages 2 , 3 , and 9 ] [ 30 ] Guodong Zhang and Yuanhao Wang . On the suboptimality of negative momentum for minimax optimiza - tion . arXiv preprint arXiv : 2008 . 07459 , 2020 . [ Cited on pages 2 and 9 ] [ 31 ] Diederik P Kingma and Jimmy Ba . Adam : A method for stochastic optimization . arXiv preprint arXiv : 1412 . 6980 , 2014 . [ Cited on pages 2 and 8 ] [ 32 ] Andrew Brock , Jeff Donahue , and Karen Simonyan . Large scale gan training for high ﬁdelity natural image synthesis . In International Conference on Learning Representations , 2018 . [ Cited on pages 2 and 8 ] [ 33 ] Kazuki Osawa , Yohei Tsuji , Yuichiro Ueno , Akira Naruse , Rio Yokota , and Satoshi Matsuoka . Large - scale distributed second - order optimization using kronecker - factored approximate curvature for deep convolutional neural networks . In Proceedings of the IEEE / CVF Conference on Computer Vision and Pattern Recognition , pages 12359 – 12367 , 2019 . [ Cited on page 3 ] [ 34 ] Lars Mescheder , Sebastian Nowozin , and Andreas Geiger . The numerics of GANs . In Advances in Neural Information Processing Systems , pages 1825 – 1835 , 2017 . [ Cited on pages 3 , 6 , and 9 ] [ 35 ] Florian Schäfer and Anima Anandkumar . Competitive gradient descent . In Advances in Neural Information Processing Systems , pages 7623 – 7633 , 2019 . [ Cited on page 3 ] [ 36 ] Yuanhao Wang , Guodong Zhang , and Jimmy Ba . On solving minimax optimization locally : A follow - the - ridge approach . In International Conference on Learning Representations , 2019 . [ Cited on page 3 ] [ 37 ] Reyhane Askari Hemmat , Amartya Mitra , Guillaume Lajoie , and Ioannis Mitliagkas . Lead : Least - action dynamics for min - max optimization . arXiv preprint arXiv : 2010 . 13846 , 2020 . [ Cited on page 3 ] [ 38 ] Florian Schäfer , Anima Anandkumar , and Houman Owhadi . Competitive mirror descent . arXiv preprint arXiv : 2006 . 10179 , 2020 . [ Cited on page 3 ] [ 39 ] Wojciech Marian Czarnecki , Gauthier Gidel , Brendan Tracey , Karl Tuyls , Shayegan Omidshaﬁei , David Balduzzi , and Max Jaderberg . Real world games look like spinning tops . arXiv preprint arXiv : 2004 . 09468 , 2020 . [ Cited on page 9 ] [ 40 ] Guojun Zhang , Kaiwen Wu , Pascal Poupart , and Yaoliang Yu . Newton - type methods for minimax optimization . arXiv preprint arXiv : 2006 . 14592 , 2020 . [ Cited on page 3 ] [ 41 ] GM Korpelevich . The extragradient method for ﬁnding saddle points and other problems . Matecon , 12 : 747 – 756 , 1976 . [ Cited on pages 3 , 7 , and 15 ] 11 [ 42 ] Lars Mescheder , Andreas Geiger , and Sebastian Nowozin . Which training methods for gans do actually converge ? In International Conference on Machine learning ( ICML ) , pages 3481 – 3490 . PMLR , 2018 . [ Cited on pages 4 and 9 ] [ 43 ] Charles R Harris , K Jarrod Millman , Stéfan J van der Walt , Ralf Gommers , Pauli Virtanen , David Cournapeau , Eric Wieser , Julian Taylor , Sebastian Berg , Nathaniel J Smith , et al . Array programming with numpy . Nature , 585 ( 7825 ) : 357 – 362 , 2020 . [ Cited on page 4 ] [ 44 ] D Bertsekas . Nonlinear Programming . Athena Scientiﬁc , 2008 . [ Cited on pages 5 and 17 ] [ 45 ] Brendan O’donoghue and Emmanuel Candes . Adaptive restart for accelerated gradient schemes . Founda - tions of computational mathematics , 15 ( 3 ) : 715 – 732 , 2015 . [ Cited on page 5 ] [ 46 ] Gabriel Goh . Why momentum really works . Distill , 2 ( 4 ) : e6 , 2017 . [ Cited on page 6 ] [ 47 ] Waïss Azizian , Ioannis Mitliagkas , Simon Lacoste - Julien , and Gauthier Gidel . A tight and uniﬁed analysis of gradient - based methods for a whole spectrum of differentiable games . In International Conference on Artiﬁcial Intelligence and Statistics , pages 2863 – 2873 . PMLR , 2020 . [ Cited on pages 6 and 9 ] [ 48 ] James Bradbury , Roy Frostig , Peter Hawkins , Matthew James Johnson , Chris Leary , Dougal Maclaurin , and Skye Wanderman - Milne . JAX : composable transformations of Python + NumPy programs , 2018 . URL http : / / github . com / google / jax . [ Cited on page 6 ] [ 49 ] Adam Paszke , Sam Gross , Soumith Chintala , Gregory Chanan , Edward Yang , Zachary DeVito , Zeming Lin , Alban Desmaison , Luca Antiga , and Adam Lerer . Automatic differentiation in PyTorch . Openreview , 2017 . [ Cited on page 6 ] [ 50 ] Chao - Kai Chiang , Tianbao Yang , Chia - Jung Lee , Mehrdad Mahdavi , Chi - Jen Lu , Rong Jin , and Shenghuo Zhu . Online optimization with gradual variations . In Conference on Learning Theory , pages 6 – 1 . JMLR Workshop and Conference Proceedings , 2012 . [ Cited on page 7 ] [ 51 ] Alexander Rakhlin and Karthik Sridharan . Optimization , learning , and games with predictable sequences . In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 , pages 3066 – 3074 , 2013 . [ Not cited . ] [ 52 ] Constantinos Daskalakis , Andrew Ilyas , Vasilis Syrgkanis , and Haoyang Zeng . Training gans with optimism . In International Conference on Learning Representations ( ICLR 2018 ) , 2018 . [ Cited on pages 7 , 9 , and 15 ] [ 53 ] Yan Wu , Jeff Donahue , David Balduzzi , Karen Simonyan , and Timothy Lillicrap . Logan : Latent optimi - sation for generative adversarial networks . arXiv preprint arXiv : 1912 . 00953 , 2019 . [ Cited on pages 8 and 9 ] [ 54 ] Alex Krizhevsky . Learning multiple layers of features from tiny images . Technical report , University of Toronto , 2009 . [ Cited on page 8 ] [ 55 ] Yurii Nesterov . Introductory lectures on convex optimization : A basic course , volume 87 . Springer Science & Business Media , 2013 . [ Cited on page 9 ] [ 56 ] Chris J Maddison , Daniel Paulin , Yee Whye Teh , Brendan O’Donoghue , and Arnaud Doucet . Hamiltonian descent methods . arXiv preprint arXiv : 1809 . 05042 , 2018 . [ Cited on page 9 ] [ 57 ] Jian Zhang and Ioannis Mitliagkas . Yellowﬁn and the art of momentum tuning . arXiv preprint arXiv : 1706 . 03471 , 2017 . [ Cited on page 9 ] [ 58 ] Dami Choi , Christopher J Shallue , Zachary Nado , Jaehoon Lee , Chris J Maddison , and George E Dahl . On empirical comparisons of optimizers for deep learning . arXiv preprint arXiv : 1910 . 05446 , 2019 . [ Not cited . ] [ 59 ] Michael R Zhang , James Lucas , Geoffrey Hinton , and Jimmy Ba . Lookahead optimizer : k steps forward , 1 step back . arXiv preprint arXiv : 1907 . 08610 , 2019 . [ Not cited . ] [ 60 ] Ricky TQ Chen , Dami Choi , Lukas Balles , David Duvenaud , and Philipp Hennig . Self - tuning stochastic optimization with curvature - aware gradient ﬁltering . arXiv preprint arXiv : 2011 . 04803 , 2020 . [ Cited on page 9 ] [ 61 ] Jakob Foerster , Richard Y Chen , Maruan Al - Shedivat , Shimon Whiteson , Pieter Abbeel , and Igor Mordatch . Learning with opponent - learning awareness . In International Conference on Autonomous Agents and MultiAgent Systems , pages 122 – 130 , 2018 . [ Cited on page 9 ] [ 62 ] Dougal Maclaurin , David Duvenaud , and Ryan Adams . Gradient - based hyperparameter optimization through reversible learning . In International Conference on Machine Learning , pages 2113 – 2122 , 2015 . [ Cited on page 9 ] [ 63 ] Alistair Letcher , David Balduzzi , Sébastien Racaniere , James Martens , Jakob N Foerster , Karl Tuyls , and Thore Graepel . Differentiable game mechanics . Journal of Machine Learning Research , 20 ( 84 ) : 1 – 40 , 2019 . [ Cited on page 9 ] 12 [ 64 ] Sai Ganesh Nagarajan , David Balduzzi , and Georgios Piliouras . From chaos to order : Symmetry and conservation laws in game dynamics . In International Conference on Machine Learning , pages 7186 – 7196 . PMLR , 2020 . [ Not cited . ] [ 65 ] Shayegan Omidshaﬁei , Karl Tuyls , Wojciech M Czarnecki , Francisco C Santos , Mark Rowland , Jerome Connor , Daniel Hennes , Paul Muller , Julien Pérolat , Bart De Vylder , et al . Navigating the landscape of multiplayer games . Nature communications , 11 ( 1 ) : 1 – 17 , 2020 . [ Cited on page 9 ] [ 66 ] Gauthier Gidel , David Balduzzi , Wojciech Marian Czarnecki , Marta Garnelo , and Yoram Bachrach . Minimax theorem for latent games or : How i learned to stop worrying about mixed - nash and love neural nets . arXiv preprint arXiv : 2002 . 05820 , 2020 . [ Cited on page 9 ] [ 67 ] Julien Perolat , Remi Munos , Jean - Baptiste Lespiau , Shayegan Omidshaﬁei , Mark Rowland , Pedro Ortega , Neil Burch , Thomas Anthony , David Balduzzi , Bart De Vylder , et al . From poincar z ’e recurrence to convergence in imperfect information games : Finding equilibrium via regularization . arXiv preprint arXiv : 2002 . 08456 , 2020 . [ Cited on page 9 ] [ 68 ] Guodong Zhang , Yuanhao Wang , Laurent Lessard , and Roger Grosse . Don’t ﬁx what ain’t broke : Near - optimal local convergence of alternating gradient descent - ascent for minimax optimization . arXiv preprint arXiv : 2102 . 09468 , 2021 . [ Cited on page 9 ] [ 69 ] Guodong Zhang , Xuchao Bao , Laurent Lessard , and Roger Grosse . A uniﬁed analysis of ﬁrst - order methods for smooth games via integral quadratic constraints . arXiv preprint arXiv : 2009 . 11359 , 2020 . [ Not cited . ] [ 70 ] Adam Ibrahim , Waıss Azizian , Gauthier Gidel , and Ioannis Mitliagkas . Linear lower bounds and con - ditioning of differentiable games . In International Conference on Machine Learning , pages 4583 – 4593 . PMLR , 2020 . [ Not cited . ] [ 71 ] James P Bailey , Gauthier Gidel , and Georgios Piliouras . Finite regret and cycles with ﬁxed step - size via alternating gradient descent - ascent . In Conference on Learning Theory , pages 391 – 407 . PMLR , 2020 . [ Not cited . ] [ 72 ] Chi Jin , Praneeth Netrapalli , and Michael Jordan . What is local optimality in nonconvex - nonconcave minimax optimization ? In International Conference on Machine Learning , pages 4880 – 4889 . PMLR , 2020 . [ Cited on page 9 ] [ 73 ] Maher Nouiehed , Maziar Sanjabi , Tianjian Huang , Jason D Lee , and Meisam Razaviyayn . Solving a class of non - convex min - max games using iterative ﬁrst order methods . Advances in Neural Information Processing Systems , 32 : 14934 – 14942 , 2019 . [ Cited on page 9 ] [ 74 ] Guojun Zhang , Pascal Poupart , and Yaoliang Yu . Optimality and stability in non - convex smooth games . arXiv e - prints , pages arXiv – 2002 , 2020 . [ Cited on page 9 ] [ 75 ] Waïss Azizian , Damien Scieur , Ioannis Mitliagkas , Simon Lacoste - Julien , and Gauthier Gidel . Accelerating smooth games by manipulating spectral shapes . arXiv preprint arXiv : 2001 . 00602 , 2020 . [ Cited on page 9 ] [ 76 ] Carles Domingo - Enrich , Fabian Pedregosa , and Damien Scieur . Average - case acceleration for bilinear games and normal matrices . arXiv preprint arXiv : 2010 . 02076 , 2020 . [ Cited on page 9 ] [ 77 ] Guojun Zhang and Yaoliang Yu . Convergence of gradient methods on bilinear zero - sum games . In International Conference on Learning Representations , 2019 . [ Cited on page 9 ] [ 78 ] Nicolas Loizou , Hugo Berard , Alexia Jolicoeur - Martineau , Pascal Vincent , Simon Lacoste - Julien , and Ioan - nis Mitliagkas . Stochastic hamiltonian gradient methods for smooth games . In International Conference on Machine Learning , pages 6370 – 6381 . PMLR , 2020 . [ Cited on page 9 ] [ 79 ] Mingrui Liu , Youssef Mroueh , Jerret Ross , Wei Zhang , Xiaodong Cui , Payel Das , and Tianbao Yang . Towards better understanding of adaptive gradient algorithms in generative adversarial nets . In Interna - tional Conference on Learning Representations , 2020 . URL https : / / openreview . net / forum ? id = SJxIm0VtwH . [ Cited on page 9 ] [ 80 ] Wei Peng , Yu - Hong Dai , Hui Zhang , and Lizhi Cheng . Training gans with centripetal acceleration . Optimization Methods and Software , 35 ( 5 ) : 955 – 973 , 2020 . [ Not cited . ] [ 81 ] Isabela Albuquerque , João Monteiro , Thang Doan , Breandan Considine , Tiago Falk , and Ioannis Mitliagkas . Multi - objective training of generative adversarial networks with multiple discriminators . In International Conference on Machine Learning , pages 202 – 211 . PMLR , 2019 . [ Cited on page 9 ] [ 82 ] Ya - Ping Hsieh , Chen Liu , and Volkan Cevher . Finding mixed nash equilibria of generative adversarial networks . In International Conference on Machine Learning , pages 2810 – 2819 . PMLR , 2019 . [ Cited on page 9 ] [ 83 ] Luke Metz , Ben Poole , David Pfau , and Jascha Sohl - Dickstein . Unrolled generative adversarial networks . arXiv preprint arXiv : 1611 . 02163 , 2016 . [ Cited on page 9 ] 13 [ 84 ] Chongli Qin , Yan Wu , Jost Tobias Springenberg , Andrew Brock , Jeff Donahue , Timothy P Lillicrap , and Pushmeet Kohli . Training generative adversarial networks by solving ordinary differential equations . arXiv preprint arXiv : 2010 . 15040 , 2020 . [ Cited on page 9 ] [ 85 ] Florian Schäfer , Hongkai Zheng , and Anima Anandkumar . Implicit competitive regularization in gans . arXiv preprint arXiv : 1910 . 05852 , 2019 . [ Not cited . ] [ 86 ] Alexia Jolicoeur - Martineau and Ioannis Mitliagkas . Connections between support vector machines , wasserstein distance and gradient - penalty gans . arXiv preprint arXiv : 1910 . 06922 , 2019 . [ Cited on page 9 ] [ 87 ] Gauthier Gidel , Hugo Berard , Gaëtan Vignoud , Pascal Vincent , and Simon Lacoste - Julien . A variational inequality perspective on generative adversarial networks . In International Conference on Learning Representations , 2018 . [ Cited on page 9 ] [ 88 ] Tatjana Chavdarova , Gauthier Gidel , Francois Fleuret , and Simon Lacoste - Julien . Reducing noise in gan training with variance reduced extragradient . In Proceedings of the international conference on Neural Information Processing Systems , number CONF , 2019 . [ Cited on page 9 ] [ 89 ] Tim Salimans , Ian Goodfellow , Wojciech Zaremba , Vicki Cheung , Alec Radford , and Xi Chen . Improved techniques for training GANs . In Advances in Neural Information Processing Systems , pages 2234 – 2242 , 2016 . [ Cited on page 15 ] [ 90 ] Simon Foucart . Matrix norm and spectral radius . https : / / www . math . drexel . edu / ~ foucart / TeachingFiles / F12 / M504Lect6 . pdf , 2012 . Accessed : 2020 - 05 - 21 . [ Cited on page 17 ] [ 91 ] Stephen Boyd , Stephen P Boyd , and Lieven Vandenberghe . Convex optimization . Cambridge university press , 2004 . [ Cited on page 19 ] [ 92 ] Richard HR Hahnloser , Rahul Sarpeshkar , Misha A Mahowald , Rodney J Douglas , and H Sebastian Seung . Digital selection and analogue ampliﬁcation coexist in a cortex - inspired silicon circuit . Nature , 405 ( 6789 ) : 947 – 951 , 2000 . [ Cited on page 21 ] 14 Table 2 : Notation SGD Stochastic Gradient Descent CM Complex Momentum SGDm , SimSGDm , . . . . . . with momentum SimSGD , SimCM Simultaneous . . . AltSGD , AltCM Alternating . . . GAN Generative Adversarial Network [ 1 ] EG Extragradient [ 41 ] OG Optimistic Gradient [ 52 ] IS Inception Score [ 89 ] : “ Deﬁned to be equal to x , y , z , ¨ ¨ ¨ P C Scalars x , y , z , ¨ ¨ ¨ P C n Vectors X , Y , Z , ¨ ¨ ¨ P C n ˆ n Matrices X J The transpose of matrix X I The identity matrix (cid:60) p z q , (cid:61) p z q The real or imaginary component of z P C i The imaginary unit . z P C ùñ z “ (cid:60) p z q ` i (cid:61) p z q s z The complex conjugate of z P C | z | : “ ? z s z The magnitude or modulus of z P C arg p z q The argument or phase of z P C ùñ z “ | z | exp p i arg p z qq z P C is almost - positive arg p z q“ (cid:15) for small (cid:15) respectively A , B A symbol for the outer / inner players d A , d B P N The number of weights for the outer / inner players θ A symbol for the parameters or weights of a player θ A P R d A , θ B P R d B The outer / inner parameters or weights L : R n Ñ R A symbol for a loss L A p θ A , θ B q , L B p θ A , θ B q The outer / inner losses – R d A ` d B ÞÑ R g A p θ A , θ B q , g B p θ A , θ B q Gradient of outer / inner losses w . r . t . their weights in R d A { d B θ ˚ B p θ A q : “ arg min θ B L B p θ A , θ B q The best - response of the inner player to the outer player L ˚ A p θ A q : “ L A p θ A , θ ˚ B p θ A qq The outer loss with a best - responding inner player θ ˚ A : “ arg min θ A L ˚ A p θ A q Outer optimal weights with a best - responding inner player d : “ d A ` d B The combined number of weights for both players ω : “ r θ A , θ B s P R d A concatenation of the outer / inner weights ˆ g p ω q : “r g A p ω q , g B p ω qs P R d A concatenation of the outer / inner gradients ω 0 “ r θ 0 A , θ 0 B s P R d The initial parameter values j An iteration number ˆ g j : “ ˆ g p ω j q P R d The joint - gradient vector ﬁeld at weights ω j ∇ ω ˆ g j : “ ∇ ω ˆ g | ω j P R d ˆ d The Jacobian of the joint - gradient ˆ g at weights ω j α P C The step size or learning rate β P C The momentum coefﬁcient β 1 P C The ﬁrst momentum parameter for Adam µ P C d The momentum buffer λ P C Notation for an arbitrary eigenvalue Sp p M q P C n The spectrum – or set of eigenvalues – of M P R n ˆ n Purely adversarial / cooperative game Sp p ∇ ω ˆ g q is purely real / imaginary ρ p M q : “ max z P Sp p M q | z | The spectral radius in R ` of M P R n ˆ n F α , β pr µ , ω sq Fixed point op . for CM , or augmented learning dynamics R : “ ∇ r µ , ω s F α , β P R 3 d ˆ 3 d Jacobian of the augmented learning dynamics in Corollary 1 α ˚ , β ˚ : “ arg min α , β ρ p R p α , β qq The optimal step size and momentum coefﬁcient ρ ˚ : “ ρ p R p α ˚ , β ˚ qq The optimal spectral radius or convergence rate κ : “ maxSp p ∇ ω g q minSp p ∇ ω g q Condition number , for convex single - objective optimization σ 2 min p M q : “ max Sp p M J M q The minimum singular value of a matrix M 15 A Supporting Results First , some basic results about complex numbers that are used : z “ (cid:60) p z q ` i (cid:61) p z q “ | z | exp p i arg p z qq ( 17 ) s z “ (cid:60) p z q ´ i (cid:61) p z q “ | z | exp p´ i arg p z qq ( 18 ) exp p iz q ` exp p´ iz q “ 2 cos p z q ( 19 ) Ě z 1 z 2 “ s z 1 s z 2 ( 20 ) 1 { 2 p z ` s z q “ (cid:60) p z q ( 21 ) (cid:60) p z 1 z 2 q “ (cid:60) p z 1 q (cid:60) p z 2 q ´ (cid:61) p z 1 q (cid:61) p z 2 q ( 22 ) z 1 ` z 2 “ p (cid:60) p z 1 q ` (cid:60) p z 2 qq ` i p (cid:61) p z 1 q ` (cid:61) p z 2 qq ( 23 ) z 1 z 2 “ p (cid:60) p z 1 q (cid:60) p z 2 q ´ (cid:61) p z 1 q (cid:61) p z 2 qq ` i p (cid:61) p z 1 q (cid:60) p z 2 q ` (cid:60) p z 1 q (cid:61) p z 2 qq ( 24 ) z 1 z 2 “ | z 1 | | z 2 | exp p i p arg p z 1 q ` arg p z 2 qqq ( 25 ) z k “ | z | k exp p i arg p z q k q “ | z | k p cos p k arg p z qq ` i sin p k arg p z qq ( 26 ) This Lemma shows how we expand the complex - valued momentum buffer µ into its Cartesian components as in ( 9 ) . Lemma 1 . µ j ` 1 “ β µ j ´ ˆ g j ðñ (cid:60) p µ j ` 1 q “ (cid:60) p β q (cid:60) p µ j q ´ (cid:61) p β q (cid:61) p µ j q ´ (cid:60) p ˆ g j q , (cid:61) p µ j ` 1 q “ (cid:61) p β q (cid:60) p µ j q ` (cid:60) p β q (cid:61) p µ j q ´ (cid:61) p ˆ g j q Proof . µ j ` 1 “ β µ j ´ ˆ g j ðñ µ j ` 1 “ p (cid:60) p β q ` i (cid:61) p β qq ` (cid:60) p µ j q ` i (cid:61) p µ j q ˘ ´ ´ (cid:60) p ˆ g j q ` i (cid:61) p ˆ g j q ¯ ðñ µ j ` 1 “ ` (cid:60) p β q (cid:60) p µ j q ´ (cid:61) p β q (cid:61) p µ j q ˘ ` i ` (cid:61) p β q (cid:60) p µ j q ` (cid:60) p β q (cid:61) p µ j q ˘ ´ ´ (cid:60) p ˆ g j q ` i (cid:61) p ˆ g j q ¯ ðñ µ j ` 1 “ ´ (cid:60) p β q (cid:60) p µ j q ´ (cid:61) p β q (cid:61) p µ j q ´ (cid:60) p ˆ g j q ¯ ` i ´ (cid:61) p β q (cid:60) p µ j q ` (cid:60) p β q (cid:61) p µ j q ´ (cid:61) p ˆ g j q ¯ ðñ (cid:60) p µ j ` 1 q“ (cid:60) p β q (cid:60) p µ j q´ (cid:61) p β q (cid:61) p µ j q´ (cid:60) p ˆ g j q , (cid:61) p µ j ` 1 q“ (cid:61) p β q (cid:60) p µ j q ` (cid:60) p β q (cid:61) p µ j q´ (cid:61) p ˆ g j q We further assume (cid:61) p ˆ g j q is 0 - i . e . , our gradients are real - valued . This Lemma shows how we can decompose the joint - parameters ω at the next iterate as a linear combination of the joint - parameters , joint - gradient , and Cartesian components of the momentum - buffer at the current iterate as in ( 10 ) . Lemma 2 . ω j ` 1 “ ω j ` (cid:60) p α µ j ` 1 q ðñ ω j ` 1 “ ω j ´ (cid:60) p α q ˆ g j ` (cid:60) p αβ q (cid:60) p µ j q ´ (cid:61) p αβ q (cid:61) p µ j q Proof . (cid:60) p α µ j ` 1 q “ ` (cid:60) p α q (cid:60) p µ j ` 1 q ´ (cid:61) p α q (cid:61) p µ j ` 1 q ˘ “ ´ (cid:60) p α q ´ (cid:60) p β q (cid:60) p µ j q ´ (cid:61) p β q (cid:61) p µ j q ´ ˆ g j ¯ ´ (cid:61) p α q ` (cid:61) p β q (cid:60) p µ j q ` (cid:60) p β q (cid:61) p µ j q ˘¯ “ ´ (cid:60) p α q ˆ g j ` ` (cid:60) p α q ` (cid:60) p β q (cid:60) p µ j q ´ (cid:61) p β q (cid:61) p µ j q ˘ ´ (cid:61) p α q ` (cid:61) p β q (cid:60) p µ j q ` (cid:60) p β q (cid:61) p µ j q ˘˘ “ ´ (cid:60) p α q ˆ g j ` p (cid:60) p α q (cid:60) p β q ´ (cid:61) p α q (cid:61) p β qq (cid:60) p µ j q ´ p (cid:60) p α q (cid:61) p β q ` (cid:61) p α q (cid:60) p β qq (cid:61) p µ j q “ ´ (cid:60) p α q ˆ g j ` (cid:60) p αβ q (cid:60) p µ j q ´ (cid:61) p αβ q (cid:61) p µ j q Thus , ω j ` 1 “ ω j ` (cid:60) p α µ j ` 1 q ðñ ω j ` 1 “ ω j ´ (cid:60) p α q ˆ g j ` (cid:60) p αβ q (cid:60) p µ j q ´ (cid:61) p αβ q (cid:61) p µ j q 16 A . 1 Theorem 1 Proof Sketch Theorem 1 ( Consequence of Prop . 4 . 4 . 1 [ 44 ] ) . Convergence rate of complex momentum : If the spectral radius ρ p R q “ ρ p ∇ r µ , ω s F α , β q ă 1 , then , for r µ , ω s in a neighborhood of r µ ˚ , ω ˚ s , the distance of r µ j , ω j s to the stationary point r µ ˚ , ω ˚ s converges at a linear rate O pp ρ p R q ` (cid:15) q j q , @ (cid:15) ą 0 . Proof . We reproduce the proof for a simpler case of quadratic games , which is simple case of [ 27 ] ’s well - known method for analyzing the convergence of iterative methods . [ 44 ] generalizes this result from quadratic games to when we are sufﬁciently close to any stationary point . For quadratic games , we have that ˆ g j “ p ∇ ω ˆ g q J ω j . Well , by Lemma 1 and Lemma 2 we have : ¨ ˝ (cid:60) p µ j ` 1 q (cid:61) p µ j ` 1 q ω j ` 1 ˛ ‚ “ R ¨ ˝ (cid:60) p µ j q (cid:61) p µ j q ω j ˛ ‚ ( 27 ) By telescoping the recurrence for the j th augmented parameters : ¨ ˝ (cid:60) p µ j q (cid:61) p µ j q ω j ˛ ‚ “ R j ¨ ˝ (cid:60) p µ 0 q (cid:61) p µ 0 q ω 0 ˛ ‚ ( 28 ) We can compare µ j with the value it converges to µ ˚ which exists if R is contractive . We do the same with ω . Because µ ˚ “ Rµ ˚ “ R j µ ˚ : ¨ ˝ (cid:60) p µ j q ´ (cid:60) p µ ˚ q (cid:61) p µ j q ´ (cid:61) p µ ˚ q ω j ´ ω ˚ ˛ ‚ “ R j ¨ ˝ (cid:60) p µ 0 q ´ (cid:60) p µ ˚ q (cid:61) p µ 0 q ´ (cid:61) p µ ˚ q ω 0 ´ ω ˚ ˛ ‚ ( 29 ) By taking norms : ››››››¨˝ (cid:60) p µ j q ´ (cid:60) p µ ˚ q (cid:61) p µ j q ´ (cid:61) p µ ˚ q ω j ´ ω ˚ ˛ ‚ ›››››› 2 “ ›››››› R j ¨ ˝ (cid:60) p µ 0 q ´ (cid:60) p µ ˚ q (cid:61) p µ 0 q ´ (cid:61) p µ ˚ q ω 0 ´ ω ˚ ˛ ‚ ›››››› 2 ( 30 ) ùñ ››››››¨˝ (cid:60) p µ j q ´ (cid:60) p µ ˚ q (cid:61) p µ j q ´ (cid:61) p µ ˚ q ω j ´ ω ˚ ˛ ‚ ›››››› 2 ď ›› R j ›› 2 ››››››¨˝ (cid:60) p µ 0 q ´ (cid:60) p µ ˚ q (cid:61) p µ 0 q ´ (cid:61) p µ ˚ q ω 0 ´ ω ˚ ˛ ‚ ›››››› 2 ( 31 ) With Lemma 11 from [ 90 ] , we have there exists a matrix norm @ (cid:15) ą 0 such that : } R j } ď p ρ p R q ` (cid:15) q j ( 32 ) We also have an equivalence of norms in ﬁnite - dimensional spaces . So for all norms } ¨ } , D C ě B ą 0 such that : B } R j } ď } R j } 2 ď C } R j } ( 33 ) Combining ( 32 ) and ( 33 ) we have : ››››››¨˝ (cid:60) p µ j q ´ (cid:60) p µ ˚ q (cid:61) p µ j q ´ (cid:61) p µ ˚ q ω j ´ ω ˚ ˛ ‚ ›››››› 2 ď C p ρ p R q ` (cid:15) q j ››››››¨˝ (cid:60) p µ 0 q ´ (cid:60) p µ ˚ q (cid:61) p µ 0 q ´ (cid:61) p µ ˚ q ω 0 ´ ω ˚ ˛ ‚ ›››››› 2 ( 34 ) So , we have : › ›››››¨˝ (cid:60) p µ j q ´ (cid:60) p µ ˚ q (cid:61) p µ j q ´ (cid:61) p µ ˚ q ω j ´ ω ˚ ˛ ‚ › ››››› 2 “ O pp ρ p R q ` (cid:15) q j q ( 35 ) Thus , we converge linearly with a rate of O p ρ p R q ` (cid:15) q . 17 A . 2 Characterizing the Augmented Dynamics Eigenvalues Here , we present polynomials whose roots are the eigenvalues of our the Jacobian of our augmented dynamics Sp p R q , given the eigenvalues of the Jacobian of the joint - gradient vector ﬁeld Sp p ∇ ω ˆ g q . We use a similar decomposition as [ 25 ] . We can expand ∇ ω ˆ g “ PTP ´ 1 where T is an upper - triangular matrix and λ i is an eigenvalue of ∇ ω ˆ g . T “ » — – λ 1 ˚ . . . ˚ 0 . . . . . . . . . . . . . . . . . . ˚ 0 . . . 0 λ d ﬁ ﬃﬂ ( 36 ) We then break up into components for each eigenvalue , giving us submatrices R k P C 3 ˆ 3 : R k : “ « (cid:60) p β q ´ (cid:61) p β q ´ λ k (cid:61) p β q (cid:60) p β q 0 (cid:60) p αβ q ´ (cid:61) p αβ q 1 ´ (cid:60) p α q λ k ﬀ ( 37 ) We can get the characteristic polynomial of R k with the following Mathematica command , where we use substitute the symbols r ` iu “ λ k , a “ (cid:60) p β q , b “ (cid:61) p β q , c “ (cid:60) p α q , and d “ (cid:61) p α q . CharacteristicPolynomial [ { { a , - b , - ( r + u I ) } , { b , a , 0 } , { a c - b d , - ( b c + a d ) , 1 - c ( r + u I ) } } , x ] The command gives us the polynomial associated with eigenvalue λ k “ r ` iu : p k p x q “ ´ a 2 x ` a 2 ` acrx ` iacux ` 2 ax 2 ´ 2 ax ´ b 2 x ` b 2 ` bdrx ` ibdux ´ crx 2 ´ icux 2 ´ x 3 ` x 2 ( 38 ) Consider the case where λ k is imaginary – i . e , r “ 0 – which is true in all purely adversarial and bilinear zero - sum games . Then ( 38 ) simpliﬁes to : p k p x q “ ´ a 2 x ` a 2 ` iacux ` 2 ax 2 ´ 2 ax ´ b 2 x ` b 2 ` ibdux ´ icux 2 ´ x 3 ` x 2 ( 39 ) Our complex λ k come in conjugate pairs where λ k “ u k i and ¯ λ k “ ´ u k i . ( 39 ) has the same roots for λ k and ¯ λ k , which can be veriﬁed by writing the roots with the cubic formula . This corresponds to spiraling around the solution in either a clockwise or counterclockwise direction . Thus , we restrict to analyzing λ k where u k is positive without loss of generality . If we make the step size α real – i . e . , d “ 0 – then ( 39 ) simpliﬁes to : p k p x q “ x p´ a 2 ` iacu ´ 2 a ´ b 2 q ` a 2 ` x 2 p 2 a ´ icu ` 1 q ` b 2 ´ x 3 ( 40 ) Using a heuristic from single - objective optimization , we look at making step size proportional to the inverse of the magnitude of eigenvalue k – i . e . , α k “ α 1 | λ k | “ α 1 u k . With this , ( 40 ) simpliﬁes to : p k p x q “ x p´ a 2 ` iaα 1 ´ 2 a ´ b 2 q ` a 2 ` x 2 p 2 a ´ iα 1 ` 1 q ` b 2 ´ x 3 ( 41 ) Notably , in ( 41 ) there is no dependence on the components of imaginary eigenvalue λ k “ r ` iu “ 0 ` iu , by selecting a α that is proportional to the eigenvalues inverse magnitude . We can simplify further with a 2 ` b 2 “ | β | 2 : p k p x q “ x p (cid:60) p β qp iα 1 ´ 2 q ´ | β | 2 q ` x 2 p 2 (cid:60) p β q ´ iα 1 ` 1 q ` | β | 2 ´ x 3 ( 42 ) We could expand this in polar form for β by noting (cid:60) p β q “ | β | cos p arg p β qq : p k p x q “ x p | β | cos p arg p β qqp iα 1 ´ 2 q ´ | β | 2 q ` x 2 p 2 | β | cos p arg p β qq ´ iα 1 ` 1 q ` | β | 2 ´ x 3 ( 43 ) We can simplify further by considering an imaginary β – i . e . , (cid:60) p β q “ 0 or cos p arg p β qq “ 0 : p k p x q “ | β | 2 ´ x | β | 2 ´ x 2 p iα 1 ´ 1 q ´ x 3 ( 44 ) The roots of these polynomials can be trivially evaluated numerically or symbolically with the by plugging in β , α , and λ k then using the cubic formula . This section can be easily modiﬁed for the eigenvalues of the augmented dynamics for variants of complex momentum by deﬁning the appropriate R and modifying the Mathematica command to get the characteristic polynomial for each component , which can be evaluated if it is a sufﬁciently low degree using known formulas . 18 A . 3 Convergence Bounds Corollary 1 ( Convergence of Complex Momentum ) . There exist α P R , β P C so Algorithm 1 converges for bilinear zero - sum games . More - so , for small (cid:15) ( we show for (cid:15) “ π 16 ) , if arg p β q “ (cid:15) ( i . e . , almost - positive ) or arg p β q “ π ´ (cid:15) ( i . e . , almost - negative ) , then we can select α , | β | to converge . Proof . Note that Theorem 1 bounds the convergence rate of Algorithm 1 by Sp p R q . Also , ( 40 ) gives a formula for 3 eigenvalues in Sp p R q given α , β , and an eigenvalue λ P Sp p ∇ ω ˆ g q . The formula works by giving outputting a cubic polynomial whose roots are eigenvalues of Sp p R q , which can be trivially evaluated with the cubic formula . We denote the k th eigenspace of Sp p ∇ ω ˆ g q with eigenvalue λ k “ ic k and | c 1 | ď ¨ ¨ ¨ ď | c n | , because bilinear zero - sum games have purely imaginary eigenvalues due to ∇ ω ˆ g being antisymmetric . Eigenvalues come in a conjugate pairs , where ¯ λ k “ i p´ c k q If we select momentum coefﬁcient β “ | β | exp p i arg p β qq and step size α k “ α 1 k | c k | , and use that λ P Sp p ∇ ω ˆ g q are imaginary , then – as shown in Appendix Section A . 2 – ( 40 ) simpliﬁes to : p k p x q “ x p | β | cos p arg p β qqp iα 1 k ´ 2 q ´ | β | 2 q ` x 2 p 2 | β | cos p arg p β qq ´ iα 1 k ` 1 q ` | β | 2 ´ x 3 ( 45 ) So , with these parameter selections , the convergence rate of Algorithm 1 in the k th eigenspace is bounded by the largest root of ( 45 ) . First , consider arg p β q “ π ´ (cid:15) , where (cid:15) “ π 16 . We select α 1 k “ 0 . 75 ( equivalently , α k “ 0 . 75 | c k | ) and | β | “ 0 . 986 via grid search . Using the cubic formula on the associated p p x q from ( 45 ) the maximum magnitude root has size « 0 . 9998 ă 1 , so this selection converges in the k th eigenspace . So , selecting : ˆ α ď min k α k ( 46 ) “ min k 0 . 75 c k ( 47 ) “ 0 . 75 max k c k ( 48 ) “ 0 . 75 } ∇ ω ˆ g } 2 ( 49 ) with β “ 0 . 986 exp p i p π ´ (cid:15) qq will converge in each eigenspace . Now , consider arg p β q “ (cid:15) “ π 16 with α 1 k “ 0 . 025 and | β | “ 0 . 9 . Using the cubic formula on the associated p p x q from ( 45 ) the maximum magnitude root has size « 0 . 973 ă 1 , so this selection converges in the k th eigenspace . So , selecting : ˆ α ď min k α k ( 50 ) “ min k 0 . 025 c k ( 51 ) “ 0 . 025 max k c k ( 52 ) “ 0 . 025 } ∇ ω ˆ g } 2 ( 53 ) with β “ 0 . 9 exp p i(cid:15) q will converge in each eigenspace . Thus , for any of the choices of arg p β q we can select ˆ α , | β | that converges in every eigenspace , and thus converges . In the preceding proof , our prescribed selection of ˆ α depends on knowing the largest norm eigenvalue of Sp p ∇ ω ˆ g q , because our selections of ˆ α 9 1 } ∇ ω ˆ g } 2 . We may not have access to largest norm eigenvalue of Sp p ∇ ω ˆ g q in - practice . Nonetheless , this shows that a parameter selection exists to converge , even if it may be difﬁcult to ﬁnd . Often , in convex optimization we describe choices of α , β in terms of the largest and smallest norm eigenvalues of Sp p ∇ ω ˆ g q ( i . e . the Hessian of the loss ) [ 91 ] . 19 B Algorithms Here , we include additional algorithms , which may be of use to some readers . Algorithm 3 show aggregated momentum [ 26 ] . Algorithm 4 shows the recurrently linked momentum that generalizes and uniﬁes aggregated momentum with negative momentum [ 25 ] . Algorithm 5 shows our algorithm with alternating updates , which we use for training GANs . Algorithm 6 shows our method with all real - valued objects , if one wants to implement complex momentum in a library that does not support complex arithmetic . Algorithm 3 Aggregated Momentum 1 : Select number of buffers K P N 2 : Select β p k q P r 0 , 1 q for k “ 1 . . . K 3 : Select α p k q P R ` for k “ 1 . . . K 4 : Initialize µ 0 p k q for k “ 1 . . . K 5 : for j “ 1 . . . N do 6 : for k “ 1 . . . K do 7 : µ j ` 1 p k q “ β p k q µ j p k q ´ ˆ g j 8 : ω j ` 1 “ ω j ` ř K k “ 1 α p k q µ j ` 1 p k q return ω N Algorithm 4 Recurrently Linked Momentum 1 : Select number of buffers K P N 2 : Select β p l , k q P R for l “ 1 . . . K and k “ 1 . . . K 3 : Select α p k q P R ` for k “ 1 . . . K 4 : Initialize µ 0 p k q for k “ 1 . . . K 5 : for j “ 1 . . . N do 6 : for k “ 1 . . . K do 7 : µ j ` 1 p k q “ ř l β p l , k q µ j p l q ´ ˆ g j 8 : ω j ` 1 “ ω j ` ř K k “ 1 α p k q µ j ` 1 p k q return ω N Algorithm 5 ( AltCM ) Momentum 1 : Select β P C , α P R ` 2 : Initialize µ 0 A , µ 0 B 3 : for j “ 1 . . . N do 4 : µ j ` 1 A “ β µ jA ´ g jA 5 : θ j ` 1 A “ θ jA ` (cid:60) p α µ j ` 1 A q 6 : µ j ` 1 B “ β µ jB ´ g B p θ j ` 1 A , θ jB q 7 : θ j ` 1 B “ θ jB ` (cid:60) p α µ j ` 1 B q return ω N Algorithm 6 ( SimCM ) Complex Momentum - R valued 1 : Select (cid:60) p β q , (cid:61) p β q , (cid:60) p α q , (cid:61) p α q P R 2 : Select (cid:60) p β q , (cid:61) p β q , (cid:60) p α q , (cid:61) p α q P R 3 : Initialize (cid:60) p µ q 0 , (cid:61) p µ q 0 4 : for j “ 1 . . . N do 5 : (cid:60) p µ j ` 1 q “ (cid:60) p β q (cid:60) p µ j q ´ (cid:61) p β q (cid:61) p µ j q ´ ˆ g j 6 : (cid:61) p µ j ` 1 q “ (cid:60) p β q (cid:61) p µ j q ` (cid:61) p β q (cid:60) p µ j q 7 : ω j ` 1 “ ω j ´ (cid:60) p α q ˆ g j ` (cid:60) p αβ q (cid:60) p µ j q´ (cid:61) p αβ q (cid:61) p µ j q return ω N B . 1 Complex Momentum in PyTorch Our method can be easily implemented in PyTorch 1 . 6 + by using complex tensors . The only necessary change to the SGD with momentum optimizer is extracting the real - component from momentum buffer as with JAX – see here . In older versions of Pytorch , we can use a tensor to represent the momentum buffer µ , step size α , and momentum coefﬁcient β . Speciﬁcally , we represent the real and imaginary components of the complex number independently . Then , we redeﬁne the operations _ _ add _ _ and _ _ mult _ _ to satisfy the rules of complex arithmetic – i . e . , equations ( 23 ) and ( 24 ) . C Experiments C . 1 Computing Infrastructure and Runtime For the purely adversarial experiments in Sections 4 . 1 and 4 . 2 , we do our computing in CPU . Training each 2 D GAN in Section 4 . 3 takes 2 hours and we can train 10 simultaneously on an NVIDIA T 4 GPU . Training each CIFAR GAN in Section 4 . 4 takes 10 hours and we can only train 1 model per NVIDIA T4 GPU . C . 2 Optimization in Purely Adversarial Games We include the alternating update version of Figure 4b in Figure 8 , which allows us to contrast simultaneous and alternating updates . With alternating updates on a Dirac - GAN for α “ 0 . 1 the best value for the momentum coefﬁcient β was complex , but we could converge with real , negative momentum . Simultaneous updates may be a competitive choice with alternating updates , only if alternating updates cost two gradient evaluations per step , which is common in deep learning setups . 20 M o m e n t u m ph a s e a r g p β q SimCM for # grad . eval . = # steps Momentum Magnitude | β | AltCM for # grad . eval . # g r a d . e v a l . t o c onv e r g e AltCM for # steps # s t e p s t o c onv e r g e Figure 8 : We show many steps and gradient evaluations , both simultaneous and alternating complex momentum on a Dirac - GAN take for a set solution distance . We ﬁx step size α “ 0 . 1 as in Figure 3 , while varying the phase and magnitude of our momentum β “ | β | exp p i arg p β qq . There is a red star at the optima , dashed red lines at real β , and a dashed magenta line for simultaneous or alternating gradient descent . We only display color for convergent setups . Left : Simultaneous complex momentum ( SimCM ) . This is the same as Figure 4b , which we repeat to contrast with alternating updates . There are no real - valued β that converge for this – or any – α with simultaneous updates [ 25 ] . Simultaneous updates can parallelize gradient computation for all players at each step , thus costing only one gradient evaluation per step for many deep learning setups . The best rate of convergence per step and gradient evaluation is « 0 . 955 . Middle : Alternating complex momentum ( AltCM ) , where we show how many gradient evaluations – as opposed to steps – to reach a set solution distance . Alternating updates are bottlenecked by waiting for ﬁrst player’s update to compute the second players update , effectively costing two gradient evaluations per step for many deep learning setups . Negative momentum can converge here , as shown by [ 25 ] , but the best momentum is still complex . Also , Alternating updates can make the momentum phase arg p β q choice less sensitive to our convergence . The best rate of convergence per gradient evaluation is « 0 . 965 . Right : AltCM , where we show how many steps to reach a set solution distance . The best rate of convergence per step is « 0 . 931 . Takeaway : If we can parallelize computation of both players gradients we can beneﬁt from SimCM , however if we can not then AltCM can converge more quickly and for a broader set of optimizer parameters . In any case , the best solution uses a complex momentum β for this α . C . 3 How Adversarialness Affects Convergence Rates We include the extragradient ( EG ) update with extrapolation parameter α 1 and step size α : ω j ` 12 “ ω j ´ α 1 ˆ g j ( EG ) ω j ` 1 “ ω j ´ α ˆ g j ` 12 and the optimistic gradient ( OG ) update with extrapolation parameter α 1 and step size α : ω j ` 1 “ ω j ´ 2 α ˆ g j ` α 1 ˆ g j ´ 1 ( OG ) Often , EG and OG are used with α “ α 1 , however we found that this constraint crippled these methods in cooperative games ( i . e . , minimization ) . As such , we tuned the extrapolation parameter α 1 separately from the step size α , so EG and OG were competitive baselines . We include Figure 9 which investigates a GANs spectrum throughout training , and elaborates on the information that is shown in Figure 7 . This shows that there are many real and imaginary eigenvalues , so GAN training is neither purely cooperative or purely adversarial . Also , the structure of the set of eigenvalues for the discriminator is different than the generator , which may motivate separate optimizer choices . The structure between the players persists through training , but the eigenvalues grow in magnitude and spread out their phases . This indicates how adversarial the game is can change during training . C . 4 Training GANs on 2 D Distributions For 2 D distributions , the data is generated by sampling from a mixture of 8 Gaussian distributions , which are distributed uniformly around the unit circle . For the GAN , we use a fully - connected network with 4 hidden ReLU [ 92 ] layers with 256 hidden units . We chose this architecture to be the same as [ 25 ] . Our noise source for the generator is a 4 D Gaussian . We trained the models for 100 000 iterations . The performance of the optimizer settings is evaluated by computing the negative log - likelihood of a batch of 100 000 generated 2 D samples . 21 J o i n t - g r a d i e n t ˆ g i nd e x l l og p | a l k | ` (cid:15) q f o r a l k i n ∇ ω ˆ g j Joint - parameter ω index k Jacobian of joint - gradient ∇ ω ˆ g j for GAN A b s o f e i g e nv ec t o r c o m pon e n t Index into v – ﬁrst 337 are for D’s params Eigenvector v components for eigenvalues λ P Sp p ∇ ω ˆ g q - π - π 2 0 π 2 π P h a s e o f e i g e nv a l u e a r g p λ q d i s c . un s u r e g e n . D o e s e i g e nv ec t o r po i n t a t a p l a y e r ? Log - magnitude of eigenvalue log p | λ | q Start of training End of training Log - polar graph of the spectrum of Jacobian of the joint - gradient Sp p ∇ ω ˆ g j q throughout training Figure 9 : These plots investigate the spectrum of the Jacobian of the joint - gradient for the GAN in Figure 7 through training . The spectrum is key for bounding convergence rates in learning algorithms . Top left : The Jacobian ∇ ω ˆ g for a GAN on a 2 D mixture of Gaussians with a two - layer , fully - connected 16 hidden unit discriminator ( D ) and generator ( G ) at the end of training . In the concatenated parame - ters ω P R 723 , the ﬁrst 337 are for D , while the last 386 are for G . We display the log of the absolute value of each component plus (cid:15) “ 10 ´ 10 . The upper left and lower right quadrants are the Hessian of D and G’s losses respectively . Top Right : We visualize two randomly sampled eigenvectors from ∇ ω ˆ g . The ﬁrst part of the param - eters is for the discriminator , while the second part is for the generator . Given an eigenvalue with eigenvector v , we roughly approximate attributing eigenvectors to players by calculating how much of it lies in D’s parameter space with } v 1 : | D | } 1 } v } 1 “ } v 1 : 337 } 1 } v } 1 . If this ratio is near 1 ( or 0 ) and say the eigenvector mostly points at D ( or G ) . The blue eigenvector mostly points at G , while the orange eigenvector is unclear . Finding useful ways to attribute eigenvalues to players is an open problem . Bottom : The spectrum of the Jacobian of the joint - gradient Sp p ∇ ω ˆ g j q is shown in log - polar coordi - nates , because it is difﬁcult to see structure when graphing in Cartesian ( i . e . , (cid:60) and (cid:61) ) coordinates , due to eigenvalues spanning orders of magnitude , while being positive and negative . The end of training is when we stop making progress on the log - likelihood . We have imaginary eigenvalues at arg p λ q “ ˘ π { 2 , positive eigenvalues at arg p λ q “ 0 , and negative eigenvalues at arg p λ q “ ˘ π . Takeaway : There is a banded structure for the coloring of the eigenvalues that persists through training . We may want different optimizer parameters for the discriminator and generator , due to asymmetry in their associated eigenvalues . Also , the magnitude of the eigenvalues grows during training , and the arg s spread out indicating the game can change eigenstructure near solutions . 22 M o m e n t u m ph a s e a r g p β q Step size α “ 0 . 001 Momentum Magnitude | β | Step Size α “ 0 . 003 N LL , l o w e r = b e tt e r Figure 10 : Heatmaps of the negative log - likelihood ( NLL ) for tuning arg p β q , | β | with various ﬁxed α on a 2 D mixture of Gaussians GAN . We highlight the best performing cell in red , which had arg p β q « π { 8 . Runs equivalent to alternating SGD are shown in a magenta box . We compare to negative momentum with alternating updates as in [ 25 ] in the top row with arg p β q “ π . Left : Tuning the momentum with α “ 0 . 001 . Right : Tuning the momentum with α “ 0 . 003 . Figure 11 ( a ) : Mixture of Gaussian samples from GAN with the best hyperparameters from the heatmaps in Appendix Figure 10 Figure 11 ( b ) : Class - conditional CIFAR - 10 sam - ples from GAN with the best hyperparameters from the heatmap in 12a M o m e n t u m ph a s e a r g p β 1 q Momentum magnitude | β 1 | I n ce p ti on s c o r e , h i g h e r = b e tt e r Figure 12 ( a ) : The inception score ( IS ) for a grid search on arg p β 1 q and | β 1 | for training Big - GAN on CIFAR - 10 with the Adam variant in Algo - rithm 2 . The β 1 is complex for the discriminator , while the generator’s optimizer is ﬁxed to author - supplied defaults . Red points are runs that failed to train to the minimum IS in the color bar . The vertical magenta line denotes runs equivalent to al - ternating SGD . Negative momentum failed to train for any momentum magnitude | β 1 | ą . 5 , so we do not display it for more resolution near values of interest . I n ce p ti on s c o r e ( I S ) Iteration Figure 12 ( b ) : We compare the best optimization parameters from grid search Figure 12a for our complex Adam variant ( i . e . , Algorithm 2 ) shown in green , with the author provided values shown in red for the CIFAR - 10 BigGAN over 10 seeds . A star is displayed at the best IS over all runs , a cross is displayed at the worst IS over all runs , while a circle is shown at the best IS for each run . Dashed lines are shown at the max / min IS over all runs at each iteration , low - alpha lines are shown for each runs IS , while solid lines are shown for the average IS over all seeds at each iteration . The results are summarized in Table 1 . 23