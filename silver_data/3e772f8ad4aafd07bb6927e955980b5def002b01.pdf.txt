Self - supervised Multi - view Disentanglement for Expansion of Visual Collections Nihal Jain âˆ— nihalj @ cs . cmu . edu Carnegie Mellon University Pittsburgh , United States Praneetha Vaddamanu â€  pvaddama @ cs . cmu . edu Carnegie Mellon University Pittsburgh , United States Paridhi Maheshwari â€  paridhi @ stanford . edu Stanford University Stanford , United States Vishwa Vinay vinay @ adobe . com Adobe Research Bangalore , India Kuldeep Kulkarni kulkulka @ adobe . com Adobe Research Bangalore , India Figure 1 : Left : query collection containing a set of images . Right : each row is a ranked list of images that match the query using three notions of image similarity â€“ objects , style , color composition â€“ each of which we refer to as a â€˜viewâ€™ . The top row weighs the views equally . The bottom row ( our approach ) weighs each view proportional to the inferred intent of the query collection . This enhances relevance ( along the primary view - objects ) and diversity ( along other views - style and color ) . ABSTRACT Image search engines enable the retrieval of images relevant to a query image . In this work , we consider the setting where a query for similar images is derived from a collection of images . For visual search , the similarity measurements may be made along multiple axes , or views , such as style and color . We assume access to a set of feature extractors , each of which computes representations for a specific view . Our objective is to design a retrieval algorithm that ef - fectively combines similarities computed over representations from multiple views . To this end , we propose a self - supervised learning method for extracting disentangled view - specific representations for images such that the inter - view overlap is minimized . We show how this allows us to compute the intent of a collection as a distribu - tion over views . We show how effective retrieval can be performed by prioritizing candidate expansion images that match the intent of a query collection . Finally , we present a new querying mechanism for image search enabled by composing multiple collections and perform retrieval under this setting using the techniques presented in this paper . 1 âˆ— Work done during an internship at Adobe Research . â€  Work done when authors were at Adobe Research . 1 A version of this paper has been accepted at WSDM 2023 . 1 INTRODUCTION The task of image search [ 2 ] requires the definition of specific axes along which image similarities can be computed . The refer - ence standard is the use of embeddings from intermediate layers of convolutional neural networks ( CNNs ) trained for supervised classification tasks [ 14 ] , which have been shown to have superior effectiveness in visual search tasks [ 2 ] . To enable retrieval along specialized notions of image similarity , multiple image feature ex - tractors have been developed . Some examples include shapes within content [ 22 ] , co - occurrences of objects and their relationships [ 17 ] , or styles [ 25 ] . We build on existing image representation meth - ods ( e . g . those described above ) , using the term â€˜viewâ€™ to refer to a representation capturing one aspect of the content within an image . We consider the setting of designers working within the context of a visual creation task . As part of ideation , designers typically compile a set of inspirational assets that represents the desired visual characteristics of the target creation â€“ such a collection is referred to as a â€œMoodboardâ€ [ 23 ] . In this work , we focus on Mood - board Expansion , i . e . , retrieving other visual assets from a corpus of images that match the userâ€™s intent as expressed by a moodboard ; this is a version of image search where the query is a collection of images . Our proposed method for moodboard expansion infers the intent of the query collection , and we show how this enables effective retrieval . The intent inference mechanism leverages the fact that , unlike typical retrieval settings where the query is often sparse ( e . g . a short textual phrase ) , in the current setting , we have a r X i v : 2302 . 02249v1 [ c s . C V ] 4 F e b 2023 Nihal Jain , Praneetha Vaddamanu , Paridhi Maheshwari , Vishwa Vinay , & Kuldeep Kulkarni access to a collection of images . It also provides a convenient visual querying mechanism for our target user personas , who operate in a domain where it might be difficult to express the information need in textual form . A stylized image of our application setting is shown in Figure 2 . On the left is an example moodboard . To surface new candidate additions to the moodboard , we are required to define a notion of similarity for retrieval from a corpus of images â€“ there could be multiple visual characteristics that we want to consider ( e . g . , object information , color or style ) . We formalize each visual characteristic as a separate representation space in which similar images may be found â€“ we have used the term views to refer to these alternative representations . In each subplot of Figure 2 , the point C indicates the collection - level representation of the moodboard along the corresponding view . We pictorially depict distances of images from this collection representation in each representation space . Solely using object representations for similarity surfaces the set A on the right , whereas incorporating information along other views may allow the retrieval of content with greater style and color diversity ( set B ) , which may aid more effective visual exploration . We describe a self - supervised model , the inputs into which are well known single - view representations , that provides disentangled view - specific representations for individual images . We develop an algorithm that utilizes these disentangled multi - view representa - tions for inferring the intent of a collection and ranks candidate images utilizing the predicted intents . Finally , we provide an em - pirical study that evaluates our representation learning and image ranking setups . In addition to retrieving images relevant to a query collection , our algorithm ensures diversification of results in the absence of a strong signal along certain views . Figure 1 provides an illustrative example of our setup and highlights the desired characteristics of the results . Learning view - specific representations for collections of images allows a novel use - case concerning image search : given two ( or possibly more ) collections of images , we can compose these to hallucinate a new set of images that has selective characteristics from each query collection ; we can then retrieve relevant images from an index that match the intent of this hallucinated collection . We present an effective approach to enable this using the techniques presented in this paper . Section 6 discusses this in more detail and provides qualitative results achieved using our method . Our main contributions are as follows : ( 1 ) We describe a self - supervised multi - view representation learn - ing method for images . Our model provides a framework to disentangle view - specific information distinct from what is common across views . ( 2 ) We propose an approach to use the view - specific representa - tions output by our model to compute the intent for a collec - tion of images . We validate our intent prediction method via a simulation - based study . ( 3 ) We present experimental results that show how our proposed method leads to more effective visual retrieval than baseline approaches . ( 4 ) Finally , we propose a novel querying mechanism for image search driven by composing collections of images , and solve this task using other techniques presented in this paper . 2 RELATED WORK In this section , we review work related to the area of multi - view representation learning of images , as well as retrieval support for visual exploration and ideation activities . 2 . 1 Multi - View Representation Learning There has been significant recent interest in representation learning for multimodal content items . Methods for extracting representa - tions from multiple views [ 15 , 37 ] usually focus on enforcing align - ment across views . Specifically , representations from the different views of the same item are mapped to a shared space , with some notion of closeness enforced between them . These works motivate the need for alignment using a cross - modal task [ 28 ] , e . g . , matching a text caption to the image modality or text - to - image retrieval [ 33 ] . In contrast , our work deals with a naturally multi - view task : even though the views are expected to capture overlapping infor - mation , our focus is on extracting the information which is specific to a given view . This notion is closely related to the area of fac - torized representations . The authors of [ 31 ] motivate the need to separately model factors common across modalities from modality specific factors . We borrow this need , but our definition of view is more general since these alternative representations can be de - rived from the same modality ( images in our case ) . Other authors ( e . g . [ 8 , 30 ] ) invoke similar intuitions and refer to the desired be - haviour as disentanglement - we use this word and orthogonalization interchangeably in the current paper . 2 . 2 Visual Exploration The sub - field of Content - based Image Retrieval ( CBIR ) [ 26 ] contains many works that are relevant to the topic of the current paper . This includes the need for extraction of the right feature representation for the images , as well as the definition of an appropriate notion of similarity to be used for retrieval . Closest in motivation to our work is â€œMindReaderâ€ [ 11 ] , where the authors focus on mechanisms that allow creatives to construct visual queries without resorting to keyword - based interfaces . So as to cover a range of plausible user requirements , MindReader utilizes multiple dimensions ( shapes , textures , colors ) with the user specifying the relative importance of each . They also account for the correlations between the dimen - sions , which is also the target of our orthogonalization procedure . Our work attempts to tackle moodboard expansion by recom - mending visual variations that are relevant to the user . Our premise is that the variations that are related to existing assets can encour - age ideation . Facilitating design ideation via moodboards has been explored by Rieuf et al [ 23 ] , though their focus was an immersive interface into the corpus of assets . The recent work of Koch et al [ 13 ] recommends that the exploratory process of ideation be an interactive one , with the system each time refining its view of the userâ€™s intent . Solving for the needs of creatives involves a holistic treatment that includes interface , interaction and many more di - mensions . The current paper focuses on the quantitative evaluation of a single retrieval iteration . This retrieval is via a weighted simi - larity across views , where the weights are a prediction of intent of the query collection . The disentangled / orthogonalized multi - view representations of images are central to this process , and are the outputs of our self - supervised model that we describe next . Self - supervised Multi - view Disentanglement for Expansion of Visual Collections Figure 2 : A reference example that is intended to motivate the use of multiple views for collection - based retrieval . 2 . 3 Compositional Representation Learning Recent progress in representation learning has enabled combining representations from multiple simple individual elements to learn representations for complex entities . These simple elements may be from different modalities such as image and text [ 1 , 3 , 34 ] or the same modality [ 19 , 20 ] . Our work focuses on the problem of learning effective representations for collections of images . We demonstrate further utility of these representations by showing how these can be used to achieve composition in a zero - shot manner , i . e . , without further fine - tuning for this task . Finally , we propose composing collections as a new way of querying images and solve this task using our representations in Section 6 . 3 RETRIEVAL USING DISENTANGLED REPRESENTATIONS Figure 3 : Our proposed model . Input features ğ‘¥ âˆ— are trans - formed into two outputs : ( 1 ) z p âˆ— are specific to a view with orthogonalization encouraged amongst them ; ( 2 ) z a âˆ— are aligned to capture common information across views . The expansion of a moodboard requires us to understand the motivation behind collating the member images of the collection representing the moodboard . Towards this , we propose the use of representations for collections . We argue ( and show empirically , in Section 5 . 2 ) that for effective retrieval , collection level represen - tations need to have independent view - specific components . We next characterize view - specific collection level representations and outline our self - supervised approach to obtain them . 3 . 1 View - Specific Representation Learning Our primary premise is that each image can be described by multiple alternative descriptors of its content . For example , images can be projected into a space capturing color semantics such that nearby points have similar color composition ; these images can also be projected onto a different space where closeness captures style properties of images . We refer to color and style as views , such that alternative aspects of image content are captured by different views . Our hypothesis is that collection - based retrieval requires effective view - specific understanding of the images comprising the collection . While obtaining shared information across views has been studied in other works ( e . g . [ 32 ] ) , in the following sections , we focus on recovering information that is unique to each view . 3 . 1 . 1 Views and Out - of - the - Box Representations . The set of views we consider in this paper are specifically chosen based on their central role in visual exploration : ( 1 ) Object : The object view of images forms the essence of several computer vision tasks such as object detection and segmen - tation . The ResNet - 152 model [ 10 ] was trained for an object classification task , so , embeddings from its penultimate layer are taken to be the object view ; the expectation is that images with similar ResNet embeddings contain visually similar objects . ( 2 ) Style : Our data , described in Section 4 , is derived from an artis - tic domain , which motivates us to consider the style view . We use the outputs of the ALADIN architecture [ 25 ] , which was developed to retrieve images based on artistic style similarity . ( 3 ) Color : Since our setting is one of visual discovery , we consider color due to its important role in image retrieval . We utilize the LAB space , with the â€˜ ğ¿ â€™ corresponding to luminance and the other two views representing chrominance . As with reference works in this domain [ 27 ] , the ranges of ğ¿ , ğ´ and ğµ values are discretized into bins of width ( 10 , 10 , 10 ) , and a color embedding is obtained as a histogram over what fraction of pixels contain a particular LAB value . We have considered three views for images and associated well - known and state - of - the - art feature extractors â€” retrieval using these out - of - the - box representations serve as our baselines . Finally , we note that while we have only described three views , our setup extends naturally to being able to consider a larger enumeration Nihal Jain , Praneetha Vaddamanu , Paridhi Maheshwari , Vishwa Vinay , & Kuldeep Kulkarni of visual axes . We leave a thorough study of the possible range of visual dimensions and their interactions to future work . 3 . 1 . 2 Disentanglement Model . We intuit that there exist correla - tions amongst the views : for example , nearest neighbors obtained using ResNet features might also capture similarities in color . Sev - eral efforts that study the extraction of features common and spe - cific to views from multi - view data have relied on reconstructing original features from factorized representations [ 9 , 30 ] . Our ap - proach , illustrated in Figure 3 , relies on the same intuition . We factorize the input into two components - the view - aligned rep - resentations contain what is common across the views , and the view - specific representations contain information unique to each view . These are then combined to reconstruct the input . We begin with input representations x m âˆˆ R ğ‘‘ ğ‘š for each data point , where view ğ‘š âˆˆ M has input representations of size ğ‘‘ ğ‘š , and as described previously , we have M = { object , style , color } in this paper . Our model processes the input through three neural network pathways : ( 1 ) View - specific : z pm = F ğ‘ğ‘š ( x m ) ( 2 ) View - aligned : z am = F ğ‘ğ‘š ( u ) , where u = F ğ‘¢ ( [ x âˆ— ] ) , with [ x âˆ— ] being the concatenation of all input view representations ( 3 ) Reconstruction : Â¯x m = F ğ‘Ÿğ‘š ( [ z pm ; z am ] ) , where [ z pm ; z am ] is a con - catenation of z pm and z am Here , all F âˆ— ğ‘š and F ğ‘¢ are two - layer and one - layer feed - forward networks respectively , where ğ‘…ğ‘’ğ¿ğ‘ˆ non - linear activation [ 21 ] is used between layers . For ease of notation , we stack B individual feature vectors into a batch to form data matrices : X ğ‘š = [ x m ] contains input rep - resentations , Z ğ‘ğ‘š = [ z pm ] contains view - specific representations , Z ğ‘ğ‘š = [ z am ] contains view - aligned representations , and Â¯ X ğ‘š = [ Â¯x m ] contains reconstructed representations for view ğ‘š . All representa - tions that are the output of our models are normalized to be of unit norm , and an inner product between them serves as the definition of similarity between the corresponding embeddings . We note that F ğ‘ğ‘š is equivalent to multimodal models that project different views to the same underlying space to obtain aligned rep - resentations [ 4 ] . Our intention in the current paper is to extract view - specific representations that capture what is uniquely con - tained within that view . The role of model components F ğ‘Ÿğ‘š and F ğ‘ğ‘š is to ensure that there is minimal loss of information with respect to the input x m . Therefore , though the focus in the current paper is on F ğ‘ğ‘š , the complete architecture illustrated in Figure 3 is required to obtain robust and useful representations . 3 . 1 . 3 Model Fitting . We define the following loss function that is minimized to estimate the parameters of our proposed model : L = ğœ† 1 Â· L ğ‘ğ‘™ğ‘– + ğœ† 2 Â· L ğ‘ ğ‘ğ‘ + ğœ† 3 Â· L ğ‘–ğ‘›ğ‘“ + ğœ† 4 Â· L ğ‘Ÿğ‘’ğ‘ ( 1 ) The ğœ† ğ‘– â€™s are hyperparameters that control the contribution of the various factors we are attempting to balance . The loss terms in Equation 1 are defined below with respect to batches of size B . â€¢ Inter - view Alignment Loss ( L ğ‘ğ‘™ğ‘– ) : We use the following ob - jective to align representations from every pair of views ( ğ‘š , ğ‘š â€² ) : L ğ‘ğ‘™ğ‘– = 1 B âˆ‘ï¸ ( ğ‘š , ğ‘š â€² ) (cid:16) B âˆ’ ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘’ ( Z ğ‘ğ‘š Ã— Z ğ‘ ğ‘‡ ğ‘š â€² ) (cid:17) ( 2 ) This term encourages Z ğ‘ğ‘š and Z ğ‘ğ‘š â€² to be aligned . It is designed to reward an increased similarity between aligned representations of the same data point from different views ğ‘š & ğ‘š â€² - captured by the diagonal entries of the matrix Z ğ‘ğ‘š Ã— Z ğ‘ ğ‘‡ ğ‘š â€² . â€¢ Inter - view Orthogonalization Loss (cid:0) L ğ‘ ğ‘ğ‘ (cid:1) : This is an orthog - onality constraint minimizing the overlap between pairs ( ğ‘š , ğ‘š â€² ) : L ğ‘ ğ‘ğ‘ = âˆ‘ï¸ ( ğ‘š , ğ‘š â€² ) 1 ğ‘‘ ğ‘š âˆ— ğ‘‘ â€² ğ‘š (cid:13)(cid:13)(cid:13) Z ğ‘ ğ‘‡ ğ‘š Ã— Z ğ‘ğ‘š â€² (cid:13)(cid:13)(cid:13) 2 ğ¹ ( 3 ) Since Z ğ‘ğ‘š containsunitnormvectors , L ğ‘ ğ‘ğ‘ isthesquaredFrobenius - norm of the cross - correlation matrix between pairs of views . â€¢ Intra - view Information Transfer Loss (cid:16) L ğ‘–ğ‘›ğ‘“ (cid:17) : To prevent degenerate view - specific representations , we introduce a regu - larization term to retain information content within a view : L ğ‘–ğ‘›ğ‘“ = 1 B âˆ‘ï¸ ğ‘š (cid:16) B âˆ’ ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘’ ( X ğ‘š Ã— Z ğ‘ ğ‘‡ ğ‘š ) (cid:17) ( 4 ) Since X ğ‘š and Z ğ‘ğ‘š contain unit norm vectors , minimizing L ğ‘–ğ‘›ğ‘“ maximizes the cosine similarity between view - specific and input representations for each sample . â€¢ Intra - view Reconstruction Loss ( L ğ‘Ÿğ‘’ğ‘ ) : The reconstruction loss is the mean squared error between input representations x m and their estimate Â¯x m . L ğ‘Ÿğ‘’ğ‘ = âˆ‘ï¸ ğ‘š 1 B âˆ— ğ‘‘ ğ‘š (cid:13)(cid:13) Â¯ X ğ‘š âˆ’ X ğ‘š (cid:13)(cid:13) 2 ğ¹ ( 5 ) Alternative formulations for these losses are possible , and we leave such explorations to future work . 3 . 2 Collection - based Retrieval In this section , we describe how the view - specific representations are used for inferring the intent of the collection of images and measuring how true a candidate image is to this intent . 3 . 2 . 1 Representing a Collection . We denote a collection of ğ‘ im - ages as C . Let the view - specific representation for view ğ‘š of the ğ‘– ğ‘¡â„ image in C be denoted as z pm , i âˆ€ ğ‘– âˆˆ { 1 , . . . , ğ‘ } , which we ob - tain as outputs of our model described in Section 3 . 1 . We define the collection - level representation of C for view ğ‘š as the mean of the view - specific representations over its member images : C pm = 1 ğ‘ (cid:205) ğ‘ğ‘– = 1 z pm , i . Computing a query in this manner is similar to how psuedo relevance feedback is used for image retrieval [ 38 ] . 3 . 2 . 2 Intent Computation . Given a collection C , we are interested in inferring why its member images were brought together . We model this by characterizing the intent as being proportional to the degree of homogeneity of images in the collection along that view . We obtain the raw intent of a collection with respect to view ğ‘š as the average similarity along view ğ‘š across all pairs of images in Self - supervised Multi - view Disentanglement for Expansion of Visual Collections the collection : Ë† ğ›½ ğ‘š = 1 ğ‘ Ã— ( ğ‘ âˆ’ 1 ) âˆ‘ï¸ ( ğ‘– , ğ‘— ) z pm , i Â· z pm , j ( 6 ) Note that the summand in Equation 6 computes the average cosine similarity since the output view - specific vectors are normalized . To ensure that these raw intent weights are comparable across views , we standardize them using statistics obtained from each viewâ€™s embedding space : ğ›½ ğ‘š = Ë† ğ›½ ğ‘š âˆ’ ğœ‡ ğ‘š ğœ ğ‘š ( 7 ) where ğœ‡ ğ‘š and ğœ ğ‘š are the mean and standard deviation of the pair - wise similarities between all pairs of images from the dataset mea - sured along view ğ‘š . Finally , our definition of intent is a normaliza - tion across views so that the intent weights sum to 1 : ğ›¼ ğ‘š = ğ‘’ğ‘¥ğ‘ ( ğ›½ ğ‘š ) âˆ‘ï¸ ğ‘š â€² ğ‘’ğ‘¥ğ‘ ( ğ›½ ğ‘š â€² ) ( 8 ) 3 . 2 . 3 Weighted Similarity for Retrieval . Given a collection C , we are interested in ranking a corpus of images D in decreasing order of relevance to C . Given a candidate image ğ‘‘ âˆˆ D , we obtain its view - specific representations d pm as outputs of our model . We then assign a score to ğ‘‘ using a weighted similarity metric as : ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ( C , ğ‘‘ ) = âˆ‘ï¸ ğ‘š ğ›¼ ğ‘š Ã— ğ‘ ğ‘–ğ‘š (cid:16) C pm , d pm (cid:17) ( 9 ) where C pm is the the view - specific representations for the collection . And , ğ›¼ ğ‘š is computed as in Equation 8 and ğ‘ ğ‘–ğ‘š ( a , b ) is a measure of similarity between a and b . For input style and color representations , we use the inverse of the ğ¿ 2 distance between a and b as the distance measure [ 18 , 25 ] . For other representations , we use ğ‘ ğ‘–ğ‘š ( a , b ) = a Â· b . Equation 9 reflects our idea that views corresponding to C â€™s intent should be given a higher weight while ranking by relevance . Finally , we obtain a ranked list R by sorting D in decreasing order of ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ( C , ğ‘‘ ) values âˆ€ ğ‘‘ âˆˆ D . We discuss the evaluation of R in Section 5 . 2 . 4 EXPERIMENTAL SETUP We use the Behance - Artistic - Media dataset ( BAM ) [ 35 ] , a publicly - available dataset of artistic images . In particular , we use the crowd - annotated subset of BAM , containing 331 , 116 images ( after filtering out broken links ) . Several images in BAM are annotated with one or more of 3 attributes â€“ ( i ) content ( associated with 143 , 480 images ) , ( ii ) media ( 60 , 225 images ) , and ( iii ) emotion ( 24 , 844 images ) . Each attribute corresponds to multiple attribute classes into which the image may be categorized . Table 1 shows the distribution of images among all attributes and classes in BAM . For each image in BAM , we obtain the outputs of the out - of - the - box feature extractors described earlier â€“ ResNet ( object ) , ALADIN ( style ) , and LAB Histogram ( color ) â€“ as our model inputs . We divide the dataset into train , validation , and test sets , maintaining a 6 : 3 : 1 ratio . While the validation set is used for hyperparameter tuning , images in the test set are set aside to enable the evaluation of our model on the collection expansion task . Table 1 : Attributes and Attribute Classes in the BAM dataset . Attribute # Images Attribute Classes content 143 , 480 bicycle , bird , building , cars , cat , dog , flower , people , tree media 60 , 225 3D graphics , vector art , watercolor , pencil sketch , comic , pen ink , oil paint emotion 24 , 844 happy , gloomy , scary , peaceful 4 . 1 Simulating Collections To evaluate the performance of our approach on the task of mood - board expansion , we simulate the gathering of moodboards of im - ages with known intent using the attribute labels in BAM . By pick - ing a subset of images that are all annotated with the same attribute class , we obtain a collection of images that are similar with respect to that characteristic . For example , gathering a collection of dog images may be simulated by picking a set of images from BAM with the label content = â€˜dogâ€™ . We refer to such simulated collection as { attribute } - type collections , where { attribute } âˆˆ { content , media , emotion } . As another example , a sample of images tagged with emotion = â€˜happyâ€™ may be taken to represent an emotion - type collection . Given a collection of a known attribute type , we retrieve additional candidate images that are relevant to the collection us - ing the method described in Section 3 . 2 . 3 . To judge the relevance of retrieved results , we compute ranking metrics on the top - 100 retrieved results using the attribute types as labels . 4 . 1 . 1 Ground Truth Intents . There are implicit correlations be - tween the attribute labels of BAM and the views we have considered . For each attribute , we identify the view that we expect the attribute to have a high correlation with . We state the following associations between views and types of collections : â€¢ content - type collections have high object intent â€¢ media and emotion - type collections have high style intent Note that this is knowledge we possess about the dataset , these associations are not made available to the model , which only has access to the input feature extractors for each view . The purpose of intent inference would be to recover the view correlated with the collectionâ€™s attribute - type as having the highest weight . We demonstrate this via empirical experiments in Section 5 . 2 . 1 . 4 . 2 Metrics Our evaluation setup comprises of two phases : ( i ) intrinsic : we assess the quality of view disentanglement achieved using our approach , and ( ii ) extrinsic : we evaluate the effectiveness of our model by computing relevance metrics for the retrieved results . 4 . 2 . 1 EvaluatingDisentanglement . Weuse twometrics toquan - tify the disentanglement of view - specific representations : ( 1 ) Pearson correlation coefficient : Let S = ğ‘šğ‘ğ‘¡ğ‘ ğ‘–ğ‘š ( P , Q ) âˆˆ R ğ‘› 1 Ã— ğ‘› 2 represent a matrix of pairwise similarities between entries of P âˆˆ R ğ‘› 1 Ã— ğ‘‘ and Q âˆˆ R ğ‘› 2 Ã— ğ‘‘ such that S ( ğ‘– , ğ‘— ) = ğ‘ ğ‘–ğ‘š ( P ( ğ‘– ) , Q ( ğ‘— ) ) . We compute Pearson correlation coefficient between the rows of ğ‘šğ‘ğ‘¡ğ‘ ğ‘–ğ‘š ( Z ğ‘ğ‘š , Z ğ‘ğ‘š ) and ğ‘šğ‘ğ‘¡ğ‘ ğ‘–ğ‘š ( Z ğ‘ğ‘š â€² , Z ğ‘ğ‘š â€² ) Nihal Jain , Praneetha Vaddamanu , Paridhi Maheshwari , Vishwa Vinay , & Kuldeep Kulkarni Figure 4 : Inter view correlations with varying ğœ† 2 and fixed ğœ† ğ‘– . As expected , both Pearson and HSIC drop with increasing ğœ† 2 . where ğ‘š â‰  ğ‘š â€² . This quantifies the inter - view correlation be - tween the pairwise similarities of data points in views ğ‘š and ğ‘š â€² . A high Pearson correlation coefficient indicates overlap between the two views , whereas low correlation values indi - cate that unique aspects are being captured by the two views individually . Similarly , by computing the inter - view correlation between the rows of ğ‘šğ‘ğ‘¡ğ‘ ğ‘–ğ‘š ( X ğ‘š , X ğ‘š ) and ğ‘šğ‘ğ‘¡ğ‘ ğ‘–ğ‘š ( X ğ‘š â€² , X ğ‘š â€² ) , we obtain measures of overlap between the views when they are represented using input representations . Further , the intra - view correlation between the rows of ğ‘šğ‘ğ‘¡ğ‘ ğ‘–ğ‘š ( X ğ‘š , X ğ‘š ) and ğ‘šğ‘ğ‘¡ğ‘ ğ‘–ğ‘š ( Z ğ‘ğ‘š , Z ğ‘ğ‘š ) informs us about the deviation of the output view - specific representations from the input representations . ( 2 ) Hilbert - Schmidt Independence Criterion ( HSIC ) : We com - pute the normalized HSIC metric [ 31 ] as a proxy for the mutual information ( MI ) between view representations : ğ»ğ‘†ğ¼ğ¶ ( Y ğ‘š , Y ğ‘š â€² ) = ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘’ ( K m HK m â€² H ) âˆ¥ HK m H âˆ¥ 2 âˆ¥ HK m â€² H âˆ¥ 2 ( 10 ) where K m = ğ‘šğ‘ğ‘¡ğ‘ ğ‘–ğ‘š ( Y ğ‘š , Y ğ‘š ) , and H = I âˆ’ 1 ğ‘› 11 T if K m âˆˆ R ğ‘› Ã— ğ‘› . Just as the case with the correlation measure above , we measure inter - view MI between output representations when Y ğ‘š = Z ğ‘ğ‘š and between input representations when Y ğ‘š = X ğ‘š , with ğ‘š â‰  ğ‘š â€² . We can similarly measure the intra - view mutual information by substituting Y ğ‘š = X ğ‘š and Y ğ‘š â€² = Z ğ‘ğ‘š . 4 . 2 . 2 Evaluating Expansion of Collections . To quantify the retrieval performance of our approach , we compute the relevance of the top - k results of the ranked list R described in Section 3 . 2 . We compute the Mean Average Precision ( MAP ) and Mean Reciprocal Rank ( MRR ) of R by using the attribute labels in the BAM dataset ( as discussed in Section 4 . 1 ) to indicate ground - truth relevance . Specifically , a retrieved image is considered relevant for a query collection if the image belongs to the same attribute class as that used for simulating the query collection . 5 EXPERIMENTS AND RESULTS We validate our method using both intrinsic and extrinsic evalua - tions using view - specific representations . 5 . 1 Model Training Our model is trained using the Adam optimizer [ 12 ] with a learning rate of 0 . 0001 , in batches of size 64 . The other important hyper - parameters associated with our model are the ğœ† ğ‘– â€™s described in Figure 5 : ( a ) Intra - view correlation and MI between input and output representations with increasing ğœ† 2 ; ( b ) Final re - construction loss for different values of ğœ† 2 . In both cases , varying ğœ† 2 gives us the control we require , even though the ğœ† ğ‘– for the other training objective components are kept fixed . Section 3 . 1 . 3 . Since our focus is on learning view - specific repre - sentations , which are directly influenced by L ğ‘ ğ‘ğ‘ , we study the effect of varying ğœ† 2 more closely . We consider ğœ† 2 from 0 . 0 to 5 . 0 while keeping the values of the other hyperparameters in the loss function fixed ( ğœ† 1 = 0 . 001 , ğœ† 3 = 0 . 001 , and ğœ† 4 = 0 . 0001 ) . By sweep - ing over this operating range for ğœ† 2 , we observe its effect on the disentanglement of input representations . Figure 4 visualizes the effect of increasing ğœ† 2 on the Pearson cor - relation and HSIC metrics discussed in Section 4 . 2 . 1 . Firstly , when no disentanglement is enforced , i . e . , ğœ† 2 = 0 , the metrics computed using input representations and output view - specific representa - tions are almost equal , across all pairs of views . This indicates a complete information transfer between input and output represen - tations . As we increase ğœ† 2 , the inter - view Pearson correlation and HSIC metrics decrease for all pairs of views ; this trend is expected for inter - view disentanglement . The decrease in correlation indi - cates that each output view - specific representation is capturing less information about all other views than their input counterparts . We are also interested in quantifying the intra - view information retained by our view - specific representations . We measure this as the Pearson correlation and mutual information between the input Self - supervised Multi - view Disentanglement for Expansion of Visual Collections Figure 6 : View intents as a function of collection purity . Experiments with disentangled representations at ğœ† 2 = 0 . 05 . and output representations for each view . Figure 5 ( a ) shows the trends as ğœ† 2 increases . Once again , the decrease in these metrics is expected because the output representations lose information com - mon across views and their overlap with the input decreases . Thus , we note that the intra - view information transfer is influenced by the disentanglement of views because ğœ† 3 , which directly influences the corresponding loss component , was held constant in these ex - periments . Similarly , the reconstruction loss ( Figure 5 ( b ) ) increases despite ğœ† 4 being held constant in these experiments . We also observe anomalous behavior of the view - specific repre - sentations when using large values of ğœ† 2 . For larger values of ğœ† 2 , L ğ‘Ÿğ‘’ğ‘ converges at relatively higher values indicating difficulty in reconstructing the input representations using these view - specific representations . In Figure 7 , we show an alternative view of the decreased correlation between input and output representations of the same view , by plotting a histogram of all pairwise similarities be - tween images in the validation set ( computed over the disentangled representations ) as we sweep over ğœ† 2 . When ğœ† 2 = 0 . 0 , no disen - tangling has been enforced and the observed distribution closely resembles the distribution of similarities in the input embedding spaces . With increasing ğœ† 2 values , we notice a shift in similarity distributions for all the views , indicating departure from the in - formation captured in the input representations . For ğœ† 2 = 5 . 0 , we observe peaks at similarity values of âˆ’ 1 . 0 and 1 . 0 , indicating that most representations are either completely orthogonal or nearly identical to others . This is a degenerate scenario that we would like to avoid . The optimal value of ğœ† 2 would therefore be somewhere in the middle . To operate in higher disentanglement regimes , future work may incorporate recently proposed regularization methods ( e . g . [ 5 ] ) to prevent degenerate situations . We would like to choose hyperparameters based on this intrinsic evaluation , and evaluate the model and learnt representations in the downstream collection - based retrieval task . Since only the relative values of the loss function components matter , we retain ğœ† 1 , ğœ† 3 and ğœ† 4 as before and choose ğœ† 2 based on the trends described above . Choosing a very low value of ğœ† 2 would not allow us to investigate the benefits of disentanglement , while we have seen that larger values of ğœ† 2 lead to degenerate behavior . From the HSIC and Pearson correlation values of the inter / intra - view representations , we pick an intermediate value , ğœ† 2 = 0 . 05 , to evaluate under the collection expansion task . We have also conducted a complete grid search of the hyper - parameters ; since they do not add many more insights to the current findings , they have not been reported here . 5 . 2 Expansion of Collections In this section , we evaluate the performance of the output view - specific representations on the task of collection - based retrieval . 5 . 2 . 1 Evaluating Intent Inference . The first step in expanding a moodboard involves predicting the intent of the collection of im - ages , and we evaluate this component in isolation . We define a pure collection where all member images belong to a single attribute class ( e . g . emotion = â€˜scaryâ€™ ) . By injecting images belonging to other attribute classes into pure collections , we obtain impure collections . The fraction of images in a collection that belong to the attribute class used to simulate a collection provides a measure of its purity . We vary the purity of collections and show that the computed intent weights respond as expected in Figure 6 . Each subplot is obtained by simulating collections that contain images belonging to a specified attribute class ( indicated as the subplot title ) and a given purity level ( ğ‘¥ âˆ’ ğ‘ğ‘¥ğ‘–ğ‘  ) . For a given collection , the view - specific intents are computed as defined in Section 3 . 2 . 2 . For each purity value , we have plotted the average view - specific intent weights computed over 100 collection simulations . Specifically , when ğ‘ğ‘¢ğ‘Ÿğ‘–ğ‘¡ğ‘¦ = 0 ( the leftmost points ) , a collection contains a uniform mixture of images from all attribute classes ; the intent weights reflect this by being 1 / 3 across the three views . As the purities of media - type and emotion - type collections are increased , the style intents increase while color and object intents reduce . Similarly , as we increase the purity of content - type collections , the object intent consistently increases and reaches a maximum value for pure collections . These findings agree with the known attribute - view correlations discussed in Section 4 . 1 , and therefore validate our method for inferring the intent . The disentanglement of view - specific representations is critical to having this behavior â€” a rising trend of intent with increasing purity is observed only for the view correlated with a collectionâ€™s attribute class . 5 . 2 . 2 Retrieving Images for Collection Queries . Our method for collection - based retrieval involves computation of intent weights and the subsequent retrieval of images using a weighted similarity . It is not necessary that the representations used for computing intent weights ( in Equation 8 ) be the same as the ones used for computing ğ‘ ğ‘–ğ‘š scores for collection - based retrieval ( in Equation 9 ) . We therefore consider the following variations in our experiments : Nihal Jain , Praneetha Vaddamanu , Paridhi Maheshwari , Vishwa Vinay , & Kuldeep Kulkarni Figure 7 : Similarities between representations of all pairs of images measured using the output representations of each view . The output representations are obtained by varying ğœ† 2 . Table 2 : Evaluation of different representations on the collection expansion task . The first three representations correspond to the baselines of using single view ( object , style and color respectively ) representations for the images . Representation Attribute - wise MAP Aggregate Attribute - wise MRR Aggregate content media emotion MAP content media emotion MRR ResNet 0 . 823 0 . 400 0 . 305 0 . 523 0 . 922 0 . 557 0 . 409 0 . 668 ALADIN 0 . 565 0 . 697 0 . 483 0 . 585 0 . 736 0 . 880 0 . 708 0 . 800 LAB Histogram 0 . 202 0 . 149 0 . 108 0 . 158 0 . 371 0 . 237 0 . 213 0 . 269 input - uniform 0 . 685 0 . 617 0 . 440 0 . 581 0 . 913 0 . 845 0 . 660 0 . 809 input - output 0 . 813 0 . 692 0 . 497 0 . 685 0 . 950 0 . 899 0 . 715 0 . 889 output - output 0 . 857 0 . 719 0 . 513 0 . 713 0 . 983 0 . 924 0 . 797 0 . 896 ( 1 ) input - uniform â€” Input representations are used for ğ‘ ğ‘–ğ‘š score computation and intent weights are uniform across views . This is a multi - view setting without intent inference . ( 2 ) input - output â€” Input representations are used for ğ‘ ğ‘–ğ‘š score computation while view - specific ( output ) representations are used for intent weight inference . ( 3 ) output - output â€” View - specific ( output ) representations are used for both computing intent weights and ğ‘ ğ‘–ğ‘š scores . Table 2 presents MAP and MRR values obtained across our exper - iments . The first three rows are our baselines : for each view ( object , style , and color ) , we use the corresponding out - of - the - box view representation ( ResNet , ALADIN , and LAB Histogram ) only for collection expansion via a simple nearest neighbor search without intent inference . The remaining three rows show the results for the variations discussed above . The Attribute - wise MAP & MRR columns are computed by fixing the attribute while simulating collection queries . Finally , Aggregate MAP & MRR values are com - puted by selecting the attribute label for each simulated collection at random and averaging across them . The results reported are the averages over 100 simulated collections for each configuration . In each case , the number of images in the query collection varies uniformly between 10 and 30 . We make the following observations about the results shown : ( 1 ) Among the baselines , for each attribute , the correlated view representation scores the highest Attribute - wise MAP & MRR . This is especially noticeable in the performance of the ResNet representation for content - type collections , which we expect to have high object intent . ( 2 ) For input - uniform , the Aggregate MAP value is higher than two of three baselines , with Aggregate MRR higher than all three baselines ; this indicates that each view provides incremen - tal value when used with others , validating the use of multi - view representations for image retrieval . ( 3 ) input - output outperforms the preceding methods that do not use intent inference ; this validates our intent prediction mecha - nism which is able to selectively invoke the relevant view based on the query collection . ( 4 ) Finally , disentangled multi - view representations combined with the inferred intent , ( output - output ) , provides the best MAP and MRR in all cases , showing the utility of cross - view disen - tangled representations . 5 . 3 Relevance - Diversity Trends In Table 2 , we show that the ranking effectiveness by using multi - ple views is on average better than what can be achieved from a single view . When the goal is to provide interesting additions to a designerâ€™s moodboard , a core requirement of the retrieval system is to ensure that the user has visibility into the full range of possi - bilities . This need for exploration is well - studied in the information retrieval community under the notion of diversity [ 6 , 7 , 24 ] . We are interested in measuring the diversity of results in the returned list of candidate images R ( Section 3 . 2 . 3 ) . Since we are Self - supervised Multi - view Disentanglement for Expansion of Visual Collections ( a ) MAP - diversity trade - off for attribute = media . ğœ† 2 = 0 . 05 . Values are means computed over 100 simulated collections , with the standard error also plotted for both dimensions . Values towards the top - right are desirable , as they indicate that the corresponding configuration provides the dual benefits of increased relevance ( MAP ) and more effective exploration ( diversity ) . Our full model ( green dot ) achieves the best performance overall . ( b ) The equivalent plot for the object view , i . e . , for attribute = content . ğœ† 2 = 0 . 05 as before . The output - output combination of using representations derived from our our model for collection expansion does the best overall . Using the ResNet representations ( expected to be correlated with the object view ) provides the most competitive single view MAP . Figure 8 : MAP - diversity trade - off for two attribute types . operating in a multi - view space , we can make diversity measure - ments along each view . Specifically , we measure diversity along view ğ‘š as ğ›¿ ğ‘š = 1 / ğ›½ ğ‘š , where ğ›½ ğ‘š is computed as in Section 3 . 2 . 2 by treating C = R [ 16 ] . This definition reflects our assumption that intent and diversity are inverse notions of each other . The results from our experiments are shown graphically in Figure 8 , where each subplot shows the diversity with respect to the specified view on the ğ‘¥ âˆ’ ğ‘ğ‘¥ğ‘–ğ‘  , and MAP on the ğ‘¦ âˆ’ ğ‘ğ‘¥ğ‘–ğ‘  . We provide results for media - type collections and content - type collections . We make the following observations concerning Figure 8a : ( 1 ) ALADIN shows higher relevance scores than LAB Histogram or ResNet . As media - type collections are anticipated to be cor - related with the style view , this behavior is expected . Further , ALADIN has greater diversities along object and color views , and the least diversity along the style view . ( 2 ) Among the variations that use intent weights while ranking , input - uniform has the least diversity along object and color views . This undesired behavior is also expected â€“ by giving uni - form intent , we weigh uncorrelated views more than necessary . ( 3 ) In a multi - view setting , using the output representations solely for intent computation ( input - output ) leads to higher rele - vance scores when compared to uniform intent weights . We observe higher diversities along uncorrelated views as desired . ( 4 ) As shown in Table 2 , when using the disentangled view - specific representations for both similarity measurement and intent computation ( output - output ) , we observe the highest rele - vance scores . In Figure 8a , we additionally observe that this scenario produces the highest diversity along the object view , with comparable diversities to ALADIN along the other views . Similar observations can be made from Figure 8b with respect to the object view as well . A minor difference is that ResNet obtains the highest diversities for uncorrelated views , indicating the alignment between ResNet representations and the object view . Thus we show that our weighted nearest neighbor computa - tion enables us to retrieve images that are similar along the view corresponding to the userâ€™s intent , while allowing diversity along the other views . Often , the relationship between relevance and diversity is described as a trade - off . The use of our model outputs for computing both intents and similarities leads to MAP values comparable to those observed with the correlated view but with increased diversity along uncorrelated views . 6 COMPOSING COLLECTIONS Deriving disentangled multi - view representations for a collection of images enables the novel use - case of composing multiple collec - tions as a query to retrieve images that selectively adhere to the collections in the query . By picking desired view representations from each collection in the query , we can create a composite rep - resentation for a new ( hypothetical ) collection which can be used as a query for expansion . Figure 9 illustrates this idea qualitatively using two examples . Consider the top row from Figure 9 . The query comprises a pair of collections ( shown as outlined boxes ) and the view that is relevant for each collection . Specifically , we consider Nihal Jain , Praneetha Vaddamanu , Paridhi Maheshwari , Vishwa Vinay , & Kuldeep Kulkarni Figure 9 : Qualitative results for composing collections using disentangled multi - view representations . ( Top ) The left subplot represents a collection with the object intent of bicycles and the center subplot represents a collection with the style intent of vector art images . Disentangled multi - view representations allows constructing a collection query resulting in a natural combined set of results â€“ bicycles styled in vector art form . ( Bottom ) The left subplot is a collection with the object intent of flowers , while the center subplot is a collection with the style intent of oilpaint images . A query formed by composing the relevant disentangled views from these collections retrieves flowers styled in oilpaint form . the object view from the collection of bicycles and the style view from the collection of vector art images . By selecting the object representations from the former , style representations from the latter , and averaging out the color representations between the two , we obtain composite representations for a hypothetical collection that has the object features of the former and style features of the latter . Since we are only interested in the style and object views , we can split the intent weights between only these for the composite collection before ranking images from the index . Proceeding as described in Section 3 . 2 , we obtain the ranked list of images shown on the top - right in Figure 9 , which are images of bicycles styled in vector art form . The disentanglement process described in our paper is critical to enable this behavior . By ensuring that repre - sentations along different views are de - correlated , these views can be mixed and matched , allowing for a powerful visual querying mechanism . A similar example , representing the intent of flower images in oilpaint style , is shown in the bottom row of Figure 9 . 7 CONCLUSION AND DISCUSSION In this work , we have introduced the notion of multi - view repre - sentations for image collections and enumerated a well - known set of image similarity axes as views â€“ object , style , color . The baseline multi - view representation of an image is taken be to a union of popular feature extractors for each view . Our primary contribution is in transforming these input representations to minimize correla - tions among the views using a self - supervised approach . We have shown that this leads to output representations that better capture the overall characteristics of an image . To illustrate the benefits of our approach , we have defined a novel collection level task involving retrieval of images relevant to a set of seed images . We have defined the intent of a collection of images to be a distribution over views â€“ a higher weight assigned to a view indicates greater homogeneity for that view across images in the collection . Finally , we have shown that using these intent weights allows us to effectively score candidate images with respect to the query collection . We have also proposed a new querying mechanism for image search driven by composing multiple collections of images . This is enabled using the ideas and techniques presented in this paper such as views of images and representations and intents of collections . While we have presented qualitative results here , future work can investigate this quantitatively on datasets more suited for this task . The work described here is related to the active topic of rep - resentation learning . We have borrowed intuitions like factorized representations [ 31 ] and disentanglement [ 36 ] from the domain of NLP and applied it to the setting of image retrieval . As future work , we will look into training our model in an end - to - end manner customized for the retrieval task . We also intend to further evaluate the benefits of our approach via a thorough user study . From the perspective of the application that we have considered , extending our current setup to multiple iterations [ 13 , 29 ] is a natural next step . While the treatment in the current paper is restricted to specific visual axes or views , the proposed framework is generalizable to a broader range of visual representation spaces . Finally , we intend to generalize the benefits of our approach to other datasets as well . Self - supervised Multi - view Disentanglement for Expansion of Visual Collections REFERENCES [ 1 ] Muhammad Umer Anwaar , Rayyan Ahmad Khan , Zhihui Pan , and Martin Klein - steuber . 2021 . A Contrastive Learning Approach for Compositional Zero - Shot Learning . In Proceedings of the 2021 International Conference on Multimodal Inter - action ( MontrÃ©al , QC , Canada ) ( ICMI â€™21 ) . Association for Computing Machinery , New York , NY , USA , 34 â€“ 42 . https : / / doi . org / 10 . 1145 / 3462244 . 3479904 [ 2 ] Artem Babenko , Anton Slesarev , Alexandr Chigorin , and Victor Lempitsky . 2014 . Neural codes for image retrieval . In European Conference on Computer Vision . Springer , 584 â€“ 599 . [ 3 ] Alberto Baldrati , Marco Bertini , Tiberio Uricchio , and Alberto Del Bimbo . 2022 . Conditioned Image Retrieval for Fashion Using Contrastive Learning and CLIP - Based Features . In ACM Multimedia Asia ( Gold Coast , Australia ) ( MMAsia â€™21 ) . Association for Computing Machinery , New York , NY , USA , Article 54 , 5 pages . https : / / doi . org / 10 . 1145 / 3469877 . 3493593 [ 4 ] Tadas BaltruÅ¡aitis , Chaitanya Ahuja , and Louis - Philippe Morency . 2018 . Multi - modal machine learning : A survey and taxonomy . IEEE transactions on pattern analysis and machine intelligence 41 , 2 ( 2018 ) , 423 â€“ 443 . [ 5 ] Adrien Bardes , Jean Ponce , and Yann LeCun . 2022 . VICReg : Variance - Invariance - Covariance Regularization for Self - Supervised Learning . In International Confer - ence on Learning Representations . [ 6 ] Harr Chen and David R Karger . 2006 . Less is more : probabilistic models for re - trieving fewer relevant documents . In Proceedings of the 29th annual international ACM Conference on Research and Development in Information Retrieval ( SIGIR ) . [ 7 ] Charles LA Clarke , Maheedhar Kolla , Gordon V Cormack , Olga Vechtomova , Azin Ashkan , Stefan BÃ¼ttcher , and Ian MacKinnon . 2008 . Novelty and diversity in information retrieval evaluation . In Proceedings of the 31st annual international ACM Conference on Research and Development in Information Retrieval ( SIGIR ) . [ 8 ] Imant Daunhawer , Thomas M Sutter , RiÄards MarcinkeviÄs , and Julia E Vogt . 2021 . Self - supervised disentanglement of modality - specific and shared factors improves multimodal generative models . In Pattern Recognition : 42nd DAGM German Conference , DAGM GCPR 2020 . Springer , 459 â€“ 473 . [ 9 ] Devamanyu Hazarika , Roger Zimmermann , and Soujanya Poria . 2020 . MISA : Modality - invariant and - specific representations for multimodal sentiment analy - sis . In Proceedings of the 28th ACM International Conference on Multimedia . [ 10 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . 2016 . Deep Residual Learning for Image Recognition . In 2016 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) . 770 â€“ 778 . [ 11 ] Yoshiharu Ishikawa , Ravishankar Subramanya , and Christos Faloutsos . 1998 . MindReader : Querying databases through multiple examples . ( 1998 ) . [ 12 ] Diederik P . Kingma and Jimmy Ba . 2015 . Adam : A Method for Stochastic Opti - mization . In 3rd International Conference on Learning Representations , ICLR 2015 , San Diego , CA , USA , May 7 - 9 , 2015 , Conference Track Proceedings , Yoshua Bengio and Yann LeCun ( Eds . ) . [ 13 ] Janin Koch , AndrÃ©s Lucero , Lena Hegemann , and Antti Oulasvirta . 2019 . May AI ? Design ideation with cooperative contextual bandits . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1 â€“ 12 . [ 14 ] Alex Krizhevsky , Ilya Sutskever , and Geoffrey E Hinton . 2012 . Imagenet classifi - cation with deep convolutional neural networks . Advances in Neural Information Processing Systems 25 ( 2012 ) , 1097 â€“ 1105 . [ 15 ] Yingming Li , Ming Yang , and Zhongfei Zhang . 2018 . A survey of multi - view representation learning . IEEE Transactions on Knowledge and Data Engineering 31 , 10 ( 2018 ) , 1863 â€“ 1883 . [ 16 ] Hao Ma , Michael Lyu , and Irwin King . 2010 . Diversifying query suggestion results . In Proceedings of the AAAI Conference on Artificial Intelligence , Vol . 24 . [ 17 ] Paridhi Maheshwari , Ritwick Chaudhry , and Vishwa Vinay . 2021 . Scene Graph Embeddings Using Relative Similarity Supervision . In Proceedings of the AAAI Conference on Artificial Intelligence , Vol . 35 . 2328 â€“ 2336 . [ 18 ] Paridhi Maheshwari , Manoj Ghuhan , and Vishwa Vinay . 2020 . Learning Colour Representations of Search Queries . In Proceedings of the 43rd International ACM Conference on Research and Development in Information Retrieval ( SIGIR ) . [ 19 ] Ishan Misra , Abhinav Gupta , and Martial Hebert . 2017 . From Red Wine to Red Tomato : Composition with Context . In 2017 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) . 1160 â€“ 1169 . https : / / doi . org / 10 . 1109 / CVPR . 2017 . 129 [ 20 ] TusharNagarajanandKristenGrauman . 2018 . Attributesasoperators : factorizing unseen attribute - object compositions . In Proceedings of the European Conference on Computer Vision ( ECCV ) . 169 â€“ 185 . [ 21 ] Vinod Nair and Geoffrey E . Hinton . 2010 . Rectified Linear Units Improve Re - stricted Boltzmann Machines . In Proceedings of the 27th International Conference on International Conference on Machine Learning ( Haifa , Israel ) ( ICMLâ€™10 ) . Omni - press , Madison , WI , USA , 807 â€“ 814 . [ 22 ] Filip Radenovic , Giorgos Tolias , and Ondrej Chum . 2018 . Deep Shape Matching . In Proceedings of the European Conference on Computer Vision ( ECCV ) . [ 23 ] Vincent Rieuf , Carole Bouchard , and AmÃ©ziane Aoussat . 2015 . Immersive mood - boards , a comparative study of industrial design inspiration material . Journal of Design Research 13 , 1 ( 2015 ) , 78 â€“ 106 . [ 24 ] LTRodrygo , CraigMacdonald , andIadhOunis . 2015 . Searchresultdiversification . Foundations and Trends in Information Retrieval 9 , 1 ( 2015 ) , 1 â€“ 90 . [ 25 ] Dan Ruta , Saeid Motiian , Baldo Faieta , Zhe Lin , Hailin Jin , Alex Filipkowski , An - drew Gilbert , and John Collomosse . 2021 . ALADIN : All Layer Adaptive Instance NormalizationforFine - grainedStyleSimilarity . In ProceedingsoftheInternational Conference on Computer Vision ( ICCV ) . [ 26 ] Arnold WM Smeulders , Marcel Worring , Simone Santini , Amarnath Gupta , and Ramesh Jain . 2000 . Content - based image retrieval at the end of the early years . IEEE Transactions on pattern analysis and machine intelligence 22 , 12 ( 2000 ) . [ 27 ] John R Smith and Shih - Fu Chang . 1997 . VisualSEEk : a fully automated content - based image query system . In Proceedings of the fourth ACM international confer - ence on Multimedia . [ 28 ] Hao Tan and Mohit Bansal . 2019 . LXMERT : Learning cross - modality encoder representations from transformers . ( 2019 ) . [ 29 ] Choon Hui Teo , Houssam Nassif , Daniel Hill , Sriram Srinivasan , Mitchell Good - man , VijaiMohan , andSVNVishwanathan . 2016 . Adaptive , personalizeddiversity for visual discovery . In Proceedings of the 10th ACM conference on Recommender Systems . 35 â€“ 38 . [ 30 ] Yonglong Tian , Dilip Krishnan , and Phillip Isola . 2020 . Contrastive Multiview Coding . In Proceedings of the European Conference on Computer Vision ( ECCV ) . [ 31 ] Yao - Hung Hubert Tsai , Paul Pu Liang , Amir Zadeh , Louis - Philippe Morency , and Ruslan Salakhutdinov . 2019 . Learning Factorized Multimodal Representations . In ICLR . [ 32 ] Michael Tschannen , Josip Djolonga , Paul K . Rubenstein , Sylvain Gelly , and Mario Lucic . 2020 . On Mutual Information Maximization for Representation Learning . In International Conference on Learning Representations . [ 33 ] Nam Vo , Lu Jiang , Chen Sun , Kevin Murphy , Li - Jia Li , Li Fei - Fei , and James Hays . 2019 . Composing Text and Image for Image Retrieval - an Empirical Odyssey . In ProceedingsoftheIEEE / CVFConferenceonComputerVisionandPatternRecognition ( CVPR ) . [ 34 ] Nam Vo , Lu Jiang , Chen Sun , Kevin Murphy , Li - Jia Li , Li Fei - Fei , and James Hays . 2019 . Composing Text and Image for Image Retrieval - an Empirical Odyssey . In 2019 IEEE / CVF Conference on Computer Vision and Pattern Recognition ( CVPR ) . 6432 â€“ 6441 . https : / / doi . org / 10 . 1109 / CVPR . 2019 . 00660 [ 35 ] MichaelJWilber , ChenFang , HailinJin , AaronHertzmann , JohnCollomosse , and Serge Belongie . 2017 . BAM ! the behance artistic media dataset for recognition beyond photography . In Proceedings of the International Conference on Computer Vision ( ICCV ) . [ 36 ] Julian Zaidi , Jonathan Boilard , Ghyslain Gagnon , and Marc - AndrÃ© Carbon - neau . 2020 . Measuring Disentanglement : A Review of Metrics . arXiv preprint arXiv : 2012 . 09276 ( 2020 ) . [ 37 ] Jing Zhao , Xijiong Xie , Xin Xu , and Shiliang Sun . 2017 . Multi - view learning overview : Recent progress and new challenges . Information Fusion 38 ( 2017 ) . [ 38 ] Xiang Sean Zhou and Thomas S Huang . 2003 . Relevance feedback in image retrieval : A comprehensive review . Multimedia systems 8 , 6 ( 2003 ) , 536 â€“ 544 .