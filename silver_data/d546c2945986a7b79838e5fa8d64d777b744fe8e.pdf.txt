Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation Yunlong Wang National University of Singapore Singapore yunlong . wang @ nus . edu . sg Priyadarshini Venkatesh University College London London , UK zcjtpve @ ucl . ac . uk Brian Y . Lim National University of Singapore Singapore brianlim @ comp . nus . edu . sg ABSTRACT Feedback in creativity support tools can help crowdworkers to im - prove their ideations . However , current feedback methods require human assessment from facilitators or peers . This is not scalable to large crowds . We propose Interpretable Directed Diversity to auto - matically predict ideation quality and diversity scores , and provide AI explanations — Attribution , Contrastive Attribution , and Coun - terfactual Suggestions — to feedback on why ideations were scored ( low ) , and how to get higher scores . These explanations provide multi - faceted feedback as users iteratively improve their ideations . We conducted formative and controlled user studies to understand the usage and usefulness of explanations to improve ideation di - versity and quality . Users appreciated that explanation feedback helped focus their efforts and provided directions for improvement . This resulted in explanations improving diversity compared to no feedback or feedback with scores only . Hence , our approach opens opportunities for explainable AI towards scalable and rich feedback for iterative crowd ideation and creativity support tools . CCS CONCEPTS • Human - centeredcomputing → Interactivesystemsandtools ; Collaborative and social computing . KEYWORDS Explainable AI , Diversity , Collective Creativity , Crowdsourcing ACM Reference Format : Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim . 2022 . Inter - pretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation . In CHI Conference on Human Factors in Computing Systems ( CHI ‘22 ) , April 29 - May 5 , 2022 , New Orleans , LA , USA . ACM , New York , NY , USA , 28 pages . https : / / doi . org / 10 . 1145 / 3491102 . 3517551 1 INTRODUCTION Creativity support tools [ 36 , 77 ] harness the power of human cre - ativity through large - scale crowdsourcing for tasks , such as text editing [ 11 , 24 , 76 ] , iterating designs [ 32 ] , information synthesis [ 58 ] , action planning of health behavior change [ 3 , 43 ] , and moti - vational messaging [ 4 , 26 , 28 , 49 ] . Among techniques for creativ - ity support , providing timely and proper feedback is a promising Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for third - party components of this work must be honored . For all other uses , contact the owner / author ( s ) . CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA © 2022 Copyright held by the owner / author ( s ) . ACM ISBN 978 - 1 - 4503 - 9157 - 3 / 22 / 04 . https : / / doi . org / 10 . 1145 / 3491102 . 3517551 method to boost crowd ideation creativity [ 12 , 31 , 34 , 66 , 68 , 92 ] . Many feedback methods require human assessment from facilita - tors or peers , but this limits their ability to scale to large crowds . Employing non - expert crowdworkers can scale more than with ex - perts [ 34 ] , but costs can escalate and they cannot provide feedback in real - time . Machine learning can be used to automatically provide feedback by predicting ideation quality and showing examples of high - quality ideations [ 69 ] . However , besides ideation quality , it is also important to increase the diversity and reduce redundancy in submitted ideations [ 14 , 47 , 74 , 79 ] . Prior collaborative creativity support methods to avoid redundancy include employing an adap - tive task workflows [ 93 ] , constructing a taxonomy of the idea space [ 44 ] , visualizing a concept map of peer ideas [ 79 ] , and directing ideation towards automatically selected diverse prompts [ 26 ] . Cur - rent methods to drive diversity provide information from a prior or peer set of ideations , and are not specific to each worker’s ideation . This limits the relevance of feedback [ 91 ] . Therefore , it is impor - tant to provide contextualized feedback tailored to the worker’s ideation . In this work , we support both criteria of higher quality and diversity for crowd ideation , specifically targeting the use case of writing motivational messages to promote physical activity . Although AI can predict scores on ideations , showing scores alone has limited usefulness . Consider receiving feedback in school . To promote learning and improvement , students not only receive graded assignments , but additional feedback to indicate problems in their work , tips or examples for improvement , and opportunities for revision . In the Hattie and Timperley feedback model , these cor - respond to a score ( Feed Up ) , critical feedback ( Feed Back ) , useful tips or examples ( Feed Forward ) [ 42 ] . For ideation writing , we pro - vide each as feedback correspondingly using explainable AI ( XAI ) with 1 ) predicted scores , 2 ) attribution explanation ( highlights ) of problematic terms , 3 ) contrastive attributions to provide feedback between revisions , and 4 ) counterfactual suggestions to provide tips for how to replace problematic terms . We provide feedback across multiple iterations to let workers revise their ideations . Un - like most uses of XAI to improve user understanding and trust of AI [ 9 , 52 , 83 , 89 ] , we focus on improving human task performance with human - XAI collaboration [ 87 ] . Furthermore , due to the limited use of AI in crowd ideation , the design and effectiveness of XAI for such tasks is an open question . Our contributions are : 1 . Interpretable Directed Diversity , an explainable AI approach to predict the quality and diversity of ideations and generate multi - faceted explanatory feedback . This enables scalable , real - time , contextual feedback to improve collective creativity . 2 . Three explanation types — Attributions , Contrastive Attri - butions , and Counterfactual Suggestions — for two criteria , quality and diversity . We implemented the algorithmic ap - proaches in an interactive crowd ideation system . a r X i v : 2109 . 10149v4 [ c s . H C ] 28 M a r 2022 CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim We evaluated the usage , understandability , and usefulness of the explanations for creative ideation in a formative user study and summative user studies with ideators and validators . We found that score feedback with attribution , contrastive attribution , and coun - terfactual suggestion explanations are complementary to improve ideation quality and diversity . We discuss the generalization of our explainable feedback approach to other domains and explanations . 2 RELATED WORK 2 . 1 Creative Ideation and Feedback Creative ideation involves complex cognitive processes [ 5 , 15 , 16 , 88 ] . Memory - based explanation models describe how people re - trieve information relevant to a cue ( prompt ) from long - term mem - ory and process it to generate ideas [ 2 , 30 , 56 , 62 , 63 ] . Ideation - based models [ 61 ] explain how individuals can generate many ideas through complex thinking processes , including analogical reasoning [ 41 , 45 , 59 ] , problem constraining [ 80 ] , and vertical or lateral thinking [ 40 ] . While users may ideate creatively , their initial ideation can be improved with proper feedback [ 31 – 33 ] . The effi - cacy of providing feedback has been much studied in the domains of education [ 42 ] and management [ 71 ] to improve learning and decision making . Hattie and Timperley [ 42 ] argued that effective feedback should answer three questions : where am I going , how am I going , and where to next . Applying goal - setting theory by showing summary feedback [ 53 , 54 ] , Carson and Carson [ 19 ] im - proved individual creativity by manually reviewing ideations and providing feedback on the quantity and creativity of distinct ideas . However , providing feedback often requires intervention from hu - man experts , and this limits their scalability and rapid iteration . In this work , we propose an automated approach for creativity support tools to provide feedback for creative ideation . Furthermore , we focus on collective creativity from ideations of a crowd of individuals . While individual creativity is important , society values ideation novelty more . Boden refers to these as P - creativity and H - creativity , respectively [ 15 ] . “An idea is P - creative if it is creative with respect to the mind of the person concerned , even if others have had that idea already . An idea is H - creative if it is P - creative and no other person has had the idea before . H - creativity is more glamorous , but P - creativity is more fundamental " [ 15 ] . In crowd ideation , we seek to avoid the wastefulness of redundant ideas , thus we value H - creativity more . Being H - creative requires the ideator to study prior ideations to avoid replicating them . In this work , we support H - creativity by providing automatic prompts and feedback to guide ideators away from prior ideations . 2 . 2 Feedback in Creativity Support Tools Creativity Support Tools have been widely studied in HCI to help crowdworkers to ideate more effectively and at scale [ 36 , 37 ] . Ef - fective methods to support crowd ideation include sharing peers’ ideas [ 79 ] , relevant concepts [ 8 ] , and expert guidance [ 91 ] , contex - tual framing [ 65 ] , constructing ideation taxonomies [ 44 ] , providing diverse prompts [ 26 ] , and feedback ratings [ 69 ] . We categorize these approaches as manual feedback from people and automated feedback from intelligent systems . Methods using manual feedback investigated who should provide the feedback [ 31 , 67 ] and how to coordinate ideators [ 34 , 68 ] . Using Voyant [ 34 ] , poster designers had access to a non - expert crowd to receive structural feedback on their designs . Likewise , CrowdUI [ 68 ] enables web designers to elicit visual feedback from the website’s community of users . While these methods are adaptable to various domains , they require much human labor to prepare feedback and are difficult to scale . In contrast , recent works have developed automatic systems to generate feedback for mind mapping [ 8 ] , story writing [ 24 ] , metaphor creation [ 39 ] , and writing supportive comments for on - line mental health community [ 69 ] . The feedback provided in these works were limited to independent examples from peers or machine - suggested words or sentences , but not contextual to the ideations . MepsBot [ 69 ] predicts scores for informational and emotional sup - port , which is specific to the submitted ideations . However , the justification for such scores is unclear to users , hence we provide AI explanations in this work for more actionable feedback . Finally , prior works focus on individual creativity , so ideations from multiple ideators may be redundant . Directed Diversity [ 26 ] addressed this problem by automatically selecting diverse prompts for ideation , but did not provide feedback after ideation . In this work , we address the difficult problem of providing contextual feedback for collective ideation , where each ideator cannot see all prior messages . We achieve this by providing model explanations towards a higher automatically - calculated diversity score . 2 . 3 Explanations in Intelligent Systems Explainable AI ( XAI ) techniques have drawn much attention as intelligent systems become more complex , requiring transparency and accountability [ 1 , 87 ] . Prior research has shown that XAI can increase user trust and understanding of AI systems [ 9 , 89 ] , and explored designing proper XAI to improve human - AI collaboration , e . g . , for computer - aided translation [ 25 ] , playing Chess [ 27 ] , and music creation [ 55 ] . Though many studies have sought to identify optimal explanation types in specific tasks [ 52 , 83 ] , some found that providing a variety of explanations can have stronger benefits [ 6 ] or support various usage strategies [ 50 , 51 ] . While prior studies have found that XAI can improve system transparency , fairness , and acceptance , we explore how XAI can improve user creativity and employed multiple explanations to stimulate creativity . 3 TECHNICAL APPROACH We aim to direct ideators towards improved ideations by providing automatic feedback and explanations . For each prompted ideation , ideators can learn from the feedback to iteratively improve their ideations . Figure 1 shows the iterative process of Interpretable Di - rected Diversity . It involves two high - level phases to prepare and provide prompts ( Steps I , II , III ) , and analyze the ideation and pro - vide feedback iteratively ( Steps 1 , 2a , 2b ) . Phase 1 follows the same steps as in Directed Diversity [ 26 ] to I ) curate prior ideations , II ) generate diverse and non - redundant prompts , and III ) send these prompts to ideators . Phase 2 is the main focus of this paper , extends Directed Diversity [ 26 ] and involves , for each ideation , 1 ) analyzing the written ideation , 2 ) showing feedback based on a ) prediction scores and b ) explanations ; the analysis and feedback are repeated as the ideator iteratively revises each ideation . The technical con - tribution of this work is the predictive and explanatory feedback after the prompting and ideation in Directed Diversity [ 26 ] . Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA (cid:0)(cid:0)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:5)(cid:6)(cid:9)(cid:10)(cid:11)(cid:6)(cid:5)(cid:12)(cid:6)(cid:3)(cid:7)(cid:5)(cid:13)(cid:14)(cid:7)(cid:15)(cid:12) (cid:0)(cid:2)(cid:3)(cid:16)(cid:17)(cid:5)(cid:8)(cid:15)(cid:6)(cid:3)(cid:7)(cid:5)(cid:10)(cid:13)(cid:5)(cid:3)(cid:10)(cid:9)(cid:6)(cid:8)(cid:15)(cid:10)(cid:13)(cid:18)(cid:12) (cid:0)(cid:0)(cid:0)(cid:2)(cid:3)(cid:19)(cid:6)(cid:18)(cid:9)(cid:3)(cid:7)(cid:5)(cid:13)(cid:14)(cid:7)(cid:15) (cid:20)(cid:2)(cid:3)(cid:21)(cid:12)(cid:6)(cid:5)(cid:3)(cid:10)(cid:9)(cid:6)(cid:8)(cid:15)(cid:6)(cid:12)(cid:10)(cid:15)(cid:6)(cid:5)(cid:8)(cid:15)(cid:10)(cid:11)(cid:6)(cid:22)(cid:23) (cid:8)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:9)(cid:10)(cid:24)(cid:15)(cid:12)(cid:24)(cid:13)(cid:5)(cid:6)(cid:25)(cid:2)(cid:3)(cid:4)(cid:5)(cid:13)(cid:9)(cid:17)(cid:24)(cid:6)(cid:3)(cid:26)(cid:6)(cid:6)(cid:9)(cid:27)(cid:8)(cid:24)(cid:28) (cid:29)(cid:10)(cid:5)(cid:6)(cid:24)(cid:15)(cid:6)(cid:9)(cid:3)(cid:29)(cid:10)(cid:11)(cid:6)(cid:5)(cid:12)(cid:10)(cid:15)(cid:23)(cid:3)(cid:30)(cid:25)(cid:31) (cid:0)(cid:18)(cid:15)(cid:6)(cid:5)(cid:7)(cid:5)(cid:6)(cid:15)(cid:8)(cid:27)(cid:22)(cid:6)(cid:3)(cid:29)(cid:10)(cid:5)(cid:6)(cid:24)(cid:15)(cid:6)(cid:9)(cid:3)(cid:29)(cid:10)(cid:11)(cid:6)(cid:5)(cid:12)(cid:10)(cid:15)(cid:23)(cid:3)(cid:30)(cid:15) ! (cid:10)(cid:12)(cid:3) " (cid:13)(cid:5)(cid:28) (cid:27)(cid:2)(cid:3) # (cid:6)(cid:18)(cid:6)(cid:5)(cid:8)(cid:15)(cid:6) (cid:6) $ (cid:7)(cid:22)(cid:8)(cid:18)(cid:8)(cid:15)(cid:10)(cid:13)(cid:18)(cid:12) Figure 1 : Overall technical pipeline to curate collected ideations , prepare diversely selected prompts , send the prompts , and provide feedback ( as scores and three expla - nation types ) for iterative ideation . Dots represent prompts ( green ) and ideations ( red for prior , blue for iterations of new ideation ) as points in a 2D vector space of ideas . A prompt will attract ideations towards it , though influence will de - pend on prior iterations and explanation type . 3 . 1 Feedback Score from Prediction Models The base feedback provides assessment scores of the ideation . Cur - rently , we predict two scores using a machine learning model and a heuristic model . The first goal for ideation is typically to ensure high - quality ideas , hence , we predict a quality score 𝑠 𝑞 . However , to mitigate redundancy in crowd ideation [ 26 ] , another goal is to in - crease diversity , which we predict with a diversity score 𝑠 𝑑 . Hence , we integrate both scores in our modeling . 3 . 1 . 1 Quality Score Prediction . We predicted quality using a ma - chine learning model , since it is rated by people . We trained the prediction model based on data collected by Cox et al . [ 26 ] . The training dataset consists of 815 ideation messages 𝒙 ∈ 𝑋 written by 250 ideators and rated by 150 validators on quality ( motivating ) . Ideators wrote messages to encourage physical activity and were shown various prompts ( None , Random , Directed Diversity ) . Each message was rated by M = 4 . 6 validators on a 7 - point Likert scale ( – 3 very demotivating to + 3 very motivating ) . We model the prediction problem as a binary classification instead of regression on 7 inter - vals to simplify it , because subjective ratings are noisy and suffer from individual variance . Furthermore , we binarized the rating of each message to whether it was highly motivating ( above Median = 1 . 17 ) to create a balanced dataset ( equal number of positive and negative classes ) for better model training . While this approach includes borderline ratings that may train an inaccurate decision boundary , it benefits from training with a larger dataset . See Table 5 in Appendix A . 1 for example training data . Similar to [ 26 ] , we computed the Universal Sentence Encoder ( USE ) [ 20 ] embedding 𝒛 for each ideation to represent its idea as a vector . Using this embed - ding and normalized message length as input features , we trained a two - layer fully - connected neural network with sigmoid activation in the final layer to predict quality . The model was accurate with AUC = 0 . 717 from a 5 - fold cross - validation . We determine the qual - ity score 𝑠 𝑞 as the prediction confidence ( 0 - 100 % ) . Note that this represents the probability of the message being highly motivating , rather than the estimated rating of a validator in [ 26 ] . 3 . 1 . 2 Diversity Score Calculation . While [ 26 ] had also collected validator ratings on perceived diversity , those measures relate to pairs or groups of messages rather than individual ones . We require a single diversity label for each message to provide direct feedback during ideation , hence we use a heuristic calculation . We deter - mined the diversity score 𝑠 𝑑 by adding the new ideation 𝒙 to the prior ideations 𝑋 , and calculating the increase in the Ideation Dis - persion ( MST Sum of Edge Weights ) metric as defined in [ 26 ] from the prior collection to the new collection with the new ideation . For evaluation ( later section ) , we initialized with 50 prior ideations from messages collected by Cox et al . [ 26 ] . 3 . 2 Feedback Explanations from Explainable AI Inspired by Miller [ 60 ] , we propose actionable explanations to in - crease ideation quality and diversity — attribution , contrastive attri - bution , and counterfactual suggestions . These provide multi - faceted feedback for ideators to understand issues and opportunities to ideate better . Our explanation techniques are agnostic to the predic - tion models and can generalize to multiple scores . For brevity , we combine the notation for the quality and diversity scores as vector 𝒔 = ( 𝑠 𝑞 , 𝑠 𝑑 ) ⊤ . Next , we describe each explanation type in detail . 3 . 2 . 1 Attribution Explanation . Attribution explanations answer the question “Why P " . They indicate which features are influential in a prediction . For ideated messages , we treat each word as a fea - ture . Each attribution may support ( positive value ) or undermine ( negative ) the prediction . The larger the magnitude , the stronger the influence . Since the goal is to increase the score , the explana - tion should explain which features most hinder a higher score , and ideators should focus on words with the most negative attribution . There are several methods for calculating feature attributions , such as calculating their gradients [ 82 ] , backpropagated relevance [ 13 ] , Shapley values [ 57 ] , or approximating linear slopes [ 72 ] . However , these methods are computationally expensive , since they involve iterating through many parameters , features , or neighboring in - stances . This is infeasible for our application , since we need fast calculations for live feedback and cannot pre - compute explanations . Instead , we use a simpler sensitivity analysis based on ablation [ 73 , 84 ] . Our approach to calculating attributions are as follows : 1 . Define the ideation message as 𝒙 with the 𝑟 th word as 𝑥 𝑟 2 . Ablate ( remove ) the r th feature 𝑥 𝑟 from the dataset 3 . Calculate the score of the new simulated ideation 𝒔 ( 𝒙 \ { 𝑥 𝑟 } ) 4 . Calculate the feature attribution 𝑤 𝑟 as the decrease in the pre - dicted score 𝒔 , i . e . , 𝑤 𝑟 = − ( 𝒔 ( 𝒙 ) − 𝒔 ( 𝒙 \ { 𝑥 𝑟 } ) ) 5 . Shift all attributions to be negative to emphasize the most im - portant features to change with largest magnitude , i . e . , 𝑤 𝑟 → 𝑤 𝑟 − min 𝜌 ∈ 𝑅 ( 𝑤 𝜌 ) . This makes the attributions more actionable . Figure 2a illustrates this algorithm . This approach can be applied to the Diversity and Quality scores , since it is agnostic to how the score is calculated ( can be machine learning or heuristic ) . See the red highlights ( negative attributions ) of the feedback user interface in Figure 6 for an example of attributions explanation . CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim (cid:0)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:7)(cid:11)(cid:12)(cid:13) (cid:4) (cid:14) (cid:15) (cid:16) (cid:4) (cid:14) (cid:15) (cid:16) (cid:4) (cid:14) (cid:15) (cid:16) (cid:4) (cid:14) (cid:15) (cid:16) (cid:4) (cid:14) (cid:15) (cid:16) (cid:17) (cid:17) (cid:18) (cid:19)(cid:2)(cid:3)(cid:15)(cid:11)(cid:9)(cid:12)(cid:10)(cid:20)(cid:6)(cid:21)(cid:0)(cid:19)(cid:10)(cid:9)(cid:0)(cid:22)(cid:3)(cid:23)(cid:9)(cid:24)(cid:24)(cid:20)(cid:13)(cid:10)(cid:7)(cid:11)(cid:12) (cid:4) (cid:14) (cid:15) (cid:16) (cid:25)(cid:20)(cid:26)(cid:22)(cid:0)(cid:19)(cid:20)(cid:6)(cid:20)(cid:22)(cid:0)(cid:10)(cid:20)(cid:27)(cid:28)(cid:11)(cid:6)(cid:27) (cid:15) (cid:29) (cid:30) (cid:31) (cid:23)(cid:20)(cid:0)(cid:6)(cid:19) (cid:6)(cid:20)(cid:22)(cid:0)(cid:10)(cid:20)(cid:27)(cid:28)(cid:11)(cid:6)(cid:27)(cid:13) (cid:8)(cid:2)(cid:3)(cid:15)(cid:11)(cid:12)(cid:10)(cid:6)(cid:0)(cid:13)(cid:10)(cid:7) ! (cid:20)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:7)(cid:11)(cid:12)(cid:13) (cid:4) (cid:14) (cid:15) (cid:16) " (cid:10)(cid:20)(cid:6)(cid:0)(cid:10)(cid:7)(cid:11)(cid:12) (cid:4) (cid:16) (cid:4) (cid:14) (cid:15) (cid:16) " (cid:10)(cid:20)(cid:6)(cid:0)(cid:10)(cid:7)(cid:11)(cid:12) (cid:4) (cid:14) (cid:15) (cid:16) # (cid:27)(cid:7)(cid:10)(cid:13) (cid:4) (cid:16) # (cid:27)(cid:7)(cid:10)(cid:4)(cid:5)(cid:6)(cid:13) Figure 2 : Conceptual approach for explanations . Each square represents a word , each letter for a different word . A word can be existing ( solid line ) , inserted ( double - line ) , or deleted ( dotted ) . Colors indicate attribution : red = word that should be changed to increase score , blue = good word that increased or could increase the score , darker color = higher score magnitude . a ) For an ideation ( top row ) , the attribution towards a word is based on the score decrease ( right stacked bar chart ) when it is deleted from the ideation ( e . g . , A , C , D ) . Their sum is the total Attributions . b ) Contrastive attribu - tions compare an ideation 𝑡 + 1 ( second row ) with its previ - ous iteration 𝑡 ( first ) . B and C were deleted , while M was in - serted . Attributions for these edits show that deleting C and adding M were beneficial , while deleting B was detrimental . c ) Counterfactual Suggestions involve searching for related words from a knowledge graph ( lower left ) and calculating their Attribution towards increasing the scoring . Using the suggested word M as inspiration , the ideator may replace C with M . 3 . 2 . 2 Contrastive Attribution Explanation . Contrastive explana - tions answer the question “Why P and not Q " . The contrastive attribution explanation extends the attribution explanation to focus on specific differences between two classification labels , and we ex - tend their use to contrast between two iterations , 𝒙 ( 𝑡 1 ) and 𝒙 ( 𝑡 2 ) , of an ideation . This identifies the edit differences in word attributions between them , which are either insertions or deletions . For sim - plicity , we do not distinguish word order . We generate contrastive explanations for ideation iteration with these steps : 1 . Foreachinsertedword 𝑥 𝑟 ∈ 𝑅 𝑖𝑛𝑠 , calculateattributionas 𝑤 ( 𝑡 2 ) 𝑟 ∈ 𝑅 𝑖𝑛𝑠 = ( 𝒔 ( 𝒙 ( 𝑡 2 ) ) − 𝒔 ( 𝒙 ( 𝑡 2 ) \ { 𝑥 𝑟 ∈ 𝑅 𝑖𝑛𝑠 } ) ) 2 . For each deleted word 𝑥 𝑟 ∈ 𝑅 𝑑𝑒𝑙 , calculate attribution in reverse , i . e . , add the word to the later iteration 𝑥 ( 𝑡 2 ) and compute the de - crease in score , as 𝑤 ( 𝑡 2 ) 𝑟 ∈ 𝑅 𝑑𝑒𝑙 = − ( 𝒔 ( 𝒙 ( 𝑡 2 ) ) − 𝒔 ( 𝒙 ( 𝑡 2 ) ∪ { 𝑥 𝑟 ∈ 𝑅 𝑑𝑒𝑙 } ) ) 3 . Calculate the change in scores , i . e . , Δ 𝑠 = 𝒔 ( 𝒙 ( 𝑡 2 ) ) − 𝒔 ( 𝒙 ( 𝑡 1 ) ) 4 . Calibrate total attributions to match the change in scores , i . e . , 1 . Min - max normalize all attributions to between 0 and 1 2 . Shift the attributions such that (cid:205) 𝑟 ∈ { 𝑅 𝑖𝑛𝑠 ∪ 𝑅 𝑑𝑒𝑠 } 𝑤 ( 𝑡 2 ) 𝑟 = Δ 𝑠 Figure 2b illustrates this algorithm . See the red ( negative attribu - tions ) and blue ( positive ) highlights of the feedback user interface in Figure 7 for an example of contrastive attributions explanation . 3 . 2 . 3 Counterfactual Suggestion Explanation . Counterfactual ex - planations answer “How to change inputs to get Q instead of P " . For ideation , this determines how an ideation could be edited to increase its score . We propose to substitute words with alternative suggestions searched using the ConceptNet knowledge graph v5 . 8 [ 81 ] based on various semantic relationships . This will reduce the (cid:0)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:2)(cid:9)(cid:10)(cid:11)(cid:12)(cid:4)(cid:13)(cid:7)(cid:14)(cid:15)(cid:4)(cid:16)(cid:7)(cid:11)(cid:17)(cid:8)(cid:14)(cid:9)(cid:12)(cid:4)(cid:8)(cid:10)(cid:11)(cid:7)(cid:18)(cid:10)(cid:8)(cid:14)(cid:19)(cid:4)(cid:10)(cid:14)(cid:4)(cid:19)(cid:11)(cid:20)(cid:10)(cid:4)(cid:8)(cid:10)(cid:11)(cid:7)(cid:18)(cid:10)(cid:8)(cid:14)(cid:19) (cid:0)(cid:21)(cid:3)(cid:4)(cid:22)(cid:8)(cid:7)(cid:11)(cid:21)(cid:10)(cid:12)(cid:4)(cid:13)(cid:7)(cid:14)(cid:15)(cid:4)(cid:8)(cid:23)(cid:11)(cid:18)(cid:10)(cid:8)(cid:14)(cid:19)(cid:4)(cid:10)(cid:14)(cid:4)(cid:8)(cid:23)(cid:11)(cid:18)(cid:10)(cid:8)(cid:14)(cid:19)(cid:4)(cid:7)(cid:11)(cid:16)(cid:24)(cid:18)(cid:21)(cid:8)(cid:19)(cid:25)(cid:4)(cid:26)(cid:14)(cid:7)(cid:23) (cid:0)(cid:18)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:2)(cid:9)(cid:10)(cid:11)(cid:12)(cid:4)(cid:10)(cid:14)(cid:26)(cid:18)(cid:7)(cid:23)(cid:12)(cid:4)(cid:27)(cid:8)(cid:25)(cid:27)(cid:11)(cid:7)(cid:4)(cid:28)(cid:9)(cid:18)(cid:24)(cid:8)(cid:10)(cid:29) (cid:0)(cid:3)(cid:4)(cid:30)(cid:11)(cid:21)(cid:14)(cid:19)(cid:23)(cid:4)(cid:8)(cid:23)(cid:11)(cid:18)(cid:10)(cid:8)(cid:14)(cid:19)(cid:4)(cid:8)(cid:10)(cid:11)(cid:7)(cid:18)(cid:10)(cid:8)(cid:14)(cid:19)(cid:4)(cid:18)(cid:31)(cid:11)(cid:7)(cid:4)(cid:28)(cid:9)(cid:18)(cid:24)(cid:8)(cid:10)(cid:29)(cid:4)(cid:13)(cid:11)(cid:11)(cid:23)(cid:2)(cid:18)(cid:21) (cid:2)(cid:3)(cid:4) ! (cid:14)(cid:19)(cid:10)(cid:7)(cid:18)(cid:12)(cid:10)(cid:8)(cid:17)(cid:11)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:2)(cid:9)(cid:10)(cid:8)(cid:14)(cid:19) (cid:21)(cid:3)(cid:4) ! (cid:14)(cid:9)(cid:19)(cid:10)(cid:11)(cid:7)(cid:13)(cid:18)(cid:21)(cid:10)(cid:9)(cid:18)(cid:24)(cid:4)(cid:30)(cid:9)(cid:25)(cid:25)(cid:11)(cid:12)(cid:10)(cid:8)(cid:14)(cid:19) (cid:18)(cid:3)(cid:5)(cid:6)(cid:7)(cid:8)(cid:2)(cid:9)(cid:10)(cid:8)(cid:14)(cid:19) " (cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:2)(cid:9)(cid:10)(cid:11)(cid:12)(cid:4)(cid:13)(cid:7)(cid:14)(cid:15)(cid:4)(cid:16)(cid:7)(cid:11)(cid:17)(cid:8)(cid:14)(cid:9)(cid:12)(cid:4)(cid:8)(cid:10)(cid:11)(cid:7)(cid:18)(cid:10)(cid:8)(cid:14)(cid:19)(cid:4)(cid:10)(cid:14)(cid:4)(cid:19)(cid:11)(cid:20)(cid:10)(cid:4)(cid:8)(cid:10)(cid:11)(cid:7)(cid:18)(cid:10)(cid:8)(cid:14)(cid:19) " (cid:21)(cid:3)(cid:4)(cid:22)(cid:8)(cid:7)(cid:11)(cid:21)(cid:10)(cid:12)(cid:4)(cid:13)(cid:7)(cid:14)(cid:15)(cid:4)(cid:8)(cid:23)(cid:11)(cid:18)(cid:10)(cid:8)(cid:14)(cid:19)(cid:4)(cid:10)(cid:14)(cid:4)(cid:8)(cid:23)(cid:11)(cid:18)(cid:10)(cid:8)(cid:14)(cid:19)(cid:4)(cid:7)(cid:11)(cid:16)(cid:24)(cid:18)(cid:21)(cid:8)(cid:19)(cid:25)(cid:4)(cid:26)(cid:14)(cid:7)(cid:23) " (cid:18)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:2)(cid:9)(cid:10)(cid:11)(cid:12)(cid:4)(cid:18)(cid:26)(cid:18)(cid:29)(cid:4)(cid:13)(cid:7)(cid:14)(cid:15)(cid:4)(cid:16)(cid:7)(cid:8)(cid:14)(cid:7)(cid:4)(cid:8)(cid:23)(cid:11)(cid:18)(cid:10)(cid:8)(cid:14)(cid:19)(cid:12) " (cid:3)(cid:4)(cid:30)(cid:11)(cid:21)(cid:14)(cid:19)(cid:23)(cid:4)(cid:8)(cid:23)(cid:11)(cid:18)(cid:10)(cid:8)(cid:14)(cid:19)(cid:4)(cid:8)(cid:10)(cid:11)(cid:7)(cid:18)(cid:10)(cid:8)(cid:14)(cid:19)(cid:4)(cid:18)(cid:31)(cid:11)(cid:7)(cid:4)(cid:23)(cid:8)(cid:17)(cid:11)(cid:7)(cid:12)(cid:8)(cid:10)(cid:29)(cid:4)(cid:13)(cid:11)(cid:11)(cid:23)(cid:2)(cid:18)(cid:21) (cid:22)(cid:8)(cid:17)(cid:11)(cid:7)(cid:12)(cid:8)(cid:10)(cid:29)(cid:4)(cid:14)(cid:19)(cid:24)(cid:29) # (cid:18)(cid:24)(cid:8)(cid:10)(cid:29)(cid:4)(cid:14)(cid:19)(cid:24)(cid:29) $ (cid:7)(cid:11)(cid:23)(cid:8)(cid:21)(cid:10)(cid:11)(cid:23)(cid:4) (cid:30)(cid:21)(cid:14)(cid:7)(cid:11)(cid:4)(cid:22)(cid:8)(cid:7)(cid:11)(cid:21)(cid:10)(cid:8)(cid:14)(cid:19) Figure 3 : Conceptual effect of providing different types of ex - planations for ideation feedback with the intent to increase the score for diversity ( top row ) or quality ( bottom row ) sep - arately . Dots represent prompts ( green ) and ideations ( red for prior , blue for iterations of new ideation ) as points in a 2D vector space of ideas . 1 ) Darker red regions indicate loca - tions in the vector space that are dense with prior ideations that new ideations should avoid increasing diversity . 2 ) Yel - low areas indicate a nonlinear decision surface with the change in color representing a sharp decision boundary . 1 , 2 ) Purple arrows indicate the directional influence of the feed - back . a ) Attribution explanations direct ideation towards a higher score . b ) Contrastive attribution explanations convey the difference between iterations along the direction of a higher score . c ) Counterfactual suggestion explanations di - rect towards an alternative , similar ideation with a higher score . cognitive load , mitigate the ideator’s lack of experience with recall - ing related terms , and stimulate more ideas [ 8 ] . In an ideation 𝒙 , for each important feature word 𝑥 𝑟 with negative attribution , 1 . Search for related words 𝑥 𝜌 𝑟 using the knowledge graph 1 . Exclude feature words 𝑥 𝑟 with too few ( < 10 ) related words 2 . Exclude relations 1 that are less actionable 2 . For each related word 𝑥 𝜌 𝑟 , 1 . Substitute feature word 𝑥 𝑟 with its related word 𝑥 𝜌 𝑟 into the ideation message 𝒙 2 . Filter word 𝑥 𝜌 𝑟 for relevance 1 . Compute USE [ 20 ] embedding vector 𝒛 𝑟 for the word 𝑥 𝜌 𝑟 2 . Compute mean pairwise distance ¯ 𝑑 𝜌 𝑟 to prior ideations 3 . Exclude word if out - of - scope , like in [ 26 ] , i . e . , ¯ 𝑑 𝜌 𝑟 > 𝛿 3 . Calculate attribution 𝑤 𝜌 𝑟 due to deleting 𝑥 𝑟 and inserting 𝑥 𝜌 𝑟 , i . e . , 𝑤 𝜌 𝑟 = − ( 𝒔 ( 𝒙 ) − 𝒔 ( 𝒙 ∪ { 𝑥 𝑟 } \ { 𝑥 𝜌 𝑟 ) } ) ) 4 . Include related word with large attribution , i . e . , 𝑤 𝜌 𝑟 > 𝜔 Figure 2c illustrates this algorithm and Figure 8 shows a demo . We limited Counterfactual suggestions for words with negative attri - bution . The approach cannot replace phrases or whole sentences . 1 Excluded : Synonym , Antonym , DerivedFrom , SymbolOf , DefinedAs , MannerOf , Ety - mologicallyRelatedTo , EtymologicallyDerivedFrom , ExternalURL . Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Table 1 : Feedback Conditions in user studies . We investi - gated 6 variants of the Feedback Interface ( columns ) , formed from combinations of four Feedback Features ( rows ) . Feed - back Interface is the main independent variable in our ex - periments . Feedback Interface Feedback Feature N S SA SAX SAC SAXC Score Prediction ( S ) ✓ ✓ ✓ ✓ ✓ Attributions ( A ) ✓ ✓ ✓ ✓ Contrastive Attributions ( X ) ✓ ✓ Counterfactual Suggestions ( C ) ✓ ✓ 3 . 2 . 4 Summary and Hypothesized Effects of Explanations . We pro - posed three actionable explanations for improving ideation scores . Specifically , we a ) shifted Attribution explanations to frame influ - ence by which words are best to improve the ideation ; b ) framed Contrastive Attributions by what changes were successful or detri - mental ; and c ) streamlined Counterfactual Suggestions to recom - mend non - distant words that are estimated to improve scores . These explanations aim to direct ideators towards higher scores , but have slight differences in direction ( Figure 3 ) . The Attributions explanation ( Figure 3a ) points in the direction of increasing score , but may be prone to some error due to the approximations in the ablation technique ( e . g . , not calculating Shapley values [ 57 ] ) . The Contrastive Attributions explanation ( Figure 3b ) describes how the difference between two iterations works towards or away from in - creasing score ; this is equivalent to resolving the blue arrow vector along the red / yellow line vector . It is also subject to approximation errors like Attributions . The Counterfactual Suggestion explana - tion ( Figure 3c ) directs the ideator towards a hypothetical ideation with the substituted word ( s ) , which may not be the most direct towards increasing score . Finally , all three explanation types do not necessarily direct the ideation straight towards the original prompt , but this can also provide opportunities for diversification . 4 PROMPT AND FEEDBACK INTERFACES We describe the ideation interface variants and how to use them . 4 . 1 Ideation Task and Prompt Interface Consider the ideator’s task to write motivational messages to en - courage exercise and physical activity . For each ideation , she is prompted with a phrase and asked to write a message inspired by the phrase ( Figure 4 ) . She does not need to use the words or concepts if she finds them too irrelevant or awkward . After the first iteration , the participant is shown feedback and asked to revise the ideation , up to two times . For each iteration , she is reminded of the fixed original prompt and shown feedback based on her writing . 4 . 2 Feedback Features and Interfaces We investigated 6 variants ( see Table 1 ) of the Feedback Interface that had different combinations of whether to show explanations and which type ( Feedback Features ) . Note that since all XAI - based feedback explain the Score , they cannot omit showing it . Further - more , since attribution is foundational to contrastive attribution and selecting words for counterfactual suggestions , all XAI - based feedback interfaces include Attribution explanations . Figure 4 : Prompt UI for ideation . Figure 5 : User interface with Score feedback ( S ) showing Di - versity and Quality scores for each iteration . Figure 6 : Score + Attribution ( SA ) feedback showing the most attributed words for the selected score ( Diversity , in this case ) . Darker red indicates higher importance . Hovering over a highlighted word shows the attribution sub - score . Figure 7 : SA + Contrastive Attribution explanations ( SAX ) feedback comparing Attempts 1 and 3 . Underlined words were inserted in Attempt 3 compared to 1 , and struck - out words were deleted from Attempt 1 to 3 . Inserting words “walk " , “sitting " and “time " , and deleting “exercise " increased Diversity score , but inserting “reduce " decreased the score . Figure 8 : SAX + Counterfactual Suggestion explanations ( SAXC ) feedback . Hovering a highlighted word will show a pop - up with attribution importance , and suggested replace - ment words . Potential score increases indicated with blue highlights ; darker blue for higher increase . This suggests changing “time " to “musical time " to increase diversity by + 1 % , but to " dreamlining " to increase quality by + 2 % . CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim The first baseline interface with No feedback ( N ) ( Feedback Condition = N , Figure 5 , left two columns only ) only shows the previous messages that the ideator wrote in separate rows in a table . The ideator is only asked to write the next iteration without any other information . The second baseline interface , adds Scores ( S ) on the right columns of the feedback table ( Figure 5 ) . Ideators can see two scores for motivating - ness ( quality ) and diversity increase . The quality score shows confidence % of the model when predicting high quality . The diversity score was normalized with 100 % for the maximum possible angular distance ( 𝜋 on the unit hypersphere ) . We next describe the XAI - based Feedback Features used in the interpretable feedback interfaces . Note that we describe them as revised after findings from the formative study ( described later ) . Attribution ( A ) explanations ( Figure 6 ) are presented as red high - lights to indicate key words that ideators should consider changing to improve the ideation scores . Darker reds indicate more negative attributions . These exclude stop words ( e . g . , “the " , “to " ) . To limit information overload , only three words are highlighted . Since the quality and diversity objectives are not necessarily aligned ( Fig - ure 3 ) , the highlights are specific to each score only one at a time . Ideators view explanations for each score by selecting its radio button ( in table heading ) . When the ideator hovers her mouse over a highlighted word , a popup will show its attribution sub - score . Contrastive Attribution ( X ) explanations ( Figure 7 ) are sim - ilar to Attribution explanations , but only word differences between two iterations are highlighted . The last iteration can be compared against any earlier iteration . Deletions are highlighted in the earlier iteration , while insertions are highlighted in the last iteration . Red highlights have the same meaning as for Attribution explanations , while blue highlights indicate beneficial edits ( positive attribution ) for verifying successful edits . Darker blues indicate stronger im - provements . Currently , only iterations of the same ideation are compared , though future work could compare different ideations . Counterfactual Suggestion ( C ) explanations ( Figure 8 ) sug - gest alternative words to replace a problematic word ( negative attribution ) . Hovering on a red highlighted word shows a popup with its attribution to the score ( same for A and X explanations ) , and lists related words that could be used for substitution . The potential change in both scores is calculated for each related word . Ideators can decide whether to improve quality or diversity , decide their relevance , and integrate that word or something else . Ideators are reminded that they can propose their own terms instead of what was suggested . To limit confusion and increase utility , only words which a ) have an increase in either score and b ) are not too irrelevant ( i . e . , not distant from “physical activity " ) are included . 4 . 3 System Implementation and Initialization The system was deployed in a survey implemented in Qualtrics , which embeded an external webpage for the ideation user inter - face , hosted from a web server with Intel Xeon CPU E5 - 2640 v4 @ 2 . 40Ghz x 40 , 128GB RAM , and Tesla P100 GPU . We used the GPU for calculating the USE embeddings for prompt phrases , ideations , and words ; this is used for diversity score predictions and attribu - tion calculations . Each calculation of the score prediction and ex - planations took 3 - 4 seconds for messages with 20 - 40 words , which pilot participants found as an acceptable wait time . To prompt ideators , we chose 50 Directed phrases from the cor - pus of [ 26 ] , which we randomly sampled without replacement for each participant . Each ideator will not see repeated prompts . The collection of prior ideations was initialized with 50 ideations from [ 26 ] randomly chosen from the None condition ( no prompting ) . For ecological ( external ) validity , we dynamically updated the collec - tion of prior ideations after each ideator submits an ideation . This captures the growth of diversity in the collection as more ideations are submitted . We do this separately for each Feedback Interface variant to keep them independent , i . e . , ideations from ideators in each UI condition are only added to the collection of prior ideations for that condition so as not to “contaminate " other collections . 5 EVALUATION We evaluated the usability , understandability , usage and usefulness of different Feedback Interfaces in multiple studies . First , we con - ducted a Formative Ideation Study to qualitatively observe how ideators use the different feedback and elicit their opinions about the feedback . This helped us to identify usage patterns , ensure that the interfaces were usable and useful , and mitigate any usability issues . Next , we conducted a controlled Summative Ideation Study with participants as ideators to determine if providing explana - tions improved ideation Quality and Diversity over No Feedback or Score - only Feedback baselines , to compare which explanations were better , and to assess the ease of understanding and ease of use for each explanation type . In this study , we measured ideation Quality and Diversity from computational metrics , but these may be non - representative of how people would perceive the ideations . Hence , we conducted Validation Studies of Ideation Quality and Ideation Diversity to measure perceptions from third - party valida - tors . Two studies were required due to the different experiment procedures for each measure — individual ideation rating for Qual - ity and pairwise ideation rating for Diversity , like in [ 26 ] . 5 . 1 Formative Ideation User Study We conducted a formative user study to investigate 1 ) how users interpreted various explanation features , 2 ) how that influenced their ideation , and to identify 3 ) usability or interpretability issues in our initial design . We then refined our user interfaces for the subsequent summative user study ( described later ) . 5 . 1 . 1 Method and Procedure . We recruited 15 participants from a university mailing list . They were 6 male , 9 female , with ages 21 - 33 years old , and all were students . The experiment took 40 - 50 min - utes and participants were compensated $ 10 SGD ( $ 7 . 43 USD ) . We employed a within - subjects design with Latin square arrangement to mitigate order effects . We conducted the study online via a Zoom audio call with screen recording , which the participant consented to . Participants completed a tutorial and could clarify instructions with an experimenter . Next , participants performed one ideation session for each of all 6 variants of the Feedback Interface listed in Table 1 . For each ideation session , the participant was prompted with a different phrase to ideate a motivational message for physical activity . After submitting the attempt , she saw the feedback , and revised her message ; this happened for two revisions . Thus , the experiment was within - subjects with 6 Conditions × 1 Ideation × Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA 3 Iterations . Using the think aloud protocol , participants were en - couraged to speak their thoughts as they read the prompt , feedback , and thought about what to write . We also asked them about their experience with feedback and comments for improvement . 5 . 1 . 2 Qualitative Findings . We transcribed the interviews and per - formed a thematic analysis of usage and utterances . Guided by the three goals for the formative study , one co - author coded the findings into themes with regular discussion with other co - authors . We organized our findings by our three objectives : feedback inter - pretations , ideation approach , and interpretability issues . Feedback interpretations . Typical feedback usage involved : 1 ) noting the prediction scores ; 2 ) examining which words were high - lighted in red to “know which words to focus on " [ P13 ] ; 3 ) looking up related words with the counterfactual suggestion popup , and con - sidering which would lead to the highest score increases for both diversity and quality . After the second iteration , participants would also use the compare mode to check which words were detrimental or useful . In summary , the exploration sequence was attributions → counterfactual suggestions → contrastive attributions . How - ever , participants had some difficulties when making sense of the feedback . Some participants tried to self - explain the quality score , since this concept is more commonplace , but had to depend on the feedback regarding the diversity score , since “without the scores I wouldn’t be able to tell whether my sentence is diverse " [ P12 ] and “with the related words I know whether there will be an increase " [ P13 ] . When not receiving explanations , some participants struggled to deeply understand why their ideation scored poorly on quality . For example , P14 found that “the scores seem a little arbitrary " ; P10 felt that it “doesn’t show why it is motivating " ; and P3 wondered why ‘exercise’ was highlighted , since “ [ exercise ] is the main word , not like I could really change anything about it " . This was actually because many participants redundantly used this word . Currently , our approach highlights culpable words , but this suggests the need for semantic explanations . Participants were also confused when their scores decreased despite following the explanations . P12 “tried to change the word with a suggestion but wasn’t sure why the score decreased " . After being suggested the word ‘desirable’ to replace ‘wanted’ with a projected + 5 % to quality the score , P13 wrote ‘de - sired’ and was confused to get a 2 % score decrease instead . Thus , the high dimensionality of language modeling can lead to spuri - ous errors due to non - linear relations , harming user experience . Nevertheless , in our later study , we found that the feedback was useful for multiple users . Finally , while our feedback was automated from a corpus and language model , P10 wanted suggestions to “try including [ words ] based on other people’s answers " to rely on social proof [ 23 ] . Ideation approach . Participants ideated differently based on feed - back type . Without explanations , they mostly depended on trial - and - error . P13 felt that writing more specifically , concretely , or with simpler words could increase diversity scores ; for example , she re - vised the term ‘physical activity’ to ‘pull up’ to be more diverse “because you’re taking a specific activity rather than a general term " . Writing more specific terms is consistent with goal setting theory [ 53 , 54 ] and distinct words have different embedding placements in our vector space . Participants could be more focused with high - lighted words as they “gave me something to work off " [ P14 ] . Next , we describe some interesting breakdowns and user mitigations . Some participants struggled with the potentially divergent na - ture of the quality and diversity criteria . There was diverse prefer - ence to focus on either improving quality since it was more intuitive ( e . g . , P6 , P10 ) , or diversity since it fluctuated more at each iteration ( e . g . , P1 , P4 ) . Hence , to prioritize either criteria , each score could be rescaled to nudge users accordingly . Due to the breadth of concepts in ConceptNet , participants found that the suggested words some - times seemed irrelevant , yet some could be tangentially inspiring . On being suggested the terms ‘skate’ and ‘release energy’ to re - place ‘exercise’ , P11 substituted with ‘swimming’ , perhaps because of finding another activity that is more energetic than skating and remembering terms starting with ‘s’ . Some participants were too adherent to the suggestions to the extent of losing task relevance . On receiving the term ‘arsenic trichloride’ ( with potential + 1 % for Diversity and + 4 % for Motivating ) to replace her word ‘organic’ , P11 used the chemical term in her ideation and rewrote “ . . . by aug - menting physical activities with organic supplements " to “ . . . by augmenting physical activities with arsenic trichloride vitamins " , which is nonsensical since Arsenic trichloride ( 𝐴𝑠𝐶𝑙 3 ) is actually a highly toxic substance . This indicates that it is important to have an additional step to filter ideations for safety . Furthermore , while the feedback is helpful to improve scores , we found that some partici - pants drifted away from the prompt phrase . Starting with the phrase ‘right to take care’ and writing ‘You are responsible for taking care of your own health’ , P8 fixated on improving the lower scoring word ‘responsible’ , and ended up writing “You are in control of your own health " , which inadvertently dropped the word ‘care’ ; thus , she neglected her original prompt , though this did slightly improve her calculated diversity ( 55 % to 56 % ) . Finally , we found that our focus on word - based feedback could limit some ideation styles . Interestingly , when ideating without feedback , some participants wrote with a collective tone , e . g . , “Let’s go and exercise " [ P1 ] , “We can start our exercise with some stretching " [ P4 ] ; but this tone was absent when feedback was provided , and ideations became neutral and formal , e . g . , " Exercising will reduce chances of you going for surgeries and you can feel better , lose weight and be fitter " [ P1 ] . Interpretability issues and remedies . We discuss issues identified by participants and remedies that we subsequently implemented for the interfaces tested in the summative study . Our attribution explanations originally highlighted about 6 words per ideation and users found it too tedious to track and manage all of them . Rem - edy : we limited the highlights to 3 words with the most negative attributions . Interviewees were confused with many counterfac - tual words , since they had negative or low improvement scores or were not semantically relevant ( e . g . , replacing ‘play’ with suggested word ‘kids’ with potential + 0 % for Diversity and + 0 % for Motivat - ing ) . Remedy : we ensured that suggested words have at least one positive score , and limited to relevant suggestions by eliminating words that had embedding distances too far from words ‘exercise " and ‘physical activity " . Participants had found the compare mode ( to show contrastive attributions ) useful , but tended to forget to switch over to it . Remedy : we set contrastive explanations as de - fault for each iteration if this explanation was available . The default can be gradually reset after users acclimatize to remembering this CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim Table 2 : Dependent variables ( DV ) measured in the summative ideation user study . Measure Of Description and Justification Ideation task time Iteration Duration to ideate at each iteration . Objectively measures ideation fluency [ 10 ] or ease of ideation [ 26 ] . Perceived Importance Quality score , Diversity score Ideators’ preference of perceived importance for message quality or diversity [ 7 - point Likert scale : – 3 = “Motivating much more important " , + 3 = “Diverse much more important " ] . PerceivedIdeationQuality Ideation ( Final iteration ) Ideators’ self - assessment of how motivating and creative the ideation is [ 7 - point Likert scale : – 3 = Strongly Disagree , + 3 = Strongly Agree ] . Understanding , Ease of Use , Helpfulness Prompt , Feedback Feature Ideators’ perceptions asked separately of the Prompt and each Feedback Feature ( S of Quality and Diversity , A , X , C ) [ 5 - point Likert scale : – 2 = Strongly Disagree , + 2 = Strongly Agree ] and rationale [ Open text ] . Usage Prompt , Feedback Qualitative description of how the feedback information was used [ Open text ] . This was only asked of the 2nd ideation for each interface section . ComputationalDiversity Ideation Metrics of diversity based on ideation embeddings [ 26 ] [ Ideation Dispersion ( MST Mean of Edge Weights ) , Ideation Disparity ( Mean Pairwise Distance ) , and Repeller Chamfer Distance ( Mean Min Pairwise Distance ) ] . feature . We implemented these improvements in the feedback UI , and launched a larger summative controlled study to measure the impact of explanations on ideation quality and diversity . 5 . 2 Summative Ideation User Study We conducted a mixed - design experiment with main independent variable ( IV ) , Feedback Condition ( N , S , SA , SAX , SAC , SAXC ) and secondary IV , Iteration ( t = 1 , 2 , 3 ) . Iteration was fully within - subjects , while Feedback Condition was partially within - subjects . For each ideation , participants saw a Directed prompt from a pre - selected set of 50 ( Section 4 . 3 ) [ 26 ] . All Feedback Conditions had the same 50 prompts . We exposed each participant to 2 randomly - selected Feedback Conditions with 2 prompts per type and 3 iterations per prompt ( total 12 ideation iterations ) to mitigate individual variance , while limiting fatigue with too many trials . We limited the prompt to contain only one phrase , since users could struggle to utilize multiple phrases [ 26 ] . While the system could show explanations for both Quality and Diversity , we showed Diversity explanations by default to prioritize improving diversity which users may find less intuitive ( Section 5 . 1 . 2 ) . Table 2 describes dependent variables measured . The experiment apparatus and survey questions were implemented in Qualtrics ( see Appendix A . 2 . 1 ) . 5 . 2 . 1 Experiment Task and Procedure . Participants were tasked to write motivational messages towards physical activity using various feedback with the following procedure : 1 . Introduction to experiment objective and consent to the study . 2 . Screening quiz with a 4 - item word association test [ 22 ] to assess English language skills . All answers must be correct to continue . 3 . UI section ( × 2 ) with different Feedback Conditions . a ) Tutorial about ideation process , and how to use the feedback . b ) Ideation session ( × 2 ) to ideate based on a prompt each . 1 . Promptedideationtoviewaprompt , writeaninitial ideation in one to three sentences , and submit for automatic review . This page is timed to measure ideation task time . 2 . Iterated revision ( × 2 ) to receive feedback and revise . 1 . Prompted ideation to view the same prompt again . 2 . Ideation feedback to inform the participant where and how to improve their previous ideation . 3 . Ideation revision and submission . With these two revi - sions , there are three ideation iterations . 3 . Perception questionnaire to ask participants about their perceptions of usage and usability ( Table 2 ) . 4 . Post - questionnaire on demographics . To control fatigue , we limited to 4 ideation trials . Instead of 4 ideations × 1 condition ( no repeated trials , potentially noisy ) , or 1 ideation × 4 conditions ( fully between - subjects , high individual variance ) , we balanced with 2 ideations × 2 conditions . 5 . 2 . 2 Experiment Data Collection . We recruited participants from Amazon Mechanical Turk ( AMT ) with high qualification ( ≥ 5000 completed HITs , > 97 % approval ) . 104 workers attempted the survey , 97 passed the screening quiz , and 70 completed the survey . They were 42 . 0 % female , between 23 and 66 years old ( M = 39 . 5 ) . Partic - ipants were compensated US $ 4 . 00 after completing the ideation tasks and surveys . Participants completed the survey in median time 26 . 0 minutes and were compensated US $ 9 . 24 / hour . We collected 4 ideations per participant ( 2 Feedback Conditions x 2 Ideations ) with up to 3 Iterations each , from 70 participants for a total of 280 ideations across the 6 Feedback Conditions . 5 . 2 . 3 Statistical Analysis . We fit linear mixed effects regression ( LMER ) modelsonvariousdependentvariables , performedANOVAs on the fixed main and interaction effects , and post - hoc contrast tests for specific differences identified . Since LMER models can ac - commodate missing data by imputing and estimating values , we can analyze our experiment with partially within - subjects independent variables . Due to the large number of comparisons in our analysis , we consider differences with p < . 001 as significant and p < . 005 as marginally significant . This is stricter than a Bonferroni correction for 50 comparisons ( significance level = . 05 / 50 ) . For perceived ratings ( Usefulness , Ease of Use , Understandabil - ity ) , we fit a LMER model with Feedback Feature ( Prompt , Diversity Score , Quality Score , Attribution , Contrastive , Counterfactual ) as fixed effect , and Participant as random effect ( Table 6 in Appendix A . 3 ) . Note that the features were considered separately rather than as a combination in the Feedback Interface . For ideation performance , we examined ideation speed , and whether Diversity and Quality Scores increased . We fit a LMER model with Feedback Condition and Prompt ( which one of the Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA c ) b ) a ) * p = . 0148 p = . 0133 p = . 0244 p = . 0238 * Figure 9 : Results of ideators’ perceived ratings for each feedback feature . Feedback Features : Pr = Prompt , S ( D ) = Diversity Score , S ( Q ) = Quality Score , A = Attribution explanation , C = Counterfactual suggestion , X = Contrastive attribution . Red markers indicate significant differences from Pr . p values indicate contrast tests relative to Pr . * indicates p < . 0001 . p = . 0057 p = . 0495 * p = . 0084 p = . 0014 * * * * * p = . 0004 c ) b ) a ) Figure 10 : Results of ideator performance . a ) Time effort ratio between Iteration 1 and 3 . b ) Increase in Quality Score from Iteration 1 to the iteration with best Quality Score . c ) Increase in Diversity Score from Iteration 1 to the iteration with best Diversity Score . Feedback Conditions : N = None , S = Score Prediction , SA = Score + Attribution , SAC = SA + Counterfactual , SAX = SA + Contrastive , SAXC = SA + X + C . p values indicate 1 - sample , 1 - tail t - tests relative to N ( a ) or 0 % ( b , c ) ; * indicates p < . 0001 . a ) b ) c ) p = . 0034 p = . 0040 * p = . 0005 * * p = . 0039 * * p = . 0011 Figure 11 : Ideation Diversity calculated with different computational metrics : a , b ) diversity of new and prior ideations , and c ) new ideations from prior ones . p values indicate contrast tests relative to N . * indicates significantly different from N at p < . 0001 . 50 prompts was shown ) as fixed main effects , and Participant as random effect ( Table 7 in Appendix A . 3 ) . We evaluated whether ideation took longer with Score and XAI feedback compared to the baseline 1st iteration , so we calculated the ratio between the 1st and 3rd iteration with a log transform ( to improve normality ) to get the Ideation Time Effort metric . Note that the best Score may be achieved at the 2nd or 3rd iteration , i . e . , the participant may inadvertently get a lower score on the 3rd attempt . Hence , we calculated the difference between the best Score of the latter two iterations with the first , and did so separately for Diversity and Quality . There was a strong negative correlation ( r = – . 436 , p < . 0001 ) between the Diversity and Quality Scores , so we included Quality Score Increase as a fixed main effect too for the Best Diversity Score Increase response , and vice versa . We also tested the Score Increase estimates from the LMER model against no increase with a 1 - sample , 1 - tail t - test for each Feedback Condition against the constant 0 . CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim To analyze objective diversity for each Feedback Condition , we resampled 50 bootstrap samples , calculated three computational diversity metrics defined in [ 26 ] ( Ideation Dispersion , Ideation Dis - parity , and Repeller Chamfer Distance ) , and fit a LMER model with Feedback Condition as fixed main effect . In Appendix A . 3 , Table 6 , Table 7 , and Table 8 summarize the goodness of fit ( R2 ) of the LMER models , and statistical significance of ANOVA tests . 5 . 2 . 4 Results . We describe participant priorities and perceptions of their ideations and the feedback , and their performance as cal - culated with metrics . When ideating , participants prioritized im - proving their Quality Score ( 56 . 5 % , rating < 0 ) instead of Diversity Score ( 12 . 6 % , rating > 0 ) , and 31 % balanced between the two . They perceived their ideations as Motivating ( high quality , 81 . 3 % with rating > 0 ) and Creative ( 73 . 8 % ) , though there was no significant dif - ference across Feedback Conditions . Participants rated all features as useful , easy to understand and use , except for Counterfactual suggestions which had lower usefulness and ease of use ( Figure 9 ) . As expected , participants took more time to ideate when receiv - ing Score ( S ) and XAI feedback , and significantly so for SAC and SAXC compared to None ( Figure 10a ) ; suggesting that they were studying these feedback . S feedback increases were non - significant or marginal compared to no feedback ( None ) . However , compared to no improvement ( Increase = 0 ) , we found that showing S or all explanations significantly improved Quality Score ( Figure 10b ) , and showing SA , SAC , and SAXC significantly improved Diversity Score ( Figure 10c ) . S did not improve Diversity Score , perhaps since ideators were blind to what they could change or do without ex - planations , and would end up randomly editing . Explanations did not improve Quality compared to just showing Scores ( p = n . s . ) , and only very marginally improved Diversity ( p = . 0414 for SAXC ) . The lack of differences could be due to the high variability in ideations and limited number of participants ; future work could recruit more than 50 participants per condition . The three computational diversity metrics [ 26 ] — Ideation Dis - persion , Ideation Disparity , and Repeller Chamfer Distance — mea - sure slightly different aspects of diversity , and had different trends ( Figure 11 ) . The first two metrics calculate the total diversity of combining new ideations with the seed prior ideations , and the third metric measures how different the new ideations are from the prior ones . In general , SAX had the highest diversity . Other results were somewhat indeterminate . SAC had the lowest Ideation Dispersion , but higher Repeller Chamfer Distance ; this suggests that Counterfactuals may help diversify ideations from those with - out any feedback ( N ) , but may not diversify ideations from other forms of feedback . The detrimental effect of Counterfactual sugges - tions may also hinder diversity when combined with Contrastive explanations ( SAXC ; Figure 11b , c ) . We qualitatively analyzed ideator rationales to interpret their perceived usefulness , understanding , and usage of various ideation feedback . Many findings echo our earlier ones from the formative study . Participants were mixed regarding whether Score feedback was helpful . P36 “found the feedback easy to understand because it gave me a number to see how well my [ message ] is . " ; but P13 “found the feedback difficult to understand because I have no idea how it’s calculated . I can’t tell what part of my idea it’s targeting so I have no idea how to improve . It feels random . " ; P46 was more positive , finding them “somewhat helpful , but did not give guidance on how to improve my scores " . This validates the need for deeper explanations . However , the receptivity towards Attribution feedback was also mixed . P41 reported that “feedback was good , preventing me from overusing common phrases " , and P43 “did like the highlights to look for different wording that is not so generic " . The highlights were more useful when used in contrast mode to show Contrastive attributions , e . g . “The compare mode definitely showed me which words I should keep and which ones I should edit , to get the optimal score . " [ P68 ] . In contrast , P64 wanted more useful explanations and found that attribution “colors and words are not as helpful as they really are ambivalent . " P63 felt “it was often not clear why a certain word would be less valuable than another or how it would affect the scores , especially diversity . " These cases indicate highlighting salient words and rating their attribution lacks rationalization insights [ 35 ] . Perceptions towards Counterfactual suggestions were more nega - tive , though sometimes positive . P22 found ideating “easier now that there were suggested words " , and P07 said it “helped me to add words to increase inspiration . " However , others found the suggested words problematic : P70 felt that “sometimes the suggested words seemed completely unrelated to the prompt or even fitness itself . " ; P28 felt that “the suggestions make the phrase a lot worse , non - grammatically cor - rect " ; P32 “found the feedback a little confusing because it seemed like when I changed words to try to increase my score it ended up keeping it close to the [ previous ] score . " Hence , Counterfactual suggestions should be relevant , grammatical , and efficacious . Providing all explanations together enabled rich usage . P22 “changed words based on suggested words , excised words that com - plicated the message needlessly , and used the comparison checker to see if there was a word that particularly hurt between each prompt . " Therefore , it is important to provide these explanations together . 5 . 3 Ideation Quality Validation User Study To evaluate ideation quality , we conducted a within - subjects exper - iment with Feedback Condition as independent variable . Similar to [ 26 , 49 ] , we measured quality with three 7 - point Likert scale questions on whether an individual ideation was Motivating , Infor - mative , or Helpful ( Table 3 ) . The ideations evaluated included the third iteration of all ideations for each Feedback Conditions from the Summative Ideation User Study ( N = 43 to 49 ; 278 total ) . 5 . 3 . 1 Experiment Task and Procedure . Participants were tasked to read motivational messages towards physical activity , and rate their quality ( Appendix A . 2 . 2 ) . Participants went through the same procedure as in the Ideation User Study , but with a different Step 3 : 3 . Assess 25 messages regarding how well they motivate for phys - ical activity . For each message , a . Read a random message from one of the 6 Feedback Condi - tions ( N , S , SA , SAC , SAX , SAXC ) . b . Rate on a 7 - point Likert scale , whether the message is moti - vating ( effective ) , informative , and helpful . c . Write their rationale for their quality ratings . This was only asked randomly two times to mitigate response fatigue with texts that are too short or copied from previous responses . 5 . 3 . 2 Experiment Data Collection . We recruited participants from AMT with the same high qualification as the ideation study . 71 work - ers attempted the survey , 60 passed the screening task and complete Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Table 3 : Dependent variables ( DV ) measured in the ideation quality and diversity validation user studies . Measure Of Description and Justification Individual Quality ( Motivating / Informative / Helpful ) Ideation Rating of how the message feels [ 7 - point Likert scale : – 3 = Very Demotivating / Uninformative / Unhelpful , + 3 = Very Motivating / Informative / Helpful ] . Pairwise Dissimilarity Ideation - Pair Ratingofperceiveddifferencebetween2ideations [ 20 - intervalslider : 0 = Identical , 100 = Highlydifferent ] . Figure 12 : Example trial in Ideation Diversity Validation User Study for participants to examine a pair of ideations ( Left ) , rationalize and rate their dissimilarity ( Middle ) , while referring to three reference pairs with varying dissimilarities . the survey ( 84 . 5 % pass rate ) . They were 41 . 7 % female , between 25 and 61 years old ( M = 36 . 0 ) . Participants completed the survey in median time 12 . 1 minutes and were compensated US $ 1 . 50 . In total , 278 messages were rated 1500 times ( M = 5 . 40x / message ) ; 2 omitted by random chance . Average aggregate - judge correlations [ 21 ] for motivation , informativeness , helpfulness ratings were r = . 472 , . 467 , . 469 , respectively , indicating reasonable inter - rater agreement . 5 . 3 . 3 Statistical Analysis and Results . We binarized ratings to eval - uate as continuous variables and fit LMER models on three quality ratings like in the Ideation User Study with Feedback Condition as fixed effect , and Participant and Ideation as random effects ( Table 9 in Appendix A . 3 ) . Ideations were rated as Motivating ( M = 72 . 6 % ) , Informative ( M = 70 . 1 % ) , and Helpful ( M = 75 . 5 % ) , but there were no significant differences across Feedback Conditions ( all p = n . s . ) . 5 . 4 Ideation Diversity Validation User Study The ideation study measured diversity with computational metrics , but people may be less sensitive to perceiving diversity , so it is necessary to evaluate with a user study . Here , we ask a new set of third - party participants to validate ideation diversity from each Feedback Condition . We conducted a mixed - design experiment with Feedback Condition ( 4 levels : N , S , SAX , SAXC ) as independent variable . We omitted SA and SAC , which did not improve compu - tational diversity ( Figure 11 ) and were thus unlikely to increase perceived diversity . Similar to [ 26 , 32 , 78 ] , participants rated the per - ceived dissimilarity between a pair of ideations ( Table 3 ) . From the ideation study , one collection of up to 100 ideations was obtained for each Condition consisting of 50 prior messages ( from [ 26 ] ) and up to 50 new ideations ( 3rd iteration ) from participants . Similar to [ 26 ] , we randomly sampled 200 ideation pairs per condition ; each pair could contain 2 prior , 2 new , or 1 + 1 mixed ideations . 5 . 4 . 1 Experiment Task and Procedure . Participants were tasked to review pairs of motivational messages , and rate their dissimilarity ( Appendix A . 2 . 3 ) . The experiment procedure is as follows : 1 . Introduction to experiment objective and consent to the study . 2 . Screening quiz which needs all correct answers with a ) 4 - item word association test [ 22 ] of English language skill , b ) Similarity judgment test on identifying the most similar ideation pair from 3 pairs ( set to be easy ) . 3 . Practice session with three pair trials . For each trial : a ) Read two ideation messages . b ) Write key concepts for each message as attention check . c ) Rationalize the pair dissimilarity by multiple categories ( iden - tified from a pilot study with 660 rationales from 110 partici - pants ) . This is to prime rigorous study of the messages and mitigate mindless rushing . We designed the question as a matrix of checkboxes ( Figure 12 , Middle Top ) instead of free text , to reduce response fatigue . d ) Rate perceived dissimilarity on a 20 - interval slider from 0 to 100 ( Figure 12 , Middle Bottom ) . We used a high - resolution slider instead of a Likert scale for more precise measurement . e ) View the “correct " dissimilarity rating of 3 practice trials with ratings 1 , 5 , 9 . The rating values were averages from pilot study participants , which we use to calibrate participants towards more consistent ratings . 4 . Main session with 28 message pairs ( 7 pairs per Feedback Con - dition , 4 Feedback Conditions ) . Same procedure as the Practice session , except for omitting Step 3e , and for Step 3d : d ) Rate perceived dissimilarity with the same interface as Step 3d and reference of the 3 practice pairs with " correct " ratings to anchor participants ( Figure 12 , Right ) . 5 . Post - questionnaire on demographics . CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim Figure 13 : Results of validator perceived dissimilarity be - tween ideation pairs from different Feedback Conditions . p values indicate difference from N . * indicates p < . 0001 5 . 4 . 2 Experiment Data Collection . We recruited participants from AMT with the same high qualification as the ideation study . 162 workers attempted the survey and 125 passed screening and com - pleted the survey ( 77 . 2 % pass rate ) . They were 41 . 6 % female , be - tween 20 and 67 years old ( M = 34 . 0 ) . Participants completed the survey in median time 47 . 6 minutes and were compensated US $ 6 . 00 . In total , 800 message pairs were rated 3 , 500 times ( M = 4 . 38x / pair ) . The average aggregate - judge correlation [ 21 ] was r = . 478 for per - ceived dissimilarity , indicating reasonable inter - rater agreement . 5 . 4 . 3 Statistical Analysis and Results . Similar to our analysis in the summative ideation study , we fit a LMER model on perceived dissimilarity with Feedback Condition , Trial Index ( in survey ) , # Ra - tionales Considered ( indicating thoughtfulness ) as fixed effects , and Participant as random effect ( Appendix A . 3 Table 10 ) . Since all Con - ditions contain pairs with only prior messages , we excluded these pairs since they will have identical dissimilarity across Conditions . Ideation message pairs from the SAX and SAXC conditions had higher perceived pairwise dissimilarity than pairs from N ( Figure 13 ) . Pairs from the S condition only had marginally higher ratings than N . This mostly agrees with our results of the computational diversity metrics ( Figure 11 ) . Therefore , Attribution and Contrastive Attribution improved diversity , while Counterfactual Suggestion had limited effect . 5 . 5 Summary of Evaluation Results We summarize our findings from the multiple experiments here and in Table 4 . 1 ) From the formative ideation study , participants wanted some feedback on their ideations , appreciated all XAI types in feedback more than just showing Scores , had a tendency to prior - itize either for Quality or Diversity , might have fixated on prompts or suggestions too much , and sometimes struggled with problem - atic Counterfactual Suggestions . 2 ) From the summative ideation study , all feedback types were rated equally understandable and useful , though Score and Attribution feedback were rated more use - ful than showing Prompts only . 3 ) From the summative ideation and both validation studies , a ) Quality Score increased with all feedback types , but this was not perceptible with third - party validators ; b ) Di - versity Score , computed diversity , and validator perceived pairwise dissimilarity were increased for feedback with combined explana - tions ( SAX , SAXC ) . Therefore , Score , Attribute and Contrastive explanations were especially useful . Counterfactual Suggestions were somewhat detrimental , but this was not significant . 6 DISCUSSION We discuss the value of ideation XAI feedback , future improvements , generalization , and implications to human - AI collaboration . 6 . 1 Effectiveness of XAI Feedback for Ideation We have shown that XAI provides helpful feedback to improve collective creativity in ideation , and multiple explanations should be combined to amplify insights for ideators . Towards improving di - versity , SAX feedback was most effective , followed by SAXC which was similarly effective . Counterfactual suggestions were promising Table 4 : Summary of results of explanation effects . Grey 0 indicates baseline or not significantly different from baseline ( N or Pr . ) , Grey arrow indicates marginally significant from N , black arrow indicates significant at p < . 0001 . Feedback Interface Effect Measure User Study Evidence N S SA SAC SAX SAXC Ideation Quality Quality Score Increase Summative Ideation Fig . 10b 0 ↑ ↑ ↑ ↑ ↑ Motivating / Informative / Helpful ratings Quality Validation - 0 0 0 0 0 0 Ideation Diversity Diversity Score Increase Summative Ideation Fig . 10c 0 0 ↑ ↑ 0 ↑ Computational Ideation Dispersion Fig . 11a 0 0 0 ↓ ↑ ↑ Computational Ideation Disparity Fig . 11b 0 ↑ ↑ 0 ↑ ↑ Computational Repeller Chamfer Distance Fig . 11c 0 0 0 ↑ ↑ ↑ Validator pairwise dissimilarity rating Diversity Validation Fig . 13 0 0 - ↑ - ↑ Ideation Effort Log ( Task Time 1 / Task Time 3 ) Summative Ideation Fig . 10a 0 0 0 ↑ 0 ↑ Feedback Feature Effect Measure User Study Evidence Pr . S ( D ) S ( Q ) A C X Ideation Effort Usefulness rating Summative Ideation Fig . 9a 0 0 ↑ ↑ 0 0 Ease of Understanding rating Fig . 9b 0 0 0 0 0 0 Ease of Use rating Fig . 9c 0 0 0 0 0 0 Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA but harmful ; we discuss improvements later . However , Score feed - back alone was very marginally useful and explanations were less effective at improving quality , and we discuss improvements later . Despite providing helpful explanations , some ideators still strug - gled to apply feedback for ideation . We had suggested the need for semantic explanations rather than correlated attributions ( Section 5 . 1 . 2 ) . To further help , we could leverage strategies in education : providing examples and self - explanation . We could improve the tutorials and provide worked examples [ 7 ] of past ideators deriving new ideations from each explanation feature , particularly including their rationale . Ideators could reflect more rigorously on the feed - back by self - explaining [ 85 ] what they learned from the feedback and how they could or could not apply it to edit their ideation . 6 . 1 . 1 Improving XAI Feedback for Ideation Quality . We had mixed results regarding the impact of Score and XAI feedback on Quality . Feedback did increase the Quality Score in the ideation study , but not the perceived quality in the validation study , perhaps due to several reasons . 1 ) Ideators were confounded with a conflicting goal to increase Diversity . Diversity and Quality Scores were highly negatively correlated , and including either as a factor in the LMER models led to significant effects . Nevertheless , we focused on in - creasing Diversity while maintaining Quality , rather than mainly increasing Quality . For applications prioritizing Quality , ideators could be told to only improve Quality , but at the cost of more re - dundancy . 2 ) Ideators may have felt self - sufficient and not have relied much on XAI to improve Quality . 3 ) The Quality prediction model may need to be more accurate to represent and explain Qual - ity , perhaps with a regression model instead of binary classifier . 4 ) Although we had used the Universal Sentence Encoder ( USE ) [ 20 ] that models whole sentences rather than just words , expla - nations were framed with individual words . This caused ideators to fixate on each word separately rather than longer phrases or gestalt concepts . Presenting the explanation as a knowledge graph , highlighting multiple associated words , or inferring and presenting associated concepts [ 94 ] may mitigate this fixation . Also , instead of using generic language models , the modeling could be domain - specific to motivation and physical activity and include knowledge bases ( e . g . , [ 17 , 70 ] ) . 6 . 1 . 2 Providing More Useful Counterfactual Suggestions . Counter - factual explanations need to be more usable by improving the rele - vance and grammatical context of suggested words . The grammar was due to lemmatizing attribution words to their root and find - ing related words in ConceptNet . The suggested words should be converted to fit the grammatical form of the word being replaced . The irrelevance of suggested words could be due to Concept - Net using domain - independent data sources ( e . g . , Wikipedia , Wik - tionary , WordNet ) [ 81 ] . Words are thus too distant from the domain of physical activity . At the potential cost of stifling creativity by reducing too many suggestions , we had limited the embedding distances of the words ( remedy in Section 5 . 1 . 2 ) , yet the remain - ing words were still insufficiently relevant . Future work should balance irrelevance and inspiration by carefully controlling the distance threshold and training ideators to be more comfortable with less relevant words . We can also leverage recent advances in synthetic data generation and use generative adversarial networks ( GAN ) to synthesize counterfactual sentences that increase quality and diversity ( e . g . , [ 90 ] ) . However , these sentences are likely to be ungrammatical or strange , needing crowdworkers to clean up . Some participantshad deviated fromthe prompt phrase , reducing the precision of the directing . To mitigate this , we can limit the distances of suggested words from the prompts . Reminders can also be sent if the new ideations are measured to be too distant from the prompts by embedding distance . Nevertheless , some deviation can accommodate inaccuracies in the embedding distance calculations . 6 . 2 Generalization to Other Applications , Domains , and Explanations By bridging the gap between the education - based feedback [ 42 ] and philosophy - driven AI explanations [ 60 ] , we proposed and eval - uated an explainable AI - based ideation feedback system to improve quality and diversity in iterative crowd ideation . Our evaluation on one use case of motivational messaging for physical activity may limit the generalization of our results . We had focused on a low - skill task of everyday writing where lay users typically do not need sophisticated reasoning . In domains requiring expertise , ideators may further appreciate complex explanations and may even better leverage simpler explanations ( e . g . , plain attribution ) . For tasks that are non - text - based , such as poster design [ 32 ] , attri - bution may be ambiguous and counterfactuals more complex to develop or describe . Therefore , explanations will need to be more carefully designed for more complex ideation tasks . Our approach can be applied to other applications , such as story writing and graphic design . The short length of motivational mes - sages makes computing on and reviewing them tractable , but pro - cessing longer documents like short stories [ 24 ] will require larger language models [ 29 ] . The long documents could also be divided to be iterated one part at a time [ 11 ] , or summarized automatically [ 38 ] before predicting a score and providing simplified feedback . For graphical ideation tasks like graphic design [ 68 ] or mood boards [ 48 ] , the design artifact can be parsed as an image , scored with a prediction from a Convolutional Neural Network , and explained with saliency maps [ 75 ] to identify problematic regions . For audio ideation tasks like music creation [ 55 ] , Recurrent Neural Networks can be trained and explained with attention mechanisms [ 86 ] . Interpretable Directed Diversity supports three popular explana - tion types , and future work can study others to stimulate creativity . For example , feature visualizations [ 64 ] provide a “vocabulary " of filters to interpret image predictions . For text , this could indicate concepts that ideators could reflect on to generate high scoring ideations ; though these latent concepts may be uninterpretable . Concept Activation Vectors ( CAVs ) [ 46 ] provide explanations with user - chosen concepts . Users could use CAVs like in SMILEY [ 18 ] to discover ideations that are more similar to their desired concept . 6 . 3 Human - AI Collaboration for Creativity Among the myriad techniques to support crowd ideation with human - guided feedback [ 31 , 34 , 68 ] , and automatic feedback [ 8 , 24 ] , we add yet another technique to improve automation and scale . Similar to [ 55 ] , our method directs or steers crowdworkers towards more creative ideas . Can human facilitators then leave the man - agement to AI ? No , we do not argue that crowd and feedback CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim management be handled fully automatically . Instead , data man - agement should be collaborative between the human facilitator and AI . Particularly , there is a need for better curation of source data for prompts and counterfactual suggestions ( e . g . , regularly update corpus [ 26 ] ) . Human support is especially needed for the early phase , since there would be insufficient ideations to train an AI model to predict scores . This presents a causality dilemma that ( ideation ) labeling needs pre - collected , pre - labeled data for training , yet crowdsourcing is needed to label the initial data . 7 CONCLUSION We have proposed Interpretable Directed Diversity to provide feed - back to direct crowd ideators with explainable AI ( XAI ) feedback to iteratively generate more collectively creative ideas . We imple - mented and evaluated automatic scoring models and explanation techniques for Attributions , Contrastive Attributions , and Coun - terfactual Suggestions to improve ideation diversity and quality . Through a series of formative ideation study , summative ideation user study , and validation user studies , with computational lan - guage modeling embedding - based metrics and subjective user rat - ings , we found significantly positive effects of the proposed XAI types on increasing collective ideation diversity , except the Coun - terfactual Suggestions that still require improvements . Our results demonstrate the use of XAI for creativity applications . Hence , In - terpretable Directed Diversity provides a generalizable method for scalable , real - time , and contextualized feedback to improve collec - tive creativity . ACKNOWLEDGMENTS This work was carried out in part at NUS Institute for Health In - novation and Technology ( iHealthtech ) and with funding support from the NUS ODPRT and Ministry of Education , Singapore . REFERENCES [ 1 ] Ashraf Abdul , Jo Vermeulen , Danding Wang , Brian Y Lim , and Mohan Kankan - halli . 2018 . Trends and Trajectories for Explainable , Accountable and Intelligible Systems : AnHCIResearchAgenda . In CHI2018 . https : / / doi . org / 10 . 1145 / 3173574 . 3174156 [ 2 ] Leonard Adelman , James Gualtieri , and Suzanne Stanford . 1995 . Examining the effect of causal focus on the option generation process : An experiment using protocol analysis . Organizational Behavior and Human Decision Processes ( 1995 ) . https : / / doi . org / 10 . 1006 / obhd . 1995 . 1005 [ 3 ] Elena Agapie , Bonnie Chinh , Laura R Pina , Diana Oviedo , Molly C Welsh , Gary Hsieh , and Sean Munson . 2018 . Crowdsourcing Exercise Plans Aligned with Expert Guidelines and Everyday Constraints . In CHI 2018 . ACM , 324 . https : / / doi . org / 10 . 1145 / 3173574 . 3173898 [ 4 ] Faez Ahmed , Sharath Kumar Ramachandran , Mark Fuge , Samuel Hunter , and Scarlett Miller . 2019 . Interpreting idea maps : Pairwise comparisons reveal what makes ideas novel . Journal of Mechanical Design 141 , 2 ( 2019 ) . https : / / doi . org / 10 . 1115 / 1 . 4041856 [ 5 ] T . Amabile . 1996 . Creativity In Context . https : / / doi . org / 10 . 4324 / 9780429501234 [ 6 ] Andrew Anderson , Jonathan Dodge , Amrita Sadarangani , Zoe Juozapaitis , Evan Newman , Jed Irvine , Souti Chattopadhyay , Matthew Olson , Alan Fern , and Mar - garet Burnett . 2020 . Mental Models of Mere Mortals with Explanations of Rein - forcement Learning . ACM Transactions on Interactive Intelligent Systems 10 , 2 ( 2020 ) . https : / / doi . org / 10 . 1145 / 3366485 [ 7 ] Robert K . Atkinson , Sharon J . Derry , Alexander Renkl , and Donald Wortham . 2000 . Learningfromexamples : Instructionalprinciplesfromtheworkedexamples research . Review of Educational Research 70 , 2 ( 2000 ) , 181 – 214 . https : / / doi . org / 10 . 3102 / 00346543070002181 [ 8 ] Suyun Sandra Bae , Oh - hyun Kwon , Senthil Chandrasegaran , and Kwan - liu Ma . 2020 . Spinneret : Aiding Creative Ideation through Non - Obvious Concept Asso - ciations . In CHI 2020 . 1 – 13 . https : / / doi . org / 10 . 1145 / 3313831 . 3376746 [ 9 ] Gagan Bansal , Tongshuang Wu , and Joyce Zhou . 2021 . Does the whole exceed its parts ? The efect of ai explanations on complementary team performance . Conference on Human Factors in Computing Systems - Proceedings ( 2021 ) . https : / / doi . org / 10 . 1145 / 3411764 . 3445717 arXiv : 2006 . 14779 [ 10 ] Baptiste Barbot . 2018 . The dynamics of creative ideation : Introducing a new assessment paradigm . Frontiers in Psychology ( 2018 ) . https : / / doi . org / 10 . 3389 / fpsyg . 2018 . 02529 [ 11 ] Michael S Bernstein , Greg Little , Robert C Miller , Björn Hartmann , Mark S Ackerman , David R Karger , David Crowell , and Katrina Panovich . 2010 . Soylent : A Word Processor with a Crowd Inside . In Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology . ACM , New York , NY , USA , 313 – 322 . https : / / doi . org / 10 . 1145 / 1866029 . 1866078 [ 12 ] Aditya Bharadwaj , Pao Siangliulue , Adam Marcus , and Kurt Luther . 2019 . Crit - ter : Augmenting Creative Work with Dynamic Checklists , Automated Qual - ity Assurance , and Contextual Reviewer Feedback . In CHI ’19 . 1 – 12 . https : / / doi . org / 10 . 1145 / 3290605 . 3300769 [ 13 ] Alexander Binder , Sebastian Bach , Gregoire Montavon , Klaus Robert Müller , and WojciechSamek . 2016 . Layer - wiserelevancepropagationfordeepneuralnetwork architectures . In Information science and applications ( ICISA ) 2016 , Vol . 376 . 913 – 922 . https : / / doi . org / 10 . 1007 / 978 - 981 - 10 - 0557 - 2 _ 87 [ 14 ] Osvald M Bjelland and Robert Chapman Wood . 2008 . An Inside View of IBM’s ’Innovation Jam’ . MIT Sloan management review 50 , 1 ( 2008 ) , 32 . [ 15 ] Margaret A Boden . 1996 . Chapter 9 - Creativity . In Artificial Intelligence , Mar - garet A Boden ( Ed . ) . Academic Press , 267 – 291 . https : / / doi . org / 10 . 1016 / B978 - 012161964 - 0 / 50011 - X [ 16 ] Margaret A Boden . 2004 . The creative mind : Myths and mechanisms . Routledge . [ 17 ] Antoine Bosselut , Hannah Rashkin , Maarten Sap , Chaitanya Malaviya , Asli Ce - likyilmaz , and Yejin Choi . 2020 . CoMET : Commonsense transformers for auto - matic knowledge graph construction . ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics , Proceedings of the Conference ( 2020 ) , 4762 – 4779 . https : / / doi . org / 10 . 18653 / v1 / p19 - 1470 arXiv : 1906 . 05317 [ 18 ] Carrie J . Cai , Emily Reif , Narayan Hegde , Jason Hipp , Been Kim , Daniel Smilkov , Martin Wattenberg , Fernanda Viegas , Greg S . Corrado , Martin C . Stumpe , and Michael Terry . 2019 . Human - centered tools for coping with imperfect algo - rithms during medical decision - making . In Conference on Human Factors in Com - puting Systems - Proceedings . 1 – 14 . https : / / doi . org / 10 . 1145 / 3290605 . 3300234 arXiv : 1902 . 02960 [ 19 ] Paula Phillips Carson and Kerry D . Carson . 1993 . Managing Creativity Enhance - ment Through Goal - Setting and Feeddback . Journal of Creative Behavior 27 , 1 ( 1993 ) , 36 – 45 . https : / / doi . org / 10 . 1002 / j . 2162 - 6057 . 1993 . tb01385 . x [ 20 ] Daniel Cer , Yinfei Yang , Sheng yi Kong , Nan Hua , Nicole Limtiaco , Rhomni St . John , Noah Constant , Mario Guajardo - Céspedes , Steve Yuan , Chris Tar , Yun Hsuan Sung , Brian Strope , and Ray Kurzweil . 2018 . Universal sentence encoder . EMNLP 2018 - Conference on Empirical Methods in Natural Lan - guage Processing : System Demonstrations , Proceedings ( 2018 ) , 169 – 174 . https : / / doi . org / 10 . 18653 / v1 / d18 - 2029 arXiv : arXiv : 1803 . 11175 [ 21 ] Joel Chan , Pao Siangliulue , Denisa Qori McDonald , Ruixue Liu , Reza Moradinezhad , Safa Aman , Erin T Solovey , Krzysztof Z Gajos , and Steven P Dow . 2017 . Semantically far inspirations considered harmful ? accounting for cognitive statesincollaborativeideation . In Proceedingsofthe2017ACMSIGCHIConference on Creativity and Cognition . 93 – 105 . https : / / doi . org / 10 . 1145 / 3059454 . 3059455 [ 22 ] Jesse Chandler , Cheskie Rosenzweig , Aaron J . Moss , Jonathan Robinson , and Leib Litman . 2019 . Online panels in social science research : Expanding sampling methods beyond Mechanical Turk . Behavior Research Methods 51 , 5 ( oct 2019 ) , 2022 – 2038 . https : / / doi . org / 10 . 3758 / s13428 - 019 - 01273 - 7 [ 23 ] RobertB . Cialdini , WilhelminaWosinska , DanielW . Barrett , JonathanButner , and MalgorzataGornik - Durose . 1999 . Compliancewitharequestintwocultures : The differentialinfluenceofsocialproofandcommitment / consistencyoncollectivists and individualists . Personality and Social Psychology Bulletin 25 , 10 ( 1999 ) , 1242 – 1253 . https : / / doi . org / 10 . 1177 / 0146167299258006 [ 24 ] Elizabeth Clark , Anne Spencer Ross , Chenhao Tan , Yangfeng Ji , and Noah A . Smith . 2018 . Creativewritingwithamachineintheloop : Casestudiesonslogans and stories . In International Conference on Intelligent User Interfaces , Proceedings IUI . 329 – 340 . https : / / doi . org / 10 . 1145 / 3172944 . 3172983 [ 25 ] Sven Coppers , Jan Van Den Bergh , Kris Luyten , Karin Coninx , Iulianna Van Der Lek - Ciudin , Tom Vanallemeersch , and Vincent Vandeghinste . 2018 . Intellingo : An intelligible translation environment . In CHI 2018 . 1 – 13 . https : / / doi . org / 10 . 1145 / 3173574 . 3174098 [ 26 ] Samuel Rhys Cox , Yunlong Wang , Ashraf Abdul , Christian Von Der Weth , and Brian Y . Lim . 2021 . Directed diversity : Leveraging language embedding distances for collective creativity in crowd ideation . In CHI’21 . https : / / doi . org / 10 . 1145 / 3411764 . 3445782 [ 27 ] DevleenaDasandSoniaChernova . 2020 . Leveragingrationalestoimprovehuman task performance . In International Conference on Intelligent User Interfaces , Pro - ceedings IUI . 510 – 518 . https : / / doi . org / 10 . 1145 / 3377325 . 3377512 arXiv : 2002 . 04202 [ 28 ] Roelof A J de Vries , Khiet P Truong , Sigrid Kwint , Constance H C Drossaert , and Vanessa Evers . 2016 . Crowd - Designed Motivation : Motivational Messages for Exercise Adherence Based on Behavior Change Theory Roelof . In CHI 2016 . Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA ACM , 297 – 308 . https : / / doi . org / 10 . 1145 / 2858036 . 2858229 [ 29 ] Jacob Devlin , Ming Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 . BERT : Pre - trainingofdeepbidirectionaltransformersforlanguageunderstanding . arXiv preprint 1810 . 04805 ( 2018 ) . arXiv : 1810 . 04805 [ 30 ] Michael R . P . Dougherty , Charles F . Gettys , and Eve E . Ogden . 1999 . MINERVA - DM : A memory processes model for judgments of likelihood . Psychological Review ( 1999 ) . https : / / doi . org / 10 . 1037 / 0033 - 295X . 106 . 1 . 180 [ 31 ] Steven Dow , Anand Kulkarni , Scott Klemmer , and Björn Hartmann . 2012 . Shep - herding the crowd yields better work . In CSCW 2012 . ACM , 1013 – 1022 . https : / / doi . org / 10 . 1145 / 2145204 . 2145355 [ 32 ] Steven P . Dow , Alana Glassco , Jonathan Kass , Melissa Schwarz , Daniel L . Schwartz , and Scott R . Klemmer . 2010 . Parallel prototyping leads to better design results , more divergence , and increased self - efficacy . ACM Transactions on Computer - Human Interaction 17 , 4 ( 2010 ) . https : / / doi . org / 10 . 1145 / 1879831 . 1879836 [ 33 ] Steven P . Dow , Kate Heddleston , and Scott R . Klemmer . 2009 . The efficacy of prototyping under time constraints . In Proceedings of the 2009 ACM SIGCHI ConferenceonCreativityandCognition . 165 – 174 . https : / / doi . org / 10 . 1145 / 1640233 . 1640260 [ 34 ] Alejandra Duque - Estrada . 2014 . Voyant : Generating Structured Feedback on Visual Designs Using a Crowd of Non - Experts . In CSCW 2014 . 1433 – 1444 . https : / / doi . org / 10 . 5377 / cultura . v24i74 . 8893 [ 35 ] Upol Ehsan , Brent Harrison , Larry Chan , and Mark O . Riedl . 2018 . Rationaliza - tion : A Neural Machine Translation Approach to Generating Natural Language Explanations . AIES 2018 - Proceedings of the 2018 AAAI / ACM Conference on AI , Ethics , and Society ( 2018 ) , 81 – 87 . https : / / doi . org / 10 . 1145 / 3278721 . 3278736 arXiv : 1702 . 07826 [ 36 ] Jonas Frich , Lindsay MacDonald Vermeulen , Christian Remy , Michael Mose Biskjaer , andPeterDalsgaard . 2019 . MappingtheLandscapeofCreativitySupport Tools in HCI . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19 . ACM Press , New York , New York , USA , 1 – 18 . https : / / doi . org / 10 . 1145 / 3290605 . 3300619 [ 37 ] Jonas Frich , Michael Mose Biskjaer , and Peter Dalsgaard . 2018 . Twenty Years of Creativity Research in Human - Computer Interaction : Current State and Future Directions . In Proceedings of the 2018 Designing Interactive Systems Conference ( DIS ’18 ) . Association for Computing Machinery , New York , NY , USA , 1235 – 1257 . https : / / doi . org / 10 . 1145 / 3196709 . 3196732 [ 38 ] Mahak Gambhir and Vishal Gupta . 2017 . Recent automatic text summarization techniques : a survey . Artificial Intelligence Review 47 , 1 ( 2017 ) , 1 – 66 . https : / / doi . org / 10 . 1007 / s10462 - 016 - 9475 - 9 [ 39 ] Katy Ilonka Gero and Lydia B Chilton . 2019 . Metaphoria : An Algorithmic Com - panion for Metaphor Creation . In CHI ’19 . 1 – 12 . [ 40 ] Vinod Goel . 2010 . Neural basis of thinking : Laboratory problems versus real - world problems . Wiley Interdisciplinary Reviews : Cognitive Science ( 2010 ) . https : / / doi . org / 10 . 1002 / wcs . 71 [ 41 ] Adam E . Green , David J . M . Kraemer , Jonathan A . Fugelsang , Jeremy R . Gray , and Kevin N . Dunbar . 2012 . Neural correlates of creativity in analogical reasoning . JournalofExperimentalPsychology : LearningMemoryandCognition ( 2012 ) . https : / / doi . org / 10 . 1037 / a0025764 [ 42 ] John Hattie and Helen Timperley . 2007 . The power of feedback . Review of Edu - cational Research 77 , 1 ( 2007 ) , 81 – 112 . https : / / doi . org / 10 . 3102 / 003465430298487 [ 43 ] Simo Hosio , Niels van Berkel , Jonas Oppenlaender , and Joorge Goncalves . 2020 . Crowdsourcing Personalized Weight Loss Diets . Computer January ( 2020 ) , 63 – 71 . https : / / doi . org / 10 . 1109 / MC . 2019 . 2902542 [ 44 ] Gaoping Huang and Alexander J Quinn . 2017 . BlueSky : Crowd - Powered Uniform Sampling of Idea Spaces . In Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition . Association for Computing Machinery , New York , NY , USA , 119 – 130 . https : / / doi . org / 10 . 1145 / 3059454 . 3059481 [ 45 ] L . RobinKellerandJoannaL . Ho . 1988 . DecisionProblemStructuring : Generating Options . IEEE Transactions on Systems , Man and Cybernetics ( 1988 ) . https : / / doi . org / 10 . 1109 / 21 . 21599 [ 46 ] Been Kim , Martin Wattenberg , Justin Gilmer , Carrie Cai , James Wexler , Fernanda Viegas , and Rory Sayres . 2018 . Interpretability beyond feature attribution : Quan - titative Testing with Concept Activation Vectors ( TCAV ) . In 35th International Conference on Machine Learning , ICML 2018 , Vol . 6 . 4186 – 4195 . arXiv : 1711 . 11279 [ 47 ] Ana Cristina Bicharra Klein , Mark , and Garcia . 2015 . High - speed idea filtering with the bag of lemons . Decision Support Systems 78 ( 2015 ) , 39 – 50 . https : / / doi . org / 10 . 1016 / j . dss . 2015 . 06 . 005 [ 48 ] Janin Koch , Nicolas Taffin , Michel Beaudouin - Lafon , Markku Laine , Andrés Lucero , and Wendy E . MacKay . 2020 . ImageSense : An Intelligent Collaborative Ideation Tool to Support Diverse Human - Computer Partnerships . Proceedings of the ACM on Human - Computer Interaction 4 , CSCW1 ( 2020 ) , 1 – 27 . https : / / doi . org / 10 . 1145 / 3392850 [ 49 ] Rafal Kocielnik and Gary Hsieh . 2017 . Send Me a Different Message : Utilizing Cognitive Space to Create Engaging Message Triggers . In CSCW . 2193 – 2207 . https : / / doi . org / 10 . 1145 / 2998181 . 2998324 [ 50 ] Brian Y . Lim and Anind K . Dey . 2011 . Design of an intelligible mobile context - aware application . Mobile HCI 2011 - 13th International Conference on Human - Computer Interaction with Mobile Devices and Services ( 2011 ) , 157 – 166 . https : / / doi . org / 10 . 1145 / 2037373 . 2037399 [ 51 ] Brian Y . Lim and Anind K . Dey . 2013 . Evaluating intelligibility usage and use - fulness in a context - aware application . In International Conference on Human - Computer Interaction . 92 – 101 . https : / / doi . org / 10 . 1007 / 978 - 3 - 642 - 39342 - 6 _ 11 [ 52 ] Brian Y Lim , Anind K Dey , and Daniel Avrahami . 2009 . Why and Why Not Explanations Improve the Intelligibility of Context - Aware Intelligent Systems . In CHI 2009 . 2119 – 2128 . http : / / www . cs . cmu . edu / $ \ sim $ byl / publications / lim _ chi09 . pdf [ 53 ] Edwin A . Locke and Gary P . Latham . 1990 . A theory of goal setting & task performance . Prentice - Hall , Inc . [ 54 ] Edwin a Locke and Gary P Latham . 2002 . Building a practically useful theory of goal setting and task motivation . A 35 - year odyssey . The American psychologist 57 , 9 ( 2002 ) , 705 – 717 . https : / / doi . org / 10 . 1037 / 0003 - 066X . 57 . 9 . 705 [ 55 ] Ryan Louie , Andy Coenen , Cheng Zhi Huang , Michael Terry , and Carrie J Cai . 2020 . Novice - AI Music Co - Creation via AI - Steering Tools for Deep Generative Models . In CHI 2020 . 1 – 13 . https : / / doi . org / 10 . 1145 / 3313831 . 3376739 [ 56 ] Todd I Lubart . 2001 . Models of the creative process : Past , present and future . Creativity research journal 13 , 3 - 4 ( 2001 ) , 295 – 308 . https : / / doi . org / 10 . 1207 / S15326934CRJ1334 _ 07 [ 57 ] ScottMLundbergandSu - InLee . 2017 . AUnifiedApproachtoInterpretingModel Predictions . In 31st Conference on Neural Information Processing Systems ( NIPS 2017 ) . 4768 – 4777 . https : / / doi . org / 10 . 5555 / 3295222 . 3295230 [ 58 ] Kurt Luther , Nathan Hahn , Steven P Dow , and Aniket Kittur . 2015 . Crowdlines : SupportingSynthesisofDiverseInformationSourcesthroughCrowdsourcedOut - lines . In The Third AAAI Conference on Human Computation and Crowdsourcing ( HCOMP - 15 ) . https : / / ojs . aaai . org / index . php / HCOMP / article / view / 13239 [ 59 ] Joke Meheus . 2000 . Analogical Reasoning in Creative Problem Solving Processes : Logico - Philosophical Perspectives . In Metaphor and Analogy in the Sciences . https : / / doi . org / 10 . 1007 / 978 - 94 - 015 - 9442 - 4 _ 2 [ 60 ] Tim Miller . 2019 . Explanation in artificial intelligence : Insights from the social sciences . Artificial Intelligence 267 ( 2019 ) , 1 – 38 . https : / / doi . org / 10 . 1016 / j . artint . 2018 . 07 . 007 [ 61 ] Fabio Del Missier , Mimì Visentini , and Timo Mäntylä . 2015 . Option generation in decision making : Ideation beyond memory retrieval . Frontiers in Psychology ( 2015 ) . https : / / doi . org / 10 . 3389 / fpsyg . 2014 . 01584 [ 62 ] BernardA . NijstadandWolfgangStroebe . 2006 . HowtheGroupAffectstheMind : ACognitiveModelofIdeaGenerationinGroups . PersonalityandSocialPsychology Review 10 , 3 ( aug 2006 ) , 186 – 213 . https : / / doi . org / 10 . 1207 / s15327957pspr1003 _ 1 [ 63 ] Bernard A . Nijstad , Wolfgang Stroebe , and Hein F . M . Lodewijkx . 2002 . Cognitive stimulationandinterferenceingroups : Exposureeffectsinanideagenerationtask . Journal of Experimental Social Psychology ( 2002 ) . https : / / doi . org / 10 . 1016 / S0022 - 1031 ( 02 ) 00500 - 0 [ 64 ] Chris Olah , Alexander Mordvintsev , and Ludwig Schubert . 2017 . Feature Visual - ization . Distill ( 2017 ) . https : / / doi . org / 10 . 23915 / distill . 00007 [ 65 ] Jonas Oppenlaender and Simo Hosio . 2019 . Design Recommendations for Aug - menting Creative Tasks with Computational Priming . In Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia ( MUM ’19 ) . Associ - ation for Computing Machinery , New York , NY , USA . https : / / doi . org / 10 . 1145 / 3365610 . 3365621 [ 66 ] Jonas Oppenlaender and Simo Hosio . 2019 . Supporting Creative Work with Crowd Feedback Systems . In DC2S2 ’19 : Workshop on Designing Crowd - powered Creativity Support Systems . arXiv : arXiv : 2004 . 09204v1 [ 67 ] Jonas Oppenlaender and Elina Kuosmanen . 2021 . Hardhats and bungaloos : Comparing crowdsourced design feedback with peer design feedback in the classroom . CHI ’21 ( 2021 ) . https : / / doi . org / 10 . 1145 / 3411764 . 3445380 [ 68 ] Jonas Oppenlaender , Thanassis Tiropanis , and Simo Hosio . 2020 . CrowdUI : Supporting Web Design with the Crowd . Proc . ACM Hum . - Comput . Interact . 4 , EICS ( 2020 ) . https : / / doi . org / 10 . 1145 / 3394978 [ 69 ] Zhenhui Peng , Qingyu Guo , Ka Wing Tsang , and Xiaojuan Ma . 2020 . Exploring the Effects of Technological Writing Assistance for Support Providers in Online Mental Health Community . In CHI ’20 . 1 – 15 . https : / / doi . org / 10 . 1145 / 3313831 . 3376695 [ 70 ] Fabio Petroni , Tim Rocktäschel , Patrick Lewis , Anton Bakhtin , Yuxiang Wu , Alexander H . Miller , and Sebastian Riedel . 2020 . Language models as knowledge bases ? EMNLP - IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing , Proceedings of the Conference ( 2020 ) , 2463 – 2473 . https : / / doi . org / 10 . 18653 / v1 / d19 - 1250 arXiv : 1909 . 01066 [ 71 ] Arkalgud Ramaprasad . 1983 . On the definition of feedback . Behavioral Science 28 , 1 ( 1983 ) , 4 – 13 . https : / / doi . org / 10 . 1002 / bs . 3830280103 [ 72 ] Marco Tulio Ribeiro , Sameer Singh , and Carlos Guestrin . 2016 . “Why Should I Trust You ? ” Explaining the Predictions of Any Classifier . In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD ’16 . ACM Press , New York , New York , USA , 1135 – 1144 . https : / / doi . org / 10 . 1145 / 2939672 . 2939778 CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim [ 73 ] Matthew Richardson , Amit Prakash , and Eric Brill . 2006 . Beyond PageRank : Machine learning for static ranking . In Proceedings of the 15th International ConferenceonWorldWideWeb . 707 – 715 . https : / / doi . org / 10 . 1145 / 1135777 . 1135881 [ 74 ] C . Riedl , I . Blohm , J . M . Leimeister , and H . Krcmar . 2010 . Rating scales for collective intelligence in innovation communities : Why quick and easy decision makingdoesnotgetitright . In ThirtyFirstInternationalConferenceonInformation Systems . https : / / papers . ssrn . com / sol3 / papers . cfm ? abstract _ id = 1714524 [ 75 ] Ramprasaath R . Selvaraju , Michael Cogswell , Abhishek Das , Ramakrishna Vedan - tam , Devi Parikh , and Dhruv Batra . 2020 . Grad - CAM : Visual Explanations from Deep Networks via Gradient - Based Localization . International Journal of Com - puter Vision 128 , 2 ( 2020 ) , 336 – 359 . https : / / doi . org / 10 . 1007 / s11263 - 019 - 01228 - 7 arXiv : 1610 . 02391 [ 76 ] Pararth Shah , Dilek Hakkani - Tür , Gokhan Tür , Abhinav Rastogi , Ankur Bapna , Neha Nayak , and Larry Heck . 2018 . Building a conversational agent overnight with dialogue self - play . arXiv preprint arXiv : 1801 . 04871 ( 2018 ) . [ 77 ] Ben Shneiderman . 2007 . CREATIVITY SUPPORT TOOLS : Accelerating Discovery and Innovation . Commun . ACM 50 , 12 ( 2007 ) , 20 – 32 . http : / / dl . acm . org / citation . cfm ? id = 1323689 [ 78 ] Pao Siangliulue , Kenneth C Arnold , Krzysztof Z Gajos , and Steven P Dow . 2015 . TowardCollaborativeIdeationatScale - LeveragingIdeasfromOtherstoGenerate More Creative and Diverse Ideas . In CSCW . https : / / doi . org / 10 . 1145 / 2675133 . 2675239 [ 79 ] Pao Siangliulue , Joel Chan , Steven P Dow , and Krzysztof Z Gajos . 2016 . Idea - Hound : Improving Large - scale Collaborative Ideation with Crowd - Powered Real - time Semantic Modeling . In UIST 2016 - Proceedings of the 29th Annual Sym - posium on User Interface Software and Technology ( UIST ’16 ) . ACM , 609 – 624 . https : / / doi . org / 10 . 1145 / 2984511 . 2984578 [ 80 ] Steven M . Smith . 2010 . The Constraining Effects of Initial Ideas . In Group Creativity : Innovation through Collaboration . https : / / doi . org / 10 . 1093 / acprof : oso / 9780195147308 . 003 . 0002 [ 81 ] Robert Speer , Joshua Chin , and Catherine Havasi . 2017 . Conceptnet 5 . 5 : An open multilingual graph of general knowledge . In AAAI ’17 ) . 4444 – 4451 . https : / / doi . org / 10 . 5555 / 3298023 . 3298212 [ 82 ] Mukund Sundararajan , Ankur Taly , and Qiqi Yan . 2017 . Axiomatic attribution for deep networks . In 34th International Conference on Machine Learning , ICML 2017 , Vol . 7 . 5109 – 5118 . https : / / doi . org / 10 . 5555 / 3305890 . 3306024 [ 83 ] Chun Hua Tsai , Yue You , Xinning Gui , Yubo Kou , and John M . Carroll . 2021 . Exploring and promoting diagnostic transparency and explainability in online symptom checkers . In CHI ’21 . https : / / doi . org / 10 . 1145 / 3411764 . 3445101 [ 84 ] Joe Tullio , Anind K . Dey , Jason Chalecki , and James Fogarty . 2007 . How it works : a field study of non - technical users interacting with an intelligent system . In CHI 2007 . 31 – 40 . https : / / doi . org / 10 . 1145 / 1240624 . 1240630 [ 85 ] Kurt Vanlehn , Randolph M Jones , Michelene T H Chi , Kurt Vanlehn , Randolph M Jones , and Michelene T H Chi . 2016 . A Model of the Self - Explanation Ef - fect A Model of the Self - Explanation Effect . Journal of the Learning Sciences 8406 , December ( 2016 ) , 1 – 59 . http : / / www . tandfonline . com / doi / abs / 10 . 1207 / s15327809jls0201 _ 1 [ 86 ] Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N . Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 . Attention Is All You Need . In NIPS 2017 . https : / / doi . org / 10 . 1109 / 2943 . 974352 [ 87 ] Danding Wang , Qian Yang , Ashraf Abdul , and Brian Y Lim . 2019 . Designing Theory - Driven User - Centric Explainable AI . In CHI ’19 . https : / / doi . org / 10 . 1145 / 3290605 . 3300831 [ 88 ] Kai Wang and Jeffrey V . Nickerson . 2017 . A literature review on individual creativity support systems . Computers in Human Behavior 74 ( 2017 ) , 139 – 151 . https : / / doi . org / 10 . 1016 / j . chb . 2017 . 04 . 035 [ 89 ] Xinru Wang and Ming Yin . 2021 . Are Explanations Helpful ? A Comparative Study of the Effects of Explanations in AI - Assisted Decision - Making . In IUI ’21 . 318 – 328 . https : / / doi . org / 10 . 1145 / 3397481 . 3450650 [ 90 ] JingjingXu , XuanchengRen , JunyangLin , andXuSun . 2018 . DP - GAN : Diversity - Promoting Generative Adversarial Network for Generating Informative and Diversified Text . arXiv ( 2018 ) . arXiv : 1802 . 01345 http : / / arxiv . org / abs / 1802 . 01345 [ 91 ] KexinBellaYang , TomohiroNagashima , JunhuiYao , JosephJayWilliams , Kenneth Holstein , and Vincent Aleven . 2021 . Can Crowds Customize Instructional Mate - rials with Minimal Expert Guidance ? Exploring Teacher - guided Crowdsourcing for Improving Hints in an AI - based Tutor . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW1 ( 2021 ) , 1 – 24 . https : / / doi . org / 10 . 1145 / 3449193 [ 92 ] Yu - Chun Grace Yen , Steven P Dow , Elizabeth Gerber , and Brian P Bailey . 2017 . Listen to Others , Listen to Yourself : Combining Feedback Review and Reflection to Improve Iterative Design . In Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition . ACM , New York , NY , USA , 158 – 170 . https : / / doi . org / 10 . 1145 / 3059454 . 3059468 [ 93 ] Lixiu Yu and Jeffrey V . Nickerson . 2011 . Cooks or cobblers ? . In CHI 2011 . 1393 – 1402 . https : / / doi . org / 10 . 1145 / 1978942 . 1979147 [ 94 ] Wencan Zhang and Brian Y . Lim . 2022 . Towards Relatable Explainable AI with the Perceptual Process . In CHI ’22 . Association for Computing Machinery . https : / / doi . org / 10 . 1145 / 3491102 . 3501826 arXiv : 2112 . 14005 Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA A APPENDIX A . 1 Example training data for Quality ( Motivation ) machine learning model Table 5 : Example messages from [ 26 ] with varying motivation ratings . Message Motivation Rating ( – 3 to 3 ) Highly Motivating ? Walking outside in nature can rejuvenate your mind while toning your body at the same time ! 2 . 60 High Push yourself , don’t wreck yourself . Set some steady goals , and work your way up ! 2 . 00 High Even small amounts of exercise can help you become healthy . You don’t need to dedicate 2 hours a day to become healthier ! 1 . 75 High Your dog would love to go on a walk with you each day ! 1 . 00 Low Switch off an air conditioner while working out . Let the sweat out , and burn some calories . 0 . 00 Low Athletes are healthy since they train every day . You will feel much healthier if you exercise too ! – 0 . 17 Low CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim A . 2 Questionnaire Interfaces for User Studies A . 2 . 1 Summative Ideation User Study . Screenshots for survey in the Feedback Condition SAXC , containing all explanation types . Other conditions will have fewer instructions and questions . Figure 14 : Introduction . Figure 15 : Screening questions on word association to assess English language skills . Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Figure 16 : Tutorial on Scores , Attribution and Counterfactual explanations . CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim Figure 17 : The tutorial of introducing the Contrastive explanations ( i . e . , color highlights and attribution to score changes ) between attempts and Counterfactual explanations ( i . e . , suggested related words and potential contribution scores ) . Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Figure 18 : Screening Quiz . CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim Figure 18 : Screening Quiz . ( continued ) Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Figure 18 : Screening Quiz . ( continued ) Figure 19 : Questions to rate priority on Motivation or Creativity . CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim Figure 20 : Questions on perceived helpfulness , ease of understanding , and ease of use regarding Feedback Features . Ellipses ( . . . ) notation indicate the same matrix of radio buttons , used for brevity . Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA A . 2 . 2 Ideation Quality Validation User Study . Figure 21 : Introduction . Figure 22 : Questions to rate qualities of Motivatingness , Informativeness , and Helpfulness . Rationale was collected as attention checks . CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim A . 2 . 3 Ideation Diversity Validation User Study . Figure 23 : Introduction . Figure 24 : Screening quiz questions on message pair dissimilarity perception . Interpretable Directed Diversity : Leveraging Model Explanations for Iterative Crowd Ideation CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA A . 3 Linear Mixed Models and Statistical Analysis Results Table 6 : Statistical analysis for Ideation User Study of perceived ratings due to Feedback Feature as fixed effect and Participant as random effect in linear mixed effects models . n . s . means not significant at 𝑝 > . 05 . 𝑝 > 𝐹 is the significance level of the fixed effect ANOVA . 𝑅 2 is the model’s coefficient of determination to indicate goodness of fit . Response Linear Effects Model ( Participant as random effect ) 𝑝 > 𝐹 𝑅 2 Usefulness Feedback Feature < . 0001 . 487 Ease of use Feedback Feature n . s . . 480 Understandability Feedback Feature < . 0001 . 471 Table 7 : Statistical analysis for Ideation User Study of responses due to Feedback Condition , Prompt , and Score Increases as fixed effects and Participant as random effect in linear mixed effects models . 𝑝 > 𝐹 is the significance level of the fixed effect ANOVA . 𝑅 2 is the model’s coefficient of determination to indicate goodness of fit . Response Linear Effects Model ( Participant as random effect ) 𝑝 > 𝐹 𝑅 2 Ideation Time Effort Feedback Condition + . 0117 . 586 Prompt n . s . Quality Score Increase Feedback Condition + . 0361 . 384 Prompt + n . s . Diversity Score Increase < . 0001 Diversity Score Increase Feedback Condition + . 0361 . 388 Prompt + n . s . Quality Score Increase < . 0001 Table 8 : Statistical analysis for Ideation User Study of computational diversity metrics due to Feedback Condition as fixed effect and Participant as random effect in linear mixed effects models . 𝑝 > 𝐹 is the significance level of the fixed effect ANOVA . 𝑅 2 is the model’s coefficient of determination to indicate goodness of fit . Response Linear Effects Model ( Participant as random effect ) 𝑝 > 𝐹 𝑅 2 Ideation Dispersion ( MST Mean of Edge Weights ) Feedback Condition < . 0001 . 405 Ideation Disparity ( Mean Pairwise Distance ) Feedback Condition < . 0001 . 360 Table 9 : Statistical analysis for Ideation Quality Validation User study of responses due to Feedback Condition as fixed effects and Participant and Ideation as random effects in linear mixed effects models . 𝑝 > 𝐹 is the significance level of the fixed effect ANOVA . 𝑅 2 is the model’s coefficient of determination to indicate goodness of fit . Response Linear Effects Model ( Participant , Ideation as random effects ) 𝑝 > 𝐹 𝑅 2 Motivatingness Rating Feedback Condition n . s . . 292 Informativeness Rating Feedback Condition n . s . . 298 Helpfulness Rating Feedback Condition n . s . . 324 CHI ‘22 , April 29 - May 5 , 2022 , New Orleans , LA , USA Yunlong Wang , Priyadarshini Venkatesh , and Brian Y . Lim Table 10 : Statistical analysis for Ideation Diversity Validation User study of responses due to Feedback Condition , Trial Index ( sequence number of pair trial ) , # Rationales Considered ( how thoughtful participant was ) and an interaction as fixed effects , and Participant as random effect in a linear mixed effects model . 𝑝 > 𝐹 is the significance level of the fixed effect ANOVA . 𝑅 2 is the model’s coefficient of determination to indicate goodness of fit . Response Linear Effects Model ( Participant as random effect ) 𝑝 > 𝐹 𝑅 2 Pairwise Dissimilarity Rating Feedback Condition + < . 0001 . 371 Trial Index + . 0128 # Rationales Considered + n . s . # Rationales Considered × Feedback Condition n . s .