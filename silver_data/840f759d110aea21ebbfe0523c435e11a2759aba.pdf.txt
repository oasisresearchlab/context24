F LORA : Low - Rank Adapters Are Secretly Gradient Compressors Yongchang Hao ♠ yongcha1 @ ualberta . ca Yanshuai Cao ♢ yanshuai . cao @ borealisai . com Lili Mou ♠♣ doublepower . mou @ gmail . com ♠ Alberta Machine Intelligence Institute ( Amii ) Dept . Computing Science , University of Alberta ♢ Borealis AI ♣ Canada CIFAR AI Chair A BSTRACT Despite large neural networks demonstrating remarkable abilities to complete different tasks , they require excessive memory usage to store the optimization states for training . To alleviate this , the low - rank adaptation ( LoRA ) is proposed to reduce the optimization states by training fewer parameters . However , LoRA restricts overall weight update matrices to be low - rank , limiting the model performance . In this work , we investigate the dynamics of LoRA and identify that it can be approximated by a random projection . Based on this observation , we propose F LORA , which is able to achieve high - rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states . We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach . 1 1 Introduction Gradient - based optimization powers the learning part of deep neural networks . In the simplest form , stochastic gradient descent ( SGD ) updates the model parameters using noisy estimation of the negative gradient . More advanced methods track various gradient statistics to stabilize and accelerate training ( Duchi et al . , 2011 ; Hinton et al . , 2012 ) . For example , the momentum technique tracks an exponential moving average of gradients for variance reduction ( Cutkosky and Orabona , 2019 ) and damping ( Goh , 2017 ) . On the other hand , gradient accumulation computes the average of gradients in the last few batches to simulate a larger effective batch for variance reduction ( Wang et al . , 2013 ) . Both cases require an additional memory buffer equal to the model size to store the information . However , such a linear space complexity of optimization states becomes problematic in modern deep learning . For example , GPT - 3 ( Brown et al . , 2020 ) and Stable Diffusion ( Rombach et al . , 2022 ) are trained with Adam ( Kingma and Ba , 2015 ) where momentum is applied . For each scalar in the parameter set , Adam maintains two additional variables ( i . e . , first - and second - moment estimates ) , tripling the memory usage . The largest GPT - 3 , for example , has 175 billion parameters taking 700GB of memory . Adam requires an additional 1 . 4TB memory for optimization states . This excessive amount of memory usage poses a scaling challenge . One line of research saves memory by training a subset of parameters ( Houlsby et al . , 2019 ; Zaken et al . , 2022 ) , so the optimizer only stores information about a small set of trainable parameters . One notable example is the low - rank adaptation ( LoRA , Hu et al . , 2022 ) . LoRA updates parameter matrices by low - rank patches , which contain much fewer trainable parameters . In this way , the momentum and gradient accumulation also have much smaller sizes . However , LoRA restricts the weight update to be in the low - rank form , limiting the optimization space of the model parameters . 1 Our code is publicly available at https : / / github . com / MANGA - UOFA / Flora . a r X i v : 2402 . 03293v1 [ c s . L G ] 5 F e b 2024 Another line of work designs new optimizers that use less memory ( Dettmers et al . , 2021 ; Feinberg et al . , 2023 ) . For instance , Adafactor ( Shazeer and Stern , 2018 ) leverages the closed - form solution of generalized Kullback – Leibler divergence ( Finesso and Spreij , 2006 ) to reconstruct the second - moment estimate in Adam . To optimize a matrix in R n × m , Adafactor reduces the memory from O ( nm ) to O ( n + m ) , making the space complexity of second - moment estimation sublinear in model size . However , Adafactor drops the momentum technique to achieve the sublinearity , sacrificing the variance reduction and damping effect of momentum ( Rae et al . , 2021 ) . Moreover , it does not reduce the memory for gradient accumulation . In this work , we propose F LORA ( f rom L oRA t o high - r ank upd a tes ) , which is a novel optimization technique that uses sublinear memory for gradient accumulation and momentum calculation . Our intuition arises from investigating LoRA and observing that a LoRA update is dominated by a random projection , which compresses the gradient into a lower - dimensional space ( Dasgupta , 2000 ; Bingham and Mannila , 2001 ) . Thus , we propose F LORA that applies such a compression technique directly to the update of the original weight matrix . Our F LORA resamples the random projection and is able to mitigate the low - rank limitation of LoRA . Further , our approach only stores the compressed gradient accumulation and momentum , thus saving the memory usage of optimization states to the sublinear level . We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach . When combined with Adafactor as a base optimizer , our approach yields similar performance to uncompressed , full - matrix update , while largely outperforming other compression techniques such as LoRA . Interestingly , the space complexity of F LORA is in the same order as LoRA but has a smaller constant in practice , leading to less memory usage than LoRA . 2 Approach In this section , we first present our observation of the dynamics of LoRA updates ( § 2 . 1 ) . Then , we show that LoRA can be approximated by random projection ( § 2 . 2 ) , which serves as gradient compression ( § 2 . 3 ) and can be used for sublinear - space gradient accumulation and momentum calculation ( § 2 . 4 ) . 2 . 1 Dynamics of low - rank adaptation ( LoRA ) For update a pre - trained weight matrix W ∈ R n × m , LoRA parameterizes B ∈ R n × r and A ∈ R r × m with r ≪ min { n , m } . After applying LoRA , the forward pass becomes y = ( W + BA ) x = Wx + BAx , ( 1 ) where x ∈ R m is the input for current layer and y ∈ R n is the pre - activation value of the next layer . At the beginning of LoRA updates , BA should not change the original weight W . The common practice is to initialize the matrix B with an all - zero matrix and A with a normal distribution . During back - propagation , the matrix W has gradient ∇ W L = ∂ L ∂ y x ⊤ , ( 2 ) where ∂ L ∂ y ∈ R n is the partial derivative w . r . t . y . LoRA only calculates the gradient w . r . t . the matrices A and B , given by ∂ L ∂ A = B ⊤ ∂ L ∂ y x ⊤ = B ⊤ ( ∇ W L ) ( 3 ) and ∂ L ∂ B = ∂ L ∂ y x ⊤ A ⊤ = ( ∇ W L ) A ⊤ . ( 4 ) Our insight is that in Equations ( 3 ) and ( 4 ) , LoRA essentially down - projects the original gradient to a lower dimension . In fact , we found that LoRA recovers the well - known random projection method ( Dasgupta , 2000 ; Bingham and Mannila , 2001 ) . We formally state this in the following theorem . Theorem 2 . 1 . Let LoRA update matrices A and B with SGD for every step t by A t + 1 ← A t − η B ⊤ t ( ∇ W L t ) ( 5 ) B t + 1 ← B t − η ( ∇ W L t ) A ⊤ t . ( 6 ) 2 We assume (cid:13)(cid:13) ∑ Tt = 0 ∇ W L t (cid:13)(cid:13) F ≤ L for every T during training , which implies that the model stays within a finite Euclidean ball . In this case , the dynamics of A t and B t are given by A T = A 0 + η A 0 f A ( T ) , B T = η f B ( T ) A ⊤ 0 , ( 7 ) where the forms of f A ( t ) ∈ R m × m and f B ( t ) ∈ R n × m are expressed in the proof . In particular , ∥ f A ( t ) ∥ 2 ≤ η L 2 (cid:0) 1 − ( η 2 L 2 ) t (cid:1) 1 − η 2 L 2 for every t . Proof . See Appendix A . Theorem 2 . 1 describes the SGD dynamics of LoRA updates . Without loss of generality , we denote the total change of A and B after T step as ∆ A and ∆ B , respectively . Then the fine - tuned forward function will be W + ( B 0 + ∆ B ) ( A 0 + ∆ A ) = W + B 0 A 0 + B 0 ∆ A + ∆ BA 0 + ∆ B ∆ A ( 8 ) = W + ∆ BA 0 + ∆ B ∆ A , ( 9 ) where B 0 = 0 is due to the initialization of the B matrix . The final expression dissects the LoRA weight into two parts . We observe that it is the first part that dominates the total weight change , as stated below . Observation 2 . 2 . When the learning rate is small , we have an approximation that W + ( B 0 + ∆ B ) ( A 0 + ∆ A ) ≈ W + ∆ BA 0 . ( 10 ) This can be seen by expanding B 0 and A 0 given by Theorem 2 . 1 . Specifically , we have that W + ∆ BA 0 + ∆ B ∆ A = W + η f B ( t ) A ⊤ 0 A 0 + η 2 f B ( t ) A ⊤ 0 A 0 f A ( t ) ( 11 ) Our insight is that the third term has a smaller magnitude when the learning rate is not large . This is because ∥ f A ( t ) ∥ 2 ≤ ∥ f A ( t ) ∥ F ≤ η L 2 (cid:0) 1 − ( η 2 L 2 ) t (cid:1) 1 − η 2 L 2 given by Theorem 2 . 1 . If η ≪ 1 / L , we have lim t → ∞ η ∥ f A ( t ) ∥ ≪ 1 , which indicates that the third term is significantly smaller than the second term , making it negligible in the final updates . 2 . 2 Random projection of gradients By Observation 2 . 2 , the change of the matrix B dominates the final weight . A straightforward simplification is to freeze the matrix A but to tune the matrix B only ( denoted by ∆ ˜ B ) . In this case , we have W + ( B 0 + ∆ B ) ( A 0 + ∆ A ) ≈ W + ∆ ˜ BA 0 ( 12 ) = : W + η ˜ f B ( T ) A ⊤ 0 A 0 . ( 13 ) In Equation ( 12 ) , B 0 is dropped because B is initialized as an all - zero matrix . Equation ( 13 ) defines ˜ f B ( T ) , which will have the update form ˜ f B ( t + 1 ) : = ˜ f B ( t ) − ∇ W L t ( 14 ) following the derivations in Theorem 2 . 1 . Therefore , ˜ f B ( t ) = − ∑ i ∇ W L i . Putting it to Equation ( 13 ) , we have W + η ˜ f B ( T ) A ⊤ 0 A 0 = W − η (cid:32) T ∑ t = 0 ∇ W L t (cid:33) A ⊤ 0 A 0 = W − η T ∑ t = 0 (cid:104) ( ∇ W L t ) A ⊤ 0 A 0 (cid:105) . ( 15 ) In other words , our derivation reveals that , with some approximations , LoRA updates can be viewed as performing random projection to the gradient . In particular , it compresses a gradient by a random down - projection A ⊤ 0 , and then decompresses it by an up - projection A 0 . 2 . 3 Our interpretation of LoRA To this end , we provide a novel interpretation of a LoRA update by framing it as the compression and decompression of gradients . 3 Figure 1 : The results of LoRA and its simplifications . We apply the LoRA patch to the first layer of the network with a shape of 768 × 768 and set r = 8 . The legend LoRA is the original LoRA method , while LoRA ( B ) is the simplification where only the matrix B is updated . RP ( random projection ) and RRP ( resampled RP ) follow the same update rule ( 15 ) , but RRP uses different projection matrices at different steps . In addition , we show the results of SGD on the full model for comparison . All experiments use the same η = 0 . 01 . Compression . LoRA first compresses the gradient by a random down - projection , which can be justified by the following result based on the Johnson – Lindenstrauss lemma ( Dasgupta and Gupta , 2003 ; Matouˇsek , 2008 ) . Lemma 2 . 3 ( Indyk and Motwani ) . Let ϵ ∈ ( 0 , 1 / 2 ] and δ ∈ ( 0 , 1 ) . Let A ∈ R r × m be a random matrix where each element is independently sampled from a standard Gaussian distribution . There exists a constant c such that when r = c ϵ − 2 log ( δ / 2 ) , we have ( 1 − ϵ ) ∥ x ∥ ≤ ( 1 / √ r ) ∥ Ax ∥ ≤ ( 1 + ϵ ) ∥ x ∥ ( 16 ) with probability at least 1 − δ for every x ∈ R m . Essentially , this lemma suggests that , with a high probability , the projection by a random Gaussian matrix largely preserves the scaled norm in the original space . In the case of LoRA , such a random projection is applied to each row of the gradient matrices , whose dimension is thus reduced from R n × m to R r × m . The lemma asserts that the norm structure of the rows is approximately preserved . Decompression . After down - projection by A ⊤ 0 , LoRA decomposes the gradient by an up - projection A 0 . We show that this will recover the original gradient in expectation : E A 0 (cid:104) W + ( ∇ W L t ) A ⊤ 0 A 0 (cid:105) = W + ( ∇ W L t ) E A 0 [ A ⊤ 0 A 0 ] ( 17 ) where ( 1 / r ) E A 0 [ A ⊤ 0 A 0 ] is an identity matrix . Moreover , the larger the rank r , the closer the expectation is to the identity . We quantify the error in our following theorem . Theorem 2 . 4 . Let A be a matrix of shape R r × m where each element is independently sampled from a standard Gaussian distribution . Let ϵ , δ ∈ ( 0 , 1 ] . There exists a constant c such that when r = c log ( 2 m / δ ) ϵ − 2 , we have for all i , j that (cid:12)(cid:12)(cid:12) [ A ⊤ A − I ] i , j (cid:12)(cid:12)(cid:12) ≤ ϵ ( 18 ) with confidence at least 1 − δ . 4 Proof . See Appendix B . Our Theorem 2 . 4 implies that r only needs to scale logarithmically to preserve the element - wise reconstruction error , which is efficient in both computation and memory . Further , the logarithmic asymptotic rate makes it an ideal candidate to be applied to the training of modern neural models where m is large . We empirically verify our interpretation of LoRA by a pilot study on the Fashion - MNIST dataset ( Xiao et al . , 2017 ) with a simple feed - forward network . We experiment with a variant of LoRA where only B is tuned ; we call the variant LoRA ( B ) . As shown in Figure 1 , the performance of LoRA ( B ) is close to the original LoRA , which is consistent with Observation 2 . 2 and suggests the overall update of LoRA is dominated by the compression - and - decompression step . Further , the curve is identical to random projection ( RP ) , well aligned with our derivation in Section 2 . 2 . 2 . 4 Our method : F LORA Based on the analyses , we propose our method , F LORA ( f rom L oRA t o high - r ank upd a tes ) , to enable overall high - rank updates . One of the main insights of F LORA is that it constantly resamples the projection matrix in Equation ( 15 ) . Therefore , our total weight change is no longer constrained to be low - rank . Moreover , we can apply the random - projection compression to the optimization states for memory saving . We demonstrate two common scenarios where F LORA can be applied : ( 1 ) an arithmetic mean ( AM ) over a period of history , for which a concrete example is gradient accumulation ; ( 2 ) an exponential moving average ( EMA ) , for which an example could be momentum calculation . We show that compression of F LORA has the same asymptotic rate as LoRA but with a lower constant . Resampling random projection matrices . With the approximation in Observation 2 . 2 , LoRA can be viewed as having a fixed random projection matrix A 0 . This restricts the overall change of W to be low - rank . However , our analysis in Section 2 . 3 holds for any random matrix at every time step . Therefore , we propose in F LORA to resample a new random matrix to avoid the total change restricted in a low - rank subspace . In our pilot study , resampling the random matrix ( RRP ) largely recovers the performance of full - matrix SGD , significantly surpassing both the original LoRA and its approximated version in Equation ( 15 ) . The empirical evidence highlights the effectiveness of avoiding the low - rank constraint in our F LORA . It should be emphasized that we cannot resample the down - projection matrix A in LoRA . This is because A and B are coupled during the updates , and if the down - projection matrix A is resampled , the already updated matrix B will not fit . On the other hand , our F LORA directly updates the weight matrix W during training , making it flexible to choose a random down - projection at every step . Sublinear memory gradient accumulation . One application of our F LORA is to compress the optimization states to save memory during training . We first show this with an example of gradient accumulation , which is widely used in practice to simulate a larger batch size ( Smith et al . , 2018 ) . Specifically , it calculates the arithmetic mean ( AM ) of gradients for τ steps and updates the model by the AM . In this way , the effective batch size is τ times larger than the original batch size . However , it requires a memory buffer , whose size is equal to the model itself , to store the accumulated gradients . In F LORA , we propose to compress the gradient accumulation with the down - projection . Within an accumu - lation cycle , we only maintain the accumulated gradient in the randomly down - projected space . During decompression , we can reuse the memory for gradients to reduce additional overheads . The resampling of the projection matrix occurs when an accumulation cycle is finished . The overall algorithm is summarized in Algorithm 1 . Sublinear memory momentum . The momentum technique is widely used in modern deep learning to reduce the variance of the gradient and accelerate training ( Nesterov , 1998 ; Goh , 2017 ; Jelassi and Li , 2022 ) . However , it requires maintaining an additional momentum scalar for each parameter , which is expensive when the model is large . Similar to the compressed gradient accumulation , we can also compress the momentum with F LORA . For each time step t , we down - project the new gradient G t by a random projection matrix A ⊤ t . However , the difficulty for accumulating momentum emerges when we use a A t different from A t − 1 , as we cannot reconstruct the original momentum from G t − 1 A ⊤ t − 1 + G t A ⊤ t . This difficulty applies to all EMA updates 5 Algorithm 1 Gradient accumulation with F LORA . Require : rank r ∈ Z + Require : accumulating steps τ ∈ Z + Require : gradient function ∇ W f ( · ; · ) Require : weight matrices W = (cid:110) W ( l ) : dim ( W ( l ) ) = 2 (cid:111) ▷ Initialization of the accumulator state 1 : for W ∈ W do 2 : C W ← 0 n × r ▷ O ( nr ) 3 : s W ← an independent random seed 4 : end for ▷ Accumulating the compressed gradients 5 : for i ∈ [ τ ] do 6 : for W ∈ W do 7 : G W ← ∇ W f i ( W ) ▷ G W ∈ R n × m 8 : A W ← N s W ( 0 , 1 / r ) ▷ A W ∈ R r × m 9 : C W ← C W + G W A ⊤ W ▷ Compresssion 10 : end for 11 : end for ▷ Reconstruction 12 : for W ∈ W do 13 : A W ← N s W ( 0 , 1 / r ) ▷ A W ∈ R r × m 14 : ˜ G W ← ( 1 / n ) D W A W ▷ Decompression 15 : end for 16 : return { ˜ G W : W ∈ W } ▷ Overwrite { G W } Algorithm 2 Momentum with F LORA . Require : decay rates 0 ≤ β ≤ 1 Require : rank r ∈ Z + , interval κ ∈ Z + Require : gradient function ∇ W f ( · ; · ) Require : weight W = (cid:110) W ( l ) : dim ( W ( l ) ) = 2 (cid:111) ▷ Initialize the optimizer state 1 : t ← 0 2 : for W ∈ W do 3 : M t , W ← 0 n × r 4 : s t , W ← an independent random seed 5 : end for ▷ Training procedure 6 : while training not converged do 7 : for W ∈ W do 8 : G t , W ← ∇ W f t ( W ) ▷ G t , W ∈ R n × m 9 : A t , W ∼ N s t , W ( 0 , 1 / r ) ▷ A t , W ∈ R r × m 10 : if t ≡ 0 ( mod κ ) then 11 : s t + 1 , W ← an independent random seed 12 : A ′ W ∼ N s t + 1 , W ( 0 , 1 / r ) ▷ A ′ W ∈ R r × m 13 : M ′ ← M t , W A t , W A ⊤ t + 1 , W 14 : else 15 : s t + 1 , W ← s t , W 16 : A ′ W ← A t , W 17 : M ′ ← M t , W 18 : end if 19 : M t + 1 , W ← β M ′ + ( 1 − β ) G t , W A ′ W ⊤ 20 : end for 21 : yield { M t + 1 , W A ′ W : W ∈ W } ▷ Decompression 22 : t ← t + 1 23 : end while where the number of accumulation steps is not finite . In this case , resampling new matrices will result in a loss of historical accumulation . We propose two remedies to address this issue . First , we keep the same projection matrix for a long time to reduce the distortion . Second , we propose to transfer the compressed momentum from the old projection to a new one by M t = M t − 1 A t − 1 A ⊤ t . This is justified by A ⊤ t − 1 A t − 1 and A ⊤ t A t are approximately the identity matrix based on Theorem 2 . 4 . The final algorithm is shown in Algorithm 2 . Overall , for each weight matrix W ∈ R n × m , we preserve the momentum term M t with sublinear memory . Compared with the original momentum , we reduce the memory from O ( nm ) to O ( nr ) . It should be pointed out that although we track momentum specifically in this case , our algorithm can be easily extended to other EMA - based statistics . Memory analysis . It should be pointed out that neither LoRA nor our F LORA saves the memory for back - propagation . This is because ∂ L ∂ W is needed for the update of A and B in LoRA ( Dettmers et al . , 2023 ) , while in our approach we also compress and decompress the gradient . That being said , saving the memory of optimization states alone could be critical to training large mod - els ( Dettmers et al . , 2021 ) . Our F LORA compresses both the AM and EMA of gradients to the sublinear level , which shares the same asymptotic rate as LoRA . In implementation , we may store the random seed that generates the projection matrix—which is highly efficient , as each element can be sampled independently with simple operations—instead of maintaining the same project matrix over batches . This allows the program to further save memory in practice with buffer reuse . On the contrary , LoRA needs to maintain two weight matrices A and B , as well as their AM or EMA matrices . Empirically shown in Section 3 , F LORA consumes less memory than LoRA while facilitating high - rank updates and outperforming LoRA to a large extent . 6 Table 1 : The results of different methods to compress gradient accumulation . The size indicates the total number of the original model parameters . The numbers in the brackets denote the rank r of the random projection matrix . ( a ) The results of T5 variants on XSum . Size Accumulation Mem ∆ M R 1 / R 2 / R L 60M None 0 . 75 - 33 . 4 / 11 . 4 / 26 . 4 Naive 0 . 87 0 . 12 34 . 0 / 11 . 5 / 26 . 7 LoRA ( 8 ) 0 . 82 0 . 07 30 . 4 / 8 . 60 / 23 . 6 LoRA ( 32 ) 0 . 86 0 . 11 30 . 7 / 8 . 90 / 23 . 9 LoRA ( 128 ) 0 . 94 0 . 19 31 . 0 / 9 . 10 / 24 . 1 LoRA ( 256 ) 1 . 07 0 . 32 31 . 4 / 9 . 34 / 24 . 5 F LORA ( 8 ) 0 . 75 0 . 00 31 . 5 / 9 . 67 / 24 . 6 F LORA ( 32 ) 0 . 75 0 . 00 32 . 2 / 10 . 3 / 25 . 2 F LORA ( 128 ) 0 . 77 0 . 02 33 . 2 / 10 . 9 / 26 . 0 F LORA ( 256 ) 0 . 79 0 . 04 33 . 6 / 11 . 3 / 26 . 5 3B None 16 . 7 - 42 . 5 / 19 . 1 / 34 . 6 Naive 26 . 6 9 . 9 44 . 4 / 20 . 9 / 36 . 3 LoRA ( 16 ) 27 . 8 11 . 1 42 . 2 / 18 . 4 / 34 . 0 LoRA ( 64 ) 29 . 5 12 . 8 42 . 3 / 18 . 6 / 34 . 1 LoRA ( 256 ) 33 . 4 16 . 7 42 . 6 / 18 . 9 / 34 . 4 LoRA ( 512 ) OOM - - F LORA ( 16 ) 17 . 0 0 . 3 43 . 5 / 20 . 0 / 35 . 5 F LORA ( 64 ) 18 . 2 1 . 5 43 . 9 / 20 . 3 / 35 . 8 F LORA ( 256 ) 19 . 5 2 . 8 44 . 3 / 20 . 7 / 36 . 2 F LORA ( 512 ) 22 . 1 5 . 4 44 . 5 / 20 . 9 / 36 . 4 ( b ) The results of GPT - 2 variants on IWSLT17 De - En . Size Accumulation Mem ∆ M BLEU 110M None 2 . 77 - 17 . 9 Naive 3 . 24 0 . 47 24 . 9 LoRA ( 8 ) 3 . 25 0 . 48 9 . 94 LoRA ( 32 ) 3 . 29 0 . 52 11 . 2 LoRA ( 128 ) 3 . 38 0 . 60 12 . 2 LoRA ( 256 ) 3 . 52 0 . 75 13 . 4 F LORA ( 8 ) 2 . 93 0 . 15 16 . 3 F LORA ( 32 ) 2 . 94 0 . 16 22 . 0 F LORA ( 128 ) 2 . 98 0 . 20 24 . 0 F LORA ( 256 ) 3 . 03 0 . 26 25 . 4 1 . 5B None 20 . 8 - 28 . 2 Naive 26 . 5 5 . 78 33 . 2 LoRA ( 16 ) 26 . 8 6 . 02 17 . 4 LoRA ( 64 ) 27 . 4 6 . 68 19 . 5 LoRA ( 256 ) 28 . 9 8 . 15 20 . 7 LoRA ( 512 ) OOM - - F LORA ( 16 ) 21 . 1 0 . 34 29 . 7 F LORA ( 64 ) 21 . 3 0 . 52 31 . 6 F LORA ( 256 ) 21 . 9 1 . 17 33 . 2 F LORA ( 512 ) 22 . 8 2 . 04 33 . 6 3 Experiments In this section , we empirically verify the effectiveness of our approach across different model architectures and datasets . 3 . 1 Experiment setup Models . Given the exceptional ability of language models , we consider Transformer - based models for our experiments . Specifically , we select two representative models , including the T5 ( Raffel et al . , 2020 ) and GPT - 2 ( Radford et al . , 2019 ) series . For the T5 series , we use T5 - small to represent small models and T5 - 3B to represent large models . T5 - small has around 60M parameters , with a hidden dimension set to 512 , while T5 - 3B has around 3B parameters with a hidden dimension of 1024 . For the GPT - 2 series , we use the base version to represent small models and GPT - 2 - XL to represent large models . The base version has around 110M parameters , with a hidden dimension set to 768 , while the large version has around 1 . 5B parameters with a hidden dimension of 1600 . Datasets . To facilitate evaluation , we use two conditional language modeling tasks , including summariza - tion and translation . For the summarization task , we train 2 T5 on the XSum dataset ( Narayan et al . , 2018 ) . Each sample is a news article with a summary . The task is to generate a summary of the article . For each input , we prepend the prefix “summarize : ” to the source sentence in the encoder ( Raffel et al . , 2020 ) . The source and target sentences are truncated to 512 and 128 tokens , respectively . For the translation task , we follow the setting of Lin et al . ( 2020 ) and train GPT - 2 on the IWSLT - 2017 German - English dataset ( Cettolo et al . , 2017 ) . Each sample is a German sentence with its English translation . The task is to generate the English translation of the German sentence . For each input , we use the template “translate German to English : [ source ] . English : [ target ] ” for training ( Raffel et al . , 2020 ) . 2 In our experiments , we fine - tune a pre - trained model in the gradient accumulation experiment , while training from scratch in the momentum experiment ( Section 3 . 2 ) . 7 Evaluation metrics . For the summarization task , we use the widely used ROUGE scores ( Lin , 2004 ) , including ROUGE - 1 , ROUGE - 2 , and ROUGE - L ( R 1 / R 2 / R L ) to evaluate the quality of the generated summary . For the translation task , we use the most commonly used SacreBLEU score ( Post , 2018 ) to evaluate the translation quality . For both ROUGE and SacreBLEU scores , the higher the score , the better the quality of the generated text . To get more insights into the training process , we monitor the peak memory usage with the built - in JAX profiling tool ( Bradbury et al . , 2018 ) . We also show the excessive memory ∆ M compared with the method where accumulation or momentum is disabled . The memory is reported in GiB ( 1024 3 bytes ) . Competing methods . In our experiment , we take Adafactor as the base optimizer , which is the default optimizer for many Transformer models including T5 ( Raffel et al . , 2020 ) , and is reported to be empirically better than Adam ( Rae et al . , 2021 ) . We use the official Adafactor implementation in Optax ( DeepMind et al . , 2020 ) . We compare the following methods : ( 1 ) None : a baseline that does not use gradient accumulation or momentum ; ( 2 ) Naive : a naive implementation of gradient accumulation or momentum , which stores the full information along training ; ( 3 ) LoRA : the original LoRA method where only the LoRA patches are trained ; ( 4 ) F LORA : our approach that compresses the gradients and decompresses them when updating the original weights . For LoRA and F LORA , we apply the projections to attention and feed - forward layers only , while following the naive procedure for other layers ( i . e . , token embeddings and vector weights ) . For small models ( T5 - small and GPT - 2 base ) , we test the rank r from 8 to 256 , ranging from the very low dimension to half of the hidden dimension , for a thorough examination of different methods . For large models ( T5 - 3B and GPT - 2 - XL ) , we test r from 16 to 512 to approximately maintain the same percentage of memory saving as small models . We do not apply learning rate schedules ( Loshchilov and Hutter , 2017 ) or weight decay ( Loshchilov and Hutter , 2019 ) in any experiments to rule out the influence of these techniques . 3 . 2 Main results Gradient accumulation . In this setting , we fine - tune pre - trained models with 16 gradient accumulation steps . To achieve a minimal memory footprint and fit large models , the physical batch size is set to 1 . We sweep the learning rate from 10 − 5 to 10 − 1 with the naive accumulation method on the validation loss . The best learning rate is applied to other methods excluding LoRA , which is tuned individually as it is reported to have different optimal learning rates ( Hu et al . , 2022 ) . For each run , the model is fine - tuned for 1 epoch to prevent over - fitting following the common practice ( Wu et al . , 2021 ) . The results are reported on the test set based on the checkpoint with the lowest validation loss . We present the results in Table 1 . As shown , the naive gradient accumulation improves the ROUGE scores over the method without accumulation , but it leads to a large memory usage , which is similar to the model size , to store the accumulation . For LoRA , we empirically observe that it generally does not reduce memory usage in this case as the state of Adafactor is already sublinear . In fact , it increases memory because it stores another four low - rank matrices for each weight matrix and adds an additional Jacobian path for the automatic differentiation . On the contrary , our F LORA reduces the memory footprint on all benchmarks compared with the naive accumulation . In addition , when r is reasonably large , our method is able to recover the performance ( in ROUGE or BLEU scores ) of full - matrix accumulation and surpass the baseline that accumulation is not enabled . Notably , for the large models ( T5 - 3B and GPT - 2 - XL ) , the memory overhead of F LORA ( r = 256 ) is only 30 % of the naive accumulation , while the performance is on par . Momentum . Given that the momentum technique is ineffective in fine - tuning ( Li et al . , 2020 ) , we train all models from scratch in this setting . The physical batch size is set to 4 as a result of balancing the generalization and variance reduction ( Masters and Luschi , 2018 ) . We disable the gradient accumulation technique to rule out its impact . Due to the expense of training from scratch , we only test the small variants of each series . Similar to the settings in gradient accumulation , we sweep the learning rate for the naive momentum method from 10 − 5 to 10 − 1 on the validation loss . The best learning rates are applied to all methods excluding LoRA , which again has its own optimal learning rate . The hyper - parameter κ ( resampling interval ) is set to 1000 for all runs of F LORA . The effect of different values of κ is shown in Section 3 . 3 . 8 Table 2 : The results of compressing momentum . The size indicates the total number of the original model parameters . The numbers in the brackets denote the rank r of the random projection matrix . Setting Momentum Mem R 1 / R 2 / R L T5 60M XSum None 1 . 65 29 . 4 / 9 . 11 / 23 . 3 Naive 1 . 89 29 . 9 / 9 . 40 / 23 . 8 LoRA ( 8 ) 1 . 88 18 . 0 / 3 . 33 / 14 . 9 LoRA ( 32 ) 1 . 91 20 . 4 / 4 . 20 / 16 . 7 / LoRA ( 128 ) 2 . 05 21 . 5 / 4 . 82 / 17 . 4 LoRA ( 256 ) 2 . 13 22 . 2 / 5 . 04 / 17 . 9 F LORA ( 8 ) 1 . 71 25 . 5 / 6 . 56 / 20 . 4 F LORA ( 32 ) 1 . 72 26 . 9 / 7 . 32 / 21 . 5 F LORA ( 128 ) 1 . 75 29 . 1 / 8 . 76 / 23 . 2 F LORA ( 256 ) 1 . 79 30 . 2 / 9 . 51 / 24 . 0 Setting Momentum Mem BLEU GPT - 2 110M IWSLT17 None 8 . 95 19 . 4 Naive 9 . 45 19 . 9 LoRA ( 8 ) 9 . 42 4 . 98 LoRA ( 32 ) 9 . 46 6 . 76 LoRA ( 128 ) 9 . 55 8 . 72 LoRA ( 256 ) 9 . 76 9 . 83 F LORA ( 8 ) 9 . 09 9 . 14 F LORA ( 32 ) 9 . 10 14 . 9 F LORA ( 128 ) 9 . 14 18 . 6 F LORA ( 256 ) 9 . 20 19 . 9 As shown in Table 2 , the naive momentum technique achieves better performance than at a cost of more memory usage . Similar to the results in gradient accumulation , LoRA does not save memory given the optimization state is already sublinear . It also has a significantly lower performance when trained from scratch , as the overall matrix update can only be low - rank . Our F LORA utilizes less memory than the naive momentum . In addition , our method recovers ( or even surpasses ) the performance of naive momentum when r is increased . This significantly distinguishes our methods from LoRA as it achieves memory - efficient training even when the initialization is random . 3 . 3 In - depth analyses The effect of κ in momentum . In our momentum implementation , we have a hyper - parameter κ controlling the resampling frequency of the random - projection matrix . In this part , we analyze the effect of κ with T5 - small on the summarization task as our testbed , due to the limit of time and resources . We vary κ by keeping other hyper - parameters the same as in Section 3 . 2 . The results are shown in Table 3 . It is seen that , when κ is below 1000 , the ROUGE scores increase with κ . After a certain threshold , however , we see that the performance starts to decrease . This aligns with our interpretation that the information is better preserved within the interval , but each interval is bottlenecked by the rank . Given the results , we choose κ = 1000 in Section 3 . 2 to balance the preserved information and the overall rank of momentum . Optimizer with linear memory . In our main experiments , we observed a counter - intuitive phenomenon that LoRA empirically increases memory usage . This is likely because that the optimization states in Adafactor are already sublinear , rendering the ineffectiveness of LoRA to save memory in this case . To further verify our method in linear - memory optimizers , we test the performance with a variant of Adafactor where the second - moment estimates are not factorized , essentially making it a linear - memory optimizer . All the other hyper - parameters remain the same as Section 3 . 2 . Table 4 shows the results . As seen , LoRA indeed saves more memory than our F LORA when the rank is low ( r < 128 ) in linear - memory optimizers . However , our F LORA becomes more memory - efficient for r = 256 , because we have a lower constant in the complexity . Moreover , our F LORA largely outperforms LoRA in all settings by 2 – 3 ROUGE points , showing the superiority of our approach . 4 Related work and discussion Parameter efficient fine - tuning . Many methods have been proposed to improve the parameter efficiency of fine - tuning large models . A straightforward way is to tune a subset of the model , such as the top layers ( Li and Liang , 2021 ) and bias vectors ( Zaken et al . , 2022 ) . Another way is to add small tunable modules ( e . g . , Adapter Houlsby et al . , 2019 and LoRA Hu et al . , 2022 ) to the pre - trained model . Although reducing the optimization memory , these methods suffer from the problem that the model parameters are restricted . For example , the total weight change of LoRA is constrained to be low - rank . In an attempt to achieve high - rank updates , ReLoRA ( Lialin et al . , 2023 ) proposes to periodically reinitialize the LoRA patch . However , it requires full - weight pre - training to work properly , growing the peak memory linearly in model size . We 9 Table 3 : The effect of κ in momentum . Setting κ Mem R 1 / R 2 / R L T5 60M XSum 1 1 . 79 0 . 00 / 0 . 00 / 0 . 00 10 1 . 79 27 . 5 / 7 . 68 / 31 . 8 100 1 . 79 29 . 3 / 8 . 89 / 23 . 2 1000 1 . 79 30 . 4 / 9 . 70 / 24 . 2 10000 1 . 79 29 . 5 / 9 . 11 / 23 . 5 Table 4 : The results of linear - memory optimizers . Setting Accumulation Mem R 1 / R 2 / R L T5 60M XSum None 0 . 99 33 . 0 / 11 . 1 / 26 . 1 Naive 1 . 12 34 . 0 / 11 . 5 / 26 . 7 LoRA ( 8 ) 0 . 82 28 . 7 / 7 . 51 / 22 . 0 LoRA ( 32 ) 0 . 86 29 . 0 / 7 . 71 / 22 . 3 LoRA ( 128 ) 1 . 00 29 . 7 / 8 . 02 / 22 . 9 LoRA ( 256 ) 1 . 20 30 . 0 / 8 . 28 / 23 . 2 F LORA ( 8 ) 1 . 00 31 . 6 / 9 . 72 / 24 . 7 F LORA ( 32 ) 1 . 00 32 . 3 / 10 . 3 / 25 . 3 F LORA ( 128 ) 1 . 00 33 . 2 / 10 . 9 / 26 . 0 F LORA ( 256 ) 1 . 04 33 . 5 / 11 . 1 / 26 . 3 hence do not include ReLoRA as a sublinear - memory baseline . By contrast , our method is able to directly start from scratch and achieve the full - training performance , while maintaining a sublinear complexity throughout the process . Matrix compression . Our method is closely connected to matrix compression techniques . For example , principal component analysis ( Shlens , 2014 ) or matrix sketching ( Liberty , 2013 ) use singular value decom - position ( SVD ) to approximate the large matrix with smaller matrices . However , the SVD procedure is computationally expensive and difficult to be parallelized , making it impractical for large - scale training . Another way to compress the matrix is to use random projection ( Bingham and Mannila , 2001 ) , which lies the foundation of our method . Our method additionally involves a simple and efficient decompression procedure justified by Theorem 2 . 4 . The simplification saves both computation and memory usage . Memory - efficient optimizers . Optimization states contribute significantly to memory usage for large - scale training ( Dettmers et al . , 2021 ) . Memory - efficient optimizers ( Shazeer and Stern , 2018 ; Feinberg et al . , 2023 ) are shown to effectively reduce the memory footprint . Our method is orthogonal to these methods , as it can be applied to enhance existing optimizers by compressing the momentum or gradient accumulation . Memory - efficient automatic differentiation . It is also possible to reduce the memory footprint of back - propagation with advanced techniques like gradient checkpointing ( Chen et al . , 2016 ) , mixed - precision training ( Micikevicius et al . , 2018 ) , randomized auto differentiation ( Oktay et al . , 2020 ) , or zeroth - order optimization ( Malladi et al . , 2023 ) . Technically , F LORA can be combined with these methods to further save memory . We leave it to future work given our focus in this paper is to compress optimization states . 5 Conclusion Summary . In this work , we introduce F LORA , a method based on random projections that achieves sub - linear memory usage for gradient accumulation and momentum . In addition , our approach effectively addresses the low - rank limitation of LoRA by resampling projection matrices . Experimental results demon - strate significant memory reduction with maintained model performance , highlighting the potential of random projection in deep learning . Future work . In this paper , the largest model is 3B . For extremely large models like GPT - 3 , we estimate that the compressed optimization state of r = 256 is only 2 . 08 % of its original memory , which would be of great practical significance . We leave the empirical verification to future work . Further , the applicable scope of F LORA is not limited to language models . We would like to test it in more cases . Acknowledgements This research was supported in part by Natural Sciences and Engineering Research Council of Canada ( NSERC ) under Grant No . RGPIN2020 - 04465 , the Amii Fellow Program , the Canada CIFAR AI Chair Program , the Digital Research Alliance of Canada ( alliancecan . ca ) , and a Mitacs Accelerate ( Cluster ) grant . 10 References John Duchi , Elad Hazan , and Yoram Singer . Adaptive subgradient methods for online learning and stochastic optimization . JMLR , 12 ( 7 ) , 2011 . URL https : / / jmlr . org / papers / v12 / duchi11a . html . Geoffrey Hinton , Nitish Srivastava , and Kevin Swersky . Neural networks for machine learning lecture 6a overview of mini - batch gradient descent . Coursera , 2012 . URL https : / / www . cs . toronto . edu / ~ tijmen / csc321 / slides / lecture _ slides _ lec6 . pdf . Ashok Cutkosky and Francesco Orabona . Momentum - based variance reduction in non - convex SGD . In NeurIPS , 2019 . URL https : / / papers . nips . cc / paper _ files / paper / 2019 / hash / b8002139cdde66b87638f7f91d169d96 - Abstract . html . Gabriel Goh . Why momentum really works . Distill , 2017 . URL http : / / distill . pub / 2017 / momentum . Chong Wang , Xi Chen , Alexander J Smola , and Eric P Xing . Variance reduction for stochastic gra - dient optimization . In NIPS , 2013 . URL https : / / papers . nips . cc / paper _ files / paper / 2013 / hash / 9766527f2b5d3e95d4a733fcfb77bd7e - Abstract . html . Tom Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared D Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert - Voss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel Ziegler , Jeffrey Wu , Clemens Winter , Chris Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , and Dario Amodei . Language models are few - shot learners . In NeurIPS , pages 1877 – 1901 , 2020 . URL https : / / papers . nips . cc / paper / 2020 / hash / 1457c0d6bfcb4967418bfb8ac142f64a - Abstract . html . Robin Rombach , Andreas Blattmann , Dominik Lorenz , Patrick Esser , and Bj¨orn Ommer . High - resolution image synthesis with latent diffusion models . In CVPR , pages 10684 – 10695 , 2022 . URL https : / / openaccess . thecvf . com / content / CVPR2022 / html / Rombach _ High - Resolution _ Image _ Synthesis _ With _ Latent _ Diffusion _ Models _ CVPR _ 2022 _ paper . html . Diederik P . Kingma and Jimmy Ba . Adam : A method for stochastic optimization . In Yoshua Bengio and Yann LeCun , editors , ICLR , 2015 . URL http : / / arxiv . org / abs / 1412 . 6980 . N . Houlsby , A . Giurgiu , Stanislaw Jastrzebski , Bruna Morrone , Quentin de Laroussilhe , Andrea Gesmundo , Mona Attariyan , and S . Gelly . Parameter - efficient transfer learning for NLP . In ICML , pages 2790 – 2799 , 2019 . URL https : / / proceedings . mlr . press / v97 / houlsby19a . html . Elad Ben Zaken , Yoav Goldberg , and Shauli Ravfogel . BitFit : Simple parameter - efficient fine - tuning for transformer - based masked language - models . In ACL , volume 2 , pages 1 – 9 , 2022 . URL https : / / aclanthology . org / 2022 . acl - short . 1 . Edward J Hu , yelong shen , Phillip Wallis , Zeyuan Allen - Zhu , Yuanzhi Li , Shean Wang , Lu Wang , and Weizhu Chen . LoRA : Low - rank adaptation of large language models . In ICLR , 2022 . URL https : / / openreview . net / forum ? id = nZeVKeeFYf9 . Tim Dettmers , M . Lewis , Sam Shleifer , and Luke Zettlemoyer . 8 - bit optimizers via block - wise quantization . In ICLR , 2021 . URL https : / / openreview . net / forum ? id = shpkpVXzo3h . Vladimir Feinberg , Xinyi Chen , Y . Jennifer Sun , Rohan Anil , and Elad Hazan . Sketchy : Memory - efficient adaptive regularization with frequent directions . In NeurIPS , 2023 . URL https : / / openreview . net / forum ? id = DeZst6dKyi . Noam Shazeer and Mitchell Stern . Adafactor : Adaptive learning rates with sublinear memory cost . In ICML , pages 4596 – 4604 , 2018 . URL https : / / proceedings . mlr . press / v80 / shazeer18a . html . Lorenzo Finesso and Peter Spreij . Nonnegative matrix factorization and I - divergence alternating minimiza - tion . Linear Algebra and its Applications , 416 ( 2 - 3 ) : 270 – 287 , 2006 . URL https : / / doi . org / 10 . 1016 / j . laa . 2005 . 11 . 012 . Jack W Rae , Sebastian Borgeaud , Trevor Cai , Katie Millican , Jordan Hoffmann , Francis Song , John Aslanides , Sarah Henderson , Roman Ring , Susannah Young , et al . Scaling language models : Methods , analysis & insights from training gopher . arXiv preprint arXiv : 2112 . 11446 , 2021 . URL https : / / arxiv . org / abs / 2112 . 11446 . Sanjoy Dasgupta . Experiments with random projection . In UAI , pages 143 – 151 , 2000 . URL https : / / dl . acm . org / doi / 10 . 5555 / 647234 . 719759 . Ella Bingham and Heikki Mannila . Random projection in dimensionality reduction : applications to image and text data . In KDD , pages 245 – 250 , 2001 . URL https : / / dl . acm . org / doi / 10 . 1145 / 502512 . 502546 . 11 Sanjoy Dasgupta and Anupam Gupta . An elementary proof of a theorem of johnson and lindenstrauss . Random Structures & Algorithms , 22 ( 1 ) : 60 – 65 , 2003 . URL https : / / doi . org / 10 . 1002 / rsa . 10073 . Jiˇr´ı Matouˇsek . On variants of the johnson – lindenstrauss lemma . Random Structures & Algorithms , 33 ( 2 ) : 142 – 156 , 2008 . URL https : / / doi . org / 10 . 1002 / rsa . 20218 . Piotr Indyk and Rajeev Motwani . Approximate nearest neighbors : towards removing the curse of dimen - sionality . In STOC , pages 604 – 613 , 1998 . URL https : / / dl . acm . org / doi / 10 . 1145 / 276698 . 276876 . Han Xiao , Kashif Rasul , and Roland Vollgraf . Fashion - mnist : a novel image dataset for benchmarking machine learning algorithms . arXiv preprint arXiv : 1708 . 07747 , 2017 . URL https : / / arxiv . org / abs / 1708 . 07747 . Samuel L Smith , Pieter - Jan Kindermans , Chris Ying , and Quoc V Le . Don’t decay the learning rate , increase the batch size . In ICLR , 2018 . URL https : / / openreview . net / forum ? id = B1Yy1BxCZ . Yurii Nesterov . Introductory Lectures on Convex Optimization : A Basic Course . Springer , 1998 . URL https : / / doi . org / 10 . 1007 / 978 - 1 - 4419 - 8853 - 9 . Samy Jelassi and Yuanzhi Li . Towards understanding how momentum improves generalization in deep learning . In ICML , pages 9965 – 10040 , 2022 . URL https : / / openreview . net / forum ? id = lf0W6tcWmh - . Tim Dettmers , Artidoro Pagnoni , Ari Holtzman , and Luke Zettlemoyer . QLoRA : Efficient finetuning of quantized LLMs . In NeurIPS , 2023 . URL https : / / openreview . net / forum ? id = OUIFPHEgJU . Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , and Peter J Liu . Exploring the limits of transfer learning with a unified text - to - text transformer . JMLR , 21 ( 1 ) : 5485 – 5551 , 2020 . URL http : / / jmlr . org / papers / v21 / 20 - 074 . html . Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , Ilya Sutskever , et al . Language models are unsupervised multitask learners . OpenAI blog , 2019 . URL https : / / openai . com / research / better - language - models . Shashi Narayan , Shay B Cohen , and Mirella Lapata . Don’t give me the details , just the summary ! topic - aware convolutional neural networks for extreme summarization . In EMNLP , pages 1797 – 1807 , 2018 . URL https : / / aclanthology . org / D18 - 1206 . Zhaojiang Lin , Andrea Madotto , and Pascale Fung . Exploring versatile generative language model via parameter - efficient transfer learning . In EMNLP Findings , pages 441 – 459 , 2020 . URL https : / / aclanthology . org / 2020 . findings - emnlp . 41 . Mauro Cettolo , Marcello Federico , Luisa Bentivogli , Jan Niehues , Sebastian St¨uker , Katsuhito Sudoh , Koichiro Yoshino , and Christian Federmann . Overview of the IWSLT 2017 evaluation campaign . In IWSLT , pages 2 – 14 , 2017 . URL https : / / aclanthology . org / 2017 . iwslt - 1 . 1 . Chin - Yew Lin . ROUGE : A package for automatic evaluation of summaries . In Text Summarization Branches Out , pages 74 – 81 , 2004 . URL https : / / aclanthology . org / W04 - 1013 . Matt Post . A call for clarity in reporting BLEU scores . In WMT , pages 186 – 191 , 2018 . URL https : / / aclanthology . org / W18 - 6319 . James Bradbury , Roy Frostig , Peter Hawkins , Matthew James Johnson , Chris Leary , Dougal Maclaurin , George Necula , Adam Paszke , Jake VanderPlas , Skye Wanderman - Milne , and Qiao Zhang . JAX : compos - able transformations of Python + NumPy programs , 2018 . URL http : / / github . com / google / jax . DeepMind , Igor Babuschkin , Kate Baumli , Alison Bell , Surya Bhupatiraju , Jake Bruce , Peter Buchlovsky , David Budden , Trevor Cai , Aidan Clark , Ivo Danihelka , Antoine Dedieu , Claudio Fantacci , Jonathan Godwin , Chris Jones , Ross Hemsley , Tom Hennigan , Matteo Hessel , Shaobo Hou , Steven Kapturowski , Thomas Keck , Iurii Kemaev , Michael King , Markus Kunesch , Lena Martens , Hamza Merzic , Vladimir Mikulik , Tamara Norman , George Papamakarios , John Quan , Roman Ring , Francisco Ruiz , Alvaro Sanchez , Laurent Sartran , Rosalia Schneider , Eren Sezener , Stephen Spencer , Srivatsan Srinivasan , Miloˇs Stanojevi´c , Wojciech Stokowiec , Luyu Wang , Guangyao Zhou , and Fabio Viola . The DeepMind JAX Ecosystem , 2020 . URL http : / / github . com / google - deepmind . Ilya Loshchilov and Frank Hutter . SGDR : Stochastic gradient descent with warm restarts . In ICLR , 2017 . URL https : / / openreview . net / forum ? id = Skq89Scxx . Ilya Loshchilov and Frank Hutter . Decoupled weight decay regularization . In ICLR , 2019 . URL https : / / openreview . net / forum ? id = Bkg6RiCqY7 . 12 Jeff Wu , Long Ouyang , Daniel M Ziegler , Nisan Stiennon , Ryan Lowe , Jan Leike , and Paul Christiano . Recursively summarizing books with human feedback . arXiv preprint arXiv : 2109 . 10862 , 2021 . URL https : / / arxiv . org / abs / 2109 . 10862 . Hao Li , Pratik Chaudhari , Hao Yang , Michael Lam , Avinash Ravichandran , Rahul Bhotika , and Stefano Soatto . Rethinking the hyperparameters for fine - tuning . In ICLR , 2020 . URL https : / / openreview . net / forum ? id = B1g8VkHFPH . Dominic Masters and Carlo Luschi . Revisiting small batch training for deep neural networks . arXiv preprint arXiv : 1804 . 07612 , 2018 . URL https : / / arxiv . org / abs / 1804 . 07612 . Xiang Lisa Li and Percy Liang . Prefix - tuning : Optimizing continuous prompts for generation . In ACL - IJCNLP , volume 1 , pages 4582 – 4597 , 2021 . URL https : / / aclanthology . org / 2021 . acl - long . 353 . Vladislav Lialin , Namrata Shivagunde , Sherin Muckatira , and Anna Rumshisky . Stack more layers differently : High - rank training through low - rank updates . arXiv preprint arXiv : 2307 . 05695 , 2023 . URL https : / / arxiv . org / abs / 2307 . 05695 . Jonathon Shlens . A tutorial on principal component analysis . arXiv preprint arXiv : 1404 . 1100 , 2014 . URL https : / / arxiv . org / abs / 1404 . 1100 . Edo Liberty . Simple and deterministic matrix sketching . In KDD , pages 581 – 588 , 2013 . URL https : / / doi . org / 10 . 1145 / 2487575 . 2487623 . Tianqi Chen , Bing Xu , Chiyuan Zhang , and Carlos Guestrin . Training deep nets with sublinear memory cost . arXiv preprint arXiv : 1604 . 06174 , 2016 . URL https : / / arxiv . org / abs / 1604 . 06174 . Paulius Micikevicius , Sharan Narang , Jonah Alben , Gregory Diamos , Erich Elsen , David Garcia , Boris Ginsburg , Michael Houston , Oleksii Kuchaiev , Ganesh Venkatesh , and Hao Wu . Mixed precision training . In ICLR , 2018 . URL https : / / openreview . net / forum ? id = r1gs9JgRZ . Deniz Oktay , Nick McGreivy , Joshua Aduol , Alex Beatson , and Ryan P Adams . Randomized automatic differentiation . In ICLR , 2020 . URL https : / / openreview . net / forum ? id = xpx9zj7CUlY . Sadhika Malladi , Tianyu Gao , Eshaan Nichani , Alex Damian , Jason D . Lee , Danqi Chen , and Sanjeev Arora . Fine - tuning language models with just forward passes . In NeurIPS , 2023 . URL https : / / openreview . net / forum ? id = Vota6rFhBQ . Beatrice Laurent and Pascal Massart . Adaptive estimation of a quadratic functional by model selection . Annals of statistics , 28 ( 5 ) : 1302 – 1338 , 2000 . URL https : / / www . jstor . org / stable / 2674095 . 13 A Proof of Theorem 2 . 1 Theorem 2 . 1 . Let LoRA update matrices A and B with SGD for every step t by A t + 1 ← A t − η B ⊤ t ( ∇ W L t ) ( 5 ) B t + 1 ← B t − η ( ∇ W L t ) A ⊤ t . ( 6 ) We assume (cid:13)(cid:13) ∑ Tt = 0 ∇ W L t (cid:13)(cid:13) F ≤ L for every T during training , which implies that the model stays within a finite Euclidean ball . In this case , the dynamics of A t and B t are given by A T = A 0 + η A 0 f A ( T ) , B T = η f B ( T ) A ⊤ 0 , ( 7 ) where the forms of f A ( t ) ∈ R m × m and f B ( t ) ∈ R n × m are expressed in the proof . In particular , ∥ f A ( t ) ∥ 2 ≤ η L 2 (cid:0) 1 − ( η 2 L 2 ) t (cid:1) 1 − η 2 L 2 for every t . Before proving the theorem , we need to obtain the form of f A ( t ) and f B ( t ) . We derive them in the following lemma . Lemma A . 1 . When t = 0 , f A ( 0 ) = f B ( 0 ) = 0 . For t ≥ 1 , the values of f A ( t ) and f B ( t ) are iteratively obtained by : f A ( t ) = − η t − 1 ∑ i = 0 f ⊤ B ( i ) ( ∇ W L i ) ( 19 ) f B ( t ) = − t − 1 ∑ i = 0 ( ∇ W L i ) ( η f ⊤ A ( i ) + I ) ( 20 ) Proof . We prove this by induction . For the base case t = 0 , it is straightforward to show f A ( 0 ) = f B ( 0 ) = 0 . Assume A t = A 0 + η A 0 f A ( t ) and B t = η f B ( t ) A ⊤ 0 holds with such functions f A and f B for 1 . . . t . Then for t + 1 , we have A t + 1 = A t − η B ⊤ t ( ∇ W L t ) ( 21 ) = A 0 + η A 0 f A ( t ) − η 2 A 0 f ⊤ B ( t ) ( ∇ W L t ) ( 22 ) = A 0 + η A 0 (cid:16) f A ( t ) − η f ⊤ B ( t ) ( ∇ W L t ) (cid:17) ( 23 ) = A 0 + η A 0 ( f A ( t + 1 ) ) , ( 24 ) where we have f A ( t + 1 ) = f A ( t ) − η f ⊤ B ( t ) ( ∇ W L t ) = − η ∑ ti = 0 f ⊤ B ( i ) ( ∇ W L i ) in the last line . Similarly , we have B t + 1 = B t − η ( ∇ W L t ) A ⊤ t ( 25 ) = η f B ( t ) A ⊤ 0 − η ( ∇ W L t ) ( η f ⊤ A ( t ) + I ) A ⊤ 0 ( 26 ) = η (cid:16) f B ( t ) − ( ∇ W L t ) ( η f ⊤ A ( t ) + I ) (cid:17) A ⊤ 0 ( 27 ) = η f B ( t + 1 ) A ⊤ 0 , ( 28 ) where we have f B ( t + 1 ) = f B ( t ) − ( ∇ W L t ) ( η f ⊤ A ( t ) + I ) = − ∑ ti = 0 ( ∇ W L i ) ( η f ⊤ A ( i ) + I ) in the last line . Proof of Theorem 2 . 1 . Define a t : = η L 2 (cid:0) 1 − ( η 2 L 2 ) t (cid:1) 1 − η 2 L 2 . We prove ∥ f A ( t ) ∥ ≤ a t by induction . For the base case t = 0 , it is trivial to see ∥ f A ( 0 ) ∥ = 0 ≤ a 0 . We then assume ∥ f A ( i ) ∥ ≤ a i for i ≤ t − 1 . Since a t is monotonically increasing , we know ∥ f A ( i ) ∥ ≤ a t − 1 for i ≤ t − 1 . By using Lemma A . 1 , we have f A ( t ) = − η t − 1 ∑ i = 0 f ⊤ B ( i ) ( ∇ W L i ) ( 29 ) = − η t − 1 ∑ i = 1 i − 1 ∑ j = 0 ( η f A ( j ) + I ) ( ∇ W L j ) ⊤ ( ∇ W L i ) . ( 30 ) 14 Taking the norm , we have ∥ f A ( t ) ∥ F = (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) η t − 1 ∑ i = 1 i − 1 ∑ j = 0 ( η f A ( j ) + I ) ( ∇ W L j ) ⊤ ( ∇ W L i ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) F ( 31 ) ≤ η 2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t − 2 ∑ j = 0 ( f A ( j ) ) ( ∇ W L j ) ⊤ t − 1 ∑ i = j + 1 ( ∇ W L i ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) F + η (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t − 1 ∑ i = 1 i − 1 ∑ j = 0 ( ∇ W L j ) ⊤ ( ∇ W L i ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) F ( 32 ) ≤ η 2 L (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t − 2 ∑ j = 0 ( f A ( j ) ) ( ∇ W L j ) ⊤ (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) F + η L 2 ( Lemma A . 2 ) ≤ η 2 L 2 a t − 1 + η L 2 ( Lemma A . 3 ) = η 2 L 2 η L 2 − ( η 2 L 2 ) t 1 − η 2 L 2 + η L 2 ( 33 ) = η L 2 − ( η 2 L 2 ) t + 1 1 − η 2 L 2 ( 34 ) = a t . ( 35 ) Therefore , we have ∥ f A ( t ) ∥ ≤ ∥ f A ( t ) ∥ F ≤ a t for every t . Lemma A . 2 . If ∥ ∑ t − 1 k = 0 ( ∇ W L j ) ⊤ ∥ F ≤ L , we have (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t − 1 ∑ i = 1 i − 1 ∑ j = 0 ( ∇ W L j ) ⊤ ( ∇ W L i ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) F ≤ L 2 ( 36 ) for every t . Proof . Let G ( k ) : = ∇ W L k for simplicity . Taking the square , (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ∑ n n ∑ m = 1 G ( m ) ⊤ G ( n ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 F = ∑ i , j ∑ n n ∑ m = 1 [ G ( m ) ⊤ G ( n ) ] 2 i , j ( 37 ) = ∑ i , j ∑ n n ∑ m = 1 (cid:16) ∑ k [ G ( m ) ] k , i [ G ( n ) ] k , j (cid:17) 2 ( 38 ) ≤ (cid:16) ∑ m ∑ i ∑ k [ G ( m ) ] 2 k , i (cid:17)(cid:16) t − 1 ∑ m = n ∑ j ∑ k [ G ( n ) ] 2 k , j (cid:17) ( Cauchy - Schwarz ) ≤ (cid:18) ∑ m ∑ i ∑ k [ G ( m ) ] 2 k , i (cid:19) 2 ( 39 ) ≤ (cid:18) ∑ i ∑ k (cid:16) ∑ m [ G ( m ) ] k , i (cid:17) 2 (cid:19) 2 ( 40 ) = (cid:13)(cid:13)(cid:13) ∑ m G ( m ) (cid:13)(cid:13)(cid:13) 4 F ( 41 ) ≤ L 4 , ( 42 ) which completes the proof by taking square roots on both sides . Lemma A . 3 . If ∥ f A ( k ) ∥ F ≤ a k for all k < t , we have (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t − 1 ∑ k = 0 f A ( k ) ( ∇ W L k ) ⊤ (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) F ≤ a t − 1 L ( 43 ) for every t . 15 Proof . Let A ( k ) : = f A ( k ) and G ( k ) = ( ∇ W L k ) for simplicity . Taking the square , (cid:13)(cid:13)(cid:13) t − 1 ∑ k = 0 A ( k ) G ( k ) ⊤ (cid:13)(cid:13)(cid:13) 2 F = t − 1 ∑ k = 0 ∑ i ∑ j [ A ( k ) G ( k ) ⊤ ] 2 i , j ( 44 ) = t − 1 ∑ k = 0 ∑ i ∑ j (cid:16) ∑ l [ A ( k ) ] i , l [ G ( k ) ] j , l (cid:17) 2 ( 45 ) = t − 1 ∑ k = 0 (cid:16) ∑ i ∑ l [ A ( k ) ] 2 i , l (cid:17)(cid:16) ∑ j ∑ l [ G ( k ) ] 2 j , l (cid:17) ( Cauchy – Schwarz ) ≤ (cid:16) max 0 ≤ k < t ∑ i ∑ l [ A ( k ) ] 2 i , l (cid:17) t − 1 ∑ k = 0 (cid:16) ∑ j ∑ l [ G ( k ) ] 2 j , l (cid:17) ( 46 ) ≤ (cid:16) max 0 ≤ k < t ∑ i ∑ l [ A ( k ) i , l ] 2 (cid:17)(cid:16) ∑ j ∑ l ( t − 1 ∑ k = 0 [ G ( k ) ] j , l ) 2 (cid:17) ( 47 ) = (cid:16) max 0 ≤ k < t ∥ A ( k ) ∥ 2 F (cid:17)(cid:13) (cid:13)(cid:13) t − 1 ∑ k = 0 G ( k ) (cid:13) (cid:13)(cid:13) 2 F ( 48 ) ≤ a 2 t − 1 L 2 , ( 49 ) which completes the proof by taking square roots on both sides . B Proof of Theorem 2 . 4 Theorem 2 . 4 . Let A be a matrix of shape R r × m where each element is independently sampled from a standard Gaussian distribution . Let ϵ , δ ∈ ( 0 , 1 ] . There exists a constant c such that when r = c log ( 2 m / δ ) ϵ − 2 , we have for all i , j that (cid:12)(cid:12)(cid:12) [ A ⊤ A − I ] i , j (cid:12)(cid:12)(cid:12) ≤ ϵ ( 18 ) with confidence at least 1 − δ . Proof . For each element of A ⊤ A , we have [ A ⊤ A ] i , j = (cid:40) ∑ rk = 1 a 2 k , i , if i = j , ∑ rk = 1 a k , i a k , j , otherwise , ( 50 ) where each element a i , k is an independent random variable following N ( 0 , 1 ) . For z i , i : = ∑ rk = 1 a 2 k , i , it follows the χ 2 ( r ) distribution . By the standard Laurent - Massart bounds Laurent and Massart ( 2000 ) , we obtain δ i , i : = (cid:12)(cid:12)(cid:12) z i , i r − 1 (cid:12)(cid:12)(cid:12) ≤ 2 (cid:115) log ( 2 / δ ′ 1 ) r + 2log ( 2 / δ ′ 1 ) r ( 51 ) with probability at least 1 − δ ′ 1 . For z i , j : = ∑ rk = 1 a k , i a k , j where i ̸ = j , we can rewrite it as z i , j = ∑ rk = 1 [ ( a k , i + a k , j 2 ) 2 − ( a k , i − a k , j 2 ) 2 ] . In addition , all ( a k , i + a k , j 2 ) 2 and ( a k , i − a k , j 2 ) 2 are i . i . d . χ 2 ( 1 ) distributions . Define z + i , j : = r ∑ k = 1 (cid:18) a k , i + a k , j 2 (cid:19) 2 and z − i , j : = r ∑ k = 1 (cid:18) a k , i − a k , j 2 (cid:19) 2 , ( 52 ) it is easy to see that z + i , j and z − i , j are i . i . d . χ 2 ( r ) distributions . In addition , z i , j = z + i , j − z − i , j . Therefore , we have 16 δ i , j : = (cid:12)(cid:12)(cid:12) z i , j r (cid:12)(cid:12)(cid:12) = (cid:12)(cid:12)(cid:12)(cid:12)(cid:16) z + i , j r − 1 (cid:17) − (cid:16) z − i , j r − 1 (cid:17)(cid:12)(cid:12)(cid:12)(cid:12) ( 53 ) ≤ (cid:12)(cid:12)(cid:12) z + i , j r − 1 (cid:12)(cid:12)(cid:12) + (cid:12)(cid:12)(cid:12) z − i , j r − 1 (cid:12)(cid:12)(cid:12) ( 54 ) ≤ 4 (cid:114) log ( 4 / δ ′ 2 ) r + 4log ( 4 / δ ′ 2 ) r ( 55 ) with probability at least 1 − δ ′ 2 . By using a union bound upon Equation ( 51 ) and ( 55 ) , we can obtain δ i , j ≤ 4 (cid:114) 2 log ( 2 m / δ ) r + 42 log ( 2 m / δ ) r ( 56 ) for all i , j with probability at least 1 − δ . Under this condition , we further have δ i , j = 4 (cid:115) 2 log ( 2 m / δ ) 128 log ( 2 m / δ ) ϵ − 2 + 4 2 log ( 2 m / δ ) 128 log ( 2 m / δ ) ϵ − 2 ( Let r = 128 log ( 2 m / δ ) ϵ − 2 ) = 1 2 ( ϵ + ϵ 2 ) ( 57 ) ≤ ϵ , ( ϵ 2 ≤ ϵ ≤ 1 ) concluding the proof by noticing δ i , j = (cid:12)(cid:12)(cid:12) [ A ⊤ A − I ] i , j (cid:12)(cid:12)(cid:12) . 17