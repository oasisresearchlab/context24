541 Stop the [ Image ] Steal : The Role and Dynamics of Visual Content in the 2020 U . S . Election Misinformation Campaign HANA MATATOV , Technion - Israel Institute of Technology , Israel MOR NAAMAN , Cornell Tech , United States OFRA AMIR , Technion - Israel Institute of Technology , Israel Images are powerful . Visual information can attract attention , improve persuasion , trigger stronger emotions , and is easy to share and spread . We examine the characteristics of the popular images shared on Twitter as part of “Stop the Steal” , the widespread misinformation campaign during the 2020 U . S . election . We analyze the spread of the forty most popular images shared on Twitter as part of this campaign . Using a coding process , we categorize and label the images according to their type , content , origin , and role , and perform a mixed - method analysis of these images’ spread on Twitter . Our results show that popular images include both photographs and text rendered as image . Only very few of these popular images included alleged photographic evidence of fraud ; and none of the popular photographs had been manipulated . Most images reached a significant portion of their total spread within several hours from their first appearance , and both popular - and less - popular accounts were involved in various stages of their spread . CCS Concepts : • Human - centered computing → Empirical studies in collaborative and social com - puting ; Collaborative and social computing . Additional Key Words and Phrases : Visual misinformation , Twitter , 2020 U . S . election , Social media , Visual communication ACM Reference Format : Hana Matatov , Mor Naaman , and Ofra Amir . 2022 . Stop the [ Image ] Steal : The Role and Dynamics of Visual Content in the 2020 U . S . Election Misinformation Campaign . Proc . ACM Hum . - Comput . Interact . 6 , CSCW2 , Article 541 ( November 2022 ) , 24 pages . https : / / doi . org / 10 . 1145 / 3555599 1 INTRODUCTION Visual content plays a significant role in online misinformation [ 28 , 41 , 53 , 66 , 78 ] . This work presents a case study of the “Stop the Steal” misinformation campaign before , during , and follow - ing the 2020 U . S . elections to expand our understanding of the nature and dynamics of popular images shared in such a campaign . This campaign , unfolding over social media platforms including Facebook and Twitter , spread unsubstantiated and misleading claims of voter fraud , allegations that the election was stolen , and calls to overturn the results [ 7 , 14 , 18 , 27 , 34 ] . These widespread claims influenced public opinion [ 8 ] to such an extent that , as of December 2020 , approximately a third of the US population did not trust the election results [ 7 , 62 ] , a number that remained similar nearly a year after the election [ 71 ] . We focus here on visual content , given the persuasive nature of images [ 21 , 41 , 43 , 66 , 79 , 84 ] and the frequent use of visual information in such campaigns [ 6 , 9 , 15 , 20 , 48 , 50 , 64 , 82 ] . Because of Authors’ addresses : Hana Matatov , hanama888 @ gmail . com , Technion - Israel Institute of Technology , Haifa , Israel ; Mor Naaman , mor . naaman @ cornell . edu , Cornell Tech , New York , New York , United States ; Ofra Amir , oamir @ technion . ac . il , Technion - Israel Institute of Technology , Haifa , Israel . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . © 2022 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . 2573 - 0142 / 2022 / 11 - ART541 $ 15 . 00 https : / / doi . org / 10 . 1145 / 3555599 Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . a r X i v : 2209 . 02007v1 [ c s . S I ] 5 S e p 2022 541 : 2 Hana Matatov , Mor Naaman , and Ofra Amir the persuasive power of images and memes [ 15 , 21 , 38 , 43 , 48 , 66 , 79 ] , visual content may be used to convey powerful messages and provide more “believable” evidence or indication that certain activities occurred . Images also provide a research opportunity because , similar to URLs , they provide a method for matching stories about the same subject or topic [ 26 , 55 , 59 , 83 ] . For example , when using a Twitter dataset , images give us a robust mechanism to associate all tweets that shared the same or a very similar image , regardless of text similarity . We use data from the Stop the Steal campaign on Twitter , and look at the forty most popular images shared during that campaign . The source of our data is the VoterFraud2020 dataset [ 1 ] , which includes over 33 million tweets and retweets containing key phrases related to voter fraud claims , surrounding the U . S . 2020 election , between October 23rd and December 16th , 2020 . The dataset includes 167 , 696 perceptual hash ( pHash ) values [ 58 ] for images appearing in the tweets . The pHash values allow matching of identical images across tweets . The forty most popular images were all shared in the dataset more than 1 , 000 times each , with the top two images appearing in the data in over 10 , 000 tweets and retweets ( each ) . Our goal is to examine the spread of images as part of the Stop the Steal campaign on Twitter by using an analysis of the top images shared and characteristics of their spread . We set out to characterize ( 1 ) the types and roles of the top images that were shared in this campaign ; ( 2 ) the temporal patterns of the images’ spread ; and ( 3 ) the role of different users with different network prominence in sharing and popularizing the images . To this end , we take a mixed - methods approach to analyzing the popular images . Our analysis uses a qualitative coding process to categorize the images along multiple categories , including type , origin , content , and role . We further conduct a quantitative data - driven analysis of how the images were shared in the campaign . Our results expand the understanding of how images are used in large - scale misinformation campaigns , and highlight the challenges these patterns pose in addressing the misinformation problem . We show that only a small number of the popular images attempted to provide “evidence” for voter fraud , and that none of the popular images were photographs that were manipulated in any way . In fact , many of the popular “images” shared were simply text which was rendered as an image to highlight its contents , and only one was a “meme” ( a captioned picture that tends to spread widely online ) [ 31 , 48 , 57 ] . Most images in this set of top 40 had similar temporal distribution patterns , regardless of their labels . The images spread quickly : almost 50 % of the images reached the half - life of their spread ( time until half of their shares in our data ) within only three hours from when they were first found in our data . We show that the spread of these images had a mixed - participation pattern with accounts of varying popularity participating during all stages of the images’ propagation . This result is consistent with the findings of earlier works showing the multi - directional nature of participation in the campaign between accounts with different levels of popularity [ 27 ] . At the same time , our results show that images that offered “evidence” of voter fraud , i . e . more directly controversial claims , were less readily shared by larger accounts ( 1 , 000 - 100 , 000 followers ) compared to smaller ones . Finally , we present a temporal order - based analysis that shows that amongst popular users , some repeat participants were particularly active in the early spread of multiple instances of images in this campaign – again supporting earlier findings [ 27 , 33 ] . The results provide new insights about the use of visual content during misinformation campaigns , and offer implications for designing solutions to cope with such content , e . g . , moderation of content or accounts [ 19 , 35 , 65 ] . For example , the identification of the most prevalent images and their characteristics shows that the most prevalent issue is , at least at this point , not image manipulation , though this topic received significant policy and research attention [ 17 , 31 , 48 , 51 , 52 , 57 , 66 , 82 ] . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . The Role and Dynamics of Visual Content in the 2020 U . S . Election Misinformation Campaign 541 : 3 Our findings emphasize the importance of addressing visual misinformation in the form of “out - of - context” images , where images are shared in a context different from that in which they were created [ 28 , 40 , 54 , 55 , 59 , 78 ] . Further , 40 % of the popular images were actually text or quotes rendered as images , suggesting that extracting and analyzing image text could also be an important tool for addressing misinformation . The quick spread of the popular images demonstrates that any effective platform approach to mitigating the spread of misinformation would need to be applied within a few hours from the time the image was first shared . Finally , the mixed - participation pattern shows the challenges of using selective user suspension as a way to address misinformation . However , we show that repeat and early offenders exist within the more popular users , which may suggest a strategy for selective yet effective enforcement . 2 BACKGROUND In this work , we study the role and dynamics of visual content in a misinformation campaign using the Stop the Steal misinformation campaign surrounding the 2020 U . S . election as a case study . We therefore build on two general bodies of work : related work in the field of visual communication , and in particular visual misinformation ( Section 2 . 1 ) ; and prior work examining misinformation in election campaigns , in particular in the context of the recent U . S . elections ( Section 2 . 2 ) . Recent social computing research efforts have focused on examining misinformation and dis - information campaigns online [ 47 , 77 , 80 ] , and were especially interested in understanding the loosely - coordinated nature of such campaigns [ 5 , 45 , 72 – 74 ] . In particular , Starbird et al . [ 72 ] call for considering “strategic information operations” as a critical concern for CSCW researchers , highlighting the organic aspects of such campaigns , often reflecting the activities of diverse actors who are implicitly coordinated . Similar dynamics had been observed in the Stop the Steal campaign that we study in this work [ 1 , 27 ] . Misinformation also received attention from social computing researchers looking at interventions that can help people avoid or detect it [ 10 , 44 , 56 , 70 , 86 ] . For example , in [ 44 ] , the authors investigated the design of lightweight interventions that nudge users to assess the accuracy of information before sharing it . Other ways social computing research had considered addressing misinformation included moderation [ 36 , 46 , 69 ] . These papers acknowledge the problems of online harassment and misinformation spread , discuss the extent of the problem , and suggest potential moderation tools – not directly studied in our work here , but related to its implications . 2 . 1 Visual Content and Misinformation Visual content can help attract attention [ 21 , 43 ] ; be more persuasive in providing evidence that something happened [ 66 , 79 , 84 ] ; trigger stronger emotional reactions [ 38 ] ; and increase or speed up the propagating of content due to images’ ease of share [ 41 , 83 , 84 ] . Research had in particular examined these advantages in relation to other formats , like text [ 43 , 79 ] . For example , research had shown that attention to web content with visual imagery is much higher compared to text - only [ 43 ] ; and that visual content ( in this case , video ) increases the believability of the presented content compared to textual form [ 79 ] . Not only are visual content and images important to study , visual content also provides an opportunity to index and track similar content , more so than text [ 26 , 59 , 83 ] . Given their influence , images naturally had major impact on politics and political campaigns [ 6 , 9 , 15 , 20 , 50 , 82 ] . For example , Casas and Williams argue that images support mobilizing because they trigger stronger emotional reactions than text , and shown that images on Twitter have a positive mobilizing effect to get people involved in online protests [ 15 ] . The use of images in a political context is deliberate . For example , past work on Media Cloud had shown diverging visual narratives used by media sources of different political affinity [ 9 ] . In social media context , Marchal Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . 541 : 4 Hana Matatov , Mor Naaman , and Ofra Amir et al . investigated the visual formats and content themes of images shared on Twitter during the 2019 EU parliamentary election campaign , showing that both photographic and graphic ( including text quotes ) images were commonly shared during this campaign [ 50 ] . Other works focused specifically on the “meme” visual format ( i . e . captioned pictures that tend to spread widely online ) [ 31 , 57 ] , with or without political context [ 6 , 22 , 26 , 42 , 48 , 75 , 82 , 83 ] . For example , Zakem et al . [ 82 ] reported several famous meme examples and examined how these memetic engagements have been used in U . S . government influence campaigns , indicating the wide range of these memes in terms of content , format , text sentiment , and their role . For example , the authors showed how these memes were utilized by various online actors including governments and individuals . The memes often transcended individual cultures and languages , and reached broad communities . Beyond the political and campaign context , research had investigated the visual elements that distinguish image memes that are highly viral on social media , by developing a codebook to characterize the memes , and using it to annotate 100 memes collected from 4chan’s Politically Incorrect Board ( / pol / ) [ 48 ] . The results showed that highly viral memes are more likely to contain characters or positive / negative emotions , and that memes presenting long text or without a clear subject are unlikely to be re - shared . Naturally , and especially serving as a focus of attention in recent years , images have been used for misinformation and manipulation [ 53 , 66 , 78 ] . The field known as visual misinformation examines how images are used for creating and propagating misinformation , while leveraging the persuasive power of images and their ease of spread [ 41 , 59 , 83 , 84 ] . Visual misinformation can be expressed in spreading visual fabricated or manipulated content ( e . g . photoshopped content , deep fakes , etc . ) [ 17 , 51 , 52 , 66 ] , or in sharing genuine images but out of their real context ( e . g . reuse an old image and share it as new ) [ 28 , 78 ] . Various papers aim to compare these two types of visual misinformation and their spread , and develop techniques to detect visual misinformation [ 28 , 40 , 54 , 55 , 59 ] . Of course , images are often used for misinformation in the political context and in political campaigns [ 64 ] . Indeed , the topic of the spread of misinformation during election campaigns , including the usage of visual misinformation , has received a lot of attention in recent years , which we review and expand on more broadly next . 2 . 2 Election Misinformation A number of studies have specifically focused on elections in recent years , studying and documenting the spread of misinformation around elections and election - related misinformation campaigns ( e . g . , [ 1 ] ) , as well as seeking to understand the impact of misinformation on the outcomes and confidence in elections ( e . g . , [ 8 ] ) . Many of the studies looked at data from different social media platforms , understanding how content and misinformation was spread on these platforms [ 1 , 4 , 11 , 27 , 29 , 39 , 68 , 76 ] . Particularly , several papers were published regarding the 2016 U . S . elections , studying misinformation spread on Twitter [ 11 , 39 ] to understand the exposure to misinformation by voters . For example , Grinberg et al . [ 39 ] showed how only 1 % of users were exposed to 80 % of fake news , and that an even smaller percentage of users were responsible for sharing most of it . Other work explored the cross - platform social media influence and misinformation by the Russian Internet Research Agency ( “Russian trolls” ) during the campaign [ 5 , 37 ] . Research attention continued during the 2020 U . S . elections , where the most prominent misin - formation campaign was Stop the Steal . In this campaign , claims regarding voter fraud were spread on different media and various social media platforms [ 27 , 30 , 60 ] . The promoters of voter fraud claims shared narratives , images and URLs in support of ( mostly false ) allegations [ 3 ] , including missing ballots , ballots cast by dead voters , voting machines irregularities like vote switching , and other claims about the voting process . These claims served as backdrop for pressuring government officials to overturn the results [ 13 , 14 , 18 , 34 ] . The misinformation campaign also targeted the Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . The Role and Dynamics of Visual Content in the 2020 U . S . Election Misinformation Campaign 541 : 5 topic of mail - in voting [ 7 , 67 ] . The Election Integrity Partnership’s election report [ 27 ] extensively documents election - related misinformation narratives and the cross - platform transmission between a wide range of social media platforms during the 2020 election “voter fraud” misinformation cam - paign . The report demonstrated the participatory nature of the misinformation campaign , showing wide participation of both low - and high - level actors ( e . g . verified accounts ) in spreading the narratives , while leveraging the features of each social media platform to spread content effectively . A number of research efforts had collected and made available social media data from this campaign , including content , narratives , URLs , and images [ 1 , 16 ] . In the VoterFraud2020 dataset paper [ 1 ] , the authors perform an analysis of the Twitter users who shared the claims . Based on the analysis , the dataset labels a subset of the users as promoters or detractors of voter fraud claims , based on their participation in the retweets graph . The authors also presented an initial exploration of the content , including the shared images . The researchers showed , for example , that the most popular images on Twitter were often shared in a small number of original tweets that were then retweeted many times . We use this dataset and the users’ labels in our work here . Other work about the 2020 campaign built on this dataset . For example , one study used shared URL data to detect polarized communities on Twitter [ 61 ] , and showed that the polarization around the fraud claims on Twitter is evident without considering tweet content other than URLs . Another work , using a multi - platform dataset , used URLs to compare communications about voter fraud claims within several social media platforms , and found that social media content about election fraud differ in content and also timing across platforms [ 60 ] . Our work here aims to specifically examine what visual content practices are used during misinformation campaigns , by using the Stop the Steal campaign of the 2020 U . S . elections as a case study . While previous work [ 1 ] presented a basic exploration of the images used in this campaign , there is a gap in understanding the type of images used , their characteristics , the role they played in the campaign , and the overall dynamics of their spread as we do in this work . 3 DATASET This work uses the case study of the Stop the Steal campaign surrounding the 2020 U . S . election , focusing on the most popular images shared on Twitter as part of this campaign . To identify and study these images , we use the VoterFraud2020 dataset [ 1 ] , a multi - modal Twitter dataset with tweets and retweets that includes key phrases and hashtags related to voter fraud claims surrounding the U . S . 2020 election , collected between October 23rd and December 16th , 2020 . The VoterFraud2020 dataset includes 7 . 6M tweets and 25 . 6M retweets from 2 . 6 million unique users . The dataset includes an analysis that is able to categorize 73 . 8 % of the users in the data into two groups : promoters of voter fraud claims or detractors of these claims . The VoterFraud2020 dataset also contains the perceptual hash ( pHash ) values of the 167 , 696 media items – images and videos – that were shared in the tweets . Amongst them , 109 , 310 are unique pHash values . Perceptual hash values are binary strings designed such that the two hashes are identical when the two corresponding images perceptually seem identical . We used the pHash values [ 58 ] for matching near - duplicate images shared in this dataset , i . e . , finding all appearances of images across different tweets in the dataset . Matching pHash values is known to be effective in finding repetitions of the same image [ 85 ] . Using the pHash values and matches , we extracted the most popular images by promoters of voter fraud claims based on the VoterFraud2020 labels [ 1 ] . The popularity is defined as the number of shares ( tweets and retweets ) of the images across all tweets where they appeared , matched using the pHash value . We aim to specifically examine images that became highly popular in the campaign . We therefore analyzed only images that were shared at least 1 , 000 times each within the dataset , resulting in a set of 40 images . To examine the consistency and representativeness of these Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . 541 : 6 Hana Matatov , Mor Naaman , and Ofra Amir top forty most retweeted images , we also reviewed the following twenty images , and observed similar trends . We further discuss the representativeness of the top forty images in Section 7 . The top forty images are attached to this submission as a supplementary file , which also includes the twenty additional images we explored for validation . We provide further details and metadata about the images in the dataset , showing that the coverage is substantial but still includes some gaps . Details about the images are included in Table 7 in Appendix A , summarizing the popularity of the images , as well as the total number of tweets and retweets in the dataset by promoters of the claims and others . Note that even the most retweeted images appeared in just a few different original tweets in the dataset . For example , the image we refer to as most popular within the promoters cluster indeed was retweeted 10 , 424 times by promoters but only 20 times by detractors in our dataset . The image appeared in only eleven different original tweets in the dataset . As the original work presenting the dataset points out , the dataset is estimated to cover over 60 % of the content shared on Twitter using the selected voter fraud keywords [ 1 ] . This gap in coverage explains the difference between the number of image shares within the dataset and the total number of shares according to the Twitter - provided metadata ( as of December 16th , 2020 ) , as shown in Table 7 . For example , according to metadata , the tweets containing the most popular image referenced above were retweeted a total of 20 , 104 times . Furthermore , as noted above , 26 . 2 % of the users in the VoterFraud2020 dataset did not have promoters or detractors labels [ 1 ] , and thus the “Total Retweets in the Dataset” column may have higher values than the sum of the “Retweets by Promoters” and “Retweets by Detractors” columns in Table 7 . We validated the effectiveness of matching exact pHash values for our task , including an ex - ploration of the impact of using a different match threshold . As we show next , the matching in general proved to be effective in aggregating appearances of the same image , and false positives and negatives had minimal impact on the outcome . One false negative resulted in a single pair of almost - identical top images which had different pHash values ( images ranked as nine and ten ) ; we did not combine these images to maintain consistency in applying the matching criteria . To assess whether we are missing a set of matching near - duplicates and modified images that do not have exactly identical pHash values , we ran the same aggregation process , but grouping together images with similar ( i . e . , non - identical ) hash values , using a distance threshold . Drmic et al . [ 25 ] evaluated the robustness of different perceptual image hashing algorithms , and different distance parameters . The authors showed that even for quite aggressive image modifications , the pHash algorithm was successful , when used with a distance threshold equal to 14 ( hash values in a binary representation , with a hash size of 64 bits ; the distance is computed using Hamming distance ) . We followed the conclusions of Drmic et al . to group together images with pHash values with Hamming distance of 14 bits or lower . Even under this grouping , the 37 most popular images are the same as the top 37 images ( out of the top 40 images ) in the exact - match approach . Further , considering near - duplicates , the top ten images did not change their ranking at all compared to the original list ; and among the following 27 images ( 11 - 37 ) , only three images had a different relative rank ( shifting up between one to eight places ) . We conducted additional analyses to verify that not only the top forty images are not replaced by others , but that we are also not missing a significant number of shares of the original top forty images because of the strict matching . We used the near - duplicate matching approach , with the threshold of 14 bits , to find near - duplicates of the original list of top forty images . We observe that considering these near - duplicate matches only adds 15 . 5 shares per image on average – excluding a single image with an increase of additional 1 , 604 shares . This exception , however , can be considered a false positive match : the original image is a screenshot of a tweet , and the pHash - based near - duplicate match for this image resulted in several screenshots of other , different tweets . Overall , then , in our analysis , matching exact pHash Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . The Role and Dynamics of Visual Content in the 2020 U . S . Election Misinformation Campaign 541 : 7 values resulted in a low false positive rate , and at most only resulted in a minor difference in rank and share counts , justifying the exact - match approach described above . Our main unit of analysis is therefore an image , where we consider all images that have the same unique hash value as the same image . For each examined image , we explored the tweets and retweets where the image was shared . For each share , i . e . tweet or retweet , the VoterFraud2020 dataset provides extensive metadata , including the timestamp of the post , the user who posted it , the user’s cluster ( detractor or promoter of voter fraud claims ) , and the number of followers the user has . Nevertheless , the user’s detailed metadata in the VoterFraud2020 dataset is limited to users who posted an original tweet at least once within the dataset , as well as users that only retweeted within the dataset but remained active on February 1st 2021 , when their data was retroactively collected . The metadata of the users who posted original tweets was collected along with user’s first share in the dataset , and is timestamped in the original VoterFraud2020 dataset [ 1 ] . In our analysis , we found that for each analyzed image , fewer than 10 % of its shares had missing user metadata ( the average is approximately 7 % ) . Due to these gaps , in all further analyses regarding the number of followers and image exposure we ignore shares with missing user metadata . For all other analyses ( e . g . temporal patterns of spread ) , we considered all shares . As noted above , some of the media items in the VoterFraud2020 dataset were actually videos posted along with the tweets , appearing as media in the tweet metadata . In order to focus on images only , we viewed the top tweeted media items , using the Internet Archive 1 when the original tweet was not available , and skipped media items that were videos . There were a total of 31 videos in the dataset that exceeded our threshold of shares , and their popularity was interleaved with that of the 40 images in the data . 4 ANALYSIS OF IMAGE CATEGORIES Using the dataset described in Section 3 , we examined the forty most popular images , first taking a qualitative approach for categorizing and characterizing these images . This examination was done to understand the types of images that reached popularity in the campaign , and their role in it . Moreover , in the next sections ( sections 5 and 6 ) , we use some of these labels to analyze the different patterns of sharing and spread of these images . 4 . 1 Creating a Codebook for the Images To perform the qualitative analysis , we used the images as well as the context of how the images were shared in tweets . For each image , we examined the image itself and up to ten tweets that shared the image , looking at the tweets’ text and associated URLs if any . As Table 7 shows , some images were shared in fewer than 10 original tweets , in which case we reviewed all of them . For images shared in more than ten original tweets , we examined the first five tweets in our dataset that shared the image , and the most recent five tweets . The method and multi - label approach of generating the codebook was inspired by previous work aiming to characterize similar content , such as images [ 50 ] , memes [ 48 ] or posts [ 49 ] . Similarly to these papers , we developed the codebook using an iterative process composed of multiple steps . First , we examined the image dataset , identified the images’ most noticeable characteristics , and assigned these emergent labels to the images . This initial step was carried out by the first author , and the next steps were performed by the entire research team . Based on the initial assignment of multiple labels for each image , we organized the labels into a higher - level set of categories , i . e . , different dimensions that characterize the image . After defining the categories , we added more 1 https : / / web . archive . org / Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . 541 : 8 Hana Matatov , Mor Naaman , and Ofra Amir The Image PopularityRank 2 9 Type Photograph Text as image Origin Online media article Computer - generated Content Photographic image of voting Poster Role Evidence Claim of voter fraud , Call to action Table 1 . Two examples of images from the dataset , and their labels from each category . labels to make them more complete , and then iterated on redefining the categories , and so on . We repeated these steps , discussed and refined , until the codebook reached stability and agreement of the research team . The next section presents the categories and labels , their interpretation , and their assignment to the images . 4 . 2 Categories and Labels Our labeling procedure exposed the different types and uses of the popular images in the dataset , represented in a set of categories and labels we describe in detail below . The categories and their associated labels are summarized in the tables below , and include the categories Type ( Table 2 ) , Origin ( Table 3 ) , Content ( Table 4 ) , and Role ( Table 5 ) . The tables also include a description for each of the labels , along with the number of images that were assigned the label among the forty images we used as our data . Note that for some categories , noted below , multiple labels can be assigned to each image . For other categories , the labels are mutually exclusive . For additional illustration , Table 1 shows two of the images and their complete set of labels in each category . Image # 2 is a photograph presenting allegedly buried ballots that in some cases was tweeted along with links to articles presenting the image and text saying that this image is evidence of voter fraud . On the other hand , image # 9 includes text , rendered visually as a poster , and serving as a call - to - action while indirectly claiming the existence of voter fraud . The labels of the top forty images are attached to this paper as a supplementary file along with the images and their pHash values . Using these pHash values , our work is reproducible when used in conjunction with the VoterFraud2020 dataset [ 1 ] . The pHash values can be used across datasets , allowing researchers to expand the work ( and the application of the labels ) beyond Twitter . Type . This category captures the type of data presented in the image , with each image having exactly one Type label . Table 2 presents the labels , sorted by the number of images that were assigned to each label . These labels were developed separately , but echo closely the classification used by Marchal et al . for Twitter images shared in the 2019 EU election campaign [ 50 ] . As shown in the table , exactly half of the most popular images we examined were photographs – i . e . , what we would consider a regular visual image , produced with a camera . More surprisingly , 40 % of the images were actually text rendered as images or some other capture of content that is mostly text ( more on content below ) . Of the top ten images in our data , seven were photographs , and three were text - as - image . A possible explanation for the large fraction of text - as - image visual Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . The Role and Dynamics of Visual Content in the 2020 U . S . Election Misinformation Campaign 541 : 9 Type : what is the type of data presented in the image ? Photograph A photograph , visual image produced with a camera . 20 Text as image A rendering of text in visual format . 16 Graphic / Infographic Information presented in a graphical form , such as charts and graphs . 3 Meme An image with text inserted over it in order to convey a message . 1 Table 2 . Labels from Type category and their prevalence within the popular images content is that Twitter users used textual images as a way to attract visual attention [ 21 , 43 ] or to increase believability [ 79 ] . Notably , research on visual misinformation often focused on memes , deep fakes , photoshopped images , etc . [ 17 , 31 , 48 , 51 , 52 , 57 , 66 , 82 ] . While some images in our data include possibly - misleading imagery , none of the analyzed images were labeled as a manipulated photograph , and only one of the popular images was a photograph that was clearly edited ( as a meme ) [ 31 , 57 ] . Origin . The origin category captures the potential source of the image , to the extent it can be determined ( as we explain below ) . For this category , each image has exactly one label . Table 3 presents the labels , sorted by the number of images that were assigned to each label , and split into three themed sections . Origin : where did the data presented in the image come from ? Mediasource Online media article An image that also appeared in URLs that were attached to the tweets . 10 Media Screen Capture A photograph of a TV screen . 2 Individualmedia Document screenshot Screenshot of a document or an article . 6 Social media segment capture An image of a social media screen , post , or comment ( Twitter or other platform ) . 4 Other Computer - generated Computer - generated content ( e . g . poster , logo , infographic , or meme ) . 8 Snapshot A photograph from non - professional source . 6 Professional stock image A photograph of a public figure that was likely taken by professionals . 4 Table 3 . Labels from Origin category and their prevalence within the popular images . Almost a third ( 12 ) of the images in our set of forty popular images could be easily associated with an external media source outside of Twitter , either because these images originated in media articles that were attached to the tweets , or simply by observing that they are media screen captures from television or video channels . The most prevalent among these labels is the “online media article” label – i . e . , an image that was published in an article ( that was also shared within the tweets in the dataset ) . An additional quarter of the images also have their origins in media , but other media types which are more individual or personal , including screenshots from emails , documents , and social media platforms . In all , the common presence of such media content within the popular images indicates how Twitter was commonly used to spread ideas from other media platforms . For the rest of the images , which did not originate from other media , it was harder to identify the source . Instead , we selected labels that capture the process through which the images were created in our evaluation , including snapshots , images borrowed from stock imagery , and graphics created on a computer . Note that images labeled as “computer - generated” were only infographics , memes , or textual content such as posters , i . e . not manipulated photographs ( as noted above ) . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . 541 : 10 Hana Matatov , Mor Naaman , and Ofra Amir Content : what is being presented in the image ? P h o t o g r a p h i c Photographic image of voting Ballots , envelopes , their transportation process , etc . 10 Public figure A photo of a well - known person . 10 Broadcast or interview capture A photograph showing TV ( or other media source ) broadcast or an interview . 2 Protest A photo of a collective public action such as protest , demonstration . 1 T e x t u a l Quote A quote from a known individual , or that is implied as such . 5 Text from an article or document Part of an article or an official document . 4 Text by social media users Text written by anonymous or ordinary user on social media platforms . 3 Poster Short information , mostly textual , designed as a poster and intended to provide details or offer proposals . 3 Other textual content Twitter’s misinformation warning . 1 Other content Graphic / infographic or meme items . 4 Table 4 . Labels from Content category and their prevalence within the popular images . Content . This category captures the actual content of the image , attempting to broadly describe what one might see in each image in our set . For this category , each image has one or more labels . Table 4 presents the labels , split into three sections : photographic content , text - based content , and other . The labels in each section of the table are sorted by the number of images attached to them . A quarter of the most popular images referred to the voting process ( ballots , envelopes , etc . ) in some manner . The same number of images were labeled as presenting a public figure . Together , these two labels represent almost the entire subset of photographic images in the dataset . Perhaps unsurprisingly , the first label , photographic images of voting , was assigned to seven of the ten most popular images . While some of the images in this category attempt to present evidence of voter fraud , others showed generic representations of the voting process . For the textual images category , the most prevalent images were those rendering quotes from well - known individuals as an image , as well as images which are mostly text - based capture from articles or documents . These two labels represent different benefits of the use of visual medium . The “quote” images build on Twitter’s rendering of tweets with images to enhance the presentation of content and make it more appealing . The capture from other sources , including from other social media users , is used to share cross - platform information in an efficient and visual manner . Role . This category captures the role that the image was meant to play in the campaign , according to our interpretation of the image and its context . For this category , each image has one or more labels . Table 5 presents the labels , split according to the type of image , and sorted by the severity of the information that we think the images were trying to convey . In this category , we used the “evidence” and the “claim of voter fraud” labels when the image presented a photographic or textual ( respectively ) attempt to provide evidence of the alleged voter fraud . We labeled five images as “evidence” , and 13 as “claim of voter fraud” . The five images labeled as evidence present content such as lost ballots , discarded envelopes , and discarded ballots boxes . While only a few such images emerged , these images were some of the most commonly shared : four of these “evidence” images were included in the ten most shared images , and the fifth was ranked at 15 . While the “evidence” photographs were some of the most popular images in this dataset , the most common label assigned to images in our dataset was the “claims of voter fraud” label for text - based images . These 13 images included textual claims that voter fraud occurred , rendered as Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . The Role and Dynamics of Visual Content in the 2020 U . S . Election Misinformation Campaign 541 : 11 Role : what is the role of the image in the context of the campaign ? P h o t o g r a p h i c Evidence Provide ( alleged ) evidence of the existence of voter fraud . 5 Sow doubt in the election process or outcome Visual hints or indications in support of the existence of voter fraud , or images meant to sow doubt . 11 Illustration only Provide a visual that is related to the text in a tweet , but is not adding information , e . g . relevant stock photo . 8 T e x t u a l Claim of voter fraud Claims the existence of voter fraud in the election . 13 Call to action Direct call to act in some way to aid the campaign . 3 React against detractors of the claims Images that include detraction or suppression of voter fraud claims , usually with tweet text that confronts it . 2 Table 5 . Labels from Role category and their prevalence within the popular images . an image to increase visibility and aid spread . For example , personal stories that individuals shared about how they or their acquaintances were witness to voter fraud , or alleged legal and official documents about the subject ( e . g . , media advisory notice of hearing about election integrity ) . Eleven additional images presented visual hints as indirect photographic indication of the idea of fraud , by using snapshots related to the subject that are not necessarily presented as “evidence” , for example photos of arrests allegedly due to fraud , or suspicious scenes from the voting process that presented to sow doubt . These images do not provide any actual evidence such as photograph of stolen ballots , but rather present photographs that , along with the tweets’ text , encourage suspicion . The most popular image in the dataset , as well as the third most - shared image , were both assigned this label . The images showed moments from the process of scanning or transferring ballots , and the tweets accompanying them included text claiming wide - spread fraud . Finally , images were also used in tweets that claimed a disorder related to voting , but without having a direct photograph from the event , shared an illustrative image , usually with content related to the claim . For example , one popular image ( # 18 ) included a photo of the person that was mentioned in the text . We labeled eight images as illustrations , and none of them were among the 10 most popular images . Other labels in the textual type of image were less prevalent . Three images presented text that directly calls to action , for example , a poster calling to report on fraud events . This direct usage of images to motivate actions aligns with prior work that shows that images have a positive mobilizing effect in the context of online protest activity [ 15 ] . Two anomalous images presented suppression of voter fraud claims , for example , a screenshot of an article headline that denies the alleged fraud . These two images were probably included in our image dataset because of their context – they were published along with tweets that confronted the text in the images . 5 ANALYSIS OF THE TEMPORAL PATTERNS Building on the image dataset and labels described in sections 3 and 4 we can perform new analysis to better understand the spread of these popular images over time , identify typical patterns ( if any ) , and explore whether the temporal patterns are related to the images’ labels . In this section , we focus on the temporal dimension in the sharing of the images by Twitter users . We study the spread over time of tweets and retweets that included the images . Figure 1 presents cumulative graphs of the tweets and retweets which shared each image over time , for the images ranked 1 – 10 and 31 – 40 in popularity based on shares in our data ( the figures for images 11 – 30 were omitted because of space considerations , but presented similar patterns ) . These plots show the cumulative proportion of the total shares of the image in our dataset since its first appearance Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . 541 : 12 Hana Matatov , Mor Naaman , and Ofra Amir Fig . 1 . Cumulative graphs showing , for a subset of our images , what proportion of its total shares ( Y - axis ) each image received over the first seven days ( X - axis ) since it first appeared in our dataset . Above each graph is the index of the image and the total number of shares the image had in our the dataset . ( Y - axis ) , over the time since it was first shared ( X - axis ) , limited to the first seven days since the image appeared in the data . For example , the most popular image ( top left ) did not reach even half of its total eventual shares until after five days since its first appearance . However , as shown in the figure , most of the images peaked very quickly after their first appearance in the dataset . Indeed , most images reached close to 100 % of their shares within the first few days , and often within the first day ( see , for example , image # 4 ) . While several images – most notably , the top three images – had more than one significant period of rapid spread , most had only one period of rapid distribution . We did not discern any significant and consistent differences in the temporal patterns of how the images were shared depending on their category labels , or according to their relative rank within the forty most popular images . To better quantify the speed in which these popular images were spread , we calculated the half - life of the spread for each image , i . e . , the time until each image reached half of its total shares in the dataset . Figure 2 presents a histogram of the half - life values for all forty images , with the X - axis bins , in hours , representing the time from the first share of the image in our data until it reached its half - life . The Y - axis shows the number of images that reached their half - life in this time frame . Note that the histogram excludes image # 1 ( the most popular image ) , as its half - life was 114 . 4 hours . The histogram shows that a total of 19 images reached their half - life within three hours ( the sum of the left - most three columns ) . After seven hours at most , 75 % of the images already reached half of their total number of shares . To understand whether sharing of the images and actual exposure to them followed a similar spread , we also calculated the half - life of the image exposure : the amount of time until the images got half of their cumulative followers’ exposure . However , limited by the ( non - ) availability of network data from Twitter and in the VoterFraud2020 dataset , our analysis was done by simply adding up the number of followers of each user who shared the tweet ( excluding a small subset of users for whom we did not have the user’s metadata , as noted in Section 3 ) . Since users can have overlapping followers , this measure provides an upper bound of the actual cumulative followers’ Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . The Role and Dynamics of Visual Content in the 2020 U . S . Election Misinformation Campaign 541 : 13 Fig . 2 . Histogram of the time ( in hours ) between each image’s first appearance until it got half of their total shares in the dataset . exposure . The temporal patterns of exposure were very similar to the sharing half - life , with most of the images reaching their exposure half - life within 3 to 5 hours . Figure 3 shows the high correlation between shares and exposure ( r > . 98 , p < . 001 ) , and explores the difference in the patterns of sharing of images with different Role labels . The figure shows , for each image , the half - life in terms of shares ( X - axis ) and in terms of followers exposure ( Y - axis ) ( again excluding image # 1 ) . The size of each marker is proportional to the total number of shares of the image in our dataset . The color of each marker corresponds to key labels from the Role category ( Section 4 ) . The figure shows that for most images , the two half - life values were similar . Moreover , the figure shows that while some of the more popularly shared images had longer half - lives ( top right of the image ) , this trend was not entirely consistent . At the same time , the labels marker colors show that the popular images that served as photographic evidence of the existence of voter fraud ( “evidence” label in Section 4 ) tended to be the ones that took longer to be shared , with relatively high half - life values . In contrast , almost all images that were labeled as text claiming voter fraud , as well as other labels , had relatively short half - lives . We performed similar analysis to compare labels from the other categories , but those did not show any meaningful trends . The high correlation of shares and exposure suggests that the involvement of popular - and less - popular users remained relatively stable in the different stages of the images’ spread . We examine the patterns of the users participating in the spread in the next section . 6 ANALYSIS OF THE SHARING USERS While Section 5 focused on the temporal patterns of the popular images , here we turn our attention to the users who shared them and their characteristics . In particular , we examine the patterns of how popular - and less - popular users participated in sharing these images . We introduce an analysis that aims to explore whether some of these popular users were likely to participate more , or earlier , than others . Also , we include a qualitative review of the type of content shared by these more eager users . First , we explored the distribution of shares of the examined images , based on the popularity of users sharing them . Previous work documented what the researchers termed multi - directional par - ticipation in the Stop the Steal campaign [ 27 ] . The term meant to capture the fact that both popular - and less - popular users participated in the spread , and the influence flowed in both directions . Our analysis is geared to explore a similar question by looking at how users with different levels of popularity participated in the spread of the images in our dataset . As a general overview , Table 6 shows the size of the different accounts participating in sharing these images . The table shows the number of shares by each group of users , as well as the percentage of shares by this group of the total number of shares in the dataset . The table shows , for example , that only 16 shares were performed by users with more than one million followers , and none of the shares of the popular Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . 541 : 14 Hana Matatov , Mor Naaman , and Ofra Amir Fig . 3 . Comparison of the time it took for each image to reach its half - life in terms of shares ( X - axis ) , and in terms of total followers exposure ( Y - axis ) . The size of each marker is proportional to the total number of shares of the image in our dataset , and the colors represent different labels from Role category . images in the dataset was by a user with more than ten million followers . Moreover , most shares ( 88 . 3 % ) were done by users with relatively fewer followers , i . e . users with 5 , 000 followers at most , and almost half the shares by users with fewer than 500 followers . At the same time , these numbers expose a significant participation of accounts with a larger follower base , as over 5 % of the shares performed by users with more than 10 , 000 followers . Were there differences in participation of higher - following accounts , based on the type of images that were shared ? To explore this question , we examined the distribution of shares by users with different number of followers based on the image labels of the different categories . Figure 4 presents a histogram of participating users based on the major labels in the Role category : ‘evidence’ , denoting a photograph that provides ( alleged ) evidence of fraud ; ‘claim of voter fraud’ , denoting images which contain textual claims about the existence of fraud , and ‘other’ , for the rest of the Number of followers Shares ( and % of shares ) by this group 0 - 50 19 , 579 ( 15 . 4 % ) 50 - 100 10 , 463 ( 8 . 2 % ) 100 - 500 32 , 655 ( 25 . 6 % ) 500 - 1K 15 , 299 ( 12 % ) 1K - 5K 34 , 452 ( 27 . 1 % ) 5K - 10K 7 , 592 ( 6 % ) 10K - 100K 7 , 051 ( 5 . 5 % ) 100K - 1M 243 ( 0 . 2 % ) 1M - 10M 16 ( 0 . 01 % ) 10M + 0 ( 0 % ) Table 6 . The participation of users with different number of followers in spreading the popular images . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . The Role and Dynamics of Visual Content in the 2020 U . S . Election Misinformation Campaign 541 : 15 images . On the X - axis are the bins based on ranges of the number of followers of users that shared the image . The Y - axis captures the average percentage of the total shares of each image that was done by this group of users . In other words , we looked at all images with a certain label . Then , per each image , we calculated the percentage of shares of the image by users from each group , and present the average over all images with this label . For example , users with 50 followers or fewer ( left - most bin ) were responsible for , on average , roughly 15 % of the shares of “evidence” images . The figure shows that the distribution of users who shared the evidence images is different from users that shared the other types of images , with “evidence” shared more often by users with 1 , 000 or fewer followers , compared to the distribution for other types of images . To verify that these results are not due to the different popularity of the images with different labels , we repeated the analysis for the top four images from each of the three categories , which resulted in the same pattern . We revisit these findings and discuss what factors may have contributed to it in Section 7 . The patterns of participation did not change depending on the timing of sharing , e . g . early or late in the images’ spread . We looked at the participation rates during different times in the spread of the image ( e . g . , the first 500 and the following 500 shares ) . These sharing patterns in terms of the participation of different groups remained consistent across these trials . Because of their smaller numbers , Figure 4 does not include users with more than 100 , 000 followers . We note , however , that in our data , none of the users with more than a million followers shared an ‘evidence’ image , and that these images were also shared less by users with 100K - 1M followers compared to other images . Fig . 4 . A comparison of the average proportion of shares of images ( Y - axis ) by users with different levels of popularity ( number of followers ; X - axis ) . The different color columns represent images that correspond to specific labels from the Role category . Nevertheless , these influential users with more than 100K followers are still important to study and understand given the impact they might have on spread of content . We therefore examined whether there are consistent patterns in whom amongst the popular users participates and when . To perform this analysis , we ran a rank - order analysis of the 146 users in our data who had more than 100K followers . These users appeared in our data ( i . e . shared these top images ) 258 times , where 15 % of their shares were original tweets , and the rest were retweets . All these tweets include text in addition to the tweeted image . None of these retweets in our data were quote tweets , i . e . , none of them included additional text beyond the original tweet . The rank in our ordering was based on each user’s first instance of sharing of each image : what was their rank in sharing that Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . 541 : 16 Hana Matatov , Mor Naaman , and Ofra Amir image amongst the other popular users ( e . g . 1st , 2nd , etc . ) . This ranking was then aggregated across all images in our dataset . Figure 5 shows a heat - map visualization of the results . The figure includes the top 20 of these users amongst those who participated at least three times . The rows are the users ( represented by their Twitter handles ) and the number of Twitter followers they had when the data was collected . The columns show the frequency in which the user was ranked at this position , amongst these users , when sharing one of the images . For example , @ OANN ( top row ) was the first to share seven out of the images in our dataset . This finding is perhaps not surprising , as this account , belonging to the far - right cable TV One America News Network [ 12 , 23 ] , was a major participant in spreading the voter fraud claims in the media [ 2 ] . These seven images the account shared were all original tweets that include text , posted as promotion of OANN articles ( all these tweets include a URL link to OANN website ) . Four of the images which OANN shared have a similar pattern as they are all images presenting a photographic image of the voting process ; additional two images present a public figure ; and one presents a protest . Four of these seven images were labeled as photographic illustrations of the voting process ( as explained in Section 4 . 2 ) . More generally , Figure 5 shows that amongst the popular users , a significant number were consistently “early adopters” in image spread . Our qualitative analysis shows that they participated in different ways . Beyond @ OANN , the only user among the “early and often” users in Figure 5 who posted original tweets ( not only retweeted ) was @ Breaking911 , an account which also shared tweets that include texts and URLs to articles on their own website . This account posted two different images , one that presented alleged evidence for voter fraud existence , and a second that aims to sow doubt in the election process or outcome . Another early adopter , the right - wing activist Bill Postmus ( @ billpostmus ) , participated three times ( two first - place shares amongst the top users ) . All his shares were retweets , that is , he did not add any text or context when sharing these images . The far - right activist Ali Alexander ( @ ali ) , sometimes referred to as “the man behind Stop the Steal” [ 63 ] , shared five images ; for four of them he was the third of the popular users to share the image . All of Ali’s shares were retweets . Four of the images he retweeted were labeled as evidence and one as call to action ( Section 4 . 2 ) . To summarize , these “early and often” users shared different types of content , and participated in a mixed manner : those who posted new original tweets mostly did that to promote their own media articles ; while others usually retweeted content from less - popular users . Generally speaking , the results and analysis in this section provide a clearer view of the patterns of participation amongst the most popular users in the dataset , and provide a new way to understand the most active popular users in this campaign . 7 DISCUSSION AND CONCLUSIONS Despite the prevalence of images in social media misinformation campaigns [ 15 , 20 , 28 , 48 , 64 , 78 , 82 ] , and a wide array of research examining visual content like images and memes in the context of social media [ 41 , 83 , 84 ] , research specifically focused on how images play a role in misinformation campaigns , as we do with this case study , has been limited . Similarly , research on election - related misinformation campaigns , including in the context of the U . S . 2020 elections [ 7 , 27 , 29 , 30 , 60 ] , had not focused on the detailed analysis of the type of images and their use , as we do here . Our analysis exposed a wide array of types and uses of images in the Stop the Steal campaign . Only about half of the forty most popular images were actual photographs , and many of the popular images were text - based , rendered as image for various purposes . Partly , these text - based images were taking advantage of the Twitter presentation of images to maximize attention [ 21 , 43 , 79 ] . In other cases , the text - based images allowed users to bring in content from other contexts , e . g . other social media posts or documents , or to use graphic layouts to create a better call - to - action . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . The Role and Dynamics of Visual Content in the 2020 U . S . Election Misinformation Campaign 541 : 17 Fig . 5 . The earliest users , amongst those with at least 100K followers , to share images in our dataset ( shows only users with at least three shares ) . For each user ( row ; also showing number of followers ) , we show how many times in our dataset they shared an image first , second , third , fourth ( columns ) amongst these users , and how many additional images they shared ( “5th + ” column ) . Such mixed use of images on Twitter was also observed in the context of a more general political campaign [ 50 ] . Even within the photographic content , only very few of the popular photos aimed to present direct “evidence” of voter fraud . We do not know from this analysis whether there were many other such images that did not succeed in getting wide spread for some reason . Regardless , the evidence photographs which did make it to our dataset of top images were quite successful ( most of them in the top 10 most shared images ) . Other photographs in the top images dataset were posted as visual imagery that suggested , in tone or content , the “menace” of voter fraud , or simply provided an illustration of the content of the tweet in some way . These illustration photos likely attempted to make use of the visual content to enhance attention , persuasion , and emotions [ 21 , 38 , 43 , 79 ] as well as encourage mobilization [ 15 ] . We do not know if the tweets , without the illustrative images , would have been as successful . Such analysis is difficult to perform and control for , though past work had provided evidence that “images evoking enthusiasm , anger , and fear should be particularly mobilizing” [ 15 ] . We were surprised to notice several key missing or underrepresented categories amongst the popular images , most interestingly memes and manipulated photographs . While the presented “evidence” photos were clearly serving as misinformation , these photos were , to the best of our knowledge , not manipulated , but instead usually just presented out of context using the accompanied text . Fears of widespread use of deep fakes and other photo manipulations in the election [ 24 ] did not significantly play out in this specific campaign , or at least in our dataset ( more on its limitations below ) . This gap , perhaps , was due to the fact that deep fakes were not needed : the people sharing the misleading content were successful in both spreading and the reception of the alleged fraud claims [ 8 , 14 , 27 , 34 , 62 , 71 ] . This finding aligns with other reports ( e . g . , [ 28 ] ) . Visual memes are another type of content that we were surprised was only sparsely represented in the set of popularly - shared images ( one in our dataset of forty ) . Given the popularity of memes Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . 541 : 18 Hana Matatov , Mor Naaman , and Ofra Amir in other platforms [ 83 ] , the role they play in political campaigns [ 82 ] and in politically oriented discussion boards [ 48 ] , we expected memes to be more highly represented in the data . We can only speculate about the reason for this absence . One such suggestion is that while memes work well on other platforms , they do not match the severity and anger that is expressed as part of the Stop the Steal campaign on Twitter . Much of the attention in the area of visual misinformation focuses on the use of manipulated or fabricated visual content ( e . g . , photoshopped content , deep fakes , memes , etc . ) [ 17 , 51 , 52 , 66 ] , or on sharing of genuine images out of their real context ( e . g . , sharing an image from another event , or reusing an old image ) [ 28 , 78 ] . Having no manipulated photographs and only one meme in our dataset suggests that systems for detecting visual misinformation in similar contexts , i . e . , political misinformation campaigns on Twitter ( or other similar social media platforms ) , should consider investing their efforts in other attributes of the shared post such as date and account characteristics , perhaps alongside the efforts of looking for manipulated content suggested in several recent works [ 6 , 24 , 52 , 81 , 87 ] . On the other hand , our findings do show that out - of - context images as visual misinformation , a “cheap fake” technique , is indeed important to address [ 28 , 40 , 54 , 55 , 59 , 78 ] . Further , given that many of the popular images were text or quotes rendered as an image , a reasonable direction for mitigating misinformation may be using text recognition and extraction techniques to make these textual images searchable and allow detection of misinformation in tweets even when there are no suspicious keywords in the post itself . Overall , our results can also serve as a “compass” for researchers and practitioners in the field of visual communication and misinformation , helping them to prioritize the most prevalent types of visual content in misinformation campaigns . Our analysis also focused on participation of users with different levels of popularity ( in terms of number of followers ) . The results are consistent with earlier reports showing the “multidirectional” nature of sharing misinformation in the Stop the Steal campaign , where popular - and less - popular users participated in the spread [ 27 ] . Moreover , our analysis showed no evidence that this co - participation changed over the lifetime of the spread of the images , with all groups of users participating in the sharing from the early instances of the image shares in our data . We do show , however , that lower - level accounts were somewhat more involved with spreading “evidence” images . It is possible that these images simply received more attention from “spammy” or “bot” accounts , perhaps operated by disinformation actors . However , high - follower - count accounts may also include Spam - like disinformation actors , e . g . , “followback networks” where accounts follow each other to intentionally create high follower numbers , which then tend to retweet in high frequency without exercising much discretion [ 32 ] . Another possible explanation of the observation that high - level accounts were less involved with spreading “evidence” images is that these popular users were careful about spreading more explicit misinformation , i . e . images that can get them banned from Twitter , risking loss of their followers , or simply concerned about their reputation . As a result , these users may suggest but do not state explicitly that fraud occurs – unlike their “foot soldiers” . Whether they were more careful or not , our results also identified some of the most active participants amongst the very top users ( those with more than 100 , 000 followers ) . We perform a novel analysis that shows whom amongst these users participated “early and often” in sharing the popular images . This analysis provided a new angle into the activity of influential repeat spreaders , known to play a role in the Stop the Steal campaign [ 27 , 30 , 33 ] , i . e . understanding the timing of their participation . Our results also expose and provide implications for the challenging landscape for moderation of misinformation . Moderation can apply to content and to accounts , and may include content removal , attaching warning labels , account suspension , etc . Moderation is a critical tool employed by platforms to curb the spread of misinformation [ 35 , 65 ] , including during the U . S . 2020 elec - tions [ 1 , 19 , 27 ] . Just like we used pHash to track appearances of the same image across tweets , Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . The Role and Dynamics of Visual Content in the 2020 U . S . Election Misinformation Campaign 541 : 19 moderation can make use of images to identify misinformation tweets with the same image content . However , as we had shown in our study , many of the popular images used in this campaign , and especially the photographic images , may not by themselves have violated content policies such as those that require identification of content manipulation . It was only the additional context , e . g . text associated with these images , that made them problematic . Even if image tracking could be successfully employed to track misinformation images , there were still a number of key challenges for moderation . First , as we have seen , these images spread quickly in our data , with a large majority reaching half of its shares within just a few hours , perhaps not enough ( for some platforms ) to make a timely decision that can help moderate their spread . Further , while most images were shared heavily by retweeting a few original tweets , the mixed - participation pattern with shares by users of different levels of popularity would make it hard to control the spread , e . g . , by suspending selected users as moderation . Nevertheless , we show that repeat and early offenders exist within the popular account group , which may offer guidelines for selective enforcement . Finally , we should acknowledge some limitations of this work . There are a number of unknown factors that potentially impact the breadth and introduce bias in the dataset we used in this work [ 1 ] . First , the dataset was constructed by streaming from Twitter content with keywords related to “Stop the Steal” , as defined by the authors [ 1 ] . As a result , images that are relevant but that were not included in tweets with the correct keywords are not represented in our data . Further , based on the analysis in the original dataset paper , the data streamed represents up to 60 % of the content posted on Twitter for these keywords [ 1 ] . It is unknown whether there is a bias in the data that was included in the stream – e . g . whether it was more likely to include tweets from popular users , or automatically excluded suspected spam accounts . Nevertheless , we do not have indication that the stream was biased in any significant way ; and while it is possible that related tweets ( and images ) were posted without using any of the streamed keywords , we assume that a substantial fraction of the popular images were indeed shared with the popular keywords used by the Stop the Steal campaign . Another limitation is the focus on a small number of popular images . We only looked at forty images , and patterns may be different for images that are less popular . However , the intent of this work was precisely to focus on the images that were shared , and seen , the most . Nevertheless , to evaluate the consistency and representativeness of these forty images , we also reviewed the next twenty images that were most shared by promoters of voter fraud claims . Overall , we observed similar patterns in these extra twenty images , in terms of image type , content , and the other categories that characterize them . We also note that the participant analysis is limited because we only have access to the number of followers of users in our dataset , and limited information about their networks beyond these counts as it is not part of the VoterFraud2020 dataset [ 1 ] . The lack of access to the set of followers for each participating account prevents us from calculating the overlap between the followers of different accounts , which in turn greatly inflates the images followers’ estimated exposure values . Thus , our potential follower exposure analysis presents an upper bound for the actual half - life of the images’ exposure . Of course , our results are limited to a single social media platform , and single campaign – though clearly an influential and consequential one [ 7 , 14 , 27 , 34 ] . Still , we do not expect that our precise findings will generalize to other platforms and settings . However , the methods we used in this work can be applied in other contexts , and the insights provided about the use of images in this specific campaign contribute to inform our understanding of the Stop the Steal campaign with its consequences , which are still playing out at the time of publication , and will continue to impact the state of democracy in the U . S . and beyond for a long time to come . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . 541 : 20 Hana Matatov , Mor Naaman , and Ofra Amir ACKNOWLEDGMENTS This material is based upon work partially supported by the National Science Foundation un - der grants SaTC - 2120651 and IIS - 1840751 . We thank the Jacobs Technion - Cornell Institute for supporting this project . A APPENDIX A : IMAGE DATASET DETAILS ImagePopularityRank RetweetsbyPromoters RetweetsbyDetractors TotalRetweetsintheDataset TotalTweetsintheDataset TotalRetweets AccordingtoMetadata 1 10424 20 12995 11 20104 2 10250 140 12370 34 28833 3 7076 16 8847 9 13769 4 5698 42 7524 9 18979 5 5619 23 6513 31 15896 6 5521 21 6385 19 15597 7 5518 21 6378 12 15594 8 4902 54 6335 9 19403 9 4090 43 5053 14 13414 10 3924 12 4837 45 32050 11 3288 9 4015 2 5547 12 2619 5 3203 100 8699 13 2012 3 2345 36 6349 14 1992 0 2451 6 3020 15 1980 3 2372 23 3748 16 1965 1 2465 1 3968 17 1954 0 2236 2 2900 18 1898 7 2373 2 4394 19 1805 3 2148 1 4033 20 1765 4 2164 1 2501 21 1724 4 2061 2 4738 22 1628 2 2053 1 2202 23 1571 4 2085 11 2261 24 1512 3 1792 4 6110 25 1507 0 1607 5 1690 26 1494 3 1772 1 6068 27 1466 7 1828 1 3262 28 1416 0 1507 6 1588 29 1405 1 1516 6 1639 30 1374 6 1589 1 1764 31 1374 2 1840 1 2046 32 1374 6 1589 1 1764 33 1366 1 1746 3 2109 34 1348 5 1580 1 1661 35 1339 2 1690 1 1799 36 1332 0 1436 1 3634 37 1332 0 1436 1 3634 38 1148 4 1523 4 1734 39 1099 0 1399 4 1557 40 1035 5 1231 1 4975 Table 7 . The forty images used in our data , i . e . the images most shared by promoters of voter fraud claims , and their data . REFERENCES [ 1 ] Anton Abilov , Yiqing Hua , Hana Matatov , Ofra Amir , and Mor Naaman . 2021 . VoterFraud2020 : a Multi - modal Dataset of Election Fraud Claims on Twitter . In Proceedings of the International AAAI Conference on Web and Social Media , Vol . 15 . 901 – 912 . [ 2 ] Rachel Abrams . 2021 . One America News Network Stays True to Trump . https : / / www . nytimes . com / 2021 / 04 / 18 / business / media / oan - trump . html . [ accessed 13 - Jan - 2022 ] . [ 3 ] Davey Alba and Sheera Frenkel . 2021 . Watch out for this misinformation when Congress meets to certify the election . https : / / www . nytimes . com / 2021 / 01 / 06 / business / election - fraud . html . [ accessed 6 - Jan - 2022 ] . [ 4 ] Hunt Allcott and Matthew Gentzkow . 2017 . Social media and fake news in the 2016 election . Journal of Economic Perspectives 31 , 2 ( 2017 ) , 211 – 36 . [ 5 ] Ahmer Arif , Leo Graiden Stewart , and Kate Starbird . 2018 . Acting the part : Examining information operations within # BlackLivesMatter discourse . Proceedings of the ACM on Human - Computer Interaction 2 , CSCW ( 2018 ) . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . The Role and Dynamics of Visual Content in the 2020 U . S . Election Misinformation Campaign 541 : 21 [ 6 ] Domagoj Bebić and Marija Volarevic . 2018 . Do not mess with a meme : the use of viral content in communicating politics . Communication & Society 31 , 3 ( 2018 ) , 43 – 56 . [ 7 ] Yochai Benkler , Casey Tilton , Bruce Etling , Hal Roberts , Justin Clark , Robert Faris , Jonas Kaiser , and Carolyn Schmitt . 2020 . Mail - In Voter Fraud : Anatomy of a Disinformation Campaign . Technical Report . Berkman Center Research Publication . https : / / papers . ssrn . com / sol3 / papers . cfm ? abstract _ id = 3703701 [ 8 ] Nicolas Berlinski , Margaret Doyle , Andrew M Guess , Gabrielle Levy , Benjamin Lyons , Jacob M Montgomery , Brendan Nyhan , and Jason Reifler . 2021 . The Effects of Unsubstantiated Claims of Voter Fraud on Confidence in Elections . Technical Report . https : / / cpb - us - e1 . wpmucdn . com / sites . dartmouth . edu / dist / 5 / 2293 / files / 2021 / 03 / voter - fraud . pdf [ 9 ] Rahul Bhargava , Cindy Bishop , and Ethan Zuckerman . 2020 . Mapping and Visualizing News Images for Media Research . In Proceedings of Computation + Journalism Symposium . [ 10 ] Md Momen Bhuiyan , Michael Horning , Sang Won Lee , and Tanushree Mitra . 2021 . NudgeCred : Supporting News Credibility Assessment on Social Media Through Nudges . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW2 ( 2021 ) . [ 11 ] Alexandre Bovet and Hernán A Makse . 2019 . Influence of fake news in Twitter during the 2016 US presidential election . Nature communications 10 , 1 ( 2019 ) , 1 – 14 . [ 12 ] Ali Breland . 2020 . Meet the Propagandists and Conspiracy Theorists Behind the One America News Network . https : / / www . motherjones . com / politics / 2020 / 06 / one - america - news - network - staff / . [ accessed 14 - Jan - 2022 ] . [ 13 ] Philip Bump . 2021 . The voter - fraud allegation that perfectly captures the post - 2020 Republican Party . https : / / www . washingtonpost . com / politics / 2021 / 10 / 22 / voter - fraud - allegation - that - perfectly - captures - post - 2020 - republican - party / . [ accessed 11 - Jan - 2022 ] . [ 14 ] Bill Mears Caitlin McFall . 2021 . Trump told DOJ officials in December 2020 to call election ’corrupt , ’ notes show . https : / / www . foxnews . com / politics / trump - doj - officials - election - corrupt - donoghue - notes . [ accessed 10 - Jan - 2022 ] . [ 15 ] Andreu Casas and Nora Webb Williams . 2019 . Images that matter : Online protests and the mobilizing role of pictures . Political Research Quarterly 72 , 2 ( 2019 ) , 360 – 375 . [ 16 ] EmilyChen , AshokDeb , andEmilioFerrara . 2020 . # Election2020 : thefirstpublicTwitterdatasetonthe2020USpresidential election . Technical Report . https : / / arxiv . org / abs / 2010 . 00600 [ 17 ] Bobby Chesney and Danielle Citron . 2019 . Deep fakes : A looming challenge for privacy , democracy , and national security . Calif . L . Rev . 107 ( 2019 ) , 1753 . [ 18 ] Kate Conger . 2020 . Twitter says it labeled 0 . 2 % of all election - related tweets as disputed . https : / / nytimes . com / 2020 / 11 / 12 / technology / twitter - says - it - labeled - 0 - 2 - of - all - election - related - tweets - as - disputed . html . [ accessed 10 - Jan - 2022 ] . [ 19 ] Kate Conger . 2021 . Twitter , in Widening Crackdown , Removes Over 70 , 000 QAnon Accounts . https : / / nytimes . com / 2021 / 01 / 11 / technology / twitter - removes - 70000 - qanon - accounts . html . [ accessed 12 - Jan - 2022 ] . [ 20 ] Catherine Corrigall - Brown . 2012 . The power of pictures : Images of politics and protest . American Behavioral Scientist 56 , 2 ( 2012 ) , 131 – 134 . [ 21 ] Scott Counts and Kristie Fisher . 2011 . Taking it all in ? visual attention in microblog consumption . In Proceedings of the International AAAI Conference on Web and Social Media , Vol . 5 . [ 22 ] Anh Dang , Abidalrahman Moh’d , Anatoliy Gruzd , Evangelos Milios , and Rosane Minghim . 2017 . An offline – online visual framework for clustering memes in social media . In From Social Data Mining and Analysis to Prediction and Community Detection . Springer , 1 – 29 . [ 23 ] Oliver Darcy . 2020 . Meet OAN , the little - watched right - wing news channel that Trump keeps promoting . https : / / edition . cnn . com / 2020 / 05 / 08 / media / one - america - news - trump / index . html . [ accessed 17 - Jan - 2022 ] . [ 24 ] Nicholas Diakopoulos and Deborah Johnson . 2021 . Anticipating and addressing the ethical implications of deepfakes in the context of elections . New Media & Society 23 , 7 ( 2021 ) , 2072 – 2098 . [ 25 ] Andrea Drmic , Marin Silic , Goran Delac , Klemo Vladimir , and Adrian S Kurdija . 2017 . Evaluating robustness of perceptual image hashing algorithms . In 2017 40th International Convention on Information and Communication Technology , Electronics and Microelectronics ( MIPRO ) . IEEE , 995 – 1000 . [ 26 ] Abhimanyu Dubey , Esteban Moro , Manuel Cebrian , and Iyad Rahwan . 2018 . Memesequencer : Sparse matching for embedding image macros . In Proceedings of the 2018 World Wide Web Conference . 1225 – 1235 . [ 27 ] Election Integrity Partnership . 2021 . The Long Fuse : Misinformation and the 2020 Election . https : / / purl . stanford . edu / tr171zs0069 . [ v1 . 3 . 0 ; accessed 12 - Jan - 2022 ] . [ 28 ] Lisa Fazio . 2020 . Out - of - context photos are a powerful low - tech form of misinformation . https : / / theconversation . com / out - of - context - photos - are - a - powerful - low - tech - form - of - misinformation - 129959 . [ accessed 15 - Jan - 2022 ] . [ 29 ] Emilio Ferrara , Herbert Chang , Emily Chen , Goran Muric , and Jaimin Patel . 2020 . Characterizing social media manipulation in the 2020 US presidential election . First Monday 25 , 11 ( 2020 ) . [ 30 ] Sheera Frenkel . 2020 . How Misinformation ‘Superspreaders’ Seed False Election Theories . https : / / nytimes . com / 2020 / 11 / 23 / technology / election - misinformation - facebook - twitter . html . [ accessed 14 - Jan - 2022 ] . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . 541 : 22 Hana Matatov , Mor Naaman , and Ofra Amir [ 31 ] Noam Gal , Limor Shifman , and Zohar Kampf . 2016 . “It gets better” : Internet memes and the construction of collective identity . New media & society 18 , 8 ( 2016 ) , 1698 – 1714 . [ 32 ] Erin Gallagher . 2019 . Trump Trains . https : / / erin - gallagher . medium . com / trump - trains - 84bea1c3170d . [ accessed 20 - Jan - 2022 ] . [ 33 ] Ryan J Gallagher , Larissa Doroshenko , Sarah Shugars , David Lazer , and Brooke Foucault Welles . 2021 . Sustained online amplification of COVID - 19 elites in the United States . Social Media + Society 7 , 2 ( 2021 ) , 20563051211024957 . [ 34 ] Amy Gardner . 2021 . ‘I just want to find 11 , 780 votes’ : In extraordinary hour - long call , Trump pressures Georgia secretary of state to recalculate the vote in his favor . https : / / www . washingtonpost . com / politics / trump - raffensperger - call - georgia - vote / 2021 / 01 / 03 / d45acb92 - 4dc4 - 11eb - bda4 - 615aaefd0555 _ story . html . [ accessed 18 - Jan - 2022 ] . [ 35 ] Tarleton Gillespie . 2018 . Custodians of the Internet . Yale University Press . [ 36 ] Tarleton Gillespie . 2020 . Content moderation , AI , and the question of scale . Big Data & Society 7 , 2 ( 2020 ) , 2053951720943234 . [ 37 ] Yevgeniy Golovchenko , Cody Buntain , Gregory Eady , Megan A Brown , and Joshua A Tucker . 2020 . Cross - Platform State Propaganda : Russian Trolls on Twitter and YouTube During the 2016 US Presidential Election . The International Journal of Press / Politics 25 , 3 ( 2020 ) , 357 – 389 . [ 38 ] Doris A Graber . 1996 . Say it with pictures . The annals of the American academy of political and social science 546 , 1 ( 1996 ) , 85 – 96 . [ 39 ] Nir Grinberg , Kenneth Joseph , Lisa Friedland , Briony Swire - Thompson , and David Lazer . 2019 . Fake news on Twitter during the 2016 US presidential election . Science 363 , 6425 ( 2019 ) , 374 – 378 . [ 40 ] Aditi Gupta , Hemank Lamba , Ponnurangam Kumaraguru , and Anupam Joshi . 2013 . Faking sandy : characterizing and identifying fake images on twitter during hurricane sandy . In Proceedings of the 22nd international conference on World Wide Web . 729 – 736 . [ 41 ] Tim Highfield and Tama Leaver . 2016 . Instagrammatics and digital methods : Studying visual social media , from selfies and GIFs to memes and emoji . Communication Research and Practice 2 , 1 ( 2016 ) , 47 – 62 . [ 42 ] Kevin Howley . 2016 . ‘I have a drone’ : Internet memes and the politics of culture . Interactions : Studies in Communication & Culture 7 , 2 ( 2016 ) , 155 – 175 . [ 43 ] Yu - Chen Hsieh and Kuo - Hsiang Chen . 2011 . How different information types affect viewer’s attention on internet advertising . Computers in human Behavior 27 , 2 ( 2011 ) , 935 – 945 . [ 44 ] Farnaz Jahanbakhsh , Amy X Zhang , Adam J Berinsky , Gordon Pennycook , David G Rand , and David R Karger . 2021 . Exploring lightweight interventions at posting time to reduce the sharing of misinformation on social media . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW1 ( 2021 ) . [ 45 ] Maurice Jakesch , Kiran Garimella , Dean Eckles , and Mor Naaman . 2021 . Trend Alert : A Cross - Platform Organization Manipulated Twitter Trends in the Indian General Election . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW2 ( 2021 ) . [ 46 ] Shagun Jhaver , Sucheta Ghoshal , Amy Bruckman , and Eric Gilbert . 2018 . Online harassment and content moderation : The case of blocklists . ACM Transactions on Computer - Human Interaction ( TOCHI ) 25 , 2 ( 2018 ) , 1 – 33 . [ 47 ] David MJ Lazer , Matthew A Baum , Yochai Benkler , Adam J Berinsky , Kelly M Greenhill , Filippo Menczer , Miriam J Metzger , Brendan Nyhan , Gordon Pennycook , David Rothschild , et al . 2018 . The science of fake news . Science 359 , 6380 ( 2018 ) , 1094 – 1096 . [ 48 ] Chen Ling , Ihab AbuHilal , Jeremy Blackburn , Emiliano De Cristofaro , Savvas Zannettou , and Gianluca Stringhini . 2021 . Dissecting the meme magic : Understanding indicators of virality in image memes . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW1 ( 2021 ) . [ 49 ] Claudia A López and Brian S Butler . 2013 . Consequences of content diversity for online public spaces for local communities . In Proceedings of the 2013 conference on Computer supported cooperative work . 673 – 682 . [ 50 ] Nahema Marchal , Lisa - Maria Neudert , Bence Kollanyi , and Philip N Howard . 2021 . Investigating visual content shared over Twitter during the 2019 EU parliamentary election campaign . Media and Communication 9 , 1 ( 2021 ) , 158 – 170 . [ 51 ] Francesco Marra , Diego Gragnaniello , Davide Cozzolino , and Luisa Verdoliva . 2018 . Detection of gan - generated fake images over social networks . In 2018 IEEE Conference on Multimedia Information Processing and Retrieval ( MIPR ) . IEEE , 384 – 389 . [ 52 ] Francesco Marra , Diego Gragnaniello , Luisa Verdoliva , and Giovanni Poggi . 2019 . Do gans leave artificial fingerprints ? . In 2019 IEEE Conference on Multimedia Information Processing and Retrieval ( MIPR ) . IEEE , 506 – 511 . [ 53 ] Alice Marwick and Rebecca Lewis . 2017 . Media manipulation and disinformation online . New York : Data & Society Research Institute ( 2017 ) . [ 54 ] Hana Matatov , Adina Bechhofer , Lora Aroyo , Ofra Amir , and Mor Naaman . 2019 . DejaVu : a system for journalists to collaboratively address visual misinformation . In Proceedings of Computation + Journalism Symposium . [ 55 ] Hana Matatov , Mor Naaman , and Ofra Amir . 2022 . Dataset and case studies for visual near - duplicates detection in the context of social media . https : / / doi . org / 10 . 48550 / arXiv . 2203 . 07167 . ( 2022 ) . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . The Role and Dynamics of Visual Content in the 2020 U . S . Election Misinformation Campaign 541 : 23 [ 56 ] Nicholas Micallef , Mihai Avram , Filippo Menczer , and Sameer Patil . 2021 . Fakey : A Game Intervention to Improve News Literacy on Social Media . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW1 ( 2021 ) . [ 57 ] Kate M Miltner . 2018 . Internet memes . The SAGE handbook of social media 55 ( 2018 ) , 412 – 428 . [ 58 ] Vishal Monga and Brian L Evans . 2006 . Perceptual image hashing via feature points : performance evaluation and tradeoffs . IEEE transactions on Image Processing 15 , 11 ( 2006 ) , 3452 – 3465 . [ 59 ] Daniel Moreira , Aparna Bharati , Joel Brogan , Allan Pinto , Michael Parowski , Kevin W Bowyer , Patrick J Flynn , Anderson Rocha , and Walter J Scheirer . 2018 . Image provenance analysis at scale . IEEE Transactions on Image Processing 27 , 12 ( 2018 ) , 6109 – 6123 . [ 60 ] Isabel Murdock , Kathleen M Carley , and Osman Yagan . 2021 . Multi - Platform Analysis of 2020 US Election Fraud and Protest Related Posts . Technical Report . [ 61 ] Sreeja Nair and Adriana Iamnitchi . 2021 . The Polarized Web of the Voter Fraud Claims in the 2020 US Presidential Election . In Proceedings of the International AAAI Conference on Web and Social Media . [ 62 ] NPR . 2020 . Poll : Just A Quarter Of Republicans Accept Election Outcome . https : / / npr . org / 2020 / 12 / 09 / 944385798 / poll - just - a - quarter - of - republicans - accept - election - outcome . [ accessed 10 - Jan - 2022 ] . [ 63 ] Luke O’Brien . 2021 . How Republican Politics ( And Twitter ) Created Ali Alexander , The Man Behind ‘Stop The Steal’ . https : / / www . huffpost . com / entry / republicans - twitter - ali - alexander - stop - the - steal _ n _ 6026fb26c5b6f88289fbab57 . [ ac - cessed 25 - Aug - 2022 ] . [ 64 ] Abby Ohlheiser . 2020 . Seven types of election misinformation to watch out for . https : / / www . technologyreview . com / 2020 / 11 / 02 / 1011543 / election - 2020 - sources - of - misinformation / . [ accessed 19 - Jan - 2022 ] . [ 65 ] OrestisPapakyriakopoulos , JuanCarlosMedinaSerrano , andSimonHegelich . 2020 . ThespreadofCOVID - 19conspiracy theories on social media and the effect of content moderation . Harvard Kennedy School Misinformation Review 10 ( 2020 ) . [ 66 ] Britt Paris and Joan Donovan . 2019 . Deepfakes and Cheap Fakes . Technical Report . Data & Society . https : / / datasociety . net / library / deepfakes - and - cheap - fakes / [ 67 ] Linda Qiu . 2021 . Fact - Checking Falsehoods on Mail - In Voting . https : / / www . nytimes . com / article / fact - checking - mail - in - voting . html . [ accessed 16 - Jan - 2022 ] . [ 68 ] Marian - Andrei Rizoiu , Timothy Graham , Rui Zhang , Yifei Zhang , Robert Ackland , and Lexing Xie . 2018 . # debatenight : The role and influence of socialbots on twitter during the 1st 2016 us presidential debate . In Proceedings of the International AAAI Conference on Web and Social Media , Vol . 12 . [ 69 ] Joseph Seering . 2020 . Reconsidering Community Self - Moderation : the Role of Research in Supporting Community - Based Models for Online Content Moderation . Proc . ACM Hum . - Comput . Interact 3 ( 2020 ) . [ 70 ] Francesca Spezzano , Anu Shrestha , Jerry Alan Fails , and Brian W Stone . 2021 . That’s Fake News ! Reliability of News When Provided Title , Image , Source Bias & Full Article . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW1 ( 2021 ) . [ 71 ] PRRI Staff . 2021 . Competing Visions of America : An Evolving Identity or a Culture Under Attack ? Findings from the 2021 American Values Survey . https : / / www . prri . org / research / competing - visions - of - america - an - evolving - identity - or - a - culture - under - attack / . [ accessed 10 - Jan - 2022 ] . [ 72 ] Kate Starbird , Ahmer Arif , and Tom Wilson . 2019 . Disinformation as collaborative work : Surfacing the participatory nature of strategic information operations . Proceedings of the ACM on Human - Computer Interaction 3 , CSCW ( 2019 ) . [ 73 ] Kate Starbird , Jim Maddock , Mania Orand , Peg Achterman , and Robert M Mason . 2014 . Rumors , false flags , and digital vigilantes : Misinformation on twitter after the 2013 boston marathon bombing . IConference 2014 proceedings ( 2014 ) . [ 74 ] Leo G Stewart , Ahmer Arif , and Kate Starbird . 2018 . Examining trolls and polarization with a retweet network . In Proc . ACM WSDM , workshop on misinformation and misbehavior mining on the web , Vol . 70 . [ 75 ] Jacqueline Ryan Vickery . 2014 . The curious case of Confession Bear : The reappropriation of online macro - image memes . Information , Communication & Society 17 , 3 ( 2014 ) , 301 – 325 . [ 76 ] Jessica Vitak , Paul Zube , Andrew Smock , Caleb T Carr , Nicole Ellison , and Cliff Lampe . 2011 . It’s complicated : Facebook users’ political participation in the 2008 election . CyberPsychology , behavior , and social networking 14 , 3 ( 2011 ) , 107 – 114 . [ 77 ] Soroush Vosoughi , Deb Roy , and Sinan Aral . 2018 . The spread of true and false news online . Science 359 , 6380 ( 2018 ) , 1146 – 1151 . [ 78 ] Claire Wardle . 2020 . Understanding Information disorder . https : / / firstdraftnews . org / long - form - article / understanding - information - disorder / . [ accessed 13 - Jan - 2022 ] . [ 79 ] Chloe Wittenberg , Ben M Tappin , Adam J Berinsky , and David G Rand . 2021 . The ( minimal ) persuasive advantage of political video over text . Proceedings of the National Academy of Sciences 118 , 47 ( 2021 ) . [ 80 ] Sijia Xiao , Coye Cheshire , and Amy Bruckman . 2021 . Sensemaking and the Chemtrail Conspiracy on the Internet : Insights from Believers and Ex - believers . Proceedings of the ACM on Human - Computer Interaction 5 , CSCW2 ( 2021 ) . [ 81 ] Xinsheng Xuan , Bo Peng , Wei Wang , and Jing Dong . 2019 . On the generalization of GAN image forensics . In Chinese Conference on Biometric Recognition . Springer , 134 – 141 . Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 . 541 : 24 Hana Matatov , Mor Naaman , and Ofra Amir [ 82 ] Vera Zakem , Megan K McBride , and Kate Hammerberg . 2018 . Exploring the utility of memes for US government influence campaigns . Technical Report . Center for Naval Analyses Arlington United States . [ 83 ] Savvas Zannettou , Tristan Caulfield , Jeremy Blackburn , Emiliano De Cristofaro , Michael Sirivianos , Gianluca Stringhini , and Guillermo Suarez - Tangil . 2018 . On the origins of memes by means of fringe web communities . In Proceedings of the Internet Measurement Conference 2018 . 188 – 202 . [ 84 ] Savvas Zannettou , Tristan Caulfield , Barry Bradlyn , Emiliano De Cristofaro , Gianluca Stringhini , and Jeremy Blackburn . 2020 . Characterizing the Use of Images in State - Sponsored Information Warfare Operations by Russian Trolls on Twitter . In Proceedings of the International AAAI Conference on Web and Social Media , Vol . 14 . [ 85 ] Christoph Zauner , Martin Steinebach , and Eckehard Hermann . 2011 . Rihamark : perceptual image hash benchmarking . In Media watermarking , security , and forensics III , Vol . 7880 . International Society for Optics and Photonics , 78800X . [ 86 ] Amy X Zhang , Aditya Ranganathan , Sarah Emlen Metz , Scott Appling , Connie Moon Sehat , Norman Gilmore , Nick B Adams , Emmanuel Vincent , Jennifer Lee , Martin Robbins , et al . 2018 . A structured response to misinformation : Defining and annotating credibility indicators in news articles . In Companion Proceedings of the The Web Conference 2018 . 603 – 612 . [ 87 ] Xu Zhang , Svebor Karaman , and Shih - Fu Chang . 2019 . Detecting and simulating artifacts in gan fake images . https : / / doi . org / 10 . 48550 / arXiv . 1907 . 06515 . ( 2019 ) . Received January 2022 ; revised April 2022 ; accepted August 2022 Proc . ACM Hum . - Comput . Interact . , Vol . 6 , No . CSCW2 , Article 541 . Publication date : November 2022 .