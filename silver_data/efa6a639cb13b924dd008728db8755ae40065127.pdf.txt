Trust , Comfort and Relatability : Understanding Black Older Adults’ Perceptions of Chatbot Design for Health Information Seeking Christina N . Harrington Carnegie Mellon University Pittsburgh , PA , USA Lisa Egede Carnegie Mellon University Pittsburgh , PA , USA ABSTRACT Conversational agents such as chatbots have emerged as a useful resource to access real - time health information online . Perceptions of trust and credibility among chatbots have been attributed to the anthropomorphism and humanness of the chatbot design , with gender and race influencing their reception . Few existing studies have looked specifically at the diversity of chatbot avatar design related to both race , age , and gender , which may have particular significance for racially minoritized users like Black older adults . In this paper , we explored perceptions of chatbots with varying identities for health information seeking in a diary and interview study with 30 Black older adults . Our findings suggest that while racial and age likeness influence feelings of trust and comfort with chatbots , constructs such as professionalism and likeability and overall familiarity also influence reception . Based on these findings , we provide implications for designing text - based chatbots that consider Black older adults . CCS CONCEPTS • Human - centered computing → HCI design and evaluation methods ; User studies ; Empirical studies in accessibility ; • Social and professional topics → User characteristics ; Race and ethnicity ; Cultural characteristics . KEYWORDS chatbots , health information , older adults , race , identity , embodied conversational assistants , diary study , trust , relatability ACM Reference Format : Christina N . Harrington and Lisa Egede . 2023 . Trust , Comfort and Relata - bility : Understanding Black Older Adults’ Perceptions of Chatbot Design for Health Information Seeking . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems ( CHI ’23 ) , April 23 – 28 , 2023 , Ham - burg , Germany . ACM , New York , NY , USA , 18 pages . https : / / doi . org / 10 . 1145 / 3544548 . 3580719 This work is licensed under a Creative Commons Attribution International 4 . 0 License . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany © 2023 Copyright held by the owner / author ( s ) . ACM ISBN 978 - 1 - 4503 - 9421 - 5 / 23 / 04 . https : / / doi . org / 10 . 1145 / 3544548 . 3580719 1 INTRODUCTION Online access to information is a well - known challenge in health maintenance for those who are historically oppressed or technol - ogy novices such as Black older adults [ 72 , 79 , 81 ] . Research across various information and communication fields has sought to un - derstand what impacts users’ willingness to use online health in - formation resources . More recently HCI research has begun to address how certain embodied conversational agents ( ECAs ) such as text - based chatbots with computerized characters might support health information seeking among groups that experience barriers to information [ 16 , 37 ] . For one , chatbots can serve as a financially cheaper / accessible resource for participants from lower socioeco - nomic backgrounds [ 67 ] as they are often embedded into existing websites or apps and don’t require buying a stand - alone device . Sec - ondly , researchers suggest that chatbot interactions have very little barrier to entry since many users have familiarity with messaging interfaces [ 32 ] . Recent studies have explored the utility of ECAs and text - based chatbots for online information , identifying the needs of marginal - ized groups such as older adults [ 60 , 63 ] and racial minority groups [ 37 ] . From these studies , chatbots have been proven to be effective both at improving conversational dynamics and presenting instant information among these populations [ 37 , 40 ] . Chatbots have also been proven to be effective in increasing conversation time among older users , albeit there was a shortness in the length of messages compared to human conversations [ 29 ] . Chatbots are thus an appro - priate tool for simple and repeating conversations with older adults and may make routine tasks like online health information seeking more seamless . With particular importance to healthcare , chatbots also show promise to increase information seeking and exchange among individuals with lower health literacy [ 10 , 50 ] . Researchers have also explored the use of chatbots to better understand patient needs prior to healthcare interactions with medical professionals [ 38 ] , finding that study participants preferred chatbot interactions due to them being more understandable than traditional pre - visit surveys . As this area of study grows in HCI , we are also beginning to see discourse around appropriate design features in the embodi - ment of conversational agents and chatbots . Previous work has looked at concepts such as anthropomorphism [ 10 , 16 ] and human - ness [ 55 , 56 ] as ways to measure the likeness of embodiment in chatbot characters and link this with a person’s willingness to in - teract with these AI systems . Much of this work suggests that these constructs are critical factors to a user’s perceptions of and experi - ence with a chatbot , thus expanding the considerations designers should have when creating them . Among these considerations are CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Harrington , et al how chatbots personify and negotiate with race and gender both in design features and in language reception . There is much debate of chatbots or other AI systems being human - passing by embodying more realistic characteristics of skin - tone , age , gender , and other distinguishable facial features [ 76 ] . On the one hand , increased hu - man likeness is linked to also increasing the reception of chatbots and ECAs . On the other hand , scholars suggest that assigning race and gender to chatbots does not just ascribe personality but it also engages how race and gender are their own technologies . Teresa de Lauretis [ 41 ] first suggested gender as technology by stating that gender is constantly being reproduced in every aspect of our society and thus assigns its own cultural codes . Race on the otherhand is mechanized as ’otherness’ when it is not the default of whiteness , and thus becomes its own social technology [ 76 ] . Racial identity in technology has been framed as a “carefully crafted , historically inflected system of tools” , and thus are a matter of social discourse in the emergence of new technological platforms [ 17 ] . As such , there is a need to contend with the depiction of race , gender and even age in the human likeness and anthropomorphism of chatbots and ECAs . Many Black older adults may not have familiarity or experience with text - based chatbots as a technology , thus there is a need to identify the perceptions of these systems as potential health infor - mation seeking resources , and whether or not the design of racial , gender , or age features will impact these perceptions . As a first step to establishing this research area , we engaged 30 Black older adults from lower - income neighborhoods in a diary and interview study to interact with text - based chatbots for health information seeking . Our study explored the impact of varying the identity of text - based chatbot characters on factors of trust , credibility , and comfort in asking health - related questions among Black older adults . The goal of this study was to understand the types of questions participants would feel comfortable asking , their opinions of these technologies for health information seeking , and their perceptions of chatbot character identity . Using paper health diary probes deployed over a 5 - day period , we sought to address the following research questions : ( 1 ) What are Black older adults’ perceptions of text - based chatbots with embodied characters for health information seeking ? ( 2 ) What types of health - related questions are Black older adults comfortable searching with chatbots ? ( 3 ) Which aspects of the chatbot’s character design impact credi - bility and trust of the system ? ( 4 ) How does the racial , gender , age , or professional identity of the chatbot character impact these perceptions ? Does common identity increase likelihood of use ? In this paper , we discuss the outcomes of a diary and interview study with 30 Black older adults that reveal their thoughts and experiences related to text - based chatbots for health information seeking . We contribute to existing research on culturally - tailored ECAs [ 9 , 10 , 24 , 37 , 64 ] by highlighting the importance of relatabil - ity and professionalism in chabot characters to potentially mitigate hesitancy to engage with chatbots among marginalized communi - ties . In addition , we discuss how past experiences using chatbots , racial historical contexts of health care , and aversions to technol - ogy intertwine and influence Black older adults’ perceptions of text - based chatbots . 2 RELATED WORK Across HCI literature , conversational agents ( CAs ) are defined as programs that deliver interactive text - based dialogues that fall into multiple categories including speech - based ( Siri , Cortona ) or text - messaging based ( Google Assistant ) [ 32 , 54 ] . More specifically , em - bodied conversational agents ( ECAs ) are those that feature em - bodiment used to communicate with the user [ 15 ] , providing face - to - face interaction that may have particular value in healthcare interactions [ 73 ] . Many studies have used the term “conversational agent” synonymously to encompass both speech - based and text - based agents . The term “chatbot” can be defined as an overarching term meant to refer to either speech - based or text - based bots per - sonified through social media , digital assistants , or stand - alone websites such as retail shopping , medical , or therapeutic sites [ 76 ] . We distinguish the term “chatbot” as a text - based embodied con - versational agent where users interact with an on - screen character through typing and reading text . In the context of our study , we will be exploring the use of text - based chatbots with embodied charac - ters as a form of ECAs . Thus , we situate this study by providing an overview of HCI and design literature in the following three areas : 1 - Conversational agents to help older adults navigate healthcare needs ; 2 - Design features and embodied conversational agent identity ; and 3 - Trust and credibility in chatbots . Because much of the HCI and design literature has explored the feasibility of chatbots in the larger scope of embodied conversational agents ( ECAs ) , we discuss both the broader and specific areas of this research . 2 . 1 Considering Chatbot and Conversational Agent Use Among Older Adults Prior work has explored how CAs as a broader area of technology can be used to aid older adults in navigating their healthcare and well - being [ 4 , 10 , 50 , 53 ] , proposing many benefits to managing health in the home [ 69 ] . Yet , for many older users , conversational agents are still considered a new and growing domain . Pradhan et al found that when exploring speech - based conversational agent use among older adults , health was the most frequent topic explored and that some participants would trust any information received from the voice assistant , whereas others wanted to verify the in - formation they received by a second source [ 60 ] . In this 3 - week study conducted with 7 older adults using Amazon’s Echo Dot , use of features to support memory ( e . g . timers ) were low which researchers determined was due to participants’ attempts to use complex commands , memory of specific voice commands and the discoverability of specific voice commands [ 60 ] . According to Jain et al’s 2018 study [ 19 ] , 84 % of internet users have not used a CA , a finding that is influential on the understanding and usability first time CA users have . Findings from their study concluded that in order for CA design to evolve there must be clarification of their capabilities and proper handling of dialog failures [ 32 ] . Aspects such as these are detrimental to the ability of older novice users to adopt and sustain use of conversational agents . To this end , researchers propose the need for CAs to adapt to the evolving needs of older adults , suggesting a framework for design - ing CAs that acknowledges different stages of health maintenance [ 53 ] . Various studies have also pointed to the social determinants of health that impact these evolving needs , and thus acceptance of Black Older Adults’ Perceptions of Chatbot Design for Health Information Seeking CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany these systems overall . Azevedo et al examined how text - based CAs can be used to translate technical medical information in order to make it easier for older patients to understand health information [ 4 ] . Among the main takeaways of this study were the influence of participant age and health literacy on the potential benefit of CA use , a point that is also touched on in Bickmore et al’s study centered on explaining medical documents to patients with low health literacy [ 10 ] . Populations more novice to technology such as many subsets of the older adult population are less likely to benefit from healthcare technological advances due to lower proficiency with , and access to newer technology [ 33 ] . In previous studies such as Azevedo and Bickmore’s work , many older adults with lower health literacy scores reported feeling more supported when talking to doctors face - to - face , oftentimes also citing mistrust of new technology or an inability to find health information online [ 10 , 50 ] . Because partici - pants with low health literacy were found to experience difficulty understanding health information , Bickmore et al established text - to - speech ECAs as a proposed solution to explain health topics in a more palatable platform that resembles face - to - face interaction [ 10 ] . Although older adults have been found to be more receptive to realistic agents that might resemble this face - to - face interaction ( e . g . agents with realistic facial expressions and gestures ) , most studies have used mainly white and eurocentric looking ECAs as examples [ 4 ] . Additionally , work in this space has primarily focused on mostly white participant samples ; a gap that can be explained by deployment contexts and eligibility criteria . Although CAs have been shown to be a helpful tool to help older adults navigate various needs related to their well - being , little work has explored the use of racially diverse CAs more broadly , nor have they been explored as an intervention among diverse samples of participants [ 4 , 53 ] . Incorporating CA identity in this space could be helpful in engaging Black and Brown older adults who might not see such representa - tion in their real life health care settings . Contextualizing CAs as a potential resource for health informa - tion seeking among different subgroups of older adults , prior work suggests that racially minoritized groups utilize internet - based tools for health information less than other groups ; a factor that can be attributed to internet access , computer access , and knowing how to use technology [ 50 ] . More recent work has built on this by looking at socioeconomic status and race , and the impact it had on the access these groups have to online health information resources [ 20 ] . Groups at the intersection of lower socioeconomic status and racial minority often face a lower likelihood of using technology for health information in general [ 20 ] , tying into findings from the Pew Research Center that found that older adults seem to be using smartphones at a higher rate , they still are falling behind when it comes to overall technological use [ 3 ] . Overall , general unfamiliarity with CAs coupled with a history of low online health information seeking has proved to be influential in how older adults interact with and trust CAs . Acknowledging the different factors that go into improving familiarity of CAs among more novice users such as older adults might be instrumental in increasing their use , particularly among racially minoritized groups . 2 . 2 Gender , Race & Identity in Chatbot Reception Racially minoritized communities face unique experiences navigat - ing healthcare settings as a result of historically documented bias and discrimination [ 35 , 77 ] . Given this , there is value in understand - ing how these communities view technologies such as chatbots and ECAs that are meant to either interface with healthcare providers or be used in place of in - person interactions . There is a question as to whether having chatbots be designed with on - screen identities that look like the user can be extremely influential . Because health literacy is outlined as an existing barrier , Bickmore et . al [ 10 ] iden - tified that using a text - to - speech embodied translation CA among racially minoritized communities may make health information more accessible ; and while this method proved to have a positive impact on participants , cultural and linguistic tailoring were also named as constructs to consider . O’Leary et al explored similar constructs through a 2 - year partici - patory design study with predominantly Black church communities [ 58 ] . Findings from this study outline the importance of not just de - signing text - based CAs of varying skin tones but also incorporating cultural factors into the design process , and working with partic - ipants through co - design methods to gain insight on design best practices for text - based chatbots [ 37 ] . While participants in this study found comfort in the CAs that “established solidarity” and understood “religious concepts and scripture” , they acknowledged that their age , level of engagement in the church and their dress attire might influence how they perceive the CA [ 58 ] . More specifi - cally , participants found that they formed a greater connection with CAs that shared their spiritual interests or spoke their “spiritual language” [ 58 ] . Similar work by [ 21 ] examining cultural specific behaviors acknowledges cultural differences in the use of speech - based CAs , further suggesting a correlation between identity and perceptions of CA design . Prior work has also explored anthropomorphism among virtual and conversational agents . McDonnell et al [ 48 ] pointed to how realistic vs . cartoon virtual humans are perceived , and found that participants’ reactions vary based on rendering styles . Conversa - tional agents’ appearance can also be deemed as “more” or “less” appropriate for healthcare settings ; in addition to the observation that text - based CA appearance can influence how “friendly” or “likable” they are perceived [ 64 ] . Work by Ring et . al , utilized Ama - zon’s Mechanical Turk to display various 3 - D animations to users manipulating rendering style and character proportions . Results showed that participants viewed CAs to have a higher level of ‘friendliness’ when they appeared more cartoonish , but seemed to attribute realistic ECAs to being “more professional” - associating these renderings to perceptions of healthcare providers [ 64 ] . This work supported the claim that when deploying CAs across various domains , acknowledging context is important ; as what might work best in a purely medical domain might not be as impactful in other settings [ 58 , 64 ] . In particular , the deployment of CAs that fit the needs of a particular demographic resulted in racially minoritized groups being left out [ 58 ] . The observed lack of diverse racial de - mographics of participants impacted the quality of conversational agents and the level at which racially minoritized communities found them relatable . Thus , anthropomorphism and personality CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Harrington , et al are critical factors that impact how users perceive and experience a chatbot or ECA [ 16 ] . Additionally , work done around the effects of chatbot attire found that professional attire and settings associ - ated with a doctors office can increase participant trust , supporting this idea that anthropomorphism is an important aspect of user’s perception of chatbots [ 59 ] . As HCI moves to discuss the implications of race in technology more [ 24 , 57 , 65 , 68 ] , we are also beginning to see conversations emerge of how race , class , and gender affect our relationship with ideas about emerging technology such as chatbots , conversational agents and AI [ 76 ] . Prior work of Vorsino [ 76 ] , Schlesinger et al [ 65 ] , and [ 57 ] , have explored the implications of race and gender personification in reception of chatbots . For instance , female chat - bots have been found to be taken less seriously and receive more vitriol and harassment [ 76 ] . Marino also notes that chatbots that are given racial identity are exposed to “particular racial readings” including “abuse and innuendo” [ 46 ] . Vorsino suggests that assign - ing race and gender to chatbots does not just ascribe personality but engages how race and gender are there own technologies [ 76 ] . Schlesinger , Hara , and Taylor suggest it important to contend with race in the design of chatbots by considering who’s words we are learning from and what do chatbots understand as language [ 65 ] . Thus there is value in exploring the representation and resulting reception of these constructs , particularly around tasks that are deemed as critical as health information seeking . There is less literature on the reception on the age of ECA char - acters on general or even older adult perception . In a systematic review of design features of embodied conversational agents [ 73 ] , Stal et al highlight that there is no conclusive research on age pref - erence of ECAs among older adults and more research is needed [ 73 ] . In one study , younger agents were preferred over older agents [ 78 ] while in another study participants preferred agents of the same age or older [ 2 ] . There are no known studies that have ex - plored both age , gender , and racial preference of ECAs among the older adult population and how this might contribute to trust and credibility of information . 2 . 3 Trust and Credibility in Chatbots Certain demographic groups still exhibit some hesitancy using ECAs and chatbots as a main source of information , citing lower feelings of trust or credibility associated with these platforms . There is general mistrust around chatbots on the basis of unfamiliarity , a point that is drawn on in Zierau et al’s study conducted with 60 participants that looked at how the design of AI / text - based chatbots influences user perception in the context of loan applications [ 82 ] . Findings from this work attributed trust to CAs user interface type and found that higher anthropomorphism is influential on user trust [ 82 ] . Trust has also been associated with the level of risk that was involved in interacting with the chatbot [ 1 ] . Findings from a survey response done with 124 adult members of consumer platform Elisa Raati found that participants had major concerns surrounding information security and task accuracy [ 1 ] a similar sentiment felt by users in Mozafari’s study conducted with 201 participants ( 45 % female with an average age of 38 years ) who reported feelings of low security when faced with the decision of having to rely on text - based chatbots making high criticality decisions [ 1 ] . High criticality decisions were influential on participant trust , especially among racially minoritized communities . Jensen et al’s work establishes that anthropomorphism in CAs through ECAs is linked to increasing user trust and acceptance [ 34 ] . This study explored how agent appearance influenced user perceptions of and reliance on an automated teammate , finding that agent appearance did not significantly influence trust appro - priateness ( defined as the degree to which perceptions of reliability match actual capability ) , but it did influence perceptions of trust - worthiness . This and many other studies suggest that trust in AI and automation - based systems is associated with the perception of humanness of the system [ 56 ] . In fact , Nass [ 56 ] asserts that the success of conversational systems and those that use Natural Language Processing is actually predicated on making machines that can communicate like humans , and that social responses occur due to a computer possessing a certain amount of social cues such as using words for output [ 55 ] . Morkes , Kernal , and Nass also indi - cate that the degree of the social response elicited by a computer depends on the degree to which human likeness is perceived [ 52 ] . However further research is needed to explore how we operational - ize ‘humanness’ . This humanness may not just be predicated on facial features however , but those features and attributes that also add to the holistic humanity of a character or embodied conversa - tional agent such as cultural knowledge , perceptions of friendliness of empathy , or professional title in the case of health . Establishing relationships with participant’ communities has also been found to have an impact on feelings of trust and perceptions of credibility . In a codesign study with 31 Black and Hispanic women , researchers worked to generate a chatbot and contraception aware - ness campaign , finding that involving local women as well as social media influencers had a notable impact on the public’s perceptions and credibility of the campaign as well as the CAs themselves [ 12 ] . Similarly , Mendez et al’s study explores perception and trust by examining text - based chatbot’s efficacy with racially minoritized doctoral students . Results from their study found that the establish - ment of trust was built off of the relationship developed between the participant and the chatbot [ 49 ] , and that overall user satisfac - tion aligned with perceived trust of the content being presented to them . Community involvement with chatbots more broadly seemed to positively impact the trust minoritized communities had ; espe - cially if the tool served as a means to uplift or positively impact participants [ 58 ] . Older adults in comparison have been noted to base their trust of platforms such as ECAs and text - based chatbots on their prior technology experiences [ 43 ] , and that these experi - ences also impact the values they associate with technology such as the need for it to be friendly or likeable [ 6 ] . Thus , there is a need to explore constructs of trust and credibility in text - based chatbot engagement among racially minoritized communities to build meaningful relationships with participants . 3 METHODS 3 . 1 Study Overview We conducted a diary and interview study to understand Black older adults’ perceptions and experiences with various online health information seeking resources , specifically looking to understand how they asked health - related questions and which resources were Black Older Adults’ Perceptions of Chatbot Design for Health Information Seeking CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Figure 1 : Florence Desktop App Used for Participant Demo most trusted for this type of inquiry [ 28 ] . The diary and interview study were a part of a larger research study aiming to discern perceptions and opinions of online health information resources with varying modalities of interaction . To address our research goals of the larger research study , participants were presented with websites , text - based chatbots , and speech - based conversational agents [ 28 ] . In this paper we present an analysis of the 5 - day diary study and the accompanying follow - up interview to address our research questions related to text - based chatbots . At the beginning of our study protocol , we held an on - boarding session to collect background demographics including age , educa - tion , socioeconomic status , health status , and frequency of health communications both with and without technology . We also col - lected participants’ prior exposure to and use of conversational agents and chatbots . At this time , we introduced participants to the concept of online health - information seeking and introduced the diary study . As a part of this introduction , we explained what each resource was by showing web pages of each site or resource and walked participants through how to ask a health - related question of the resource . For the chatbot in particular , participants briefly interacted with ’Florence’ , an online personal health assistant in the form of a chatbot that helps its users to manage their health and wellness via text - based chats 1 . Participants were instructed to ask any sample question they wanted of the Florence desktop interface to understand the interaction of typing questions and getting immediate responses prior to doing the diary study ( see Figure 1 ) . 3 . 1 . 1 Diary Study . We designed a physical paper diary with mock - ups of various resources that participants would be engaging with 1 https : / / www . florence . chat / during the diary study : online websites commonly used to search for health - related terms or questions ( e . g . Google search engine , local hospital portal websites , the CDC website , and then websites like Mayo Clinic and WebMD ) ; speech - based conversational agents ( or mobile voice assistants ) such as Siri or Google Assistant ; and a text - based chatbot that may appear on an online health website ( see Figure 2 ) . Physical paper - based diaries have been used in HCI research to capture daily thoughts and activities and to self - report questions and symptoms related to health [ 14 , 18 ] . Our aim was to capture questions through a method that these older adults might feel comfortable with based on previous research with this group that identified health journals to capture and communicate ques - tions and symptoms as a health priority [ 27 ] . Thus , we created a physical paper diary and avoided any technology - based platforms to capture information . Diaries featured space for participants to share stories about a time where they have used the resource in the past and what they used it for , an evaluation of how likely they were to use this resource for health information seeking , and a space for participants to list health - related questions that they might ask that resource ( see Figure 3b ) . Chatbots appeared on Day 5 as a medical professional on a health website intended to answer health - related questions , similar to Florence ( see Figure 3a ) . Each participant was randomly assigned to one of four different representations of a chatbot character ( see Figure 4 ) : 1 - older Black woman ; 2 - younger White female ; 3 - older White male ; 4 - younger Black male ( see Figures 4a , 4b , 4c , 4d ) . Racial identity , gender , and age of the characters were chosen based on prior literature [ 46 , 76 ] . Additionally , we varied the title of the medical professional ( e . g . ’Dr . ’ , Nurse , Physical Therapist ) . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Harrington , et al Figure 2 : Pages of Paper Diary Used in Our Study . The diary study was explained as a 5 - day activity where each day of the diary correlated to a front and back page where participants were asked to write down their experiences with each of the health - information resources presented , and then list any health - related questions that they would feel comfortable asking that particular resource . Participants were instructed to go online via computer or mobile phone to interact with each of these resources that they were introduced to during the onboarding . Participants were provided with their own Google Drive folder to upload pictures or scans of their diary pages for review by the research team prior to follow - up interviews . 3 . 1 . 2 Follow - up Semi - Structured Interviews . At the end of the five days , we conducted follow - up interviews either in small groups or virtually one - on - one 2 . During the follow - up interview , partici - pants were asked to discuss their diary entries , including any prior experiences they had with chatbots , and the questions they listed . We collected perceptions of the chatbot as a health information resource by having participants respond to an 8 - item questionnaire based on a 7 - point scale where 1 = Extremely unlikely ; 2 = Quite un - likely ; 3 = Slightly unlikely ; 4 = Neither ; 5 = Slightly likely ; 6 = Quite likely ; 7 = Extremely likely . Participants were then presented with four search scenarios and prompted to ask questions that would address the scenario : 1 - information related to medication man - agement , 2 - symptoms of high blood pressure , 3 - information on diabetes and health diets , and then 4 - anything else the participant might want to search for . We asked follow - up questions about their perceptions of the text - based chatbot for health - information seek - ing , and asked to provide feedback on the design of the character 2 The format of these interviews was changed from group to individual due to the global health pandemic which required social distancing and prevented visits in older adult living facilities that was presented in their diary . Finally , we shared the other three identities of the chatbot character and asked about their perceptions of the other characters . Interviews lasted between 60 and 120 min - utes and participants were compensated $ 75 cash for participating in the study . 3 . 2 Participants Participants were recruited via community partners in two Mid - western cities through community involvement with one group and a research registry in another [ 23 ] . In total , we recruited 34 Black older adults : 13 from Chicago and 21 from Detroit . Thirty participants ( 27 female ) completed the entire study from the two locations ( n = 11 and n = 19 respectively ) ; 2 participants dropped out due to lack of stable internet connectivity , and two dropped out due to health reasons . P01 - P11 were participants from Chicago , and P19 - P43 were participants from Detroit . Participant ages ranged from 60 to 84 years old ( M = 72 ; S . D . = 5 . 44 ) . We asked participants about their ownership of technologies such as computers , laptops , and mobile phones , and the frequency with which participants communicated with healthcare professionals about their health . Twenty - three participants reported owning a traditional computer , 11 reported owning a tablet computer , and 27 reported owning smartphones . Four participants reported communicating with a healthcare professional ‘once a week’ , nine participants reported communicating with a healthcare professional ‘once a month’ , and 21 reported this communication happening only ‘a few times a year’ . Six participants were unfamiliar with text - based chatbots prior to this study , and 17 had never used them . For additional participant demographics , see Table 3 in the Appendix . Black Older Adults’ Perceptions of Chatbot Design for Health Information Seeking CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany ( a ) Diary page : onscreen character ( b ) Diary page : chatbot use prompt Figure 3 : Chatbot Diary Pages 3 . 3 Data Analysis Interview audio files were professionally transcribed for accuracy . Using Atlas . ti , we open - coded the 23 interview transcripts that contained a mixture of small group and virtual one - on - one inter - views . Guided by a grounded theory approach [ 71 ] , axial coding was used to construct themes following the study . [ 8 , 25 , 74 ] . As a result of the theme groupings produced by this analysis method , our final codebook consisted 106 participant quotations with 10 major themes . The research team met iteratively throughout data analysis to reflect and discuss qualitative themes . Questionnaire data was analyzed for descriptive statistics of the sample and for initial preferences and opinions of chatbots prior to the study . ( a ) Black older woman ( b ) White younger woman ( c ) White older male ( d ) Black younger male Figure 4 : Various Chatbot Characters CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Harrington , et al We state our positionality by detailing that our initial research team consisted of three college - educated research scholars of var - ious racial ethnicities who were all younger than the participant sample . Data was analyzed by two Black , college - educated women scholars and one White college - educated woman scholar who pro - vided guidance on data interpretation . Two members of our original research team have prior experience with aging research , one of which has focused on racially minoritized older adult populations . These backgrounds supported our analysis of how constructs of identity impact perceptions of text - based chatbots as health infor - mation seeking resources . We acknowledge that the first author’s positionality as racially identifying with the participant sample may have made participants more comfortable to share certain per - spectives and experiences . To alleviate any potential bias stemming from our positionality as authors , we examine our analysis and findings against existing literature [ 10 , 36 , 37 , 76 ] . 4 FINDINGS The goals of this study were to understand how Black older adults asked health - related questions of text - based chatbots , and to iden - tify the perceptions of chatbots as a health information resource . We first discuss participant’s thoughts and experiences with chat - bots and types of health questions asked from their diaries as a way to contextualize perceptions and opinions of chatbot identity . We then discuss findings from the analysis of perceptions of chatbots as a health information resource in three major thematic areas : 1 - Perceptions of Chatbots and Health Questions Asked ; 2 - Comfort and Trust with Chatbots ; and 3 - Relatability , Credibility and Perceptions of Chatbot Identity . 4 . 1 Perceptions of Chatbots and Health Questions Asked We build on Kim et al’s [ 37 ] work looking at designing chatbots to increase African Americans’ trust in health information during COVID - 19 . This study is instrumental to our understanding of how specific identities of chatbots were associated with trust , specifically as it tied to race and personability of the chatbots [ 37 ] . Building on these findings , we identified additional components that help us un - derstand the needs of older Black adults who interacted with ECA’s . Addressing our first and second research questions , we establish initial opinions of these platforms for health information seeking , and the specific types of questions that Black older adults would ask . While most participants reported never having engaged with a chatbot prior to this study , many reported on the questionnaire survey that they felt chatbots could be an efficient health infor - mation resource ( P8 , P9 , P23 , P27 , P30 , P32 ) , echoing sentiments from Kim et al [ 37 ] that were associated with the general Black community . Most notably , when surveyed about their perceptions of chatbots , items such as ‘Learning to operate a chatbot would be easy for me’ , and ‘It would be easy for me to become skillful at asking health - related questions through a chatbot’ had the most participant agreement ( see Table 1 ) . As a result of prompts in the diary and follow up interviews , we identified the types of health - related questions that participants would ask a chatbot as : Medication Dosage / Interactions , Health cov - erage and Insurance , Diet and Food , and Blood pressure related . Some participant questions related to the chatbot’s capabilities as op - posed to health questions that would be asked of the chatbot ( see Table 2 ) . For instance , P39 wanted to know what kinds of topics the chatbot could answer and the extent of that knowledge , and also whether using a chatbot for health information was covered by health insurance . Contrary to other research studies exploring health information needs [ 11 , 28 , 47 ] , we found that participant questions for chatbots also appeared to be more tailored to their specific needs , indicating a comfort with chatbots providing per - sonalized information . Participants discussed how chatbots could be a resource to con - sult about everyday illnesses and things that were lower in severity in terms of their health . For health issues that are more common or non life - threatening , chatbots were seen as a quick resource to access health information that would’ve otherwise required a doctor’s visit . This was similar for participants who may not have the mobility or financial means to see a doctor as frequently . P34 details their experience using a chatbot to answer questions about their sinus issues in the following quote : “I diaried about my sinus . I was having problems , but I really wasn’t sure whether I had sinus or not . So I used the chatbot for a response to determine if I had the problem . And according to my symptoms using the chatbot , it determined that some of the symptoms included congestion , headaches , stuffy nose , fatigue , watery eyes and all of that , which I had been experi - encing . ” - ( P34 ) Participant perceptions fell into two thematic areas : Familiarity and Technology - Influenced Impressions of Chatbot Use , and Chatbot Usability and Efficiency . One of the main constructs that emerged Questionnaire Item Average Rating M ( S . D . ) 1 . Using a chatbot would enable me to find out health information quickly . 5 . 1 ( 1 . 7 ) 2 . I would find a chatbot useful . 5 . 0 ( 1 . 8 ) 3 . Learning to operate a chatbot would be easy for me . 5 . 5 ( 1 . 4 ) 4 . I would find it easy to get the chatbot to respond in the ways I want it to . 4 . 8 ( 1 . 5 ) 5 . My interaction with the chatbot would be clear and understandable . 4 . 9 ( 1 . 6 ) 6 . I would find a chatbot flexible to interact with . 4 . 8 ( 1 . 6 ) 7 . It would be easy for me to become skillful at asking health - related questions through a chatbot . 5 . 4 ( 1 . 4 ) 8 . I would find a chatbot easy to use . 4 . 8 ( 1 . 8 ) Table 1 : Participant Average Agreement Ratings Black Older Adults’ Perceptions of Chatbot Design for Health Information Seeking CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Health Question Categories Example Questions PID Medication Dosage / Interaction " What is the dosage that I should be taking and how often should I take it ? And also can I take it with food , or do I have to take it on an empty stomach ? " P8 " What is this medication , and what are the dosages that are recommended for someone of my age , weight , and ethnic group ? " P24 " What is this medication for ? What is the side effects of this medication ? What is the mild side effect , and what is the aggressive side effect ? " P23 Health Coverage / Insurance " Did my insurance [ cover me ] at the time and [ can I ] have a chatbot for helpline ? P39 " And also , does it cost extra ? In addition to having insurance ? " P39 Diet / Food " What am I eating that I shouldn’t be eating ? And if they have a list or something . Do you have a list that I can go by ? " P26 " What , what would you suggest I cut out of my diet or put in my diet ? Or what should I eat ? Okay , I’m allergic to this . What should I have instead ? " P11 " I asked , what can I do to improve my diabetes ? And then I asked , what can I eat so as not to affect my diabetes greatly ? " P6 Blood Pressure Related " If my blood pressure is high , what are some of the symptoms I should be experiencing ? " P23 " I found out that I should eat things with less salt in it . Can I eat ham or can I wash my ham or can I . . . Or should I fry or bake my chicken ? Is that okay for high blood pressure ? " P32 " What are some of the over - the - counter products that I could use to lower my numbers ? Is it true that dark chocolate will lower your [ blood pressure ] numbers ? " P23 Table 2 : Examples of Diary Questionnaire Responses was the concept of familiarity and how that influenced participants’ perceptions of usability and efficiency . Many participants discussed their perceived knowledge of technology in general and prior ex - periences with other conversational agents as a reason for feeling that they wouldn’t be able to engage with a chatbot . 4 . 1 . 1 Familiarity and Technology - Influenced Impressions of Chat - bot Use . At the onboarding of this study participants were intro - duced to text - based chatbots through the use of Florence and several UI mockups . During follow - up interviews , many participants ex - pressed a hesitancy to use chatbots based on unfamiliarity and past experience with conversational agents in other domains ( i . e . Siri , Alexa ) . Participants’ experiences using other search engine tools for information seeking seemed to influence how confident they were in using chatbots . Prior work suggests that older users in general possess low beliefs and self - perceptions of using emerging technologies [ 22 , 44 , 61 ] . P42 expounds on this by associating lack of understanding and perceptions related to chatbots with and their lack of experience and confidence in using Google for information seeking : “ . . . the chatbot , it’s like , I really don’t know about that because I really haven’t heard anything about it until now . And Google , I just don’t have a lot of confidence in doing research on computers right now , but I think that this will help me and has helped me a little to gain some confidence because I noticed that the younger generation , everyone Googles everything . ” - ( P42 ) Additionally , some participants expected that they would not be talking to an actual human behind the chatbot and this might impact ease - of - use . P10 describes these perceptions based on their experiences with speech - based conversational agents : “My experience is that it’s generally not a person on the other end [ of the system ] . So it’s programmed to answer in certain ways . And if you don’t ask the questions the way they want to answer the question , they’re going to answer the question that they think you meant . So I’m not very fond of them . ” - ( P10 ) Similar to P10 , P2 shares that “all information from Google to here [ chatbot ] , they are all the same” - indicating that information visibility is just as unknown in dealing with a chatbot as it is with other online health information resources . This opinion was in contrast with the viewpoint of P10 who stated the following : “Yeah , see , for me , the expectation , my expectation for a chatbot is that I’m talking to someone . I’m getting a response to my specific question . Even if I get the question wrong , I can refine it , and the immediacy of it is attractive . There’s not , I don’t have that expecta - tion with Google . My expectation with Google is I ask the question , I get the answer , I have to infer this . I expect some degree of understanding and personaliza - tion . I have no expectation of personalization on the Google side . So my reluctance to use this [ chatbot ] is that if my presumption is not borne out by somebody actually being on the other end , then they’ve negated the whole purpose of them being there for me . ” - ( P10 ) For P10 , their expectations for a chatbot was a more personalized and accurate result based on the desire that there is an actual human on the other end ; whereas with using a search engine they were tasked with having to infer the accuracy of their search results . To their understanding , communicating with a chatbot would provide a more tailored experience since there was more back and forth dialogue involved . Overall , participant’s past experiences using other conversational agents resulted in mixed perceptions of the chatbots used in this study . Even though the dialogue dynamic may be more conducive to natural conversation style [ 19 ] and thus getting a more clear answer , there was still reluctance due to lack of visibility in the information source . This dimension paired with CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Harrington , et al their understanding of technology in general influenced partici - pants’ confidence of whether or not they would be able to properly use the chatbot to get the information they needed . 4 . 1 . 2 Usability and Efficiency of the Chatbot . A few participants expressed difficulty in using chatbots due to the ability to easily make mistakes when typing into the chat box . P36 expressed a level of precision required to engage with text - based chatbots , indicating barriers in usability and ease - of - use that would impede them being successful seeking health information this way : “Well , I think I was writing a question , and then be - cause I think . . . Like it happens with me and comput - ers , I fumbled or something , hit the wrong keystroke , which took me out of the box that I was writing the question [ in ] . I hadn’t finished it yet . So , without in - tending to , the question I was asking went back to [ the ] chatbot but it wasn’t the full question , so the chatbot said , ’I don’t understand what you’re talking about . ’ or something like that . So , I had to go back into the chatbot and write it down specifically what I meant , and I had to be careful that I didn’t do the wrong keystroke and take me out of the chat box and send it back to them without it being completely done correctly with all the , you know - I just had to be careful how I wrote it so that I wouldn’t make a mis - take , and I make a lot of mistakes online , you know , computer - wise . ” - ( P36 ) Although many participants felt that chatbots could be an ef - ficient tool for quick questions such as dealing with a common cold or illness ( P3 ) , some participants discussed perceptions of this efficiency as being impacted by having to repeat information to the chatbot , as noted by P22 who states : “I have to go back and forth explaining exactly what it is that I’m searching for . If I had an actual verbal conversation , it would be a better communication between myself and the receiver” - ( P22 ) . Participants also expressed that the pre - arranged nature of asking questions with conversational agents influenced participants’ expectations of the chatbot , as exemplified by P41 who stated : “I use chatbots for other things other than health infor - mation , and I really didn’t like it , it’s like a computer giving you prearranged questions , answers . And it was just frustrating and so I definitely wouldn’t ask it to think about my health . ” - ( P41 ) Similarly , P31 points out their concerns around chatbot compre - hension stating that a participant could have “dementia or some kind of mental disturbance” and may not realize they are not talking to a healthcare professional ( P31 ) . Many participants discussed wait times when using conversa - tional agents in other aspects of their lives ( P22 , P36 , P43 ) , and explained that these experiences also impacted perceptions of chat - bot efficiency . P30 states that because “companies are tied up , they’re not able to get to you quickly” and noted that they found using chatbots as an advantage because “you’ll get an answer right away . ” Similarly , participants expressed difficulty staying within a certain word count and framing questions in a way that the chatbot would understand ( P3 , P22 , P36 , P43 ) . 4 . 2 Comfort and Trust with Chatbots We identified participant’s perceptions on trust regarding the health information they were being presented with , as well as concerns about how accurate the chatbot would be with their health needs . These dimensions influenced participant’s acceptance and percep - tions of chatbots as a health information resource . For many partic - ipants , using a chatbot to access health information evoked feelings of uneasiness around health information accuracy . For some par - ticipants , their trust centered around where the information came from ; as seen by P10 who based the credibility of a source by its approximation to their doctor : “if I’m at my doctor’s site and I know it’s manned by my doctor or one of her nurses , then OK . And then I trust it” - ( P10 ) . Similarly , P42 expressed sentiments around where the source of chatbot information was coming from : “I’m like , ‘Who programmed them ? ’ If these things are so accurate , they’re gonna take over it seems like . It’s just that it’s my mistrust . Because see , I’m not up with time , even though I try to stay with time . ” - ( P42 ) . After expressing concerns , P42 claims mistrust as a major reason - ing for their skepticism in using chatbots , confirming prior work in users’ comfort with healthcare AI [ 42 ] . Because their experiences in accessing health information may not have relied on technology , the idea of using a chatbot as a source was not particularly priori - tized . The concerns expressed by P42 tie into transparency , which was another theme observed that influenced participants’ trust in using chatbots . Many participants were already unfamiliar with the concept of a chatbot , so not understanding where the information was coming from resulted in more hesitation . Similarly , participants expressed distrust in using a chatbot to access health information due to their complex personal health needs stating that more sever health needs were reserved for in - person medical consultations [ 75 ] . P3 gets at this concern by stating : “Listen , let me say something . See , you are young and we are old . So that’s , you know , that’s the difference . You’re used to the stuff like that . . . you might just have a cold . We’ve got a thousand things going on . . . Well , you know , I had this cough for three days and I want to know , do I need to go to the doctor or should I buy something over the counter ? Whereas we will be , I got this ache , this pain , this head , this leg , this everything . What should I do ? And then they’ll be like , we’ll call you back in two days , because we need to research this . ” - ( P3 ) For many of our participants , health is a complex issue that sometimes even professionals need time to assess . Having grown accustomed to speaking to their healthcare providers in person about questions or concerns related to their personal health , many participants viewed chatbots as unreliable to answer serious health questions . Because participants may regularly see a trusted doctor to receive information about their health conditions , they expressed hesitations with relying on a chatbot without first verifying infor - mation with their healthcare provider . More specifically , there was an assumption that the participant would need to “dumb down” their questions for the chatbot to understand . Participants’ hesitation to use chatbots stemmed from informa - tion visibility and perceptions of complex health needs . Lack of Black Older Adults’ Perceptions of Chatbot Design for Health Information Seeking CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany personalization also seemed to influence participant privacy con - cerns as discussed by P5 : “When you go to the bank or the ATM , you know , they got tellers in different towns and stuff that you can visit and see , but that’s your money , you know , and you put in your serial number for nothing . But when you go to a chatbox , you don’t even have a special code or nothing . It’s just that , that’s the difference . So if you don’t have a password , you know , if you had a password to go into this box , I go to [ a neigh - boring ] Hospital when I had my surgery and I’ve got the patient portal you asked about . I had that to get my tests and whatever , my results , but at least I had a password . And plus it ain’t no privacy because , you know , when you talk to the physician there’s privacy , with these you can’t , like you just said when we come in , what we say among ourselves is private . ” - ( P5 ) For P5 , their experiences accessing sensitive ( e . g . ATM banking ) information involved security measures put in place to protect their information and not seeing these same security steps with chatbots made them uneasy . In addition to this , the information they were sharing with their physician was deemed sensitive health information , which added an extra layer to their privacy concerns . P3 echoes these views by stating : “ . . . you just giving your information to a stranger on the other end [ of the computer ] . And you’re sharing and you don’t know who you’re sharing it with . ” - ( P3 ) . Views held by P3 and P5 tie into the hesitancy to use chatbots on the basis of transparency , skepticism around what the chatbot’s information source was , and how such sensitive information will be used . Participant hesitancy to engage with chatbots can also be tied to a more general aversion to smart or intelligent technology that is designed to replace human - human interaction [ 42 , 62 ] . Many participants expressed this aversion in their diaries when asked to document health - related questions that they might ask chatbots . Among some of the documented diary questions , participants stated that they would not ask any health - related question of a chatbot due to feeling like it is a “robot” or “scary machines” , and questioning why we need this in place of actual human medical professionals . One of P2’s diary questions stated , “What [ is ] wrong with people ? Why do we need robot [ s ] ? ” This question asks why a “robot” was necessary to use as a health information seeking source and ties into prior studies where participants expressed chatbots had a level of “creepiness” thus decreasing feelings of privacy and reliability among artificial intelligence . Another participant even detailed how the rise of artificial intelligence technologies in healthcare and other areas of our lives reminded her of the movie ‘1984’ where people are no longer able to think for themselves . Rajaobelina et al’s work identifies perceptions of chatbot “creepi - ness” as impacting trust and negative emotions [ 62 ] , which in the context of our study seems to be heightened given the sensitive nature of health information discussed by our participants . Other participant diary entries declined to provide a health question and instead stated “no” and expressed their reasoning - ranging from lack of perceived access to the chatbot platform or a participant’s reluctance to ever want to use a chatbot due to its resemblance to a machine . Instead many participants expressed a desire to simply make a call to ask their health question ( P3 , P6 , P43 ) . 4 . 3 Relatability , Credibility and Perceptions of Chatbot Identity To understand how the racial , gender , age , or professional identity of the chatbot character impacts perceptions of these platforms as health information resources , participants were randomly pre - sented with 1 of 4 chatbots in their diaries : older Black woman , younger white woman , older white male , and younger Black male ( see Fig 4 ) . For many participants ( P6 , P9 , P19 , P22 , P24 , P27 , P28 , P32 , P34 , P39 ) , the identity of their chatbot character had a strong influence on comfort and trust . For others , seeing a chatbot of a particular race did not evoke any strong reactions . Analysis of these participant interview responses revealed that the race of the Black chatbot character made it more relatable and thus comfortable for some participants . However other participants felt having a Black character wasn’t enough and could potentially be “pandering” or something done to make Black people trust computers . Overall , participants had strong preferences for interacting with a trusted health care provider over a text - based chatbot . In particu - lar , this preference was influenced by existing familiarity and trust with their doctor and existing negative perceptions of chatbots , and perceptions of the chatbot based on demographic features such as race , gender and age . For example , participants expressed hesita - tion in wanting to engage with the older white male and younger white woman chatbots . After being presented with a younger white woman chatbot , P28 expressed their stance on whether or not they would “use medicine with an image of a white girl on it” , stating : “I am fairly convinced from my time in healthcare that the white male dominance still rules pharmacy , still rules healthcare . ” P28 explains here that they might not trust information coming from a younger white woman representation of the chatbot because her experience has always been with older white men in medicine being more knowledgable . This suggests that more trust may be associated with older male medical professionals due to the history of doctors being older white males and women and people of color being nurses , administrative staff or not present at all . Participants also expressed skepticism on the intended goals of the chatbot . Based on seeing a white chatbot character , P24 went on to state : “it doesn’t reflect my community , and I’m not convinced that they’re even interested in my community or even the information that they give back would be something relevant to someone in my community . ” Similar feelings were expressed with the older Black woman chatbot , as noted by the response from P39 . After being prompted to explain their comment “looks can be deceiving” P39 goes on the state the following : “Well . . . Well when we get into the history of the medi - cal profession and the way it’s treated minority groups in this country , the Tuskegee experiment , you have to remember that , yes , it was run by white people , but the woman who was the point of contact with the men was an African - American woman , and she had to know what was really going on , and I don’t think . . . The fact that they may have felt comfortable with her was as deceptive as the entire operation . So , you could CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Harrington , et al put a Black face on it , you could put a white face on it , it doesn’t matter , if the . . . If there’s deception going on , there’s deception going on . ” - ( P39 ) P39 draws on the history of racism in healthcare , specifically the Tuskegee experiment [ 5 , 7 ] , as a reasoning for their feelings of mistrust . For them , the Black woman chatbot was viewed as a decoy . On the contrary , some participants viewed Black chatbots as knowledgeable and credible as seen by P34 who expresses their views upon seeing the Black chatbot : “He [ Black chatbot ] looks like he’s an intelligent person , professional and . . . He looked like he may be knowledgeable and just waiting for you to ask him some questions , and that he’s prepared , waiting on you . ” - ( P34 ) . P34’s explains that the way the Black male chatbot is dressed impacts participants viewing him as knowledgeable and credible . For many participants who were more accustomed to talking directly to their healthcare provider about health related questions , the professional qualification of the chatbot and the information source ranked highest in importance : “Hopefully , [ the age ] wouldn’t bother me because like I said , it’s the qualification of the company , knowing that she can handle the questions . Often though or not , I would like to see an Afro - American . . . A more diverse picture , all of that . I don’t know , would that make me more comfortable , because I’m trying to get information , but it’s always good to see that . ” - ( P32 ) For P32 getting information from a qualified information source behind the chatbot seemed to be the main priority and the chat - bot being Black made it relatable as a bonus . Based on participant feedback , the age of the chatbot did not have an overall impact on how likely participants were to use it . P19 describes : “No , uh - huh . The race or anything like that wouldn’t matter . The age , I don’t think the age would matter either because you know a person can have knowledge without being a certain age or whatever , so I don’t think that would matter . ” P19 views age as just another factor when engaging with chatbots . As stated earlier , P32 held similar views and emphasizes that the qualification of the chatbot is an impor - tant factor to them . While some participant’s perceptions weren’t swayed by the age of the chatbot , P22 viewed younger chatbots as more trustworthy based on the perceptions of new knowledge and skills that younger doctors might have compared to older medical professionals : “Age wouldn’t matter . In fact , I prefer to have people younger than me , sometimes , to get a better feel or different type of knowledge . I do feel the younger generation would say things differently than I do . . . . I’m thinking about seniors , it would matter , because I have a stereotype , unfortunately , even though I am elderly myself , about seniors and what they can give to me , as far as knowledge . Because again , if they only have the knowledge and skills , they [ only ] have the experience related to an illness or something . Then you couldn’t . . . He or she couldn’t help me . ” - ( P22 ) . For P22 , their preference of a younger chatbot tied into their perceptions that the younger generation was more knowledgeable . While knowledge was an important preferential factor , this did not impact P22’s trust , as the chatbot may not have expertise on a specific health topic . This ties into perceptions felt by participants who viewed the information source of the chatbot as a higher priority . Tying into this theme around expertise in association with appearance , the dress attire of the chatbot played a role in some participants’ perceptions ( P27 , P34 ) . P27 described their chatbot by stating : “well , he’s a Black guy . . . Young Black guy , yeah with a shirt and tie on . So he looks professional . ” P34 shared similar views : “ [ He’s ] just a Black guy who looks like he’s concerned and ready for you to ask him some questions . . . He looks like he’s an intelligent person , professional and . . . like he may be knowledgeable and just waiting for you to ask him some questions , and that he’s prepared , waiting on you . ’Cause he seems like he’s sincere and that he just seemed like . . . He’s introducing himself , he’s telling you that he’s a nurse , and he’s trying to find out how can he help you , which is the profes - sional thing to do , so he’s asking all the right ques - tions . So that’s why I take him to be professional and maybe knowledgeable of the questions that we think he might be able to answer from the chatbots . ” - ( P34 ) Building on the idea that participants’ comfort is associated with appearance , P34’s perception of the Black chatbot character leads her to describe him as an intelligent person . Based on participant perceptions ( P22 , P26 , P34 , P27 ) , it was observed that feelings of positive emotions were evoked when participants interacted with chatbots who seemed friendly or wore professional attire . Our findings of professionalism and friendliness build upon Parmar et al’s study of professional attire being more credible among virtual health doctors [ 59 ] , and consider race as an additional construct that elicits relatability . Participants found Black chatbots to be more relatable but the trustworthiness across different chatbots identities was mixed , as information source of the chatbot was a main determinant in par - ticipant’s perceptions . When presented with Black characters in the design of the chatbot , the overall response from participants was feelings of comfort and happiness with seeing representation , but for some participants ( P26 , P29 , P30 ) , seeing a Black chatbot did not seem to have a major impact on whether or not the participant would view the chatbot as more trustworthy . P30 expressed this by stating , “I don’t know if I would feel differently , I just sort of feel a little better seeing her , not that I would not take their advice either , I would , but she just makes me feel a little bit more comfortable . ” P24 felt that seeing a Black chatbot would make them feel like the creators are knowledgeable about different ethnic groups , however they still had reservations about the information provided without being able to see who’s behind it : “But when I see something like this , it doesn’t reflect me . It doesn’t reflect my community . And I’m not con - vinced that they’re even interested in my community or the information they give back would be something relevant to someone in my community . ” - ( P24 ) . Although there was positive sentiment around seeing Black chat - bots , there was still skepticism when it came to the intent of chatbot creators . P24 notes that the information they would receive from this chatbot might not align with what their community needs . P31 expressed similar feelings of distrust when seeing a Black chatbot Black Older Adults’ Perceptions of Chatbot Design for Health Information Seeking CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany by stating , “don’t be trying to make nothing look like me . You’re just trying to ingratiate me mentally that I can trust this robot” - ( P31 ) . Skepticism around chatbot intentions seemed to be a com - mon theme for participants who responded negatively to seeing a Black chatbot image , as evidenced by P39’s connection to history of distrust with medical professionals . While many participants had conflicting feelings upon viewing a chatbot , the main con - cern seemed to center around intent and how the source of the information they would be receiving would impact them or their communities . 5 DISCUSSION The goal of this study was to capture the types of health - related questions that Black older adults would feel comfortable asking text - based chatbots , and assess how identity of the chatbots im - pacted trust and credibility . We present findings from 30 Black older adults in lower - income environments and detail their perceptions of these platforms as potential health resources , including how prior experiences with technology inform these perceptions . There is a significant volume of prior work that suggests the importance of understanding aspects of identity in AI platforms such as chatbots [ 16 , 17 , 56 , 57 , 65 , 76 ] . Our analysis contributes to the discussion by highlighting how perceptions of trust , comfort , familiarity and relatability are informed by the design of chatbot characters , and discussing how this may impact human - AI interactions among Black older adults . The use of a diary study provided a daily , real - time convenience capture [ 14 , 18 ] of the types of health - related questions Black older adults might ask a text - based chatbot as well as their initial opin - ions on this platform as a health resource . By having participants interact with the online chatbot Florence , we were able to educate a demographic that might not otherwise be familiar with chatbots on how to use these platforms and the method of asking health - related questions . Follow - up interviews allowed us to go further in - depth about perceptions of chatbot character identity , and how charac - ter identity might influence this platform being a potential health information resource . Findings from analysis of the questionnaire data indicate that many of our Black older adult participants would consider a chatbot to be useful for health information seeking . This mirrors previous research proposing chatbots as having particular benefit to older adults [ 69 ] , yet contributes an additional level of nuance in that many of these benefits may slightly differ for Black older adults . Previous research identifies that it is sometimes diffi - cult for Black older adults and other racially minoritized groups to interact with voice - based conversational agents due to dialect or not knowing the correct words or questions to search [ 28 , 31 , 36 , 37 , 39 ] . Text - based chatbots have the potential to be more supportive in the back and forth dialogue that may provide more clarification among these users [ 16 ] , yet their design may be a barrier if not addressed in ways that consider user preference and comfort . Findings from our follow - up interviews provided insight into older Black adults’ perceptions of chatbots , suggesting that study participants felt that chatbots would be useful to ask " lower - risk " health - related questions , if anything . Diary questions collected re - vealed that for those participants who would be comfortable engag - ing with a chatbot , they were comfortable asking more personally tailored questions , yet questions that were considered higher risk or those dealing with complex health issues might not be suitable for asking a chatbot . Similar to other studies assessing health in - formation seeking with embodied and non - embodied CAs [ 11 ] , participants felt that they would have to ask their questions in very particular ways to get an accurate and relevant response from the chatbot , perhaps a deterrence for those participants who reported not being comfortable interacting with chatbots . Our qualitative analysis revealed that many participant’s follow - up interview re - sponses seemed to contradict the positive ratings of the question - naire item related to becoming skillful at asking health - related questions of a chatbot . Interview responses indicated negative per - ceptions of efficiency , despite participants’ initial ratings that the chatbot could provide quick , real - time information . We note that the presence of an interviewer may have allowed for more reflec - tion and detailed explanations of these perceptions when compared to categorical ratings . 5 . 1 On Identity and Trust in Chatbot Interactions Our analysis also contributes to the larger conversation around trust in chatbot and AI systems . There has been a long history of distrust in healthcare [ 7 , 30 ] , and much of this is impacted by the history of racism and deception in healthcare [ 5 , 7 ] . These negative experiences often translate to skepticism in the technology associ - ated with healthcare and health information [ 42 , 45 , 51 ] . Many of our participants expressed a general concern about not just under - standing the source of health information as previously established in prior research [ 37 ] , but also in the perceived deception that may also be present in online systems paralleling histories of in - person medical deception . There was an overall skepticism around the intent of the chatbots and the assumption that information Black older adults were being presented with could be harmful to partici - pant’s health or communities . A history of racism in healthcare in general has particular influence in the hesitancy of Black elders to use chatbots regardless of race or gender . Harrington et al discussed how Black older adults’ perceptions of health institutions is influenced by injustices experienced during segregation and historically significant clinical trials throughout history [ 26 , 27 ] . Prior research makes the connection between cul - tural mistrust of healthcare institutions and skepticism of emerging technologies intended to support health behaviors [ 42 , 51 ] . As chat - bots are an unfamiliar platform to many of the participants in our study , trusting information was also associated with how well Black older adults could trust exchanges about health in general , and whether or not the source behind the chatbot had the best interest of the Black community that the participant was a part of . Thus , designers / researchers should consider systems which al - low a certain level of transparency and visibility to patients and other users about where medical information comes from and also how equitable and trustworthy their development process is [ 42 ] . Transparency and trust are some noted gaps in the development of chatbots , particularly around the need to further strengthen dialogue with historically marginalized communities and explor - ing what meaningful roles chatbots can play in these communities [ 32 , 66 ] . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Harrington , et al Perceptions of chatbot racial , gender , and age identity were mixed among our sample . Many participants felt that seeing a Black face felt relatable and provided comfort for older Black users . This was seen as something nice to have and a nod to diversity that may have been instituted from the developer of the chatbot or the healthcare professional . Interestingly , a few participants com - mented on how a Black chatbot character might make them more skeptical without knowing who is behind the design and informa - tion of the chatbot . This is indicated by some participants such as P39 feeling that the race would not change them questioning who was behind the health information and whether or not they should trust it . Simply changing the race of a chatbot does not erase the history of distrust Black people have towards the health care sys - tem , a perspective that is further validated by P31 , who expressed discomfort with chatbots being designed to be Black to garner trust . These viewpoints support this idea that the integration of Black chatbots without integrating credibility can impact ( and sometimes even heighten ) participant distrust . This speaks to the importance of information visibility and transparency and that race alone does not mitigate elements of trust and credibility when it comes to ECA design . Similarly , participants were not automatically drawn to older characters among the four they were presented , but felt that knowl - edge could be held by a person at any age . Among our sample , trust was less directly associated with racial or age identity ( al - though this did introduce an element of comfort ) , and more related to perceptions of professional identity and perceptions of knowl - edge that the chatbots possessed . Similar to [ 16 , 59 ] , many of our participants indicated a high level of trust associated with health information relayed by those chatbot characters considered to be " experts " . Among the four characters , those with the designation of ’Dr . ’ and dressed in a professional shirt and tie were considered more trustworthy . This supports some of the findings from Parmar et al’s work around chatbot professional attire and the impact it has on increasing participant trust [ 59 ] . Assigning professional titles to the chatbot can improve this dynamic beyond the likeness or relata - bility of race , age , and gender . The relatability and comfort provided by a chatbot character of the same race may however speak to more empathetic or less biased interactions in online healthcare , as is evident by participant comments about how the Black male doctor looked like he was “ready to help” . This also has the potential to shift the politics of what information users believe . Marino [ 46 ] asserts that “to create a chatbot is to engage in racial construction to combine speech , dialogue patterns , and phenotypic features in order to construct a recognizable representative of a given cultural group” . Thus , considering both professional qualities and race as a social construct may increase positive associations of interactions with chatbots and ECAs among Black older adults . 5 . 2 Design Implications Very little work has explored the design of onscreen chatbot charac - ters or ECAs for older adults [ 4 , 10 , 50 , 73 ] . Thus , there is a need for understanding the experiences of Black older adults engagement with chatbots and the affordances needed to increase comfort and familiarity . Our study revealed a technology gap among Black older adults which resulted in feelings of insecurity and skepticism about engaging with chatbots . In particular , participants expressed lack of comfort or self - efficacy , and negative expectations . Overall , we recommend for designers to acknowledge various dimensions such as : trust and credibility in relation to chatbot attire , conversational dynamics with chatbots mirroring community norms and chatbots utilization based on the severity of participants healthcare needs . Addressing these implications will help to produce meaningful interaction experiences for older Black users . 5 . 2 . 1 Incorporation of Professional Identity to Gain Trust and Cred - ibility . Our findings suggest that incorporation of features such as attire , title , and facial expressions had more influence on trust and credibility . Prior work suggests an influence of professional attire user trust [ 48 , 59 ] , a sentiment observed by many participants in our study including P27 who associated the Black chatbot’s " suit and tie " to " intelligence " on health related topics . Incorporating at - tire , title , and facial expression into more racially inclusive chatbot characters may help with representation in users being able to see friendly Black medical professionals that are relatable . Thus , we recommend designers incorporate occupational specific attire in chatbot design as a way to increase participant’s perceptions of system credibility . 5 . 2 . 2 Tailoring Conversational Dynamics to Meet Community - Specific Needs . Simply changing the skin tone of chatbot characters without also addressing the dialogue to reflect the cultural norms of the community can create a disconnect between users and chatbots . This may further contribute to skepticism Black communities associate with chatbots . Although there is question to whether conversational dialogue between users and CA’s should mirror that of human to human interaction [ 19 ] , there is a need for thoughtful design around chatbot - participant dialogue . Designers should place critical thought into the language norms of the community and how they can be incorporated into chatbot design . Building on prior work [ 58 , 59 ] , we recommend the incorporation of content specific dialogue that is tailored to the communities that they are being deployed . For Black older adults specifically , this may include check - in features which ask the user if they need more time to finish typing a question , or clarification questions that consider cultural idioms to ask a user what they meant . 5 . 2 . 3 Considering Severity of Health Needs . Our participants ex - pressed health information needs that vary in severity levels , and indicated that questions that are considered more severe may not be things they are comfortable discussing with chatbots . This find - ing parallels prior literature that suggests less critical illnesses and symptoms as those suitable for online information seeking [ 75 ] . This finding implies that future chatbot design should in - clude interaction features to gauge severity of a health need or question . We recommend designers work closely with healthcare providers and Black elder adults to determine what is considered severe enough to be communicated via a healthcare chatbot versus a medical professional . In doing this , a dialogue between a chat - bot and participant can give a participant who might have more severe or specific healthcare needs the option to speak to a medical professional / specialist . 5 . 2 . 4 Information Source Transparency . Brewer et al [ 13 ] discuss the need to design for transparency as a way to address uncertainty Black Older Adults’ Perceptions of Chatbot Design for Health Information Seeking CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany among voice assistant use for older adults . Our findings extend this by suggesting the need for information source transparency for text - based chatbots to mitigate skepticism among Black older adults whose hesitance may stem from historical precedence in addition to the uncertainty identified among more general samples of older adults [ 13 , 70 , 80 ] . This may take the form of flagging resources that are credible and trusted from the perspective of medical professionals , or detailing how the chatbot chose which information to present to the user . 5 . 3 Study Limitations & Future Work A limitation in our study was that due to the gender makeup of the community organizations we recruited , our study participants were primarily women . Future work might provide additional analysis of preferences among male and gender non - confirming or non - binary Black older adults . Results of our study may also have a different outcome among those who do not regularly have access to or use home computers and video conferencing software . Due to the timing of data collection during the global health pandemic , most of our participants had access to a home computer or laptop and videoconferencing software . Future studies might recruit among Black older adults that differ from this demographic . 6 CONCLUSION Our study sought to understand Black older adults’ perceptions and opinions of embodied conversational assistants in the form of text - based chatbots . We identified the impact of character identity in chatbot design on participants’ perceptions of trust and credibility , comfort , and relatability . Upon conducting a diary and interview study with 30 Black older adults our findings suggested that feelings of trust and comfort with chatbots are influenced by perceptions of authority and professionalism , and many participants found age and race should be incorporated in ways that are relatable yet not pandering . Additionally , we found that participants’ skepticism of chatbots is rooted in distrust of the healthcare system and that trust and positive perceptions towards chatbots goes beyond diversifying skin tones and gender . We present several design implications based on our findings that serve to improve the design of chatbot charac - ters and ECAs . Our work contributes to current research efforts in HCI that aim to make human - AI interactions more equitable and inclusive . ACKNOWLEDGMENTS This study was supported by a grant from the National Institutes of Health , P30 AG015281 , and the Michigan Center for Urban African American Aging Research . We acknowledge the participation of our community partners from the Renaissance Collaborative Senior Village in Chicago and the Healthier Black Elders Center in Detroit for their assistance in this research . REFERENCES [ 1 ] Emma Ahonen et al . 2020 . The effects of transactionality on trust in customer service chatbots . Master’s thesis . Aalto University . [ 2 ] Bayan Alsharbi and Deborah Richards . 2017 . Using virtual reality technology to improve reality for young people with chronic health conditions . In Proceedings of the 9th international conference on computer and automation engineering . ACM , New York , NY , 11 – 15 . [ 3 ] Monica Anderson and Andrew Perrin . 2017 . Tech adoption climbs among older adults . Technical Report 17 . Pew research center . [ 4 ] Renato FL Azevedo , Dan Morrow , James Graumlich , Ann Willemsen - Dunlap , Mark Hasegawa - Johnson , Thomas S Huang , Kuangxiao Gu , Suma Bhat , Tarek Sakakini , Victor Sadauskas , et al . 2018 . Using conversational agents to explain medication instructions to older adults . In AMIA annual symposium proceedings , Vol . 2018 . American Medical Informatics Association , Washington , DC , 185 . [ 5 ] Simar Singh Bajaj and Fatima Cody Stanford . 2021 . Beyond Tuskegee—vaccine distrust and everyday racism . New England Journal of Medicine 384 , 5 ( 2021 ) , e12 . [ 6 ] Belén Barros Pena , Rachel E Clarke , Lars Erik Holmquist , and John Vines . 2021 . Circumspect users : Older adults as critical adopters and resistors of technology . In Proceedingsofthe2021CHIConferenceonHumanFactorsinComputingSystems . ACM , New York , NY , USA , 1 – 14 . [ 7 ] Krystal Batelaan . 2022 . ‘It’s not the science we distrust ; it’s the scientists’ : Reframing the anti - vaccination movement within Black communities . Global Public Health 17 , 6 ( 2022 ) , 1099 – 1112 . [ 8 ] Cynthia L . Bennett , Cole Gleason , Morgan Klaus Scheuerman , Jeffrey P . Bigham , Anhong Guo , and Alexandra To . 2021 . “It’s Complicated” : Negotiating Accessibil - ityand ( Mis ) RepresentationinImageDescriptionsofRace , Gender , andDisability . In Proceedingsofthe2021CHIConferenceonHumanFactorsinComputingSystems ( Yokohama , Japan ) ( CHI ’21 ) . Association for Computing Machinery , New York , NY , USA , Article 375 , 19 pages . https : / / doi . org / 10 . 1145 / 3411764 . 3445498 [ 9 ] Timothy W Bickmore , Laura M Pfeifer , Donna Byron , Shaula Forsythe , Lori E Henault , Brian W Jack , Rebecca Silliman , and Michael K Paasche - Orlow . 2010 . Usability of conversational agents by patients with inadequate health literacy : evidence from two clinical trials . Journal of health communication 15 , S2 ( 2010 ) , 197 – 210 . [ 10 ] Timothy W Bickmore , Laura M Pfeifer , and Michael K Paasche - Orlow . 2009 . Using computer agents to explain medical documents to patients with low health literacy . Patient education and counseling 75 , 3 ( 2009 ) , 315 – 320 . [ 11 ] Kenneth A Blocker , Travis Kadylak , Lyndsie M Koon , Christopher E Kovac , and Wendy A Rogers . 2020 . Digital Home Assistants and Aging : Initial Perspec - tives from Novice Older Adult Users . In Proceedings of the Human Factors and Ergonomics Society Annual Meeting , Vol . 64 . SAGE Publications Sage CA , Los Angeles , CA , 1367 – 1371 . Issue 1 . [ 12 ] Erika Bonnevie , Tiffany D Lloyd , Sarah D Rosenberg , Kara Williams , Jaclyn Gold - barg , and Joe Smyser . 2021 . Layla’s Got You : Developing a tailored contraception chatbot for Black and Hispanic young women . Health Education Journal 80 , 4 ( 2021 ) , 413 – 424 . [ 13 ] Robin Brewer , Casey Pierce , Pooja Upadhyay , and Leeseul Park . 2022 . An Empirical Study of Older Adult’s Voice Assistant Use for Health Information Seeking . ACM Trans . Interact . Intell . Syst . 12 , 2 , Article 13 ( jul 2022 ) , 32 pages . https : / / doi . org / 10 . 1145 / 3484507 [ 14 ] R . N . Brewer , M . R . Morris , and S . E . Lindley . 2017 . How to Remember What to Remember : Exploring Possibilities for Digital Reminder Systems . Proc . ACM Interact . Mob . Wearable Ubiquitous Technol . 1 , 3 , Article 38 ( sep 2017 ) , 20 pages . https : / / doi . org / 10 . 1145 / 3130903 [ 15 ] Justine Cassell . 2001 . Embodied conversational agents : representation and intel - ligence in user interfaces . AI magazine 22 , 4 ( 2001 ) , 67 – 67 . [ 16 ] Zhifa Chen , Yichen Lu , Mika P . Nieminen , and Andrés Lucero . 2020 . Creating a Chatbot for and with Migrants : Chatbot Personality Drives Co - Design Activities . In Proceedings of the 2020 ACM Designing Interactive Systems Conference ( Eind - hoven , Netherlands ) ( DIS ’20 ) . Association for Computing Machinery , New York , NY , USA , 219 – 230 . https : / / doi . org / 10 . 1145 / 3357236 . 3395495 [ 17 ] Wendy Hui Kyong Chun . 2009 . Race and / as Technology , or How to do Things to Race . Camera Obscura : Feminism , Culture , and Media Studies 24 , 1 ( 70 ) ( may 2009 ) , 7 – 35 . https : / / doi . org / 10 . 1215 / 02705346 - 2008 - 013 [ 18 ] Caroline Claisse , Bakita Kasadha , Simone Stumpf , and Abigail C . Durrant . 2022 . Investigating Daily Practices of Self - Care to Inform the Design of Supportive Health Technologies for Living and Ageing Well with HIV . In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems ( New Orleans , LA , USA ) ( CHI ’22 ) . Association for Computing Machinery , New York , NY , USA , Article 524 , 19 pages . https : / / doi . org / 10 . 1145 / 3491102 . 3501970 [ 19 ] Leigh Clark , Nadia Pantidi , Orla Cooney , Philip Doyle , Diego Garaialde , Justin Edwards , BrendanSpillane , EmerGilmartin , ChristineMurad , CosminMunteanu , VincentWade , andBenjaminR . Cowan . 2019 . WhatMakesaGoodConversation ? Challenges in Designing Truly Conversational Agents . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( Glasgow , Scotland Uk ) ( CHI ’19 ) . Association for Computing Machinery , New York , NY , USA , 1 – 12 . https : / / doi . org / 10 . 1145 / 3290605 . 3300705 [ 20 ] Sara J Czaja , Joseph Sharit , Sankaran N Nair , and Chin Chin Lee . 2009 . Older adults and internet health information seeking . In Proceedings of the Human Factors and Ergonomics Society Annual Meeting , Vol . 53 . SAGE Publications Sage CA , Los Angeles , CA , 126 – 130 . [ 21 ] Birgit Endrass , Matthias Rehm , and Elisabeth André . 2009 . Culture - Specific Communication Management for Virtual Agents . In Proc . of 8th Int . Conf . on Autonomous Agents and Multiagent Systems ( AAMAS 2009 ) - Volume 1 ( Budapest , CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Harrington , et al Hungary ) ( AAMAS ’09 ) . International Foundation for Autonomous Agents and Multiagent Systems , Richland , SC , 281 – 287 . [ 22 ] NastaranHajiheydariandMahdiAshkani . 2018 . Mobileapplicationuserbehavior in the developing countries : A survey in Iran . Information Systems 77 ( 2018 ) , 22 – 33 . [ 23 ] LaToya N Hall , Lisa J Ficker , Letha A Chadiha , Carmen R Green , James S Jackson , and Peter A Lichtenberg . 2016 . Promoting retention : African American older adults in a research volunteer registry . Gerontology and geriatric medicine 2 ( 2016 ) , 2333721416677469 . [ 24 ] David Hankerson , Andrea R . Marshall , Jennifer Booker , Houda El Mimouni , Imani Walker , and Jennifer A . Rode . 2016 . Does Technology Have Race ? . In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems ( San Jose , California , USA ) ( CHI EA ’16 ) . Association for Computing Machinery , New York , NY , USA , 473 – 486 . https : / / doi . org / 10 . 1145 / 2851581 . 2892578 [ 25 ] AlexHanna , EmilyDenton , AndrewSmart , andJamilaSmith - Loud . 2020 . Towards a Critical Race Methodology in Algorithmic Fairness . In Proceedings of the 2020 Conference on Fairness , Accountability , and Transparency ( Barcelona , Spain ) ( FAT * ’20 ) . Association for Computing Machinery , New York , NY , USA , 501 – 512 . https : / / doi . org / 10 . 1145 / 3351095 . 3372826 [ 26 ] Christina Harrington , Sheena Erete , and Anne Marie Piper . 2019 . Deconstructing Community - Based Collaborative Design : Towards More Equitable Participatory Design Engagements . Proc . ACM Hum . - Comput . Interact . 3 , CSCW , Article 216 ( nov 2019 ) , 25 pages . https : / / doi . org / 10 . 1145 / 3359318 [ 27 ] Christina N . Harrington , Katya Borgos - Rodriguez , and Anne Marie Piper . 2019 . Engaging Low - Income African American Older Adults in Health Discussions through Community - Based Design Workshops . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( Glasgow , Scotland Uk ) ( CHI ’19 ) . Association for Computing Machinery , New York , NY , USA , 1 – 15 . https : / / doi . org / 10 . 1145 / 3290605 . 3300823 [ 28 ] ChristinaNHarrington , RadhikaGarg , AmandaWoodward , andDimitriWilliams . 2022 . “It’s Kind of Like Code - Switching” : Black Older Adults’ Experiences with a Voice Assistant for Health Information Seeking . In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems ( New Orleans , LA , USA ) ( CHI ’22 ) . Association for Computing Machinery , New York , NY , USA , Article 604 , 15 pages . https : / / doi . org / 10 . 1145 / 3491102 . 3501995 [ 29 ] Jennifer Hill , W Randolph Ford , and Ingrid G Farreras . 2015 . Real conversations with artificial intelligence : A comparison between human – human online con - versations and human – chatbot conversations . Computers in human behavior 49 ( 2015 ) , 245 – 250 . [ 30 ] Martha Hostetter and Sarah Klein . 2021 . Understanding and ameliorating medical mistrust among Black Americans . Technical Report . The Commonwealth Fund . [ 31 ] Mohit Jain , Pratyush Kumar , Ishita Bhansali , Q Vera Liao , Khai Truong , and Shwetak Patel . 2018 . FarmChat : a conversational agent to answer farmer queries . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technolo - gies 2 , 4 ( 2018 ) , 1 – 22 . [ 32 ] Mohit Jain , Pratyush Kumar , Ramachandra Kota , and Shwetak N Patel . 2018 . Evaluating and informing the design of chatbots . In Proceedings of the 2018 Designing Interactive Systems Conference . ACM , New York , NY , 895 – 906 . [ 33 ] Jakob D Jensen , Andy J King , LaShara A Davis , and Lisa M Guntzviller . 2010 . Utilizationofinternettechnologybylow - incomeadults : theroleofhealthliteracy , health numeracy , and computer assistance . Journal of aging and health 22 , 6 ( 2010 ) , 804 – 826 . [ 34 ] Theodore Jensen , Mohammad Maifi Hasan Khan , Md Abdullah Al Fahim , and YusufAlbayram . 2021 . TrustandAnthropomorphisminTandem : TheInterrelated Nature of Automated Agent Appearance and Reliability in Trustworthiness Perceptions . In Designing Interactive Systems Conference 2021 ( Virtual Event , USA ) ( DIS ’21 ) . Association for Computing Machinery , New York , NY , USA , 1470 – 1480 . https : / / doi . org / 10 . 1145 / 3461778 . 3462102 [ 35 ] Esther Jones . 2022 . Many black Americans aren’t rushing to get the covid - 19 vaccine – a long history of medical abuse suggests why . https : / / theconversation . com / many - black - americans - arent - rushing - to - get - the - covid - 19 - vaccine - a - long - history - of - medical - abuse - suggests - why - 152368 [ 36 ] KIM JUNHAN , JANA MUHIC , SUN YOUNG PARK , and LIONEL ROBERT . 2021 . Trustworthy Conversational Agent Design for African Americans with Chronic Conditions during COVID - 19 . In CHI 2021 Workshop Realizing AI in Healthcare : Challenges Appearing in the Wild . ACM , New York , NY , 1 – 7 . [ 37 ] Junhan Kim , Jana Muhic , Lionel Peter Robert , and Sun Young Park . 2022 . De - signing Chatbots with Black Americans with Chronic Conditions : Overcoming Challenges against COVID - 19 . In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems ( New Orleans , LA , USA ) ( CHI ’22 ) . Asso - ciation for Computing Machinery , New York , NY , USA , Article 439 , 17 pages . https : / / doi . org / 10 . 1145 / 3491102 . 3502116 [ 38 ] Rafal Kocielnik , Elena Agapie , Alexander Argyle , Dennis T Hsieh , Kabir Yadav , Breena Taira , and Gary Hsieh . 2019 . HarborBot : a chatbot for social needs screening . In AMIA Annual Symposium Proceedings , Vol . 2019 . American Medical Informatics Association , Rockville , MD , 552 . [ 39 ] Allison Koenecke , Andrew Nam , Emily Lake , Joe Nudell , Minnie Quartey , Zion Mengesha , Connor Toups , John R Rickford , Dan Jurafsky , and Sharad Goel . 2020 . Racial disparities in automated speech recognition . Proceedings of the National Academy of Sciences 117 , 14 ( 2020 ) , 7684 – 7689 . [ 40 ] V . M . Kumar , A . Keerthana , M . Madhumitha , S . Valliammai , and V . Vinithasri . 2017 . Sanative Chatbot For Health Seekers . International Journal of Engineering and Computer Science 5 , 3 ( Dec . 2017 ) , 16022 – 16025 . http : / / www . ijecs . in / index . php / ijecs / article / view / 720 [ 41 ] TERESA DE LAURETIS . 1987 . Technologies of Gender : Essays on Theory , Film , and Fiction . Indiana University Press , Indiana . http : / / www . jstor . org / stable / j . ctt16gzmbr [ 42 ] MinKyungLeeandKatherineRich . 2021 . WhoIsIncludedinHumanPerceptions of AI ? : Trust and Perceived Fairness around Healthcare AI and Cultural Mistrust . In Proceedingsofthe2021CHIConferenceonHumanFactorsinComputingSystems ( Yokohama , Japan ) ( CHI ’21 ) . Association for Computing Machinery , New York , NY , USA , Article 138 , 14 pages . https : / / doi . org / 10 . 1145 / 3411764 . 3445570 [ 43 ] Q Vera Liao and Wai - Tat Fu . 2014 . Age differences in credibility judgments of online health information . ACM Transactions on Computer - Human Interaction ( TOCHI ) 21 , 1 ( 2014 ) , 1 – 23 . [ 44 ] Weijane Lin , Hong - Chun Chen , and Hsiu - Ping Yueh . 2021 . Using Different Error Handling Strategies to Facilitate Older Users’ Interaction With Chatbots in Learning Information and Communication Technologies . Frontiers in Psychology 12 ( 2021 ) , 1 – 6 . https : / / doi . org / 10 . 3389 / fpsyg . 2021 . 785815 [ 45 ] Gabriela Marcu , Karina Caro , Juan Fernando Maestre , Kay H . Connelly , Robin Brewer , and Christina N . Harrington . 2019 . Strategies for Inclusion in the Design of Pervasive Computing for Health and Wellbeing . IEEE Pervasive Computing 18 , 1 ( 2019 ) , 89 – 93 . https : / / doi . org / 10 . 1109 / MPRV . 2019 . 2898485 [ 46 ] Mark C . Marino . 2014 . The Racial Formation of Chatbots . Clcweb - comparative Literature and Culture 16 ( 2014 ) , 13 . [ 47 ] AqueashaMartin - Hammond , SravaniVemireddy , andKartikRao . 2019 . Exploring older adults’ beliefs about the use of intelligent assistants for consumer health information management : A participatory design study . JMIR aging 2 , 2 ( 2019 ) , e15381 . [ 48 ] Rachel McDonnell , Martin Breidt , and Heinrich H Bülthoff . 2012 . Render me real ? Investigatingtheeffectofrenderstyleontheperceptionofanimatedvirtual humans . ACM Transactions on Graphics ( TOG ) 31 , 4 ( 2012 ) , 1 – 11 . [ 49 ] Sylvia Mendez , Katie Johanson , Valerie Martin Conley , Kinnis Gosha , Naja A Mack , Comas Haynes , and Rosario A Gerhardt . 2020 . Chatbots : A Tool to Supplement the Future Faculty Mentoring of Doctoral Engineering Students . International Journal of Doctoral Studies 15 ( 2020 ) , 373 – 392 . [ 50 ] Lisa M Soederberg Miller and Robert A Bell . 2012 . Online health information seeking : the influence of age , information trustworthiness , and search challenges . Journal of aging and health 24 , 3 ( 2012 ) , 525 – 541 . [ 51 ] EnidNHMontague , BrianMKleiner , andWoodrowWWinchester . 2009 . Empiri - callyunderstandingtrustinmedicaltechnology . InternationalJournalofIndustrial Ergonomics 39 , 4 ( 2009 ) , 628 – 634 . https : / / doi . org / 10 . 1016 / j . ergon . 2009 . 01 . 004 [ 52 ] John Morkes , Hadyn K Kernal , and Clifford Nass . 1999 . Effects of humor in task - oriented human - computer interaction and computer - mediated communication : A direct test of SRCT theory . Human - Computer Interaction 14 , 4 ( 1999 ) , 395 – 435 . [ 53 ] Daniel G Morrow , H Chad Lane , and Wendy A Rogers . 2021 . A framework for design of conversational agents to support health self - care for older adults . Human Factors 63 , 3 ( 2021 ) , 369 – 378 . [ 54 ] BradfordWMott , JamesCLester , andKarlBranting . 2004 . ConversationalAgents . In The Practical Handbook of Internet Computing . Citeseer , University Park , PA , 220 – 240 . [ 55 ] Clifford Nass and Youngme Moon . 2000 . Machines and mindlessness : Social responses to computers . Journal of social issues 56 , 1 ( 2000 ) , 81 – 103 . [ 56 ] Clifford Nass , Jonathan Steuer , and Ellen R . Tauber . 1994 . Computers Are Social Actors . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Boston , Massachusetts , USA ) ( CHI ’94 ) . Association for Computing Machinery , New York , NY , USA , 72 – 78 . https : / / doi . org / 10 . 1145 / 191666 . 191703 [ 57 ] Teresa K . O’Leary , Elizabeth Stowell , Jessica A . Hoffman , Michael Paasche - Orlow , Timothy Bickmore , and Andrea G . Parker . 2021 . Examining the Intersections of Race , Religion & Community Technologies : A Photovoice Study . In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems ( Yokohama , Japan ) ( CHI ’21 ) . Association for Computing Machinery , New York , NY , USA , Article 698 , 19 pages . https : / / doi . org / 10 . 1145 / 3411764 . 3445418 [ 58 ] Teresa K O’Leary , Elizabeth Stowell , Everlyne Kimani , Dhaval Parmar , Stefan Olafsson , Jessica Hoffman , Andrea G Parker , Michael K Paasche - Orlow , and Timothy Bickmore . 2020 . Community - based cultural tailoring of virtual agents . In Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents . ACM , New York , NY , 1 – 8 . [ 59 ] Dhaval Parmar , Stefan Olafsson , Dina Utami , and Timothy Bickmore . 2018 . Look - ing the Part : The Effect of Attire and Setting on Perceptions of a Virtual Health Counselor . In Proceedingsofthe18thInternationalConferenceonIntelligentVirtual Agents ( Sydney , NSW , Australia ) ( IVA’18 ) . AssociationforComputingMachinery , New York , NY , USA , 301 – 306 . https : / / doi . org / 10 . 1145 / 3267851 . 3267915 Black Older Adults’ Perceptions of Chatbot Design for Health Information Seeking CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany [ 60 ] Alisha Pradhan , Amanda Lazar , and Leah Findlater . 2020 . Use of intelligent voice assistants by older adults with low technology use . ACM Transactions on Computer - Human Interaction ( TOCHI ) 27 , 4 ( 2020 ) , 1 – 27 . [ 61 ] Astrid Rosenthal - Von Der Pütten and Nikolai Bock . 2018 . Development and validation of the self - efficacy in human - robot - interaction scale ( se - hri ) . ACM Transactions on Human - Robot Interaction ( THRI ) 7 , 3 ( 2018 ) , 1 – 30 . [ 62 ] Lova Rajaobelina , Sandrine Prom Tep , Manon Arcand , and Line Ricard . 2021 . Creepiness : Itsantecedentsandimpactonloyaltywheninteractingwithachatbot . Psychology & Marketing 38 , 12 ( 2021 ) , 2339 – 2356 . [ 63 ] S . Zahra Razavi , Lenhart K . Schubert , Kimberly van Orden , Mohammad Rafayet Ali , Benjamin Kane , and Ehsan Hoque . 2022 . Discourse Behavior of Older Adults Interacting with a Dialogue Agent Competent in Multiple Topics . ACM Trans . Interact . Intell . Syst . 12 , 2 , Article 14 ( jul 2022 ) , 21 pages . https : / / doi . org / 10 . 1145 / 3484510 [ 64 ] Lazlo Ring , Dina Utami , and Timothy Bickmore . 2014 . The right agent for the job ? . In International Conference on Intelligent Virtual Agents . Springer , New York , NY , 374 – 384 . [ 65 ] AriSchlesinger , KentonP . O’Hara , andAlexS . Taylor . 2018 . Let’sTalkAboutRace : Identity , Chatbots , and AI . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI ’18 ) . Association for Computing Machinery , New York , NY , USA , 1 – 14 . https : / / doi . org / 10 . 1145 / 3173574 . 3173889 [ 66 ] Joseph Seering , Michal Luria , Geoff Kaufman , and Jessica Hammer . 2019 . Beyond dyadicinteractions : Consideringchatbotsascommunitymembers . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . ACM , New York , NY , 1 – 13 . [ 67 ] Patricia Simon , Suchitra Krishnan - Sarin , and Ting - Hao’Kenneth’ Huang . 2019 . On using chatbots to promote smoking cessation among adolescents of low socioeconomic status . In Conference : Artificial Intelligence and Work : Association for the Advancement of Artificial Intelligence ( AAAI ) 2019 Fall Symposium ( AAAI FSS 2019 ) . Association for the Advancement of Artificial Intelligence , Palo Alto , CA , 1 – 3 . [ 68 ] Angela D . R . Smith , Alex A . Ahmed , Adriana Alvarado Garcia , Bryan Dosono , IhudiyaOgbonnaya - Ogburu , YolandaRankin , AlexandraTo , andKentaroToyama . 2020 . What’s Race Got To Do With It ? Engaging in Race in HCI . In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems ( Honolulu , HI , USA ) ( CHI EA ’20 ) . Association for Computing Machinery , New York , NY , USA , 1 – 8 . https : / / doi . org / 10 . 1145 / 3334480 . 3375156 [ 69 ] Kanyarat Sriwisathiyakun and Chawaporn Dhamanitayakul . 2022 . Enhancing digital literacy with an intelligent conversational agent for senior citizens in Thailand . Education and Information Technologies 27 , 5 ( 2022 ) , 6251 – 6271 . https : / / doi . org / 10 . 1007 / s10639 - 021 - 10862 - z [ 70 ] Brodrick Stigall , Jenny Waycott , Steven Baker , and Kelly Caine . 2020 . Older Adults’ Perception and Use of Voice User Interfaces : A Preliminary Review of the Computing Literature . In Proceedings of the 31st Australian Conference on Human - Computer - Interaction ( Fremantle , WA , Australia ) ( OZCHI’19 ) . Association for Computing Machinery , New York , NY , USA , 423 – 427 . https : / / doi . org / 10 . 1145 / 3369457 . 3369506 [ 71 ] Anselm Strauss and Juliet Corbin . 1994 . Grounded theory methodology : An overview . In Handbookofqualitativeresearch . SagePublications , Inc , LosAngeles , CA , 273 – 285 . [ 72 ] RuthMTappen , MaryECooley , RogerLuckmann , andSomiPanday . 2022 . Digital Health Information Disparities in Older Adults : a Mixed Methods Study . Journal of Racial and Ethnic Health Disparities 9 , 1 ( 2022 ) , 82 – 92 . https : / / doi . org / 10 . 1007 / s40615 - 020 - 00931 - 3 [ 73 ] Silke Ter Stal , Marijke Broekhuis , Lex van Velsen , Hermie Hermens , Monique Tabak , et al . 2020 . Embodied conversational agent appearance for health as - sessment of older adults : explorative study . JMIR human factors 7 , 3 ( 2020 ) , e19987 . [ 74 ] AlexandraTo , WenxiaSweeney , JessicaHammer , andGeoffKaufman . 2020 . " They Just Don’t Get It " : Towards Social Technologies for Coping with Interpersonal Racism . Proc . ACM Hum . - Comput . Interact . 4 , CSCW1 , Article 24 ( may 2020 ) , 29 pages . https : / / doi . org / 10 . 1145 / 3392828 [ 75 ] Anne M Turner , Katie P Osterhage , Jean O Taylor , Andrea L Hartzler , and George Demiris . 2018 . A closer look at health information seeking by older adults and involved family and friends : design considerations for health information tech - nologies . In AMIA annual symposium proceedings , Vol . 2018 . American Medical Informatics Association , Rockville , MD , 1036 . [ 76 ] Zoe Vorsino . 2021 . Chatbots , Gender , and Race on Web 2 . 0 Platforms : Tay . AI as Monstrous Femininity and Abject Whiteness . Signs : Journal of Women in Culture and Society 47 ( 2021 ) , 105 – 127 . [ 77 ] Lindsay Wells and Arjun Gowda . 2020 . A legacy of mistrust : African Americans and the US healthcare system . Proceedings of UCLA Health 24 ( 2020 ) , 1 – 3 . [ 78 ] Arlette Wissen , Charlotte Vinkers , and Aart Halteren . 2016 . Developing a Virtual Coach for Chronic Patients : A User Study on the Impact of Similarity , Familiarity and Realism . In Proceedings of the 11th International Conference on Persuasive Technology - Volume9638 ( Salzburg , Austria ) ( PERSUASIVE2016 ) . Springer - Verlag , Berlin , Heidelberg , 263 – 275 . https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 31510 - 2 _ 23 [ 79 ] Hyunwoo Yoon , Yuri Jang , Phillip W Vaughan , and Michael Garcia . 2020 . Older Adults’ Internet Use for Health Information : Digital Divide by Race / Ethnicity and Socioeconomic Status . Journal of Applied Gerontology 39 , 1 ( 2020 ) , 105 – 110 . https : / / doi . org / 10 . 1177 / 0733464818770772 [ 80 ] Yixuan Zhang , Nurul Suhaimi , Rana Azghandi , Mary Amulya Joseph , Miso Kim , Jacqueline Griffin , and Andrea G . Parker . 2020 . Understanding the Use of Crisis Informatics Technology among Older Adults . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems ( Honolulu , HI , USA ) ( CHI ’20 ) . Association for Computing Machinery , New York , NY , USA , 1 – 13 . https : / / doi . org / 10 . 1145 / 3313831 . 3376862 [ 81 ] Yuxiang Chris Zhao , Mengyuan Zhao , and Shijie Song . 2022 . Online Health Information Seeking BehaviorsAmong Older Adults : Systematic Scoping Review . J Med Internet Res 24 , 2 ( feb 2022 ) , e34790 . https : / / doi . org / 10 . 2196 / 34790 [ 82 ] Naim Zierau , Korbinian Flock , Andreas Janson , Matthias Söllner , and Jan Marco Leimeister . 2021 . The Influence of AI - Based Chatbots and Their Design on Users’ Trust and Information Sharing in Online Loan Applications . In Hawaii International Conference on System Sciences ( HICSS ) . - Koloa ( Hawaii ) , USA . Social Science Research Network , Rochester , NY , 1 – 11 . A PARTICIPANT DEMOGRAPHICS CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Harrington , et al PID ( Gender ) Age Chatbot Assignment Prior Chatbot Use P01 ( F ) 72 older white male Not used P02 ( F ) 74 older white male Not used P03 ( F ) 67 younger white female Not used P04 ( F ) 73 older black female Not used P05 ( F ) 76 younger black male Not sure what it is P06 ( M ) 71 older black female Not used P07 ( F ) 65 younger white female Not sure what it is P08 ( M ) 75 younger black male Not used P09 ( F ) 71 older black female Not used P10 ( F ) 66 older white male Not used P11 ( F ) 71 older black female Not used P19 ( F ) 75 younger white female Not used P22 ( F ) 74 older black female Not sure what it is P23 ( F ) 65 younger black male Not used P24 ( F ) 66 younger white female Used occasionally P26 ( F ) 70 older black female Not used P27 ( F ) 65 younger black male Used frequently P28 ( M ) 79 younger white female Not used P29 ( F ) 67 older white male Used occasionally P30 ( F ) 72 older black female Not used P31 ( F ) 63 younger black male Used once P32 ( F ) 72 younger white female Used occasionally P34 ( F ) 71 younger black male Used occasionally P36 ( F ) 72 younger white female Not used P37 ( F ) 78 older white male Not sure what it is P38 ( F ) 60 older black female Not used P39 ( M ) 67 younger black male Used occasionally P40 ( M ) 84 younger white female Not sure what it is P42 ( F ) 76 older black female Not sure what it is P43 ( F ) 78 younger black male Used occasionally Table 3 : Participant Demographics & Prior Experience with Text - based Chatbots .