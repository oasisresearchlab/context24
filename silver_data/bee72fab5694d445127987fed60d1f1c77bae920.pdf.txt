Math . Program . , Ser . B ( 2008 ) 112 : 159 – 181 DOI 10 . 1007 / s10107 - 006 - 0089 - x FULL LENGTH PAPER Accelerating the cubic regularization of Newton’s method on convex problems Yu . Nesterov Received : 23 January 2006 / Published online : 20 January 2007 © Springer - Verlag 2007 Abstract In this paper we propose an accelerated version of the cubic regu - larization of Newton’s method ( Nesterov and Polyak , in Math Program 108 ( 1 ) : 177 – 205 , 2006 ) . The original version , used for minimizing a convex function with Lipschitz - continuous Hessian , guarantees a global rate of convergence of order O (cid:1) 1 k 2 (cid:2) , where k is the iteration counter . Our modiﬁed version converges for the same problem class with order O (cid:1) 1 k 3 (cid:2) , keeping the complexity of each iteration unchanged . We study the complexity of both schemes on different classes of convex problems . In particular , we argue that for the second - order schemes , the class of non - degenerate problems is different from the standard class . Keywords Convex optimization · Unconstrained minimization · Newton’s method · Cubic regularization · Worst - case complexity · Global complexity bounds · Non - degenerate problems · Condition number Mathematics Subject Classiﬁcation ( 2000 ) 49M15 · 49M37 · 58C15 · 90C25 · 90C30 The research results presented in this paper have been supported by a grant “Action de recherche concertè ARC 04 / 09 - 315” from the “Direction de la recherche scientiﬁque - Communautè française de Belgique” . The scientiﬁc responsibility rests with the author . Yu . Nesterov ( B ) Center for Operations Research and Econometrics ( CORE ) , Catholic University of Louvain ( UCL ) , 34 voie du Roman Pays , 1348 Louvain - la - Neuve , Belgium e - mail : nesterov @ core . ucl . ac . be 160 Yu . Nesterov 1 Introduction 1 . 1 Motivation Newton’s method is one of the oldest schemes in Numerical Analysis [ 1 ] . The ﬁrst theoretical study of its local performance was carried out in [ 4 ] . During many years , numerous useful suggestions were developed for stabilizing its local behavior ( a detailed description of the results and references can be found in the comprehensive monographs [ 2 , 3 ] ) . However , the global worst - case com - plexity analysis for the second - order schemes was only given recently . In [ 6 ] a cubic regularization of the Newton’s method ( CNM ) , which can be applied to a general smooth unconstrained minimization problem , was pro - posed . The main advantage of this scheme consists in its natural geometrical interpretation : At each iteration we minimize a cubic model , which appears to be a global upper estimate for the objective function . This key feature ensures predictable global behavior of the process . In [ 6 ] , attention was mainly paid to different classes of nonconvex problems . For convex problems , some of the statements of [ 6 ] can be strengthened . More - over , we can employ convex optimization techniques for accelerating the local scheme . In this paper we assume that the objective function of a convex uncon - strained minimization problem has Lipschitz - continuous Hessian ( that is the basic problem class under consideration ) . As was shown in [ 6 ] , the global rate of convergence of CNM on this problem class is of the order O ( 1 k 2 ) , where k is the iteration counter . However , note that CNM is a local one - step second - order method . From the complexity theory of smooth convex optimization ( see , for example , Chap . 2 in [ 5 ] ) , it is known that the rate of convergence of the local one - step ﬁrst - order method ( that is just a gradient method ) can be improved from O (cid:1) 1 k (cid:2) to O (cid:1) 1 k 2 (cid:2) by passing to a multi - step strategy . In this paper we show that a similar trick works with CNM also . As a result , we get a new method , which converges on the speciﬁed class of problems as O (cid:1) 1 k 3 (cid:2) . 1 . 2 Contents Section 2 contains all necessary results related to regular functions . Namely , we present the main properties of uniformly convex functions and functions with Lipschitz - continuous derivatives of a certain degree . In Sect . 3 we introduce a cubic regularization of the Newton step and prove several useful inequalities . At the end of this section we show that CNM , as applied to a convex function from the basic problem class , converges globally as O (cid:1) 1 k 2 (cid:2) . The majority of the results of this section can be found in [ 6 ] , but in a slightly weaker form . In Sect . 4 we derive an accelerated multi - step version of CNM . We prove that on the basic problem class it converges as O ( 1 k 3 ) . This acceleration is achieved by a modiﬁcation of the technique of estimate sequences described in Sect . 2 . 2 . 1 [ 5 ] .