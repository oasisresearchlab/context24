Towards a Consensus Gesture Set : A Survey of Mid - Air Gestures in HCI for Maximized Agreement Across Domains Masoumehsadat Hosseini University of Oldenburg Oldenburg , Germany masoumehsadat . hosseini @ uni - oldenburg . de Tjado Ihmels University of Oldenburg Oldenburg , Germany frank . tjado . ihmels @ uni - oldenburg . de Ziqian Chen Saarland University Saarbrücken , Germany s8zichen @ stud . uni - saarland . de Marion Koelle OFFIS – Institute for Information Technology Oldenburg , Germany marion . koelle @ ofs . de Heiko Müller OFFIS – Institute for Information Technology Oldenburg , Germany heiko . mueller @ ofs . de Susanne Boll University of Oldenburg Oldenburg , Germany susanne . boll @ uni - oldenburg . de ABSTRACT Mid - air gesture - based systems are becoming ubiquitous . Many mid - air gestures control diferent kinds of interactive devices , ap - plications , and systems . They are , however , still targeted at spe - cifc devices in specifc domains and are not necessarily consistent across domain boundaries . A comprehensive evaluation of the trans - ferability of gesture vocabulary between domains is also lacking . Consequently , interaction designers cannot decide which gestures to use for which domain . In this systematic literature review , we contribute to the future research agenda in this area , based on an analysis of 172 papers . As part of our analysis , we clustered gestures according to the dimensions of an existing taxonomy to identify their common characteristics in diferent domains , and we investigated the extent to which existing mid - air gesture sets are consistent across diferent domains . We derived a consensus gesture set containing 22 gestures based on agreement rates calculation and considered their transferability across diferent domains . CCS CONCEPTS • Human - centered computing → Interaction paradigms ; In - teraction devices ; Interaction design process and methods . KEYWORDS Mid - air gestures , systematic literature review , agreement rate , ap - plication domain ACM Reference Format : Masoumehsadat Hosseini , Tjado Ihmels , Ziqian Chen , Marion Koelle , Heiko Müller , and Susanne Boll . 2023 . Towards a Consensus Gesture Set : A Survey of Mid - Air Gestures in HCI for Maximized Agreement Across Domains . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page . Copyrights for components of this work owned by others than the author ( s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specifc permission and / or a fee . Request permissions from permissions @ acm . org . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany © 2023 Copyright held by the owner / author ( s ) . Publication rights licensed to ACM . ACM ISBN 978 - 1 - 4503 - 9421 - 5 / 23 / 04 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3544548 . 3581420 Systems ( CHI ’23 ) , April 23 – 28 , 2023 , Hamburg , Germany . ACM , New York , NY , USA , 24 pages . https : / / doi . org / 10 . 1145 / 3544548 . 3581420 1 INTRODUCTION Technological advancements have transformed the way humans interact with computers , going from command - line interfaces , the frst GUIs , and pointing devices [ 171 ] over multi - touch gestures [ 76 ] on surfaces to spatial , free - hand gestures performed in mid - air . As paradigm shifts have occurred in the past , such as the switch from WIMP 1 to direct touch interactions , guidelines and conventions have had to be renegotiated to support the broad adoption of the new paradigm across applications and domains . The importance of such guidelines and conventions lies in ensuring consistency , which in turn allows users to transfer knowledge between diferent devices and applications . Due to their ability to be performed eyes - free [ 80 ] and their perceived naturalness , intuitiveness , and ease of learning [ 17 , 79 ] , mid - air ( or free - hand ) gestures have become a compelling and increasingly popular interaction paradigm . To date , however , there is no established convention for ensuring that a set of mid - air gestures is appropriate for a particular domain . A large body of research exists investigating gestures for a cer - tain device , context , application or tracking technology subsumed . Those gesture sets are usually designed for and targeted at specifc settings . They do not ( necessarily ) span across diferent devices , scenarios or domains . Very few works have tackled cross - device con - sistency . Vatavu [ 187 ] conducted two device - specifc guessability studies , one for handheld interaction and one for free - hand interac - tion in a home entertainment scenario . He found that only 9 out of 22 gestures were consistent in both settings . Dingler et al . [ 53 ] designed a set of touch gestures that successfully transferred across smartphones , smart watches , and smart glasses . Yet , with comput - ing technology continuing to reach all aspects of people’s lives , users will expect gestures to transfer not only between diferent devices but also between diferent contexts and environments , i . e . , cross - domain consistency . Reaching transferability of gesture sets is essential for a seamless user experience and will require both cross - device consistency and cross - domain consistency . 1 “windows , icons , menus , pointer” CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Hosseini , et al . The vision of universally valid gesture sets is compelling , particu - larly in the context of smart environments and pervasive computing technologies , where interfaces might blend in with furniture , appli - ances or other everyday objects . Here , cross - domain consistency is especially crucial for mid - air gestures as it is often hard to com - municate which gestures are available , e . g . , in a specifc room . As a result , it is difcult for users to discover , learn , and memorize new gestures . For instance , a user controlling an ensemble of lights in an intelligent living room would use one gesture to select the lights in question , then a gesture to indicate the desired function - ality ( e . g . , light intensity ) , and then a gesture to set the intensity level ( e . g . , the degree of brightness ) . Ideally , the same sequence of gestures would yield exactly the same result when performed in any other room , even including other types of rooms , such as hotel or hospital rooms , ofces , classrooms , gymnasiums or large lecture halls . Extending this line of thought , functionalities would transfer across applications : the user might use the same gesture to modulate other intensity levels , such as the volume of music while driving in the car or watching TV . Yet , it is unclear how far we are from implementing this vision . It remains unknown whether and to what extent existing gesture sets presented in HCI literature already possess the potential to transfer across domains and what challenges will need to be overcome to achieve full cross - domain consistency . This paper investigates cross - domain consistency and adds to the studies that have examined consistency beyond a specifc domain or device in the context of mid - air gestural interaction . Based on a systematic literature review ( SLR ) of N = 172 papers , we investigate the extent to which existing mid - air gesture sets are consistent across diferent application domains . We make use of agreement rates , an established measure originally proposed by Wobbrock et al . [ 199 ] , to determine the consensus between diferent gestures proposed in literature and compile a consensus gesture set . In summary , this paper makes the following contributions : • An overview of the diverse application domains for which mid - air gestural interaction has been researched . Mid - air gestural interactions are used in a wide variety of application domains and scenarios , ranging from media & entertainment , over human - robot interaction and mobile interaction to medical technology . • An in - depth understanding of the characteristics of mid - air ges - tures in the diferent application domains . Collecting and analyz - ing representative mid - air gestures from literature , we classify each gesture based on its Nature , Binding , Form , Flow , and used Body Parts . • An investigation of cross - domain consistency resulting in a con - sensus gesture set . We determined agreement rates for diferent referents suggested in literature . We elaborate on the challenges of agreeing on a consistent use of gesture set across diferent application domains and compile a consensus gesture set . Our analysis shows that although full cross - domain consistency of mid - air gestures in HCI has not been reached ( yet ) , a consensus gesture set of 22 gestures can be proposed based on our analysis . 2 BACKGROUND AND PRIOR WORK This work is most closely related to several areas of prior research in HCI , namely gesture surveys & meta analyses , gesture taxonomies and prior work on end - user elicitation studies and agreement rates . 2 . 1 Gesture Surveys & Meta Analyses Previous work has contributed to gaining an overview of mid - air or 3D gestures . In a meta - review of 3D gestures , Groenewald et al . [ 72 ] surveyed 65 papers regarding how mid - air hand gestures were developed and used . They also grouped the gestures accord - ing to a classifcation scheme and identifed how these gestures had been empirically evaluated . Their research showed that hand gestures have been designed primarily for selection , navigation , and manipulation tasks . This fnding implies that 3D gestures have typically mimicked WIMP interfaces . In another categorization study , Aigner et al . [ 4 ] analyzed 5 , 500 mid - air hand gestures in terms of hand usage and gesture type using a novel classifcation scheme . Their classifcation , however , remained on the functional level of the gesture , and gestures were gathered from 12 participants purely independent of the situation and context . Karam and schrae - fel [ 89 ] analyzed a large body of HCI literature to collect knowledge about gestures and to understand if there are standardized terms to describe gestures , “to move gesture - based interactions out of the research labs and into everyday computing applications” . Specifc to the smart home domain , Kühnel et al . [ 101 ] elicited 3D gestures for controlling various household objects such as lamps , televisions , and blinds . However , the gestures were performed with a mobile phone in hand , with the implementation using accelerometer data for gesture recognition . Vuletic et al . [ 192 ] conducted an SLR on hand gestures in human computer interaction and mentioned the Gesture Elicitation Study ( GES ) method , but GES were not the main goal of their investigation . Villarreal - Narvaez et al . [ 190 ] sur - veyed 216 gesture elicitation studies , showing that GES have not yet reached full maturity , which indicates that there will be many more gesture vocabularies in the future . Following an exhaustive literature review , a more recent work by Xia et al . [ 201 ] identifed 13 crucial factors for gesture vocabulary design and examined the evaluation and interaction techniques commonly associated with each factor . In summary , consistency and transferability of gestures across domains have yet to be explored . In this work we examine HCI literature to gain an understanding about the transferability of mid - air gestures across diferent domains . 2 . 2 Gesture Taxonomies There are several high - level classifcation schemes of gesture types and gestural interactions that are applicable to HCI . Yet , classifca - tion can difer depending on the context . A recent work in this space by Karam and schraefel [ 88 ] provides a comprehensive classifcation of gesture types in the feld of HCI . Starting from the observation that gestures have diferent forms in diferent application domains , they classify gestures based on four diferent dimensions : gesture style , input , application domain , and output systems . While they categorize gestures based on application domain , they do not inves - tigate consistency and don’t explicitly compare across domains . In their seminal work on gesture elicitation , Wobbrock et al . [ 200 ] look at a specifc application domain , surface computing , and addressed Towards a Consensus Gesture Set CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany a gesture classifcation based on the user’s mental model and their behavior . Ruiz et al . [ 157 ] addressed gesture classifcation for mo - tion gestures for mobile interaction , where they addressed physical characteristics of gestures as dimensions of their taxonomy . Bostan et al . [ 21 ] focused on gesture classifcation of micro - gestures , while Arefn et al . [ 10 ] defned a taxonomy based on mapping features of gestures and their physical characteristics for smartwatches . De - spite the extensive discussion of gesture classifcation , less focus has been placed on the diferences between the proposed gestures in diferent application domains . Our work adopts dimensions of existing taxonomies appropriate for building our classifcation sys - tem and reports which kinds of gestures are being used to interact with interactive objects across diferent domains of HCI . 2 . 3 End - User Elicitation Studies and Agreement Rates Finding gestures that feel natural and can be intuitively “guessed” by end - users , is a key goal in HCI . Throughout the last two decades , gesture elicitation studies ( GES ) or “guessability studies” have been gaining popularity as an approach to democratize gesture design . Many of the key concepts of gesture elicitation were introduced by Nielsen et al . [ 134 ] and Wobbrock et al . [ 199 , 200 ] . Wobbrock et al . [ 199 ] investigated how to maximize gesture guessability with a bottom - up approach , allowing end - users to propose gestures for actions . Wobbrock [ 200 ] further summarized GES as a method where participants are provided with referents or actions , and then perform gestures that cause those actions . Since then , many user - defned gesture sets have been developed . A core component of end - user elicitation is the concept of agreement , which indicates when gestures proposed by participants are , essentially , the same or at least substantially similar . Agreement has been evaluated in various ways . Wobbrock et al . [ 199 ] provided an agreement mea - sure to analyze and interpret elicited data . This measure has been widely adopted by other elicitation studies [ 165 , 189 ] . Vatavu and Wobbrock [ 188 ] later refned this measure to more accurately rep - resent fndings , although there is debate around the accuracy of the agreement measures and concerns about over - optimistic esti - mates [ 182 ] . Further measures for the level of agreement are agree - ment rate [ 63 ] , max - consensus and consensus - distinct ratios [ 126 ] , or chance - corrected coefcients of agreement , and the growth rate of the dissimilarity - consensus curve . Cafaro et al . Cafaro et al . [ 23 ] suggest framing the gestures through “Embodied Allegories” to reach a better consensus among users . A review of a large num - ber of publications by Villarreal - Narvaez et al . [ 190 ] shows that agreement measures have largely been used for analyzing gesture elicitation data . Our work extends the related work by taking a cross - domain perspective and by making use of agreement rates as a tool to investigate the consistency of gestures across domains . Some GES were designed for various platforms and devices , such as interactive surfaces [ 128 ] , smartphones [ 157 ] , smart TVs [ 186 ] , tabletops and large displays [ 33 ] . GES are designed either for a user - independent mode ( i . e . , no particular profle of users was involved ) or a user - dependent mode ( particular profles of users were included ) , for example , Connell et al . [ 39 ] examined child - defned gestures . Further , GES looked at diferent types of users , their capabilities , and how their culture infuences the proposed gestures [ 121 ] . Diferent body parts were also targeted in GES , ranging from micro [ 30 ] and on - skin gestures [ 21 ] , unimanual or bimanual gestures [ 29 ] , and foot - based inputs [ 124 ] to full body interaction [ 7 ] . Studies designed for diferent environments and locations signifcantly infuenced the results of the gesture sets : for example , public settings , where social acceptance played an important role [ 97 ] ; in - vehicle user interfaces , where creation of ergonomic and easy - to - use interfaces without compromising safety is a challenge [ 96 ] ; or high - risk environments such as operating rooms , where the key challenge is to meet sterility requirements and to create precise user interfaces [ 162 ] . This work strives to es - tablish a set of gestures with high cross - domain agreement , making it suitable for use in many applications – a consensus gesture set . 3 METHOD Our method to investigate cross - domain consistency is to collect a large , representative sample of papers using systematic search techniques [ 40 , 139 ] . To this end , we systematically created a corpus of relevant papers on mid - air gestural interaction . We then extracted gestures from the corpus and coded them in terms of application domain and gesture characteristics , and analyzed agreement rates . 3 . 1 Corpus Generation We employed the PRISMA guidelines’ fve - phase process [ 139 ] to create our corpus : ( i ) exploration , ( ii ) identifcation , ( iii ) screening , ( iv ) eligibility , and ( v ) inclusion . Exploration . We exploratively surveyed the existing research landscape on mid - air gestures to identify prominent keywords and publication outlets . We closely examined the search results of dif - ferent combinations of “mid - air” and “gestural interaction” as well as synonyms in combination with domain - specifc keywords such as “smart home” or “mixed reality” . This process ensured that our search terms ( cf . , Table 1 ) would be consistent with terms used across diferent relevant domains . For the fnal appraisal of search terms we prioritized seminal publications and frequently cited pa - pers , but we also used our own expertise of gestural interaction as domain to identify relevant and frequently employed keywords . We compiled a search term that returned a comprehensive list of 2078 initial search results across all databases . Identifcation . We compiled a search term ( cf . , Table 1 ) across seven online publication outlets : ACM Digital Library 2 , IEEEX - plore 3 , Taylor & Francis 4 , SpringerLink 5 , Elsevier ScienceDirect 6 , SagePub 7 and MDPI 8 . We targeted all felds including keywords , title , abstract , and full text of the paper in our search . We did not em - ploy any restrictions in terms of publication date , but we limited our search to publications available as open access or via institutional subscriptions . 2 ACM Digital Library , https : / / dl . acm . org , accessed 23 / 01 / 2023 3 IEEEXplore , https : / / ieeexplore . ieee . org , accessed 23 / 01 / 2023 4 Taylor & Francis , https : / / www . tandfonline . com , accessed 23 / 01 / 2023 5 SpringerLink , https : / / link . springer . com , accessed 23 / 01 / 2023 6 Elsevier ScienceDirect , https : / / www . sciencedirect . com , accessed 23 / 01 / 2023 7 SagePub , https : / / journals . sagepub . com / , accessed 23 / 01 / 2023 8 MDPI , https : / / www . mdpi . com / , accessed 23 / 01 / 2023 CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Hosseini , et al . Our search on April 6 , 2021 , yielded an initial set of 2078 papers across all outlets . After removing broken links and duplicates 2057 papers were kept for screening . Table 1 : Search terms used to search seven common online publication outlets . ( " Mid - Air " OR " In - Air " OR " 3D " OR " Kinaesthetic " ) AND ( " Gesture Interaction " OR " Gesture Interactions " OR " Gestural Interaction " OR " Gestural Interactions " OR " Gesture Elicitation " ) . Screening . We evaluated the relevance of all 2057 papers that re - sulted from our search term based on their titles and abstracts . Three of the authors reviewed all references independently and in parallel used the following screening criteria for inclusion and exclusion : ( 1 ) We included only papers written in English ; ( 2 ) We included only full papers published in peer - reviewed journals , conferences , sym - posiums or workshops . Notably , this excluded technical reports , dissertations or other theses , work - in - progress reports , patents , posters , and workshop or tutorial proposals ; ( 3 ) We fltered for topi - cal match , which at this stage included all papers that ( a ) employed mid - air gestures to control an interactive system or ( b ) evaluated and / or compared a set of mid - air gestures . We did not apply restric - tions regarding specifc body parts , methodologies , user groups , or application domains ; ( 4 ) We excluded papers on mid - air gestures assisted by a physical medium such as pens , wands or remote con - trols ( e . g . , using the Wii Remote as input device [ 22 , 161 ] ) ; ( 5 ) We excluded papers not pertaining to our domains of interest , human - computer interaction ( HCI ) or human - robot interaction ( HRI ) , e . g . , works on co - verbal gesturing , dancing , or sign - language . We made use of Ouzzani et al . ’s Rayyan 9 to facilitate collaborative screening . We determined inter - rater agreement between the three authors performing the screening based on Fleiss’ kappa , which indicated substantial agreement  = . 665 ( 95 % CI , . 642 to . 688 ) ,  < . 001 . Dis - crepancies between the reviewers’ assessments were then discussed and conficts resolved . In total , we kept 288 papers . Eligibility . To determine eligibility for further analysis , we re - viewed all 288 candidate papers at full - text level . Again , this was done by three of the authors independently and in parallel . At this stage , we used the following ( additional ) eligibility criteria for inclu - sion and exclusion : we only included papers : ( 1 ) that contain at least a minimal , textual description or documentation of the employed gestures , for example the work by Cheng et al . [ 35 ] presented a gesture - based interface for human - drone interaction without pro - viding information about the gestures employed , and papers ( 2 ) that contain a mapping between action and gesture , i . e . , indicate which action is executed by a particular mid - air gesture , for exam - ple , Datcu et al . [ 43 ] presented four hand - based mid - air gestures for interaction with mobile augmented reality applications , but did not specify the respective actions . We furthermore excluded papers ( 3 ) that gestures are not designed as commands , for instance , Xiao et al . ’s work [ 202 ] investigating natural , non - verbal body language 9 Rayyan , https : / / www . rayyan . ai , accessed 23 / 01 / 2023 ( e . g . , refecting emotional states ) in the context of interactions be - tween humans and virtual humans , and papers ( 4 ) that were re - used from previous work . Regarding the last case , there were papers by the same authors that , based on their previous study , presented a similar system in the same domain in their follow - up paper . In this case , we retained the primary work , where the gesture set was frst presented , and we excluded the latter one . In addition , we re - evaluated all full texts based on our screening criteria ( see above ) , which excluded works of the wrong publication types or works making use of a physical medium ( e . g . , wands ) . After careful review , this resulted in a corpus of 172 papers . Again , we determined Fleiss’ kappa , which indicated substantial agreement between the three reviewers :  = . 662 ( 95 % CI , . 602 to . 721 ) , p < . 001 . Any discrepancies were discussed and resolved . Inclusion . Our fnal corpus consists of N = 172 papers containing a total of 1728 gestures and corresponding commands ( see Section 3 . 2 , Gesture Coding , for details ) . The papers span a time period from 1996 ( Sharma et al . ’s 3D display for molecular biologists [ 167 ] ) to 2020 ( 15 papers in total ) . Table 2 shows the list of mid - air gesture studies among diferent journals and conferences . A large majority of the papers ( 92 % ) have been published in or later than 2010 , which coincides with the frst releases of afordable ( gesture and motion - sensing ) depth cameras such as the Microsoft Kinect [ 27 ] . The publications are split over 100 venues . Conference full papers ( 151 , 87 % ) , with ACM CHI ( 10 papers ) , IEEE 3DUI , IEEE VR , ACM AVI , and ACM DIS ( 5 each ) are the largest contributors to the corpus . Journal articles ( e . g . , IEEE Access , IMWUT ) account for 12 % of the corpus ( 21 papers ) . The data extraction was performed by three researchers independently and in parallel ( each paper by one reviewer ) . 3 . 2 Gesture Types and Classifcations Characterizing gestured based on diferent dimensions of a tax - onomy allows to better distinguishing between diferent gestures as well as for describing major attributes of each gesture . After reviewing the literature and collecting gestures , we decided that by incorporating some of the dimensions of Wobbrock’s [ 200 ] and Vafaei’s [ 185 ] taxonomy and by adapting a few of the dimensions , we could form the basis of a taxonomy for our work . From Wob - brock’s taxonomy , we selected Binding ( original format ) and Flow ( original format ) . From Vafaei’s taxonomy , we chose Temporal ( original format , similar to Wobbrock’s Flow ) , Nature ( original format , modifed of Wobbrock taxonomy ) , Form ( modifed , as a mix of Wobbrock and Vafaei ) and Body Part ( modifed ) . There are multiple subcategories within each dimension , which we explain in the following section . Nature of the gesture denotes the relationship between gesture and meaning / object and divides into fve categories : manipulative , pantomimic , symbolic , abstract , and pointing . A gesture is manip - ulative when there is a tight and direct relationship between the movements of the gesture and its impact on the object / entity , such as move away two hands to scale a virtual object . Pantomimic ges - tures mimic a meaningful action or object , such as hand in grabbing posture ( grabbing knob ) and then rotating it to increase or decrease volume . Symbolic gestures visually depict symbols , such as thumbs - up gesture , OK sign . Pointing gestures are considered as pointing Towards a Consensus Gesture Set CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Table 2 : Publication venues of selected papers . CHI ( 10 papers per venue ) 10 5 . 81 % 3DUI , VR , AVI , DIS ( 5 papers per venue ) 4 * 5 = 20 11 . 63 % ISS , ICMI , UIST ( 4 papers per venue ) 3 * 4 = 12 6 . 98 % PerDis , OzCHI , TVX , UbiComp , VRCAI , SMC , SUI ( 3 papers per venue ) 7 * 3 = 21 12 . 21 % AutomotiveUI , CHItaly , ICIMCS , NordiCHI , APCHI , AH , MUM , IUI , ITS , ROMAN , ICPR , MobileHCI ( 2 papers per venue ) 12 * 2 = 24 13 . 95 % HCII , DAPI , INTERACT , IEOM , IHCI , ISCID , ICCIT , CW , AICT , ICARCV , ICCSN , URAI , VL / HCC , ICHMS , CHIuXiD , MM , AINAW , 64 * 1 = 64 37 . 21 % CISIS , ICMCS , PETRA , EurolTV , CompSysTech , CSNT , 3DTV , DigitalHeritage , DASC , Mobility , Expresive , IVCNZ , SACI , ICIM , ACE , JCSSE , Cyberworlds , ISCC , ISCMI , ROBIO , ICSIPA , IHC , SVR , ICCSNT , RCAR , IECON , Chinese CHI , TEI , ISMARW , CGS , BioVis , Muc , IndiaHCI , MIG , HAI , Mindtrek , BMEiCON , CENIM , EMBC , W4A , DIVAnet , ISETC , SISY , CVPRW , CCNC , VAST , FG ( 1 paper per venue ) Journals : N = 21 12 . 21 % IEEE Access ( 3 papers per venue ) 3 1 . 74 % TVCG , EICS , IMWUT ( 2 papers per venue ) 3 * 2 = 6 3 . 49 % Interacting with Computers , Informatics , Systems Science & Control Engineering , Procedia Technology , Procedia Manufacturing , 12 * 1 = 12 6 . 98 % Procedia Cirp , Energy Procedia , VRIH , Procedia - Social and Behavioral sciences , Biomedical Informatics , TACCESS , TOMM ( 1 paper per venue ) Conferences : N = 151 87 . 79 % gestures that indicate real , implied or imaginary persons , objects , directions . For example , it can be used for identifying appliances in ubiquitous computing [ 191 ] . Abstract gestures map gestures to the interactive task arbitrarily . Indeed there is no manipulative , sym - bolic , or pantomimic connection between gesture and task , such as make a fst to turn on TV . The Binding dimension is considered as the location where the gesture is performed . In object - centric gestures , the location is defned with respect to object features . This category mostly covers transformation tasks , such as moving or rotating the objects . In world - dependent gestures , the location is defned with respect to the UI space ( app . environment ) features . Opening a horizontal and vertical menu with swiping hand from right to left and from up to down respectively is classifed as world - dependent . Indeed , the user needs to know the screen coordinates to perform this gesture . World - independent gestures can occur anywhere , such as snap fngers to turn on TV . Gestures that occur in multiple spaces ( both object - centric and world - dependent ) are categorized as mixed dependencies , such as grabbing an object and moving it to a specifc space . Form categorizes gestures based on if and how the position and movement of the body part vary within a gesture . This dimension is defned as uni - manual , which means , in the case of two - handed gestures , that it applies separately to each hand [ 200 ] . Form includes static pose gestures , static pose and path gestures , dynamic pose , dynamic pose and path gestures , stroke gestures and multiple . Static pose gestures are performed in only one location , where the pose is held over time , such as point with index fnger . Static pose and path gestures hold the pose of the gesture while moving the body part , such as swipe hand . Dynamic pose gestures change the position of body part during the gesture while gestures perform in only one location such as , closing fst . Dynamic pose and path gestures consist of changing both position and location , such as closing both hands into fsts and then moving them apart . Stroke gestures consist of tap / ficks , such as press an imaginary button in mid - air . Multiple gestures include bi - manual gestures in which each hand performs a diferent form , for example , clench the right hand into a fst and move the index fnger of the left hand . The Flow dimension indicates whether the command occurs during a movement or at the end of a movement . In this dimension , a gesture is discrete if the task occurs after the gesture ends , e . g . draw a circle to turn on TV . Continuous gestures are gestures that complete the task during the gesture . For example , gestures such as move hands up or down to make commands such as increase or decrease volume are considered to be continuous gestures . The Body Parts dimension refers to which body parts are in - volved in a gesture . It distinguishes between hand , wrist , forearm , arm , shoulder , foot , leg , torso , and head . In this paper , we use the following description for body parts . Hand refers to changing the position of fngers and palm , for example , making a fst . Wrist refers to functions of the wrist joint , including bending , straightening , moving laterally , and rotating . Forearm refers to the movement of the forearm relative to the elbow ; for example , the forearm moves upward and close to the arm , resulting in a decreased angle be - tween them , or it moves downward , increasing the angle relative to the arm . Arm refers to the movement or rotation of the arm relative to the shoulder ; for example , moving the arm toward or away from the body . Shoulder refers to shoulder movement relative to the neck ; for example , elevating shoulders to convey I don’t know something . Foot refers to movements upward or downward and rotation of the foot at the ankle . Leg refers to movement or rotation of the leg relative to the hip or knee , including moving the leg toward or away from the median plane . Torso refers to the ver - tebral column movements relative to the sacrum and hip bone , for example , bending forward . Head refers to turning the head relative to the trunk , including turning the head laterally or turning it back to look straight ahead or moving the head and neck forward and downward , like looking down . In our classifcation , we distinguish between gestures that in - clude one body part and those that include variants of other body ∑ |  | |   | 1   (  ) = ( ) 2 − ( 1 ) |  | − 1 |  | |  | − 1   ⊆  CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Hosseini , et al . Figure 1 : Classifcation according to diferent subcategories of Body Parts : ( a ) The forearm moves to the right ( forearm gesture ) , ( b ) wave wrist from inside to outside ( wrist gesture ) , ( c ) Spread the fngers outwards ( hand gesture ) , ( d ) Spread the fngers outward and move the forearm to the right ( hand + forearm gesture ) . parts . For example , pinch with thumb and index fnger ( hand ges - ture ) vs . move the hand back and forth while pinching the thumb and index fnger ( hand plus forearm ) . Figure 1 illustrates some of the subcategories of Body Parts : ( a ) forearm movement , ( b ) wrist gesture , ( c ) hand gesture , and ( d ) hand plus forearm . 3 . 3 Agreement Between Gestures About 65 % of the gestures in this paper were designed by end - user elicitation studies . In these studies , the functions are shown to users , who are then asked to produce gestures for each action that would execute the function [ 200 ] . A key element in end - user gesture elici - tation studies is the concept of “agreement” , meaning that gestures defned by participants are the same or essentially similar [ 6 ] . The level of agreement between participants was the most frequent met - ric used in gesture elicitation studies . The agreement rate measures the level of consensus between participants for each task . It was introduced by Wobbrock [ 199 ] and later improved by Vatavu and Wobbrock [ 188 ] . To determine the consensus gesture set , this paper makes use of literature as a basis for calculating agreement rates , not end - user elicitation . After providing the gestures for each referent , we calcu - lated the agreement rate for each referent based on the defnition proposed by Vatavu and Wobbrock [ 188 ] : In Equation ( 1 ) , P is the set of all gestures proposed for referent r . Pi are subsets of gestures that are unique from P . The range of agree - ment rate varies between 0 ( every suggested gesture is diferent for referent r ) and 1 ( all suggested gestures are similar for referent r ) . According to Vatavu and Wobbrock [ 188 ] , the agreement rate level is to be interpreted as low agreement ( AR ( r ) < 0 . 100 ) , medium agree - ment ( 0 . 100 < AR ( r ) < 0 . 300 ) , high agreement ( 0 . 300 < AR ( r ) < 0 . 500 ) , and very high agreement ( AR ( r ) > 0 . 500 ) . 4 APPLICATION DOMAINS Our systematic literature analysis revealed a diverse range of appli - cation domains of mid - air gestures . We classifed each paper based on the subject or motivating use case into one of 12 high - level appli - cation domains . Some terminologies used for application domains are motivated by ACM index terms , which we list in Table 3 . Here , the classifcation of application domains was done iteratively by two researchers individually and in group discussions . The largest cluster of applications focuses on selection and ma - nipulation of 2D / 3D objects and navigation within diferent rep - resentation types ( 30 papers ) . A considerable number of papers in this category were systems that primarily focus on mid - air gestures in a 3D space and immersive VR scenarios , such as interaction with a virtual environment displayed on a mobile device [ 116 ] , above stereoscopic interactive tables [ 162 ] , or virtual assembly sys - tems [ 111 ] . Some articles examined interaction with a 2D display or projection [ 36 , 173 ] ; some examined enabling users to navigate into 3D virtual buildings , museums or pseudo - universes [ 75 , 180 ] ; and some focused on interaction with augmented reality [ 143 ] . Typical tasks ( but not limited ) within this domain include select [ 87 ] , ro - tate [ 143 ] , scale [ 36 ] , and translate [ 145 ] . One trait of this category was that some systems here were multi - modal [ 197 ] . Other application domains that frequently make use of mid - air gestures are large ( public ) displays ( 16 papers ) . These include tabletop displays or interactive walls specifcally designed to be placed in public ( or semi - public ) locations , including museums [ 70 ] , exhibitions [ 33 ] , and various city settings [ 1 , 172 ] . These domains require designing intuitive gestures for heterogeneous users with diverse technical backgrounds . We identifed the use of gestures to control entertainment and multimedia systems , including TV sets [ 186 , 189 , 210 ] and music playback systems [ 79 , 109 ] , as prevalent application domains ( 19 papers ) . In these works , the entertainment system is the central element at which all interactions are directed – as found in ‘classi - cal’ living room arrangements . Thus , this application domain also included studies performed in virtual home entertainment scenar - ios [ 109 ] . Here , we also grouped related activities performed in home entertainment systems , such as photo - browsing tasks per - formed on a TV while sitting on a couch or in an arm chair [ 65 ] , as well as drawing applications [ 13 ] . In contrast , smart rooms and appliances ( 10 papers ) include a setting in which mid - air gestures are used to control spatially distributed , multi - device environments that do not follow a traditional layout with one appliance at the center . These include smart home appliances [ 78 , 191 ] and house - hold or building devices [ 8 ] , including lighting control [ 129 ] . Again , Towards a Consensus Gesture Set CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany we included both real and virtual environments with otherwise comparable characteristics [ 181 , 206 ] . Applications in the domain human – robot interaction ( 13 papers ) employ gestures to control robotic devices over a distance , such as drones [ 28 , 85 , 136 , 164 ] , movable displays [ 178 ] , industrial robots or robot arms [ 11 , 58 , 183 , 203 , 211 ] , and tele - operation systems [ 58 , 95 ] . We distinguished mobile interaction ( 11 papers ) as a separate domain . These include works that focus on mobile interactions , that look into gestures for controlling personal mobile devices , e . g . , around - device gestures for smart phones [ 100 , 175 ] , or wearable gadgets , e . g . , smart watches [ 10 , 91 ] . Gestures designed to inter - act with another person’s device ( here : smart glasses [ 97 ] ) were included in this domain . In contrast , we grouped under other do - mains the use of wearables ( e . g . , smart watches [ 90 ] ) as proxies to interact with stationary devices or screens ( e . g . , public displays ) . A specifc application domain that traditionally is set in a desk - top environment and involves manipulation and navigation tasks is 3D modeling ( 7 papers ) . We decided to keep this application domain separate , as 3D modeling activities are often performed by expert users . Works contained in our corpus include gestural commands for three - dimensional shape modeling , ranging from CAD applications to industrial and graphics design . In this appli - cation domain we see a focus on tasks such as manipulating faces , edges , vertices , and polygons by means of translation , rotation and scaling , as well as other formations , e . g . , [ 42 ] . This domain focuses on efective , high - fdelity , precise manipulations [ 115 ] . Following the same rationale , we included gaming ( 5 papers ) as a separate ap - plication domain . Gamers often develop great expertise in playing . Works grouped here include the use of gestures to control video games [ 26 ] or games set in Virtual or Augmented Reality [ 7 ] . They typically seek a set of Natural Engagement hand poses during game play [ 44 ] , and gesture sets can help to achieve better immersion in the VR - based games [ 7 ] . E - Learning and education ( 8 papers ) , medical technology ( 8 pa - pers ) , accessibility & assistive technologies ( 5 papers ) and in - vehicle & automotive ( 11 papers ) are other popular application domains where gesture sets are designed for specifc ( expert ) user groups . Education applications have primarily focused on gestural interac - tion with learning environments , including interactive exploration of electronic learning resources and educational media [ 110 ] . This domain includes : educational computer games to develop frame - works reducing the difculty for educational computer game devel - opers [ 209 ] ; smart teaching interfaces [ 60 ] that design inputs for manipulating lecture materials on a projected display ; and inter - active applications for learning and understanding 3D geometry and exploring molecular structures [ 25 , 133 , 140 , 158 , 167 ] . In these applications , the focus most of the time is on pupils and teachers manipulating educational content [ 25 , 60 ] . In the medical domain , mid - air gestures have been explored for their promise of supporting touch - less interaction , which is ben - efcial in clinical environments . Our corpus contains gesture sets developed for various medical devices [ 162 ] and imaging applica - tions [ 105 ] to be used by experts , i . e . , doctors and nurses . Similarly , gesture - based interaction techniques have been explored to enable various assistive , adaptive and rehabilitative applications . Exam - ples include information browsing designed for vision - impaired users [ 73 ] and gesture sets designed to meet the requirements of people experiencing chronic pain [ 147 ] . In these cases , users of assistive devices are typically experts in using their devices . Gesture sets designed for in - vehicle or automotive user interfaces target primary driving tasks [ 47 ] , i . e . , moment - to - moment control of the vehicle , and secondary non - driving tasks , such as control of in - vehicle infotainment systems ( IVIS ) [ 207 ] . They are designed to be performed by the driver [ 122 , 207 ] . Gesture sets designed for this domain highlight the need for ( within - domain ) consistency : a gesture that controls music in one car model would ideally not start up the hazard lights when performed in another car . Other papers ( 29 papers ) include those not associated with a specifc application domain or those covering general - purpose sce - narios associated with multiple application domains . 5 RESULTS 5 . 1 Participants Overview We found that most papers reviewed ( 66 papers ) had between 10 and 20 participants in their user studies . Thirty - two papers included fewer than 10 participants . Twenty - one papers included between 20 and 30 participants . Papers that had signifcantly larger numbers of participants had 127 , 79 , and 74 participants . Among the participants , the number of males was about 1 . 5 times higher than the number of females . About 33 % of the partic - ipants were students or a combination of students and academic staf . Around 40 % of participants were adults with diferent back - grounds , including surgeons , engineers , designers , etc . Participants in 4 articles were related to people with special needs , including blind people and individuals with developmental disorders , and 30 papers did not report sufcient information about their participants . We found that 28 . 5 % of papers ( 49 out of 172 ) did not conduct a user study or did not report the number of participants included in the study . There were 10 papers that conducted 2 user studies . 5 . 2 Agreement per Referent We extracted a total number of 1728 gestures and corresponding commands from the papers in our corpus , omitting gestures that received low or no consensus in gesture elicitation studies . Various terms are used in the literature for interaction ( including task , command , afect , etc ) . To be consistent with those terms in our paper , we grouped those with similar concepts into one category and called them " referent " in our paper . For example , for " increase volume " , " brightness up " , and " heater value up " , " increase " was assigned as a referent . Two researchers analyzed all the commands collected from the literature , then agreed on a referent for each command . The referents were derived entirely bottom up from the data . Afterward , one researcher grouped the referents into 6 categories based on their concept , including Transform , Navigate , Confrm , Se - lect , Adjust Settings , and Action . We took the Select and Transform categories from [ 145 ] and extended them with Navigate , Confrm , Adjust Settings , and Action . The grouping was done so that ref - erents in the same level were not compared and overlapping . For example , if a gesture such as tapping results in acceptance , it will be classifed as accept / confrm and not select , or if we decompose select , it will not be compatible with any referent on navigate , such CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Hosseini , et al . Table 3 : Application domains of mid - air gestures . Manipulation / Navigation ( 30 ) Large ( Public ) Displays ( 16 ) - 2D Display : [ 36 , 57 , 173 ] - Projected Display : [ 41 , 94 , 132 ] - VR / 3D : [ 12 , 20 , 55 , 61 , 67 , 69 , 87 , 92 , 103 , 104 , 111 , 116 , 123 , - Application in Public Setting : [ 2 , 33 , 70 , 172 ] 124 , 137 , 138 , 176 ] - Curved Displays : [ 19 ] - AR : [ 29 , 143 , 145 , 160 , 197 ] - Others : [ 1 , 3 , 14 , 32 , 90 , 114 , 144 , 155 ] - Building / Museum : [ 37 , 75 , 107 , 141 , 180 ] Media & Entertainment ( 19 ) Smart Rooms & Appliances ( 10 ) - TV : [ 45 , 51 , 83 , 146 , 186 , 189 , 204 , 210 ] - Controlling Mlti - Device in Smart Kitchen and Home : - Audio / Video Interaction : [ 50 , 79 , 82 , 109 , 120 , 130 , 156 , 194 ] [ 8 , 18 , 71 , 78 , 191 ] - Photo - Browsing Tasks : [ 65 , 99 ] - Specifc Class of Devices or Single - Task Scenarios : - Drawing Applications : [ 13 ] [ 54 , 68 , 129 ] - Navigation in the Virtual House : [ 206 ] - Interact with IoT Devices : [ 181 ] Human - Robot Interaction ( 13 ) Mobile Interaction ( 11 ) - Human - Drone Interaction : [ 28 , 85 , 136 , 164 ] - Interaction with Industrial Robots or Robot Arms for Robot Navigation : [ 11 , 183 , 203 , 211 ] - Robotic Teleoperation : [ 58 , 95 ] - Controlling a Group of UAVs : [ 142 ] - Solving Pick - and - Place Tasks : [ 148 ] - Controlling TV Display Direction : [ 178 ] eLearning & Education ( 8 ) - Typical Tasks in Smartphones : [ 5 , 15 , 56 , 119 , 175 , 198 ] - Around Mobile Device Interaction : [ 100 , 175 ] - Smartwatches : [ 10 , 91 ] - Smart Glasses : [ 97 ] In - Vehicle and Automotive ( 11 ) - Educational Computer Game Frameworks : [ 209 ] - Primary Driving Tasks : [ 47 , 48 , 80 ] - Smart Teaching Interfaces : [ 60 ] - Secondary Non - Driving Related Tasks : - Interactive Applications for Learning and Understanding 3D [ 66 , 80 , 96 , 122 , 150 , 179 , 196 , 207 ] Geometry : [ 25 , 133 , 140 , 158 , 167 ] - Digital Storytelling Systems : [ 110 ] Medical Technology ( 8 ) Gaming ( 5 ) - 3D Medical Image exploration and Navigation : - Video Games : [ 26 , 64 ] [ 77 , 105 , 117 , 154 , 169 , 177 , 184 ] - VR Based Games : [ 7 , 44 , 159 ] - Operating Table Control Systems : [ 162 ] Accessibility & Assistive ( 5 ) 3D Modeling ( 7 ) - Elderly People : [ 106 ] - 3D Architectural Urban Planning : [ 115 , 170 ] - Individuals with Impairments or Disorders : [ 62 , 73 , 147 , 168 ] - Others : [ 42 , 49 , 131 , 135 , 195 ] Others or Not Specifed ( 29 ) - Mix : [ 16 , 21 , 31 , 34 , 52 , 102 , 163 ] - Data Analysis / Exploration : [ 30 , 118 , 193 ] - Shape Display Interaction : [ 108 ] - Understanding Specifc Body Part : [ 205 ] - Others : [ 9 , 24 , 38 , 46 , 59 , 74 , 84 , 86 , 93 , 98 , 113 , 125 , 149 , 151 – 153 , 166 , 208 ] as rewind . To avoid bias and divergent interpretations , researchers constantly consulted each other when coding and categorizing data . The set consists of more than 100 referents , Table 4 includes those that exist at least in 3 papers . Table 4 lists these referents and their agreement rate . The frst two columns identify each referent and its related command cate - gory . Column 3 presents the defnition of the referent that specifes what tasks are included . Column 4 gives a sample gesture for each referent . Column 5 lists the number of papers for each referent , and the last column shows the agreement rate for each referent . We calculated the agreement rate for each referent based on the defnition described in Section 3 . 3 . Gestures were then clustered to identify identical groups of ges - tures within each referent separately to calculate agreement rate ( AR ) . We carefully considered how prior works classifed gestures for calculating agreement . Consequently , to identify similar ges - tures , instead of simply coding the gestures found in the literature we used the concept of a “gesture theme” that has been used for grouping the proposed gestures based on the mental model in previous papers [ 143 ] . To the best of our knowledge , there is no comprehensive taxonomy to specify diferent mental models of Towards a Consensus Gesture Set CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany gestures . We clustered the gestures using a bottom - up , inductive analysis approach to identify their themes based on a mental model . As gestures sometimes vary , even if only in a very minor way , following a strict coding of these gestures is not useful . This is especially true in our case , where gestures belong to an extensive and varied range of application domains derived from a multitude of publications . A gesture theme , on the other hand , is a particular mental model or idea of one or multiple gestures , i . e . gestures placed within the same theme can be distinct in how they are executed but their men - tal model and main idea are similar . Mental models are sometimes afected by the referent , so each referent is grouped separately . For instance , one of the themes we identifed for zoom was Pull / Squish . Pull / Squish was performed in four diferent ways : move open hands to or away from each other , regroup or open all fngers , pinch in or out with fngers , or grab two hands together and then move them away from each other . One other theme for zoom we identifed was move close / away ( make a grabbing gesture with a hand and move it closer to or away from the eyes ; move the head away from or closer to the screen ) . As a result of our coding gestures to themes , gestures within a particular mental model difer mostly in terms of body part ( hand , leg , etc . ) , path direction ( straight vs . fexible ) , path fow ( multiple strokes vs . continuous ) , hand posture ( pointing one fnger , spreading or closing fngers , etc . ) , and movements ( fxed sign vs . drawing ) . To cluster the gestures , frst we conducted an initial coding round where two researchers carefully evaluated one - third of all gestures independently and chose the appropriate theme for each of them . We then held a code adjustment session where two coders discussed with each other and made a mutual decision when they had a confict . Afterward , one of the researchers coded the rest of the gestures to assign the themes . The process of obtaining agreement rates is described in Sec - tion 3 . 3 . As a reminder , according to Vatavu and Wobbrock [ 188 ] , the agreement rate level can be interpreted as low agreement ( AR ( r ) < 0 . 100 ) , medium agreement ( 0 . 100 < AR ( r ) < 0 . 300 ) , high agree - ment ( 0 . 300 < AR ( r ) < 0 . 500 ) , and very high agreement ( AR ( r ) > 0 . 500 ) . Table 4 shows that among diferent referents , 10 ( 17 . 86 % ) were with very high agreement rate , 12 ( 21 . 43 % ) were with high agree - ment rate , 21 ( 37 . 50 % ) were with medium agreement rate , and 13 ( 23 . 21 % ) were with low agreement rate . Dichotomous tasks in most cases , such as next / previous , increase / decrease , group / ungroup , and accept / reject , had almost the same level of consensus . All of the tasks in the transform category and adjust setting had a high or very high AR . In the confrm category there were no high AR at all . 5 . 3 Cross - Domain Consensus Gesture Set We constructed a consensus gesture set of gestures collected from the literature . To construct the gesture vocabulary , we selected referents with a high or very high AR ( containing 22 referents ) . For each of these referents , we selected gesture themes with a frequency of more than 25 % . Table 5 summarizes the 22 referents . We listed each referent with AR > 0 . 3 and selected the most frequent gesture themes . In those cases where a theme had multiple variations ( in terms of execution , body part , uni - manual or bi - manual , and direction ) , we noted them in the variation part of the table . For the various gestures introduced for a referent , we determined the percentage of agreement with the introduced theme . We also determined the nature of each gesture theme based on our taxonomy . A gesture theme can difer in other dimensions of taxonomy depending on its variation , so we have omitted adding other dimensions of taxonomy . For example , the main idea of " scaling " is Pull apart / Squeeze , which can be done in diferent ways . One way , for example , is to use one hand pinching fngers in or out ( dynamic pose ) . Another is to use open hands moving away from or close to each other ( static pose and path ) , or by frst grabbing both hands together and then moving them away from each other ( dynamic pose and path ) . Given that the goal is to achieve cross - domain consistency , users may not be able to perform bi - manual gestures in many applications or may not prefer gestures made with other parts of the body . Table 5 shows uni - manual gestures in most cases . The bi - manual gestures included in Table 5 are only for those cases without an alternative uni - manual variation in the reviewed literature . Since we intended to present one of the executions of gesture themes already found in the literature , we did not add an alternative uni - manual gesture based on our speculations for these bi - manual gestures in Table 5 . Some other gestures are visually diferent but the theme seems identical . For example , Next and Open Door are both slide / swipe , and Zoom and Scale are both pull apart / squeeze . The nature of most gestural themes in Table 5 is manipulative , i . e . gesture movements act on the objects themselves . Example themes are group , ungroup , translate , pan / scroll , scale , rotate , move cursor , and zoom . After that , Symbolic gestures were the most common . Examples here are skip , next , previous , create , minimize , maximize , increase , and decrease . Pantomimic gestures can be seen in themes for cut . Abstract gestures and Pointing gestures were not among these themes . To understand how this 22 - gesture set is mapped to the diferent referents across diferent domains , we identifed the 3 most fre - quent gestures for each referent and determined their distribution among diferent domains . Appendix A gives the 3 most frequent gestures for each referent . A comparison of gesture distribution across diferent application domains reveals just a few conficts between the gestures we proposed for a given referent ( Table 5 ) and the most frequent gestures identifed between diferent application domains . They include translate in Accessibility and Adaptive Sys - tems and In - Vehicle and Automotive , rotate in Human - Robot Inter - action , increase and decrease in Manipulation / Navigation , pan / scroll in Manipulation / Navigation and Medical Technology , deselect in 3D Modeling , and take picture in Mobile Interaction . 5 . 4 Are Agreement Rates Consistent Across Domains ? We calculated agreement rates in diferent application domains for all referents listed in Table 4 . Figure 2 shows a heat map plot of the results . Green ( above 0 . 3 ) indicates high agreement , red ( below 0 . 29 ) indicates low agreement . Domains with only one paper per referent ( i . e . , too few to calculate AR ) are marked as " * " ( asterisk ) . Domains with no papers for a referent are marked as " NA " . In all 22 referents with high ARs from our candidate set , 9 had high ARs in each application domain . These were move cursor , scale , CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Hosseini , et al . Table 4 : An overview of referents , their defnition , an example of their gestures , their AR , and the number of papers for each referent . Category Referent Defnition Example # AR Translate Moving or dragging an entity Push one hand forward [ 197 ] 59 0 . 45 Rotate Rotation of entities Rotate grab hand [ 36 ] 56 0 . 54 Transform Scale Move Cursor Decrease or increase the size of object Cursor movement on the screen Close up two hands [ 36 ] Index fnger move around [ 115 ] 27 12 0 . 780 . 6 Minimize Reduce the size of object to smallest amount Close hand [ 170 ] 4 0 . 4 Maximize Expand object to its full size Move thumb and index fnger from closed to open [ 204 ] 4 0 . 3 Pan / Scroll Navigate within a single view Palm move [ 2 ] 31 0 . 50 Zoom Change the scale level of an entity Two hands move apart [ 52 ] 39 0 . 49 Next Go to next item , channel , web page Hand swipe right [ 31 ] 43 0 . 52 Previous Go back to previous item , channel , web page Hand swipe left [ 31 ] 41 0 . 49 Navigate Fast Forward Rewind Move forwards through the tape / video faster that Wind a tape / video back to the beginning usual Point right with thumb [ 45 ] Point left with thumb [ 45 ] 7 8 0 . 14 0 . 15 Main Menu Returning to the main menu Swipe from right to left [ 33 ] 4 0 . 19 Home Screen Going to home screen Making a fst and open it [ 56 ] 6 0 . 05 View control Change the camera position Pinch and move around [ 176 ] 10 0 . 22 Locomotion Moving from place A to B ( robot , avatar , VR ) Put left leg in front of the right leg [ 7 ] 18 0 . 27 Confrm Accept / Confrm Reject Agreeing in binary Answering “no” in queries binary queries Thumb up [ 145 ] X symbol [ 51 ] 20 11 0 . 14 0 . 15 Select Single Select Multiple Select Select a single entity or Select group of entities location Closing hand into a fst [ 26 ] Tap one after another [ 109 ] 77 4 0 . 22 0 . 21 Deselect Cancel the selection Opening hand [ 117 ] 16 0 . 37 Adjust Setting Increase Decrease Rise in Reduce amount of in amount volume , light , temperature , . . . of volume , light , temperature , . . . Move Move hand hand up [ 78 ] down [ 78 ] 42 41 0 . 33 0 . 42 Turn on Turn on any device Snap fngers [ 78 ] 18 0 . 07 Turn Of Turn of any device Rub hands to turn of heater [ 71 ] 16 0 . 17 Play Playing multimedia content Hand tap [ 207 ] 23 0 . 12 Stop Stopping temporary or permanent an action Halt gesture ( show palm ) [ 191 ] 39 0 . 12 Phone Call Answer an incoming call Phone pose [ 93 ] 12 0 . 14 Delete Remove or erase entities Throwing away [ 152 ] 11 0 . 17 Undo Reverse the previous action Wave hand [ 170 ] 9 0 . 14 Ungroup Separate objects from each other Two hands move apart [ 135 ] 3 1 Group Move objects together Two hands move together [ 135 ] 3 1 Mute Not giving out sound or speech Shsss gesture [ 191 ] 10 0 . 09 Unmute Make a sound or speech again Opening palm [ 45 ] 4 0 . 17 Click ( Cursor ) Mouse simulation Rotate wrist [ 90 ] 10 0 . 13 Create Create object or interaction space Draw outline [ 195 ] 7 1 Skip Skipping directly to a desired entity Swipe / Slide [ 16 ] 3 1 Activate Ready to work on object / device Show number 1 to active interface [ 54 ] 10 0 . 05 Action Copy Paste / Duplicate & Exact copy of another thing Hold the original object and pull a second hand [ 143 ] out 11 0 . 04 Close Menu Close diferent type of menu Swipe [ 66 ] 8 0 . 42 Close Object Close document , tools , . . . X symbol [ 204 ] 5 0 . 2 Open Menu Open diferent type of menu Swipe [ 109 ] 16 0 . 2 Open Object Open document , tools , . . . Grab and open [ 46 ] 5 0 . 06 Open - Swipe / Slide [ 8 ] 5 0 . 47 Door / Window Cut A Remove the selected data from its original position Snap index and middle fnger [ 145 ] 6 1 Cut B Slashing an object Knife gesture [ 159 ] 5 0 . 42 Start Begin navigating Point forward [ 47 ] 4 0 . 1 Take Picture Take screenshot , selfe , image of object Forming a square with both hands [ 164 ] 6 0 . 33 End Call Reject , Hang - up or end current Call Flick to the left [ 71 ] 7 0 . 09 Switch Switch between multiple functions or state Select one of the fngers [ 21 ] 8 0 . 04 Toggle Switch between binary functions or state Downward bounce [ 31 ] 4 0 Info - More Get more information on an entity or location Hold ( briefy ) one arm up [ 1 ] 4 0 . 17 Help Ask ( invoke ) system for help Draw question mark [ 186 ] 3 0 Exit Exit from system or application X symbol [ 114 ] 3 0 Trigger Invoke or summon an object Tap index and middle to summon the spinbox [ 74 ] 7 0 Deactivate Unable to respond Ok gesture to diactive virtual guide [ 180 ] 5 0 Towards a Consensus Gesture Set CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany rotate ( from transform category ) , zoom ( from navigate category ) , create , close menu , cut , take a picture ( from action category ) , and decrease ( from adjust setting ) . Several referents had a high value in cross - domain as well as in most domains and had a low AR in only one or two domains . For example , next and previous had low AR in accessibility and adaptive systems and in e - learning and education domains ( 2 papers , AR = 0 for both ) . Increase had medium AR in manipulation / navigation ( 3 papers , AR = 0 . 17 ) . Pan had only medium AR in in - vehicle and automotive ( AR = 0 . 24 ) , and deselect had low AR in media and entertainment ( 3 papers , AR = 0 . 048 ) . Some referents , for example Single Select and stop / pause , had a high degree of variability across domains . It seems that for stop , AR is lower when we want to stop a device such as a drone [ 85 ] or a robot [ 58 ] , compared to when we want to stop digital content [ 120 ] such as media or a cursor . Among the referents with a lower AR across all domains , most had a low AR even in separate application domains . However , there were some exceptions . Across all domains open menu had AR = 0 . 2 . But in manipulation / navigation ( 2 papers ) and in - vehicle and auto - motive ( 3 papers ) , it was very high ( respective AR = 1 and AR = 0 . 5 ) . Across all domains accept / confrmation had medium AR ( AR = 0 . 14 ) , but in media and entertainment ( 4 papers ) and mobile interaction ( 2 papers ) it was high ( respective AR = 0 . 3 and AR = 0 . 33 ) . We do not see that in its dichotomous task ( i . e . rejection ) ; rejection still has low AR in media and entertainment ( 4 papers ) . Across all domains close object had AR = 0 . 2 but in media and entertainment ( 2 papers ) it was high ( AR = 1 ) . Across all domains turn on and turn of had low AR ( respective AR = 0 . 06 and AR = 0 . 17 ) but they had high AR in media and entertainment ( 4 papers turn on , 3 papers turn of ) and in - vehicle ( 2 papers turn on , 2 papers turn of ) . Across all domains view control had medium AR but it was high in manipulation / navigation ( 2 papers ) ; and main menu is high in large display interaction ( 2 papers ) . Among all the domains , 3D modeling had the most number of referents with a high rate and the mobile interaction had the least . Here are the percentages of referents with high AR in each domain : 3D modeling : 87 . 5 % ; robotic : 85 . 7 % ; gaming : 75 % ; large display : 66 . 67 % ; manipulation / navigation : 61 . 1 % ; medical technology : 60 % ; in - vehicle and automotive : 58 . 8 % ; media and entertainment : 57 . 7 % ; e - learning and education : 57 . 1 % ; smart room : 54 . 55 % ; accessibility and adaptive systems : 50 % ; mobile interaction : 50 % . 5 . 5 Does the Task Afect Agreement Rates ? As can be seen in Table 6 , in some referents we used standardized “meta - meanings” to summarize similar tasks , for example for tasks “increase volume” , “increase lightness” , and “increase speed” we used “increase” as a general referent . To understand how the pro - posed gestures for a referent difer in diferent settings ( tasks ) , we calculated AR for diferent tasks of a referent ( i . e . same referent in diferent settings ) . As can be seen in Table 6 , the tasks in most of the cases do not have an efect on the agreement rate except in next page / slide and previous page / slide , in which AR was medium and low Respectively . 5 . 6 Gesture Types per Domain We manually classifed each gesture based on dimensions of the taxonomy described in Section 3 . 2 . Our goal was to identify the characteristics of mid - air gestures in diferent application domains . For coding the gestures based on taxonomy , three researchers in - dividually coded gesture sets , 70 % of them by the frst researcher , 15 % by the second one , and 15 % by the third one ( all are authors of this paper ) . To avoid bias and divergent interpretations , researchers constantly consulted each other when coding and categorizing data . Figure 3 shows the percentage of gestures in each application domain in the 5 dimensions of the taxonomy . Each gesture was classifed based on Flow , Nature , Form , Bind - ing , and Body Part dimensions . In the Flow dimension , discrete gestures dominate at least slightly in most application domains but , with only a few exceptions , are relatively balanced overall . Only in manipulation / navigation and eLearning and education do continu - ous gestures account for over 60 % of all gestures . Typical referents of these domains are predominantly continuous ( such as scale or rotate ) . It seems that the domains with more discrete gestures have less AR . For example , accessibility , mobile interaction , and smart rooms compared to other application domains have more discrete gestures , and according to the heat map in section 5 . 4 they have lower AR compared to other application domains . In the Nature dimension , as seen in Figure 3 , the gesture na - ture difers depending on the application domain . In this category , manipulative gestures account for at least 28 % in 6 domains . This can be explained by the inherent continuous nature of manipula - tive gestures , which is particularly suitable for performing various geometric and coordinate transformations , especially , in 3D mod - eling , manipulation / navigation , or medical technology domains . Pointing and abstract gestures are also rather rare in all domains . Some domains use a signifcant number of Symbolic gestures , espe - cially in smart rooms and media and entertainment , which include about 50 % of gestures . It seems that in domains with more Sym - bolic gestures and fewer Manipulative gestures , AR is less than in other domains , such as smart rooms , media and entertainment , and accessibility and adaptive systems . Some domains use a sig - nifcant number of pantomimic gestures , especially in accessibility and adaptive systems and gaming , where they account for about a third or more of all gestures . In the Form dimension , in most application domains except mobile interaction , gaming and accessibility , and adaptive systems , the most common gestures are static pose and path ( at least 40 % of all gestures ) . Gaming had more than 50 % static poses , mobile interaction had almost 40 % dynamic poses , and accessibility and assistive systems had almost 40 % dynamic poses . In the Binding dimension , world - independent gestures were the most common in all application domains , accounting for at least 50 % of all gestures . World - dependent gestures were observed in all application domains except for accessibility and assistive systems . Objective - centric gestures were observed in all applica - tion domains , especially 3D modeling and manipulation / navigation , which include about 30 % of the gestures . Mixed dependencies ges - tures were observed only in manipulation / navigation , media and entertainment , and mobile interaction , with an occurrence less than 5 % . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Hosseini , et al . Table 5 : Consensus gesture set compiled from the agreement rate analysis sorted in descending order of papers for each referent . Referent Translate ( AR : 0 . 45 ) Rotate ( AR : 0 . 54 ) Next ( AR : 0 . 52 ) Increase ( AR : 0 . 33 ) Theme ( s ) Variant ( s ) Occurrence Nature Swipe / Slide . With fnger , hand , arm , leg , uni - manual , bimanual 63 . 7 % of 135 gestures in 59 papers Manipulative Turning Movement . With fngers , hands , unimanual , bimanual 71 . 9 % of 121 gestures in 56 papers Manipulative Slide / Swipe . With fnger , hand , leg , diferent direction 71 . 83 % of 71 gestures in 43 papers Symbolic Swipe / Slide . With fnger , hand , arm 53 . 42 % of 73 gestures in 42 papers Symbolic Referent Decrease ( AR : 0 . 42 ) Previous ( AR : 0 . 49 ) Zoom ( AR : 0 . 49 ) Pan / Scroll ( AR : 0 . 50 ) Theme ( s ) Variant ( s ) Occurrence Nature Swipe / Slide . With fnger , hand , arm 62 . 85 % of 70 gestures in 41 papers Symbolic Slide / Swip . With fnger , open hand , leg , dif - ferent direction 69 . 86 % of 73 gestures in 41 papers Symbolic Pull Apart / Squeeze . With fngers , unimanual , biman - ual , arm 67 . 82 % of 87 gestures in 39 papers Manipulative Swipe / Slide . With fnger , palm , arm 69 . 35 % of 62 gestures in 31 papers Manipulative Referent Scale ( AR : 0 . 78 ) Deselect ( AR : 0 . 37 ) Move Cursor ( AR : 0 . 6 ) Close Menu ( AR : 0 . 42 ) Theme ( s ) Variant ( s ) Occurrence Nature Pull Apart / Squeeze . With fngers , hands , unimanual , bimanual 88 . 31 % of 77 gestures in 27 papers Manipulative Open Hand . - 61 . 9 % of 21 gestures in 16 papers Pantomimic Move or Slide Hand Around . With fnger , hand 77 . 78 % of 18 gestures in 12 papers Manipulative Swipe / Slide . With fnger , hand , arm , uniman - ual , bimanual 66 . 67 % of 12 gestures in 8 papers Symbolic Referent Create ( AR : 1 ) Cut A ( AR : 1 ) Open Door ( AR : 0 . 47 ) Cut B ( AR : 0 . 42 ) Theme ( s ) Variant ( s ) Occurrence Nature Draw / Defning Area . With fnger , hand , arm , head , leg , unimanual , bimanual 100 % of 30 gestures in 7 papers symbolic Snap Index and Middle Finger . - 100 % of 6 gestures in 6 papers Pantomimic Swipe . - 66 . 67 % of 6 gestures in 5 papers Symbolic Slit / Knife . - 66 . 67 % of 9 gestures in 5 papers Pantomimic Referent Maximize ( AR : 0 . 3 ) Minimize ( AR : 0 . 4 ) Group ( AR : 1 ) Skip ( AR : 1 ) Theme ( s ) Variant ( s ) Occurrence Nature Move Apart Fingers . With thumb and index , all fn - gers 60 % of 5 gestures in 4 papers Symbolic Close Hand . - 60 % of 5 gestures in 4 papers Symbolic Move Hands Together . - 100 % of 3 gestures in 3 papers Manipulative Swipe / Slide . With Finger , Arm 100 % of 3 gestures in 3 papers Symbolic Towards a Consensus Gesture Set CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Table 5 : Continued from previous page . Variant ( s ) Theme ( s ) Nature Referent Occurrence Ungroup ( AR : 1 ) Manipulative Move Hands Apart . - 100 % of 3 gestures in 3 papers Make Frame with Both Hands . Symbolic - 57 . 14 % of 7 gestures in 3 papers . Take Picture ( AR : 0 . 33 ) Figure 2 : Agreement rate of collected referents in different application domains . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany In the Body Part dimension , hand gestures have a higher per - centage compared with other gestures in most application do - mains , except for 3D modeling , large display interaction , manipula - tion / navigation , and robotics ( less than 30 % ) . In those exceptions , arm or arm + hand gestures cover the highest percentage of gestures . The wrist gestures were most repeated ( around 10 % ) in accessibility and adaptive systems , in - vehicle and automotive , and mobile inter - action . Forearm gestures are observed in all domains . Foot , torso , head , hand + wrist , and full body gestures were most rare , observed in less than 1 % and in just a few application domains . Table 6 : Comparison of agreement rate of referents with diferent tasks for each referent . Referent / Task AR Theme Nr . papers Increase 0 . 34 Swipe / Slide 42 1 - Volume 0 . 33 Swipe / Slide 32 2 - Speed 0 . 38 Swipe / Slide 9 2 - Brightness 0 . 48 Swipe / Slide 6 Decrease 0 . 42 Swipe / Slide 43 1 - Volume 0 . 37 Swipe / Slide 33 2 - Speed 0 . 27 Swipe / Slide 6 2 - Brightness 0 . 53 Swipe / Slide 7 Next 0 . 53 Swipe / Slide 43 1 - Menu / List Item 0 . 71 Swipe / Slide 26 2 - Channel / Audio / Video 0 . 4 Swipe / Slide 17 3 - Page / Slide 0 . 17 Swipe / Slide 4 Previous 0 . 65 Swipe / Slide 41 1 - Menu / List Item 0 . 67 Swipe / Slide 22 2 - Channel / Audio / Video 0 . 45 Swipe / Slide 17 3 - Page / Slide 0 - 3 Hosseini , et al . referents showed much more variety and lower agreement , which suggests more fexibility in designing gesture languages in those cases . Cross - domain consistency , therefore , may not be achieved for those referents and other interaction techniques may suit them better . Those referents include turn on , turn of , accept , reject , mute , unmute , and some other abstract referents , such as activate or deac - tivate . While agreement rates for some referents are either too low or too high , such as exit , or group , the occurence of those referents in various studies is scarce . Thus , although the consensus gesture sets in table 5 are not necessarily consistent in each application domain , we believe they can be used as a basis for transferable gesture sets . We defned this consensus gesture set by selecting the most frequent gesture themes from referents that have high or very high agreement rates . In addition , we identifed three of the most frequent gestures in each domain for each referent , giving us an overview of how a consensus set maps to the referent across diferent domains . In general , our analysis suggests that the selected consensus set remains more or less constant between domains , be - cause the selected gestures were the most frequent in most domains ( or at least the second - or third - most frequent ) . 6 DISCUSSION This literature review confrmed that mid - air gestural interaction is extensive , specifcally in the area of interaction with remote displays ( TVs , large Public Displays , Medical Image Viewer Monitors , etc . ) and Home Appliances , In - Vehicle information systems , and Robot Control . To draw deeper conclusions about the applicability and usefulness of this modality of interaction , more specifc studies should be conducted in each feld as the application range grows . Now that pervasive computing and using gestures as an interac - tion modality have penetrated so many diferent domains in daily life , users need to be able to transfer interaction knowledge from one application or domain to another . Designer - and user - driven gesture studies have been widely used to develop intuitive gestures for individual devices and applications . However , much less work has investigated their use across domain boundaries . Conducting studies for diferent devices and domains individu - ally does not necessarily ensure cross - domain consistency of the resulting gestures . We calculated the consistency of mid - air ges - tures for diferent types of referents , looking at consistency both across domains and individually within singular application do - mains . We have found that : ( 1 ) there is cross - domain consistency for some of the referents , especially for move cursor , scale , rotate , zoom , create , close menu , cut , take a picture , and decrease ; ( 2 ) with one or two exceptions , some referents are consistent in most domains , in - cluding the referents next , previous , increase , and pan ; and ( 3 ) other 6 . 1 General Requirement of Using Consensus Gesture Set The criteria used to determine the similarity of gestures can afect the agreement rate and consistency of the gestures . Our coding using gesture themes results in a considerably higher agreement rate than the gestures themselves because we count each gesture as unique , regardless of which body part is used ( fngers vs . hands vs . fsts vs . arms ) or if a specifc gesture is executed in diferent ways ( draw an X symbol vs . crossing two fngers to represent X ) . Users may vary their gestures within a specifc interaction gesture theme as a response to diferent device types with diferent form factors , such as distance from a device , space , and diversity in the nature of application domains . Design implications , therefore , must be considered for gestural interaction systems that do not allow simply reusing a specifc gesture . The transferability of gestures across diferent application domains needs to be considered . For example , in some application domains , such as 3D modeling , gaming , and medical technology , gestures with more freedom of movement are more intuitive and create a more immersive feeling in the user . Designers need to take into account the size of the space / object and the users’ proxemics . For example , to rotate virtual objects , users may switch from a single - handed gesture in a small space to a two - handed gesture in a larger space . In addition , in the consensus set , several gestures are identical but correspond to diferent referents . Consequently , only a selec - tion of gestures from the consensus gesture set can be used in an application . We did not replace these with the next most repetitive gesture for that referent , because we assumed that not all of the ref - erents in Table 5 are going to be used together in the same domain . In the situation we encountered , however , we encourage gesture set designers to choose the gesture from Table 5 for the referent with the highest rate of agreement and choose the second most repeated one for the other referents from Table 7 in appendix . Towards a Consensus Gesture Set CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany 0 % 25 % 50 % 75 % 100 % 3 D m od e li ng A cce ss i b ilit y a nd A d a p ti v e S y s t e m s e L ea r n i ng a nd E du ca ti on G a m i ng I n - V e h i c l e a nd A u t o m o ti v e L a r g e D i s p l a y I n t e r ac ti on M a n i pu l a ti on / n a v i g a ti on M e d i a a nd E n t e r t a i n m e n t M e d i ca l T ec hno l ogy M ob il e i n t e r ac ti on H u m a n – R obo t I n t e r ac ti on S m a r t R oo m s / A pp li a n ce s P e r ce n t a g e s o f g e s t u r e s Application domains Nature Abstract Pointing Symbolic Pantomimic Manipulaive 0 % 25 % 50 % 75 % 100 % 3 D m od e li ng A cce ss i b ilit y a nd A d a p ti v e S y s t e m s e L ea r n i ng a nd E du ca ti on G a m i ng I n - V e h i c l e a nd A u t o m o ti v e L a r g e D i s p l a y I n t e r ac ti on M a n i pu l a ti on / n a v i g a ti on M e d i a a nd E n t e r t a i n m e n t M e d i ca l T ec hno l ogy M ob il e i n t e r ac ti on H u m a n – R obo t I n t e r ac ti on S m a r t R oo m s / A pp li a n ce s P e r ce n t a g e s o f g e s t u r e s Application domains Form multiple Stroke Dynamic Pose and Path Dynamic Pose Static Pose Static Pose and Path 0 % 25 % 50 % 75 % 100 % 3 D m od e li ng A cce ss i b ilit y a nd A d a p ti v e S y s t e m s e L ea r n i ng a nd E du ca ti on G a m i ng I n - V e h i c l e a nd A u t o m o ti v e L a r g e D i s p l a y I n t e r ac ti on M a n i pu l a ti on / n a v i g a ti on M e d i a a nd E n t e r t a i n m e n t M e d i ca l T ec hno l ogy M ob il e i n t e r ac ti on H u m a n – R obo t I n t e r ac ti on S m a r t R oo m s / A pp li a n ce s P e r ce n t a g e s o f g e s t u r e s Application domains Flow Discrete Continuous 0 % 25 % 50 % 75 % 100 % 3 D m od e li ng A cce ss i b ilit y a nd A d a p ti v e S y s t e m s e L ea r n i ng a nd E du ca ti on G a m i ng I n - V e h i c l e a nd A u t o m o ti v e L a r g e D i s p l a y I n t e r ac ti on M a n i pu l a ti on / n a v i g a ti on M e d i a a nd E n t e r t a i n m e n t M e d i ca l T ec hno l ogy M ob il e i n t e r ac ti on H u m a n – R obo t I n t e r ac ti on S m a r t R oo m s / A pp li a n ce s P e r ce n t a g e s o f g e s t u r e s Application domains Binding Mixed Dependencies Object - Centric World - Dependent World - Independent 0 % 25 % 50 % 75 % 100 % 3 D m od e li ng A cce ss i b ilit y a nd A d a p ti v e S y s t e m s e L ea r n i ng a nd E du ca ti on G a m i ng I n - V e h i c l e a nd A u t o m o ti v e L a r g e D i s p l a y I n t e r ac ti on M a n i pu l a ti on / n a v i g a ti on M e d i a a nd E n t e r t a i n m e n t M e d i ca l T ec hno l ogy M ob il e i n t e r ac ti on H u m a n – R obo t I n t e r ac ti on S m a r t R oo m s / A pp li a n ce s P e r ce n t a g e s o f g e s t u r e s Application domains Body part Other Full Body Torso Leg Foot Head Hand + Arm Hand + Forearm Arm Forearm Hand + Wrist Wtist Hand Figure 3 : The percentage of gestures in each application domain in the fve - dimensional taxonomy . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Hosseini , et al . 6 . 2 Design Guidelines for Gesture Sets This section is a more in - depth discussion of selected gestures , including our formulation of a set of guidelines indicating possible ways to design mid - air gestures . Each of the following propositions may of course beneft from further empirical evaluation to verify their validity . • Gestures for Operating Imaginary Objects in Mid - air : Many of the most frequent gestures were derived from mid - air control of virtual objects . For example , a variety of ro - tary controllers , slider controls , or buttons are emulated for many referents . They are particularly prevalent in Smart Rooms / Appliance , In - Vehicle and Automotive , Media and Entertainment , and Manipulation / Navigation . Thus , we rec - ommend exploring such gestures along with corresponding virtual objects to create intuitive mappings between gestures and referents . • Reuse Touch - Based Interaction Paradigms : A substan - tial number of gestures we observed in diferent domains were variants of taps , directional swipes , slides , and pinch in / out fngers . Mid - air gestures therefore seem to mimic com - mon touch gesture inputs . Although this is called a legacy bias in the literature [ 127 ] , we suggest that mid - air , gesture - based input should take advantage of traditional interaction techniques and touch - screen gestures . • Opposing Referents : Dichotomous referents had almost the same level of agreement , instances of reverse gestures , and similar proportions . This occurred either when the agree - ment rate was high , such as zoom in / zoom out , scale in / scale out , next / previous , and increase / decrease , or when it was low , such as turn on / turn of . Designers might therefore want to assign to dichotomous gestures two opposing states ( such as closing the fst and spreading the fngers ) . • Similar Themes for Diferent Referents : Within the con - sensus set , similar gesture themes may be assigned to difer - ent referents . For example , both Zoom and Scale are squeezed together or pulled apart . Using similar gesture themes with diferent variations for various referents may result in greater memorability of gestures used in various domains . • Simple and Common Gestures : Simple and common ges - tures such as swipes , slides , points , taps , rotates , pulls , and squeezes were among the gestures most frequently found in diferent application domains . A good system , therefore , will likely include some variation of this set of well - known gestures . 7 CHALLENGES AND LIMITATIONS Through our survey and analysis of literature on cross - domain consistency of mid - air gestures , we identifed challenges and open issues for developing a set of transferable gestures . Below are key challenges drawn from 172 papers . 7 . 1 Building Interaction Systems in Real Environments Many systems and elicitation studies in this review were built and tested in controlled lab setups , and they often involved a set of simulation environments [ 96 ] . Systems and interactions may not transfer and scale well to environments more representative of everyday use , and users may change their preferred gestures outside the lab . Researchers , therefore , need to embed the process of gesture design in the users’ typical environments . For example , in a study by Ackad et al . [ 1 ] evaluating mid - air gesture interaction , participants interacted with a system available in a public mall . 7 . 2 Conducting User Studies Many of the papers we examined , especially those focused on designer - made gestures , did not include a user study , and gestures were introduced based on system preference . For example , Dinh et al . proposed gestures based on a depth recognitions for smart home appliances [ 54 ] , but did not explicitly consider the users’ mental model ( i . e . , memorability , intuitiveness , discoverability , learnabil - ity , and social acceptability ) . Assessing a user’s mental model is essential to design gestural interactions usable across application domains in terms of investigated ambiguity , representativeness , understandability , social acceptability , and comfort of gestures . De - signing with the user in the loop will result in better gesture designs than designs optimized purely towards a system’s capabilities . 7 . 3 Reproducing the Adequate Gesture in Context Some studies in this review were removed from the context of use , thus their real - world application is uncertain [ 74 , 84 ] . The number of papers in some domains is low , as well . For example , we identifed only 5 papers for gaming and 5 papers for accessibility & assistive systems . Looking at the results section , we see that the referent infuences AR . For example , in the transformation category , translate , rotate , and scale had higher AR compared to minimize and maximize . More work is needed in some domains to create a better balance between referents and each application domain . 7 . 4 Specifying Output Devices Output devices can vary in multiple ways , including the size of the screen ( PC , smartphones , or wall size display ) , 3D scenes in VR / AR , or even real objects in the smart home or in automotive vehicles . Because many papers did not report data related to the output devices they used [ 153 ] , we could not evaluate the efect of diferent devices on the consistency of the proposed gestures . Going forward , any study should explicitly defne the devices used , not only to allow evaluation of consistency across devices , but also to enable researchers to repeat that study with other devices or other user groups . 7 . 5 Considering a Gesture’s Ergonomic Aspects Another important aspect of gestural interaction design is the er - gonomics of the gestures . Research has shown that mid - air gesture systems developed without ergonomic considerations can have a negative efect on the health and experience of users long - term . Indeed , any gesture puts some level of strain on the musculoskeletal system , so that pain , discomfort , and fatigue can result , depending on the duration and amount of strain [ 174 ] . The design phase , there - fore , should include considerations of the ergonomics of the device or gesture and the anatomy of the body . Table 5 is based on the Towards a Consensus Gesture Set CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany mental model of gestures and lacks the ergonomic aspects . Existing research has provided guidelines to reduce the risk of disorders stemming from poor ergonomics . For example , using gestures with shorter duration and less use of extended arm range and shoulder movements [ 81 ] as well as micro gestures [ 112 ] can reduce arm fatigue . 7 . 6 Limitations and Future Work Our survey has some limitations . We used the query method to identify the corpus of papers , and the criteria we used for selection necessarily constrained the fnal corpus . For example , we limited our study to full papers . Some high - quality short papers , therefore , may have been left out . Second , we excluded databases that were not accessible by subscription from our academic institution . Third , the query search was conducted in April 2021 , so articles published after that time are not part of this review . We identifed a consensus set of gestures from a number of independent gesture elicitation studies and designers’ proposed gestures . This gesture set is derived from literature that reports on diferent studies using diferent similarity criteria to cluster the elicited gestures , as well as diferent sets of referents to elicit those gestures . In future work , we plan to conduct an experimental validation study to investigate the validity of our proposed gesture set in the social , spatial , cognitive , and physical dimensions . 8 CONCLUSION We conducted a systematic literature review of mid - air gestures in HCI and selected 172 related articles . We collected an extensive set of gestures in interactive systems from the literature and clustered them according to the dimensions of taxonomy to understand their commonality across various applications . Using agreement rate evaluations of gestures for more than 50 referents , we constructed a consensus gesture set containing 22 referents . Furthermore , we examined the infuence of the application domain on agreement rate and reported challenges in developing or selecting a transferable gesture set based on current studies . The lack of a universal gesture set that can be applied cross - domain is a major challenge in HCI . This paper and the resulting consensus gesture set contributes to addressing this challenge . ACKNOWLEDGMENTS The authors thank Donald Degraen . This project is funded by the Deutsche Forschungsgemeinschaft ( DFG , German Research Foun - dation ) under grant agreement 425868555 and is part of Priority Program SPP2199 on Scalable Interaction Paradigms for Pervasive Computing Environments . REFERENCES [ 1 ] Christopher Ackad , Andrew Clayphan , Martin Tomitsch , and Judy Kay . 2015 . An in - the - wild study of learning mid - air gestures to browse hierarchical in - formation at a large interactive public display . In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing . 1227 – 1238 . [ 2 ] Vamsi Kiran Adhikarla , Pawel Woźniak , Attila Barsi , Dave Singhal , Péter Tamás Kovács , and Tibor Balogh . 2014 . Freehand interaction with large - scale 3d map data . In 2014 3DTV - Conference : The True Vision - Capture , Transmission and Display of 3D Video ( 3DTV - CON ) . IEEE , 1 – 4 . [ 3 ] Naveed Ahmed , Hind Kharoub , Selma Medjden , Areej Alsaafn , and Mohammed Lataifeh . 2020 . A Comparative Study to Assess Human Machine Interaction Mechanisms for Large Scale Displays . In 2020 IEEE International Conference on Human - Machine Systems ( ICHMS ) . IEEE , 1 – 4 . [ 4 ] Roland Aigner , Daniel Wigdor , Hrvoje Benko , Michael Haller , David Lindbauer , Alexandra Ion , Shengdong Zhao , and JTKV Koh . 2012 . Understanding mid - air hand gestures : A study of human preferences in usage of gesture types for hci . Microsoft Research TechReport MSR - TR - 2012 - 111 2 ( 2012 ) , 30 . [ 5 ] Jason Alexander , Teng Han , William Judd , Pourang Irani , and Sriram Subrama - nian . 2012 . Putting your best foot forward : investigating real - world mappings for foot - based gestures . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 1229 – 1238 . [ 6 ] Abdullah X Ali , Meredith Ringel Morris , and Jacob O Wobbrock . 2018 . Crowd - sourcing similarity judgments for agreement analysis in end - user elicitation studies . In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology . 177 – 188 . [ 7 ] Mohd Hezri Amir , Albert Quek , Nur Rasyid Bin Sulaiman , and John See . 2016 . Duke : enhancing virtual reality based fps game with full - body interactions . In Proceedings of the 13th International Conference on Advances in Computer Entertainment Technology . 1 – 6 . [ 8 ] Dimitra Anastasiou , Cui Jian , and Christoph Stahl . 2013 . A German - Chinese speech - gesture behavioural corpus of device control in a smart home . In Pro - ceedings of the 6th International Conference on PErvasive Technologies Related to Assistive Environments . 1 – 6 . [ 9 ] SP Kasthuri Arachchi , Noorkholis Luthfl Hakim , Hui - Huang Hsu , Stanislav Vladimirovich Klimenko , and Timothy K Shih . 2018 . Real - time static and dynamic gesture recognition using mixed space features for 3D virtual world’s interactions . In 2018 32nd International Conference on Advanced Information Networking and Applications Workshops ( WAINA ) . IEEE , 627 – 632 . [ 10 ] Shaikh Shawon Arefn Shimon , Courtney Lutton , Zichun Xu , Sarah Morrison - Smith , Christina Boucher , and Jaime Ruiz . 2016 . Exploring non - touchscreen gestures for smartwatches . In Proceedings of the 2016 chi conference on human factors in computing systems . 3822 – 3833 . [ 11 ] Susanna Aromaa , Nikos Frangakis , Domenico Tedone , Juhani Viitaniemi , and Iina Aaltonen . 2018 . Digital human models in human factors and ergonomics evaluation of gesture interfaces . Proceedings of the ACM on Human - Computer Interaction 2 , EICS ( 2018 ) , 1 – 14 . [ 12 ] Rahul Arora , Rubaiat Habib Kazi , Danny M Kaufman , Wilmot Li , and Karan Singh . 2019 . Magicalhands : Mid - air hand gestures for animating in vr . In Pro - ceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology . 463 – 477 . [ 13 ] Ilhan Aslan , Tabea Schmidt , Jens Woehrle , Lukas Vogel , and Elisabeth André . 2018 . Pen + mid - air gestures : Eliciting contextual gestures . In Proceedings of the 20th ACM International Conference on Multimodal Interaction . 135 – 144 . [ 14 ] Sriram Karthik Badam , Fereshteh Amini , Niklas Elmqvist , and Pourang Irani . 2016 . Supporting visual exploration for multiple users in large display envi - ronments . In 2016 IEEE Conference on Visual Analytics Science and Technology ( VAST ) . IEEE , 1 – 10 . [ 15 ] Gilles Bailly , Jörg Müller , Michael Rohs , Daniel Wigdor , and Sven Kratz . 2012 . Shoesense : a new perspective on gestural interaction and wearable applications . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 1239 – 1248 . [ 16 ] Peter Barrie , Andreas Komninos , and Oleksii Mandrychenko . 2009 . A pervasive gesture - driven augmented reality prototype using wireless sensor body area networks . In Proceedings of the 6th International Conference on Mobile Technology , Application & Systems . 1 – 4 . [ 17 ] Thomas Baudel and Michel Beaudouin - Lafon . 1993 . Charade : Remote Control of Objects Using Free - Hand Gestures . Commun . ACM 36 , 7 ( jul 1993 ) , 28 – 35 . https : / / doi . org / 10 . 1145 / 159544 . 159562 [ 18 ] Vincent Becker , Felix Rauchenstein , and Gábor Sörös . 2019 . Investigating uni - versal appliance control through wearable augmented reality . In Proceedings of the 10th Augmented Human International Conference 2019 . 1 – 9 . [ 19 ] Hrvoje Benko and Andrew D Wilson . 2010 . Multi - point interactions with immer - sive omnidirectional visualizations in a dome . In ACM International Conference on Interactive Tabletops and Surfaces . 19 – 28 . [ 20 ] Shimmila Bhowmick , Pratul Kalita , and Keyur Sorathia . 2020 . A Gesture Elicita - tion Study for Selection of Nail Size Objects in a Dense and Occluded Dense HMD - VR . . In IndiaHCI’20 : Proceedings of the 11th Indian Conference on Human - Computer Interaction . 12 – 23 . [ 21 ] Idil Bostan , Oğuz Turan Buruk , Mert Canat , Mustafa Ozan Tezcan , Celalettin Yurdakul , Tilbe Göksun , and Oğuzhan Özcan . 2017 . Hands as a controller : User preferences for hand specifc on - skin gestures . In Proceedings of the 2017 Conference on Designing Interactive Systems . 1123 – 1134 . [ 22 ] Jared N Bott , James G Crowley , and Joseph J LaViola Jr . 2009 . Exploring 3D gestural interfaces for music creation in video games . In Proceedings of the 4th international Conference on Foundations of Digital Games . 18 – 25 . [ 23 ] Francesco Cafaro , Leilah Lyons , Raymond Kang , Josh Radinsky , Jessica Roberts , and Kristen Vogt . 2014 . Framed guessability : using embodied allegories to increase user agreement on gesture sets . In Proceedings of the 8th International Conference on Tangible , Embedded and Embodied Interaction . 197 – 204 . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Hosseini , et al . [ 24 ] Minghao Cai and Jiro Tanaka . 2017 . Trip together : a remote pair sightseeing sys - tem supporting gestural communication . In Proceedings of the 5th International Conference on Human Agent Interaction . 317 – 324 . [ 25 ] Rui Cao and Yue Liu . 2019 . Hand controlar : An augmented reality application for learning 3d geometry . In 2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct ( ISMAR - Adjunct ) . IEEE , 144 – 149 . [ 26 ] Marcus Carter , Joshua Newn , Eduardo Velloso , and Frank Vetere . 2015 . Remote gaze and gesture tracking on the microsoft kinect : Investigating the role of feedback . In Proceedings of the Annual Meeting of the Australian Special Interest Group for Computer Human Interaction . 167 – 176 . [ 27 ] L Caruso , R Russo , and S Savino . 2017 . Microsoft Kinect V2 vision system in a manufacturing application . Robotics and Computer - Integrated Manufacturing 48 ( 2017 ) , 174 – 181 . [ 28 ] Jessica R Cauchard , Jane L E , Kevin Y Zhai , and James A Landay . 2015 . Drone & me : an exploration into natural human - drone interaction . In Proceedings of the 2015 ACM international joint conference on pervasive and ubiquitous computing . 361 – 365 . [ 29 ] Nikolas Chaconas and Tobias Höllerer . 2018 . An evaluation of bimanual gestures on the microsoft hololens . In 2018 IEEE Conference on Virtual Reality and 3D User Interfaces ( VR ) . IEEE , 1 – 8 . [ 30 ] Edwin Chan , Teddy Seyed , Wolfgang Stuerzlinger , Xing - Dong Yang , and Frank Maurer . 2016 . User elicitation on single - hand microgestures . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . 3403 – 3414 . [ 31 ] Ishan Chatterjee , Robert Xiao , and Chris Harrison . 2015 . Gaze + gesture : Ex - pressive , precise and targeted free - space interactions . In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction . 131 – 138 . [ 32 ] Debaleena Chattopadhyay and Davide Bolchini . 2015 . Motor - intuitive interac - tions based on image schemas : Aligning touchless interaction primitives with human sensorimotor abilities . Interacting with Computers 27 , 3 ( 2015 ) , 327 – 343 . [ 33 ] Li - Chieh Chen , Po - Ying Chu , and Yun - Maw Cheng . 2016 . Exploring the er - gonomic issues of user - defned mid - air gestures for interactive product ex - hibition . In International Conference on Distributed , Ambient , and Pervasive Interactions . Springer , 180 – 190 . [ 34 ] Yu - Chun Chen , Chia - Ying Liao , Shuo - wen Hsu , Da - Yuan Huang , and Bing - Yu Chen . 2020 . Exploring User Defned Gestures for Ear - Based Interactions . Proceedings of the ACM on Human - Computer Interaction 4 , ISS ( 2020 ) , 1 – 20 . [ 35 ] Xiaogang Cheng , Qi Ge , Shipeng Xie , Guijin Tang , and Haibo Li . 2015 . UAV gesture interaction design for volumetric surveillance . Procedia Manufacturing 3 ( 2015 ) , 6639 – 6643 . [ 36 ] Allan Christensen , Simon A Pedersen , Per Bjerre , Andreas K Pedersen , and Wolfgang Stuerzlinger . 2016 . Transition times for manipulation tasks in hybrid interfaces . In International Conference on Human - Computer Interaction . Springer , 138 – 150 . [ 37 ] Karim Cissé , Aprajit Gandhi , Danielle Lottridge , and Robert Amor . 2020 . User Elicited Hand Gestures for VR - based Navigation of Architectural Designs . In 2020 IEEE Symposium on Visual Languages and Human - Centric Computing ( VL / HCC ) . IEEE , 1 – 5 . [ 38 ] Gabe Cohn , Daniel Morris , Shwetak Patel , and Desney Tan . 2012 . Human - tenna : using the body as an antenna for real - time whole - body interaction . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 1901 – 1910 . [ 39 ] Sabrina Connell , Pei - Yi Kuo , Liu Liu , and Anne Marie Piper . 2013 . A Wizard - of - Oz elicitation study examining child - defned gestures with a whole - body interface . In Proceedings of the 12th International Conference on Interaction Design and Children . 277 – 280 . [ 40 ] Harris Cooper , Larry Vernon Hedges , and Jefrey C Valentine . 2009 . The hand - book of research synthesis and meta - analysis 2nd edition . In The Hand . of Res . Synthesis and Meta - Analysis , 2nd Ed . Russell Sage Foundation , 1 – 615 . [ 41 ] Stefania Cuccurullo , Rita Francese , Sharefa Murad , Ignazio Passero , and Maurizio Tucci . 2012 . A gestural approach to presentation exploiting motion capture metaphors . In Proceedings of the International Working Conference on Advanced Visual Interfaces . 148 – 155 . [ 42 ] Jian Cui , Arjan Kuijper , and Alexei Sourin . 2016 . Exploration of natural free - hand interaction for shape modeling using leap motion controller . In 2016 International Conference on Cyberworlds ( CW ) . IEEE , 41 – 48 . [ 43 ] Dragos Datcu and Stephan Lukosch . 2013 . Free - hands interaction in augmented reality . In Proceedings of the 1st symposium on Spatial user interaction . 33 – 40 . [ 44 ] Eder de Oliveira , Esteban Walter Gonzalez , Daniela Gorski Trevisan , and Lu - ciana Cardoso de Castro Salgado . 2020 . Investigating Users’ Natural Engagement with a 3D Design Approach in an Egocentric Vision Scenario . In 2020 22nd Sym - posium on Virtual and Augmented Reality ( SVR ) . IEEE , 74 – 82 . [ 45 ] William Delamare , Chaklam Silpasuwanchai , Sayan Sarcar , Toshiaki Shiraki , and Xiangshi Ren . 2019 . On Gesture Combination : An Exploration of a Solution to Augment Gesture Interaction . In Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces . 135 – 146 . [ 46 ] Matthias Deller , Stefan Agne , Achim Ebert , Andreas Dengel , Hans Hagen , Bertin Klein , Michael Bender , Tony Bernardin , and Bernd Hamann . 2008 . Managing a document - based information space . In Proceedings of the 13th international conference on Intelligent user interfaces . 119 – 128 . [ 47 ] Henrik Detjen , Sarah Faltaous , Stefan Geisler , and Stefan Schneegass . 2019 . User - defned voice and mid - air gesture commands for maneuver - based interventions in automated vehicles . In Proceedings of Mensch und Computer 2019 . 341 – 348 . [ 48 ] Henrik Detjen , Stefan Geisler , and Stefan Schneegass . 2020 . Maneuver - based Control Interventions During Automated Driving : Comparing Touch , Voice , and Mid - Air Gestures as Input Modalities . In 2020 IEEE International Conference on Systems , Man , and Cybernetics ( SMC ) . IEEE , 3268 – 3274 . [ 49 ] Kapil Dev , Nicolas Villar , and Manfred Lau . 2016 . StandUp : understanding body - part and gestural preferences for frst - person 3D modeling . In Proceedings of the Joint Symposium on Computational Aesthetics and Sketch Based Interfaces and Modeling and Non - Photorealistic Animation and Rendering . 97 – 102 . [ 50 ] Christina Dicke and Jörg Müller . 2015 . Evaluating Mid - air List Interaction for Spatial Audio Interfaces . In Proceedings of the 3rd ACM Symposium on Spatial User Interaction . 24 – 33 . [ 51 ] Nem Khan Dim , Chaklam Silpasuwanchai , Sayan Sarcar , and Xiangshi Ren . 2016 . Designing mid - air TV gestures for blind people using user - and choice - based elicitation approaches . In Proceedings of the 2016 ACM Conference on Designing Interactive Systems . 204 – 214 . [ 52 ] Tilman Dingler , Markus Funk , and Florian Alt . 2015 . Interaction proxemics : Combining physical spaces for seamless gesture interaction . In Proceedings of the 4th International Symposium on Pervasive Displays . 107 – 114 . [ 53 ] Tilman Dingler , Rufat Rzayev , Alireza Sahami Shirazi , and Niels Henze . 2018 . Designing Consistent Gestures Across Device Types : Eliciting RSVP Controls for Phone , Watch , and Glasses . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI ’18 ) . Association for Computing Machinery , New York , NY , USA , 1 – 12 . https : / / doi . org / 10 . 1145 / 3173574 . 3173993 [ 54 ] Dong - Luong Dinh , Jeong Tai Kim , and Tae - Seong Kim . 2014 . Hand gesture recognition and interface via a depth imaging sensor for smart home appliances . Energy Procedia 62 ( 2014 ) , 576 – 582 . [ 55 ] Fatih Ergüner and Pınar Onay Durdu . 2015 . Multimodal natural interaction for 3D images . In 2015 9th International Conference on Application of Information and Communication Technologies ( AICT ) . IEEE , 305 – 309 . [ 56 ] Sharif AM Faleel , Michael Gammon , Yumiko Sakamoto , Carlo Menon , and Pourang Irani . 2020 . User gesture elicitation of common smartphone tasks for hand proximate user interfaces . In Proceedings of the 11th Augmented Human International Conference . 1 – 8 . [ 57 ] Yikai Fang , Xiujuan Chai , Lei Xu , and Kongqiao Wang . 2009 . Hand tracking and application in map navigation . In Proceedings of the First International Conference on Internet Multimedia Computing and Service . 196 – 200 . [ 58 ] Memon Md Farhan Md Fareed , Quraishi Imran Akram , Shaikh Bilal Ahmed Anees , and Awab Habib Fakih . 2015 . Gesture based wireless single - armed robot in cartesian 3D space using Kinect . In 2015 Fifth International Conference on Communication Systems and Network Technologies . IEEE , 1210 – 1215 . [ 59 ] Farzin Farhadi - Niaki , Reza GhasemAghaei , and Ali Arya . 2012 . Empirical Study of a Vision - Based Depth - Sensitive Human - Computer Interaction System . In Proceedings of the 10th Asia Pacifc Conference on Computer Human Interaction ( Matsue - city , Shimane , Japan ) ( APCHI ’12 ) . Association for Computing Machin - ery , New York , NY , USA , 101 – 108 . https : / / doi . org / 10 . 1145 / 2350046 . 2350070 [ 60 ] Shichang Feng , Zhiquan Feng , and Liujuan Cao . 2019 . Many - to - one gesture - to - command fexible mapping approach for smart teaching interface interaction . IEEE Access 7 ( 2019 ) , 179517 – 179531 . [ 61 ] Toni Fetzer , Christian Petry , Frank Deinzer , and Karsten Hufstadt . 2015 . 3D interaction design : Increasing the stimulus - response correspondence by using stereoscopic vision . In 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition ( FG ) , Vol . 1 . IEEE , 1 – 6 . [ 62 ] Sebastian Feuerstack , Mauro dos Santos Anjo , and Ednaldo Brigante Pizzolato . 2011 . Model - based design and generation of a gesture - based user interface navigation control . . In IHC + CLIHC . Citeseer , 227 – 231 . [ 63 ] Leah Findlater , Ben Lee , and Jacob Wobbrock . 2012 . Beyond QWERTY : augment - ing touch screen keyboards with multi - touch gestures for non - alphanumeric input . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 2679 – 2682 . [ 64 ] Rita Francese , Ignazio Passero , and Genovefa Tortora . 2012 . Wiimote and Kinect : gestural user interfaces add a natural third dimension to HCI . In Proceedings of the International Working Conference on Advanced Visual Interfaces . 116 – 123 . [ 65 ] Dustin Freeman , Ramadevi Vennelakanti , and Sriganesh Madhvanath . 2012 . Freehand pose - based gestural interaction : Studies and implications for interface design . In 2012 4th International Conference on Intelligent Human Computer Interaction ( IHCI ) . IEEE , 1 – 6 . [ 66 ] Thomas M Gable , Siddharth R Raja , Dean P Samuels , and Bruce N Walker . 2015 . Exploring and evaluating the capabilities of Kinect v2 in a driving simulator environment . In Proceedings of the 7th International Conference on Automotive User Interfaces and Interactive Vehicular Applications . 297 – 304 . [ 67 ] Priya Ganapathi and Keyur Sorathia . 2019 . Elicitation Study of Body Gestures for Locomotion in HMD - VR Interfaces in a Sitting - Position . In Motion , Interaction and Games . 1 – 10 . Towards a Consensus Gesture Set CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany [ 68 ] Franca Garzotto and Matteo Valoriani . 2013 . Touchless gestural interaction with small displays : a case study . In Proceedings of the Biannual Conference of the Italian Chapter of SIGCHI . 1 – 10 . [ 69 ] Vito Gentile , Daniele Fundarò , and Salvatore Sorce . 2019 . Elicitation and eval - uation of zoom gestures for touchless interaction with desktop displays . In Proceedings of the 8th ACM International Symposium on Pervasive Displays . 1 – 7 . [ 70 ] Vito Gentile , Salvatore Sorce , Giuseppe Russo , Dario Pirrone , and Antonio Gentile . 2016 . A Multimodal Fruition Model for Graphical Contents in Ancient Books . In Proceedings of the 17th International Conference on Computer Systems and Technologies 2016 . 65 – 72 . [ 71 ] Bogdan - Florin Gheran , Jean Vanderdonckt , and Radu - Daniel Vatavu . 2018 . Ges - tures for smart rings : empirical results , insights , and design implications . In Proceedings of the 2018 Designing Interactive Systems Conference . 623 – 635 . [ 72 ] Celeste Groenewald , Craig Anslow , Junayed Islam , Chris Rooney , Peter J Pass - more , and BL Wong . 2016 . Understanding 3D mid - air hand gestures with interactive surfaces and displays : a systematic literature review . ( 2016 ) . [ 73 ] Mikaylah Gross , Joe Dara , Christopher Meyer , and Davide Bolchini . 2018 . Ex - ploring Aural Navigation by Screenless Access . In Proceedings of the 15th Inter - national Web for All Conference . 1 – 10 . [ 74 ] Aakar Gupta , Thomas Pietrzak , Cleon Yau , Nicolas Roussel , and Ravin Balakr - ishnan . 2017 . Summon and select : Rapid interaction with interface controls in mid - air . In Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces . 52 – 61 . [ 75 ] Emilie Guy , Parinya Punpongsanon , Daisuke Iwai , Kosuke Sato , and Tamy Boubekeur . 2015 . LazyNav : 3D ground navigation with non - critical body parts . In 2015 IEEE symposium on 3D user interfaces ( 3DUI ) . IEEE , 43 – 50 . [ 76 ] Jeferson Y Han . 2005 . Low - cost multi - touch sensing through frustrated total internal refection . In Proceedings of the 18th annual ACM symposium on User interface software and technology . 115 – 118 . [ 77 ] Benjamin Hatscher and Christian Hansen . 2018 . Hand , foot or voice : Alternative input modalities for touchless interaction in the medical domain . In Proceedings of the 20th ACM International Conference on Multimodal Interaction . 145 – 153 . [ 78 ] Zhifan He , Ruifo Zhang , Zheng Liu , and Zhengyu Tan . 2020 . A User - Defned Gesture Set for Natural Interaction in a Smart Kitchen Environment . In 2020 13th International Symposium on Computational Intelligence and Design ( ISCID ) . IEEE , 122 – 125 . [ 79 ] Niels Henze , Andreas Löcken , Susanne Boll , Tobias Hesselmann , and Martin Pielot . 2010 . Free - hand gestures for music playback : deriving gestures with a user - centred process . In Proceedings of the 9th international conference on Mobile and Ubiquitous Multimedia . 1 – 10 . [ 80 ] Jahani F Hessam , Massimo Zancanaro , Manolya Kavakli , and Mark Billinghurst . 2017 . Towards optimization of mid - air gestures for in - vehicle interactions . In Proceedings of the 29th Australian Conference on Computer - Human Interaction . 126 – 134 . [ 81 ] Juan David Hincapié - Ramos , Xiang Guo , Paymahn Moghadasian , and Pourang Irani . 2014 . Consumed endurance : a metric to quantify arm fatigue of mid - air interactions . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 1063 – 1072 . [ 82 ] Lynn Hof , Eva Hornecker , and Sven Bertel . 2016 . Modifying gesture elicita - tion : Do kinaesthetic priming and increased production reduce legacy bias ? . In Proceedings of the TEI’16 : Tenth International Conference on Tangible , Embedded , and Embodied Interaction . 86 – 91 . [ 83 ] Dan Ionescu , Bogdan Ionescu , Cristian Gadea , and Shahidul Islam . 2011 . An intelligent gesture interface for controlling TV sets and set - top boxes . In 2011 6th IEEE international symposium on applied computational intelligence and informatics ( SACI ) . IEEE , 159 – 164 . [ 84 ] Simon Ismair , Julie Wagner , Ted Selker , and Andreas Butz . 2015 . Mime : Teach - ing mid - air pose - command mappings . In Proceedings of the 17th International Conference on Human - Computer Interaction with Mobile Devices and Services . 199 – 206 . [ 85 ] LE Jane , LE Ilene , James A Landay , and Jessica R Cauchard . 2017 . Drone & Wo : Cultural Infuences on Human - Drone Interaction Techniques . . In CHI . 6794 – 6799 . [ 86 ] Muhammad Jehanzeb , Muhammad Sajid Khan , Muhammad Ahmed Khan , Umair Hassan , Majid Mehmood , and Muhammad Imran Babar . 2020 . A new 3D Viewer system based on hand gesture recognition for smart interaction . In 2020 Interna - tional Conference on Computing and Information Technology ( ICCIT - 1441 ) . IEEE , 1 – 4 . [ 87 ] Hyo Jeong Kang , Jung - hye Shin , and Kevin Ponto . 2020 . A comparative analysis of 3d user interaction : How to move virtual objects in mixed reality . In 2020 IEEE conference on virtual reality and 3D user interfaces ( VR ) . IEEE , 275 – 284 . [ 88 ] Maria Karam et al . 2005 . A taxonomy of gestures in human computer interac - tions . ( 2005 ) . [ 89 ] Maria Karam and mc schraefel . 2005 . A study on the use of semaphoric gestures to support secondary task interactions . In CHI’05 Extended Abstracts on Human Factors in Computing Systems . 1961 – 1964 . [ 90 ] Keiko Katsuragawa , Krzysztof Pietroszek , James R Wallace , and Edward Lank . 2016 . Watchpoint : Freehand pointing with a smartwatch in a ubiquitous display environment . In Proceedings of the International Working Conference on Advanced Visual Interfaces . 128 – 135 . [ 91 ] Frederic Kerber , Markus Löchtefeld , Antonio Krüger , Jess McIntosh , Charlie McNeill , and Mike Fraser . 2016 . Understanding same - side interactions with wrist - worn devices . In Proceedings of the 9th Nordic Conference on Human - Computer Interaction . 1 – 10 . [ 92 ] Chaowanan Khundam . 2015 . First person movement control with palm normal and hand gesture interaction in virtual reality . In 2015 12th International Joint Conference on Computer Science and Software Engineering ( JCSSE ) . IEEE , 325 – 330 . [ 93 ] David Kim , Otmar Hilliges , Shahram Izadi , Alex D Butler , Jiawen Chen , Iason Oikonomidis , and Patrick Olivier . 2012 . Digits : freehand 3D interactions any - where using a wrist - worn gloveless sensor . In Proceedings of the 25th annual ACM symposium on User interface software and technology . 167 – 176 . [ 94 ] Hyosun Kim and Dieter W Fellner . 2004 . Interaction with hand gesture for a back - projection wall . In Proceedings Computer Graphics International , 2004 . IEEE , 395 – 402 . [ 95 ] Hyoungnyoun Kim , Jun - Sik Kim , Kwanghyun Ryu , Seyoung Cheon , Yonghwan Oh , and Ji - Hyung Park . 2014 . Task - oriented teleoperation through natural 3D user interaction . In 2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence ( URAI ) . IEEE , 335 – 338 . [ 96 ] Myeongseop Kim , Eunjin Seong , Younkyung Jwa , Jieun Lee , and Seungjun Kim . 2020 . A cascaded multimodal natural user interface to reduce driver distraction . IEEE Access 8 ( 2020 ) , 112969 – 112984 . [ 97 ] Marion Koelle , Swamy Ananthanarayan , Simon Czupalla , Wilko Heuten , and Susanne Boll . 2018 . Your smart glasses’ camera bothers me ! exploring opt - in and opt - out gestures for privacy mediation . In Proceedings of the 10th Nordic Conference on Human - Computer Interaction . 473 – 481 . [ 98 ] Barry Kollee , Sven Kratz , and Anthony Dunnigan . 2014 . Exploring gestural interaction in smart spaces using head mounted devices with ego - centric sensing . In Proceedings of the 2nd ACM symposium on spatial user interaction . 40 – 49 . [ 99 ] Panayiotis Koutsabasis and Chris K Domouzis . 2016 . Mid - air browsing and selec - tion in image collections . In Proceedings of the International Working Conference on Advanced Visual Interfaces . 21 – 27 . [ 100 ] Sven Kratz , Michael Rohs , Dennis Guse , Jörg Müller , Gilles Bailly , and Michael Nischt . 2012 . PalmSpace : continuous around - device gestures vs . multitouch for 3D rotation tasks on mobile devices . In Proceedings of the international working conference on advanced visual interfaces . 181 – 188 . [ 101 ] Christine Kühnel , Tilo Westermann , Fabian Hemmert , Sven Kratz , Alexander Müller , and Sebastian Möller . 2011 . I’m home : Defning and evaluating a gesture set for smart - home control . International Journal of Human - Computer Studies 69 , 11 ( 2011 ) , 693 – 704 . [ 102 ] Senthil Kumar and Jakub Segen . 1999 . Gesture based 3d man - machine inter - action using a single camera . In Proceedings IEEE International Conference on Multimedia Computing and Systems , Vol . 1 . IEEE , 630 – 635 . [ 103 ] Samuel A Lacolina , Alessandro Soro , and Riccardo Scateni . 2011 . Natural ex - ploration of 3D models . In Proceedings of the 9th ACM SIGCHI Italian Chapter International Conference on Computer - Human Interaction : Facing Complexity . 118 – 121 . [ 104 ] Marc Erich Latoschik , Martin Frohlich , Bernhard Jung , and Ipke Wachsmuth . 1998 . Utilize speech and gestures to realize natural interaction in a virtual environment . In IECON’98 . Proceedings of the 24th Annual Conference of the IEEE Industrial Electronics Society ( Cat . No . 98CH36200 ) , Vol . 4 . IEEE , 2028 – 2033 . [ 105 ] Hyong - Euk Lee , Nahyup Kang , Jae - Joon Han , Ji - Yeon Kim , Kyung Hwan Kim , James DK Kim , and ChangYeong Kim . 2013 . Interactive manipulation and visualization of a deformable 3D organ model for medical diagnostic support . In 2013 IEEE 10th Consumer Communications and Networking Conference ( CCNC ) . IEEE , 50 – 55 . [ 106 ] Jeannie SA Lee . 2018 . Speech and Gestures for Smart - Home Control and In - teraction for Older Adults . In Proceedings of the 3rd International Workshop on Multimedia for Personal Health and Health Care . 49 – 57 . [ 107 ] Sang - Su Lee , Jeonghun Chae , Hyunjeong Kim , Youn - kyung Lim , and Kun - pyo Lee . 2013 . Towards more natural digital content manipulation via user freehand gestural interaction in a living room . In Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing . 617 – 626 . [ 108 ] Daniel Leithinger , David Lakatos , Anthony DeVincenzi , Matthew Blackshaw , and Hiroshi Ishii . 2011 . Direct and gestural interaction with relief : a 2 . 5 D shape display . In Proceedings of the 24th annual ACM symposium on User interface software and technology . 541 – 548 . [ 109 ] Hoo Yong Leng , Noris Mohd Norowi , and Azrul Hazri Jantan . 2017 . A user - defned gesture set for music interaction in immersive virtual environment . In Proceedings of the 3rd International Conference on Human - Computer Interaction and User Experience in Indonesia . 44 – 51 . [ 110 ] Hui Liang , Jian Chang , Shujie Deng , Can Chen , Ruofeng Tong , and Jianjun Zhang . 2015 . Exploitation of novel multiplayer gesture - based interaction and virtual puppetry for digital storytelling to develop children’s narrative skills . In Proceedings of the 14th ACM SIGGRAPH International Conference on Virtual Reality Continuum and its Applications in Industry . 63 – 72 . CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Hosseini , et al . [ 111 ] Hongjian Liao and Xiaoli Long . 2013 . Study on virtual assembly system based on Kinect somatosensory interaction . In 2013 International Conference on Infor - mation Science and Cloud Computing . IEEE , 55 – 60 . [ 112 ] Jaime Lien , Nicholas Gillian , M Emre Karagozler , Patrick Amihood , Carsten Schwesig , Erik Olson , Hakim Raja , and Ivan Poupyrev . 2016 . Soli : Ubiquitous gesture sensing with millimeter wave radar . ACM Transactions on Graphics ( TOG ) 35 , 4 ( 2016 ) , 1 – 19 . [ 113 ] Hongzhe Liu , Yulong Xi , Wei Song , Kyhyun Um , and Kyungeun Cho . 2013 . Gesture - based NUI application for real - time path modifcation . In 2013 IEEE 11th International Conference on Dependable , Autonomic and Secure Computing . IEEE , 446 – 449 . [ 114 ] Jiaxin Liu , Hongxin Zhang , and Chuankang Li . 2020 . COMTIS : Customizable touchless interaction system for large screen visualization . Virtual Reality & Intelligent Hardware 2 , 2 ( 2020 ) , 162 – 174 . [ 115 ] Lidun Long , Xinsha Fu , Honglei Zhu , and Ting Ge . 2015 . Finger gesture - based natural user interface for 3D highway alignment design in virtual environment . In 2015 4th International Conference on Computer Science and Network Technology ( ICCSNT ) , Vol . 1 . IEEE , 105 – 111 . [ 116 ] Daniel Lopes , Filipe Relvas , Soraia Paulo , Yosra Rekik , Laurent Grisoni , and Joaquim Jorge . 2019 . FEETICHE : FEET Input for Contactless Hand gEsture Interaction . In The 17th International Conference on Virtual - Reality Continuum and its Applications in Industry . 1 – 10 . [ 117 ] Daniel Simões Lopes , Pedro Duarte de Figueiredo Parreira , Soraia Figueiredo Paulo , Vitor Nunes , Paulo Amaral Rego , Manuel Cassiano Neves , Pedro Silva Rodrigues , and Joaquim Armando Jorge . 2017 . On the utility of 3D hand cur - sors to explore medical volume datasets with a touchless interface . Journal of biomedical informatics 72 ( 2017 ) , 140 – 149 . [ 118 ] Shufang Lu , Li Cai , and Fei Gao . 2019 . Immersive interaction design based on perception of vector feld climate data . Systems Science & Control Engineering 7 , 1 ( 2019 ) , 116 – 124 . [ 119 ] Yiqin Lu , Bingjian Huang , Chun Yu , Guahong Liu , and Yuanchun Shi . 2020 . Designing and Evaluating Hand - to - Hand Gestures with Dual Commodity Wrist - Worn Devices . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 4 , 1 ( 2020 ) , 1 – 27 . [ 120 ] Zhiyuan Lu , Xiang Chen , Zhangyan Zhao , and Kongqiao Wang . 2011 . A proto - type of gesture - based interface . In Proceedings of the 13th International Confer - ence on Human Computer Interaction with Mobile Devices and Services . 33 – 36 . [ 121 ] Dan Mauney , Jonathan Howarth , Andrew Wirtanen , and Miranda Capra . 2010 . Cultural similarities and diferences in user - defned gestures for touchscreen user interfaces . In CHi’10 extended abstracts on human factors in computing systems . 4015 – 4020 . [ 122 ] Keenan R May , Thomas M Gable , and Bruce N Walker . 2017 . Designing an in - vehicle air gesture set using elicitation methods . In Proceedings of the 9th International Conference on Automotive User Interfaces and Interactive Vehicular Applications . 74 – 83 . [ 123 ] Daniel Mendes , Fernando Fonseca , Bruno Araujo , Alfredo Ferreira , and Joaquim Jorge . 2014 . Mid - air interactions above stereoscopic interactive tables . In 2014 IEEE Symposium on 3D User Interfaces ( 3DUI ) . IEEE , 3 – 10 . [ 124 ] Bob - Antoine J Menelas and Martin JD Otis . 2013 . Use of foot for direct inter - actions with entities of a virtual environment displayed on a mobile device . In 2013 IEEE International Conference on Systems , Man , and Cybernetics . IEEE , 3745 – 3750 . [ 125 ] Pallavi Mohan , Wooi Boon Goh , Chi - Wing Fu , and Sai - Kit Yeung . 2019 . Head - Fingers - Arms : Physically - Coupled and Decoupled Multimodal Interaction De - signs in Mobile VR . In The 17th International Conference on Virtual - Reality Continuum and its Applications in Industry . 1 – 9 . [ 126 ] Meredith Ringel Morris . 2012 . Web on the wall : insights from a multimodal inter - action elicitation study . In Proceedings of the 2012 ACM international conference on Interactive tabletops and surfaces . 95 – 104 . [ 127 ] Meredith Ringel Morris , Andreea Danielescu , Steven Drucker , Danyel Fisher , Bongshin Lee , MC Schraefel , and Jacob O Wobbrock . 2014 . Reducing legacy bias in gesture elicitation studies . interactions 21 , 3 ( 2014 ) , 40 – 45 . [ 128 ] Meredith Ringel Morris , Jacob O Wobbrock , and Andrew D Wilson . 2010 . Un - derstanding users’ preferences for surface gestures . In Proceedings of graphics interface 2010 . 261 – 268 . [ 129 ] Bojan Mrazovac , Milan Z Bjelica , Djordje Simić , Srdjan Tikvić , and Ištvan Papp . 2011 . Gesture based hardware interface for RF lighting control . In 2011 IEEE 9th International Symposium on Intelligent Systems and Informatics . IEEE , 309 – 314 . [ 130 ] Jörg Müller , Matthias Geier , Christina Dicke , and Sascha Spors . 2014 . The boomroom : mid - air direct interaction with virtual sound sources . In Proceedings of the SIGCHI conference on human factors in computing systems . 247 – 256 . [ 131 ] Kanako Nakazato , Hiroaki Nishino , and Toshitada Kodama . 2016 . A desktop 3D modeling system controllable by mid - air interactions . In 2016 10th International Conference on Complex , Intelligent , and Software Intensive Systems ( CISIS ) . IEEE , 633 – 637 . [ 132 ] Michael Nebeling , Alexander Huber , David Ott , and Moira C Norrie . 2014 . Web on the wall reloaded : Implementation , replication and refnement of user - defned interaction sets . In Proceedings of the Ninth ACM International Conference on Interactive Tabletops and Surfaces . 15 – 24 . [ 133 ] Stelian Nicola , Lăcrămioara Stoicu - Tivadar , Ioan Virag , and Mihaela Crişan - Vida . 2016 . Leap motion supporting medical education . In 2016 12th IEEE International Symposium on Electronics and Telecommunications ( ISETC ) . IEEE , 153 – 156 . [ 134 ] Michael Nielsen , Moritz Störring , Thomas B Moeslund , and Erik Granum . 2003 . A procedure for developing intuitive and ergonomic gesture interfaces for HCI . In International gesture workshop . Springer , 409 – 420 . [ 135 ] Hiroaki Nishino , Midori Fushimi , Kouichi Utsumiya , and Kazuyoshi Korida . 1999 . A virtual environment for modeling 3D objects through spatial interaction . In IEEE SMC’99 Conference Proceedings . 1999 IEEE International Conference on Systems , Man , and Cybernetics ( Cat . No . 99CH37028 ) , Vol . 6 . IEEE , 81 – 86 . [ 136 ] Mohammad Obaid , Felix Kistler , Gabriele˙ Kasparavičiut¯ e , ˙ Asim Evren Yantaç , and Morten Fjeld . 2016 . How would you gesture navigate a drone ? a user - centered approach to control a drone . In Proceedings of the 20th International Academic Mindtrek Conference . 113 – 121 . [ 137 ] Francisco R Ortega , Alain Galvan , Katherine Tarre , Armando Barreto , Naphtali Rishe , Jonathan Bernal , Ruben Balcazar , and Jason - Lee Thomas . 2017 . Gesture elicitation for 3D travel via multi - touch and mid - Air systems for procedurally generated pseudo - universe . In 2017 IEEE symposium on 3D User Interfaces ( 3DUI ) . IEEE , 144 – 153 . [ 138 ] Francisco R Ortega , Katherine Tarre , Mathew Kress , Adam S Williams , Ar - mando B Barreto , and Naphtali D Rishe . 2019 . Selection and manipulation whole - body gesture elicitation study in virtual reality . In 2019 IEEE Conference on Virtual Reality and 3D User Interfaces ( VR ) . IEEE , 1723 – 1728 . [ 139 ] Matthew J Page , Joanne E McKenzie , Patrick M Bossuyt , Isabelle Boutron , Tammy C Hofmann , Cynthia D Mulrow , Larissa Shamseer , Jennifer M Tet - zlaf , Elie A Akl , Sue E Brennan , et al . 2021 . The PRISMA 2020 statement : an updated guideline for reporting systematic reviews . Bmj 372 ( 2021 ) . [ 140 ] Anshul Patro , Vineet Kataria , and Shalini Sharma . 2015 . EducApp . In 2015 International Conference on Industrial Engineering and Operations Management ( IEOM ) . IEEE , 1 – 6 . [ 141 ] Sofa Pescarin , Eva Pietroni , Leonardo Rescic , Mattias Wallergárd , Karim Omar , and Claudio Rufa . 2013 . NICH : a preliminary theoretical study on Natural Interaction applied to Cultural Heritage contexts . In 2013 Digital Heritage Inter - national Congress ( DigitalHeritage ) , Vol . 1 . IEEE , 355 – 362 . [ 142 ] Ekaterina Peshkova and Martin Hitz . 2017 . Exploring user - defned gestures to control a group of four UAVs . In 2017 26th IEEE International Symposium on Robot and Human Interactive Communication ( RO - MAN ) . IEEE , 169 – 174 . [ 143 ] Tran Pham , Jo Vermeulen , Anthony Tang , and Lindsay MacDonald Vermeulen . 2018 . Scale impacts elicited gestures for manipulating holograms : Implications for ar gesture design . In Proceedings of the 2018 Designing Interactive Systems Conference . 227 – 240 . [ 144 ] Krzysztof Pietroszek , Liudmila Tahai , James R Wallace , and Edward Lank . 2017 . Watchcasting : Freehand 3D interaction with of - the - shelf smartwatch . In 2017 IEEE Symposium on 3D User Interfaces ( 3DUI ) . IEEE , 172 – 175 . [ 145 ] Thammathip Piumsomboon , Adrian Clark , Mark Billinghurst , and Andy Cock - burn . 2013 . User - defned gestures for augmented reality . In IFIP Conference on Human - Computer Interaction . Springer , 282 – 299 . [ 146 ] Katrin Plaumann , David Lehr , and Enrico Rukzio . 2016 . Who Has the Force ? : Solving Conficts for Multi User Mid - Air Gestures for TVs . . In TVX . 25 – 29 . [ 147 ] G Michael Poor and Alvin Jude . 2019 . Interaction can hurt - Exploring gesture - based interaction for users with Chronic Pain . In Symposium on Spatial User Interaction . 1 – 5 . [ 148 ] Camilo Perez Quintero , Romeo Tatsambon , Mona Gridseth , and Martin Jäger - sand . 2015 . Visual pointing gestures for bi - directional human robot interaction in a pick - and - place task . In 2015 24th IEEE International Symposium on Robot and Human Interactive Communication ( RO - MAN ) . IEEE , 349 – 354 . [ 149 ] Dorothy Rachovides , James Walkerdine , and Peter Phillips . 2007 . The conductor interaction method . ACM Transactions on Multimedia Computing , Communica - tions , and Applications ( TOMM ) 3 , 4 ( 2007 ) , 1 – 23 . [ 150 ] ASM Mahfujur Rahman , Jamal Saboune , and Abdulmotaleb El Saddik . 2011 . Motion - path based in car gesture control of the multimedia devices . In Pro - ceedings of the frst ACM international symposium on Design and analysis of intelligent vehicular networks and applications . 69 – 76 . [ 151 ] Md Asif Ur Rahman , Md Saef Ullah Miah , M Abrar Fahad , and Debajyoti Kar - maker . 2014 . SHIMPG : Simple human interaction with machine using Physical Gesture . In 2014 13th International Conference on Control Automation Robotics & Vision ( ICARCV ) . IEEE , 301 – 305 . [ 152 ] Hanae Rateau , Laurent Grisoni , and Bruno De Araujo . 2014 . Mimetic interaction spaces : Controlling distant displays in pervasive environments . In Proceedings of the 19th international conference on Intelligent User Interfaces . 89 – 94 . [ 153 ] Siddharth S Rautaray and Anupam Agrawal . 2012 . Real time gesture recognition system for interaction in dynamic environment . Procedia Technology 4 ( 2012 ) , 595 – 599 . [ 154 ] Mohammad Riduwan , Ahmad Hoirul Basori , and Farhan Mohamed . 2013 . Finger - based gestural interaction for exploration of 3D heart visualization . Procedia - Social and Behavioral Sciences 97 ( 2013 ) , 684 – 690 . Towards a Consensus Gesture Set CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany [ 155 ] Isabel Benavente Rodriguez and Nicolai Marquardt . 2017 . Gesture elicitation study on how to opt - in & opt - out from interactions with public displays . In Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces . 32 – 41 . [ 156 ] Gustavo Alberto Rovelo Ruiz , Davy Vanacken , Kris Luyten , Francisco Abad , and Emilio Camahort . 2014 . Multi - viewer gesture - based interaction for omni - directional video . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 4077 – 4086 . [ 157 ] Jaime Ruiz , Yang Li , and Edward Lank . 2011 . User - defned motion gestures for mobile interaction . In Proceedings of the SIGCHI conference on human factors in computing systems . 197 – 206 . [ 158 ] Kenneth Sabir , Christian Stolte , Bruce Tabor , and Seán I O’Donoghue . 2013 . The Molecular Control Toolkit : Controlling 3D molecular graphics via gesture and voice . In 2013 IEEE Symposium on Biological Data Visualization ( BioVis ) . IEEE , 49 – 56 . [ 159 ] Hannah Sampson , Dan Kelly , Burkhard C Wünsche , and Robert Amor . 2018 . A hand gesture set for navigating and interacting with 3d virtual environments . In 2018 International Conference on Image and Vision Computing New Zealand ( IVCNZ ) . IEEE , 1 – 6 . [ 160 ] Kadek Ananta Satriadi , Barrett Ens , Maxime Cordeil , Bernhard Jenny , Tobias Czauderna , and Wesley Willett . 2019 . Augmented reality map navigation with freehand gestures . In 2019 IEEE Conference on Virtual Reality and 3D User Inter - faces ( VR ) . IEEE , 593 – 603 . [ 161 ] Thomas Schlömer , Benjamin Poppinga , Niels Henze , and Susanne Boll . 2008 . Gesture recognition with a Wii controller . In Proceedings of the 2nd international conference on Tangible and embedded interaction . 11 – 14 . [ 162 ] Stephan Schröder , Nina Loftfeld , Benjamin Langmann , Klaus Frank , and Ed - uard Reithmeier . 2014 . Contactless operating table control based on 3D image processing . In 2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society . IEEE , 388 – 392 . [ 163 ] Jakub Segen and Senthil Kumar . 1998 . Fast and accurate 3D gesture recogni - tion interface . In Proceedings . Fourteenth International Conference on Pattern Recognition ( Cat . No . 98EX170 ) , Vol . 1 . IEEE , 86 – 91 . [ 164 ] Matthias Seuter , Eduardo Rodriguez Macrillante , Gernot Bauer , and Christian Kray . 2018 . Running with drones : desired services and control gestures . In Proceedings of the 30th Australian Conference on Computer - Human Interaction . 384 – 395 . [ 165 ] Teddy Seyed , Chris Burns , Mario Costa Sousa , Frank Maurer , and Anthony Tang . 2012 . Eliciting usable gestures for multi - display environments . In Proceedings of the 2012 ACM international conference on Interactive tabletops and surfaces . 41 – 50 . [ 166 ] Syed Akhlaq Hussain Shah , Ali Ahmed , Iftekhar Mahmood , and Khurram Khur - shid . 2011 . Hand gesture based user interface for computer using a camera and projector . In 2011 IEEE International Conference on Signal and Image Processing Applications ( ICSIPA ) . IEEE , 168 – 173 . [ 167 ] Rajeev Sharma , Thomas S Huang , Vladimir I PavloviC , Yunxin Zhao , Zion Lo , Stephen Chu , and K Schul . 1996 . Speech / gesture interface to a visual comput - ing environment for molecular biologists . In Proceedings of 13th international conference on pattern recognition , Vol . 3 . IEEE , 964 – 968 . [ 168 ] Sumita Sharma , Blessin Varkey , Krishnaveni Achary , Jaakko Hakulinen , Markku Turunen , Tomi Heimonen , Saurabh Srivastava , and Nitendra Rajput . 2018 . De - signing gesture - based applications for individuals with developmental disabil - ities : guidelines from user studies in India . ACM Transactions on Accessible Computing ( TACCESS ) 11 , 1 ( 2018 ) , 1 – 27 . [ 169 ] Junchen Shen , Yanlin Luo , Xingce Wang , Zhongke Wu , and Mingquan Zhou . 2014 . Gpu - based realtime hand gesture interaction and rendering for volume datasets using leap motion . In 2014 International Conference on Cyberworlds . IEEE , 85 – 92 . [ 170 ] Mohd Fairuz Shiratuddin and Kok Wai Wong . 2011 . Non - contact multi - hand gestures interaction techniques for architectural design in a virtual environment . In ICIMU 2011 : Proceedings of the 5th international Conference on Information Technology & Multimedia . IEEE , 1 – 6 . [ 171 ] Ben Shneiderman . 1981 . Direct manipulation : A step beyond programming languages . In Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems . ( Part - II ) : Human Interface and the User Interface - Volume 1981 . 143 . [ 172 ] Shaishav Siddhpuria , Keiko Katsuragawa , James R Wallace , and Edward Lank . 2017 . Exploring at - your - side gestural interaction for ubiquitous environments . In Proceedings of the 2017 Conference on Designing Interactive Systems . 1111 – 1122 . [ 173 ] Adalberto L Simeone , Eduardo Velloso , Jason Alexander , and Hans Gellersen . 2014 . Feet movement in desktop 3D interaction . In 2014 IEEE Symposium on 3D User Interfaces ( 3DUI ) . IEEE , 71 – 74 . [ 174 ] Alison Smith . 1996 . Upper Limb Disorders – Time to relax ? Physiotherapy 82 , 1 ( 1996 ) , 31 – 38 . [ 175 ] Jie Song , Gábor Sörös , Fabrizio Pece , Sean Ryan Fanello , Shahram Izadi , Cem Keskin , and Otmar Hilliges . 2014 . In - air gestures around unmodifed mobile devices . In Proceedings of the 27th annual ACM symposium on User interface software and technology . 319 – 329 . [ 176 ] Peng Song , Wooi Boon Goh , William Hutama , Chi - Wing Fu , and Xiaopei Liu . 2012 . A handle bar metaphor for virtual object manipulation with mid - air inter - action . In Proceedings of the SIGCHI conference on human factors in computing systems . 1297 – 1306 . [ 177 ] Stefan Soutschek , Jochen Penne , Joachim Hornegger , and Johannes Kornhuber . 2008 . 3 - d gesture - based scene navigation in medical imaging applications using time - of - fight cameras . In 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops . IEEE , 1 – 6 . [ 178 ] Kashmiri Stec and Lars Bo Larsen . 2018 . Gestures for controlling a moveable tv . In Proceedings of the 2018 ACM International Conference on Interactive Experiences for TV and Online Video . 5 – 14 . [ 179 ] M Stecher , E Baseler , L Draxler , L Fricke , B Michel , A Zimmermann , and Klaus Bengler . 2015 . Tracking down the intuitiveness of gesture interaction in the truck domain . Procedia Manufacturing 3 ( 2015 ) , 3176 – 3183 . [ 180 ] Surya Sumpeno , I Gede Aris Dharmayasa , Supeno Mardi Susiki Nugroho , and Diana Purwitasari . 2019 . Immersive Hand Gesture for Virtual Museum using Leap Motion Sensor Based on K - Nearest Neighbor . In 2019 International Con - ference on Computer Engineering , Network , and Intelligent Multimedia ( CENIM ) . IEEE , 1 – 6 . [ 181 ] Yongbin Sun , Alexandre Armengol - Urpi , Sai Nithin Reddy Kantareddy , Joshua Siegel , and Sanjay Sarma . 2019 . Magichand : Interact with iot devices in aug - mented reality environment . In 2019 IEEE Conference on Virtual Reality and 3D User Interfaces ( VR ) . IEEE , 1738 – 1743 . [ 182 ] Theophanis Tsandilas . 2018 . Fallacies of Agreement : A Critical Review of Consensus Assessment Methods for Gesture Elicitation . ACM Trans . Comput . - Hum . Interact . 25 , 3 , Article 18 ( jun 2018 ) , 49 pages . https : / / doi . org / 10 . 1145 / 3182168 [ 183 ] Panagiota Tsarouchi , Athanasios Athanasatos , Sotiris Makris , Xenofon Chatzi - georgiou , and George Chryssolouris . 2016 . High level robot programming using body and hand gestures . Procedia Cirp 55 ( 2016 ) , 1 – 5 . [ 184 ] Apivan Tuntakurn , Saowapak S Thongvigitmanee , Vera Sa - Ing , Shoichi Hasegawa , and Stanislav S Makhanov . 2013 . Natural interactive 3D medical image viewer based on fnger and arm gestures . In The 6th 2013 Biomedical Engineering International Conference . IEEE , 1 – 5 . [ 185 ] Fereydoon Vafaei . 2013 . Taxonomy of gestures in human computer interaction . ( 2013 ) . [ 186 ] Radu - Daniel Vatavu . 2012 . User - defned gestures for free - hand TV control . In Proceedings of the 10th European conference on Interactive tv and video . 45 – 48 . [ 187 ] Radu - Daniel Vatavu . 2013 . A comparative study of user - defned handheld vs . freehand gestures for home entertainment environments . Journal of Ambient Intelligence and Smart Environments 5 , 2 ( 2013 ) , 187 – 211 . [ 188 ] Radu - Daniel Vatavu and Jacob O Wobbrock . 2015 . Formalizing agreement analysis for elicitation studies : new measures , signifcance test , and toolkit . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems . 1325 – 1334 . [ 189 ] Radu - Daniel Vatavu and Ionut - Alexandru Zaiti . 2014 . Leap gestures for TV : insights from an elicitation study . In Proceedings of the ACM International Con - ference on Interactive Experiences for TV and Online Video . 131 – 138 . [ 190 ] Santiago Villarreal - Narvaez , Jean Vanderdonckt , Radu - Daniel Vatavu , and Ja - cob O Wobbrock . 2020 . A Systematic Review of Gesture Elicitation Studies : What Can We Learn from 216 Studies ? . In Proceedings of the 2020 ACM Designing Interactive Systems Conference . 855 – 872 . [ 191 ] Panagiotis Vogiatzidakis and Panayiotis Koutsabasis . 2019 . Frame - based elici - tation of mid - air gestures for a smart home device ecosystem . In Informatics , Vol . 6 . Multidisciplinary Digital Publishing Institute , 23 . [ 192 ] Tijana Vuletic , Alex Dufy , Laura Hay , Chris McTeague , Gerard Campbell , and Madeleine Grealy . 2019 . Systematic literature review of hand gestures used in human computer interaction interfaces . International Journal of Human - Computer Studies 129 ( 2019 ) , 74 – 94 . [ 193 ] Jorge A Wagner Filho , Wolfgang Stuerzlinger , and Luciana Nedel . 2019 . Eval - uating an immersive space - time cube geovisualization for intuitive trajectory data exploration . IEEE Transactions on Visualization and Computer Graphics 26 , 1 ( 2019 ) , 514 – 524 . [ 194 ] Yu Tian Wang , Lu Yuan , Juan Juan Cai , Hui Wang , and Qin Zhang . 2017 . Inter - action design of gesture control for DMS multi - channel system . In 2017 IEEE 9th International Conference on Communication Software and Networks ( ICCSN ) . IEEE , 1524 – 1528 . [ 195 ] Christian Weichel , Manfred Lau , David Kim , Nicolas Villar , and Hans W Gellersen . 2014 . MixFab : a mixed - reality environment for personal fabrica - tion . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 3855 – 3864 . [ 196 ] Florian Weidner and Wolfgang Broll . 2019 . Interact with your car : a user - elicited gesture set to inform future in - car user interfaces . In Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia . 1 – 12 . [ 197 ] Adam S Williams , Jason Garcia , and Francisco Ortega . 2020 . Understanding Multimodal User Gesture and Speech Behavior for Object Manipulation in Augmented Reality Using Elicitation . IEEE Transactions on Visualization and CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Hosseini , et al . Computer Graphics 26 , 12 ( 2020 ) , 3479 – 3489 . [ 198 ] Julie R Williamson , Andrew Crossan , and Stephen Brewster . 2011 . Multimodal mobile interactions : usability studies in real world settings . In Proceedings of the 13th international conference on multimodal interfaces . 361 – 368 . [ 199 ] Jacob O Wobbrock , Htet Htet Aung , Brandon Rothrock , and Brad A Myers . 2005 . Maximizing the guessability of symbolic input . In CHI’05 extended abstracts on Human Factors in Computing Systems . 1869 – 1872 . [ 200 ] Jacob O Wobbrock , Meredith Ringel Morris , and Andrew D Wilson . 2009 . User - defned gestures for surface computing . In Proceedings of the SIGCHI conference on human factors in computing systems . 1083 – 1092 . [ 201 ] Haijun Xia , Michael Glueck , Michelle Annett , Michael Wang , and Daniel Wigdor . 2022 . Iteratively Designing Gesture Vocabularies : A Survey and Analysis of Best Practices in the HCI Literature . ACM Transactions on Computer - Human Interaction ( TOCHI ) 29 , 4 ( 2022 ) , 1 – 54 . [ 202 ] Yang Xiao , Junsong Yuan , and Daniel Thalmann . 2013 . Human - virtual human interaction by upper body gesture understanding . In Proceedings of the 19th ACM symposium on virtual reality software and technology . 133 – 142 . [ 203 ] Dan Xu , Yen - Lun Chen , Chuan Lin , Xin Kong , and Xinyu Wu . 2012 . Real - time dynamic gesture recognition system based on depth perception for robot navigation . In 2012 IEEE International Conference on Robotics and Biomimetics ( ROBIO ) . IEEE , 689 – 694 . [ 204 ] Li Xuan , Guan Daisong , Zhou Moli , Zhang Jingya , Liu Xingtong , and Li Siqi . 2019 . Comparison on user experience of mid - air gesture interaction and traditional remotes control . In Proceedings of the Seventh International Symposium of Chinese CHI . 16 – 22 . [ 205 ] Yukang Yan , Chun Yu , Xin Yi , and Yuanchun Shi . 2018 . Headgesture : Hands - free input approach leveraging head movements for hmd devices . Proceedings of the ACM on Interactive , Mobile , Wearable and Ubiquitous Technologies 2 , 4 ( 2018 ) , 1 – 23 . [ 206 ] Zhong Yang , Yi Li , Yang Zheng , Weidong Chen , and Xiaoxiang Zheng . 2012 . An interaction system using mixed hand gestures . In Proceedings of the 10th asia pacifc conference on Computer human interaction . 125 – 132 . [ 207 ] Gareth Young , Hamish Milne , Daniel Grifths , Elliot Padfeld , Robert Blenkin - sopp , and Orestis Georgiou . 2020 . Designing mid - air haptic gesture controlled user interfaces for cars . Proceedings of the ACM on Human - Computer Interaction 4 , EICS ( 2020 ) , 1 – 23 . [ 208 ] Ge Yu , Yan Wang , Taotao Fu , Ji Liang , and Lili Guo . 2016 . Visuo - Somatosensory Interaction for Space Telescience of Fluid Experiments . In 2016 3rd International Conference on Soft Computing & Machine Intelligence ( ISCMI ) . IEEE Computer Society , 196 – 200 . [ 209 ] Kai Zhang , Ying Zhai , Hon Wai Leong , and Shengming Wang . 2012 . An interac - tion educational computer game framework using hand gesture recognition . In Proceedings of the 4th international conference on internet multimedia computing and service . 219 – 222 . [ 210 ] Shun Zhang and Shizhou Zhang . 2019 . A novel human - 3DTV interaction system based on free hand gestures and a touch - based virtual interface . IEEE Access 7 ( 2019 ) , 165961 – 165973 . [ 211 ] Lijun Zhao , Xiaoyu Li , Zhenye Sun , Ke Wang , and Chenguang Yang . 2017 . A robot navigation method based on human - robot interaction for 3D environment mapping . In 2017 IEEE International Conference on Real - time Computing and Robotics ( RCAR ) . IEEE , 409 – 414 . A APPENDIX Table 7 : List of 3 most frequent gestures for each referent and their distribution between diferent domains . frequently < 25 % h . 25 % ≤ frequently < 50 % hr . 50 % ≤ frequently < 75 % uh x . frequently ≥ 75 % d a p t i v e S y s t e m E d u c a t i o n u t o m o t i v e A o ss - D o m a i n a n d A D i s p l a y d e l i n g cc e ss i b i l i t y a n d a n d e h i c l e ( P u b l i c ) M o V C r 3 D A e L e a r n i n g G a m i n g H R I I n - L a r g e uh hu x uh x rh uh M a n i p u l a t i o n / N a v i g a t i o n a n d E n t e r t a i n m e n t T e c h n o l o g y I n t e r a c t i o n o m s / A pp l i a n c e s d i c a l M o b i l e R o d i a M e M e S m a r t Swipe / Slide hu uh hr x x Translate ( 63 . 7 % of 135 gestures in 59 papers ) Grab and Move h h x hr hr hr hr hr hr ( 20 . 74 % of 135 gestures in 59 papers ) Point h h h uh uh ( 5 . 92 % of 135 gestures in 59 papers ) Turning Movement uh x x x hr x uh x x uh Rotate ( 71 . 9 % of 121 gestures in 56 papers ) Push and Pull h h hu h ( 12 . 4 % of 121 gestures in 56 papers ) Swipe / Slide h h hr h hr ( 10 . 74 % of 121 gestures in 56 papers ) Swipe / Slide hu hr hu uh uh x hu hu x Next ( 71 . 83 % of 71 gestures in 43 papers ) Rotate h hr h h ( 5 . 63 % of 71 gestures in 43 papers ) Tap h hu hr ( 5 . 63 % of 71 gestures in 43 papers ) Swipe / Slide hu x x hu hu x hu Increase ( 53 . 42 % of 73 gestures in 42 papers ) Turn Knob / Rotate h h uh h h ( 16 . 44 % of 73 gestures in 42 papers ) Move the Slider h h h h ( 10 . 96 % of 73 gestures in 42 papers ) Swipe / Slide hu x x uh uh x x Decrease ( 62 . 85 % of 70 gestures in 41 papers ) Turn Knob / Rotate h h x h h ( 14 . 28 % of 70 gestures in 41 papers ) Move / Grab the Slider h h h h ( 10 . 00 % of 70 gestures in 41 papers ) Swipe / Slide hu hr hu hu hu hu x Previous ( 69 . 86 % of 73 gestures in 41 papers ) Rotate h hr h h h ( 7 . 04 % of 73 gestures in 41 papers ) Tap h hr ( 4 . 22 % of 73 gestures in 41 papers ) Pull / Squish hu x x x x uh x hu hu x Zoom ( 67 . 82 % of 87 gestures in 39 papers ) Swipe / Slide h h ( 12 . 64 % of 87 gestures in 39 papers ) Move Closer / Move Away h h h h hr ( 12 . 64 % of 87 gestures in 39 papers ) Swipe / Slide hu x hr hu hr x x Pan / Scroll ( 69 . 35 % of 62 gestures in 31 papers ) Grab and Move h h x hr x ( 14 . 5 % of 62 gestures in 31 papers ) Rotate h h ( 6 . 45 % of 62 gestures in 31 papers ) Pull / Squish x x x x hu x x x Scale ( 88 . 31 % of 77 gestures in 27 papers ) Swipe / Slide h h ( 3 . 89 % of 77 gestures in 27 papers ) Move Closer / Move Away h uh h ( 3 . 89 % of 77 gestures in 27 papers ) Open Hand uh x x x x x h x x Deselect ( 61 . 9 % of 21 gestures in 16 papers ) Point h x h ( 4 . 5 % of 21 gestures in 16 papers ) Thumb Down h ( 4 . 5 % of 21 gestures in 16 papers ) Towards a Consensus Gesture Set CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Table 7 : Continued from previous page . d a p t i v e S y s t e m e A u t o m o t i v a n d E d u c a t i o n A D i s p l a y a n d a n d E n t e r t a i n m e n t c h n o l o g y o ss - D o m a i n d e l i n g cc e ss i b i l i t y e h i c l e ( P u b l i c ) e L e a r n i n g a n d T e I n t e r a c t i o n o m s / A pp l i a n c e s M o G a m i n g V d i a d i c a l R o C r 3 D A H R I I n - L a r g e M a n i p u l a t i o n / N a v i g a t i o n M e M e M o b i l e S m a r t Move or Slide Hand Around x x x x x x Move Cursor ( 77 . 78 % of 18 gestures in 12 papers ) Pinch / Grab and Move it h ( 11 . 11 % of 18 gestures in 12 papers ) Rotate Arm h h ( 5 . 55 % of 18 gestures in 12 papers ) Swipe / Slide hu x hu x Close Menu ( 66 . 67 % of 12 gestures in 8 papers ) Squish h hr ( 8 . 33 % of 12 gestures in 8 papers ) Wave h hr ( 8 . 33 % of 12 gestures in 8 papers ) Create Defning Area x x x ( 100 % of 30 gestures in 7 papers ) Cut A Scissor x x x x ( 100 % of 6 gestures in 6 papers ) Swipe / Slide hu x x Open Door ( 66 . 67 % of 6 gestures in 5 papers ) Open Hand hr ( 33 . 33 % of 6 gestures in 5 papers ) Slit / Knife hu uh x x Cut B ( 66 . 67 % of 9 gestures in 5 papers ) Point New Area h h ( 11 . 11 % of 9 gestures in 5 papers ) Karate h h ( 11 . 11 % of 9 gestures in 5 papers ) Move Apart Fingers hu x Maximize ( 60 . 0 % of 5 gestures in 4 papers ) Move One Hand Away from Other Hand h ( 20 . 0 % of 5 gestures in 4 papers ) Swipe / Slide h ( 20 . 0 % of 5 gestures in 4 papers ) Close Hand hu x Minimize ( 60 . 0 % of 5 gestures in 4 papers ) Swipe / Slide hr ( 40 . 0 % of 5 gestures in 4 papers ) Ungroup Move Hands Apart x x x x ( 100 % of 3 gestures in 3 papers ) Group Move Hands Together x x x x ( 100 % of 3 gestures in 3 papers ) Skip Swipe / Slide x x x ( 100 % of 3 gestures in 3 papers ) Make Frame hu x Take Picture ( 57 . 14 % of 7 gestures in 3 papers ) Show Palm hr x ( 28 . 57 % of 7 gestures in 3 papers ) Tap h h ( 14 . 28 % of 7 gestures in 3 papers ) CHI ’23 , April 23 – 28 , 2023 , Hamburg , Germany Hosseini , et al .