Mixed Initiative Interfaces for Learning Tasks : SMARTedit Talks Back (cid:3) Steven A . Wolfman Tessa Lau Pedro Domingos Daniel S . Weld Department of Computer Science & Engineering University of Washington , Box 352350 Seattle , Washington 98195 - 2350 fwolf , tlau , pedrod , weldg @ cs . washington . edu ABSTRACT Applications of machine learning can be viewed as teacher - student interactions in which the teacher provides train - ing examples and the student learns a generalization of the training examples . One such application of great interest to the IUI community is adaptive user interfaces . In the tra - ditional learning interface , the scope of teacher - student in - teractions consists solely of the teacher / user providing some numberof training examples to the student / learner and test - ing the learned model on new examples . Active learning approaches go one step beyond the traditional interaction model and allow the student to propose new training ex - amples that are then solved by the teacher . In this paper , we propose that interfaces for machine learning should even more closely resemble human teacher - student relationships . A teacher’s time and attention are precious resources . An intelligent student must proactively contribute to the learn - ing process , by reasoning about the quality of its knowl - edge , collaborating with the teacher , and suggesting new examples for her to solve . The paper describes a variety of rich interaction modes that enhance the learning process and presents a decision - theoretic framework , called DIAManD , for choosing the best interaction . We apply the framework to the SMARTedit programming by demonstration system and describe experimental validation and preliminary user feedback . Keywords Mixed initiative , machine learning applications , program - ming by demonstration 1 . INTRODUCTION Machine learning ( ML ) is widely used in areas such as wrapper induction , credit approval , image analysis , data mining , and intelligent user interfaces . A learning module (cid:3) To appear IUI’01 . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior speciﬁc permission and / or a fee . IUI’01 , January 14 - 17 , 2001 , Santa Fe , New Mexico . Copyright 2001 ACM 1 - 58113 - 325 - 1 / 01 / 0001 . . $ 5 . 00 can endow a system with the ability to extract valuable in - formation from substantial amounts of data or to adapt to new circumstances . However , many traditional applications of machine learning involve their users | the domain ex - perts who provide and label training examples | in only the most primitive ways . The systems operate on batches of prelabelled examples and neither provide nor request feed - back during the learning process . Indeed , the only \ commu - nication " between the user and the learning system besides the provision of this batch of examples is to test the system on yet another batch of examples . Yet , experience and research shows that great bene(cid:12)ts can be realized by machine learning applications with rich user interfaces . Work in active learning has shown that just al - lowing the system to pick which example to label next can greatly reduce the number of examples the user need con - sider [ 5 ] . Bauer et al . describe how the inclusion of a variety of interaction modes in a wrapper generation system can re - sult in more robust wrappers with less annoyance to the user [ 3 ] . Our previous work on the SMARTedit text - editing system [ 10 ] led us to the same conclusion : machine learning tasks can bene(cid:12)t from careful management of interactions with the user . The result is a system that requires fewer ex - amples to learn a concept , acquires domain knowledge with less user e(cid:11)ort , and is more appropriate for users unfamiliar with machine learning . 1 . 1 Vision We view the process of training a machine learning system as analogous to the interaction between a teacher and stu - dent . The teacher guides the student toward a concept by presenting and explaining examples . Although the teacher ultimately controls the interaction , the student also takes the initiative , directing the teacher’s attention toward areas of knowledge that are unclear . Interaction between a user and a machine learning system should mimic the interaction between a teacher and a stu - dent . The user / teacher demonstrates examples of a concept to the system / student . Traditional machine learning has (cid:12)lled the role of student by listening quietly . However , this strategy may waste valuable resources | the teacher’s time and e(cid:11)ort | by withholding information from her about the student’s level of understanding . We propose that ma - chine learning systems should instead follow the example of the proactive yet considerate student : asking questions , proposing examples and solutions , and relating its level of knowledge when appropriate to make the interaction more LearningSystem Learnercontext Attributes InteractionMode Library User Current InteractionMode Attribute vector for each mode SolvedExamples User actions Queries DIAManD Learnedmodel Figure 1 : Diagram of the DIAManD interaction manager in use in a machine learning application . Traditional ML systems have only the bottom three components with a (cid:12)xed interaction mode . DIA - ManD allows us to introduce a library of interaction modes and choose among them based on a set of attributes . The chosen interaction mode regulates the next stage of communication ( i . e . , interaction ) between the user and the learning system . e(cid:11)ective . Such a mixed initiative interface is uniquely appropriate to machine learning systems because the user is directly in - terested in helping the underlying system . Indeed , the user’s primary goal is for the system to re(cid:12)ne its knowledge to the point that it has learned the correct concept and can aid her in her task . Thus , the user interface has a prerogative to act to maximize the value of the user’s e(cid:11)orts to the learn - ing system . The interface’s guidance may take the form of informing the user of the learner’s internal state in a com - prehensible , domain - oriented manner , or of taking control of the discourse with the user to refocus her attention on new aspects of the learning process . 1 . 2 Overview In the rest of this paper , we explore the issues that arise in constructing mixed initiative user interfaces for machine learning systems . We describe a general framework for inter - action management in machine learning based on the com - mon structure of most machine learning tasks : examples , labels or solutions , predictions , and data sets . In particular , we describe : (cid:15) a comprehensive list of classes of interaction modes for machine learning systems , (cid:15) a set of attributes for measuring aspects of the utility of these interaction modes , (cid:15) DIAManD : the Decision - theoretic InterAction Manager for Discourse , a system for selecting among interaction modes using a multi - attribute utility function , (cid:15) a learning framework for adapting the weights of DIA - ManD’s utility function to individual users , along with experiments validating the framework , (cid:15) a full implementation of DIAManD in the SMARTedit programming by demonstration system for text - editing , (cid:15) and preliminary user feedback on the SMARTedit imple - mentation of DIAManD . Figure 1 shows the high - level architecture of our system . DIAManD selects from among a set of interaction modes the one it judges most appropriate based on their attribute vectors . The best of these modes is presented to the user and controls the next stage of discourse , updating the state of the learner . The modes are then rescored based on the new state of the learner . Our implementation of DIAManD in SMARTedit currently includes six interaction modes , (cid:12)ve attributes , the original SMARTedit learning system , and an adaptive utility function . In the next section , we motivate the subject of user inter - faces for machinelearning with a brief case studyof SMART - edit . We then present the top three blocks of Figure 1 in three sections : a list of classes of interaction modes , the DIA - ManD decision - theoretic framework for adaptively selecting among these modes , and an example set of attributes for use with DIAManD . The following section ( Section 6 ) describes our implementation of the framework in the SMARTedit programming by demonstration system and discusses some preliminary user feedback . Finally , we conclude with our contributions and sketch out future lines of research build - ing on this paper . 2 . CASE STUDY : SMARTEDIT Onemachine learning application that bene(cid:12)ts frommixed initiative discourse is programming by demonstration , in which the user demonstrates to the system how to perform a task . The system treats this demonstration as a partial execution trace of the program that solves the task , and generalizes from the trace to the original program . In prior work , we have formulated programming by de - monstration ( PBD ) as an inductive learning problem [ 10 , 11 ] . In our formulation , the learner is given the sequence of changes in the application state observed as the user per - forms some task on a concrete example . The learner then generalizes from these to a program that is capable of per - forming the task on new examples by learning the functions that transform from the initial application state to the de - sired (cid:12)nal state . The actual learning mechanism involves maintaining a compact representation of a set of candidate hypotheses which is pared during learning to include only those hypotheses consistent with the observed task . The user interface for PBD is similar to the regular macro recording interface : the user demonstrates the behavior of a program by recording how the program performs on a con - crete example . In contrast to regular macros , however , the PBD system doesn’t merely record a series of keystrokes . Instead , it generalizes from one or more demonstrations to a robust , executable program . After recording each demon - stration , the user can ask the system to execute its learned program on the next example . In e(cid:11)ect , the system has two interaction modes : recording the user’s solution of the next example , and solving the next example using its learned pro - gram . We have implemented this interface in the SMARTedit PBD system for text - editing . For example , a simple task for SMARTedit is to delete HTML comments from a text Figure 2 : Screenshot of the original SMARTedit PBD system . The task is to delete all HTML com - ments . The (cid:12)rst comment has already been deleted from just after the word \ sample " . The system now incorrectly predicts the user’s next action is to move after the string \ ample " rather than before \ < ! - - " . (cid:12)le . A user demonstrates this task by starting the macro recorder , moving the cursor to the next comment , selecting the commentwith the shift and cursor keys , and pressing the delete key to delete it ( Figure 2 ) . She then stops the macro recorder . After this demonstration , one of SMARTedit’s candidate programs is a program consisting of three actions of the form \ move to the next occurrence of < ! - - , select to the next occurrence of - - > , and delete the selection . " 2 . 1 Need for mixed - initiative interface When we presented the interface to users , however , several problems were revealed . One user performed ten examples before asking the system to predict an action when , in fact , the system had learned the task correctly after the (cid:12)rst two demonstrations . Another performed only one example be - fore asking for the system’s prediction . Because the system had not yet learned the right program , the user had to cycle through a large number of predicted actions before (cid:12)nding the correct one . A third user asked the system to begin pre - dicting actions and then wished to return to demonstrating examples , but the system did not support returning to the recording mode . These problems re(cid:13)ect on interactive machine learning ap - plications as a whole . SMARTedit is an instance of a ma - chine learning algorithm : it generalizes from solutions sup - plied by the user and learns a model explaining those solu - tions . The user interacts with the learning algorithm either by solving examples or by supervising the system’s perfor - mance on an example . However , these are only two points in the space of possible interaction modes between the user and the learner . The problems revealed in our user tests suggest that other interaction modes could have bene(cid:12)ts both in ac - quiring the concepts more quickly ( e . g . , through judicious choice of the example to classify , as in active learning ) and in allowing the user more control over the learning process . The next section describes our space of interaction modes in more detail . 3 . INTERACTION MODES FOR LEARNING We have formed a comprehensive list of e(cid:11)ective , gen - eral classes of interaction modes for machine learning . Each of these classes relies on only a few features common across learning domains and none assumes machine learning exper - tise on the part of the user . A supervised machine learning task includes distinct examples that the user solves , either by classifying them or by some more complex manipula - tion . Moreover , a possibly - ordered set of unsolved examples is either readily available or easily constructed . The learner searches through a space of hypotheses that explain the ob - served examples , and can execute one of these hypotheses on a new unsolved example to predict a solution . Although users are generally familiar with the application domain ( e . g . , text - editing ) , they may vary in their knowl - edge about the underlying machine learning system . While a machine learning expert might be able to peruse and correct the learner’s individual hypotheses , we assume that users are not machine learning experts . This assumption is espe - cially true in the case of adaptive user interfaces , where the users may not even be directly aware that they are using a machine learning system . Our list of interaction mode classes includes : 1 (cid:15) User solution : In our simplest class , the standard for ma - chine learning , the learning system observes an example and solution that the user provides and uses the result as input to the learning algorithm . SMARTedit uses an interaction mode in this class when the user provides solu - tions ( or partial solutions ) to examples by demonstrating her text - editing procedure on a section of text . (cid:15) Collaborative solution : The system executes a learned hy - pothesis on the next example and presents a solution to the user . The user either con(cid:12)rms the solution | in which case the example becomes training data for the system | or she asks for an alternate solution . The system can then propose an alternate solution to the user for acceptance or rejection , and so the process continues . For example , af - ter a user demonstrates how to delete the (cid:12)rst comment in Figure 2 , she asks the system to predict the next action . Because the system incorrectly predicts a move to the word \ ample " , she might then choose to cycle to the next most likely guess . Members of these (cid:12)rst two interaction classes were implemented in the original SMARTedit . (cid:15) System solution : In this class of interactions the learn - ing system assumes slightly more control , executing its learned concept on the next example ( or a part of it ) , demonstrating the solution to the user , and giving her a chance to reject that solution . If the user does not take this chance , the system assumes the solution was correct and carries on to the next example . The user need exert very little e(cid:11)ort in this type of interaction as long as the system’s learned concept is of high quality . Note that un - like in a collaborative solution mode , the user does not actually correct the system’s predictions but rather in - dicates only whether the prediction is correct . Recalling the example of Figure 2 , SMARTedit could run its learned program a step at a time ; if the program were correct and the user did not interrupt , SMARTedit would proceed to delete all the remaining HTML comments in the (cid:12)le . (cid:15) Performance : In a performance interaction mode , the learning system takes full control and executes its con - cept autonomously on new examples . If implemented in SMARTedit , for example , performance mode might be useful once the system has learned a program to delete HTML comments . It can then proceed to delete all the 1 As described in Section 6 , we have implemented represen - tatives from all these classes except the generation classes and user example selection . comments in all of the HTML (cid:12)les in a directory . More - over , this program might be applied to new (cid:12)les immedi - ately or at a future date . Even in a performance mode , there is the potential that the system can make further progress in learning by studying the distribution of new examples ; however , the system is not supervised by the user . (cid:15) System example selection : The system proposes to the user that she shift her attention to a particular example , rather than a user - selected example , a randomly - chosen example , or the next one in order . As related in the active learning literature , system - driven example selection can often provide greater learning bene(cid:12)t because the system can collect data on the most interesting examples (cid:12)rst . (cid:15) User example selection : The system requests that the user select or provide a promising new example to in - vestigate . Ideally , this request is accompanied with a high - level description of the kind of example that would be most valuable to operate on next . This class of in - teractions trades o(cid:11) increased control for the user with increased burden ( the onus of selecting the example ) . (cid:15) System example generation : In this class the system gen - erates a new example for the user to solve . In learning ap - plications with relatively unstructured and unrestricted input , the system may be able to generate arbitrary ex - amples . However , in more structured environments , mod - ifying existing examples may be more e(cid:11)ective than gen - erating entirely novel examples . (cid:15) User example generation : Here , the system asks the user to create a new example which can then be solved . This type of interaction is valuable if the system can specify to the user some quality of the new example which would help to discriminate among contending hypotheses . For example , SMARTedit might request an example in which an open HTML comment is unmatched by a closing tag . If the user provides the example , the system can clarify its learned concepts . If , on the other hand , the user indi - cates that such examples are impossible or irrelevant , the system has learned information about the distribution of examples . 4 . DECISION - THEORETIC FRAMEWORK The interaction modes described in the previous section form the core primitives of a rich and comprehensible dis - course with the user . However , we can’t assume that the human user will e(cid:11)ectively control the dialogue unassisted because she will likely (cid:12)nd the complete set of options be - wildering . This assumption would be especially hazardous since ( 1 ) we expect that many users will have no experience with machine learning algorithms , and ( 2 ) the choice of the most e(cid:11)ective interaction depends crucially on the state of the ML system . Instead , we adopt a mixed - initiative framework ( DIA - ManD ) in which the learner and human user are each par - ticipants in a dialogue aimed at improving the learner’s hy - pothesis with minimal e(cid:11)ort on the part of the user . The user and the learning system communicate through one of the various interaction modes . A set of attributes provides the means to discriminate among interaction modes ; each attribute can provide a numerical evaluation in the range [ (cid:0)1 ; 1 ] for a given interaction and state of the learning sys - tem . DIAManD scores the interactions using the attributes and , when appropriate , presents the best one to the user . The user may either proceed with this interaction or over - ride the system to choose a di(cid:11)erent one . DIAManD scores the interactions based on a decision - theoretic framework [ 17 ] ( i . e . , using a utility model ) which allows the learner to balance an interaction’s expected bur - den to the user against its estimated value to the task and learner . However , we believe that each human user will per - ceive the utility of a given interaction in a given situation di(cid:11)erently . Some users may prefer a learning system which is highly active , taking control of the discourse early at the possible expense of user corrections . Others would prefer a system that watches quietly until it is more certain that it understands the user’s concept . To account for these di(cid:11)er - ences and others , DIAManD adapts its utility function to the preferences of each individual user . To render the problem of adapting DIAManD’s utility function feasible in a reasonable number of interactions , we model the function by a linear combination of the weighted attributes . Thus , DIAManD models an individual user’s preferences as a set of weights on the available attributes . Given these weights and an interaction , the interaction’s score is the dot product of the weight vector and the vector of values of each attribute for the interaction . To use the DIAManD framework ( Figure 1 ) , an applica - tion designer provides a learning system , a set of modes for interacting with this system , and a set of attributes which measure important aspects of these interaction modes . The core DIAManD component then analyzes the available in - teraction modes based on its utility function and chooses the mode with the highest utility to present to the user . DIAManD also imposes two constraints on the user inter - face of a system it mediates . First , in order to support col - laborative selection of the appropriate interaction , the user of the system should be able to override the system’s choice of interaction mode and choose a mode that she prefers . In the SMARTedit DIAManD implementation , we accom - plish this by presenting the available interactions to the user as a list of radio buttons ( Figure 4 ) . Second , to facilitate rapid learning , the interface should provide some mechanism for feedback to DIAManD on particularly poor interaction mode choices . This requirement is satis(cid:12)ed by the \ Bad choice " button in Figure 4 . Given these interface capabilities , DIAManD must now codify the feedback from which it is to learn . We use a simple feedback model that would lend itself to a variety of actual learning algorithms . Each interaction receives a feedback value in the range [ (cid:0)1 ; 1 ] that should represent an update to its correlation with the user’s preferences : an undervalued \ good " interaction would receive positive feedback whereas an overvalued interaction would receive negative feedback . DIAManDgathers this feedback bytwo mechanismsbased on utility values . ( For the purposes of this discussion , let the utility of an interaction choice c be U ( c ) . ) First , any time the user overrides its choice of interaction mode , DIAManD takes this as implicit negative feedback on its choice ( cs ) and positive feedback on the user’s choice ( cu ) . In response , it generates a positive feedback value for the user’s choice equal to U ( cu ) (cid:0) U ( cs ) . DIAManD also generates negative feedback on its own choice based on the interaction with the second highest utility ( cs 0 ) equal to U ( cs 0 ) (cid:0) U ( cs ) . Sec - ond , when the user rebukes a choice made by DIAManD , the system generates a strong negative feedback value for 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 0 10 20 30 40 50 Steps E rr o r / S i m il a r i t y Similarity Error Figure 3 : Error and similarity metrics averaged across the (cid:12)rst 50 steps of 100 simulated DIAManD runs . There are (cid:12)fteen attributes and eight choices in the domain . Con(cid:12)dence bars are set to 99 % as - suming a two - tailed Gaussian distribution of results . that choice equal U ( cs 0 ) (cid:0)U ( cs ) (cid:0)(cid:26) where (cid:26) is a parameter to the learning system . Our learning mechanism for DIAManD responds to user feedback using an online reinforcement learning technique based loosely on the one described by Auer [ 2 ] . However , DIAManD performs no exploration | that is , it does not try apparently inferior interaction modes in order to solicit feedback on these choices . 2 Also , DIAManD allows feed - back on more than one interaction at a time . The algorithm adjusts its weight vector by adding to it a scaled version of each interaction’s attribute vectors . Each attribute vector is scaled by its interaction’s feedback value and a uniform fac - tor controlling the learning rate ( as in gradient descent [ 15 ] ) . Figure 3 shows the action of this learning system in an arti(cid:12)cial domain . We have assumed that the domain has (cid:12)fteen attributes and eight interaction modes ( these num - bers approximate those we hope to eventually achieve in SMARTedit ) . We model the user’s preferences as a set of weights on the attributes ; these are not revealed to DIA - ManD . We generate a random vector ( with one entry for each attribute ) by selecting values in the range [ (cid:0)1 ; 1 ] for each element of the vector . For each run , a \ user " is cre - ated by generating a random weight vector and scaling its length to 1 : 0 . DIAManD’s initial weight vector is also a random vector . At each step , a random attribute vector is constructed for each interaction . Based on their respective weight vectors , DIAManD and the user model each pick the best ( i . e . , highest utility ) interaction . If these choices di(cid:11)er , feedback is generated as if the user overrode DIAManD’s choice in favor of her own . Moreover , if the user’s utility for DIAManD’s choice is less than 0 , she issues a rebuke ( i . e . , presses the \ Bad choice " button ) . Finally , DIAManD updates its own model based on this feedback . 2 An earlier version of DIAManD that did explore the space of interaction modes in this manner frustrated users by mak - ing the \ wrong " choices for no apparent reason . We measured two values ( both averaged across 100 runs ) at each step . The error at a step is the di(cid:11)erence between the user’s utility value for DIAManD’s interaction choice and the user’s utility for her own choice . The similarity between DIAManD’s weight vector and the user’s vector is the cosine of the angle between the vectors ( the dot product of the two divided by their lengths ) . This value approaches 1 : 0 as the vectors become more and more similar . Figure 3 shows that as long as the user’s preferences really can be represented as a linear combination of the attributes , DIAManD will quickly learn the user’s preferences . In this scenario , DIAManD’s error is below 0 . 2 after only 18 interac - tions with the user | about one or two typical SMARTedit tasks . We also tested variations on the number of attributes and interaction modes . The results indicate that more interac - tions make learning faster while more attributes slow learn - ing ; however , even with (cid:12)fty attributes and only (cid:12)ve choices , DIAManD’s error level still drops well below 0 . 2 and its sim - ilarity level rises above 0 . 75 within 50 steps . Finally , starting DIAManD with a zero weight vector ( rather than a random vector ) makes little di(cid:11)erence across all settings after only a few steps . 5 . DIAManD ATTRIBUTE SET The attribute set used in a DIAManD system de(cid:12)nes the space of user adaptations available to that system . DIA - ManD can adapt to a user only if that particular user’s preferences are expressible in the \ basis set " de(cid:12)ned by these attributes . Although the best attribute set for a speci(cid:12)c DI - AManD implementation will depend somewhat on both the learning system and interaction set used , the vital consid - eration that any attribute set must re(cid:13)ect is the balance between user e(cid:11)ort and value to the task and system . 3 . For the SMARTedit implementation of DIAManD , we chose (cid:12)ve appropriate but general attributes ; each of these should be viable for most learning system and interaction library combinations . The (cid:12)rst three ( user input , level of continuity , and probability of correction ) focus on user ef - fort and represent physical and mental e(cid:11)ort required from the user . The latter two ( task progress and value to the system ) focus on achievement of the user’s objective . These measures re(cid:13)ect the typical objective of the user of a ma - chine learning system : complete the task by re(cid:12)ning the hypothesis of the learning system until it correctly describes the data . In the remainder of this section , we describe the (cid:12)ve attributes used in our implementation . (cid:15) The user input attribute measures the diÆculty of com - pleting the interaction . For some interactions it may be possible to calculate the expected amount of input ( e . g . , keystrokes and mouse clicks ) necessary to complete the interaction . However , in the case of SMARTedit , we take a simple approach and hand - craft a function for each in - teraction which returns a rating between 0 and 1 . The rating re(cid:13)ects the calculated possible courses of the inter - action and our subjective estimate of the amount of input required for these potential courses . Note , however , that 3 Ideally , attributes measuring user e(cid:11)ort will draw on ger - mane work in human - computer interaction such as task analysis [ 7 ] However , the initial work described here uses simple heuristics for these attributes . any more complex or realistic scheme could be applied if its results could be mapped to the range [ (cid:0)1 ; 1 ] . (cid:15) The level of continuity between a new interaction and those preceding it measures the additional burden of some interactions on the user imposed by the cognitive context - switch required to change interaction modes . For exam - ple , if DIAManD moves the discourse to a performance mode directly from a user - solution mode , the sudden shift in initiative might confuse to the user . On the other hand , moving from collaborative - solution mode to performance mode will be less confusing because the change in ini - tiative is not as great . In our SMARTedit DIAManD implementation we create a matrix of values between 0 and 1 that re(cid:13)ect our subjective estimation of the level of continuity between each pair of interactions . We track the most recently completed interaction mode and assign a value for this attribute based on the matrix entry for the last mode and the mode currently under consideration . (cid:15) The probability of correction is the probability that the user will be forced to correct an error by the system . This measure is used to re(cid:13)ect the amount of attention the user will be forced to pay to the system during an interaction . Typically , this attribute can be estimated using the learner’s own con(cid:12)dence in its hypotheses . In the case of SMARTedit , we measure the value directly using its learning systems probability framework . Each interaction is simulated to determine the probability that the user will be forced to correct the system’s prediction . (cid:15) The task progress achieved during an interaction is a mea - sure of how much of the user’s task is completed over the course of that interaction . In SMARTedit , we measure this value as the expected numberof program actions that will be completed over the course of the interaction . This measure is mapped to the range [ (cid:0)1 ; 1 ] by establishing an upper bound on the reported value and scaling . (cid:15) The value to the system of an interaction is the amount of improvement to or increase in con(cid:12)dence of the system’s hypothesis space . To measure this change in quality of the hypothesis space , we note that a space with fewer distinct hypotheses is less \ ambiguous " and more likely to favor the correct concept . In SMARTedit , we quantify this ambiguity by the entropy of hypothesis space : the amount of information the system would gain were it to ask the user : \ Which of these hypotheses is the correct one ? " 4 Note that each possible interaction mode implicitly asks a limited version of this broad question | by asking the user explicitly , displaying a proposed solution , or some other mechanism . So , each interaction divides the space of programs into disjoint subsets and discriminates among these subsets . For instance , recording an example divides the learning system’s hypothesis space into the distinct subsets which predict each possible recording trace . Even before the demonstration , we can calculate these subsets . After the demonstration , we can point to the subset which explains the user’s actions . Given this model , we can view the expected change in quality of the hypothesis space as the information gain of the interaction , IG : 4 For an introduction to entropy and information gain , see Chapter 3 of [ 15 ] . Figure 4 : Screenshot of SMARTedit enhanced with DIAManD . The task , as in Figure 2 , is to delete all HTML comments , and the (cid:12)rst one has been deleted . The interactions are lined up on the left of the screen , and their scores are represented by the darkness of their fonts and the bar gauges to their right . The system has just selected the \ Record one step " interaction , asking the user to demonstrate the next move . DIAManD selected this interaction over \ Step " ( the third down ) because of the high probability of error in the \ Step " interaction . IG = (cid:0) X o2O ( X c2o P ( c ) ) log ( X c2o P ( c ) ) Each o is a set of concepts ( drawn from the universe O ) which predict indistinguishable solutions on the current example , and P ( c ) is the probability of concept c . As with the task progress , this measure is mapped to the range [ (cid:0)1 ; 1 ] by truncating and scaling the range of entropy values . 5 6 . IMPLEMENTATION We have implemented our framework in the context of the SMARTedit system . SMARTedit previously supported only two modes of interaction | one user solution and one collaborative solution | and had no intelligent interaction management . It was the user’s responsibility to select the interaction mode and she made this choice without guid - ance . Moreover , it was impossible to switch back to the user solution mode from collaborative solution mode . In order to introduce DIAManD’s collaborative frame - work into SMARTedit , we followed the steps described in Section 4 : provide a learning system , an interaction library , and an attribute set , and include in the interface the capac - ity to switch between interactions and rebuke a poor inter - action choice . Our learning system is the SMARTedit learning engine , based on Version Space Algebra [ 10 ] . Our (cid:12)ve attributes are described in the Section 5 . We also constructed a new library of six interaction modes . To create the interaction library , we cleanly separated two interactions from SMARTedit’s recording mode : one asks the user to record an entire example while the other asks 5 This measurement is also used to compare the utility of demonstrating on di(cid:11)erent examples for the system selection interaction mode . the user to record just a single action ( \ Record one full ex - ample " and \ Record one step " in Figure 4 ) . Each of these (cid:12)ts into the user solution class from Section 3 . SMART - edit’s ability to present a guess at the next action became \ Step " , a collaborative solution mode . We also introduced three entirely new interaction modes : (cid:15) \ Run to end of example " successively presents the sys - tem’s guesses for each step up to the end of the current example . After a brief pause to allow the user to in - terrupt , the system commits the guess . This interaction mode is from the system solution category . (cid:15) The \ Run while sure " interaction immediately executes the learned program step by step until the system’s con - (cid:12)dence in the program at any step drops below a thresh - old ( currently 99 . 9 % ) . This interaction is a performance mode . (cid:15) Finally , we introduced system example selection in the form of the \ Jump and record an example " mode . In this mode , the system repositions the cursor to just before an example that is particularly confusing to the system and asks the user to demonstrate that example . SMARTedit’s user interface was altered to display the interaction choices as a set of radio buttons . DIAManD’s scores for the interactions are displayed by the contrast of the font and a horizontal gauge to the right of each interac - tion ( as shown in Figure 4 ) . Finally , the \ Bad choice " but - ton was added to allow users to rebuke DIAManD’s choice . The result of this implementation is a system capable of intelligently recommending which interaction to perform next . The user interface problems motivated in our case study are each addressed by the interaction manager . Users can tell when to stop recording examples because the system lowers the ranking of user solution mode interactions and in - stead recommends collaborative or system solution modes . Moreover , if a user does enter collaborative solution mode ( or system solution mode ) too early , she can now switch back to solving examples herself using the interaction manager . We have not yet performed a formal user study focusing on DIAManD . However , we have received informal feedback on the SMARTedit implementation of DIAManD from col - laborators and students in our department . Feedback from these users has motivated the current design of the user in - terface . For example , one user used SMARTedit to perform a task and felt he had trained the system correctly after a pair of examples . However , when he then had SMARTedit run in system solution mode 6 it made a mistake on one ir - regular example which went unnoticed . When he performed this task again with DIAManD , the interaction manager cor - rectly recommended recording that irregular example , but the user failed to notice the change in recommendations and initiated the \ Run " interaction anyway . To avoid this prob - lem , weights are now represented in a format which admits easy comparison between options ( the shaded \ gauges " in Figure 4 ) and also by the attention - grabbing fading of fonts . A similar process led to a new \ User control " interaction in which the system declines to select an interaction au - tomatically and asks the user to take the initiative . This 6 The \ Run " or system solution mode was so useful for DIA - ManD that we also introduced it into the plain SMARTedit system . interaction always receives a (cid:12)xed utility rating . \ User con - trol " serves two purposes . First , it maintains the credibility of the system : DIAManD takes control and recommends an interaction only when it actually has a good one to propose . Second , it provides a \ bar " toward which the user’s choice is raised by the feedback mechanism . The e(cid:11)ect of this ex - tra positive feedback balances the highly negative feedback provided by the \ Bad choice " button . The qualitative feedback from users has so far been mixed . Some users are aggravated by the intrusion of DIAManD into control of the discourse . Others are happy to receive guidance and assistance . We believe that the existing DI - AManD mechanism , with appropriate attributes measuring user tolerance of interruptions , can eventually accommodate both styles of users . 7 . RELATED WORK Several other lines of research have approached the prob - lem of designing interfaces for machine learning in di(cid:11)erent ways | either by developing methodologies for learning in - terfaces or by creating interfaces for speci(cid:12)c learning appli - cations . Work in active learning [ 1 , 5 ] focuses on the \ value to the system " attribute , ignoring the burden of interactions on the user . This focus requires the assumption that all in - teractions pose the same burden to the user , an assumption that may be reasonable in the face of a single mode of inter - action . However , our work introduces numerous interaction modes , and distinguishing among these requires a richer set of attributes . Boicu et al . [ 4 ] describe a framework for enabling domain experts to construct intelligent agents . Their framework se - lects among interactions to guide the user during exploration of a particular example . However , they do not address the problem of proactively selecting and moving among exam - ples , nor are their interactions or interaction manager de - signed to generalize beyond their speci(cid:12)c learning system . Bauer and Dengler [ 3 ] describe a PBD system for wrapper induction . Their system performs heuristic search through the space of wrappers | programs that extract data from web pages . They describe a utility function speci(cid:12)c to wrap - per induction that can help users choose how to re(cid:12)ne a wrapper . They introduce an attribute ( the number of ques - tions asked of the user ) to help determine whether to bother the user with another interaction or use the current wrapper . Previous work on adaptive user interfaces provided some mechanisms for collaborating to re(cid:12)ne the learner’s concept . Peridot [ 16 ] and Metamouse [ 14 ] are early PBD systems that request guidance from the user when generalizing actions . The \ mail clerk " agent [ 13 ] learns by observing the user , from explicit feedback , and by being trained . In addition , it compares its con(cid:12)dence against two user - set thresholds ( \ tell - me " and \ do - it " ) to decide whether to initiate action , make a suggestion , or remain quiet . None of these systems describes a general interface for machine learning , and none supports all the interaction modes presented herein . Our work is partly inspired by recent work on mixed - initiative planning [ 8 , 12 ] . For example , the TRAINS - 95 system [ 9 ] describes a robust , multi - modal interface that treats the AI planner and the user as equal participants in the problem solving dialogue . 8 . CONCLUSIONS AND FUTURE WORK Machine learning applications are a fertile area for re - search into intelligent user interfaces . Users who are experts in their own domain are thrown together with complex sys - tems whose internal functioning may be arcane to them , yet their goal in their interactions is to assist the system’s learn - ing process . Our paper makes the following contributions : (cid:15) We advocate a mixed - initiative interface in which the ma - chine learner and human user more equally share respon - sibility for guiding the learning process . (cid:15) We de(cid:12)ne a comprehensive set of eight interaction modes which capture a wide variety of interaction types . (cid:15) We present a decision - theoretic framework in which the learner repeatedly chooses the interaction modethat max - imizes expected utility . (cid:15) We de(cid:12)ne a multi - attribute utility function that balances direct user e(cid:11)ort , cognitive load , and the cost of cor - recting computer - introduced errors , against the potential progress of the user and gain in the learner’s \ understand - ing . " (cid:15) We describe our initial implementation of DIAManD in the context of the SMARTedit system for programming by demonstration . There are a plethora of interesting directions for future work stemmingfrom this paper . This interface model should be thoroughly tested through user studies . Applying the DIAManD framework to other domains will reveal new and challenging problems . Graphical programming by demon - stration systems such as Eager [ 6 ] or Peridot [ 16 ] seem a natural next step , allowing us to explore collaborative ex - ample selection in a less constrained environment than the naturally sequential text domain . The DIAManD frame - work may also be applicable to choosing interaction modes in a general user interface context rather than solely for the interfaces of machine learning systems . Our current ongoing research involves introducing novel interaction modes ( such as automatic example generation ) and more realistic and ef - fective attributes into DIAManD . Finally , di(cid:11)erent learning algorithms and more complex , non - linear utility functions might improve the eÆcacy of DIAManD’s core learning com - ponent . 9 . ACKNOWLEDGMENTS We thank people who provided code , help , and discus - sion : Corin Anderson , Peter Auer , Jim Guerber , Geo(cid:11) Hul - ten , Zachary Ives , Dutch Meyer , Denise Pinnel , Rachel Pot - tinger , and Kelly Shaw . This research was funded in part by the OÆce of Naval Research Grant N00014 - 98 - 1 - 0147 , National Science Foundation Grants IRI - 9303461 and IIS - 9872128 , the ARCS Foundation Barbara and Thomas Ca - ble Fellowship , a National Science Foundation Graduate Fel - lowship , and a Microsoft Fellowship , and an NSF CAREER Award . 10 . REFERENCES [ 1 ] D . Angluin . Queries and concept learning . Machine Learning , 2 : 319 { 42 , 1987 . [ 2 ] Peter Auer . An improved on - line algorithm for learning linear evaluation functions . In Proceedings of the Thirteenth Annual Conf . on Computational Learning Theory . Morgan Kaufmann , June 2000 . [ 3 ] M . Bauer , D . Dengler , and G . Paul . Instructible information agents for web mining . In Proceedings of the 2000 Conf . on Intelligent User Interfaces , January 2000 . [ 4 ] M . Boicu , G . Tecuci , D . Marcu , M . Bowman , P . Shyr , F . Ciucu , and C . Levcovici . Disciple - coa : From agent programming to agent teaching . In Proceedings of the Seventeenth Int’l . Conf . on Machine Learning , June 2000 . [ 5 ] David Cohn , Les Atlas , and Richard Ladner . Improving generalization with active learning . Machine Learning , 15 ( 2 ) : 201 { 221 , 1994 . [ 6 ] Allen Cypher . Eager : Programming repetitive tasks by demonstration . In Allen Cypher , editor , Watch What I Do : Programming by Demonstration , pages 205 { 217 . MIT Press , Cambridge , MA , 1993 . [ 7 ] D . Diaper , editor . Task Analysis for Human { Computer Interaction . Ellis Horwood , 1989 . [ 8 ] G . Ferguson and J . Allen . Arguing about plans : Plan representation and reasoning in mixed - initiative planning . In Proceedings of the Second Int’l . Conf . on Arti(cid:12)cial Intelligence Planning Systems , pages 43 { 48 . Menlo Park , Calif . : AAAI Press , 1994 . [ 9 ] G . Ferguson , J . Allen , and B . Miller . TRAINS - 95 : Towards a mixed - initiative planning assistant . In Proceedings of the Third Int’l . Conf . on Arti(cid:12)cial Intelligence Planning Systems , pages 70 { 77 , Edinburgh , Scotland , May 1996 . Menlo Park , Calif . : AAAI Press . [ 10 ] Tessa Lau , Pedro Domingos , and Daniel S . Weld . Version space algebra and its application to programming by demonstration . In Proceedings of the Seventeenth Int’l . Conf . on Machine Learning , pages 527 { 534 , June 2000 . [ 11 ] Tessa Lau and Daniel S . Weld . Programming by Demonstration : an Inductive Learning Formulation . In Proceedings of the 1999 Int’l . Conf . on Intelligent User Interfaces , pages 145 { 152 , Redondo Beach , CA , USA , January 1999 . [ 12 ] N . Lesh , C . Rich , and C . Sidner . Using plan recognition in human - computer collaboration . In Proceedings of the Seventh Int’l . Conf . on User Modelling , Ban(cid:11) , Canada , July 1999 . [ 13 ] Pattie Maes and Robyn Kozierok . Learning interface agents . In Proceedings of the Fourteenth National Conf . on Arti(cid:12)cial Intelligence , pages 459 { 465 , 1993 . [ 14 ] D . Maulsby and I . Witten . Metamouse : An Instructible Agent for Programming by Demonstration . In Allen Cypher , editor , Watch What I Do : Programming by Demonstration , pages 154 { 181 . MIT Press , Cambridge , MA , 1993 . [ 15 ] T . Mitchell . Machine Learning . McGraw Hill , 1997 . [ 16 ] Brad A . Myers . Peridot : Creating User Interfaces by Demonstration . In Allen Cypher , editor , Watch What I Do : Programming by Demonstration , pages 125 { 153 . MIT Press , Cambridge , MA , 1993 . [ 17 ] Howard Rai(cid:11)a . Decision Analysis : Introductory Lectures on Choices Under Uncertainty . Addison - Wesley , 1968 .