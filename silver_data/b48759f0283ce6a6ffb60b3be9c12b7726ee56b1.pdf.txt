a r X i v : c ond - m a t / 0505107v1 [ c ond - m a t . s t a t - m ec h ] 4 M a y 2005 A step beyond Tsallis and R´enyi entropies Marco Masi ∗ Dipartimento di Fisica G . Galilei , Padova , Italy . Abstract Tsallis and R´enyi entropy measures are two possible diﬀerent generalizations of the Boltzmann - Gibbs entropy ( or Shannon’s information ) but are not generalizations of each others . It is however the Sharma - Mittal measure , which was already de - ﬁned in 1975 ( B . D . Sharma , D . P . Mittal , J . Math . Sci 10 , 28 ) and which received attention only recently as an application in statistical mechanics ( T . D . Frank & A . Daﬀertshofer , Physica A 285 , 351 & T . D . Frank , A . R . Plastino , Eur . Phys . J . , B 30 , 543 - 549 ) that provides one possible uniﬁcation . We will show how this generalization that uniﬁes R´enyi and Tsallis entropy in a coherent picture natu - rally comes into being if the q - formalism of generalized logarithm and exponential functions is used , how together with Sharma - Mittal’s measure another possible ex - tension emerges which however does not obey a pseudo - additive law and lacks of other properties relevant for a generalized thermostatistics , and how the relation between all these information measures is best understood when described in terms of a particular logarithmic Kolmogorov - Nagumo average . Key words : Generalized information entropy measures , Tsallis , R´enyi , Sharma - Mittal PACS : 05 . 70 , 65 . 50 , 89 . 70 , 05 . 70 . L ∗ Corresponding author . Email address : marco . masi @ spiro . fisica . unipd . it , marco masi2 @ tin . it ( Marco Masi ) . Preprint submitted to Elsevier Science 23 March 2022 1 Introduction To gain a uniﬁed understanding of the diﬀerent entropy measures and how they relate to each others in the frame of a generalized picture , it is ﬁrst necessary to recall what characterizes ”classical” entropies and emphasize some aspects which are important for the present paper . 1 . 1 The Boltzmann - Gibbs entropy and Shannon’s information measure As it is well known , given a probability distribution P = { p i } , ( i = 1 , . . . , N ) , with p i representing the probability of the system to be in the i - th microstate , the Boltzmann - Gibbs ( BG ) entropy reads S BG ( P ) = − k N X i = 1 p i log p i , where k is the Boltzmann constant and N the total number of possible con - ﬁgurations . If all states are equi - probable it leads to the famous Boltzmann principle S = k log W ( N = W ) . BG entropy is equivalent to Shannon’s expres - sion if we set k = 1 ( as we will do from now on ) and use the immaterial base b for the logarithm function S S ( P ) = − N X i = 1 p i log b p i . It is common to use the natural base for the BG entropy , while base 2 has the advantage to deliver information quantities in bits . What characterizes BG and Shannon’s measure is additivity of information . Given two systems , described by two independent probability distributions A and B ( i . e . P ( A ∩ B ) = P ( A ) P ( B ) ) , using an additive information measure means that S S ( A ∩ B ) = S S ( A ) + S S ( B | A ) , with S S ( B | A ) = X i p i ( A ) S S ( B | A = A i ) , being the conditional entropy . In this case we are talking about extensive sys - tems , i . e . systems where the entropy is given by the sum of all the entropies of their parts , as it is customary to do in standard statistical mechanics . The unique function which assures additivity is the logarithm . Also in the ax - iomatic derivation of Shannon’s entropy performed by A . I . Khinchin ( 6 ) , it is the additive property which leads to the appearance of the logarithm function . 2 This is the real reason that stands behind the ubiquitous presence of the log - arithm function in information theory , and we can conﬁdently say that every modiﬁcation to it reﬂects a deviation from the additive law . We will from now on use the natural base . Shannon’s entropy can be written in the form of a ”linear” ( the arithmetic ) mean as S S ( P ) = h I i i lin = * log 1 p i ! + lin , ( 1 . 1 ) where we will call the quantity I i = log 1 p i ! , the elementary information gain associated to an event of probability p i ( in information theory it is sometimes called the code length ) . The quantity 1 p i is also called the surprise ( less probable events are considered more ”surprising” than more probable ones ) , and we will see that it is this quantity which is really measured in one way or another , not − log p i . 1 . 2 Tsallis’ entropy Additivity is however not always preserved , especially in nonlinear complex systems , e . g . when we have to deal with long range forces , as it is in the case of the dynamic evolution of star clusters or in systems with long range microscopic memory , in fractal - or multifractal - like and self - organized critical systems , etc . We are dealing in this case with non - extensive systems ; a case which received much attention in the last decade . ( 15 ) A generalization of the BG entropy to non - extensive systems is known as Tsallis entropy ( 14 ) . C . Tsallis noted that if non - extensivity enters into the play things are described better by power law distributions , p qi , so called q - probabilities , i . e . by scaled probabilities where q is a real parameter . This introduces the formal possibility not to set rare and common events on the same footing , as in BG or Shannon statistics , but it enhances or depresses them according to the parameter chosen ( in complex systems rare events can have dramatic eﬀects on the overall evolution ) . With the introduction of the normalized q - probabilities it became customary to deﬁne so called escort - or zooming - distribution π i ( P , q ) = p qi P Ni = 1 p qi ; q > 0 , q ∈ ℜ . 3 In this frame Tsallis postulated his now famous generalization of Shannon’s entropy to non - extensivity ( 14 ) : S T ( P , q ) = P Ni = 1 p qi − 1 1 − q = 1 q − 1 N X i = 1 p i ( 1 − p q − 1 i ) . ( 1 . 2 ) For q → 1 , Shannon’s measure is recovered , i . e . : S T ( P , 1 ) = S S ( P ) . Tsallis entropy extends to a pseudo - additive law S T ( A ∩ B ) = S T ( A ) + S T ( B | A ) + ( 1 − q ) S T ( A ) S T ( B | A ) , ( 1 . 3 ) with S T ( B | A ) = X i π i ( A ) S T ( B | A = A i ) . Let us introduce the generalized q - logarithm function log q x = x 1 − q − 1 1 − q , ( 1 . 4 ) which , for q = 1 , becomes again the common natural logarithm . Its inverse is the generalized q - exponential function e xq = [ 1 + ( 1 − q ) x ] 11 − q , ( 1 . 5 ) which becomes the exponential function for q = 1 . The importance of the q - logarithm is that it satisﬁes a pseudo - additive law log q xy = log q x + log q y + ( 1 − q ) ( log q x ) ( log q y ) . ( 1 . 6 ) Then Tsallis entropy 1 . 2 can be written as the q - deformed Shannon entropy S T ( P , q ) = − N X i = 1 p qi log q p i = N X i = 1 p i log q 1 p i ! = * log q 1 p i ! + lin = h I i i lin , ( 1 . 7 ) with the last term resulting as the q - extension of 1 . 1 . This reﬂects the non - extensive character of the system on the elementary information gains . Note also that the classical power laws and the additivity rules for the loga - rithm and exponential do no longer hold in this generalized context . Except for q = 1 , in general log q x α 6 = α log q x , which explains why we keep writing throughout this paper Shannon’s elementary information gain as D log (cid:16) 1 p i (cid:17)E lin instead of − h log p i i lin . Useful for our purposes will be the equality e x + y + ( 1 − q ) xy q = e xq e yq . ( 1 . 8 ) 4 We will see how the q - deformed formalism ﬁts naturally in the mathematical descriptions of generalized entropy measures . 1 . 3 R´enyi’s entropy Either in the case of BG as for Tsallis entropy , in 1 . 1 and 1 . 7 , an entropy measure is the average S obtained over many elementary information gains I i ≡ I i ( 1 p i ) = log q (cid:16) 1 p i (cid:17) associated to the i - th event of probability p i ( if the system is extensive , q = 1 ) . Another possible generalization exists and has become commonplace through - out the literature , namely R´enyi’s measure ( 12 ) . A . R´enyi maintained a still additive measure , as in BG entropy , but considered that another form of av - eraging is possible . His starting point was the generalized notion of average of A . N . Kolmogorov and M . Nagumo ( ( 7 ) , ( 10 ) ) , who independently showed that , in the frame of the Kolmogorov axioms of probability theory , the deﬁni - tion of the average must be extended to the quasi - arithmetic or quasi - linear mean deﬁned as S = f − 1 N X i = 1 p i f ( I i ) ! , ( 1 . 9 ) where f is a strictly monotone continuous and invertible function , the so called Kolmogorov - Nagumo function ( KN function ) . On his side , R´enyi showed that if we restrict to additive measures then only two possible KN functions exist . The ﬁrst one is the common arithmetic mean and is associated with the KN function f ( x ) = x , and the second is the exponential mean with f ( x ) = c 1 b ( 1 − q ) x + c 2 , ( 1 . 10 ) where q is a real parameter , and c 1 and c 2 are two arbitrary constants . The exponential mean leads to R´enyi’s information measure or R´enyi’s en - tropy S R ( P , q ) = 1 1 − q log b N X i = 1 p qi , ( 1 . 11 ) with b the logarithm base ( we will from now on assume the natural base , b = e , for R´enyi’s entropy either ) . For q → 1 R´enyi’s measure becomes Shannon’s entropy . It should be noted how P . Jizba and T . Arimitsu ( 5 ) showed that R´enyi’s measure can be obtained also extending the Shannon - Khinchin axioms to a quasi - linear conditional information S R ( B | A ) = f − 1 X i π i ( A ) f ( S R ( B | A i ) ) ! , ( 1 . 12 ) 5 with f as given in 1 . 10 . Therefore Shannon’s information measure is an averaged information in the ordinary sense , while R´enyi’s measure represents an exponential mean over the same elementary information gains log (cid:16) 1 p i (cid:17) . 2 The Sharma - Mittal and Supra - extensive entropy 2 . 1 Generalizing with Kolmogorov - Nagumo means It is important to understand that Tsallis and R´enyi entropies are two diﬀerent generalizations along two diﬀerent paths . Tsallis generalized to non - extensive systems , while R´enyi to quasi - linear means . But we can search for an entropy which generalizes to non - extensive sets and non - linear means containing Tsal - lis and R´enyi measures as limiting cases . Let us unify the picture of all the entropies considered here through KN av - erages ( as J . Naudts and M . Czachor did ( 11 ) , tough by a slightly diﬀerent approach ) . It is immediate to see from 1 . 7 and 1 . 9 how for Tsallis’s measure it is the KN function f ( x ) = x ( 2 . 1 ) which averages over the elementary information gain I i = log q 1 p i ! . This led us to write it as S T ( P , q ) = * log q 1 p i ! + lin . While , for R´enyi’s measure , choose in 1 . 10 , c 1 = 11 − q = − c 2 ( remember 1 . 4 ) , then the KN function takes the form f ( x ) = log q e x , ( 2 . 2 ) which , applied on I i = log 1 p i ! , 6 in 1 . 9 ( f − 1 ( x ) = log e xq ) leads us to rewrite 1 . 11 as S R ( P , q ) = * log 1 p i ! + exp , where , of course , h . i exp ≡ h I i i exp stands for the exponential mean deﬁned by the KN function 2 . 2 over the elementary information I i . But , what Tsallis and R´enyi measures have in common is that in both cases f ( I i ) = log q 1 p i ! . ( 2 . 3 ) Then , for a further generalization , the simplest step beyond them would be that to generalize 2 . 1 and 2 . 2 with f ( x ) = log q e xr ( 2 . 4 ) and set I i = log s 1 p i ! , where r , s are new parameters on the generalized exponential and logarithm functions . Maintaining constraint 2 . 3 implies s = r . Then calculating 1 . 9 ( f − 1 ( x ) = log r e xq ) , one obtains the Sharma - Mittal information measure ( 13 ) S SM ( P , { q , r } ) = log r e P i p i log q (cid:16) 1 pi (cid:17) q = ( 2 . 5 ) = * log r 1 p i ! + q − exp = = 1 1 − r   X i p qi ! 1 − r 1 − q − 1   , where h · i q − exp stands for an average deﬁned by the KN function 2 . 4 and that we will call the quasi - exponential mean . We can see that for r → 1 R´enyi’s measure , and for r → q Tsallis measure , are recovered as limiting cases . We will show in the next section that for two statistical independent systems A and B it is easy to check that S SM ( A ∩ B ) = S SM ( A ) + S SM ( B | A ) + ( 1 − r ) S SM ( A ) S SM ( B | A ) , 7 i . e . a pseudo - additive law holds as in the case of Tsallis entropy . Therefore Sharma - Mittal’s measure generalizes R´enyi’s extensive entropy to non - extensivity , characterized by the r - logarithm . It is the parameter r which determines the degree of non - extensivity , while q is the deformation param - eter of the probability distribution ( however , when r → q the two parame - ters become intertwined and in Tsallis entropy it is q which measures non - extensivity ) . On information theoretic grounds , B . D . Sharma and D . P . Mittal ( 13 ) , ad - vanced already in 1975 this non - additive measure which shows to have a non - extensive character either . But it wasn’t until recently ( ( 2 ) , ( 3 ) , and without mentioning it explicitly ( 11 ) ) that Sharma - Mittal’s measure has been investi - gated in statistical mechanics . 2 . 2 Generalizing with q - logarithms and q - exponentials At this point let us see how by using the q - deformed logarithm and expo - nential formalism , one could express in a much more compact form the same generalization path . First of all recall a well known relationship which exists between Tsallis and R´enyi entropies , namely S R ( P , q ) = 1 1 − q log [ 1 + ( 1 − q ) S T ( P , q ) ] . ( 2 . 6 ) Here we can eﬃciently exploit the generalized logarithm and exponential func - tions 1 . 4 and 1 . 5 , rewriting 2 . 6 in the more compact form S R ( P , q ) = log e S T ( P , q ) q , ( 2 . 7 ) from which follows immediately S T ( P , q ) = log q e S R ( P , q ) . ( 2 . 8 ) Looking at the structure of 2 . 7 and 2 . 8 we can ask if , given another parameter r , the following S SM ( P , { q , r } ) = log r e S T ( P , q ) q = 1 1 − r   X i p qi ! 1 − r 1 − q − 1   , ( 2 . 9 ) 8 and S SE ( P , { q , r } ) = log q e S R ( P , q ) r = h 1 + ( 1 − r ) ( 1 − q ) log P i p qi i 1 − q 1 − r − 1 1 − q , ( 2 . 10 ) might then be other possible generalizations ? 2 . 9 can be recognized imme - diately as Sharma - Mittal’s measure 2 . 5 and can be already accepted as an extension . 2 . 10 instead needs a closer look . For r → q it obviously boils down to R´enyi’s entropy . For r → 1 we obtain Tsallis’ measure again . So , from a formal point of view it can be regarded as another generalization too . It is however not entirely clear what kind of statistics it expresses . Its particular status might be best evidenced expressing all the measures in terms of ( logarithmic averaged ) surprise quantities . Indeed , notice that we can rewrite the quantity X i p qi ! 11 − q =  X i p i 1 p i ! 1 − q   11 − q = * 1 p i ! 1 − q + 11 − q lin = e D log q (cid:16) 1 pi (cid:17)E lin q = * 1 p i + log q , ( 2 . 11 ) where we used the logarithmic mean h· i log q deﬁned by the KN function f ( x ) = log q x . Then , from 1 . 7 and 1 . 11 , and using 2 . 11 , equations 2 . 7 to 2 . 10 can be rewritten as S R ( P , q ) = log * 1 p i + log q ; ( 2 . 12 ) S T ( P , q ) = log q * 1 p i + log q ; ( 2 . 13 ) S SM ( P , { q , r } ) = log r * 1 p i + log q ; ( 2 . 14 ) S SE ( P , { q , r } ) = log q e log D 1 pi E log q r . ( 2 . 15 ) With the q - deformed logarithm and exponential formalism we could easily see the generalization path to follow and write all the measures into a more compact form ( 2 . 7 to 2 . 10 ) . Moreover this makes it easier to recognize the behavior of the limits than in their explicit form ( the r . h . s . of 2 . 9 and 2 . 10 ) . With no or only few passages it is immediate to see how 2 . 9 reduces to Tsallis entropy for r → q , and for r → 1 it reduces to R´enyi’s entropy ( without any 9 need to apply Hopital rule , ﬁrst order approximations or whatever , insert 1 . 2 in 1 . 5 ) . The limit for q → 1 for Sharma - Mittal measure is lim q → 1 S SM = lim q → 1 log r e S T q = log r e S S = log r * 1 p i + log = = e − ( 1 − r ) P i p i log p i − 1 1 − r , which Frank and Daﬀertshofer ( 2 ) used to call the gaussian entropy . By the way lim q → 1 S SE = lim q → 1 log q e S R r = log e S S r = log e log D 1 pi E log r = = 1 1 − r log 1 − ( 1 − r ) X i p i log p i ! . But the point is that when compared with 2 . 12 , 2 . 13 , 2 . 14 , measure 2 . 15 seems to stand apart and does not correspond to some quasi - linear mean in the style of 1 . 9 . 2 . 3 Comparing the supra - extensive entropy with Sharma - Mittal’s entropy Let us then focus shortly on the separate nature of 2 . 15 ( or 2 . 10 ) and some of its properties . First of all note that it can be shown how for two statistical independent systems A and B , similarly to Tsallis’ entropy , the Sharma - Mittal’s entropy obeys a pseudo - additive law and can be decomposed as in 1 . 3 . It is almost immediate to see this by employing the generalized exponential formalism . Thanks to 1 . 3 , 1 . 6 , 1 . 8 , starting from the middle term of 2 . 9 we can write S SM ( A ∩ B ) = log r e S T ( A ∩ B ) q = = log r e q [ S T ( A ) + S T ( B | A ) + ( 1 − q ) S T ( A ) S T ( B | A ) ] = log r (cid:16) e S T ( A ) q e S T ( B | A ) q (cid:17) = = log r e S T ( A ) q + log r e S T ( B | A ) q + ( 1 − r ) log r e S T ( A ) q log r e S T ( B | A ) q = = S SM ( A ) + S SM ( B | A ) + ( 1 − r ) S SM ( A ) S SM ( B | A ) . ( 2 . 16 ) 10 Proceeding in the same manner with 2 . 10 leads however not to the same decomposition . Because of R´enyi’s measure additive character one can’t go further than S SE ( A ∩ B ) = log q e S R ( A ∩ B ) r = log q e r [ S R ( A ) + S R ( B | A ) ] with S R ( B | A ) as given in 1 . 12 . Entropy 2 . 15 therefore obeys a new form of non - extensivity , we call supra - extensivity . There are also other aspects which should be mentioned . Let us brieﬂy recall the notions of concavity and stability applied to entropy measures . Given two probability distributions P = { p 1 , . . . , p N } and P ′ = { p ′ 1 , . . . , p ′ N } and deﬁning an intermediate distribution P ′′ = { p ′′ 1 , . . . , p ′′ N } with p ′′ i ≡ µp i + ( 1 − µ ) p ′ i ; ∀ µ ∈ [ 0 , 1 ] , S ( P ) is said to be a concave entropic functional if and only if S ( P ′′ ) ≥ µS ( P ) + ( 1 − µ ) S ( P ′ ) . Otherwise , S ( P ) is said to be convex . Concavity implies thermodynamic stabil - ity ( e . g . thermal equilibrium between two initial temperatures in BG statistical mechanics ) . Recall also the notion of stability ( or experimental robustness , as Tsallis calls it , in order to avoid confusion with the previous form of thermodynamic sta - bility ) which implies that for arbitrary small variations of the probabilities p i a statistical functional remains ﬁnite . That is , given a deformation | | p − p ′ | | = X i | p i − p ′ i | , such that | | p − p ′ | | < δ ε , we obtain stability of S ( P ) if △ = (cid:12) (cid:12)(cid:12)(cid:12)(cid:12) S ( P ) − S ( P ′ ) S max (cid:12) (cid:12)(cid:12)(cid:12)(cid:12) < ε ; ∀ δ ε > 0 , ∀ ǫ > 0 , with S max the maximum value S can attain and for all microstates i = 1 , . . . , N . Lesche claims ( 8 ) that this is a necessary condition for an entropy measure to be a physical quantity and showed that , while BG entropy is always stable , R´enyi’s measure is unstable for all q 6 = 1 . It is also known that BG entropy is always concave , while R´enyis measure is concave only for q ≤ 1 , and can be either concave or convex for q > 1 . More recently , Abe ( 1 ) showed that Tsallis entropy is concave and stable for all positive values of q . It might 11 also be worth mentioning that a physical entropy is not only expected to be generically concave and Lesche - stable but should lead also to a ﬁnite entropy production per unit time . BG and Tsallis entropies share all these properties . R´enyi entropy shares none . ( 4 ) So , being an extension of it , it is clear that the properties of concavity and stability and ﬁnite entropy production per unit time are generically violated also in Sharma Mittal’s and the supra - extensive entropy . Finally , it should also be underlined how Frank and Plastino showed ( 3 ) that the Sharma - Mittal entropy is the only measure that allows for a pseudo - additive decomposition and at the same time gives rise to a thermostatistics based on escort mean energy values U = P i p qi ε i P i p q i , ( ε i are the energy levels ) admitting of a generalized partition function e Z de - ﬁned by log r e Z SM : = log r Z SM − βU with Z SM = X i p qi ! 11 − q = * 1 p i + log q , ( 2 . 17 ) ( Z SM is the partition function which takes U while e Z SM takes zero as the energy reference , and where β is an inverse temperature measure ) , that leads to the usual expressions for the free energy F = U − T S SM = − 1 β log r e Z SM and the mean energy U = − ∂ ∂β log r e Z SM . We saw that the new measure we considered here does not allow for a pseudo - additive decomposition like 2 . 16 , and therefore it is to expect that the partition function describing the free and mean energy cannot have the same structure e Z SM common to Sharma - Mittal entropy unless , as can be shown ( 9 ) applying the maximum entropy principle , one substitutes 2 . 17 with Z SE = e log D 1 pi E log q r . Summing up , the supra - extensive entropy , does no longer obey a pseudo - additive statistics , if based on escort mean values the partition function must take an intrinsically diﬀerent form than Sharma - Mittals one , but what they have in common with R´eny’s entropy is that , in general , they do not possess the property of concavity , Lesche - stability and ﬁnite entropy production per unit time . 12 3 Conclusion We showed how the Sharma - Mittal and a new generalized entropy measure both unify Tsallis and R´enyi entropies on two diﬀerent paths in a way that appears natural and almost immediate when we make use of the generalized q - logarithm an q - exponentials as in 2 . 7 , 2 . 8 , 2 . 9 and 2 . 10 . We underlined how the relationship among all measures becomes particularly clear using the logarithmic KN average 2 . 11 rewriting them as in 2 . 12 , 2 . 13 , 2 . 14 and 2 . 15 . This path naturally leads to the supra - extensive entropy which does not emerge from the KN means approach alone and does not conform to a pseudo - additive law , lacks of concavity , Lesche - stability and ﬁnite entropy production . However , because the new measure here proposed emerges so naturally as another possible extension of R´enyi and Tsallis entropy it is therefore worth of being mentioned . It is tempting to conclude that , while it might not have applications in a generalized thermostatistics , it nevertheless might be of some interest in the frame of information theory , cybernetics , control theory , etc . Finally we obtained a way of understanding all these entropy measures in a uniﬁed picture that can be summarized in the following table and diagram . 13 Entropy measure Explicit form KN - mean form KN log - mean form log q × exp q - form Supra - extensive [ 1 + ( 1 − r ) ( 1 − q ) log P i p qi ] 1 − q 1 − r − 1 1 − q log q e D log (cid:16) 1 pi (cid:17)E exp r log q e log D 1 pi E log q r log q e S R ( P , q ) r Sharma - Mittal 11 − r (cid:20) ( P i p qi ) 1 − r 1 − q − 1 (cid:21) D log r (cid:16) 1 p i (cid:17)E q − exp log r D 1 p i E log q log r e S T ( P , q ) q Tsallis P N i = 1 p qi − 1 1 − q D log q (cid:16) 1 p i (cid:17)E lin log q D 1 p i E log q log q e S R ( P , q ) R´enyi 11 − q log P Ni = 1 p qi D log (cid:16) 1 p i (cid:17)E exp log D 1 p i E log q log e S T ( P , q ) q BG - Shannon − P Ni = 1 p i log p i D log (cid:16) 1 p i (cid:17)E lin log D 1 p i E log log e S S ( P ) Sharma - Mittal log r (cid:28) 1 p i (cid:29) log q ❅❅❅ r → 1 ❅❅❅❘ ❍❍❍ r → q ❍❍❍❍❍❍❍❍❍❍❍❍❥ Supra - extensive log q e log D 1 pi E log q r (cid:0)(cid:0)(cid:0) r → 1 (cid:0)(cid:0)(cid:0)✠ ✟✟✟ r → q ✟✟✟✟✟✟✟✟✟✟✟✟✙ R´enyi log (cid:28) 1 p i (cid:29) log q ❅❅❅ q → 1 ❅❅❅❘ Tsallis log q (cid:28) 1 p i (cid:29) log q (cid:0)(cid:0)(cid:0) q → 1 (cid:0)(cid:0)(cid:0)✠ Shannon ( Boltzmann - Gibbs ) log (cid:28) 1 p i (cid:29) log 14 References [ 1 ] S . Abe , Stability of Tsallis entropy and instabilities of R´enyi and normal - ized Tsallis entropies : A basis for q - exponential distributions . Phys . Rev . E 66 , 046134 ( 2002 ) . [ 2 ] T . D . Frank , A . Daﬀertshofer , Exact time - dependant solutions of the R´enyi Fokker - Planck equation and the Fokker - Planck equations related to the en - tropies proposed by Sharma and Mittal , Physica A 285 , 351 ( 2000 ) . [ 3 ] T . D . Frank , A . R . Plastino , Generalized thermostatistics based on the Sharma - Mittal entropy and escort mean values , Eur . Phys . J . B 30 , 543 - 549 ( 2002 ) . [ 4 ] See , for instance , the Preface of Nonextensive Entropy - Interdisciplinary Applications . M . Gell - Mann and C . Tsallis , Oxford University Press , New York ( 2004 ) . [ 5 ] P . Jizba , T . Arimitsu , The world according to R´enyi : thermodynamics of multifractal systems , Annals of Physics , Volume 312 , Issue 1 , 17 - 59 ( July 2004 ) . [ 6 ] A . I . Khinchin , Mathematical Foundations of Information Theory , Dover , New York ( 1957 ) . [ 7 ] A . N . Kolmogorov Sur la notion de la moyenne , Atti Accad . Naz . Lincei Mem . Cl . Sci . Fis . Mat . Natur . Sez . ( 6 ) 12 , 388 - 391 ( 1930 ) . [ 8 ] B . Lesche , J . Stat . Phys . , 27 , 419 ( 1982 ) . [ 9 ] Author’s paper in preparation . [ 10 ] M . Nagumo , ¨Uber eine Klasse der Mittelwerte , Japan . J . Math . , 7 , 71 - 79 ( 1930 ) . [ 11 ] J . Naudts , M . Czachor Generalized thermostatistics and Kolmogorov - Nagumo averages , arXiv : cond - mat / 0110077 ( 3 Oct 2001 ) . [ 12 ] A . R´enyi , Probability theory , North Holland , Amsterdam ( 1970 ) ; Selected Papers of Alfred R´enyi , Vol . 2 Akademia Kiado , Budapest ( 1976 ) . [ 13 ] B . D . Sharma and D . P . Mittal , J . Math . Sci . 10 , 28 ( 1975 ) . See also New Nonadditive Measures of Relative Information , J . Comb . Inform . and Syst . Sci . , 2 , 122 - 133 ( 1977 ) . [ 14 ] C . Tsallis , Possible Generalization of Boltzmann - Gibbs Statistics , J . Stat . Phys . 52 , 479 ( 1988 ) . [ 15 ] For a general bibliography about nonextensive thermodynamics and up - dates see : http : / / tsallis . cat . cbpf . br / biblio . htm 15