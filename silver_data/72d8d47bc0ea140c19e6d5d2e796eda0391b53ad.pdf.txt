“A cold , technical decision - maker” : Can AI provide explainability , negotiability , and humanity ? ALLISON WOODRUFF , Google YASMIN ASARE ANDERSON , Ipsos KATHERINE JAMESON ARMSTRONG , Ipsos MARINA GKIZA , Ipsos JAY JENNINGS , PixelInsight CHRISTOPHER MOESSNER , Ipsos FERNANDA VIÉGAS , Google MARTIN WATTENBERG , Google LYNETTE WEBB , Google FABIAN WREDE , Google PATRICK GAGE KELLEY , Google Algorithmic systems are increasingly deployed to make decisions in many areas of people’s lives . The shift from human to algorithmic decision - making has been accompanied by concern about potentially opaque decisions that are not aligned with social values , as well as proposed remedies such as explainability . We present results of a qualitative study of algorithmic decision - making , comprised of five workshops conducted with a total of 60 participants in Finland , Germany , the United Kingdom , and the United States . We invited participants to reason about decision - making qualities such as explainability and accuracy in a variety of domains . Participants viewed AI as a decision - maker that follows rigid criteria and performs mechanical tasks well , but is largely incapable of subjective or morally complex judgments . We discuss participants’ consideration of humanity in decision - making , and introduce the concept of ‘negotiability’ , the ability to go beyond formal criteria and work flexibly around the system . CCS Concepts : • Social and professional topics → Computing / technology policy ; • Human - centered computing → Human computer interaction ( HCI ) ; • Computing methodologies → Artificial intelligence . Additional Key Words and Phrases : accountability , algorithmic decision - making , artificial intelligence , explainability , interpretability ACM Reference Format : Allison Woodruff , Yasmin Asare Anderson , Katherine Jameson Armstrong , Marina Gkiza , Jay Jennings , Christopher Moessner , Fernanda Viégas , Martin Wattenberg , Lynette Webb , Fabian Wrede , and Patrick Gage Kelley . 2020 . “A cold , technical decision - maker” : Can AI provide explainability , negotiability , and humanity ? . 1 , 1 ( December 2020 ) , 23 pages . Authors’ addresses : Allison Woodruff , Google , woodruff @ google . com ; Yasmin Asare Anderson , Ipsos , yas . asareanderson @ ipsos . com ; Katherine Jameson Armstrong , Ipsos , katherine . j . armstrong @ ipsos . com ; Marina Gkiza , Ipsos , marina . gkiza @ ipsos . com ; Jay Jennings , PixelInsight , jay @ pixelinsight . co . uk ; Christopher Moessner , Ipsos , christopher . moessner @ ipsos . com ; Fernanda Viégas , Google , viegas @ google . com ; Martin Wattenberg , Google , wattenberg @ google . com ; Lynette Webb , Google , lwebb @ google . com ; Fabian Wrede , Google , fwrede @ google . com ; Patrick Gage Kelley , Google , patrickgage @ acm . org . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . © 2020 Association for Computing Machinery . Manuscript submitted to ACM Manuscript submitted to ACM 1 a r X i v : 2012 . 00874v1 [ c s . C Y ] 1 D ec 2020 2 Woodruff , et al . 1 INTRODUCTION Motivated by expectations of improved efficiency , efficacy , consistency , and objectivity , algorithmic systems are increas - ingly deployed to make decisions in many areas of people’s lives , from finance , employment , medicine and criminal justice to advertising , media recommendations and more [ 20 , 31 , 75 , 84 ] . This shift from individual or organizational human decision - making to algorithmic decision - making has been accompanied by concern that important decisions are being made in new ways that are often mysterious , and that current oversight does not adequately ensure social values such as fairness , privacy , and safety [ 21 , 59 , 75 , 84 ] . A variety of approaches have been proposed in technical , social science , legal , and regulatory communities to address this issue and provide accountability for algorithmic systems . However , the best path forward is as yet unclear . For example , increased transparency or explainability are often proposed as mechanisms to provide accountability , and frequently feature in regulatory and tech ethics requirements or discussions [ 38 , 45 , 55 , 70 , 94 ] . While these have significant potential upside , concerns have been raised that such mechanisms are unlikely to be sufficiently effective or satisfactory , due to fundamental challenges in producing meaningful human explanations of the operation of complex machines [ 2 , 31 , 59 , 85 ] . Further , in some contexts practical constraints may require that transparency or explainability be traded off against qualities such as a system’s accuracy or its ability to resist adversarial attacks [ 13 , 59 , 61 ] . Limited understanding of public attitudes towards algorithmic decision - making makes it difficult to include public needs and desires in balancing these complex tradeoffs . To inform policy and technical approaches to algorithmic decision - making , we present results from a qualitative study comprised of five workshops conducted with a total of 60 participants in Finland , Germany , the United Kingdom , and the United States . Our contributions are as follows : • We present results from a qualitative study of algorithmic decision - making conducted in four countries • We identify key desirable qualities of decision - makers and discuss how AI is perceived as able or not able to provide them • We discuss how algorithmic decision - making is intended to regularize institutional processes and make them more efficient , but in so doing is perceived as reducing flexibility , compassion , and other qualities desired by recipients of a decision • We consider how our findings inform potential design of algorithmic decision - making systems , as well as allocation of responsibilities between AI and humans in human - in - the - loop approaches In the remainder of the paper , we review relevant background , describe our methodology , present and discuss our findings , and conclude . 2 BACKGROUND 2 . 1 Algorithmic Decision - Making Systems , Artificial Intelligence , and Machine Learning In recent decades , physical and digital systems across a range of sectors have increasingly transitioned to automation for a range of functions including decision - making , 1 in hopes of improving efficiency and performance , and regularizing decisions by removing human and procedural variability [ 36 , 50 , 72 ] . The significant impact , risks , and challenges of this transition have been an object of substantial study , raising concerns related to fairness , privacy , safety , and more as algorithmic systems increasingly shape information people are exposed to as well as influence decisions about employment , finances , and other opportunities [ 7 , 10 , 21 , 22 , 24 , 36 , 39 , 40 , 43 , 51 , 62 , 66 , 71 , 75 , 83 ] . 1 In this article we use the term algorithmic decision - making systems , following [ 26 , 54 ] and others . The term automated decision - making is often used synonymously , commonly describing decision - making functions that are carried out in part or in full by a device or machine [ 74 ] . Manuscript submitted to ACM “A cold , technical decision - maker” : Can AI provide explainability , negotiability , and humanity ? 3 Artificial intelligence ( AI ) underlies many algorithmic decision - making systems . Machine learning is a particular type of artificial intelligence that has the ability to learn and improve over time , and it is an increasingly common choice for many applications including decision - making . Accordingly , in our study we focused primarily although not exclusively on algorithmic decision - making systems that use different types of AI , including machine learning . 2 . 2 Accountability , Transparency , Explainability , and Interpretability In search of mechanisms to detect and address social ills that may result from the use of algorithmic decision - making systems , and building on the expectation that insight into a system’s production , operation , and effects can enable accountability , many regulators , civil society groups , and scholars have called for a cluster of related ideas encompassing transparency , explainability , and interpretability . 2 Consideration of these approaches dates to the 1970s with the recognition of the need to explain the decisions of expert systems , with a resurgence of this topic in recent years as contemporary machine learning models have become both pervasive and increasingly complex [ 1 , 9 , 44 , 63 , 84 ] . Due to its often impenetrable and opaque nature , machine learning , particularly deep learning , has generated concern and attracted heavy critique as it can be difficult to interrogate its reasoning and remediate potential injustices or errors [ 13 , 43 ] . Interpretability is therefore a topic of substantial debate at the highest levels of machine learning expertise [ 16 , 81 ] , with many arguing there is an inherent tension between a system’s accuracy and its interpretability , but others arguing it is possible to finesse this technical challenge with methods such as creating human - understandable models that are proxies for more complex models , intentionally designing models that are more readily interpretable , or conducting “outside the box” analyses of system design and behavior [ 11 , 44 , 47 , 60 , 79 , 84 ] . A number of scholars further argue that making systems more understandable is at odds with other values such as protecting systems from adversarial attacks or seamless design [ 13 , 35 , 59 ] , and even when there is not an inherent tradeoff , it can distract from more important priorities such as empirical ( non - explanatory ) verification of system performance [ 2 , 31 , 60 , 61 , 85 ] . More fundamentally , there is a growing recognition that transparency , explainability , and interpretability are often intended as a means to an end , for example laying the groundwork to challenge a decision or to ensure fairness , yet they provide inadequate support for such underlying objectives [ 58 , 77 , 100 ] . As a result , a number of approaches have been articulated in recent years that hew more closely to direct needs of users and society . Counterfactuals specify conditions under which a different decision would have been made [ 56 , 82 , 95 ] . Contestability provides the ability to challenge decisions , and allows domain experts to collaborate with and correct predictive algorithms [ 49 , 58 , 92 ] . Recourse considers the ability to correct faulty or incomplete data , and also identify clear actions an individual can take in the future to get a different outcome [ 91 , 93 ] . 2 . 3 Algorithmic Literacy and Explanation Design Beyond the challenges described above , provision of good explanations is further complicated by the fact that end users often have fundamental questions or misconceptions about the operation of algorithmic systems [ 12 , 34 , 78 , 90 , 97 ] , and the construction of useful explanations remains elusive [ 68 , 77 ] . These challenges notwithstanding , a number of studies have shown that explanations have potential to increase users’ trust and satisfaction [ 9 , 68 ] , and they remain an object of great interest with a number of efforts pursuing best practices for their design [ 6 , 14 , 33 , 48 , 63 , 68 , 76 , 96 ] . 2 These notions are strongly related and these terms are often used interchangeably although some scholars and disciplines also distinguish their meanings in various ways [ 9 , 44 , 58 , 60 , 63 , 77 ] . For our purposes in this article , we define transparency as broad visibility into the construction , behavior , and processes of a system ; explainability as the ability to provide human understandable reasons for individual decisions or a set of decisions made by an algorithmic system ; and interpretability as the ability of a given model to explain how it calculates specific decisions . Manuscript submitted to ACM 4 Woodruff , et al . 2 . 4 Public Perception of AI and Algorithmic Decision - Making A number of studies have explored public perception of AI , for example , survey research [ 15 , 17 , 30 , 52 , 57 , 65 , 67 , 89 , 98 , 101 ] , sentiment analysis [ 37 , 41 ] , and narrative analysis [ 18 ] , most often in Western , English - speaking contexts . AI is viewed as likely to have significant impact on the future and overall expectation is often positive . At the same time , AI is neither interpreted as exclusively beneficial nor exclusively disadvantageous , and public response is often ambivalent [ 15 , 57 , 67 ] . Specific concerns have been expressed regarding social issues , such as privacy , job loss , AI benefiting the wealthy and harming the poor , and AI increasing social isolation and reducing human capabilities [ 30 , 57 ] . Resonating with this interest in the impact of AI on social issues , some public perception research has focused specifically on its use in decision - making , e . g . a survey in the United Kingdom found that people were not very familiar with the use of automated decision systems , and indicated low levels of support and high levels of opposition to them [ 5 ] . 2 . 5 Workshop as Method In taking up a workshop format , we draw on traditions within and just beyond HCI . This includes programs of participatory action research , participatory design , and living labs [ 27 , 28 , 53 , 80 ] . Within the context of HCI and design research , workshop approaches often seek to invite members of the public to engage with practices of design while exploring values and beliefs around technology with each other , positing alternative techniques and outcomes . We also draw inspiration from Citizens’ Juries , a form of deliberative democracy dating from the 1970s that brings together individuals from different backgrounds to spend several days hearing expert evidence and considering a public policy question [ 4 , 5 , 19 , 69 , 73 ] . Two deliberative democracy studies run in the United Kingdom that used Citizens’ Juries methodology are closest in nature to our own work . One explored under what circumstances participants would find the use of automated decision - making appropriate , finding they valued accountability and ethical development processes [ 86 ] . Another focused on how participants prioritized accuracy versus explainability for AI decision - making , reporting that context mattered and accuracy outweighed explainability in healthcare but not other non - medical situations presented [ 73 , 86 ] . We build on , extend , and broaden this previous work by exploring complementary research objectives ; exploring a wider range of domains , tradeoffs , and types of transparency ; and expanding the scope to three additional countries beyond the United Kingdom . 3 METHODOLOGY In order to better understand perception of algorithmic decision - making systems , we partnered with our co - authors from [ anonymized market research firm ] to conduct five workshops with 60 participants in major cities in Finland , Germany , the United Kingdom , and the United States . These countries were selected due to European and United States regulatory interest in accountability of AI systems [ 55 , 70 ] and pre - existing literature on perceptions of AI or algorithmic decision - making that we could build on and triangulate with , for example [ 5 , 17 , 57 , 69 , 101 ] . Partnering with an international market research firm ensured professional quality recruiting and moderation across multiple countries , languages , and cultural contexts . Our research objectives were to learn more about the following : • What qualities do members of the public want in algorithmic decision - making systems ? • To what extent do members of the public value or prioritize explainability in particular , for what reasons ? • What qualities do members of the public believe an AI - based algorithmic decision - making system has , or is capable of ? In what circumstances do they see its use as beneficial ? Manuscript submitted to ACM “A cold , technical decision - maker” : Can AI provide explainability , negotiability , and humanity ? 5 3 . 1 Participants We recruited 12 participants for each of five workshops ( two in Germany and one each in the other countries ) through professional recruiting firms in each country . In each region , participants were recruited to represent a diverse spread across the following : age , gender , income , life stage , socioeconomic status , educational level , digital literacy , and exposure to AI . They were recruited to represent a mix of ethnicity representative of the region . Participants were compensated for their time , at or above the living wage for their area . 3 . 2 Workshops Each group participated in a 3 - hour workshop at third party facilities in each country during November and December 2019 . Two moderators were in the discussion rooms with the 12 participants , and participants were aware that several researchers and staff were in observation rooms or viewing remotely . All moderators were from a professional qualitative research background and were fluent in the local language . In non - English speaking countries ( Finland and Germany ) , simultaneous translators were present in the observation rooms and their translations were broadcast to remote viewers and captured on recordings so that the researchers and staff observing the sessions could hear the discussion and pass questions and input to the moderators in real - time . Both moderators and simultaneous translators received specific training in the objectives and content of the workshops before the sessions . Refreshments were available throughout the sessions . Workshops followed this agenda ( times are approximate ) : • Introductions and warm - up activity sharing experiences with human decision - making – 10 minutes • Presentation about AI and machine learning , their use in decision - making , tradeoffs , and Q / A – 60 minutes • First scenario ( in two breakout groups , with six participants each ) – 40 minutes • Break – 10 minutes • Second scenario ( in same breakout groups as the first scenario ) – 30 minutes • Reconvene and summarize – 30 minutes The scenarios described hypothetical situations in which organizations were considering implementing algorithmic decision - making and were choosing among three systems with different characteristics ( see Tables 3 and 4 in Appendix A ) . For each scenario , we began by sharing a handout describing it and asking participants to take a few minutes to read and consider it . We then invited participants to share their initial reactions and facilitated a group discussion of the scenario . Midway through the discussion , we asked participants to fill out a handout and then vote in group discussion on which system they preferred . 3 The scenarios we discussed represented a wide range of domains and we encouraged discussion of other domains , so the discussion often branched out to other areas in which algorithmic decision - making might be used . After completing both scenarios , all participants joined back together and we concluded the workshop with a broad group discussion where participants reflected on ideas that had emerged throughout the session and synthesized and summarized their opinions . If participants requested , they were informed of the sponsor of the research at the end of the session . During the workshops , we took care to encourage collaborative interpretation , problem - solving , and discussion among participants , and to make space for all participants to share their ideas and opinions . After each session , moderators , researchers , and other team members held a debriefing session to discuss early observations and themes , as well as potential procedural refinements for future sessions such as timing adjustments . 3 While most of the procedure was consistent across the different countries , the voting process was somewhat variable as the study evolved , for example , in some cases participants voted both midway and at the end of the discussion . As a result , we do not report voting data here , although it was consistent with the qualitative findings we report . Manuscript submitted to ACM 6 Woodruff , et al . Detailed notes were taken on observations and themes for each country as the study progressed . All sessions were video - and audio - recorded and transcribed in English . For the United Kingdom and the United States , the transcripts were then compared against the original recordings for accuracy , and for Finnish and German the transcripts were reviewed and compared against the simultaneous translations for accuracy . 3 . 3 Material Development and Translation We developed materials designed to explore our research objectives , drawing heavily for inspiration on Citizens’ Juries work previously conducted in the United Kingdom [ 5 , 69 , 73 , 86 ] . This included a slide deck and accompanying script to provide participants relevant background on AI and machine learning ; descriptions of rule - based systems , conventional machine learning , and deep learning ; the use of AI and machine learning in decision - making ; and tradeoffs between accuracy and various forms of transparency . In some sessions we also showed a short introductory video about machine learning available on the internet . We also developed four hypothetical scenarios chosen to represent decision - making in topical and diverse contexts : stroke diagnosis , job applicant screening , credit scoring , and law enforcement , as shown in Table 3 ( Appendix A ) . For each scenario , participants were presented with three different systems which varied in a number of ways , similar to [ 69 , 73 ] , as shown in Table 4 ( Appendix A ) . The primary differences centered on accuracy versus a form of transparency ( explainability , appeal , or oversight ) , which we emphasized due to the centrality of this discussion in current policy and academic discussions . In some cases the magnitude and nature of the tradeoff were exaggerated in order to prompt useful discussion , rather than strictly adhering to specific current performance levels , again similar to [ 69 , 73 ] . Finally , we developed a moderator discussion guide to use with these materials . The materials were developed in English , and after they were complete , our market research partner translated materials into Finnish and German . 3 . 4 Analysis In our analysis , we used a general inductive approach [ 87 ] , which relies on detailed readings of raw data to derive themes relevant to evaluation objectives . In our case , the primary evaluation objectives were those listed at the beginning of this section , chosen to inform technical and policy approaches to algorithmic decision - making . After the workshops were over , one member of our research team reviewed all transcripts and prepared participant precis and breakout group precis summarizing the main points for each individual and breakout group , and then prepared a spreadsheet summarizing the key ideas raised by each participant and breakout group . This was work done in active consultation with another member of the research team and built on the collective debriefs conducted by the broader team after each workshop . Throughout , we continued to identify emergent themes [ 8 ] , iteratively revising and refining categories . In keeping with the general inductive approach , our analytic process yielded a small number of summary categories . We summarize those most relevant to our evaluation objectives in Section 4 . 3 . 5 Limitations We note several limitations of our methodology that should be considered when interpreting this work . First , it carries with it the standard issues attendant with qualitative methodologies and group interviews . While it yields rich insight , one must be cautious in generalizing beyond those studied or drawing conclusions across countries [ 4 ] . Second , we asked participants to consider hypothetical scenarios that most did not perceive as common in their daily lives at the current time . They may have had different reactions if they had been speaking to commonplace experiences , rather than extrapolating from more limited experiences in their current lives . Third , we studied a small number of scenarios , Manuscript submitted to ACM “A cold , technical decision - maker” : Can AI provide explainability , negotiability , and humanity ? 7 often with high stakes import . Different scenarios might yield varying results or further refine the insights we share here . Fourth , while we were heavily inspired by the Citizens’ Juries approach [ 4 , 19 ] , our workshops did not follow all aspects of this approach , most notably in that they were significantly shorter and more lightweight engagements ( three hours in our case , versus several days for Citizens’ Juries ) with less instructional content , no expert witnesses , and less discussion . In this way , while our data represents participants who were provided substantial information , our data does not represent what members of the public might think with more extensive intervention . Fifth , the discussion allowed open - ended conversation but also at some point required participants to express a clear choice for each scenario based largely on the tradeoff between explainability and accuracy . While Citizens’ Juries typically require jurors to make such a clear choice which prompts useful insight and discussion [ 73 ] , it also focuses some of the discussion on particular issues . To mitigate this , both in moderation and analysis , we took a wide view beyond these specific choices . Sixth , while members of the research team and / or market research partner team have experience conducting research in all markets studied , most reside in the United States or the United Kingdom . We have worked to minimize the risk of misinterpretation by collaboration and discussion with in - country colleagues but recognize that our interpretations may lack context or nuance that would have been more readily available to local residents . 4 FINDINGS In this section , we describe the main findings that emerged from our analysis . We begin by describing how participants perceived algorithmic decision - making systems as offering efficiency and performance advantages in some circumstances but potentially making substantial errors in others , as well posing a potential threat to dignity and autonomy . We discuss the perceived opportunity for explanations and human intervention to ameliorate some of these concerns . We then summarize the desirable qualities of decision - makers , whether AI or human , and close this section by discussing situational factors that influence tradeoffs among those qualities and influence the allocation of responsibility to algorithmic decision - making systems versus humans . 4 . 1 AI as an emotionless , mechanical decision - maker Participants recognized strong advantages of algorithmic decision - making in some circumstances , such as speed , scale , or improved accuracy . Participants perceived AI as performing objective , mechanical tasks well , and characterized it as thinking in black - and - white terms and following rigid criteria . Correspondingly , they often viewed AI as incapable of subjective or morally complex judgments . 4 “We are talking about 3 systems that have no gut instinct algorithm or anything similar , but which simply make decisions based on clear structures . ” – Germany A08 5 Germany A11 : “This is all theory , and life is everything in between ! Artificial Intelligence has black or white . I mean , when it comes to the rules . And that’s where we also , as people , can make different decisions based on the situation . ” Germany A07 : “We’ve heard about a system which , in the personnel field , can do the work for us and decide within two or three minutes whether the applicant is good or not . . . But then there’s a certain ‘gut feeling’ involved , there’s not just black or white or grey , but also something in between , and you have to look at whether somebody as a whole actually fits , and 4 Our discussions with participants focused on AI - based algorithmic decision - making systems , with a broad range of functionality from basic rule - based systems to advanced deep learning systems . Accordingly , when participants speak of AI they typically refer to it in the context of an AI - based algorithmic decision - making system . 5 For ease of reading , we have followed editing conventions consistent with applied social science research practices as described in [ 23 ] . Specifically , we edited quotes to remove content such as filler words and false starts , and in some cases we re - punctuated . We use ellipses to indicate substantial omissions . We identify each participant by country name plus a numerical identifier from 01 to 12 for each workshop . For Germany , we distinguish between the two workshops by prepending A or B to the numerical identifier . Manuscript submitted to ACM 8 Woodruff , et al . the system can’t do that , because it only looks at ‘studies’ yes or no , ‘foreign languages’ yes or no , but there are always some things in between that a computer , from my perspective , can’t evaluate . ” Beyond perceiving AI as well - suited to making straightforward decisions , participants sometimes saw AI as appro - priate for complex technical tasks , such as autopilot for airplanes or cars , with some variation in preferred tradeoffs between safety ( accuracy ) versus explanation . “I would imagine it could apply to areas of autonomous driving or flying or similar . Complex things that are technically - based where I , as a user , would like to have the highest amount of safety [ and would not require explanation ] . ” – Germany A09 “I wanted to add something regarding trust . For me , it depends on how difficult the task is . . . When I know the task is quite simple and it has received much data , then I think , I don’t care how it does it , but okay , it has done it . But when it is a more complex task , then I would rather like to know how it works . Otherwise I wouldn’t feel too safe . ” – Germany B05 By contrast , AI was seen as poorly suited to tasks that dealt with non - specific factors , such as complex moral judgments or assessments of intangible factors , consistent with an argument made in legal scholarship that policies that require the exercise of human discretion cannot be automated [ 20 ] . Participants in a study in the United Kingdom believed that machines operate with logic and rules that cannot form the basis of ethical decisions [ 64 ] ; here we develop this argument further and provide evidence of it in other countries . “ [ AI ] doesn’t allow for those things that can’t be defined by specifics . ” – United Kingdom 12 “In these kinds of humane decision - making things , where more complicated , abstract things are weighed . . . justice , fairness and such . . . something much higher than just crunching numbers , for that artificial intelligence is not suitable . ” – Finland 11 “I’d say it depends on the role ethics play in the situation . So , when it comes to hospital decisions , we very likely all agree that we want to save lives , everybody should be as healthy as possible and therefore , if technology can help us with that , we might be happy to take it . Whereas with other questions , where we are not even sure how we should act as humans , questions where we don’t even agree on how much power the police should actually have , in those cases AI might be a tool that some reject as a matter of principle . And these are no longer yes or no questions , they are far more complicated because they are ethical , legal and philosophical . And when even we as humans can’t even agree on that , there’s no point in bringing AI into it . ” – Germany B04 Many participants emphasized that AI does not take into account soft factors ( qualities that are intangible or difficult to specify ) , leading to strong skepticism that AI could make good or fair decisions for tasks where such factors are important , consistent with [ 86 ] . “If I was an applicant myself , then in my opinion , it would be unfair for me if only my knowledge was evaluated , and not that ‘that’s a really good person , matches this team exactly , it’s exactly who I’m looking for . ’ Can it do that , the artificial intelligence , can it know what type of people I like and with whom I like to work ? . . . I don’t believe an artificial intelligence can know whether I’m suitable for some job or not . ” – Finland 04 Germany B11 : “ [ Even with a hip replacement ] , there are still certain things to weigh . . . there are still soft factors involved and a human assessment of whether this is still justified for someone . ” . . . Germany B01 : “Even if the computer says you need a new hip , it always depends on how much a person is suffering . The individual physical strain , how the body works , something does not fit together , whether you got an inflammation . This is individually different . It always depends on the individual level of suffering and the future intelligence cannot decide this at all . ” . . . Germany B07 : “Many also managed to overcome cancer based on their personal willpower . A machine won’t be able to tell how strong a human’s will is . ” Manuscript submitted to ACM “A cold , technical decision - maker” : Can AI provide explainability , negotiability , and humanity ? 9 AI was sometimes characterized as cold and emotionless , consistent with [ 5 , 57 ] , and lacking human qualities such as ‘gut instinct . ’ Finland 04 : “I’m thinking about whether it becomes so - called too intelligent , that it becomes a cold , technical decision - maker . . . Then certain issues , where there should be the humane aspect , can it then do such things ? ” . . . Finland 06 : “I was reminded of the old saying : ‘Good servant but poor master . ’” 4 . 2 Mysterious Dictates Concerns have long been raised in both scholarly and fictional literature that algorithmic decision - making and auto - mated systems can undermine human autonomy if powerful institutions issue consequential decisions that cannot be understood or appealed [ 17 , 31 , 54 , 84 , 99 ] . Autonomy is considered essential to human wellbeing , making the notion of such unfettered power both frightening and dehumanizing [ 18 , 36 , 57 , 58 , 84 , 93 ] . Participants often brought up these issues . For example , the notion that deep learning systems learn on their own , developing their own unseen rules and assumptions , was sometimes experienced as disturbing , eerie or even scary . “Deep learning concerns me because it seems totally computer led and based , and there’s no human intervention . Although they all have ultimately some human behind it . . . But I think the deep - learning route is just making up its own set of rules and assumptions . ” – United Kingdom 12 Participants associated this autonomy and opacity with a loss of human influence and control . Sometimes this was framed as a transfer of power from humans to AI , or even as humans being “at the mercy of the machines . ” In some cases these concerns resonated with common media tropes about AI taking over society . “It is difficult , if there is something starting to develop itself , and you can’t influence it anymore , at least only in a limited way . ” – Germany B09 “I also got a scary feeling from [ deep learning ] , the feeling that we’ve given power to somewhere else . ” – Finland 07 Germany B02 : “ [ System C is ] not transparent . It can’t give a reason . ” Germany B11 : “As they don’t know it themselves . Nobody knows why the computer is making a certain decision because it is learning by itself , somewhere deep inside . ” Germany B07 : “And we are supposed to trust them . [ laughter ] ” Germany B02 : “ . . . there is no argument against the decision . ” Germany B10 : “So you elevate the machine to be equal to God . ” Some participants formulated the use of deep learning in algorithmic decision - making as overreliance on machines , expressing concern about blindly accepting decisions made by machines . Some were also concerned about humans subjecting themselves to the dictates of algorithmic decision - making systems , saying for example that a deep learning credit scoring system that did not provide explanations was “too authoritarian . ” “You don’t want somebody to just dictate everything for you . You want to have the ability to be like . . . ‘Wait , that’s not really right . ’” – United States 07 “I’m so anti - relying on machines and this would force me to say , ‘ . . . because you , the machine tell me this is 95 % accurate , I have to automatically accept it . ’ . . . ‘Well , F you , sorry , I don’t agree with it . ’” – United States 12 Such concerns were exacerbated by fear or anger at the thought of ending up in the group about whom AI made an inaccurate decision , and a sense that it was “random” chance whether one ended up in that group . Participants sometimes compared subjecting oneself to decisions from such a system to gambling at poker or dice . A few participants Manuscript submitted to ACM 10 Woodruff , et al . also worried that it would be difficult to escape from an inaccurate decision once it had been made , as decisions might be shared across institutions , leaving them essentially powerless and without recourse . “If I’m one of that 5 % , then it’s really shitty that I’m accused of something I really haven’t done . ” – Finland 01 “Transparency [ is more important than accuracy ] . Because 95 % means that there are still five percent which are wrong . And if I am part of the 5 % then it is bad . ” – Germany A03 “It only sucks if you are the five percent . That’s the issue . . . for most it is good , but for those for whom it does not work , it is shit . . . Oh man , that’s like playing poker . [ laughter ] That is so much like playing poker . Even with the best hand , you can always lose . ” – Germany B11 Germany A09 : “I think this would be bullshit [ to not get an explanation regarding a decision , for example , the reason your application for a loan has been rejected ] , because you cannot escape from this hamster’s wheel . No matter what you do , you cannot escape it . You are simply unlucky . And you cannot learn from it . You cannot get behind the reason et cetera , you cannot do anything . ” Germany A05 : “You cannot prepare better for the next job application because you don’t know why they did not accept your previous application . ” Germany A07 : “And if everybody works with the same AI system , it would not matter whether I write a hundred or a thousand applications . The AI system would always say , ‘You are out . ’ . . . Nowadays , the situation is different . I can go to [ three different institutions ] and they might apply different criteria . . . With System C I have to hope that I don’t belong to the 5 % group . ” Germany A09 : “Yes . . . There will always be winners or losers , however , once you are a loser , you will always be a loser . ” 4 . 3 What do explanations offer ? Explanations are a potential mechanism to ameliorate the concerns raised in the previous sections , and participants portrayed explanations as having a number of useful or desirable functions as summarized in Table 1 . Explanations were seen as providing transparency , allowing people to detect errors ( e . g . incorrect data leading to a poor credit score ) , and laying the groundwork for appeals to change decisions . Explanations were also seen as a mechanism for oversight and accountability , for example , a way to detect and expose discrimination in job application screening decisions . United States 02 : “You need the explanation to make the appeal . ” United States 01 : “First . That’s the foundation which you base your appeal on . ” . . . United States 05 : “The explanation . . . that’s the most important component of it . If you get the explanation , you might realize that there is no reason for you to appeal or you might not need to appeal . ” Beyond corrective measures and transparency , explanations were highlighted as informing personal future behavior or actions , for example , understanding how to modify one’s future financial decisions in order to increase a credit score . United States 12 : “What’s bothering me about deep learning is that there’s no way of explaining how it reaches its decision . That is bothering me . . . because , when push comes to shove , I want to know , ‘How’d you reach the decision ? What was the basis ? What was the factors ? ’ I need to know . ” United States 06 : “It’s going to be a common problem that somebody has a low credit score , and if there’s no way to tell them how to improve , that’s a little frustrating . ” “Sometimes , there’s not a need for an appeal , but there’s a need for improvement . So if you have help on what you could do to improve , then that’s the most important . ” – United States 05 “If you had a system where they could pick out specific bits of the CV . . . what it would allow people to do is to go away and go , ‘Oh right , okay . I just haven’t got this job because I haven’t got the criteria . This is the specific things that I need to work on . ’” – United Kingdom 08 “What can I do myself to better my situation , so the computer will smile upon me ? ” – United States 01 Manuscript submitted to ACM “A cold , technical decision - maker” : Can AI provide explainability , negotiability , and humanity ? 11 Purposes of Explanations Notable Mentions in Other Work Enable individual challenge Legal [ 21 , 31 , 54 , 55 , 84 , 95 ] ; Deliberative Democracy [ 69 ] ; Industry Roundtable [ 69 ] ; Technical [ 88 ] Inform personal behavior Legal [ 84 , 95 ] ; Deliberative Democracy [ 69 , 73 ] ; Industry Roundtable [ 69 ] Enable accountability & oversight Legal [ 21 , 54 , 84 ] ; Deliberative Democracy [ 69 ] Be respectful Legal [ 31 , 54 , 84 ] Build user trust & satisfaction Technical [ 9 , 48 , 88 ] Advance human knowledge – I’m the kind of person who needs explanations – Reasons explanations may be seen as unnecessary May not understand explanation Deliberative Democracy [ 69 ] Bad time to receive explanation Deliberative Democracy [ 69 ] Trust system ( e . g . system accuracy has been verified ) Ethics [ 61 ] ; Technical [ 29 ] Don’t require an explanation from humans in that situation Deliberative Democracy [ 73 ] Reason for decision is obvious – Already know reason for decision – Table 1 . In the lefthand column we first list purposes participants discussed in our study and then reasons why explanations were seen by our participants as unnecessary , as described in Section 4 . 3 . In the righthand column we draw connections with notable mentions of these ideas in legal , technical and other literatures . Previous work has been largely conceptual , with rare mentions in user research . Our work provides new and supporting evidence that these ideas resonate with members of the public . A few participants also highlighted that explanations can advance human knowledge , for example , expressing concern medical progress might be slowed in the stroke diagnosis scenario if decision - making was given over to deep learning and explanations were not available . “Doctors can perhaps even learn from it , that this has made the decision based on these grounds . Doctors might learn something , whether it’s right or wrong . ” – Finland 08 “ [ Deep learning means ] man does not learn anything anymore . . . the knowledge is transferred into a system , where you cannot get it out . ” – Germany B11 “For me , the limit is , where it is better than a doctor . If I suppose that if normal doctors have a failure diagnosis rate of 25 % , then I have a higher chance to survive if a system with the 85 % probability executes the examination . The only thing that would bother me is , if we don’t get any explanations and only have the system , that the knowledge about the disease and the illness will get lost . ” – Germany 05 Others spoke of explanations as building good will or demonstrating respect , consistent with discussion in the legal literature that explanations serve a dignitary and humane purpose [ 31 , 54 , 84 ] . Some participants also wanted explanations due to broad personal preference . Manuscript submitted to ACM 12 Woodruff , et al . “My issue with deep learning is that I’ve always been the type of person – I’m not going to just take something for what it is . I need to know . I’m 100 % okay with whatever result it is , but just take me through the process of how we got to that . ” – United States 07 “I’m a person that’s very analytical , so I need to be able to understand , to see what it is . Let me see it , let me read it , let me understand it , let me digest it . ” – United States 02 However , some participants also argued that there were situations in which explanations were simply not useful or necessary , or would be substantially less important than other qualities such as accuracy or appeal , as shown in Table 1 . Some participants even challenged others who said they prioritized explanations over accuracy in given scenarios , arguing that those explanations would not be helpful to them . “IfIknowthatitworkswellin19outof20cases , thenIdon’tnecessarilyneedtoknowhowexactlyitworks . ” – GermanyA12 “At the end of the day , there are things that no matter how much someone explains to me , I’m not going to understand regardless , because I know sometimes I have really intelligent friends who try to explain things to me . I’m not going to get it . . . I’m not going to understand everything , so if it’s just going to tell me the score and then I still have the opportunity to appeal it and discuss it with someone , I’m going to go with C , because it’s the most accurate . ” – United States 07 Participants often viewed straightforward decisions as not requiring explanation or human review . “It’s situational . You run a red light , it recognizes your plate . You should get a ticket for that . You don’t need [ an explana - tion ] . . . the factors are undisputable , right ? ” – United States 08 “It depends on the example . If we look , for example , at a case of parking in a wrong spot and the fact that the owner of a vehicle needs to pay a fine for parking in a prohibited zone , and furthermore assume that the photo that has been taken is 100 percent accurate , I won’t need a [ human ] judge . It is quite clear that my vehicle was parked in that spot . ” – Germany A03 “Maybe in areas of sports . For example , when a decision has to be made whether the person was in front or behind a line . In this case , I would rely on a technique that has a high hit - ratio [ and not need additional explanation ] . ” – Germany A01 In Table 1 we summarize the ideas described in this subsection , i . e . the purposes participants thought explanations could serve as well as the reasons they sometimes saw them as unnecessary . Legal , ethical , and technical scholarship has explored some of these ideas conceptually , but to our knowledge , at best limited evidence has been provided that they resonate with members of the public . Here we draw together these disparate ideas from multiple disciplines , and provide new and supporting evidence that these ideas have a meaningful role in public discourse and policy . 4 . 4 Participants felt some decisions require a human touch In previous sections we focused on how participants perceived algorithmic decision - makers , and how explanations might address potential issues with them . We now turn our attention to how participants perceived human decision - makers as a potential alternative or remedy to deficits in algorithmic decision - making . Participants frequently brought up humans and emphasized that humans have important decision - making capabilities that algorithmic decision - making systems do not . Relative to algorithmic decision - making systems , humans were seen ( in some or all situations ) as more trustworthy , compassionate and capable of treating people well , capable of complex judgment and assessing intangible factors , and engaging in negotiation . Decision - making tasks involving such capabilities were seen as best left to humans . Further , participants viewed humans as having a critical role in overseeing and intervening in algorithmic decision - making systems , in order to prevent the loss of autonomy discussed in Section 4 . 2 . Participants often brought up the importance of ‘the human touch’ or ‘the human element’ in decision - making . Human knowledge , collective or individual , was often seen as more trustworthy than machine knowledge or artificial Manuscript submitted to ACM “A cold , technical decision - maker” : Can AI provide explainability , negotiability , and humanity ? 13 intelligence . Some participants observed that System A ‘had the most human element’ because it was based on human thinking and the knowledge of human professionals , and preferred it for that reason . “I somehow wish for a human for this kind of thing [ stroke diagnosis ] . ” – Finland 03 “I am worried about moving to a future where there’s literally no human element . I really am . ” – United States 09 “I think there’s that element of us as humans , just not being able to trust anything that’s not us . ” – United Kingdom 08 As mentioned in Section 4 . 1 , AI was characterized as lacking human capabilities such as emotion , empathy , or compassion . Relatedly , the lack of human involvement in a decision was sometimes characterized as devaluing the recipient . Human participation was viewed as adding emotional capacity and respect to the decision - making process . “That’s the human factor , someone who explains it empathically , not just the machine , which says ‘You’re bad’ . ” – Finland 10 “I mean , the robot can’t be merely a robot . It has to sort of adopt some human features of empathy and compassion . ” – United Kingdom 09 As also discussed in Section 4 . 1 , AI was typically seen as lacking the capacity to make complex moral judgments or assess soft factors . By contrast , humans were viewed as strong at these tasks . Therefore , participants felt decision - making responsibilities involving these more subjective judgments should be given to humans rather than AI . Human involvement was called out as particularly important for negotiation . Some participants highlighted the importance of going beyond formal decision - making criteria , and having flexibility to work around the system . This moved beyond assessment of intangibles like soft factors , to actions like considering extenuating circumstances , granting leniency for catastrophic events in people’s lives , ‘giving people a chance , ’ or taking into account personal trust . “I prefer a human - based one because I can say , ‘Look , I’ve got some of the criteria that you’ve asked for , but the others , I’m willing to learn . I’m a fast learner . Hire me , for a month , and if I haven’t learnt it , then obviously get someone else in . But I’m prepared to learn . And I’ll try my best . ’” – United Kingdom 02 “I work in social housing , and there’s some questions that are asked all the time . And rather than having someone answer the same question all the time [ chuckling ] . . . do you know what I mean ? ‘How do I pay my rent ? ’ Those basic questions . A chatbot can do that . But if it’s , ‘I can’t pay my rent because . . . ’ ‘I can pay universal credit’ or something else , for me , a human will always have to do that . ” – United Kingdom 12 “I mean , if you went in the Old West , into a bank and you’re sitting down with the manager and they say , ‘Well , I trust you , I know your family , let me give you a loan . ’ That’s the human element . There is no human element in the credit system . ” – United States 08 Humans were also seen as more variable than algorithmic decision - making systems , with approaches to multiple humans offering another avenue to get a favorable decision . Algorithmic decision - making systems were seen as consistent within institutions , and perhaps even across institutions . United States 01 : “The human ability to make judgment calls . We are not all the same , with the same parameters . There are times within the human experience – ” United States 02 : “Things happen . ” United States 01 : “Things happen . I mentioned , your kid falls out of a tree , you all of a sudden have huge medical bills , which changes your whole ability to handle other finances in your life and this is why I think appeal is important . And here’s another thing , it depends on who you get on the phone . ” . . . United States 02 : “That’s true , because I’ve gotten people on the phone when I spoke to my credit card company – like I’ll speak to somebody at 11 : 00 and I may speak to somebody at 1 : 00 – ” United States 01 : “You get two different – ” United States 02 : “You get two different answers . Two different temperaments . ” Manuscript submitted to ACM 14 Woodruff , et al . Perceived to do well Humans AI Speed No Yes Scale No Yes Consistency No Yes Objectivity No Mixed Accuracy Mixed Mixed Provide Explanations Mixed Mixed Provide Opportunity to Challenge Mixed Mixed Provide Opportunity to Negotiate Yes No Provide Human Touch Yes No Make Subjective Judgments Yes No Make Straightforward Decisions Yes Yes Table 2 . Desirable qualities of decision - makers . Finally , humans were seen as having an essential role in decision - making systems . Human oversight was viewed as necessary to keep control over machines and ensure they were behaving in a socially responsible manner , for example , by periodically auditing the system . Further , it was seen as advantageous to integrate humans in particular circumstances or at moments when algorithmic decision - making might be flawed , for example to use a human instead or have a human review output from the algorithm to make a final decision . However , other research points to challenges in successfully allocating responsibilities between humans and AI , as we discuss further in Section 5 . 4 . 4 . 5 Comparison of Human and AI Decision - Making In Table 2 we summarize the desirable qualities of good decision - makers , whether human or AI , that were raised during the workshops . 6 We note that while consistency is often positioned as an advantage of automation , as discussed above , some participants saw downsides in removing human variability and variability across institutions , as that variability gave them more ‘chances’ to get a favorable decision . Although there is some debate on this topic , the predominant position appears to be that all these qualities cannot be simultaneously achieved in entirety , leading to the need to prioritize . We now discuss the relative value of these qualities to participants when considering : ( 1 ) which factors were most valued for an algorithmic decision - making system , in a given situation ; and ( 2 ) which factors would prompt more or less human versus AI involvement , in a given situation . Considering tradeoffs between different algorithmic decision - making systems , qualities such as accuracy , speed , scale , explanation , and challenge were seen as critically important for at least one of the scenarios presented , and often more . For example , participants often emphasized accuracy and speed in the stroke diagnosis scenario . “You go for accuracy when your life is at stake . ” – United States 03 “In this kind of stroke , the faster the correct decision comes , the better it is . . . There’s no time for nitpicking . ” – Finland 11 Finland02 : “IfIwerethepatient , Iwouldbeinterestedespeciallyinaccuracy . Ifwethinkaboutthe25 % probabilityofgetting a wrong diagnosis , I’m more interested in whether the diagnosis is correct , instead of the explanation of the diagnosis , in 6 While some of the items on the list were presumably influenced by information provided during the workshops , workshop discussion was open - ended . Participants sometimes disagreed with or challenged the information provided , and often raised new issues that had not been discussed . A follow - up study , either qualitative or quantitative , could be performed to determine the extent to which participants generate these concepts spontaneously . See also the related discussion of advantages and disadvantages of automated decision - making in [ 86 ] which mentions a subset of these issues . Manuscript submitted to ACM “A cold , technical decision - maker” : Can AI provide explainability , negotiability , and humanity ? 15 this case . . . ” Finland 08 : “Yes . It doesn’t give much consolation to get a good explanation if you’re already in a casket . ” A small number of participants called out a potential downside of accuracy , that decisions could be harder to overturn if the algorithmic decision - making system were known to be highly accurate . Therefore , they expressed a preference for lower accuracy in some situations because they thought it would be easier to successfully appeal if the system were less trusted , resonating with the concept of ambiguity as a design resource [ 3 , 42 ] . “One of the reasons I chose A is because I thought that if it was going to be that inaccurate – only 75 % – it wouldn’t be trusted as much , like it would maybe be used to speed up the investigations or something , but it wouldn’t be used as evidence . ” – United States 10 Participants sometimes preferred a system that balances competing priorities . “ [ System B is ] a decent compromise . I’m willing to give up some accuracy to get more communication , more of a two - way street of dialogue , versus the machine saying , ‘This is my final roll in , lots of luck . ’ . . . So , it’s a good middle ground . ” – United States 01 “I am the kind of person who takes the middle road . There is still room for a little bit of freedom . You can influence the system and you can trace the process . It is not the extreme system of the three of them . ” – Germany A11 “ [ System B is ] the middle path . Fairly precise , but gives the possibility of challenging it . ” – Finland 01 Situational factors influenced not only preference among different algorithmic decision - making systems ( e . g . one that was more accurate but provided less explanation , or vice versa ) , but also perception of whether a human must be involved in the decision - making process , for example , providing oversight over an algorithmic system and making the final decision , or whether an algorithmic decision - making system was seen as appropriate to implement at all versus relying solely on a human decision - maker . “It depends so much on the situation . There are situations where [ AI ] could be and surely also is more precise than a human . ” – Finland 08 “I think that it really depends on the topic . If we are talking about face recognition and stroke , you are talking about health and your civil rights and whether you’ll be detained or not , so it’s about your freedom . Those are two absolutely extreme things concerning you and I would never give any AI the full and 100 % authority to make a decision . ” – Germany B11 “I’ve experienced something like that in my family a year ago . A life - saving surgery had to happen , and parts of it were done by a robot , which couldn’t be done by humans . And my dad wouldn’t be alive if that wouldn’t have happened . . . so I thought , okay there isn’t any doubting anymore , you just try it , because it is the only thing that can help . And then , that is the thing you cling to . In that moment , completely other things matter . When you know there is no other way , then you do it . It is maybe also in relation to what the initial circumstances are , are there still many options , or aren’t there any left ? And then it is maybe the only possibility . And in this case , I believe it is good that it exists , and that the development got so far , that they are more accurate and more unerring and can do things that humans aren’t able to . ” – Germany B09 Overall , considering tradeoffs such as accuracy versus explainability , a number of situational factors were important . Some of the most influential factors were : whether the decision was mechanical versus subjective ; the severity of the consequences ; whether it was the recipient’s only chance at a decision ( e . g . a medical decision or legal trial might offer only one meaningful opportunity at a decision , whereas an individual could apply multiple times for a loan ) ; whether the recipient could take meaningful action based on having information ; or the perspective of different parties ( for example , for an algorithmic decision - making system that screens job applications , an applicant might prioritize explanations but a hiring manager might prioritize accuracy ) . “It varies between scenarios . . . it just depends on the situation . ” – United States 07 Manuscript submitted to ACM 16 Woodruff , et al . “This is obviously different from the first one [ job application screening ] , where this [ credit scoring ] is very black and white . You either did or you did not pay on time your mortgage bill , your credit card , blah , blah , blah . And so initially you think , this is so accurate you go with C . However , this is so much more important than a job interview , because if you get an inaccurate credit rejection and you can’t find out why , that’s really concerning . Because that . . . will affect your life . ” – United Kingdom 11 “Facial recognition affects us on a much more personal level . ” – Germany A11 “Theeffectsoftheinaccuracyaremoredamaging [ forlawenforcement ] thanwithcredit . Imean , awrongfacialrecognition could change a life . ” – United States 05 “If you are all of a sudden part of the 25 % and you have been categorised in the wrong way and then you are up for a crime . . . It goes much deeper than a decision about whether an application is accepted or not . ” – Germany A02 “System A becomes more relevant in terms of the consequences that result from it . If there is a threat to go to jail , it is more important for me that the system is transparent and that I can prove it was not me . System A might be less relevant in cases where I apply for a job and am declined , because I can apply to other companies . ” – Germany A06 “I would like to know how it functions , if it makes decisions this big , as medical ones or legal ones , in this country . It’s important that there’s access to see how it functions . ” – Finland 11 “From the company’s point of view , it would be the accuracy . But from someone applying , it would be the explanation [ murmurs of agreement ] . ” – United Kingdom 10 5 DISCUSSION Our findings suggest productive directions for design , technical and research approaches to algorithmic decision - making . 5 . 1 Design choices related to explainability should account for situational variation Our research demonstrated that tradeoffs change across scenarios , for example , in some cases explanation , appeal , or other forms of transparency were a higher priority than accuracy but in other cases the reverse was true . Our research therefore suggests that compromising other qualities of an algorithmic decision - making system in order to provide an explanation is not always the right choice . We also highlight that while severity was sometimes a factor , it was often not the dominant consideration and was not sufficient to predict whether participants felt an explanation should be required . These findings reinforce and extend previous arguments that broad requirements to provide explanations are unlikely to meet the full spectrum of public needs [ 5 , 69 , 73 , 86 ] . Our research also underscores that explanation per se is not necessarily the end goal [ 58 , 77 , 100 ] , but rather there are goals underlying requests for explanation that can often be met via other mechanisms that provide accountability , contestability , appeal , advancement of human knowledge , or other qualities . 5 . 2 AI’s ( perceived ) incapacity for humanity and subjectivity AI was seen as lacking human qualities , such as the ability to show empathy , or the ability to make subjective decisions that involved complex moral judgments or intangible qualities . These perceptions are likely accurate for most of the algorithmic decision - making systems people have interacted with and that are in production today . However , future systems may be designed with more of these qualities as goals . Further , even for today’s systems , perception of AI as highly inflexible may not be entirely accurate . Communication and education about AI might be valuable towards influencing the public’s perception of its qualities . Manuscript submitted to ACM “A cold , technical decision - maker” : Can AI provide explainability , negotiability , and humanity ? 17 5 . 3 Negotiability in decision - making systems Our findings surfaced a new concept that is on the periphery of related capabilities in the literature such as contestability and recourse [ 49 , 58 , 91 – 93 ] . Discussion of these capabilities generally focuses on a decision - making framework , for example , examining how to challenge decisions that have not been made correctly within that framework , how experts may engage with the system to modify the framework , or how individuals can receive guidance on how they can change their behavior to meet the criteria of the framework . We propose negotiability , the ability of a decision - making system to allow individuals to request that a decision be modified not because it is based on inaccurate data and not because the decision is incorrect , but rather because they would like to ask the decision - maker to go beyond the established decision - making framework and criteria , to take into account factors outside of the accepted decision - making process , such as extenuating circumstances or personal relationships . In existing encounters with institutional decision - making , participants had sometimes been able to navigate human variability , empathy , and inconsistency to get a favorable result , and they felt that this opportunity would be lost with a non - human decision - maker . This connected with their beliefs that algorithmic decision - making systems were dispassionate , authoritarian , inflexible , and consistent , and would not be susceptible to appeals for compassion or special treatment . Such negotiability is not without challenges and drawbacks . Institutional movement towards automation and bureaucracy in many ways seeks to eliminate exactly such inconsistency and attendant concerns such as unfairness or bias . Even with these limitations , negotiability is a feature that some respondents appear to value in existing approaches , and it is worth considering further whether or how this need might be met ( whether by algorithmic systems or human review ) while also preserving other important qualities . 5 . 4 Further research on human - in - the - loop Participants saw strong advantages to AI in many cases , and participant interest in instilling humanity in decision - making processes raises interesting questions about the role of humans - in - the - loop [ 25 , 38 ] . Participants were highly interested in the roles humans and AI would play at different moments in the decision - making process , and their opinions regarding whether the use of AI was beneficial for a given use case often rested on detailed specifics of roles played by humans versus AI in the overall decision - making process . Further , well - executed human - in - the - loop strategies have the potential to ameliorate concerns about autonomy . Given that participants perceived that AI and humans had many complementary strengths , it would be productive to further explore how to effectively allocate responsibility and tasks between them in algorithmic decision - making systems . At the same time , other research has shown that handoffs between humans and AI are complex and difficult to design well [ 32 , 46 ] . We believe this to be an underexplored area and highlight this as a valuable topic for future research . 6 CONCLUSION We reported on a qualitative study of perceptions of algorithmic decision - making , conducted in Finland , Germany , the United Kingdom , and the United States . Participants shared thoughtful , nuanced insights on the qualities of human and AI decision - makers . Participants saw strong advantages to AI in many circumstances , but in some cases also felt that at certain moments humans should assume important responsibilities in algorithmic decision - making systems . We found high situational variance in the prioritization of different qualities , for example , explainability and appeal were prioritized in certain situations but not others . Further , algorithmic decision - making has been heralded as offering many benefits in terms of efficiency and consistency . While participants saw some of these qualities as desirable , they saw Manuscript submitted to ACM 18 Woodruff , et al . others as undesirable , and associated them with a loss of humanity , judgment , and flexibility . Future efforts can explore whether these qualities are inherently in tension with a shift towards algorithmic decision - making , and whether AI can effectively support perceived humanity and negotiability . REFERENCES [ 1 ] Ashraf Abdul , Jo Vermeulen , Danding Wang , Brian Y . Lim , and Mohan Kankanhalli . 2018 . Trends and trajectories for explainable , accountable and intelligible systems : An HCI research agenda . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . 1 – 18 . https : / / doi . org / 10 . 1145 / 3173574 . 3174156 [ 2 ] Mike Ananny and Kate Crawford . 2016 . Seeing without knowing : Limitations of the transparency ideal and its application to algorithmic accountability . New Media & Society ( 2016 ) . https : / / doi . org / 10 . 1177 / 1461444816676645 [ 3 ] Paul M . Aoki and Allison Woodruff . 2005 . Making space for stories : ambiguity in the design of personal communication systems . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI 2005 ) . 181 – 190 . https : / / doi . org / 10 . 1145 / 1054972 . 1054998 [ 4 ] Audrey Armour . 1995 . The citizens’ jury model of public participation : a critical evaluation . In Fairness and Competence in Citizen Participation . Springer , 175 – 187 . [ 5 ] Brhmie Balaram , Tony Greenham , and Jasmine Leonard . 2018 . Artificial Intelligence : Real Public Engagement . www . thersa . org / discover / publications - and - articles / reports / artificial - intelligence - real - public - engagement [ 6 ] Krisztian Balog and Filip Radlinski . 2020 . Measuring Recommendation Explanation Quality : The Conflicting Goals of Explanations . In Proceedings of the43rdInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval . 329 – 338 . https : / / doi . org / 10 . 1145 / 3397271 . 3401032 [ 7 ] Solon Barocas and Andrew D . Selbst . 2016 . Big Data’s Disparate Impact . California Law Review 104 ( 2016 ) , 671 . [ 8 ] Hugh Beyer and Karen Holtzblatt . 1997 . Contextual Design : Defining Customer - centered Systems . Elsevier . [ 9 ] Or Biran and Courtenay Cotton . 2017 . Explanation and justification in machine learning : A survey . In IJCAI 2017 Workshop on Explainable Artificial Intelligence ( XAI ) , Vol . 8 . 8 – 13 . [ 10 ] danah boyd and Kate Crawford . 2012 . Critical Questions For Big Data . Information Communication & Society 15 , 5 ( 2012 ) , 662 – 679 . https : / / doi . org / 10 . 1080 / 1369118X . 2012 . 678878 [ 11 ] Leo Breiman et al . 2001 . Statistical modeling : The two cultures ( with comments and a rejoinder by the author ) . Statist . Sci . 16 , 3 ( 2001 ) , 199 – 231 . [ 12 ] Taina Bucher . 2017 . The algorithmic imaginary : exploring the ordinary affects of Facebook algorithms . Information , Communication & Society 20 , 1 ( 2017 ) , 30 – 44 . [ 13 ] Jenna Burrell . 2016 . How the machine ‘thinks’ : Understanding opacity in machine learning algorithms . Big Data & Society 3 , 1 ( 2016 ) . https : / / doi . org / 10 . 1177 / 2053951715622512 [ 14 ] Carrie Jun Cai , Samantha Winter , David Steiner , Lauren Wilcox , and Michael Terry . 2019 . “Hello AI " : Uncovering the Onboarding Needs of Medical Practitioners for Human - AI Collaborative Decision - Making . In Proceedings of the 2019 ACM Conference on Computer Supported Cooperative Work and Social Computing ( CSCW ’19 ) . https : / / doi . org / 10 . 1145 / 3359206 [ 15 ] Blumberg Capital . 2019 . Artificial Intelligence in 2019 : Getting Past the Adoption Tipping Point . [ 16 ] Rich Caruana , Patrice Simard , Kilian Weinberger , and Yann LeCun . 2017 . The Great AI Debate – Position : Interpretability is necessary for machine learning , for and against . The 31st Conference on Neural Information Processing Systems ( NIPS 2017 ) . [ 17 ] Stephen Cave , Kate Coughlan , and Kanta Dihal . 2019 . " Scary Robots " : Examining Public Responses to AI . In Proceedings of the 2019 AAAI / ACM Conference on AI , Ethics , and Society ( AIES 2019 ) . 331 – 337 . https : / / doi . org / 10 . 1145 / 3306618 . 3314232 [ 18 ] Stephen Cave , Claire Craig , Kanta Sarasvati Dihal , Sarah Dillon , Jessica Montgomery , Beth Singler , and Lindsay Taylor . 2018 . Portrayals and perceptions of AI and why they matter . ( 2018 ) . [ 19 ] The Jefferson Center . 2004 . The Citizens’ Jury Handbook . [ 20 ] Danielle Keats Citron . 2007 . Technological Due Process . Washington University Law Review 85 ( 2007 ) , 1249 – 1313 . [ 21 ] Danielle Keats Citron and Frank Pasquale . 2014 . The Scored Society : Due Process for Automated Predictions . Washington Law Review 89 ( 2014 ) , 1 . [ 22 ] Federal Trade Commission . 2016 . Big Data : A Tool for Inclusion or Exclusion ? Understanding the Issues . [ 23 ] Anne Corden and Roy Sainsbury . 2006 . Using verbatim quotations in reporting qualitative social research : researchers’ views . University of York . [ 24 ] Tressie McMillan Cottom . 2015 . Credit Scores , Life Chances , and Algorithms . https : / / tressiemc . com / uncategorized / credit - scores - life - chances - and - algorithms / [ 25 ] Maria De - Arteaga , Riccardo Fogliato , and Alexandra Chouldechova . 2020 . A Case for Humans - in - the - Loop : Decisions in the Presence of Erroneous Algorithmic Scores . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . https : / / doi . org / 10 . 1145 / 3313831 . 3376638 [ 26 ] Nicholas Diakopoulos . 2016 . Accountability in algorithmic decision making . Commun . ACM 59 , 2 ( 2016 ) , 56 – 62 . https : / / doi . org / 10 . 1145 / 2844110 [ 27 ] Carl DiSalvo , Thomas Lodato , Laura Fries , Beth Schechter , and Thomas Barnwell . 2011 . The collective articulation of issues as design practice . CoDesign 7 , 3 - 4 ( 2011 ) , 185 – 197 . https : / / doi . org / 10 . 1080 / 15710882 . 2011 . 630475 [ 28 ] Carl DiSalvo , Illah Nourbakhsh , David Holstius , Ayça Akin , and Marti Louw . 2008 . The Neighborhood Networks Project : A case study of critical engagement and creative expression through participatory design . In Proceedings of the Tenth Anniversary Conference on Participatory Design 2008 ( PDC’08 ) . 41 – 50 . Manuscript submitted to ACM “A cold , technical decision - maker” : Can AI provide explainability , negotiability , and humanity ? 19 [ 29 ] Finale Doshi - Velez and Been Kim . 2017 . Towards a rigorous science of interpretable machine learning . arXiv preprint arXiv : 1702 . 08608 ( 2017 ) . [ 30 ] Edelman . 2019 . 2019 Edelman AI Survey . [ 31 ] Lilian Edwards and Michael Veale . 2017 . Slave to the algorithm ? Why a right to an explanation is probably not the remedy you are looking for . Duke Law and Technology Review 16 ( 2017 ) . [ 32 ] Madeleine Clare Elish . 2019 . Moral crumple zones : Cautionary tales in human - robot interaction . Engaging Science , Technology , and Society 5 ( 2019 ) , 40 – 60 . [ 33 ] Motahhare Eslami , Karrie Karahalios , Christian Sandvig , Kristen Vaccaro , Aimee Rickman , Kevin Hamilton , and Alex Kirlik . 2016 . First I “like " it , then I hide it : Folk Theories of Social Feeds . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . 2371 – 2382 . https : / / doi . org / 10 . 1145 / 2858036 . 2858494 [ 34 ] Motahhare Eslami , Aimee Rickman , Kristen Vaccaro , Amirhossein Aleyasen , Andy Vuong , Karrie Karahalios , Kevin Hamilton , and Christian Sandvig . 2015 . " I always assumed that I wasn’t really that close to [ her ] " : Reasoning about Invisible Algorithms in News Feeds . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems . ACM , 153 – 162 . https : / / doi . org / 10 . 1145 / 2702123 . 2702556 [ 35 ] Motahhare Eslami , Kristen Vaccaro , Min Kyung Lee , Amit Elazari Bar On , Eric Gilbert , and Karrie Karahalios . 2019 . User attitudes towards algorithmic opacity and transparency in online reviewing platforms . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . https : / / doi . org / 10 . 1145 / 3290605 . 3300724 [ 36 ] Virginia Eubanks . 2017 . Automating inequality : How high - tech tools profile , police , and punish the poor . St . Martin’s Press . [ 37 ] Ethan Fast and Eric Horvitz . 2017 . Long - term trends in the public perception of artificial intelligence . In Thirty - First AAAI Conference on Artificial Intelligence . [ 38 ] Jessica Fjeld , Nele Achten , Hannah Hilligoss , Adam Nagy , and Madhulika Srikumar . 2020 . Principled artificial intelligence : Mapping consensus in ethical and rights - based approaches to principles for AI . Berkman Klein Center Research Publication 2020 - 1 ( 2020 ) . [ 39 ] Marion Fourcade and Kieran Healy . 2013 . Classification situations : Life - chances in the neoliberal era . Accounting , Organizations and Society 38 , 8 ( 2013 ) , 559 – 572 . [ 40 ] Batya Friedman and Helen Nissenbaum . 1996 . Bias in computer systems . ACM Transactions on Information Systems ( TOIS ) 14 , 3 ( 1996 ) , 330 – 347 . https : / / doi . org / 10 . 1145 / 230538 . 230561 [ 41 ] Colin Garvey and Chandler Maskal . 2019 . Sentiment Analysis of the News Media on Artificial Intelligence Does Not Support Claims of Negative Bias Against Artificial Intelligence . OMICS : a Journal of Integrative Biology ( 2019 ) . [ 42 ] William W . Gaver , Jacob Beaver , and Steve Benford . 2003 . Ambiguity as a resource for design . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 233 – 240 . https : / / doi . org / 10 . 1145 / 642611 . 642653 [ 43 ] Tarleton Gillespie . 2014 . The relevance of algorithms . MIT Press . [ 44 ] Leilani H . Gilpin , David Bau , Ben Z . Yuan , Ayesha Bajwa , Michael Specter , and Lalana Kagal . 2018 . Explaining explanations : An overview of interpretability of machine learning . In 2018 IEEE 5th International Conference on Data Science and Advanced Analytics ( DSAA ) . IEEE , 80 – 89 . [ 45 ] Bryce Goodman and Seth Flaxman . 2017 . European Union regulations on algorithmic decision - making and a “right to explanation” . AI magazine 38 , 3 ( 2017 ) , 50 – 57 . [ 46 ] Ben Green and Yiling Chen . 2019 . The Principles and Limits of Algorithm - in - the - Loop Decision Making . 3 , CSCW ( 2019 ) . https : / / doi . org / 10 . 1145 / 3359152 [ 47 ] Maya Gupta , Andrew Cotter , Jan Pfeifer , Konstantin Voevodski , Kevin Canini , Alexander Mangylov , Wojciech Moczydlowski , and Alexander Van Esbroeck . 2016 . Monotonic calibrated interpolated look - up tables . The Journal of Machine Learning Research 17 , 109 ( 2016 ) , 1 – 47 . [ 48 ] Bernease Herman . 2017 . The promise and peril of human evaluation for model interpretability . In Proceedings of the 31st Conference on Neural Information Processing Systems ( NIPS 2017 ) . [ 49 ] Tad Hirsch , Kritzia Merced , Shrikanth Narayanan , Zac E . Imel , and David C . Atkins . 2017 . Designing contestability : Interaction design , machine learning , and mental health . In Proceedings of the 2017 Conference on Designing Interactive Systems . 95 – 99 . https : / / doi . org / 10 . 1145 / 3064663 . 3064703 [ 50 ] Mika Immonen , Sanna Sintonen , and Jouni Koivuniemi . 2018 . The value of human interaction in service channels . Computers in Human Behavior 78 ( 2018 ) , 316 – 325 . [ 51 ] Lucas D . Introna and Helen Nissenbaum . 2000 . Shaping the Web : Why the politics of search engines matters . The Information Society 16 , 3 ( 2000 ) , 169 – 185 . https : / / doi . org / 10 . 1080 / 01972240050133634 [ 52 ] Ipsos . 2019 . Widespread concern about artificial intelligence . www . ipsos . com / en / widespread - concern - about - artificial - intelligence / [ 53 ] Robert Jungk and Norbert Müllert . 1987 . Future Workshops : How to create desirable futures . Institute for Social Inventions . [ 54 ] Margot E . Kaminski . 2019 . Binary Governance : Lessons from the GDPR’s Approach to Algorithmic Accountability . Southern California Law Review 92 , 6 ( 2019 ) . [ 55 ] Margot E . Kaminski . 2019 . The right to explanation , explained . Berkeley Technology Law Journal 34 , 1 ( 2019 ) . [ 56 ] Amir - Hossein Karimi , Gilles Barthe , Borja Balle , and Isabel Valera . 2020 . Model - Agnostic Counterfactual Explanations for Consequential Decisions . In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics ( AISTATS ) . [ 57 ] Patrick Gage Kelley , Yongwei Yang , Courtney Heldreth , Christopher Moessner , Aaron Sedley , Andreas Kramm , David Newman , and Allison Woodruff . 2020 . “Happy and Assured that life will be easy 10years from now . ” : Perceptions of Artificial Intelligence in 8 Countries . arXiv preprint arXiv : 2001 . 00081 ( 2020 ) . Manuscript submitted to ACM 20 Woodruff , et al . [ 58 ] Daniel Kluttz , Nitin Kohli , and Deirdre K . Mulligan . 2020 . Shaping Our Tools : Contestability as a Means to Promote Responsible Algorithmic Decision Making in the Professions . In After the Digital Tornado : Networks , Algorithms , Humanity , Kevin Werbach ( Ed . ) . Cambridge University Press . [ 59 ] Joshua A . Kroll , Solon Barocas , Edward W . Felten , Joel R . Reidenberg , David G . Robinson , and Harlan Yu . 2016 . Accountable algorithms . University of Pennsylvania Law Review 165 ( 2016 ) . [ 60 ] Zachary C . Lipton . 2016 . The mythos of model interpretability . In 2016 ICML Workshop on Human Interpretability in Machine Learning ( WHI 2016 ) . [ 61 ] Alex John London . 2019 . Artificial intelligence and black - box medical decisions : accuracy versus explainability . Hastings Center Report 49 , 1 ( 2019 ) , 15 – 21 . [ 62 ] Caitlin Lustig , Katie Pine , Bonnie Nardi , Lilly Irani , Min Kyung Lee , Dawn Nafus , and Christian Sandvig . 2016 . Algorithmic authority : the ethics , politics , and economics of algorithms that interpret , decide , and manage . In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems . 1057 – 1062 . https : / / doi . org / 10 . 1145 / 2851581 . 2886426 [ 63 ] Tim Miller . 2019 . Explanation in artificial intelligence : Insights from the social sciences . Artificial Intelligence 267 ( 2019 ) , 1 – 38 . [ 64 ] Ipsos MORI . 2017 . Public views of Machine Learning : Findings from public research and engagement conducted on behalf of the Royal Society . https : / / royalsociety . org / - / media / policy / projects / machine - learning / publications / public - views - of - machine - learning - ipsos - mori . pdf [ 65 ] Mozilla . 2019 . We Asked People Around the World How They Feel About Artificial Intelligence . Here’s What We Learned . https : / / foundation . mozilla . org / en / blog / we - asked - people - around - the - world - how - they - feel - about - artificial - intelligence - heres - what - we - learned / [ 66 ] Cecilia Munoz , Megan Smith , and D . J . Patil . 2016 . Big data : A report on algorithmic systems , opportunity , and civil rights . Executive Office of the President . [ 67 ] ARM | Northstar . 2017 . AI Today , AI Tomorrow . Awareness and Anticipation of AI : A Global Perspective . www . arm . com / solutions / artificial - intelligence / survey / [ 68 ] Ingrid Nunes and Dietmar Jannach . 2017 . A systematic review and taxonomy of explanations in decision support and recommender systems . User Modeling and User - Adapted Interaction 27 , 3 - 5 ( 2017 ) , 393 – 444 . [ 69 ] Information Commissioner’s Office . 2019 . Project ExplAIn Interim Report . https : / / ico . org . uk / about - the - ico / research - and - reports / project - explain - interim - report / [ 70 ] High - Level Expert Group on Artificial Intelligence . 2019 . Ethics Guidelines for Trustworthy AI . [ 71 ] Cathy O’Neil . 2016 . Weapons of math destruction : How big data increases inequality and threatens democracy . Broadway Books . [ 72 ] Amy L . Ostrom , Mary J . Bitner , and Matthew L . Meuter . 2002 . Self - service technologies . e - Service : New Directions in Theory and Practice , ME Sharpe , NY ( 2002 ) , 45 – 64 . [ 73 ] Malcolm Oswald . 2019 . Artificial Intelligence ( AI ) & Explainability Citizens’ Juries Report . https : / / assets . mhs . manchester . ac . uk / gmpstrc / C4 - AI - citizens - juries - report . pdf [ 74 ] Raja Parasuraman , Thomas B . Sheridan , and Christopher D . Wickens . 2000 . A model for types and levels of human interaction with automation . IEEE Transactions on Systems , Man , and Cybernetics - Part A : Systems and Humans 30 , 3 ( 2000 ) , 286 – 297 . [ 75 ] Frank Pasquale . 2015 . The Black Box Society : The Secret Algorithms that Control Money and Information . Harvard University Press . [ 76 ] Forough Poursabzi - Sangdeh , Daniel G . Goldstein , Jake M . Hofman , Jennifer Wortman Vaughan , and Hanna Wallach . 2018 . Manipulating and measuring model interpretability . arXiv preprint arXiv : 1802 . 07810 ( 2018 ) . [ 77 ] Emilee Rader , Kelley Cotter , and Janghee Cho . 2018 . Explanations as Mechanisms for Supporting Algorithmic Transparency . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . https : / / doi . org / 10 . 1145 / 3173574 . 3173677 [ 78 ] Emilee Rader and Rebecca Gray . 2015 . Understanding User Beliefs about Algorithmic Curation in the Facebook News Feed . In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems . 173 – 182 . https : / / doi . org / 10 . 1145 / 2702123 . 2702174 [ 79 ] Marco Tulio Ribeiro , Sameer Singh , and Carlos Guestrin . 2016 . ‘Why should I trust you ? ’ : Explaining the predictions of any classifier . In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 1135 – 1144 . [ 80 ] Daniela K . Rosner , Saba Kawas , Wenqi Li , Nicole Tilly , and Yi - Chen Sung . 2016 . Out of time , out of place : Reflections on design workshops as a research method . In Proceedings of the 19th ACM Conference on Computer - Supported Cooperative Work & Social Computing ( CSCW ’16 ) . 1131 – 1141 . https : / / doi . org / 10 . 1145 / 2818048 . 2820021 [ 81 ] Cynthia Rudin . 2019 . Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead . Nature Machine Intelligence 1 , 5 ( 2019 ) , 206 – 215 . [ 82 ] ChrisRussell . 2019 . EfficientSearchforDiverseCoherentExplanations . In ProceedingsoftheConferenceonFairness , Accountability , andTransparency ( FAT * ’19 ) . 20 – 28 . https : / / doi . org / 10 . 1145 / 3287560 . 3287569 [ 83 ] Christian Sandvig , Kevin Hamilton , Karrie Karahalios , and Cedric Langbort . 2015 . Can an Algorithm be Unethical ? . In 65th Annual Meeting of the International Communication Association . [ 84 ] Andrew D . Selbst and Solon Barocas . 2018 . The intuitive appeal of explainable machines . Fordham Law Review 87 ( 2018 ) , 1085 – 1139 . [ 85 ] Mark Sendak , Madeleine Clare Elish , Michael Gao , Joseph Futoma , William Ratliff , Marshall Nichols , Armando Bedoya , Suresh Balu , and Cara O’Brien . 2020 . ‘The human body is a black box’ : supporting clinical decision - making with deep learning . In Proceedings of the 2020 Conference on Fairness , Accountability , and Transparency . 99 – 109 . https : / / doi . org / 10 . 1145 / 3351095 . 3372827 [ 86 ] Asheem Singh . 2019 . Democratising decisions about technology : A toolkit . ( 2019 ) . https : / / www . thersa . org / discover / publications - and - articles / reports / democratising - decisions - technology - toolkit Manuscript submitted to ACM “A cold , technical decision - maker” : Can AI provide explainability , negotiability , and humanity ? 21 [ 87 ] David R . Thomas . 2006 . A general inductive approach for analyzing qualitative evaluation data . American Journal of Evaluation 27 , 2 ( 2006 ) , 237 – 246 . [ 88 ] Nava Tintarev and Judith Masthoff . 2007 . A survey of explanations in recommender systems . In Proceedings of the 2007 IEEE 23rd International Conference on Data Engineering Workshop . 801 – 810 . [ 89 ] Northeastern University and Gallup . 2018 . Optimism and Anxiety : Views on the Impact of Artificial Intelligence and Higher Education’s Response . [ 90 ] Blase Ur , Pedro Giovanni Leon , Lorrie Faith Cranor , Richard Shay , and Yang Wang . 2012 . Smart , useful , scary , creepy : perceptions of online behavioraladvertising . In ProceedingsoftheEighthSymposiumonUsablePrivacyandSecurity ( SOUPS ) 2012 . https : / / doi . org / 10 . 1145 / 2335356 . 2335362 [ 91 ] Berk Ustun , Alexander Spangher , and Yang Liu . 2019 . Actionable Recourse in Linear Classification . In Proceedings of the Conference on Fairness , Accountability , and Transparency . 10 – 19 . https : / / doi . org / 10 . 1145 / 3287560 . 3287566 [ 92 ] Kristen Vaccaro , Karrie Karahalios , Deirdre K . Mulligan , Daniel Kluttz , and Tad Hirsch . 2019 . Contestability in Algorithmic Systems . In Conference Companion Publication of the 2019 Conference on Computer Supported Cooperative Work and Social Computing . 523 – 527 . https : / / doi . org / 10 . 1145 / 3311957 . 3359435 [ 93 ] Suresh Venkatasubramanian and Mark Alfano . 2020 . The Philosophical Basis of Algorithmic Recourse . In Proceedings of the 2020 Conference on Fairness , Accountability , and Transparency . 284 – 293 . [ 94 ] Sandra Wachter , Brent Mittelstadt , and Luciano Floridi . 2017 . Why a right to explanation of automated decision - making does not exist in the General Data Protection Regulation . International Data Privacy Law 7 , 2 ( 2017 ) , 76 – 99 . [ 95 ] Sandra Wachter , Brent Mittelstadt , and Chris Russell . 2018 . Counterfactual explanations without opening the black box : Automated decisions and the GDPR . Harvard Journal of Law and Technology 31 ( 2018 ) . [ 96 ] Danding Wang , Qian Yang , Ashraf Abdul , and Brian Y . Lim . 2019 . Designing theory - driven user - centric explainable AI . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . https : / / doi . org / 10 . 1145 / 3290605 . 3300831 [ 97 ] Jeffrey Warshaw , Nina Taft , and Allison Woodruff . 2016 . Intuitions , Analytics , and Killing Ants : Inference Literacy of High School - educated Adults in the US . In Proceedings of the Twelfth Symposium on Usable Privacy and Security ( SOUPS ) 2016 . 271 – 285 . [ 98 ] Darrell M . West . 2018 . Brookings survey finds divided views on artificial intelligence for warfare , but support rises if adversaries are developing it . Brookings ( August 2018 ) . [ 99 ] Sarah Myers West . 2018 . Censored , suspended , shadowbanned : User interpretations of content moderation on social media platforms . New Media & Society 20 , 11 ( 2018 ) , 4366 – 4383 . https : / / doi . org / 10 . 1177 / 1461444818773059 [ 100 ] Allison Woodruff . 2019 . 10 things you should know about algorithmic fairness . Interactions 26 , 4 ( 2019 ) , 47 – 51 . [ 101 ] Baobao Zhang and Allan Dafoe . 2019 . Artificial intelligence : American attitudes and trends . Available at SSRN 3312874 ( 2019 ) . Manuscript submitted to ACM 22 Woodruff , et al . Appendix A SCENARIO METHODOLOGY Scenario Description Stroke Diagnosis Rapid and accurate diagnosis of stroke greatly increases chances of survival and recovery of the patient . This is highly specialised work which ideally should be done by neurora - diologists with many years of training and experience . However , these experts are not available in every hospital , 24 hours a day , 7 days a week , and in practice diagnosis is often done by nonspecialist emergency medicine doctors . As diagnostic data are accumulated from previous stroke patients , AI systems could provide stroke diagnosis that is fast , and always available in hospitals . Draws heavily from the NIHR Greater Manchester Patient Safety Translational Research Centre and the Information Commissioner’s Office ( ICO ) Citizens’ Juries work http : / / www . bit . ly / GMPSTRCCitizensJuries Job Applicant Screening A large company in the private sector receives a high volume of applications , and plans to use an AI system to screen job applications and make shortlisting decisions . This will allow the company to screen every application and process applications more quickly . This system may also more objectively make decisions that would otherwise be prone to human errors or judgment . On the other hand , depending on how the system is trained , it may learn to reinforce existing patterns of hiring . Draws heavily on the ICO and RSA ( Royal Society for the encouragement of Arts , Manufac - tures and Commerce ) Citizens’ Juries work [ 5 ] Credit Scoring A new credit scoring company has decided to enter the market , and provide a new credit score . They would like their scores to be more accurate , and are considering using AI to assign people scores in the hopes of better predicting whether they will repay money . Using AI will allow for many more features to be input and also pick out a more accurate way to combine features to predict a person’s likelihood of repaying . AI advocates and experts think this will allow companies to lend more money to people who are likely to pay it back , but who might have been excluded in the traditional system . New , but fits within the generic ICO format Law Enforcement Imagine that local law enforcement in your city is under - resourced , sometimes causing it to fail to prevent crimes or leave crimes unsolved . The city government decides to procure a facial recognition system to make law enforcement more efficient . The system will allow law enforcement to identify people or verify their identities , for example , to find missing persons , match surveillance footage from a crime scene against a database of faces , or verify people’s identities when they are stopped by the police . New , but fits within the generic ICO format Table 3 . Excerpts from each of the four hypothetical scenarios . Manuscript submitted to ACM “A cold , technical decision - maker” : Can AI provide explainability , negotiability , and humanity ? 23 System A System B System C Rule - Based Conventional Machine Learning Deep Learning Stroke Diagnosis Accuracy 75 % ( nonspecialist doctor’s level ) 85 % ( expert doctor’s level ) 95 % ( beyond human level ) Transparency Full explanation of all the specific reasons that led to diagnosis Partial explanation describing features that played a role in di - agnosis No explanation Law Enforcement Accuracy 75 % ( accurate in good lighting and conditions ) 85 % ( highly accurate ) 95 % ( most accurate AI tech - nology ) Transparency Provides an ability for people who have had their faces matched to appeal ( request human review prior to formal decision - making ) , plus monthly reports for regular external oversight Provides an ability for people who have had their faces matched to appeal ( request human review prior to formal decision - making ) , but no process for regular exter - nal oversight No process to appeal ( re - quest human review prior to formal decision - making ) , no process for regular exter - nal oversight Table 4 . Descriptions of the three systems participants could choose among for the Stroke Diagnosis and Law Enforcement scenarios . For all scenarios , participants were offered three systems with 75 % , 85 % , and 95 % accuracy , where the most accurate system provided the least transparency . In all scenarios except Law Enforcement , transparency was accomplished through explanations ; for Law Enforcement we offered transparency options focused on appeal and oversight . Manuscript submitted to ACM