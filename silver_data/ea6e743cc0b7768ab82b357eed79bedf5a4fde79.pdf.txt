Submitted by Stefan Lattner Submitted at Institute of ComputationalPerception Supervisor and First Examiner Gerhard Widmer Second Examiner Geraint Wiggins Further Examiners Sepp Hochreiter Armin Biere Co - Supervisor Maarten Grachten October 2019 JOHANNES KEPLER UNIVERSITY LINZ Altenbergerstraße 69 4040 Linz , ¨Osterreich www . jku . at DVR 0093696 Modeling Musical Structure with Artiﬁcial Neural Networks Doctoral Thesis to obtain the academic degree of Doktor der technischen Wissenschaften in the Doctoral Program Technische Wissenschaften a r X i v : 2001 . 01720v1 [ c s . S D ] 6 J a n 2020 Eidesstattliche Erklärung Ich erkläre an Eides statt , dass ich die vorliegende Dissertation selbstständig und ohne fremde Hilfe verfasst , andere als die angegebenen Quellen und Hilfsmittel nicht benutzt bzw . die wörtlich oder sinngemäß entnommenen Stellen als solche kenntlich gemacht habe . Die vorliegende Dissertation ist mit dem elektronisch übermittelten Textdokument identisch . Ort , Datum Unterschrift I Abstract In recent years , artiﬁcial neural networks ( ANNs ) have become a universal tool for tackling real - world problems . ANNs have also shown great success in music - related tasks including music summarization and classiﬁcation , similarity estima - tion , computer - aided or autonomous composition , and automatic music analysis . As structure is a fundamental characteristic of Western music , it plays a role in all these tasks . Some structural aspects are particularly challenging to learn with current ANN architectures . This is especially true for mid - and high - level self - similarity , tonal and rhythmic relationships . In this thesis , I explore the application of ANNs to diﬀerent aspects of musical structure modeling , identify some challenges involved and propose strategies to address them . First , using probability estimations of a Restricted Boltzmann Machine ( RBM ) , a probabilistic bottom - up approach to melody segmentation is studied . Then , a top - down method for imposing a high - level structural template in music genera - tion is presented , which combines Gibbs sampling using a convolutional RBM with gradient - descent optimization on the intermediate solutions . Furthermore , I mo - tivate the relevance of musical transformations in structure modeling and show how a connectionist model , the Gated Autoencoder ( GAE ) , can be employed to learn transformations between musical fragments . For learning transformations in sequences , I propose a special predictive training of the GAE , which yields a representation of polyphonic music as a sequence of intervals . Furthermore , the applicability of these interval representations to a top - down discovery of repeated musical sections is shown . Finally , a recurrent variant of the GAE is proposed , and its eﬃcacy in music prediction and modeling of low - level repetition structure is demonstrated . III Kurzfassung In den letzten Jahren haben sich Künstliche Neuronale Netze ( KNNs ) zu einem universellen Werkzeug für unterschiedliche Problemstellungen entwickelt . Auch für musikalische Anwendungen erwiesen sie sich als nützlich . Sowohl für die Musik - analyse , die Klassiﬁkation von Musikstücken , für Musikähnlichkeitsbestimmung , aber auch in der automatischen Komposition wurden sie erfolgreich eingesetzt . Da Struktur eines der bestimmenden Merkmale von westlicher Musik ist , spielt sie auch für diese Tasks eine große Rolle . Manche Aspekte von Struktur in der Musik sind jedoch für KNNs schwer zu erlernen . Vor allem mit Aspekten von Wieder - holung und Variation sowie mit tonalen und rhythmischen Zusammenhängen , die sich über längere Zeiträume erstrecken , haben KNNs oft Probleme . In dieser Dis - sertation beschäftige ich mich mit der Anwendung von KNNs auf unterschiedliche Aspekte von musikalischer Struktur , untersuche die damit verbundenen Probleme und diskutiere mögliche Lösungen . Das erste Experiment behandelt das probabilistische bottom - up Segmentieren von Melodien mithilfe einer Restricted Boltzmann Machine ( RBM ) . Danach wird gezeigt , wie beim automatischen Generieren eines Musikstücks mit einer Convolu - tional RBM einer Struktur - Vorlage gefolgt werden kann . Dafür wird beim Gener - ieren die vorgegebene Struktur durch Optimierung der Lösung mittels des Gra - dientenverfahrens erzwungen . Dann erläutere ich die Relevanz vom Lernen von Transformationen für das Modellieren von musikalischer Struktur und zeige , dass ein Modell namens Gated Autoencoder ( GAE ) dafür gut geeignet ist . Dann stelle ich eine Methode vor , in einem “predictive Training” mit dem GAE Tonhöhen - Intervalle als Transformationen zwischen Noten in polyphoner Musik zu lernen . Die dabei gelernten Intervall - Repräsentationen können für das top - down Erkennen von wiederholten musikalischen Teilen verwendet werden . Abschließend erweit - ere ich den GAE mit rekurrenten Verbindungen , um ihm das Lernen von kurzen Wiederholungsstrukturen zu ermöglichen . V Acknowledgements First and foremost , I want to thank my supervisor Gerhard Widmer for his valuable support , the opportunity to work in his welcoming team , and to obtain my Ph . D . in such a scientiﬁcally productive and professional environment . This work would not have been possible without the great support of my co - supervisor Maarten Grachten , who has taught me the scientiﬁc tools of the trade from the ﬁrst day of my Ph . D . , and since that accompanied me through my sci - entiﬁc career . His exceptional eﬀectiveness , scientiﬁc precision and common sense made every collaboration with him an enlightening experience , and his unshakeable calmness when approaching deadlines kept me from panicking and helped me to keep focussed . I also want to thank Geraint Wiggins for his eﬀort and time to review this thesis , and for his work on computational creativity , perception and information dynamics in music , which was very inspiring for my own research . Furthermore , I am grateful for my colleagues at the Austrian Research Institute for Artiﬁcial Intelligence in Vienna and the Johannes Kepler University in Linz . It was a pleasure to be part of the team , from both a professional and a personal point of view . Special thanks go to Carlos Eduardo Cancino - Chacon with whom I shared the oﬃce for more than three years . He was not only a delightful colleague and friend , but he also was my advisor in mathematics , helping me to structure my ideas and to turn them into equations . Lastly , and most importantly , I would not have succeeded without my great family and friends , which helped me not to forget that there is a life outside of work . Their unconditional support and the great time we shared during the years provided the emotional basis for my professional progress . This work has been supported by the European Commission under the Future and Emerging Technologies ( FET ) programme within the Seventh Framework Pro - gramme ( FET grant number 610859 , project " Learning to Create " ) , and by the European Research Council ( ERC ) under the European Union’s Horizon 2020 Re - search and Innovation Programme ( grant agreement 670035 , project " Con Espres - sione " ) . VII Contents B Eidesstattliche Erklärung I List of Figures XIII List of Tables XIX 1 Introduction 1 1 . 1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 . 1 . 1 Musical Structure . . . . . . . . . . . . . . . . . . . . . . . . 1 1 . 1 . 2 Challenges in Structure Learning . . . . . . . . . . . . . . . . 2 1 . 2 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1 . 3 List of Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2 Bottom - Up Structure Analysis via Probability Estimation 11 2 . 1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2 . 2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2 . 2 . 1 Rule - Based Segmentation . . . . . . . . . . . . . . . . . . . . 14 2 . 2 . 2 Statistical and Information - Theoretic Segmentation . . . . . 15 2 . 2 . 3 Pseudo - Supervised Training . . . . . . . . . . . . . . . . . . . 16 2 . 3 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2 . 3 . 1 Relation to Other Statistical Models . . . . . . . . . . . . . . 17 2 . 3 . 2 Restricted Boltzmann Machines . . . . . . . . . . . . . . . . . 17 2 . 3 . 3 Approximation of the Probability of v . . . . . . . . . . . . . 18 2 . 3 . 4 Posterior Probabilities of Visible Units . . . . . . . . . . . . . 18 2 . 3 . 5 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2 . 3 . 6 Data Representation . . . . . . . . . . . . . . . . . . . . . . . 19 2 . 3 . 7 Information Content . . . . . . . . . . . . . . . . . . . . . . . 19 2 . 3 . 8 Pseudo - Supervised Optimization . . . . . . . . . . . . . . . . 20 2 . 3 . 9 Peak Picking . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2 . 4 Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2 . 4 . 1 Training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2 . 4 . 2 Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2 . 5 Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2 . 6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 IX 3 Top - Down Imposition of Higher - Level Structure 31 3 . 1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3 . 2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3 . 3 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 3 . 3 . 1 Convolutional Restricted Boltzmann Machine ( C - RBM ) . . . 36 3 . 3 . 2 Imposing Constraints with Gradient Descent . . . . . . . . . 39 3 . 4 Constrained Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . 44 3 . 4 . 1 Example Scheme and Details . . . . . . . . . . . . . . . . . . 45 3 . 4 . 2 Simulated Annealing . . . . . . . . . . . . . . . . . . . . . . . 48 3 . 5 Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 3 . 5 . 1 Training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 3 . 5 . 2 Data Representation . . . . . . . . . . . . . . . . . . . . . . . 51 3 . 5 . 3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 3 . 5 . 4 Quantitative Evaluation . . . . . . . . . . . . . . . . . . . . . 51 3 . 5 . 5 Qualitative Evaluation . . . . . . . . . . . . . . . . . . . . . . 54 3 . 6 Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 54 3 . 6 . 1 Quantitative Evaluation . . . . . . . . . . . . . . . . . . . . . 54 3 . 6 . 2 Qualitative Evaluation . . . . . . . . . . . . . . . . . . . . . . 56 3 . 7 Conclusion and Future Work . . . . . . . . . . . . . . . . . . . . . . 58 4 Learning Musical Structure by Learning Transformations 61 4 . 1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 4 . 2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4 . 3 Learning Transformations of Musical Material . . . . . . . . . . . . . 66 4 . 3 . 1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4 . 3 . 2 Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 4 . 3 . 3 Results and Discussion . . . . . . . . . . . . . . . . . . . . . . 75 4 . 3 . 4 Conclusion and Future Work . . . . . . . . . . . . . . . . . . 78 4 . 4 Learning Interval Representations from Polyphonic Music Sequences 80 4 . 4 . 1 Motivation for Modeling Relative Pitch Processing . . . . . . 81 4 . 4 . 2 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 4 . 4 . 3 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 4 . 4 . 4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 4 . 4 . 5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 4 . 4 . 6 Results and Discussion . . . . . . . . . . . . . . . . . . . . . . 92 4 . 4 . 7 Conclusion and Future work . . . . . . . . . . . . . . . . . . . 93 4 . 5 Learning Sequences of Intervals and Repetition Structure . . . . . . 95 4 . 5 . 1 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 4 . 5 . 2 Gated Autoencoder Pre - Training . . . . . . . . . . . . . . . . 98 4 . 5 . 3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 4 . 5 . 4 Conclusion and Future Work . . . . . . . . . . . . . . . . . . 104 X 5 Conclusion and Future Work 107 5 . 1 Information Theory and Structure Analysis . . . . . . . . . . . . . . 107 5 . 2 Learning Transformations and Invariances in Music . . . . . . . . . . 109 Bibliography 113 Curriculum Vitae of the Author 129 XI List of Figures 2 . 1 Seven examples of n - gram training instances ( n = 10 ) used as input to the RBM . Within each instance ( delimited by a dark gray border ) , each of the ten columns represents a note . Each column consists of four one - hot encoded viewpoints : | interval | , contour , IOI and OOI ( indicated by the braces on the left ) . The viewpoints are separated by horizontal light gray lines for clarity . The ﬁrst instance shows an example of noise padding ( in the ﬁrst six columns ) to indicate the beginning of a melody . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2 . 2 A BSP calculated from 11 - grams . The upper ﬁgure shows the notes of 9 measures ( 36 beats ) of a German folk song . The lower ﬁgure shows a BSP ( i . e . , IC ) used for segmentation . The correct segmen - tation ( ground truth ) is depicted as vertical grey bars at the top of the ﬁgures , segment boundaries found by our model are shown as dashed vertical lines . Note that the BSP has particularly high peaks at rests and large intervals . However , the segment boundary found at beat 28 does not have any of those cues and was still correctly classiﬁed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2 . 3 Schematic depiction of the pseudo - supervised optimization . Note that the n - gram is linearized before it is fed into the Feed - Forward Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2 . 4 F1 scores for diﬀerent N - gram lengths and methods . The or - ange horizontal line marks the baseline of the probabilistic IDyOM model [ Pearce et al . , 2010b ] . . . . . . . . . . . . . . . . . . . . . . . . 26 2 . 5 The eﬀect of pseudo - training on estimated IC values ; Line seg - ments connect IC values estimated directly from the probabilistic model ( RBM10 + DO ) with the corresponding IC values after pseudo - training ( RBM10 + DO + PS ) ; Green lines indicate music events that mark a segment boundary , red lines indicate those that do not . . . . 29 XIII 3 . 1 Constrained sampling using an existing piece x as a structure tem - plate . A randomly initialized sample v is alternately updated with Gibbs sampling ( GS ) and gradient descent ( GD ) . In GD , the error φ ( x , y ) between structural features of x and v is lowered , in GS the training data distribution is approximated . The Convolutional RBM consists of visible layer v and hidden layer h . The ﬁlter W k is shared among all units in feature map h k . Depicted equations are also given in Section 3 . 3 . 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 3 . 2 Illustration of a C - RBM with strided convolution ( using stride d ) in the time dimension t of a music piece v ∈ R T × P in two - dimensional piano roll representation using K one - dimensional feature maps h k where all units in a map share their weights W k ∈ R R × P and their bias b k ( bias not depicted in the illustration ) . . . . . . . . . . . . . . 39 3 . 3 Depiction of calculating the self - similarity matrix s ( z ) ∈ R I × J using convolution . A music piece in piano roll representation z ∈ [ 0 , 1 ] T × P is horizontally tiled , and those tiles are used as ﬁlters for a convo - lution with z . The response for a single ﬁlter constitutes a single line in the resulting self - similarity matrix . Low to high response is depicted in a range from darker blue to brighter red colors . . . . . . 41 3 . 4 Example of key estimation vectors over time . k ( z ) maj represent esti - mations for 12 possible major keys and k ( z ) min represent estimations for 12 possible minor keys , where the pitch classes constituting the tonic are ordered from the top to the bottom . Bright pixels represent high strength , and dark pixels represent low strength of the corre - sponding key . For example , the upper most line in k ( z ) maj represents the estimation strength of the C major key over time , the third line represents the strength of the D major key , etc . . . . . . . . . . . . . 43 3 . 5 Relative ( standardized ) onset frequencies ρ 0 ( z ) on bar positions of a music piece as obtained from Equation 3 . 13 . . . . . . . . . . . . . . . 44 3 . 6 Illustration of constrained sampling . ( 1 ) Template piece , ( 2 ) Interme - diate sample after the GD phase , ( 3 ) Sample after the GS phase . Fig - ures in each group : ( a ) Piano roll representation , ( b ) Self - similarity matrix , ( c ) Onset distribution in 4 / 4 meter , ( d ) Keyscape ( see Sec - tion 3 . 5 . 5 . 1 for an explanation ) . After the GD phase ( 2 ) , the target higher - level properties imposed as constraints are relatively well ap - proximated . Due to limited training data and the stochastic nature of Gibbs sampling , after the GS phase the higher - level properties are more dissimilar again . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 XIV 3 . 7 Standardised cost , free energy and their mean in a constrained sam - pling process over 250 iterations . Periods of constant cost ( horizontal line segments ) in later iterations are a result of Simulated Annealing , where some unfavorable solution candidates are rejected . . . . . . . . 48 3 . 8 Inﬂuence of Gibbs sampling ( GS ) and gradient descent ( GD ) on free energy F ( v ) ( see Equation 3 . 1 ) and cost φ ( x , v ) ( see Equation 3 . 5 ) . Using only GS results in low free energy but relatively high cost . Using only GD , the cost is very low , but the free energy is high . When using GS and GD , both methods compete , resulting in a trade - oﬀ between low cost and low free energy although we choose enough GS steps in the GS phase to always return to a “meaningful” , low free energy state . For reference we test against random uniform noise , resulting in very high free energy and cost . For each cluster , 50 data points were generated with the trained C - RBM model ( for GS ) and the cost function ( for GD ) used in our experiment ( see Section 3 . 5 ) . 49 3 . 9 Box plot showing Average Information Rates for 34 original Mozart piano sonatas , 102 C - RBM samples with structure constraints , 102 C - RBM samples without constraints , 102 samples from an RNN - RBM and 102 samples from a GRU - RBM . Whiskers show standard deviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3 . 10 Template piece ( 1 ) , Constrained samples ( 2 to 5 ) and an uncon - strained sample as baseline ( 6 ) . Figures in each group : ( a ) Piano roll representation , ( b ) Self - similarity matrix , ( c ) Onset distribution in 4 / 4 meter , ( d ) Keyscape . By constrained sampling , the template piece’s self - similarity and tonal structure , as well as the onset dis - tributions , are transferred to the generated solutions 2 to 5 . The unconstrained sample ( 6 ) at the bottom was sampled without con - straints , and thus does not reﬂect the structure of the template piece . 55 4 . 1 Beginning of the “Rondo in D major” ( K . 485 ) by W . A . Mozart in Western music notation and in a piano roll representation , where transformation functions f n eﬀect diatonic transpositions ( best viewed in color ) . In the piano roll ( bottom ) , green horizon - tal lines mark the diatonic pitches in the scale of D major . . . . . . . 62 4 . 2 Schematic illustration of the Gated Autoencoder . . . . . . . . . . . . 68 4 . 3 Some complementary ﬁlters manually selected from U ( top ) and V ( bottom ) learned from transposed pairs of musical n - grams . . . . . . 69 XV 4 . 4 First two principal components ( a ) and second and third principal component ( b ) of mappings resulting from unsupervised learning of pairs exhibiting the chromatic transposition ( TransC ) relation . Points are colored , and cluster centers are named according to the diﬀerent classes within the TransC relation type ( i . e . distances of note shifts ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 4 . 5 Results of the analogy - making task . Transformations are inferred by the GAE from pairs of 8 - grams ( A ) and applied to new n - grams , not seen during training ( B , left parts ) to generate counterparts with analogous transformations ( B , right parts ) , where the level of black - ness indicates the certainty of the model that the respective note is part of the transformed result : 1 ) Chromatic transposition , 2 ) dia - tonic transposition , 3 ) tempo change ( halftime ) , and 4 ) retrograde . Green horizontal lines mark scale notes in diatonic transposition , for visual guidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 4 . 6 Schematic illustration of the Gated Autoencoder ( GAE ) architecture used in the experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 83 4 . 7 Some ﬁlter pairs ∈ { U , V } of a GAE trained on polyphonic Mozart piano pieces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 4 . 8 Distance matrix of cluster centers of intervals represented in map - ping space . Darker cells indicate higher distances between respective clusters , brighter cells indicate closeness . . . . . . . . . . . . . . . . . 88 4 . 9 Symbolic music and corresponding self - similarity matrix calculated from transposition - invariant mapping codes . Warmer colors indicate similarity , colder colors indicate dissimilarity . . . . . . . . . . . . . . 90 4 . 10 Absolute sensitivity of the model when looking backwards on the temporal context , averaged over the whole dataset . . . . . . . . . . . 92 4 . 11 Schematic illustration of the proposed Recurrent Gated Autoencoder architecture . Arrows represent weight matrices , rounded rectangles represent vectors . The triangles depict the Hadamard product . The speciﬁcs of the Gated Recurrent Unit are omitted for better clarity . . 98 4 . 12 Distribution of precisions for continuation of highly structured se - quences in the test set of size 150 . The median is marked with a orange line , the boxes indicate the interquartile range , and circles indicate outliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 XVI 4 . 13 Generated structure schemes and hidden unit activations of the RGAE and the RNN models after input of a primer indicating the { − 4 , + 8 , − 4 , + 8 , . . . } scheme , realized with melodies of length 16 not contained in the train set . Black notes indicate correct continuation , green notes indicate false negatives , red notes indicate false posi - tives . Hidden units activations of the RNN are pruned due to space limitation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 XVII List of Tables 2 . 1 Results of the model comparison , ordered by F1 score . . . . . . . . . 28 3 . 1 Relative weightings w d of the terms used in the GD objective function φ ( x , v ) ( see Section 3 . 3 . 2 ) . . . . . . . . . . . . . . . . . . . . . . . . . 50 4 . 1 Confusion Matrix for classiﬁer FFNN trained on representations of GAE TransD 128 / 64 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 4 . 2 Mis - classiﬁed rates ( in percent ) from the classiﬁcation Feed - Forward Neural Network trained on representations of the Restricted Boltz - mann Machine ( RBM ) and the Gated Autoencoder ( GAE ) in diﬀer - ent architecture sizes and for diﬀerent transformation types . Random denotes the random guessing baseline dependent on the number of classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 4 . 3 Reconstruction cross - entropies for the Restricted Boltzmann Ma - chine ( RBM ) and the Gated Autoencoder ( GAE ) in diﬀerent sizes and for diﬀerent transformation types . . . . . . . . . . . . . . . . . . 77 4 . 4 Results of the k - nn classiﬁcation in the mapping space and in the input space for the original symbolic data and data randomly trans - posed by [ − 24 , 24 ] semitones . “All” is a lower bound ( always predict all intervals ) , “None” returns the empty set . . . . . . . . . . . . . . . 89 4 . 5 Diﬀerent precision , recall and f - scores ( details on the measures are given in [ Collins , 2017 ] ) of diﬀerent methods in the Discovery of Repeated Themes and Sections MIREX task , for symbolic music and audio . The F 3 score constitutes a summarization of all measures . * Results of VMO symbolic and VMP deadpan from [ Wang et al . , 2015 ] , SIARCT - CFP from [ Collins et al . , 2013 ] , COSIATEC from [ Meredith , 2013 ] and VMO deadpan from [ Nieto and Farbood , 2014 ] . 91 4 . 6 Cross - Entropies of the 10 - fold cross validation in the prediction task for diﬀerent data sets and diﬀerent models . When combining the RGAE with an absolute pitch model ( i . e . , RNN , RTDRBM ) , results improve substantially . The results suggest that absolute and relative pitch models are complementary in the aspects they learn about music and can be eﬀectively used in an ensemble method . . . . . . . 101 XIX 4 . 7 The diﬀerent relative transposition schemes used in the “learning copy - and - transpose operations” experiment . . . . . . . . . . . . . . . 102 4 . 8 Results of the “learning copy - and - transpose operations” task . Aver - age precision ( Pr ) , percentage of continuations above 99 % precision , cross - entropy ( CE ) and number of parameters of the respective model . 103 XX 1 Introduction 1 . 1 Motivation 1 . 1 . 1 Musical Structure Music is a highly structured artifact . Music as we know it 1 consists of ( notated and / or sounding ) events that are organized along dimensions such as pitch , time , loudness , and timbre , via a complex network of relationships ( e . g . , time and dura - tion ratios , melodic intervals , scales ) . From this interplay of musical events arise higher - level structural concepts such as texture , melody , harmony , rhythm , group - ing and phrase structure , repetition and variation , motivic relationships , style , or genre . In short , music is organized sound [ Varèse and Wen - Chung , 1966 ] , with organization or structure apparent at many levels . Fundamental to this is the human perception with its abstraction capabilities . Octave equivalence and , more generally , the ability to abstract away from absolute pitch permit us to perceive musical contents as being “the same” or related even when they are transposed in pitch or , in fact , modiﬁed via a number of other types of musical transformations ( such as ornamentation , diminution , inversion , etc . ) . Also underlying this is our tendency to organize musical events in a network or hierarchy of relative importance , with some events being perceived as subordinate to others , at multiple levels [ Lerdahl and Jackendoﬀ , 1983 ] . This abstraction ability permits us to recognize musical passages as similar or related even in the face of surface diﬀerences . In the literature , the term musical structure is a general term used for diﬀerent relationships between musical events and features . From an information - theoretic point of view , and in order to be able to talk about musical structure in machine learning and statistics , we can deﬁne it very broadly as a non - random organization of musical events and their properties . This means that one can estimate the state of an event when the states of some other events are observed . The non - randomness of music is key to a pleasant listening experience — in fact , auditory stimuli without structure are hardly identiﬁed as being music at all . In this work , I will tackle the problems of learning , analysis , and generation of musical textures considering the dependencies in diﬀerent hierarchical levels . In particular , I will focus on group - ing and segmentation , tonal relationships , meter , self - similarity and transformation 1 In this thesis , we only consider Western tonal music . 1 ( i . e . , variation ) . I will discuss reasons why modeling such dependencies with ma - chine learning ( ML ) methods is challenging , and I will contribute some ﬁndings to amend the situation . 1 . 1 . 2 Challenges in Structure Learning In computer science , musical structure is approached with diﬀerent intentions . On the one hand , in music information retrieval ( MIR ) , a typical goal is to analyze the structure of particular features of interest . For example , harmonic analysis unveils the chord structure , and self - similarity analysis provides insight into the musical form of a song . Such tasks focus on speciﬁc musical features and can thus be solved at a manageable cost . On the other hand , music prediction and music generation tasks pose a much more challenging problem . The models typically applied to this task are called sequence models ( or language models ) . They are usually trained in a so - called self - supervised fashion , to predict events in the immediate future ( typically the next time step or the next event ) given the past . The training objective in such models is to maximize the likelihood of the data under the model . Thereby , the model has to pick up on the mid - and high - level structure of music , while being explicitly trained to predict elementary low - level events . However , the capacity of current models and training methods is often too limited to learn higher - level structure by sheer exposition to elementary events . In music generation , this poses a challenging situation , because humans have a solid expectation regarding speciﬁc norms in the musical structure and do not readily tolerate violations thereof . Ideally , a model should , therefore , cover all the relevant structural characteristics in order to generate convincing music . A particular case of musical structure learning with computer models is self - similarity ( i . e . , repetition and variation ) . While higher - level features like har - mony or rhythm , can be determined by local analysis of the texture , modeling self - similarity requires a comparison between at least two musical entities . This results in two fundamental problems : First , self - similarity can exist between any musical entities in a piece , and a pairwise comparison between all of them results in a high computational cost . Second , it is not straight - forward to determine what similarity means in a musical context . The perceived similarity is highly subjective and is , therefore , an insuﬃcient measure for an objective description of similar - ity in musical structure . The approach I am taking is that musical textures can be considered similar when they can be transferred into each other by transfor - mations which occur regularly in a corpus . Some well - known examples thereof are retrograde , inversion , insertion and deletion , tempo change , chromatic and diatonic transposition , as well as the identity transform , denoting a simple copy operation of a particular event or feature . However , there may be additional , yet undeﬁned 2 transformations musical textures can undergo in musical variation . Ideally , such transformations should , therefore , be learned using self - supervised training , as it is not possible or desired to pre - deﬁne them exhaustively . In this thesis , it is shown that the architecture of Gated Autoencoders ( GAEs ) foster unsupervised learning of musical transformations . By discussing the diﬀer - ent architectures , it will also become clear that GAEs diﬀer fundamentally from conventional prediction models . The most striking diﬀerence of GAEs is that they represent transformations explicitly in their latent space , rather than representing the characteristics of the data itself . The way transformation learning with GAEs is achieved ( i . e . , by using three - way multiplicative interactions ) leads to surpris - ingly compact models . This “transformational” ( i . e . , content - invariant ) view on musical material , which is provided by the latent space of GAEs , has proven ad - vantageous for several tasks . The experiments conducted in Chapter 4 show that GAEs are useful for music analysis and prediction and that the concept of learning transformations is vital for eﬀective modeling of repetition and variation . In some recent developments , the problem of learning musical structure is tackled with powerful sequence models , so - called Transformers [ Huang et al . , 2019 ] . ( Self - ) attention mechanisms [ Vaswani et al . , 2017 ] help Transformers to “focus” on speciﬁc subsequences or important positions in a musical piece . The material deemed to de - serve focus is then used for prediction . The inner working of Transformers involves re - ordering of data , which relates , to some degree , to copy operations necessary for modeling repetitions . However , these models learn to generalize to musical rela - tions only through redundancy , rather than through learning of transformations . For example , in the diatonic transposition operation , the input - to - output pathways of diﬀerent transposition distances and diﬀerent keys would be represented diﬀer - ently in a Transformer . Therefore , substantial model sizes and amounts of training data are necessary to obtain some degree of mid - and high - level structure in the generated material . Also , data augmentation is usually used during training , for example by transposing or time - stretching musical input sequences . I believe that modeling transformations in music as a complementary approach to conventional data representation can allow for more compact models which can generalize on fewer data . In particular repetition and variation , but also tonal and rhythmic relationships could be better learned from data when utilizing a transformational approach . 3 1 . 2 Outline This thesis is based upon six publications , which are grouped into three chapters of similar topics . In Chapter 2 , a connectionist n - gram model is employed to iden - tify segment boundaries in melodies based on the information content ( IC ) of note events . Chapter 3 introduces a method to impose structural constraints in mu - sic generation using a generative connectionist model and diﬀerentiable constraint functions . Finally , in Chapter 4 , I report on the successive adaption of a GAE architecture for learning musical transformations to musical sequence modeling . By learning interval representations , the GAE yields better accuracy than base - line methods in a prediction task and is also able to perform copy - and - transpose operations . Furthermore , it is shown that the interval representations themselves are useful for transposition - invariant detection of repeated sections in polyphonic music . In summary , the contributions are structured as follows : • Bottom - Up Structure Analysis of monophonic melodies via probability estimation ( see Chapter 2 ) . • Top - Down Imposition of Higher - Level Structure in polyphonic music generation , using soft - constraints ( see Chapter 3 ) . • Learning Musical Structure by learning transformations ( see Chapter 4 ) : – Learning Transformations in Music ( see Section 4 . 3 ) . – Learning Transposition - Invariant Interval Representations from both , symbolic music and audio , and employing these representations for top - down structure analysis ( see Section 4 . 4 ) . – Learning to Predict Musical Sequences —with the potential to learning repetition and variation in music—using a recurrent sequence model operating on learned interval representations ( see Section 4 . 5 ) . In the following , I shall brieﬂy introduce each chapter and discuss its relevance for musical structure modeling . Chapter 2 : Bottom - up Structure Analysis The objective of Chapter 2 is the unsupervised segmentation of musical sequences ( i . e . , estimating phrase markers as annotated in a collection of monophonic folk song melodies ) . The level of surprise when observing an event in a sequence can be quantiﬁed by the information theoretic measure information content ( IC ) . Prior research shows that the IC can act as a kernel for segment boundaries [ Pearce et al . , 2010a ] . This ﬁnding indicates that the conditional probabilities within a musical 4 segment ( i . e . , subsequences perceived as coherent “chunks” by a human listener ) tend to be higher ( i . e . , have lower IC ) than those between segments . Considering the evidence that IC is a useful kernel for segment boundary detection , it follows that an improved IC estimation should result in an improved segmentation performance . The probabilistic model used so far for the IC estimation in this task is a so - phisticated variable - order Markov model [ Pearce et al . , 2010a ] . I show that an im - provement can be reached by instead employing a connectionist n - gram model ( i . e . , a Restricted Boltzmann Machine ( RBM ) ) for the IC estimation . Furthermore , a feed - forward neural network is trained to predict the IC values based on the musical texture , as opposed to using the IC estimation directly . After applying that further optimization step ( denoted as “pseudo - supervised training” , see Section 2 . 3 . 8 ) , this unsupervised method is on a par with an expert system [ Cambouropoulos , 2001 ] , explicitly designed to detect segment boundaries . The results strengthen the ﬁnding that IC can be used for bottom - up structure analysis and support the hypothesis of Pearce et al . [ 2010a ] that there exists a relationship between expectation and grouping in auditory perception . Chapter 3 : Top - Down Imposition of Higher - Level Structure In Chapter 3 , the focus is on the generation of structured , polyphonic music . In music generation , it is vital that the predictions of elementary events are con - ditioned on representations of higher - level characteristics , in order to reproduce structural characteristics of the training data . Such characteristics include , among others , tonality , meter , rhythmic structure , and repetition structure . If higher - level characteristics are not adequately represented by the model , the generated output frequently drifts oﬀ into diﬀerent keys , shows inconsistent harmonic successions , or has implausible and continuously changing meter . As stated above , self - supervised sequence models have to infer such higher - level structural characteristics while be - ing trained to predict elementary events . In this regard , current sequence models are still limited , which becomes evident in the quality of the results in ( polyphonic ) music generation tasks without additional guidance ( i . e . , without additional in - puts , like meter information or chord symbols ) 2 . In particular , learning repetition structure is very challenging , because it is a concept which cannot be modeled by conditioning events on abstract symbols . For example , given a speciﬁc chord symbol , it is possible to derive a distribution over note events when analyzing a corpus of musical works . In contrast , a model has to learn a copy operation in order to realize an exact repetition , which is fundamentally diﬀerent from deriving a distribution given an abstract symbol . Therefore , due to limited capacity , limited training data , or simply because of the delicate nature of some musical concepts , 2 Very recent models [ Huang et al . , 2019 ] already show a better capacity in reproducing structural characteristics in a self - supervised setting . 5 it is challenging for self - supervised sequence models to learn the diﬀerent forms of higher - level structure . In the generation of lower - level structure , however , the performance of sequence models is considerably better . Therefore , in Chapter 3 , I restrict the learning task solely to the musical lower - level structure . For that , I employ a Convolutional Re - stricted Boltzmann Machine ( C - RBM ) with limited ( i . e . , local ) receptive ﬁelds to learn the local statistics of polyphonic Mozart piano sonatas . In generation , the C - RBM is combined with diﬀerentiable functions deﬁning higher - level structural properties ( i . e . , meter , tonal structure , and self - similarity structure ) . By minimiz - ing these functions during sampling , they act as external soft - constraints on the generated material . That way , higher - level structural properties can be transferred from an existing work to a newly generated piece of music . The idea of constrained sampling is not new ( e . g . , in [ Pachet and Roy , 2011 ] hard - constraints are used , and in [ Hadjeres and Nielsen , 2018 ] a Recurrent Neural Network ( RNN ) going backward in time “collects” unary constraints which are then applied in forward generation by another RNN ) , but it is a novel contribution to use diﬀerentiable functions as global soft - constraints and impose them in Gibbs sampling for music generation . The method could apply to other models using Gibbs sampling for music generation ( e . g . , to RNNs , as used for music generation by Gibbs sampling in [ Hadjeres et al . , 2017 ] ) . Chapter 4 : Learning Musical Structure In Chapter 4 , I tackle some critical problems in learning musical characteristics with sequence models . As stated before , musical structure implies many interdependen - cies between elementary and abstract musical events . Some of these dependencies can be represented as transformations between data , which leads to more compact representations than representing the data itself . This observation motivates the approach taken in Section 4 . 3 , namely to represent musical variations as sets of transformations . Some experiments performed in Chapter 4 show that learning transformations is challenging for common connectionist models ( see Section 4 . 3 . 2 and Section 4 . 5 . 3 ) . We successfully apply GAEs to learning musical transformations and use them ( including some variants , speciﬁcally developed for musical problems ) for several dif - ferent tasks . These include learning transformations between musical n - grams ( see Section 4 . 3 ) , learning interval representations ( as transformations between pitches , see Section 4 . 4 ) , and sequence prediction based on the learned interval representa - tions ( see Section 4 . 5 ) . In the following , these tasks are described in more detail . 6 Section 4 . 3 : Learning Transformations in Music In this Section , I test the performance of GAEs to learn transformations between n - grams of polyphonic , symbolic music in piano - roll representation . The performed experiments constitute a proof - of - concept to learning some common musical trans - formations , like chromatic and diatonic transposition , retrograde , and halftime . Pairs of n - grams are constructed which obey such transformations and the perfor - mance of two architectures in learning the transformations from these n - grams is tested . More precisely , I test an RBM and a GAE and ﬁnd that GAEs show a much better performance . As a result , transformations are meaningfully organized in the latent space of the GAE , and the learned representations can be used to transform test data in an analogy - making task . Section 4 . 4 : Learning Transposition - Invariant Interval Representations In Section 4 . 4 , a GAE is trained in a predictive setting ( as opposed to a symmetric setting employed in Section 4 . 3 ) . To that end , the GAE is trained to predict a single time - step of music in piano - roll representation based on a short temporal context ( similar to the setup used in Chapter 2 ) . By predicting the conﬁguration of pitches based on some temporal context using transformation learning , I obtain representations which behave like interval representations . This is because intervals can be considered shift transformations between single notes . The resulting rep - resentations show musically plausible organization and transposition - invariance . I show that such properties are relevant for tasks like transposition - invariant repeated section discovery in ( polyphonic ) symbolic music and audio . Section 4 . 5 : Learning to Predict Musical Sequences In Section 4 . 5 , the predictive GAE proposed in Section 4 . 4 is combined with an RNN , to obtain the Recurrent Gated Autoencoder ( RGAE ) , internally operating on learned interval representations . This internal working on interval representa - tions resembles the relative pitch processing of humans . The RGAE yields improved prediction accuracy in a music sequence learning task , assumingly because inter - val representations reduce sparsity in the data and therefore help generalization of the model . An additional advantage of the RGAE is that it can learn copy - and - transpose operations in a self - supervised sequence prediction task ( see Section 4 . 5 . 3 . 2 ) . Compared to a baseline RNN , the RGAE performs superior in that task . The results suggest that the RGAE is promising for learning musical repetition structure and variations , which is a challenging problem for current sequence mod - els . 7 1 . 3 List of Publications Chapter 2 , 3 and 4 of this thesis are based on the following peer - reviewed publica - tions . Chapter 2 • Stefan Lattner , Maarten Grachten , Kat Agres , and Carlos Eduardo Cancino Chacón . Probabilistic segmentation of musical sequences using restricted Boltzmann machines . In Proceedings of the 5th International Conference on Mathematics and Computation in Music , MCM 2015 , London , UK , June 22 - 25 , 2015 , pages 323 – 334 , 2015b . doi : 10 . 1007 / 978 - 3 - 319 - 20603 - 5 _ 33 . URL https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 20603 - 5 _ 33 • Stefan Lattner , Carlos Eduardo Cancino Chacón , and Maarten Grachten . Pseudo - supervised training improves unsupervised melody segmentation . In Proceedings of the Twenty - Fourth International Joint Conference on Artiﬁcial Intelligence , IJCAI 2015 , Buenos Aires , Argentina , July 25 - 31 , 2015 , pages 2459 – 2465 , 2015a . URL http : / / ijcai . org / Abstract / 15 / 348 Chapter 3 • Stefan Lattner , Maarten Grachten , and Gerhard Widmer . Imposing higher - level structure in polyphonic music generation using convolutional restricted Boltzmann machines and constraints . Journal of Creative Music Systems , 2 ( 2 ) , 2018c . URL https : / / www . jcms . org . uk / article / id / 522 / Chapter 4 • Stefan Lattner and Maarten Grachten . Learning transformations of musical material using gated autoencoders . In Proceedings of the 2nd Conference on Computer Simulation of Musical Creativity , CSMC 2017 , Milton Keynes , UK , September 11 - 13 , 2017 , 2017b . URL https : / / drive . google . com / open ? id = 0B12H6lLCqzLWdVgwMFZON25UM3c • Stefan Lattner , Maarten Grachten , and Gerhard Widmer . Learning transposition - invariant interval features from symbolic music and audio . In Proceedings of the 19th International Society for Music Information Re - trieval Conference , ISMIR 2018 , Paris , France , September 23 - 27 , 2018a . URL http : / / ismir2018 . ircam . fr / doc / pdfs / 172 _ Paper . pdf • Stefan Lattner , Maarten Grachten , and Gerhard Widmer . A predictive model for music based on learned interval representations . In Proceedings 8 of the 19th International Society for Music Information Retrieval Confer - ence , ISMIR 2018 , Paris , France , September 23 - 27 , 2018b . URL http : / / ismir2018 . ircam . fr / doc / pdfs / 179 _ Paper . pdf Related Publications The following publications are related , but not part of this thesis . • Stefan Lattner , Monika Dörﬂer , and Andreas Arzt . Learning complex basis functions for invariant representations of audio . In Proceedings of the 20th In - ternational Society for Music Information Retrieval Conference , ISMIR 2019 , Delft , The Netherlands , November 4 - 8 , 2019 • Stefan Lattner and Maarten Grachten . High - level control of drum track gener - ation using learned patterns of rhythmic interaction . In 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics , WASPAA 2019 , New Paltz , NY , USA , October 20 - 23 , 2019 . IEEE , 2019 • Andreas Arzt and Stefan Lattner . Audio - to - score alignment using transposition - invariant features . In Proceedings of the 19th International Soci - ety for Music Information Retrieval Conference , ISMIR 2018 , Paris , France , September 23 - 27 , 2018 , pages 592 – 599 , 2018 . URL http : / / ismir2018 . ircam . fr / doc / pdfs / 166 _ Paper . pdf • Stefan Lattner and Maarten Grachten . Improving content - invariance in gated autoencoders for 2d and 3d object rotation . CoRR , abs / 1707 . 01357 , 2017a . URL http : / / arxiv . org / abs / 1707 . 01357 • Kat Agres , Carlos Cancino , Maarten Grachten , and Stefan Lattner . Harmon - ics co - occurrences bootstrap pitch and tonality perception in music : Evidence from a statistical unsupervised learning model . In Proceedings of the 37th Annual Meeting of the Cognitive Science Society , CogSci 2015 , Pasadena , California , USA , July 22 - 25 , 2015 . cognitivesciencesociety . org , 2015 . URL https : / / mindmodeling . org / cogsci2015 / papers / 0018 / index . html • Carlos Eduardo Cancino Chacón , Stefan Lattner , and Maarten Grachten . Developing tonal perception through unsupervised learning . In Hsin - Min Wang , Yi - Hsuan Yang , and Jin Ha Lee , editors , Proceedings of the 15th In - ternational Society for Music Information Retrieval Conference , ISMIR 2014 , Taipei , Taiwan , October 27 - 31 , 2014 , pages 195 – 200 , 2014 . URL http : / / www . terasoft . com . tw / conf / ismir2014 / proceedings / T036 _ 230 _ Paper . pdf 9 2 Bottom - Up Structure Analysis via Probability Estimation A salient characteristic of human perception of music is that musical events are perceived as being grouped temporally into structural units such as phrases or mo - tifs . Segmentation of musical sequences into structural units is a topic of ongoing research , both in cognitive psychology and music information retrieval . Computa - tional models of music segmentation are typically based either on explicit knowledge of music theory and human perception or on statistical and information - theoretic properties of musical data . The former , rule - based approach has been found to bet - ter account for ( human annotated ) segment boundaries in music than probabilistic methods , although the statistical model proposed in [ Pearce et al . , 2010b ] per - forms almost as well as state - of - the - art rule - based approaches . In this chapter , we propose a new probabilistic segmentation method , based on Restricted Boltzmann Machines ( RBMs ) . By sampling , we determine a probability distribution over a subset of visible units in the model , conditioned on a conﬁguration of the remain - ing visible units . We apply this approach to an n - gram representation of melodies , in order to estimate the conditional probability of a note given its n - 1 predeces - sors . From this estimation , we calculate the information content , which is used in combination with a threshold to determine the location of segment boundaries . Furthermore , we demonstrate that , remarkably , a substantial increase in segmenta - tion accuracy can be obtained by not using information content estimates directly , but rather in a bootstrapping fashion . More speciﬁcally , we use information content estimates as a target for a feed - forward neural network that is trained to estimate the information content directly from the data . We hypothesize that the improved segmentation accuracy of this bootstrapping approach may be evidence that the generative model provides noisy estimates of the information content , which are smoothed by the feed - forward neural network , yielding more accurate information content estimates . Comparative evaluation shows that probabilistic segmentation ( on a dataset of simple , monophonic melodies ) is now on a par with state - of - the - art rule - based models . 11 2 . 1 Introduction Across perceptual domains , grouping and segmentation mechanisms are crucial for our disambiguation and interpretation of the world . Both top - down , schematic pro - cessing mechanisms and bottom - up , grouping mechanisms contribute to our ability to break the world down into meaningful , coherent “chunks” [ Gobet et al . , 2001 ] . Indeed , a salient characteristic of human perception of music is that musical se - quences are not experienced as an indiscriminate stream of events , but rather as a sequence of temporally contiguous musical groups or segments . Elements within a group are perceived to have a coherence that leads to the perception of these events as a structural unit ( e . g . , a musical phrase or motif ) . A prominent example of chunking has been shown in the context of chess [ Gobet and Simon , 1998 ] , where increased skill level is associated with more eﬃcient chunking of information about board conﬁgurations . Moreover , chunking is involved more generally in visual [ Mc - Collough and Vogel , 2007 ] and acoustic / speech processing [ Baddeley , 1966 ] tasks . Just as in speech , perception in terms of meaningful constituents is an essential trait of music cognition . This is immanent in the ubiquitous notion of constituent structure in music theory . The origin and nature of this sense of musical coherence , or lack thereof , which gives rise to musical grouping and segmentation has been a topic of ongoing re - search . A prominent approach from music theory and cognitive psychology has been to apply perceptual grouping mechanisms , such as those suggested by Gestalt psychology , to music perception . Gestalt principles , such as the laws of proximity , similarity , and closure , were ﬁrst discussed in visual perception [ Wertheimer , 1938 ] , and have been successfully applied to auditory scene analysis [ Bregman , 1990 ] and inspired theories of music perception [ Meyer , 1956 , Narmour , 1990 , Lerdahl and Jackendoﬀ , 1983 ] . Narmour’s Implication - Realization theory [ Narmour , 1990 ] , for example , uses measures of pitch proximity and closure that oﬀer insight into how lis - teners perceive the boundaries between musical phrases . This type of theory - driven approach has given rise to various rule - based computational models of segmenta - tion . This class of models relies upon the speciﬁcation of one or more principles according to which musical sequences are grouped . An alternative account of grouping and segmentation is based on the intuition that the distribution , or statistical structure of the sensory information , has an es - sential eﬀect on how we perceive constituent structure . This idea has been explored for diﬀerent areas , such as vision [ Glicksohn and Cohen , 2011 ] , speech [ Brent , 1999 ] , and melody perception [ Pearce et al . , 2010a ] . The key idea is that the sensory in - formation that comprises a chunk is relatively constant , whereas the succession of chunks ( which chunk follows which ) is more variable . In information - theoretic terms , this implies that the information content ( informally : unexpectedness ) of events within a chunk is lower than that of events that mark chunk boundaries . As 12 a side note on vocabulary : We will use the term segment , rather than chunk in the rest of this chapter , to express that we take an agnostic stance toward the precise nature of constituents , and instead focus on their demarcation . While Gestalt principles are sometimes rather abstractly deﬁned laws , informa - tion theory has some potential to describe and quantify such perceptive phenomena formally . The Gestalt idea of grouping based on " good form " ( i . e . , Prägnanz ) , for example , has an information theoretic counterpart in the work of von Helmholtz [ 2005 ] , where human vision is assumed to resolve ambiguous perceptive stimuli by preferring the most probable interpretation . Also , it is intuitively clear that in most real - world scenarios , the uncertainty about observing speciﬁc events ( i . e . , the entropy ) tends to increase with higher distances from already observed events in any relevant dimension . Thus , while a direct link between the two paradigms is beyond dispute , the question remains which of it is more parsimonious and might have given rise for the other to emerge as a perceptual mechanism . Prior work has shown that the information content of music events , as estimated from a generative probabilistic model of those events , is a good indicator of seg - ment boundaries in melodies [ Pearce et al . , 2010b ] . The statistical model proposed in [ Pearce et al . , 2010b ] ( IDyOM ) is capable of much better segmentations than simpler statistical models based on di - gram transition probabilities and point - wise mutual information [ Brent , 1999 ] , but still falls slightly short of state - of - the - art rule - based models . In this chapter , we introduce a new probabilistic segmentation method , based on a class of stochastic neural networks known as Restricted Boltzmann Machines ( RBMs ) . We present a Monte - Carlo method to determine a probability distribu - tion over a subset of visible units in the model , conditioned on a conﬁguration of the remaining visible units . Processing melodies as n - grams , the RBM generates the conditional probability of a note given its n - 1 predecessors . This quantity , in combination with a threshold , determines the location of segment boundaries . Moreover , we demonstrate that a substantial increase in segmentation accuracy can be obtained by not using information content estimates directly , but rather in a bootstrapping fashion . More speciﬁcally , we use information content estimates computed from the RBM as a target for a feed - forward neural network ( FFNN ) that is trained to estimate the information content directly from the data . This method facilitates a probabilistic approach to be on par with rule - based systems . Since the FFNN relies on computed , rather than hand - labeled targets , we call this a “pseudo - supervised” scenario . In an experimental setup , we compare our approach to other methods in eval - uation against human segment boundary annotations . Moreover , we explain the improved accuracy of the pseudo - supervised approach by describing how it can be regarded as employing a form of entropy regularization [ Grandvalet and Bengio , 2004 ] . 13 In Section 2 . 2 we give a brief overview of both rule - based and statistical models for melodic segmentation , with which we compare our approach ( and which were evaluated in [ Pearce et al . , 2010b ] ) , and discuss related work regarding the pseudo - supervised regularization scheme . Then , we will argue that our model ( explained in Section 2 . 3 ) has advantages over statistical models based on n - gram counting . In Section 2 . 3 we explain how we estimate the conditional probability and infor - mation content of notes using an RBM , how notes are represented as input to the model , how an FFNN is used to predict information content , and how the informa - tion content is used to predict segment boundaries . In Section 2 . 4 , we reproduce a quantitative evaluation experiment by Pearce et al . [ 2010b ] . The results are pre - sented and discussed in Section 2 . 5 , and conclusions and future work are presented in Section 2 . 6 . 2 . 2 Related Work 2 . 2 . 1 Rule - Based Segmentation One of the ﬁrst models of melodic segmentation based on Gestalt rules was pro - posed by Tenney and Polansky [ 1980 ] . This theory quantiﬁes some local rules to predict grouping judgments . However , this theory does not account for vague or ambiguous grouping judgments , and the selection of their numerical weights is somewhat arbitrary [ Tenney and Polansky , 1980 , Lerdahl and Jackendoﬀ , 1983 ] . One of the most popular music theoretical approaches is Lerdahl and Jackendoﬀ’s Generative Theory of Tonal Music ( GTTM ) [ Lerdahl and Jackendoﬀ , 1983 ] . This theory pursues the formal description of musical intuitions of experienced listeners through a combination of cognitive principles and generative linguistic theory . In GTTM , the hierarchical segmentation of a musical piece into motifs , phrases , and sections is represented through a grouping structure . This structure is expressed through consecutively numbered grouping preference rules ( GPRs ) , which model possible structural descriptions that correspond to experienced listeners’ hearing of a particular piece [ Lerdahl and Jackendoﬀ , 1983 ] . According to GTTM , two types of evidence are involved in the determination of the grouping structure . The ﬁrst kind of evidence to perceive a phrase boundary between two melodic events is local detail , i . e . relative temporal proximity like slurs and rests ( GPR 2a ) , inter - onset - interval ( IOI ) ( GPR 2b ) and change in register ( GPR 3a ) , dynamics ( GPR 3b ) , articulation ( GPR 3c ) or duration ( GPR 3d ) . The organization of larger - level grouping involves an intensiﬁcation of the eﬀects picked out by GPRs 2 and 3 on a larger temporal scale ( GPR 4 ) , symmetry ( GPR 5 ) and parallelism ( GPR 6 ) . While Lerdahl and Jackendoﬀ’s work did not attempt to quantify these rules , a computational model for identiﬁcation of segment boundaries that numerically quantiﬁes the GPRs 2a , 2b , 3a and 3d was proposed by Frankland 14 and Cohen [ 2004 ] . This model encodes melodic proﬁles using the absolute duration of the notes , and MIDI note numbers for representing absolute pitch . A model related to the GPRs was proposed by Cambouropoulos [ 2001 ] . The Local Boundary Detection Model ( LBDM ) consists of a change rule and a proximity rule , operated on melodic proﬁles that encode pitch , IOI and rests . On the one hand , the change rule identiﬁes the strength of a segment boundary in relation to the degree of change between consecutive intervals ( similar to GPR 3 ) . On the other hand , the proximity rule considers the size of the intervals involved ( as in GPR 2 ) . The total boundary strength is then computed as a weighted sum of the boundaries for pitch , IOI and rests , where the weights were empirically selected . Temperley [ 2001 ] introduced a similar method , called Grouper , that partitions a melody ( represented by onset time , oﬀ time , chromatic pitch and a level in a metrical hierarchy ) into non - overlapping groups . Grouper uses three phase structure preference rules ( PSPR ) to assess the existence of segment boundaries . PSPR 1 locates boundaries at large IOIs and large oﬀset - to - onset intervals ( OOIs ) , and is similar to GPR 2 , while PSPR 3 is a rule for metrical parallelism , analogous to GPR 6 . PSPR 2 relates to the length of the phrase and was empirically determined by Temperley using the Essen Folk Song Collection ( EFSC ) , and therefore , may not be a general rule [ Pearce et al . , 2010a ] . 2 . 2 . 2 Statistical and Information - Theoretic Segmentation Pearce et al . [ 2010a ] applied two information theoretic approaches , initially de - signed by Brent [ 1999 ] for word identiﬁcation in unsegmented speech , to construct boundary strength proﬁles ( BSPs ) for melodic events . This method relies on the assumption that segmentation boundaries are located in places where certain infor - mation theoretic measures have a higher numerical value than in the immediately neighboring locations . The ﬁrst approach constructs BSPs using transition prob - ability ( TP ) , the conditional probability of an element of a sequence given the preceding element , while the second method relies on point - wise mutual informa - tion ( PMI ) , that measures to which degree the occurrence of an event reduces the model’s uncertainty about the co - occurrence of another event , to produce such BSPs Inspired by developments in musicology , computational linguistics and machine learning , Pearce , Müllensiefen and Wiggins oﬀered the IDyOM model . IDyOM is an unsupervised , multi - layer , variable - order Markov model that computes the conditional probability and Information Content ( IC ) of a musical event , given the prior context . An overview of IDyOM can be found in [ Pearce et al . , 2010b ] . 15 2 . 2 . 3 Pseudo - Supervised Training Although the term pseudo - supervised does not seem to have a well - established meaning , our use of the term is compatible with its use in [ Nøklestad , 2009 ] , in the sense that a supervised approach is used to predict targets that are computed from the input data , rather than relying on hand - labeled ( or otherwise authoritative ) targets . The automatically generated targets ( in this case IC values ) are not them - selves the actual targets of interest ( the boundary segments ) but are instrumental to the prediction of the actual targets . Similar methods are proposed by Lee [ 2013 ] and Hinton et al . [ 2014 ] , where supervised models are used to generate targets ( pseudo labels or soft - targets ) from new data . But in contrast to a pseudo - supervised approach , these methods require hand - labeled data , and are strictly taken a semi - supervised approach , in which predictive models are trained partly in an unsupervised manner , and partly using hand - labeled data . In general , both semi - and pseudo - supervised learning beneﬁt from the use of unlabeled information by using Bayesian approaches , regarding the distribution of unlabeled data . From a formal standpoint , these techniques act as regularizers of the model parameters , and thus , prevent overﬁtting . Approaches like entropy regularization [ Grandvalet and Bengio , 2004 ] use the principle of maximum entropy to select a prior distribution of the model parameters , and then optimize the model in Maximum a Posteriori ( MAP ) sense . 2 . 3 Method The primary assumption underlying statistical models of melodic segmentation is that the perception of segment boundaries is induced by the statistical properties of the data . RBMs ( Section 2 . 3 . 2 ) can be trained eﬀectively as a generative prob - abilistic model of data ( Section 2 . 3 . 5 ) , and are therefore a good basis for deﬁning a segmentation method . However , in contrast to sequential models such as recur - rent neural networks , RBMs are models of static data and do not model temporal dependencies . A common way to deal with this is to feed the model sub - sequences of consecutive events ( n - grams ) as if they were static entities , without explicitly encoding time . This n - gram approach allows the model to capture regularities among events that take place within an n - gram . With some simpliﬁcation , we can state that these regularities take the form of a joint probability distribution over all events in an n - gram . With Monte - Carlo methods , we can use this joint distri - bution to approximate the conditional probability of some of these events , given others . This procedure is explained in Sections 2 . 3 . 3 and 2 . 3 . 4 . The representa - tion of music events is described in Section 2 . 3 . 6 , and Section 2 . 3 . 7 details how the IC of music events is computed based on their estimated conditional probabilities . 16 In Section 2 . 3 . 8 , we describe how training a supervised model using IC values as ( pseudo ) targets can act as a form of regularization . Finally , Section 2 . 3 . 9 describes how segment boundaries are predicted from sequences of IC values . 2 . 3 . 1 Relation to Other Statistical Models Although our RBM - based method works with n - gram representations just as the statistical methods discussed in Section 2 . 2 . 2 , the approaches are fundamentally diﬀerent . Models such as IDyOM , TP and PMI are based on n - gram counting , and as such have to deal with the trade - oﬀ between longer n - grams and sparsity of data that is inevitable when working with longer sub - sequences . In IDyOM , this problem is countered with “back - oﬀ” , a heuristic to dynamically decrease or increase the n - gram size as the sparsity of the data allows . In contrast , an RBM does not assign probabilities to n - grams based directly on their frequency counts . The non - linear connections between visible units ( via a layer of hidden units ) allow a much smoother probability distribution , that can also assign a non - zero probability to n - grams that were never presented as training data . As a result , it is possible to work with a ﬁxed , relatively large n - gram size , without the need to reduce the size in order to counter data sparsity . Every computational model requires a set of basic features that describe musical events . In IDyOM , these basic features are treated as statistically independent , and dependencies between features are modeled explicitly by deﬁning combined viewpoints as cross - products of subsets of features . An advantage of the RBM model is that dependencies between features are modeled as an integral part of learning , without the need to specify subsets of features explicitly . Finally , the statistical methods discussed in Section 2 . 2 are fundamentally n - gram based , and it is not obvious how these methods can be adapted to work with polyphonic music rather than monophonic melodies . Although the RBM model presented here uses an n - gram representation , it is straight - forward to adopt the same segmentation approach using a diﬀerent representation of musical events , such as the note - centered representation proposed in [ Grachten and Krebs , 2014 ] . This would make the RBM suitable for segmenting polyphonic music . 2 . 3 . 2 Restricted Boltzmann Machines An RBM is a stochastic Neural Network with two layers , a visible layer with units v ∈ { 0 , 1 } r and a hidden layer with units h ∈ { 0 , 1 } q [ Hinton , 2002 ] . The units of both layers are fully interconnected with weights W ∈ R r × q , while there are no connections between the units within a layer . In a trained RBM , the marginal probability distribution of a visible conﬁguration 17 v is given by the equation p ( v ) = 1 Z X h e − E ( v , h ) , ( 2 . 1 ) where E ( v , h ) is an energy function . The computation of this probability distri - bution is usually intractable , because it requires summing over all possible joint conﬁgurations of v and h as Z = X v , h e − E ( v , h ) . ( 2 . 2 ) 2 . 3 . 3 Approximation of the Probability of v A possibility to circumvent the intractability to compute the probability of a visible unit conﬁguration v is to approximate it through Monte Carlo techniques . To that end , for N randomly initialized fantasy particles 1 Q , we execute Gibbs sampling until thermal equilibrium . In the visible activation vector q i of a fantasy particle i , element q ij speciﬁes the probability that visible unit j is on . Since all visible units are independent given h , the probability of v based on one fantasy particle’s visible activation is computed as : p ( v | q i ) = Y j p ( v j | q ij ) . ( 2 . 3 ) As we are using binary units , such an estimate can be calculated by using a binomial distribution with one trial per unit . We average the results over N fantasy particles , leading to an increasingly close approximation of the true probability of v as N increases : p ( v | Q ) = 1 N N X i Y j 1 v j ! q v j ij ( 1 − q ij ) 1 − v j . ( 2 . 4 ) 2 . 3 . 4 Posterior Probabilities of Visible Units When the visible layer consists of many units , N will need to be very large to obtain good probability estimates with the method described above . However , for conditioning a ( relatively small ) subset of visible units v y ⊂ v on the remaining visible units v x = v \ v y , the above method is very useful . This can be done by Gibbs sampling after randomly initializing the units v y while clamping all other units v x according to their initial state in v . In Eq . 2 . 4 , all v x contribute a probability of 1 , which results in the conditional probability of v y given v x . 1 See [ Tieleman , 2008 ] 18 We use this approach to condition the units belonging to the last time step of an n - gram on the units belonging to preceding time steps . For the experiments reported in this chapter , we found that it is suﬃcient to use 150 fantasy particles and for each to perform 150 Gibbs sampling steps . 2 . 3 . 5 Training We train a single RBM using persistent contrastive divergence ( PCD ) [ Tieleman , 2008 ] with fast weights [ Tieleman and Hinton , 2009 ] , a variation of the standard contrastive divergence ( CD ) algorithm [ Hinton et al . , 2006 ] . PCD is more suitable for sampling than CD because it results in a better approximation of the likelihood gradient . Based on properties of neural coding , sparsity and selectivity can be used as constraints for the optimization of the training algorithm [ Goh et al . , 2010 ] . Spar - sity encourages competition between hidden units , and selectivity prevents over - dominance by any individual unit . A parameter µ speciﬁes the desired degree of sparsity and selectivity , whereas another parameter φ determines how strongly the sparsity / selectivity constraints are enforced . 2 . 3 . 6 Data Representation From the monophonic melodies , we construct a set of n - grams by using a sliding window of size n and a step size of 1 . For each note in the n - gram , four basic features are computed : 1 ) absolute values of the pitch interval between the note and its pre - decessor ( in semitones ) ; 2 ) the contour ( up , down , or equal ) ; 3 ) inter - onset - interval ( IOI ) ; and 4 ) onset - to - oﬀset - interval ( OOI ) . The IOI and OOI values are quantized into semiquaver and quaver , respectively . Each of these four features is represented as a binary vector , and its respective value for any note is encoded in a one - hot representation . The ﬁrst n - 1 n - grams in a melody are noise - padded to account for the ﬁrst n - 1 preﬁxes of the melody . Some examples of binary representations of n - grams are given in Figure 2 . 1 . 2 . 3 . 7 Information Content After training the model as described in 2 . 3 . 5 , we estimate the probability of the last note conditioned on its preceding notes for each n - gram as introduced in 2 . 3 . 4 . From the probabilities p ( e t | e t − 1 t − n + 1 ) computed thus , we calculate the IC as : h ( e t | e t − 1 t − n + 1 ) = log 2 1 p ( e t | e t − 1 t − n + 1 ) , ( 2 . 5 ) where e t is a note event at time step t , and e lk is a note sequence from position k to l of a melody . IC is a measure of the unexpectedness of an event given its 19 n | i n t e r v a l | c o n t o u r I O I OO I Figure 2 . 1 : Seven examples of n - gram training instances ( n = 10 ) used as input to the RBM . Within each instance ( delimited by a dark gray border ) , each of the ten columns represents a note . Each column consists of four one - hot encoded viewpoints : | interval | , contour , IOI and OOI ( indicated by the braces on the left ) . The viewpoints are separated by horizontal light gray lines for clarity . The ﬁrst instance shows an example of noise padding ( in the ﬁrst six columns ) to indicate the beginning of a melody . context . According to a hypothesis of Pearce et al . [ 2010b ] , segmentation in auditory perception is determined by perceptual expectations for auditory events . In this sense , the IC relates directly to this perceived boundary strength ; thus we call the IC over a note sequence boundary strength proﬁle . 2 . 3 . 8 Pseudo - Supervised Optimization As an optimization step , we do not use the BSP estimated from the RBM for segmentation . Instead , we train an FFNN to predict the estimated BSP directly from the data in a non - probabilistic manner , and use that curve for predicting segment boundaries ( by the procedure described in Section 2 . 3 . 9 , see Figure 2 . 3 for a depiction of the pseudo - supervised training ) . This is a way of context - sensitive smoothing , which is achieved by the generalization ability of the NN . Note that no labeled data is used at any stage of the processing pipeline . The fact that this approach still improves the segmentation results is evidence that the generative model , as described in Section 2 . 3 . 2 , provides noisy IC estimates . This is either due to poor approximations to the actual IC by the model itself , or since the data is noisy with respect to prototypical segment endings . The proposed pseudo - supervised training method is shown in Algorithm 1 . For - mally , this method is an approximate MAP estimation of the parameters using entropy regularization [ Grandvalet and Bengio , 2004 ] . In this method , the MAP 20 2 . 3 Method M I D I n o t e nu m b e r I n f o r m a t i o n C o n t e n t Beat number Beat number Figure 2 . 2 : A BSP calculated from 11 - grams . The upper ﬁgure shows the notes of 9 measures ( 36 beats ) of a German folk song . The lower ﬁgure shows a BSP ( i . e . , IC ) used for segmentation . The correct segmentation ( ground truth ) is depicted as vertical grey bars at the top of the ﬁgures , segment boundaries found by our model are shown as dashed vertical lines . Note that the BSP has particularly high peaks at rests and large intervals . However , the segment boundary found at beat 28 does not have any of those cues and was still correctly classiﬁed . 21 2 Bottom - Up Structure Analysis via Probability Estimation Algorithm 1 : Pseudo - supervised training Data : Set of n - grams : V = { v 1 , . . . , v N } 1 Train an RBM by optimizing the model parameters as ˜ θ = argmax θ log p ( v | θ ) ( 2 . 6 ) 2 Compute the set of pseudo - targets T = { t 1 , . . . , t N } as t t ( v t ; ˜ θ ) = h ( e t | e t − 1 t − n + 1 ) , ( 2 . 7 ) where v t is the encoding of the n - gram { e t − n + 1 , . . . , e t } , and h ( e t | e t − 1 t − n + 1 ) is the IC computed as in Eq . ( 2 . 5 ) . 3 Build a three layered FFNN and optimize it in a supervised way , using the set of pseudo - targets T as ˆ θ = argmin θ N X i = 1 k t ( v t ; ˜ θ ) − y ( v t ; θ ) k 2 , ( 2 . 8 ) where y ( v t ; θ ) is the output of the FFNN for v t given the model parameters θ . 4 return Model parameters ˆ θ 22 estimate of the model parameters is computed as θ MAP = argmax θ log p ( v | θ ) − λH ( t | v ; θ ) , ( 2 . 9 ) where H ( t | v ) is the conditional Shannon entropy of the targets given the inputs , and λ is a Lagrange multiplier . In the proposed algorithm , this approximation is obtained by independently optimizing log p ( v | θ ) ( see Eq . ( 2 . 6 ) ) , and then mini - mizing Eq . ( 2 . 8 ) , which is equivalent to maximizing p ( t | v ; θ , β ) = N ( t | y ( v , θ ) , β − 1 ) , ( 2 . 10 ) where β is the precision ( inverse variance ) of the distribution . This precision can be found by minimizing the negative log - likelihood of the above probability to give β = N P i k t i − y ( v i , θ ) k 2 . ( 2 . 11 ) The Shannon entropy for this distribution is given by H ( t | v ; θ , β ) = E { − log p ( t | v ) } = 1 2 log (cid:18) 2 π β (cid:19) + 1 2 , ( 2 . 12 ) which is minimal , since P i k t i − y ( v i , θ ) k 2 is minimal . Therefore , optimizing Eq . ( 2 . 8 ) is equivalent to minimizing the entropy term in Eq . ( 2 . 9 ) . We use the fact that the RBM is a generative model , and therefore , the pseudo targets t come from the computation of the IC from a probabilistically sound esti - mate of the input data . In this way , pseudo - supervised learning can be understood as a suboptimal entropy - regularized MAP model of the model parameters . 2 . 3 . 8 . 1 Training To compute ˆ θ in Equation ( 2 . 8 ) , we use a three - layered FFNN with sigmoid hidden units and a single linear unit in the output layer . We pre - train the ﬁrst hidden layer with PCD and ﬁne - tune the whole stack with Backpropagation , by minimizing the mean square error . As targets , we use the boundary strength values , estimated by the initial model described in Section 2 . 3 . 4 . After training , the outputs y ( v i ; ˆ θ ) of the FFNN are used as ( improved ) estimates of the information content of e t , given { e i − n + 1 , . . . , e t − 1 } . 23 2 Bottom - Up Structure Analysis via Probability Estimation . . . . . . Information Content ( IC ) FFNN : - Pretraining - Dropout N - gram : - | Interval | - Contour - IOI - OOI Figure 2 . 3 : Schematic depiction of the pseudo - supervised optimization . Note that the n - gram is linearized before it is fed into the Feed - Forward Neural Network . 24 2 . 3 . 9 Peak Picking Based on the BSP described in the previous section , we need to ﬁnd a concrete binary segmentation vector . For that , we adopt the peak picking method described in [ Pearce et al . , 2010b ] . This method ﬁnds all peaks in the proﬁle and keeps those which are k times the standard deviation greater than the mean boundary strength , linearly weighted from the beginning of the melody to the preceding value : S n > k vuutP n − 1 i = 1 ( w i S i − ¯ S w , 1 . . . n − 1 ) 2 P n − 1 1 w i + P n − 1 i = 1 w i S i P n − 1 1 w i , ( 2 . 13 ) where S m is the m - th value of the BSP , and w i are the weights which emphasize recent values over those of the beginning of the song ( triangular window ) , and k has to be found empirically . 2 . 4 Experiment 2 . 4 . 1 Training Data In this work , we use the Essen Folk Song Collection ( EFSC ) [ Schaﬀrath , 1995 ] . This database is a widely used corpus in MIR for experiments on symbolic music . This collection consists of more than 6000 transcriptions of folk songs primarily from Germany and other European regions . The EFSC collection is commonly used for testing computational models of music segmentation , because it is annotated with phrase markers . In accordance with [ Pearce et al . , 2010b ] , we used the Erk subset of the EFSC , which consists of 1705 German folk melodies with a total of 78 , 995 note events . Phrase boundary annotations are marked at about 12 % of the note events . 2 . 4 . 2 Procedure The model is trained and tested on the data described in Section 2 . 4 . 1 with var - ious n - gram lengths between 3 and 10 . For each n - gram length , we perform 5 - fold cross - validation and average the results over all folds . Similar to the ap - proach in [ Pearce et al . , 2010b ] , after computing the BSPs , we evaluate diﬀerent k from the set { 0 . 70 , 0 . 75 , 0 . 80 , 0 . 85 , 0 . 90 , 0 . 95 , 1 . 00 } ( initial IC estimation ) , and { 0 . 24 , 0 . 26 , 0 . 28 , 0 . 30 , 0 . 32 , 0 . 34 , 0 . 36 } ( after pseudo - supervised optimization ) , and choose the value that maximizes F1 for the respective n - gram length . To make results comparable to those reported in [ Pearce et al . , 2010b ] , the output of the model is appended with an implicit ( and correct ) phrase boundary at the end of each melody . 25 3 4 5 6 7 8 9 10 N - gram length 0 . 57 0 . 58 0 . 59 0 . 60 0 . 61 0 . 62 0 . 63 F 1 N - gram length vs . F1 RBM RBM + DO RBM + DO + PS Figure 2 . 4 : F1 scores for diﬀerent N - gram lengths and methods . The orange horizontal line marks the baseline of the probabilistic IDyOM model [ Pearce et al . , 2010b ] . Since the hyper - parameters of the model are inter - dependent , it is infeasible to search for the optimal parameter setting exhaustively . We have manually chosen a set of hyper - parameters that give reasonable results for the diﬀerent models tested . For the initial IC estimation , we use 200 hidden units , a momentum of 0 . 6 , and a learning rate of 0 . 0085 which we linearly decrease to zero during training . With increasing n - gram length we linearly adapt the batch size from 250 to 1000 . Also , we use 50 % dropout on the hidden layer and 20 % dropout on the visible layer . The fast weights used in the training algorithm ( see Section 2 . 3 . 5 ) help the fan - tasy particles mix well , even with small learning rates . The learning rate of the fast weights is increased from 0 . 002 to 0 . 007 during training . The training is continued until convergence of the parameters ( typically between 100 and 300 epochs ) . The sparsity parameters ( see Section 2 . 3 . 5 ) are set to µ = 0 . 04 , and φ = 0 . 65 , respec - tively . In addition , we use a value of 0 . 0035 for L 2 weight regularization , which penalizes large weight coeﬃcients . For pre - training of the ﬁrst layer in the FFNN , we change the learning rate to 0 . 005 , leave the batch size constant at 250 and increase the weight regularization to 0 . 01 . We again use dropout , for both the pre - training and the ﬁne - tuning . 26 2 . 5 Results and Discussion We tested three diﬀerent representations for pitch , yielding the following F1 scores for 10 - grams ( without dropout and pseudo - supervised training ) : absolute pitch ( 0 . 582 ) , interval ( 0 . 600 ) , and the absolute value of interval ( i . e . | interval | ) plus contour ( 0 . 602 ) . The latter representation was chosen for our experiments , as it showed the best performance . Not surprisingly , relative pitch representations lead to better results , as they reduce the number of combination possibilities in the in - put . Even though the diﬀerence in F1 score between interval and | interval | plus contour representation is not signiﬁcant , it still shows that it is valid to decompose viewpoints into their elementary informative parts . Such an approach , next to re - ducing the input dimensionality , may also support the generalization ability of a model ( e . g . , | interval | representation in music may help to understand the concept of inversion ) . Figure 2 . 4 shows the F1 scores for diﬀerent N - gram lengths and methods . By using dropout , the F1 score increases considerably , as dropout improves the gen - eralization abilities of the RBM . With the pseudo - supervised approach , again a signiﬁcant improvement of the classiﬁcation accuracy can be achieved . This is re - markable , considering that no additional information was given to the FFNN , the improvement was based solely on “context - sensitive smoothing” . Figure 2 . 5 shows the adaptation of single IC values through pseudo - supervised optimization . Some previously true positives are erroneously regularized downwards ( green lines from upper left to lower right ) , while some previously false negatives are correctly moved upwards ( green lines from lower left to the upper right ) . Quan - titative tests show that our method increases IC values at boundaries more often than it decreases them . In general , if the initial BSP curve is correct in most cases , in pseudo - supervised training , such regularities are detected and utilized . Table 2 . 1 shows prediction accuracies in terms of precision , recall , and F1 score , both for our method and for the various alternative approaches mentioned in Sec - tion 2 . 2 . The table shows that with the proposed method ( RBM10 + DO + PS ) , an information - theoretic approach is now on a par with a Gestalt - based approach ( LBDM ) , while Grouper still provides the best estimates of melodic segment bound - aries . However , Grouper exploits additional domain knowledge like musical paral - lelism , whereas the LBDM model , as well as ( RBM10 + DO + PS ) , are pure repre - sentatives of the Gestalt - based paradigm and the information - theoretic paradigm , respectively . The GPR 2a method is a simple rule that predicts a boundary whenever a rest occurs between two successive notes . Note how GPR 2a accounts for a large portion of the segment boundaries ( approx . 45 % ) . This implies that the challenge is mainly in recognizing boundaries that do not co - occur with a rest . For boundaries without rests , the pseudo - supervised approach yields an improvement of 3 . 7 % in the F - 27 Model Precision Recall F1 Grouper 0 . 71 0 . 62 0 . 66 LBDM 0 . 70 0 . 60 0 . 63 RBM10 + DO + PS 0 . 80 0 . 55 0 . 63 RBM10 + DO 0 . 78 0 . 53 0 . 61 RBM10 0 . 83 0 . 50 0 . 60 IDyOM 0 . 76 0 . 50 0 . 58 GPR 2a 0 . 99 0 . 45 0 . 58 GPR 2b 0 . 47 0 . 42 0 . 39 GPR 3a 0 . 29 0 . 46 0 . 35 GPR 3d 0 . 66 0 . 22 0 . 31 PMI 0 . 16 0 . 32 0 . 21 TP 0 . 17 0 . 19 0 . 17 Always 0 . 13 1 . 00 0 . 22 Never 0 . 00 0 . 00 0 . 00 Table 2 . 1 : Results of the model comparison , ordered by F1 score . score , while boundaries indicated by a rest did not improve any more ( as for those boundaries the initial approach already yields an F - score of 0 . 99 ) . 2 . 6 Conclusion In this chapter , an RBM - based unsupervised probabilistic method for segmenta - tion of melodic sequences was presented . In contrast to other statistical methods , our method does not rely on frequency counting and thereby circumvents problems related to data sparsity . Furthermore , we showed how a technique we call pseudo - supervised training improves the prediction accuracy of the method . We used the information content ( IC ) of musical events ( estimated from the probabilistic model ) as a proxy for the actual target to be predicted ( segment boundaries ) . With these pseudo targets , we trained a feed - forward neural network . We showed that seg - ment boundaries estimated from the output of this network are more accurate than boundaries estimated from the pseudo targets themselves . In this study , we used the IC as estimated from an RBM , but the pseudo - supervised approach may beneﬁt from including IC estimates from other models , such as IDyOM [ Pearce , 2005 ] , which , in addition , uses a short - term memory . Be - sides , there are other probabilistic architectures , such as conditional RBMs [ Taylor et al . , 2006 ] , that seem appropriate for estimating IC values from data . Further - 28 2 . 6 Conclusion RBM10 + DO RBM10 + DO + PS 0 10 20 30 40 50 i n f o r m a t i o n c o n t e n t no segment boundary segment boundary Figure 2 . 5 : The eﬀect of pseudo - training on estimated IC values ; Line segments connect IC values estimated directly from the probabilistic model ( RBM10 + DO ) with the corre - sponding IC values after pseudo - training ( RBM10 + DO + PS ) ; Green lines indicate music events that mark a segment boundary , red lines indicate those that do not . 29 more , although the focus of this chapter has been on IC , it is intuitively clear that IC is not the only factor that determines the perception of segment boundaries in melodies . Future experimentation is necessary to determine whether ( combinations of ) other information - theoretic quantities are also helpful in detecting melodic seg - ment boundaries . Finally , we wish to investigate whether there are further problems where our method could be beneﬁcial . In general , pseudo - supervised optimization could improve features which are noisy either because of the way they are calculated or because of noise in the data upon which the features are based . 30 3 Top - Down Imposition of Higher - Level Structure In this chapter , we introduce a method for imposing higher - level structure on gener - ated , polyphonic music . A Convolutional Restricted Boltzmann Machine ( C - RBM ) as a generative model is combined with gradient descent constraint optimization to provide further control over the generation process . Among other things , this allows for the use of a “template” piece , from which some structural properties can be extracted , and transferred as constraints to newly generated material . The sam - pling process is guided with Simulated Annealing in order to avoid local optima , and ﬁnd solutions that both satisfy the constraints and are relatively stable con - cerning the C - RBM . Results show that with this approach it is possible to control the higher - level self - similarity structure , the meter , as well as tonal properties of the resulting musical piece while preserving its local musical coherence . 3 . 1 Introduction For centuries , mathematical formalisms have been used to generate musical material [ Kirchmeyer , 1968 ] . Since computers can automate such processes , automatic music generation has become a small , but a steadily emerging ﬁeld in Artiﬁcial Intelligence and Machine Learning . Nevertheless , automatic music generation as a problem is far from solved : musical outputs created by artiﬁcial systems are regarded as a curiosity by human listeners at best , but all too often they are taken as a direct oﬀense to our sense of musical aesthetics . This sensitivity to violations of even the most subtle musical norms illustrates how complex the problem of ( mainly polyphonic ) music generation is . Besides , there are only a few objective evaluation criteria to rigorously test and compare music generation systems , all of which involve human judgment [ Jordanous , 2012 , Pearce and Wiggins , 2001 ] . This is lamentable , not least since successful methods for automatic music gen - eration would be of considerable commercial interest to the music , gaming and ﬁlm industries . Moreover , potential applications that have remained unexplored as of yet , including adaptive music in cars or ﬁtness applications , could personalize music and thus provide a completely new listening experience . In line with a global surge in deep learning and neural network modeling over the past decade , several studies address the task of music modeling as a form 31 of sequence learning , in which musical pieces are formulated as a time series of musical events , using state - of - the - art sequence models such as Recurrent Neural Networks ( RNN ) and Long Short - Term Memory ( LSTM ) . For restricted genres or representations such as monophonic folk melodies [ Sturm et al . , 2016 ] , symbolic chord sequences , or drum tracks [ Choi et al . , 2016 ] and even in polyphonic music with clearly deﬁned melodic voices , such as Bach chorales [ Boulanger - Lewandowski et al . , 2012 , Hadjeres et al . , 2017 , Liang et al . , 2017 , Huang et al . , 2017 ] , sequence modeling approaches yield impressive results that are sometimes hard to distinguish from human - composed material . However , in more complex musical material , such as piano music from the classi - cal ( e . g . , Mozart ) or romantic period ( e . g . , Chopin , Liszt ) , not to mention orchestral works , important musical characteristics may defy straight - forward time series mod - eling approaches . Tonality for example , is the characteristic that music is perceived to be in a particular ( possibly time - variant ) musical key , implying that some pitches are regarded as more stable than others . Although the perception of musical key is a complex topic in itself , there is evidence that an essential determining factor is the frequency of occurrence of pitches in the piece [ Smith and Schmuckler , 2004 ] . In addition to tonality , meter is a vital aspect of music . Perception of meter is the sensation that musical time can be divided into equal intervals at diﬀerent levels and that positions that coincide with the start of higher - level intervals have more importance than those coinciding with lower levels . Analogous to the perception of musical key , the perception of meter is in part related to the distribution of musical events over time [ Palmer and Krumhansl , 1990 ] . Lastly , music often transmits a sense of coherence over the course of the piece , in that it has a structural organization in which motifs ( small musical patterns ) , but also larger units such as phrases , melodies or complete sections of the music are repeated throughout the piece , either literally or in an altered form . This characteristic is reﬂected in the self - similarity matrix of the music , where entry ( i , j ) expresses the similarity between the music at positions i and j . This coherence by way of repeating and developing musical material throughout the piece is arguably one of the aspects of music that make listening and re - listening a valuable experience to human listeners . Important musical characteristics such as these are not straight - forward to cap - ture using a simple sequence modeling approach unless the musical material is restricted or simpliﬁed as mentioned above . For example , it is a challenge for cur - rent models to generate music that is diverse and interesting , and at the same time induces a stable musical key over an extended period . It is even more challenging to generate music that exhibits the hierarchical organizational structure common in human - composed music . For instance , the rather common pattern of a melodic line from the opening of a piece being repeated as the conclusion of that piece is diﬃcult to capture , even if state - of - the - art sequence models like LSTMs are capa - 32 ble of learning long - term dependencies in the data . Models that fail to capture these higher - level musical characteristics may still produce music that on a short timescale sounds very convincing , but on longer stretches of time tends to sound like it wanders , and misses a sense of musical direction . In this work , we do not address the problem of learning the discussed properties from musical data . Instead , our contribution is a method to enforce such properties as constraints in a sampling process . We start from the observation stated above , that neural network models in the various forms that have recently been proposed are adequate for learning the local structure and coherence of the musical surface , that is , the musical texture . The strategy we propose here uses such a neural network ( more speciﬁcally , a Convolutional Restricted Boltzmann Machine , see Section 3 . 3 . 1 ) as one of several components that jointly drive an iterative sampling process of music generation . This model is trained on musical data and is used to ensure that the musical texture is similar to that of the training data . The other components involved in the sampling process are cost functions that ex - press how well higher - level constraints like tonal , metrical and self - similarity struc - ture are satisﬁed in the musical material at each stage in the process . By performing gradient descent on these cost functions , the sampling process is driven to produce musical material that better satisﬁes the constraints . The desired shape of these higher - level structural constraints on the piece is not hard - coded in the cost func - tions but is instantiated from an existing piece . As such , the existing piece serves as a structure template . The generation process then results in a re - instantiation of that template with new material . Through recombination of structural character - istics from a musical piece that is not part of the neural network’s training data , the model is forced to produce novel solutions . We refer to the above process as constrained sampling . Informally , it can be imagined as a musical drawing board that is initially ﬁlled with random pitches at random times , and where the neural network model , as well as each of the constraints , take turns to slightly tweak the current content of the drawing board to their liking . This process continues until the musical content can no longer be tweaked to better satisfy the model and constraints jointly . We believe this approach provides a novel and useful contribution to the problem of polyphonic music generation . Firstly , it takes advantage of the strengths of state - of - the - art deep learning methods for data modeling . The combination with multi - objective constraint optimization compensates for the weaknesses of these methods for music generation , mentioned above . Moreover , it provides high - level user control 1 over the generation process , and allows for relating the generated 1 The user has control over the generation process by choice of the template piece , but also more directly by manipulation of the structure templates extracted from the template piece . This aspect is beyond the scope of the current study . 33 material to existing pieces , both of which are interesting from a musical point of view . In addition to the description of the constrained sampling approach to music generation , the goal of the present study is to validate the approach in several ways . First , we present a qualitative discussion of generated musical samples , illus - trating the eﬀect of the constraints on the musical result . We show that although the constraints and the neural network embody diﬀerent objectives , the evolving musical material produced by the constrained sampling process tends to approxi - mate these diﬀerent objectives simultaneously . Furthermore , we adopt Information Rate as an independent measure of musical structure [ Wang and Dubnov , 2015 ] , in order to assess the eﬀect of the repetition structure constraint , and compare our approach to two variants of the state - of - the - art RNN - RBM model for poly - phonic music generation [ Boulanger - Lewandowski et al . , 2012 ] . This comparison shows that the constrained sampling approach substantially increases the Informa - tion Rate of the produced musical material over both unconstrained approaches ( including the RNN - RBM variants ) , implying a higher degree of structure . The chapter is structured as follows : Section 3 . 2 gives an overview of related mod - els and computational approaches to music generation . Section 3 . 3 describes the components involved in the constrained sampling approach , which is subsequently introduced in Section 3 . 4 . Section 3 . 5 describes the experimental validation of the constrained sampling approach in the context of Mozart piano sonatas . We discuss the empirical ﬁndings in Section 3 . 6 and give conclusions and future perspectives in Section 3 . 7 . 3 . 2 Related work Early attempts using neural networks for music generation were reported in [ Todd , 1989 ] , where monophonic melodies were encoded in pitch and duration and an RNN was trained to predict upcoming events . In [ Mozer , 1994 ] , an RNN system called CONCERT was proposed , and ﬁrst systematic tests on how well local and global musical structure ( e . g . AABA ) of simple melodies could be learned , were made . Also , chords were used to test if this facilitates the learning of higher - level structure , but the results were not convincing . This was one of the ﬁrst experiments which showed the diﬃculties of learning structure in music . More recently , Eck and Schmidhuber [ 2002 ] trained a Long Short - Term Mem - ory ( LSTM ) network ( a state - of - the - art RNN variant ) , jointly on a single chord sequence along with several diﬀerent melodies . This is an example of a harmonic template which guides a melodic improvisation . Chords and melody notes were separated in the input and output connections so that the model could not mix up harmony and melody notes . That way , the LSTM could overﬁt on the single 34 chord sequence and generalize on the monophonic melodies . In a polyphonic set - ting , common RNNs are not suitable for generation in a random walk fashion as the distribution at time t is conditioned only on the past , but it would be necessary to consider the full joint distribution also for all possible settings in t . This limitation was overcome by the RNN - RBM model for polyphonic music gen - eration introduced in [ Boulanger - Lewandowski et al . , 2012 ] , and the similar LSTM Recurrent Temporal RBM ( LSTM - RTRBM ) model proposed in [ Lyu et al . , 2015 ] . In those architectures , the recurrent components ensure temporal consistency , while the RBM component is used for sampling a plausible conﬁguration in t . Our con - tribution lies between the before - mentioned LSTM approach where a higher - level structure is imposed by using a template , and the RNN - RBM approach , where the ability of an RBM to model low - level structure is utilized . Further methods to con - strain generated material by pre - deﬁning voices to guide the sampling process are introduced in [ Hadjeres et al . , 2017 ] ( based on LSTMs ) , and [ Huang et al . , 2017 ] ( based on Convolutional Neural Networks ) , both of which generate Bach chorales . Another approach that uses a probabilistic model and constraints is called “Markov constraints” [ Pachet and Roy , 2011 ] , which allows for sampling from a Markov chain while satisfying pre - deﬁned hard constraints . This is conceptually similar to our method , but we use a diﬀerent probabilistic model and soft con - straints . Our method is more ﬂexible in deﬁning new constraints , and it is of linear runtime , while Markov constraints are more costly , but also more exact . Herre - mans and Chew [ 2016 ] use a constrained variable neighborhood search to generate polyphonic music obeying a tension proﬁle and the repetition structure from a template piece . Furthermore , Barbieri [ 2011 ] uses soft constraints to incorporate a - priori - information in a Gibbs sampling process for a User Rating Proﬁle model . Cope [ 1996 ] explicitly imposes higher - level structure in a generation process . So - called SPEAC identiﬁers are used to generate music in a given tension - relaxation scheme . Another example of generating structured material is that in [ Eigenfeldt and Pasquier , 2013 ] , where Markov chains and evolutionary algorithms are used to generate repetition structure for Electronic Dance Music . Collins et al . [ 2016 ] use Markov chains together with structure schemes and speciﬁc methods for han - dling transitions between repeating segments in order to generate structured music . Similarly , Whorley and Conklin [ 2016 ] use a transformational approach to gener - ate Bach chorales , and Conklin [ 2016 ] generates chords using Markov chains and pre - deﬁned repetition structures . A Hierarchical Variational Autoencoder for music generation , able to learn hierarchical tonal structure , is proposed in [ Roberts et al . , 2017 ] . A method similar to our approach is that of Gatys et al . [ 2016 ] for image style transfer . They also use gradient descent on the input for satisfying multiple objec - tives ( approximating a gram - matrix deﬁning the style , as well as the initial picture deﬁning the structure ) . In contrast to our method , there is no probabilistic model 35 involved . Diﬀerent solutions for the same objectives are merely due to the initial - ization of the input with random noise . Application of the method to music would , however , not allow for the control needed to comply with some music - speciﬁc prop - erties like self - similarity or the somewhat strict rules of musical tonality . Examples of connectionist generation approaches with constraints in other do - mains are that in [ Graves , 2013 ] where biasing and priming is used in LSTMs to control the generation of sequences of handwritten text , and in [ Taylor et al . , 2006 ] where a conditional RBM is used to generate diﬀerent human walking styles . In such problems , the number of variables is ﬁxed and lower than in music gener - ation , and structural properties like repetition are either not a property of the data ( handwritten text ) or periodic ( walking ) , whereas polyphonic music exhibits complex structure in multiple hierarchical levels . 3 . 3 Method In this Section , we describe the methods used to create musical output . We start by describing the C - RBM used for sampling new content ( Section 3 . 3 . 1 ) . The gradient descent ( GD ) method used to impose constraints on the sampling process is introduced in Section 3 . 3 . 2 . The complete process , referred to as Constrained Sampling ( CS ) and depicted in Figure 3 . 1 , is introduced in Section 3 . 4 . 3 . 3 . 1 Convolutional Restricted Boltzmann Machine ( C - RBM ) A C - RBM [ Lee et al . , 2009 ] is a two - layered stochastic version of a convolutional neural network with binary units , as known from LeCun et al . [ 1989 ] . In our setting , the visible layer with units v ∈ R T × P , where 0 ≤ v tp ≤ 1 , constitutes a piano roll representation ( see Section 3 . 5 . 2 ) with time 1 ≤ t ≤ T and midi pitch number 1 ≤ p ≤ P . All units in the hidden layer belonging to the k th feature map share their weights ( i . e . their ﬁlter ) W k ∈ R R × P and their bias b k ∈ R , where R denotes the ﬁlter width ( i . e . the temporal expansion of the receptive ﬁeld ) , and each ﬁlter covers the whole midi pitch range [ 1 , P ] . We convolve only in the time dimension , which is padded with R / 2 zeros on either side ( the reason for this design decision is given at the end of this section ) . We use a stride of d , meaning the ﬁlters are shifted over the input with step size d . This results in a hidden layer h ∈ R K × ( T / d ) , where 0 ≤ h kj ≤ 1 and j ∈ 0 . . . T / d . See Figure 3 . 2 for an illustration of the C - RBM used in our experiments . We train the C - RBM with Persistent Contrastive Divergence [ Tieleman , 2008 ] aiming to minimise the free energy function F ( v ) = − X t a v t − X k , j log (cid:16) 1 + e ( b k + ( W k ∗ v ) j × d ) (cid:17) ( 3 . 1 ) 36 F i g u r e 3 . 1 : C o n s t r a i n e d s a m p li n g u s i n g a n e x i s t i n g p i ece x a s a s t r u c t u r e t e m p l a t e . A r a nd o m l y i n i t i a li ze d s a m p l e v i s a l t e r n a t e l y upd a t e d w i t h G i bb s s a m p li n g ( G S ) a nd g r a d i e n t d e s ce n t ( G D ) . I n G D , t h e e rr o r φ ( x , y ) b e t w ee n s t r u c t u r a l f e a t u r e s o f x a nd v i s l o w e r e d , i n G S t h e t r a i n i n g d a t a d i s t r i bu t i o n i s a pp r o x i m a t e d . T h e C o n v o l u t i o n a l R B M c o n s i s t s o f v i s i b l e l a y e r v a nd h i dd e n l a y e r h . T h e ﬁ l t e r W k i s s h a r e d a m o n g a ll un i t s i n f e a t u r e m a p h k . D e p i c t e d e q u a t i o n s a r e a l s o g i v e n i n S ec t i o n 3 . 3 . 2 . 37 for training instances v , where a ∈ R P and b ∈ R K are bias vectors , and ∗ is the convolution operator . Note that in two - dimensional convolution , each feature map usually has a scalar as bias ( e . g . b k ) because all positions in a feature map are assumed to be equivalent . However , since we convolve only in the time dimension , and since there is a non - uniform distribution over the pitch dimension , we deﬁne the bias for the input feature map v as a vector a of length P . The probability of a unit being active depends on the full conﬁguration of the opposing layer . When updating hidden units h and visible units v , each unit is randomly chosen to be active ( i . e . 1 ) or inactive ( i . e . 0 ) with probabilities P ( h kj = 1 | v ) = σ (cid:0)(cid:0) R , P X r , p W kr , p × v j × d + r − R 2 , p (cid:1) + b k (cid:1) ( 3 . 2 ) and P ( v tp = 1 | h ) = σ (cid:0)(cid:0) R / d , K X r , k ˜ W kr × d , p × h kt + r − R 2 d (cid:1) + a p (cid:1) , ( 3 . 3 ) where ˜ W k denotes the horizontally ﬂipped weight matrix . Note that it is also valid to propagate such probability values through the network ( i . e . calculate the activation probabilities of one layer based on the probabilities of the opposing layer ) . A sample can be drawn from the model by randomly initializing v ( following the standard uniform distribution ) , and running block Gibbs sampling ( GS ) until convergence . To this end , hidden units and visible units are alternately updated given the other . In doing so , it is common to sample the states of the hidden units for the top - down pass , but use the probabilities of the visible units for the bottom - up pass . After an inﬁnite number of such Gibbs sampling iterations , v is an accurate sample under the model . In practice , convergence is reached when F ( v ) stabilises . The reason for convolving only in the time dimension is that there are correla - tions between notes over the whole pitch range . In a one - layered setting with 2D convolution , the ﬁlter height ( i . e . the expansion of ﬁlters in the pitch dimension ) is typically limited , for example , to one octave . In that case , correlations would only be learned between notes within one octave . Learning correlations over a wider range would usually be the role of higher layers in a neural network stack . However , in order to show the principle of constrained sampling , it is suﬃcient to use only one layer with 1D convolution , which is also advantageous for limiting the overall complexity of the architecture . 38 Figure 3 . 2 : Illustration of a C - RBM with strided convolution ( using stride d ) in the time dimension t of a music piece v ∈ R T × P in two - dimensional piano roll representation using K one - dimensional feature maps h k where all units in a map share their weights W k ∈ R R × P and their bias b k ( bias not depicted in the illustration ) . 3 . 3 . 2 Imposing Constraints with Gradient Descent When sampling from a C - RBM , the solution is randomly initialized and converges to an accurate sample of the data distribution after many steps ( see Section 3 . 3 . 1 ) . During this process , we repeatedly adjust the current solution v towards satisfying a desired higher - level structure regarding some musical properties . To this end , we subject v ( i . e . the input , not the model parameters ) to a Gradient Descent ( GD ) optimization process aiming to minimize a diﬀerentiable cost function φ ( · ) using learning rate γ as ˆ v = v − γ ∂φ ( x , v ) ∂ v , ( 3 . 4 ) where x ∈ R T × P , 0 ≤ x tp ≤ 1 , is a template piece from which we want to transfer some structural properties to our sample v . After every GD update , we set each entry ˆ v tp = min ( 1 , max ( 0 , ˆ v t , p ) ) , to ensure ˆ v ∈ [ 0 , 1 ] T × P . The cost function may consist of several terms g d ( x , v ) ( weighted with factors w d ) , each deﬁning a soft constraint which is to be imposed on the sample : φ ( x , v ) = g 0 ( x , v ) w 0 + · · · + g D − 1 ( x , v ) w D − 1 . ( 3 . 5 ) Note that x and v , as representations of a musical score , could be assumed to be binary , but we deﬁne them as continuous variables . This is because we want to store continuous results of the GD optimization in v , as well as intermediate probabilities 39 during Gibbs sampling . Deﬁning x as a continuous variable is a generalization towards encoding note intensities or note probabilities , making it possible to express relative importance between notes . In the following , we will introduce three constraints we tested in our experiments . Note that the method is not limited to those constraints , and can be extended with additional terms which are diﬀerentiable with respect to v . 3 . 3 . 2 . 1 Self - Similarity Constraint The purpose of the self - similarity constraint is to specify the repetition structure ( e . g . AABA ) in the generated music piece , using a template self - similarity ma - trix as a target . Such a self - similarity representation is particularly useful because it also provides distances between any two parts of a piece . Thus , the degree of similarity , including substantial dissimilarity , may be encoded , too . Such a repre - sentation abstracts from the actual musical texture and is therefore to a large extent content - invariant . This allows for transferring the similarity structure in diﬀerent hierarchical levels between pieces of diﬀerent style , tonality , or rhythm . A self - similarity matrix s ( z ) ∈ R I × J for an arbitrary music piece z ∈ [ 0 , 1 ] T × P in piano roll representation is calculated by tiling z horizontally in tiles of width Λ and by using them as 2 - D ﬁlters for a convolution over the time dimension of z ( see Figure 3 . 3 ) . Therefore I = T and J = T / Λ , and we calculate a single entry at position i , j of the self - similarity matrix as s ( z ) i , j = Λ , P X λ , p z j × Λ + λ , p z i + λ , p . ( 3 . 6 ) To impose the self - similarity constraint , we minimize the mean squared error ( MSE ) between a target self - similarity matrix of the squared template piece s ( x 2 ) and the self - similarity matrix of the squared intermediate solution s ( v 2 ) as g ( x , v ) self - sim = P I , J i , j ( s ( x 2 ) i , j − s ( v 2 ) i , j ) 2 I × J . ( 3 . 7 ) The reason for squaring x and v is that it leads to higher stability in the opti - mization because it reduces low - intensity noise and it adds contrast to the resulting self - similarity matrix . We also tried to represent transposed repetition as a con - straint using two - dimensional convolution . However , we found that this leads to a perfect reconstruction of the template piece , as such a self - similarity representation fully speciﬁes the musical texture . 40 Figure 3 . 3 : Depiction of calculating the self - similarity matrix s ( z ) ∈ R I × J using convo - lution . A music piece in piano roll representation z ∈ [ 0 , 1 ] T × P is horizontally tiled , and those tiles are used as ﬁlters for a convolution with z . The response for a single ﬁlter constitutes a single line in the resulting self - similarity matrix . Low to high response is depicted in a range from darker blue to brighter red colors . 3 . 3 . 2 . 2 Tonality Constraint Tonality is another fundamental higher - order property in music . It describes per - ceived tonal relations between notes and chords . This information can be used to , for example , determine the key of a piece or a musical section . A key is charac - terized by a tonal center ( the pitch class that is considered to be central , e . g . , C , or A ] ) , and a mode ( the subset of pitch classes that form part of the key , e . g . , major or minor ) . The distribution of pitch classes in the musical texture within a ( temporal ) window of interest is an essential factor in the perceived key of that window . Diﬀerent window lengths M may lead to diﬀerent key estimates , consti - tuting a hierarchical tonal structure . A popular method to estimate the key in a given window is to compare the distribution of pitch classes in the window with so - called key proﬁles u mode ( i . e . , paradigmatic relative pitch - class strengths for spe - ciﬁc modes ; the proﬁles are invariant to changes of tonal center ) . In [ Temperley , 2001 ] , key proﬁles for major mode u maj and minor mode u min are deﬁned as u maj = ( 5 , 2 , 3 . 5 , 2 , 4 . 5 , 4 , 2 , 4 . 5 , 2 , 3 . 5 , 1 . 5 , 4 ) > , u min = ( 5 , 2 , 3 . 5 , 4 . 5 , 2 , 4 , 2 , 4 . 5 , 3 . 5 , 2 , 1 . 5 , 4 ) > , where the numerical values express the strengths of the diﬀerent pitch classes that make up the key . We use these two key proﬁles as ﬁlters for a music piece z ∈ 41 [ 0 , 1 ] T × P . By repeating them M times in the time dimension , we obtain a ﬁlter for a window of size M . By repeating them O = P / 12 times in the pitch dimension , we extend the ﬁlters over all octaves represented in z . When shifted in the pitch dimension with shifts κ ∈ 0 . . . K − 1 we obtain a ﬁlter for each of the K = 12 possible keys . If we choose the proﬁle for a speciﬁc mode u mode , an estimation window size M , and the number of octaves O represented by z , we obtain a key estimation vector k ( z ) mode t ∈ R K at time t for all shifts κ as k ( z ) mode t = M , O , I X m , o , i u mode ( ( i + κ ) mod I ) · z t + m , i + o ∗ 12 , ( 3 . 8 ) for I = 12 entries in key proﬁle u η , and · denotes the common multiplication of scalars . Subsequently , we concatenate the key estimation vectors of both modes , k ( z ) maj t and k ( z ) min t , to obtain a combined estimation vector k ( z ) t ∈ R 2 K in t , which is ﬁnally normalized as k 0 ( z ) t = k ( z ) t − min ( k ( z ) t ) I max ( k ( z ) t ) − min ( k ( z ) t ) , ( 3 . 9 ) where I is a vector of ones of length 2 K . 2 Figure 3 . 4 depicts the resulting concate - nated key estimation vectors . Using these vectors , we may impose a speciﬁc tonal progression on our solution by minimizing the MSE between the target estimate k 0 ( x ) t and the estimate of our current solution k 0 ( v ) t such that : g ( x , v ) tonal = P t k k 0 ( x ) t − k 0 ( v ) t k 2 2 K T . ( 3 . 10 ) 3 . 3 . 2 . 3 Meter Constraint The meter ( e . g . 3 / 4 , 4 / 4 , 7 / 8 ) deﬁnes the duration and the perceived accent patterns in regularly occurring bars of a music piece . For example , in a 4 / 4 meter , one can expect relatively strong accents on the ﬁrst and the third beat of a bar . We impose the meter extracted from a template piece on our sample , to obtain a degree of global rhythmic coherence . Perceived accent patterns depend on the relative occurrence of note onsets in a bar , on the intensity of played notes , or the length of notes starting at the respective positions of a bar . However , note intensities are not encoded in our data , and it is not obvious how to incorporate note durations in our diﬀerentiable cost function . 2 Even though the derivatives of min ( · ) and max ( · ) are not guaranteed to be always deﬁned , in practice these cases are hardly ever a problem in gradient descent , and are typically dealt with in software frameworks for symbolic diﬀerentiation such as Theano [ Theano Development Team , 2016 ] . 42 Figure 3 . 4 : Example of key estimation vectors over time . k ( z ) maj represent estimations for 12 possible major keys and k ( z ) min represent estimations for 12 possible minor keys , where the pitch classes constituting the tonic are ordered from the top to the bottom . Bright pixels represent high strength , and dark pixels represent low strength of the corresponding key . For example , the upper most line in k ( z ) maj represents the estimation strength of the C major key over time , the third line represents the strength of the D major key , etc . Therefore , we use note onsets only . To this end , we constrain the relative occurrence of note onsets within a bar to follow that of a template piece . Abiding by such a distribution helps the generated material to keep implying a regular meter . The onset function ω ( · ) results from a discrete diﬀerentiation over the time di - mension of an arbitrary music piece in piano roll representation z ∈ [ 0 , 1 ] T × P . We rectify that result ( as we are not interested in note oﬀsets ) , and sum over the pitch dimension : ω ( z , t ) = P X p max ( 0 , z t , p − z t − 1 , p ) . ( 3 . 11 ) In order to calculate the relative occurrences of onsets within a bar , the length T of a bar has to be pre - deﬁned . We count the number of onsets occurring on the respective positions of all bars in the music piece . This is , we sum up all values of distance T in the onset function ω ( · ) as ρ ( z ) τ = T / T X µ ω ( z , τ + µ ∗ T ) , ( 3 . 12 ) where τ ∈ 0 . . . T − 1 is the position in a bar . In our experiments , we use a resolution of 16 th notes in the representation and the template is in 4 / 4 meter , therefore T = 16 . To keep the function independent of the absolute number of onsets involved , ρ ( z ) is standardized by subtracting its mean ρ ( z ) and dividing through its standard 43 Bar position R e l a t i v e o n s e t f r e q u e n c y Relative onset frequencies on bar positions Figure 3 . 5 : Relative ( standardized ) onset frequencies ρ 0 ( z ) on bar positions of a music piece as obtained from Equation 3 . 13 . deviation σ ( ρ ( z ) ) , resulting in zero mean and unit variance : ρ 0 ( z ) = ρ ( z ) − ρ ( z ) σ ( ρ ( z ) ) . ( 3 . 13 ) A standardized onset distribution is plotted in Figure 3 . 5 . Finally , we minimize the MSE between a standardized onset distribution ρ 0 ( x ) and that of our interme - diate solution ρ 0 ( v ) as g ( x , v ) meter = k ρ 0 ( x ) − ρ 0 ( v ) k 2 T . ( 3 . 14 ) 3 . 4 Constrained Sampling In this Section , we describe how the C - RBM is used as a generative model to pro - duce musical textures that resemble that of human - composed music , and combined with the soft constraints described above , to enforce additional tonal , meter and self - similarity structure on those textures . The method proposed here has several practical merits . First , a C - RBM can take any input as a starting point for ( further ) sampling . This allows for local “mutations” of intermediate solution candidates in a heuristic process like Simu - lated Annealing ( see Section 3 . 4 . 2 ) and facilitates the controlled exploration of the search space . Second , in a C - RBM continuous values in the input are interpreted as probabilities . This facilitates external guidance through a gradual adaption of 44 note probabilities in a directed gradient descent ( GD ) optimization process . For illustration , Figure 3 . 6 [ 2a ] shows an example of a piano roll after a GD phase . The grey tones ( non - zero probabilities ) in the background of the piano roll will inﬂuence the subsequent sampling step from the C - RBM . Third , the solution is sampled as a single instance ( i . e . all notes in a music piece are updated simultaneously ) and temporal dependencies are modeled in a bi - directional manner . That way , global constraints can be imposed by iterative adaption of local structures . 3 . 4 . 1 Example Scheme and Details In Figure 3 . 1 , an overview of Constrained Sampling ( CS ) is shown . During the con - strained sampling process we alternate between a GS phase with the one - layered C - RBM ( Section 3 . 3 . 1 ) , and a GD optimization phase on the cost functions ( Sec - tion 3 . 3 . 2 ) . In each phase , typically multiple iterative updates take place , and the sampling results are sensitive to the balance struck between the GS and GD phases , in terms of the number of updates performed in each phase . The numbers proposed in the following CS sampling scheme have been found to work well in our experiments . The scheme may have to be adapted to work well with other training settings ( e . g . , diﬀerent C - RBM architectures , or diﬀerent constraints ) , and is mainly for illustrative purposes . In Algorithm 2 , the whole process including Simulated Annealing ( see Section 3 . 4 . 2 ) is shown . Starting from a random uniform noise in v , we alternate 20 GD steps using learning rate γ = 10 ( i . e . GD phase , see Figure 3 . 6 [ 2a ] for a result of this phase ) , and 1500 GS steps ( i . e . GS phase , see Figure 3 . 6 [ 3a ] for a result of this phase ) . We consider this one constrained sampling iteration . We found that results improve when , during the GS phase , after every 100 GS steps we execute 1 GD step with learning rate γ = 5 . After 250 CS iterations , the sample with the minimum average value of the standardized GD cost and the standardized free energy over the whole CS process is chosen ( see Section 3 . 4 . 2 on standardizing the cost and free energy functions ) . During CS , in the C - RBM the free energy is to be reduced ( i . e . a high probability solution is to be found ) , while in GD optimization the objective function is to be minimized ( see Figure 3 . 7 for a plot of the curves ) . As the two models used compete in approximating their objectives ( see Figure 3 . 8 ) , their mutual inﬂuence has to be balanced . In addition to using Simulated Annealing to prevent strong deteriorations of the solution concerning the objectives ( see Section 3 . 4 . 2 ) , some parameters need to be carefully adjusted . The main parameters for balancing the models are the number of GD and GS steps used in a CS iteration , as well as the learning rate and the relative weighting of the cost terms in the GD optimization ( see Tab . 3 . 1 for weightings used in our experiments ) . In general , the optimal number of steps in each model is inversely 45 F i g u r e 3 . 6 : I ll u s t r a t i o n o f c o n s t r a i n e d s a m p li n g . ( 1 ) T e m p l a t e p i ece , ( 2 ) I n t e r m e d i a t e s a m p l e a f t e r t h e G D ph a s e , ( 3 ) S a m p l e a f t e r t h e G S ph a s e . F i g u r e s i n e a c h g r o up : ( a ) P i a n o r o ll r e p r e s e n t a t i o n , ( b ) S e l f - s i m il a r i t y m a t r i x , ( c ) O n s e t d i s t r i bu t i o n i n 4 / 4 m e t e r , ( d ) K e y s c a p e ( s ee S ec t i o n 3 . 5 . 5 . 1 f o r a n e x p l a n a t i o n ) . A f t e r t h e G D ph a s e ( 2 ) , t h e t a r g e t h i g h e r - l e v e l p r o p e r t i e s i m p o s e d a s c o n s t r a i n t s a r e r e l a t i v e l y w e ll a pp r o x i m a t e d . D u e t o li m i t e d t r a i n i n g d a t a a nd t h e s t o c h a s t i c n a t u r e o f G i bb s s a m p li n g , a f t e r t h e G S ph a s e t h e h i g h e r - l e v e l p r o p e r t i e s a r e m o r e d i ss i m il a r aga i n . 46 Algorithm 2 : Constrained Sampling . Number of iterations represent an ex - ample scheme , as used in the experiments . Data : x ∈ [ 0 , 1 ] T × P – Template Piece v ∈ [ 0 , 1 ] T × P – Random ( standard uniform dist . ) ˆv = v , N = 250 , M = 15 1 for i ∈ 1 . . . N do 2 v 0 ← v 3 v ← 20 GD steps using Eq . 3 . 4 with γ = 10 4 for j ∈ 1 . . . M do 5 v ← 100 GS steps using v 6 v ← 1 GD step using Eq . 3 . 4 with γ = 5 7 end / * Simulated Annealing * / 8 T i = 1 − i / N 9 r e , r c ← random values ∈ [ 0 , 1 ] 10 if r e < exp (cid:16) − F 0 ( v ) −F 0 ( v 0 ) T i (cid:17) or r c < exp (cid:16) − φ 0 ( x , v ) − φ 0 ( x , v 0 ) T i (cid:17) then 11 v ← v 0 12 end / * Store best solution so far * / 13 if F 0 ( v ) + φ 0 ( x , v ) 2 < F 0 ( ˆv ) + φ 0 ( x , ˆv ) 2 then 14 ˆv ← v 15 end 16 end 17 return ˆv proportional to the size of the training corpus . The more training data , the more possible solutions can be sampled by the probabilistic model making it easier to satisfy constraints imposed by the GD optimization . Conversely , with a model trained on little data , more GS steps are necessary in order to ﬁnd another low free energy solution after being distracted by the GD phase . Although we do not provide a formal convergence proof , all experiments show a joint decrease of the various quantities to be minimized ( the C - RBM free energy , and the cost - functions of each of the constraints ) . Convergence is reached when both the gradient descent cost and the free energy of the C - RBM reach a minimum . When reaching equilibrium in an RBM , the visible unit conﬁguration ( the sample ) keeps changing during further sampling while the free energy remains at the minimum . Therefore , with our method convergence is reached concerning the overall ( average ) cost , but not concerning a ﬁnal solution in v . 47 Figure 3 . 7 : Standardised cost , free energy and their mean in a constrained sampling pro - cess over 250 iterations . Periods of constant cost ( horizontal line segments ) in later itera - tions are a result of Simulated Annealing , where some unfavorable solution candidates are rejected . 3 . 4 . 2 Simulated Annealing Due to the interdependency between the sampling process of the probabilistic model and the GD optimizer , it can easily happen that good intermediate solutions de - teriorate again by further sampling . Simulated Annealing ( SA ) helps to ﬁnd good minima by preventing sampling steps which would lower the solution quality too much ( see Algorithm 2 for the integration of SA in CS ) . After each constrained sampling ( CS ) iteration , we evaluate the SA equation to obtain the probability p k ( v , v 0 , i ) = exp (cid:18) − f ( v ) − f ( v 0 ) T i (cid:19) ( 3 . 15 ) of keeping solution candidate v , where v 0 is the previous solution . We evaluate this equation twice after each CS iteration . The ﬁrst time , f ( · ) is the standardised RBM free energy function F 0 ( · ) ( see Equation 3 . 1 ) and the second time , f ( · ) is the standardised GD cost function φ 0 ( · ) ( see Equation 3 . 5 ) . For each of the two resulting probabilities , we generate a random number between 0 and 1 , and evaluate if it is smaller than the respective probability . If this is the case for both random numbers , 48 - 20000 - 10000 0 10000 20000 30000 40000 0 . 1 1 10 100 1000 10000 F r ee e n e r g y o f t h e C - R B M Cost φ ( x , v ) Cost vs . Free Energy GS and GD Only GS Only GD Uniform Noise Figure 3 . 8 : Inﬂuence of Gibbs sampling ( GS ) and gradient descent ( GD ) on free energy F ( v ) ( see Equation 3 . 1 ) and cost φ ( x , v ) ( see Equation 3 . 5 ) . Using only GS results in low free energy but relatively high cost . Using only GD , the cost is very low , but the free energy is high . When using GS and GD , both methods compete , resulting in a trade - oﬀ between low cost and low free energy although we choose enough GS steps in the GS phase to always return to a “meaningful” , low free energy state . For reference we test against random uniform noise , resulting in very high free energy and cost . For each cluster , 50 data points were generated with the trained C - RBM model ( for GS ) and the cost function ( for GD ) used in our experiment ( see Section 3 . 5 ) . we go on with solution candidate v , otherwise , we return to the former solution v 0 . The most crucial factor for the sensitivity of SA is the variance of f ( · ) over all solutions , where a higher variance leads to smaller probabilities for acceptance of a solution . Therefore , we standardise F ( · ) and φ ( · ) , resulting in F 0 ( · ) and φ 0 ( · ) , to obtain comparable probabilities in SA . This is done by scaling both functions to approximately zero mean and unit variance , based on the observed values during the experiments . As annealing scheme we use T i = 1 − i / N . In Figure 3 . 7 the standardized curves over a CS process are depicted . In later iterations , Simulated Annealing causes periods of constant cost , as some solution candidates are rejected . 3 . 5 Experiment This section describes an experimental validation of the method described in Sec - tions 3 . 3 and 3 . 4 . In Section 3 . 5 . 1 and Section 3 . 5 . 2 , we introduce the training data and the data representation scheme , respectively . Section 3 . 5 . 3 describes the training of the C - RBM . In Section 3 . 5 . 4 we use the Information Rate ( IR ) adopted from [ Wang and Dubnov , 2015 ] to quantify the structural organization of music . 49 Constraint w d Self - similarity 1 . 5 Tonality 5 . 0 Meter 0 . 5 Table 3 . 1 : Relative weightings w d of the terms used in the GD objective function φ ( x , v ) ( see Section 3 . 3 . 2 ) . With the IR metric , we compare our constrained sampling approach with other state - of - the - art polyphonic music generation methods and with original pieces of Mozart . Lastly , Section 3 . 5 . 5 brieﬂy describes the procedure followed to produce musical material for qualitative evaluation . 3 . 5 . 1 Training Data We use MIDI ﬁles encoding the scores of the second movement of three Mozart piano sonatas , as encoded in the Mozart / Batik data set [ Widmer , 2003 ] : Sonata No . 1 in C major , Sonata No . 2 in F major and Sonata No . 3 in B ﬂat major . When applying a ( major ) tonality constraint , we want to make sure that there is enough training data for the probabilistic model in any possible ( major ) key . Otherwise , in the GS phase , an intermediate solution might always be changed back from a key imposed by the GD optimization to the closest key available in the training data . Therefore , we transpose each piece into all possible keys , which also helps to reduce sparsity in the training data . This results in a training corpus size of 15144 time steps ( of sixteenth note resolution ) . 3 . 5 . 1 . 1 A Note on Training Data Set Size A widely shared insight in machine learning is that more data is better when training neural network models . In general , it is easy to see why this is the case since larger amounts of data provide a richer coverage of the relations to be learned in the data . However , depending on the intended purpose of the model , there may be exceptions to this rule . In the present study , where the primary purpose of the model is to generate plausible musical textures in the style of Mozart piano sonatas , we have found that the set of all available training data ( 34 pieces ) is likely too small for the C - RBM model to approximate the data distribution well enough to produce samples of high musical quality . A pragmatic trade - oﬀ we have chosen in this case is to reduce the size of the training data to a few pieces , and let the model slightly overﬁt those pieces . This will improve the musical quality of the samples , at the cost 50 of increased local resemblances of generated samples to fragments of the training data . 3 . 5 . 2 Data Representation We transform MIDI data in a binary piano roll representation of T = 512 time steps over a range of P = 64 pitches ( MIDI pitch number 28 - 92 ) , using a temporal resolution of sixteenth notes ( see Figure 3 . 6 [ 1a ] ) . Notes are represented by active units ( black pixels ) , and note durations are encoded by activating units up to the note oﬀset . If two notes directly follow each other at the same pitch , they cannot be distinguished anymore . Thus , the ﬁrst note is shortened by a sixteenth note if possible ( i . e . if it is longer than a sixteenth note ) ; otherwise the merger has to be accepted . Note that a temporal subdivision of sixteenth notes cannot represent all rhythmic patterns in the data without distortion . For example , the durations { 1 / 12 , 1 / 12 , 1 / 12 } of 1 / 8 note triplets ( as contained in the Sonata No . 1 ) change to { 1 / 16 , 1 / 8 , 1 / 16 } using this representation . We accept this bias , as it does not hinder our eﬀorts to test the inﬂuence of constraints on a generated texture . 3 . 5 . 3 Training We train a single C - RBM using Persistent Contrastive Divergence ( PCD ) [ Tiele - man , 2008 ] with 10 fantasy particles , using learning rate 15 × 10 − 4 . Compared to standard Contrastive Divergence [ Hinton et al . , 2006 ] , the PCD variant is known to draw better samples . One training instance has a length of T = 512 , and we use a batch size of 1 . The ﬁlter width R ( see Section 3 . 3 . 1 ) is set to 17 , and we convolve only in the time dimension with stride 4 , using 2048 hidden units . We apply the well - known L1 and L2 weight regularization with strengths 8 × 10 − 4 and 1 × 10 − 2 , respectively , to prevent overﬁtting and exploding weights . Also , we use the max - norm regularization [ Srebro and Shraibman , 2005 ] , which is additional protection against exploding weights when using high learning rates . We also use sparsity regularization as introduced in [ Lee et al . , 2007 ] , to increase sparsity and selectivity in the hidden unit activations , leading to a better generalization of the data . When training with PCD , single neurons may always be active , independent of the presented input . Therefore we reset ( i . e . randomize ) the weights of any neuron which exceeds the threshold of 0 . 85 average activation over the data . 3 . 5 . 4 Quantitative Evaluation Based on the observation that probabilistic models can generate meaningful low - level structure but struggle in obeying some higher - level structure , the focus of this study is to increase the structural organization of the generated material . In the important case of self - similarity structure , a critical property in music is the balance 51 of repetition and variation . This ratio is expressed by an information theoretic measure called Information Rate ( IR ) . It is the mutual information between the present and the past observations and is maximal when repetition and variation are in balance . Thus , the IR is minimal for random sequences , as well as for very repetitive sequences . It has been shown that it provides a meaningful estimator on musical structure , for instance in parameter selection for musical pattern discovery [ Wang and Dubnov , 2015 ] . For a given sequence v N 0 = { v 0 , v 1 , v 2 , . . . , v N } , the average IR is deﬁned by IR ( v N 0 ) = 1 N N X n H ( v n ) − H ( v n | v n − 1 0 ) , ( 3 . 16 ) where H ( v ) is the entropy of v , which is estimated based on the statistics of the sequence up to event v n . We approximate H ( v n | v n − 1 0 ) using a ﬁrst - order Markov Chain , and H ( v n ) by counting identical time slices . It may seem counterintuitive at ﬁrst sight to utilize a ﬁrst - order model for measuring the higher - level structure of a piece . Arguably , a low - order estimation yields too optimistic IR values , as the conditional entropy tends to be underestimated . However , the initial idea of con - trasting the prior entropy of events with their conditional entropy is still applicable using a ﬁrst - order entropy estimation . That is , a high IR is achieved when speciﬁc events occur rarely but are very likely given their direct predecessors—a situation which occurs mainly in sequences with higher - level repetition structure . Note that the IR does not provide a measure for the overall musical quality of the evaluated sequences , but only for the aspect of self - similarity structure from an information theoretic point of view . 3 . 5 . 4 . 1 Model Comparison In addition to using the C - RBM without constraints , we use the RNN - RBM [ Boulanger - Lewandowski et al . , 2012 ] , a state - of - the - art polyphonic music gener - ation model , as a baseline for the quantitative evaluation . Furthermore , we replace the RNN portion of the RNN - RBM with Gated Recurrent Units ( GRUs , Cho et al . , 2014 ) resulting in a GRU - RBM . Both recurrent models are trained on the same data as the C - RBM , as described in Section 3 . 5 . 1 3 . We compare the average Information Rates between original Mozart piano sonatas ( all 34 pieces of the Mozart / Batik data set , Widmer , 2003 ) , C - RBM con - strained samples using the original pieces as structure templates ( 3 samples per original piece resulting in 102 samples with diﬀerent lengths ) , 102 C - RBM un - constrained samples , 102 RNN - RBM unconstrained samples and 102 GRU - RBM 3 Samples from the RNN - RBM and the GRU - RBM can be listened to on Soundcloud under http : / / www . soundcloud . com / pmgrbm 52 I n f o r m a t i o n r a t e MozartOriginal C - RBM Constrained C - RBM Unconstrained RNN - RBM Unconstrained GRU - RBM Unconstrained Figure 3 . 9 : Box plot showing Average Information Rates for 34 original Mozart piano sonatas , 102 C - RBM samples with structure constraints , 102 C - RBM samples without constraints , 102 samples from an RNN - RBM and 102 samples from a GRU - RBM . Whiskers show standard deviations . unconstrained samples . The 102 unconstrained samples per model are created by generating three samples for each original piece in the length of the original piece . For results on this comparison see Figure 3 . 9 , for a discussion see Section 3 . 6 . 3 . 5 . 4 . 2 Further Measures for Evaluating Musical Structure Information theory could provide additional quantitative measures for the evalua - tion of structure in music . An essential basis for that is Information Content ( IC ) , a measure of the predictability of an event in a speciﬁc context . Prior research has shown that IC can act as a kernel for determining segment boundaries ( see [ Pearce et al . , 2010a ] and Chapter 2 ) . Evaluating the plausibility of IC over time in gener - ated sequences could , therefore , constitute an adequate measure for the evaluation of musical structure . Moreover , the principle of uniform information density ( UID ) is a theory origi - nating in linguistics . It is based on the proposal of Shannon [ 1948 ] that for optimal data ﬂow through a noisy channel , the transferred information density ( i . e . , the IC per time step ) should be as uniform as possible . It was shown that speakers intu - itively follow these rules to keep the processing eﬀort of the receiver at a moderate level [ Levy and Jaeger , 2006 , Aylett and Turk , 2006 ] . Recent research provides evidence that UID could also account for structural decisions in music composition , where it implies that the average IC in any window of ﬁxed duration over a musical piece should be constant . For example , Temperley [ 2014 ] states “There is a tendency that when an intervallic pattern is repeated 53 with alterations , the alterations tend to lower the probability of the pattern rather than raising it” . When a musical passage is repeated , its Information Content ( i . e . , the listener’s surprise ) declines . The ﬁnding mentioned above provides some evidence that in such cases , surprising alterations should be inserted to keep the UID constant . Since the IR is suﬃcient for evaluating our results , we do not use IC and UID . Nevertheless , they seem promising as further evaluation measures to quantify struc - ture in generated music . 3 . 5 . 5 Qualitative Evaluation The C - RBM is trained as described in Section 3 . 5 . 3 on the Mozart Sonatas ( see Section 3 . 5 . 1 ) . After that , we pick a template piece ( the ﬁrst movement of the piano sonata No . 6 in D major ) and generate constrained samples , as introduced in Section 3 . 4 . For the weights used to balance the diﬀerent terms in the GD cost function , please see Tab . 3 . 1 . In the self - similarity constraint ( see Section 3 . 3 . 2 . 1 ) , we use a window size Λ of 8 ( i . e . half a bar ) , and for the tonality constraint we use an estimation window width M of 4 ( see Section 3 . 3 . 2 . 2 ) . Figure 3 . 10 shows some resulting samples , which are discussed in detail in Section 3 . 6 . 3 . 5 . 5 . 1 Keyscape We use keyscapes to illustrate the tonality of the pieces in Figure 3 . 6 and Fig - ure 3 . 10 . A keyscape illustrates the tonal context over a musical piece , where each key receives a distinct color . We use the humdrum mkeyscape tool by David Huron , which analyses the musical piece with the Krumhansl - Schmuckler key - ﬁnding algo - rithm [ Krumhansl , 1990 ] in diﬀerent levels of detail . The top of the pyramid depicts the key estimation for the entire piece , while towards the base the analysis is based on ever smaller window sizes . Each scale has a distinct color assigned to it , and the keyscape is colored according to the most predominant scale estimation . 3 . 6 Results and Discussion 3 . 6 . 1 Quantitative Evaluation Figure 3 . 9 shows average Information Rates ( IRs ) for original Mozart piano sonatas and for samples from diﬀerent models ( see Section 3 . 5 . 4 ) , where higher IRs indi - cate more distinct self - similarity structures . It should be pointed out in advance that sampling from a probabilistic model introduces some sampling noise which increases the predictive entropy and therefore lowers the IR . It is diﬃcult to judge to what extent sampling noise on the one hand , and diﬃculties of the model to 54 3 . 6 Results and Discussion Figure 3 . 10 : Template piece ( 1 ) , Constrained samples ( 2 to 5 ) and an unconstrained sam - ple as baseline ( 6 ) . Figures in each group : ( a ) Piano roll representation , ( b ) Self - similarity matrix , ( c ) Onset distribution in 4 / 4 meter , ( d ) Keyscape . By constrained sampling , the template piece’s self - similarity and tonal structure , as well as the onset distributions , are transferred to the generated solutions 2 to 5 . The unconstrained sample ( 6 ) at the bottom was sampled without constraints , and thus does not reﬂect the structure of the template piece . 55 adapt to given constraints , on the other hand , lead to the signiﬁcantly lower IR of the constrained samples compared to the IR of the original Mozart transcriptions . Nevertheless , it is clear from the results ﬁrstly that the IR of the original music is higher than that of the generated music , and secondly that the models without constraints produce music with lower IR than the constrained C - RBM does . Note that the latter point is a non - trivial result , since the self - similarity constraint does not explicitly optimize the Information Rate ( neither do the tonal or meter con - straints , obviously ) , but encourages similarity or dissimilarity between the music at speciﬁc positions . This result is in accordance with the initial observation that the baseline models fail to generate higher - level self - similarity structure . Due to their gating mechanism , GRUs are usually better at learning long - term dependen - cies than regular recurrent units . The fact that the GRU - RBM does not perform better than the RNN - RBM shows that GRUs also have problems in modeling the content - invariant self - similarity property . 3 . 6 . 2 Qualitative Evaluation Figure 3 . 10 shows piano roll representations for the template piece ( Figure 3 . 10 [ 1a ] ) , four generated samples that were constrained with properties from the template piece ( Figure 3 . 10 [ 2a ] to Figure 3 . 10 [ 5a ] ) and a baseline sample generated with - out constraints from the template piece ( Figure 3 . 10 [ 6a ] ) . The corresponding con - straints for each musical piece are depicted in the respective ﬁgures b - d . The repetition structure is labelled on top of the ﬁgure with the typical uppercase let - ters used to describe musical form , and the section boundaries are indicated with vertical , green lines . 4 We chose the constrained samples by creating 20 solutions and picking the best four concerning the minimum average value of the standardized GD cost and the standardized free energy over a constrained sampling process ( see Section 3 . 4 . 2 on standardizing the cost and free energy functions ) . Thus , results are selected which satisfy the given constraints better , rather than according to their musical quality . Empirically we found that the musical quality in our setting increases when loosening the inﬂuence of the constraints , as this allows the probabilistic model to create more plausible samples ( e . g . the examples in Figure 3 . 10 sometimes lack appropriate transitions between diﬀerent sections which are an eﬀect of both constraint satisfaction and limited training data ) . By approximating the self - similarity matrix of the template piece , some aspects of the repetition structure were convincingly transferred to the constrained samples ( see Figure 3 . 10 [ 1b ] to Figure 3 . 10 [ 5b ] ) . For example , the exact repetitions C / C’ and H / H’ occur in every sample . It is interesting to see how the extension of B to 4 All samples illustrated in Figure 3 . 10 can be listened to on Soundcloud under http : / / www . soundcloud . com / pmgrbm 56 B’ is solved . Especially in the samples depicted in Figure 3 . 10 [ 2 ] and Figure 3 . 10 [ 3 ] , the extension of B is realized by musical textures consistent with the immediate past . In the sample in Figure 3 . 10 [ 5 ] , the model did not produce satisfactory results for phrase B and B’ in a musical sense , although it found a solution which is self - similar over that time period and therefore satisﬁes that self - similarity constraint to a certain degree . Parts E / E’ / E” are special cases , because even though they are very similar at ﬁrst sight , they are transposed repetitions which cannot be captured by the self - similarity matrix as it is currently deﬁned . In the self - similarity matrix of the template , we see that each of those “E” sections is more or less similar or dissimilar to diﬀerent regions in the piece . Also , we note that each repetition has the length of one bar . When comparing these “E” sections with those in the samples , we recognize the limits of the method concerning temporal resolution . The C - RBM has a ﬁlter length of one bar , which is too wide for sampling three bars with diﬀerent requirements concerning similarity while keeping a plausible low - level structure . Therefore , in some samples , the generated patterns span the whole , or at least two of the “E” sections . Part G in the repetition structure is similar to most parts of the piece , as can be seen from the bright areas over the full height of the respective self - similarity matrices . In the samples , this is realized by choosing textures which are also similar to most parts . Part J , in contrast , is very dissimilar to most areas of the template piece . Probably due to limited training data , this sometimes results in empty areas in the samples . Except for an apparent similarity in B and B’ , which is not reﬂected in the self - similarity matrix , the unconstrained baseline sample does not follow the repetition structure of the template piece . The onset distributions ( see Subplots c in Figure 3 . 10 ) , which are plots resulting from Equation 3 . 13 , are sometimes rather dissimilar to the onset distribution of the template . This shows that it is not easy to approximate this global property . One reason for this may be that it is a property which summarizes the complete music piece in only a few values , which makes it easy in the GD optimization to approx - imate by distributing small changes over the whole sample . Those are , however , locally not strong enough to be kept during GS . Incorporating note durations for emphasizing onsets of longer notes would lead to more characteristic onset distribu - tions , which could further lead to more substantial local changes in the probability of notes in the piano roll . However , in the onset distributions , there is a tendency of the peaks at position 0 and 8 to be higher than the others , which corresponds to the tendency in the onset distribution of the template piece . Note that the reason for every second value in the distributions being low is not the meter constraint but the stride of one beat in the convolution . This provides the model with a regular grid allowing it to learn that the probability of an onset is lower at every second time step . Therefore , those values are also low for the unconstrained baseline piece . 57 The keyscape ( see Section 3 . 5 . 5 . 1 ) for each sample is depicted in the respective subplots d . We can see that the main key ( i . e . A major ) of the template piece got well transferred to the constrained samples , as the colors of the upper areas of the keyscapes ( purple ) match exactly . Towards the lower areas of the keyscapes , the colors of some samples do not correlate with those of the template piece’s keyscape . However , especially the modulation to E major in the second quarter of the piece , depicted in red , and the blue area at the beginning of the piece ( D major ) are to some degree approximated in Figure 3 . 10 [ 2d ] and Figure 3 . 10 [ 4d ] . In sample Figure 3 . 10 [ 3 ] , the green area indicates a F ] minor scale , which is similar to the E major scale ( red ) of the template piece ( i . e . there is a diﬀerence in one note , namely D / D ] ) . In general , the tonal structure of constrained samples is more stable than that of the unconstrained baseline sample , where the keyscape indicates tonal incoherence . As mentioned above , the illustrated samples are the best four of 20 concerning the overall cost . The most obvious shortcoming of samples not selected because of the higher cost is that they do not satisfy some of the given structural constraints on a local level . This includes the failure to reproduce a repetition at speciﬁc positions , or erroneously modulating into a key which does not occur in the template piece . However , a closer inspection of such cases shows that incorrect keys are often closely related to the desired keys , for instance , the parallel minor / major key . Parallel minor / major keys have most of their pitches in common , but they are expected to follow a diﬀerent distribution . Another common problem in the non - optimal samples is that they show areas without any notes , which is probably a symptom of contrasting objectives of GD and GS . We found that the C - RBM is very sensitive to changes in the higher - level parameters of the system . Other models amenable to GS could lead to a more stable functioning , like LSTMs used with GS in [ Hadjeres et al . , 2017 ] . 3 . 7 Conclusion and Future Work Music is typically highly structured at both lower and higher levels . State - of - the - art sequence models such as RNNs and LSTMs have been successfully used to generate music in restricted settings , but in more complex musical material , such as piano music from the classical or romantic period , not to mention orchestral works , important musical characteristics such as tonal , metrical and self - similarity structure tend to defy straight - forward time series modeling approaches . The method for music generation presented here addresses this problem by com - bining a stochastic neural network for sampling plausible musical textures at a local level with soft constraints that impose higher - level structure regarding meter , tonality and self - similarity structure , obtained from a template piece . 58 The experimental validation of the proposed method reveals that the generated music possesses a stronger degree of structural organization ( as measured by Infor - mation Rate ( IR ) ) than unconstrained models , including a state - of - the - art RNN - RBM model for polyphonic music generation . A qualitative analysis of some gen - erated music supports this ﬁnding and clearly reveals repeated ( but not identical ) musical patterns , as well as global similarities in tonal structure to the template piece . The empirical results also reveal some shortcomings . Firstly , while imposing constraints with the proposed method helps to generate high - level structure , a meaningful low - level structure can currently only be generated when the model is trained on relatively small amounts of data . Overcoming this drawback may require more powerful generative models—amenable to some form of Gibbs sampling—as an alternative to the C - RBM . We hypothesize that generative models can only gen - eralize well on the low - level structure if they can explicitly represent ( transposed ) repetition . A promising approach to this is proposed in the next chapter , where it is shown that relations between musical sections can be learned and represented as so - called “mapping codes” . Secondly , when listening to the generated musical samples , it is clear that the tonal , meter and self - similarity constraints presented here are by no means fully elaborated nor exhaustive . For example , more specialized constraints—like a dif - ferentiable formalization of the IR measure—could optimize sequences to obey de - sired structural properties directly . Perhaps most importantly , what is currently missing is a constraint that enforces musical closure at boundaries of structural units . Without such a constraint , the music contains repeated musical structures , but these structures are hard to identify perceptually because their boundaries are not marked by salient musical cues ( such as harmonic resolution ) . That said , the proposed constrained sampling approach is general enough to accommodate this and possibly other constraints . 59 4 Learning Musical Structure by Learning Transformations In Section 3 , we proposed to generate music with higher - level structure using ex - ternal constraints in the sampling process . A more elegant solution would combine both the generative model and structural constraints into a generative model with implicit awareness of structural properties . In this chapter , we start from the obser - vation that structured music can be described by sets of transformations between musical sections and show that a Gated Autoencoder ( GAE ) can learn such mu - sical transformations . We then adapt the GAE to musical sequence learning , ﬁrst by employing predictive training , and then by combining the GAE with an RNN . The competitive results in prediction tasks suggest that learning musical transfor - mations is a powerful concept in music modeling and that in architectures like the GAE both generation and structure learning can be combined in a single , integrated solution . 4 . 1 Introduction An important notion in western music is that of structure : the phenomenon that a musical piece is not an indiscriminate stream of events , but can be decomposed into temporal segments , often in a hierarchical manner . An important factor de - termining what we regard as structural units is the relation of these units to each other . The most apparent relation is the literal repetition of a segment of music on multiple occasions in the piece . However , more complex music tends to convey structure not only through repetition , but also through other types of relations be - tween structural units , such as chromatic or diatonic transpositions , or rhythmical changes . We refer to these relations in general as transformations . Figure 4 . 1 shows an example of mid - level structure induced by transformations in an excerpt of a rondo by W . A . Mozart where related phrases are marked by boxes in the same color ( note that this interpretation is not unique ) . The yellow boxes mark the input and output of a function f 0 performing a diatonic transposition by - 1 scale step . The same function is applied in the bass section , marked with red boxes . Likewise , f 1 performs a diatonic transposition by + 1 scale step in the melody section ( blue boxes ) and the bass section ( purple boxes ) . The transformations deﬁned in f 0 and f 1 constitute structural relationships which may be applied to any musical 61 Figure 4 . 1 : Beginning of the “Rondo in D major” ( K . 485 ) by W . A . Mozart in Western music notation and in a piano roll representation , where transformation functions f n eﬀect diatonic transpositions ( best viewed in color ) . In the piano roll ( bottom ) , green horizontal lines mark the diatonic pitches in the scale of D major . material and in that sense are content - invariant . Note how this view on music as a collection of basic musical material that is transformed in a variety of ways can provide very concise and schematically simple representations of a musical piece . Finding such a representation may be a goal in itself ( music analysis ) , or it may serve as the basis for other tasks , such as music summarization , music classiﬁcation , and similarity estimation , music generation or computer - assisted composition . There are some fundamental challenges to automated detection of structure in music , however . Firstly , although there are a few common types of transformations such as chromatic and diatonic transposition , it is not easy to give an exhaustive list of transformation types that should be considered when comparing potential structural units of a musical piece . Ideally , rather than relying on human intuition , we would like to infer transformations from actual music that can account for a large portion of structure in that music . A second challenge is that composers may use the concept of transformation as a compositional tool , but in their artistic freedom , they may perform further ad - hoc alterations at will . The resulting musical material may thus exhibit approximate transformations , deviating from exact transformations in subtle but unpredictable ways . State of the art methods for the motivic analysis of music , such as the compression - based methods discussed in [ Louboutin and Meredith , 2016 ] , do not 62 directly address these issues . We believe that a more data - driven approach is needed , where transformations are learned from data in an unsupervised man - ner . This naturally leads us to consider connectionist models for unsupervised learning , such as Restricted Boltzmann Machines ( RBMs ) [ Hinton et al . , 2006 ] , and autoencoders [ Bengio , 2009 ] . A particularly promising approach towards unsu - pervised learning of content - invariant musical transformations are bilinear models like Gated Autoencoders ( GAEs ) [ Memisevic , 2011 ] or Factored Boltzmann Ma - chines [ Memisevic and Hinton , 2010 ] . By employing multiplicative interactions , these models can learn co - variances between pairs of data instances . It was shown that GAEs are eﬀective at encoding transformations between image pairs , such as spatial translation and rotation of objects [ Memisevic , 2013 ] . In this chapter , we incrementally extend the original GAE architecture in order to better adapt it to music processing and structure learning . First , we investigate its ability to learn transformations from given pairs of musical fragments ( i . e . , a symmetrical setting ) , more speciﬁcally n - grams of vertical slices from a piano roll representation . A comparison of the GAE with an RBM architecture demonstrates the GAE’s particular suitability for learning transformations in music ( see Section 4 . 3 ) . Second , we modify the GAE architecture and training procedure to obtain a predictive setting . Thereby , transformations between musical notes—intervals— can be eﬀectively learned . Interval representations are transposition - invariant and are therefore useful , for example , to detect repeated themes and sections in music ( see Section 4 . 4 ) . Third , we combine the GAE with an RNN in order to model the temporal succes - sion of intervals . We show that the thereby obtained Recurrent Gated Autoencoder ( RGAE ) is more general than common RNNs because it yields improved prediction performance and can better learn structural aspects of music ( i . e . , repetition and transposition , see Section 4 . 5 ) . 4 . 2 Related Work In the following , we shall cover the related work of this chapter , ordered according to the sections of the chapter . First , related work and the origin of the underlying model—the GAE—is presented . Second , works intending to perform analogy mak - ing by learning relations between data instances are discussed . Finally , related work to invariance learning and ( musical ) sequence prediction with GAEs is covered . GAEs utilize multiplicative interactions to learn correlations between or within data instances and are also referred to as bi - linear models . Bi - linear models are two - factor models whose outputs are linear in either factor when the other is held constant , a property which also applies to the GAE . The method was inspired by the 63 Correlation Theory of the Brain [ Von Der Malsburg , 1994 ] , where it was pointed out that some cognitive phenomena cannot be explained with the conventional brain theory and an extension was proposed which involves the correlation of neural patterns . This principle was further used in the work of Adelson and Bergen [ 1985 ] , where motion patterns in the three - dimensional x - y - t space are modeled by ﬁlter pairs receptive to distinct orientations in that space . This work shows that mapping units in a GAE function similar to complex cells in the visual cortex , which gives rise to critical perceptual processes like detecting ﬂuent motion from a series of static images . In machine learning , this principle was deployed in bi - linear models , for exam - ple , to separate person and pose of face images [ Tenenbaum and Freeman , 2000 ] . [ Olshausen et al . , 2007 ] proposed another variant of a bi - linear model in order to learn objects and their optical ﬂow . Due to its similar architecture , the Gated Boltzmann Machine ( GBM ) [ Memisevic and Hinton , 2007 , 2010 ] can be seen as a direct predecessor of the GAE . The GAE was introduced by [ Memisevic , 2011 ] as a derivative of the GBM , as standard learning criteria ( e . g . , minimizing cross - entropy or mean - squared error ) became applicable through the development of Denoising Autoencoders [ Vincent et al . , 2010 ] . GAEs were further used to learn transformation - invariant representations for classiﬁcation tasks [ Memisevic , 2012 ] , for parent - oﬀspring resemblance [ Dehghan et al . , 2014 ] , for learning to negate adjectives in linguistics [ Rimell et al . , 2017 ] , for activity recognition with the Kinekt sensor [ Mocanu et al . , 2015 ] , in robotics to learn to write numbers [ Droniou et al . , 2014 ] , for learning multi - modal mappings between action , sound , and visual stimuli [ Droniou et al . , 2015 ] , and for modeling facial expression [ Susskind et al . , 2011 ] . In Section 4 . 3 , we test the GAE in learning transformations between musical fragments . The problem of detecting musical relations falls in the class of analogy - making [ Hofstadter and Mitchell , 1995 ] , an essential capability of the human brain in which the objective is to produce a data instance X given the three instances A , B , C , and the query “X is to A as B is to C . ” Nichols [ 2012 ] shows how analogy - making also plays a vital role in music cognition , and “Musicat” , a musical analogy - making model is presented . Identifying analogies between musical sections exhibiting the same transformation is a ﬁrst step towards identifying similarities between trans - formed musical objects ( i . e . by utilizing transformation - invariant representations [ Memisevic and Exarchakis , 2013 ] ) . The problem of relating data instances is also tackled by some deep - learning methods , like in [ Reed et al . , 2015 ] , where visual analogy - making is performed by a deep convolutional neural network using a supervised strategy . Siamese architec - tures are used by Bromley et al . [ 1993 ] for signature veriﬁcation , and by Chopra et al . [ 2005 ] to identify identical persons from face images in diﬀerent poses . With a 64 GAE , a mapping between two inputs is learned explicitly . In contrast , with conven - tional deep - learning methods , relations are often indirectly qualiﬁed by comparing representations of related instances . For example , it is a common approach to con - struct a space in which operations like addition and subtraction of data vectors imply some semantic meaning [ Mikolov et al . , 2013 , Pennington et al . , 2014 ] . An analogy - making model similar to the GAE is the Transforming Autoencoder introduced by Hinton et al . [ 2011 ] . In contrast to the GAE , this model is super - vised , as during training the respective transformations have to be speciﬁed in a parameterized way ( e . g . , rotation angle or distance of shift in image transforma - tion ) . Another related model is the Spatial Transformer Network , which transforms an input image ( conditioned on itself ) before passing it on to classiﬁcation layers [ Jaderberg et al . , 2015 ] . Very few works exist on bi - linear models applied to music and audio . A form of a bi - linear model was applied to learn co - variances within spectrogram data for music similarity estimation [ Schlueter and Osendorfer , 2011 ] . In Section 4 . 4 , we show how the GAE can be used for learning transposition - invariant interval representations in music . Transposition - invariance is also achieved in [ Meredith et al . , 2002 ] , by transforming symbolic music into point - sets , in which translatable patterns are identiﬁed . In [ Marolt , 2008 ] , an approach for calculating transposition - invariant mid - level representations from audio , based on the 2 - D power spectrum of melodic fragments , is introduced . Similarly , a method to calculating non - invertible but interpretable interval representations from audio is given in [ Walters et al . , 2012 ] , where chromagrams which are close in time are cross - correlated to obtain local pitch - invariance . In contrast to these methods , our approach is invertible and learns ﬁlters to correlate spectrogram bins of variable time lags without the need for preprocessing . In Section 4 . 5 , we propose the Recurrent Gated Autoencoder ( RGAE ) and show its potential for sequence prediction , and the learning of possibly transposed repetitions of short musical fragments . In sequence modeling , the GAE was utilized to learn co - variances between subsequent frames in movies of rotated 3D objects [ Memisevic and Exarchakis , 2013 ] and to predict accelerated motion by stacking more layers in order to learn higher - order derivatives [ Michalski et al . , 2014 ] . The latter method is very similar to the one proposed in Section 4 . 5 , as it also learns transformations from sequential data . It assumes constant transformation between all time steps in a sequence , and the input and output of a model instance have the same size . In contrast , we use diﬀerent dimensionalities between input and output , and we do not assume constant transformation but rather learn sequences of transformations using an RNN . Probabilistic n - gram models specialized on learning to predict monophonic pitch sequences include IDyOM [ Pearce , 2005 ] , and [ Langhabel et al . , 2017 ] . Both employ multiple features of the musical surface . In this chapter , we do not compare the 65 RGAE with these models , but rather with other recurrent neural network models , which use the same feature representation as the RGAE . In particular , we compare the RGAE to the currently best performing recurrent connectionist sequence model , the Recurrent Temporal Discriminative Restricted Boltzmann Machine ( RTDRBM ) [ Cherla , 2016 ] . Its architecture is similar to the well - known RTRBM proposed in [ Sutskever et al . , 2008 ] , but it employs a diﬀerent cost function . For structured sequence generation , Markov chains together with pre - deﬁned repetition structure schemes were employed in [ Collins et al . , 2016 ] , where speciﬁc methods for handling transitions between repeating segments were proposed ; in [ Pachet et al . , 2017 ] , where an approach to a controlled creation of variations was introduced ; in [ Conklin , 2016 ] , where chords were generated , obeying a pre - deﬁned repetition structure . A constrained variable neighborhood search to generate poly - phonic music obeying a tension proﬁle and the repetition structure from a template piece was proposed in [ Herremans and Chew , 2016 ] . In [ Eigenfeldt and Pasquier , 2013 ] , Markov chains and evolutionary algorithms were used to generate repeti - tion structure for Electronic Dance Music . Recently , connectionist architectures have been proposed able to reproduce higher - level structural characteristics of the training data [ Huang et al . , 2019 ] . 4 . 3 Learning Transformations of Musical Material As stated in the introduction of this chapter ( see Section 4 . 1 ) , structural relation - ships are often based on transformations of musical material , like chromatic or diatonic transposition , inversion , retrograde , or rhythm change . In this section , we study the potential of two unsupervised learning techniques—Restricted Boltz - mann Machines ( RBMs ) and Gated Autoencoders ( GAEs ) —to capture pre - deﬁned transformations from constructed data pairs . More speciﬁcally , we deﬁne common musical transformations ( chromatic transposition , diatonic transposition , tempo change and retrograde ) , and construct n - gram pairs accordingly ( see Section 4 . 3 . 2 ) . Such a controlled setting provides ground - truth data and allows for performing a proof - of - concept on the general suitability of GAEs in music , and should be re - garded as a ﬁrst step towards identifying and learning transformations implicit in a music corpus . We evaluate the models by using the learned representations as inputs in a dis - criminative task where for a given type of transformation ( e . g . , diatonic transposi - tion ) , the speciﬁc relation between two musical patterns must be recognized ( e . g . , an upward transposition of 3 diatonic steps , see Section 4 . 3 . 3 . 1 ) . Furthermore , we measure the error of the models when reconstructing transformed musical pat - terns ( see Section 4 . 3 . 3 . 2 ) . We also test the performance of the GAE in applying learned relations to data instances not seen during training ( i . e . , analogy - making , 66 see Section 4 . 3 . 3 . 3 ) . In Section 4 . 3 . 1 we describe the GAE and the RBM models used in our experi - ments . The experiment setup including data preparation , used model architectures , and training is introduced in Section 4 . 3 . 2 . Results are presented and discussed in Section 4 . 3 . 3 and Section 4 . 3 . 4 gives an outlook on future work and challenges in musical transformation learning . We ﬁnd that it is challenging to learn musical transformations with the RBM and that the GAE is much more adequate for this task since it can learn representations of speciﬁc transformations that are largely content - invariant . We believe these results show that transformation - learning models such as GAEs may provide the basis for more encompassing music analysis systems , by endowing them with a better understanding of the structures underlying music . 4 . 3 . 1 Method 4 . 3 . 1 . 1 Gated Autoencoder A GAE learns mappings between data pairs x , y ∈ R P using latent mapping units m ∈ R L as m = σ ( W ( Ux · Vy ) ) , ( 4 . 1 ) where U , V ∈ R P × O and W ∈ R O × L are weight matrices , and · denotes the element - wise ( Hadamard ) product ( see Figure 4 . 2 for an illustration ) . In our experiments , σ is the sigmoid function , but other non - linearities , like the square - root or softplus are also reported in the literature [ Adelson and Bergen , 1985 , Alain and Olivier , 2013 , Memisevic , 2012 ] . The element - wise product between ﬁlter responses Ux and Vy , leads to ﬁlter pairs in U and V encoding correspondences between the inputs x and y . GAEs are very expressive in representing transformations , with the potential to represent any orthogonal transformation ( including any permutation in the input space—”shuﬄing pixels” [ Memisevic , 2013 ] ) . The resulting ﬁlter pairs show transformation - speciﬁc features , like phase - shifted Fourier components when learning transposition between n - grams of polyphonic music in our experiments ( see Figure 4 . 3 ) . Such phase - shifted ﬁlter pairs are then sensitive to corresponding shifts between pairs of data instances . An interesting aspect of a GAE is its symmetry when solving Equation 4 . 1 for its inputs , allowing for a reconstruction of an input given the other input and a mapping code . The reconstruction for binary data is given by ˜x = σ ( U > ( W > m · Vy ) ) , ( 4 . 2 ) and likewise ˜y = σ ( V > ( W > m · Ux ) ) . ( 4 . 3 ) 67 Figure 4 . 2 : Schematic illustration of the Gated Autoencoder . A commonly used training loss [ Memisevic , 2013 , Michalski et al . , 2014 ] is the symmetric reconstruction error L = | | x − ˜x | | 2 + | | y − ˜y | | 2 . ( 4 . 4 ) When using additive interactions in neural networks , units resemble logical OR - gates which accumulate evidence . In contrast , multiplicative interactions resemble AND - gates that can detect co - occurrences . Multiplicative interactions enable the model to ignore input that does not exemplify a known transformation , making it less content - dependent [ Memisevic , 2013 ] . Ideally , learned representations encode fully content - invariant transformations between data pairs . In practice , there is always some content - dependence in the resulting codes ( see Figure 4 . 4 , where the variance along the ﬁrst principal component is content - dependent , as it is not cor - related with transformation classes ) . A solution for improving content - invariance in GAEs is given in [ Lattner and Grachten , 2017a ] . 4 . 3 . 1 . 2 Restricted Boltzmann Machine An RBM is an energy - based stochastic neural network with two layers , a visible layer with units v ∈ { 0 , 1 } r and a hidden layer with units h ∈ { 0 , 1 } q [ Hinton , 2002 ] . The units of both layers are fully interconnected with weights W ∈ R r × q , while there are no connections between the units within a layer . Given a visible vector v , the free energy of the model is calculated as F ( v ) = − a (cid:124) v − X i log (cid:16) 1 + e ( b i + W i v ) (cid:17) , ( 4 . 5 ) 68 4 . 3 Learning Transformations of Musical Material Figure 4 . 3 : Some complementary ﬁlters manually selected from U ( top ) and V ( bottom ) learned from transposed pairs of musical n - grams . a b Figure 4 . 4 : First two principal components ( a ) and second and third principal compo - nent ( b ) of mappings resulting from unsupervised learning of pairs exhibiting the chro - matic transposition ( TransC ) relation . Points are colored , and cluster centers are named according to the diﬀerent classes within the TransC relation type ( i . e . distances of note shifts ) . 69 where a ∈ R r and b ∈ R q are bias vectors , and W i is the i - th row of the weight matrix . The model is trained by minimizing the free energy given the training data , and by maximizing the free energy for samples from the model—a method called Contrastive Divergence [ Hinton , 2002 ] . Reconstruction is performed by projecting a data instance into the hidden space , followed by a projection of the resulting hidden unit activations back into the input space . The reconstruction cross - entropies—as presented in Section 4 . 3 . 3 . 2—arise from the error between the input before and after the projection . 4 . 3 . 2 Experiment The current experiment aims to test the performance of a GAE in learning mu - sical relations between pairs of n - grams . As a baseline , we train an RBM with approximately the same number of parameters on concatenated pairs of n - grams , like in [ Susskind et al . , 2011 ] . In order to have a controlled setting and labels for the transformation classes , we construct pairs where one item is an original n - gram selected at random from a set of polyphonic Mozart sonatas ( see Figure 4 . 5 ( A ) for n - gram examples ) , and the other item is constructed by applying a randomly se - lected transformation to this n - gram . The experiment is unsupervised in that there are no targets or labels when training the GAE , but we only present one type of transformation per training session . The implicit goal of the GAE is to cluster the classes within each type of transformation . For example , within the transforma - tion type chromatic transposition ( TransC ) , the classes are the diﬀerent semitone intervals by which the items in a pair are shifted relative to each other . We use the resulting representations as an input to a classiﬁcation feed - forward Neural Network ( FFNN ) , to assess how discriminative they are concerning the given classes . Furthermore , we measure the reconstruction cross - entropies for both models , and we apply transformations extracted from single examples to new data . 4 . 3 . 2 . 1 Data We use MIDI ﬁles encoding the scores of Mozart piano sonatas from the Mozart / Batik data set [ Widmer , 2003 ] . From those pieces , we create binary piano - roll representations , using the MIDI pitch range [ 36 , 100 ] and a temporal resolution of 1 / 16th note . From those representations we create 8 - grams ( i . e . , any consecu - tive eight 1 / 16th slices from the piano - roll representation ) and for each 8 - gram we construct a counterpart which exhibits the respective transformation . In order to remove potential peculiarities of the corpus concerning absolute shifts and keys , we start by shifting the notes of the original n - grams by k semitones taken randomly from the set [ − 12 , 11 ] . In the following , we describe how the respective counterparts are constructed . 70 Chromatic Transposition ( TransC ) Chromatic transposition is performed by shifting every pitch in a set of notes by a ﬁxed number of semitones ( see Figure 4 . 5 ( 1 ) ) . For each n - gram , we construct a counterpart by shifting all notes by k semitones taken from the set [ − 12 , 11 ] . Note that this also includes the transposition by 0 semitones , which is the particular case “exact repetition” . The resulting 24 classes for testing the discriminative performance of our models are consequently the elements of the set [ − 12 , 11 ] . Diatonic Transposition ( TransD ) Diatonic transposition is the transposition within a key . That is , any pitch in a set of notes is shifted by a ﬁxed number of scale steps ( i . e . transposition using only allowed notes , e . g . depicted as green lines in Figure 4 . 1 ) . This operation may change the intervals between notes ( see Figure 4 . 5 ( 2 ) for examples of diatonic transposition ) . At ﬁrst , we estimate the key for each n - gram by choosing the one with the fewest accidentals from the set of keys for which all pitches of the n - gram are “allowed” , omitting n - grams which do not ﬁt in a key . Using this method , we obtain n - grams which are assigned to a unique key and can be unambiguously transformed into another key . That way , a unique transformation can be learned for any constructed instance pair . We create counterparts for each n - gram by shifting each pitch by k scale steps of the estimated scale , taken from the set [ − 7 , 6 ] . Thus , the resulting 14 classes the models are trained on are the elements of the set [ − 7 , 6 ] . Half time / double time ( Tempo ) “Half time” means doubling the duration of all notes , and “double time” is the inverse operation . We realize the half time relation by scaling each n - gram to double its width and then taking only the ﬁrst half of the result ( see Figure 4 . 5 ( 3 ) ) , and for the inverse operation , we swap the thus obtained n - gram pairs . Consequently , we assess the performance of the models in discriminating between the relations { double time , half time , identity } . Retrograde ( Retro ) In music , retrograde means to reverse the temporal order of given notes . We can simply create pairs exhibiting the retrograde relation by ﬂipping n - grams horizontally . This results in two classes : { retrograde , identity ( ¬ retrograde ) } 4 . 3 . 2 . 2 Model Architectures Architecture of the GAE and the RBM In order to obtain comparable results for the GAE and the RBM , for each test we choose the same number of parameters and layers for each model . The GAE may be seen as a two - layered model , where the factors constitute the hidden units of the ﬁrst layer , and the mapping units 71 constitute the hidden units of the second layer . Consequently , we also use a two - layered RBM architecture where the number of hidden units in the ﬁrst layer is equal to the number of factors in the GAE , and the number of hidden units in the top layer is equal to the number of mapping units in the GAE . We test three diﬀerent model sizes for every type of transformation . The smallest model ( 128 / 64 ) has 128 units in the ﬁrst layer and 64 units in the second layer . The next bigger model has 256 / 128 units , and the largest model has 512 / 256 units in the respective layers . Architecture of the Classiﬁcation FFNN For all classiﬁcation tasks , the same architecture is used : A three - layered feed - forward Neural Network ( FFNN ) with 512 Rectiﬁed Linear units ( ReLUs ) in the ﬁrst layer , 256 ReLUs in the second layer , and Softmax units in the output layer . The size of the output layer is equal to the number of classes . 72 4 . 3 Learning Transformations of Musical Material p r e d i c t e d i n t e r v a l P - 7 - 6 - 5 - 4 - 3 - 2 - 1 0 1 2 3 4 5 6 d i ag target interval - 7 7187 1 1 1 3 3 0 10 1 2 1 0 4 0 - 6 6 694 3 2 14 2 7 9 1 30 1 30 0 7 6 0 - 5 15 2 1 7043 14 26 13 6 3 3 29 0 6 10 3 10 - 4 1 1 0 3 7055 7 5 0 0 6 1 17 0 5 0 10 - 3 2 1 6 55 7090 6 11 0 2 3 8 34 1 9 11 - 2 5 4 3 16 11 6960 10 2 4 0 12 2 20 0 52 - 1 1 8 1 1 6 3 7050 5 6 3 6 7 13 24 3 0 7 0 2 0 0 4 0 7124 0 6 0 3 3 0 164 1 0 46 3 14 7 5 18 6 7028 0 23 1 4 13 28 2 13 1 25 3 16 8 3 0 2 70 82 15 16 4 2 59 3 1 5 0 12 10 9 5 2 5 0 6972 39 6 1 33 4 1 2 6 0 12 3 6 2 3 7 19 7027 1 1 29 5 9 7 5 17 0 89 11 8 7 4 48 11 6874 35 117 6 2 21 5 4 17 6 41 1 36 6 7 23 4 7053 135 P d i ag 2 30 13 12 58 7 232 32 10 4 42 30 158 141 98488 T a b l e 4 . 1 : C o n f u s i o n M a t r i x f o r c l a ss i ﬁ e r FF NN t r a i n e d o n r e p r e s e n t a t i o n s o f G A E T r a n s D 128 / 64 . 73 4 . 3 . 2 . 3 Training In the following , we report how the GAE , the RBM , and the FFNN are trained in our experiments . For each transformation type , we train on 490 000 pairs using a validation set of 10 000 pairs . The ﬁnal testing is done on a test set of size 100 000 per transformation type . Training the Gated Autoencoder The model parameters of the GAE are trained using stochastic gradient descent to minimize the symmetric reconstruction error ( see Equation 4 . 4 ) . We train the model on the data pairs for 1000 epochs , using a mini - batch size of 500 , a learning rate of 3 × 10 − 5 and a momentum of 0 . 93 . Learning improves when the input ( i . e . y and x in Equations 4 . 2 and 4 . 3 , respectively ) is corrupted during training , as it is done in de - noising autoencoders [ Vincent et al . , 2010 ] . We achieve this by randomly setting 35 % of the input bits to zero and training the GAE to reconstruct an uncorrupted , transformed version of it ( i . e . ˜x and ˜y in Equations 4 . 2 and 4 . 3 , respectively ) . For the ﬁrst 100 epochs , the input weights are re - scaled after each parameter update to their average norm , as described by Susskind et al . [ 2011 ] , Memisevic [ 2011 , 2012 ] . We use L1 and L2 weight regularization on all weights , and Lee’s sparsity regularization [ Lee et al . , 2007 ] on the mapping units and on the factors . Training the Restricted Boltzmann Machine We train two RBM layers on concatenated data pairs with greedy layer - wise training using persistent contrastive divergence ( PCD ) [ Tieleman , 2008 ] , a variation of the standard contrastive diver - gence ( CD ) algorithm [ Hinton , 2002 ] , which is known to result in a better approx - imation of the likelihood gradient . We use a learning rate of 3 × 10 − 3 which we reduce to zero during 300 training epochs . We use a batch size of 100 , and we reset ( i . e . , randomize ) the weights of neurons , whose average activity over all training examples exceeds 85 % . We use L1 and L2 weight regularization and the sparsity regularization proposed by Goh et al . [ 2010 ] setting µ = 0 . 08 and φ = 0 . 75 . Training the Classiﬁcation FFNN The FFNN is trained in a supervised manner on the latent representations of both the GAE ( i . e . , its mapping unit conﬁgurations ) and the RBM ( i . e . , its hidden unit conﬁgurations ) , resulting from unsupervised training on related data pairs using the categorical cross - entropy loss . The network is trained for 300 epochs with a learning rate of 0 . 005 , a batch size of 100 and a momentum of 0 . 93 . We apply L2 weight regularization on all weights , sparsity regularization as in [ Lee et al . , 2007 ] , 50 % dropout [ Srivastava et al . , 2014 ] on all except the top - most layer , as well as batch normalization [ Ioﬀe and Szegedy , 2015 ] . 74 Transformation Model TransC TransD Tempo Retro # Classes 24 14 3 2 Random 95 . 83 92 . 86 66 . 67 50 . 00 RBM 128 / 64 23 . 10 87 . 55 50 . 09 50 . 06 256 / 128 6 . 12 51 . 05 47 . 52 50 . 11 512 / 256 2 . 18 19 . 47 40 . 33 50 . 19 GAE 128 / 64 1 . 88 1 . 51 2 . 47 3 . 24 256 / 128 0 . 02 0 . 26 0 . 89 1 . 10 512 / 256 0 . 03 0 . 23 0 . 11 0 . 28 Table 4 . 2 : Mis - classiﬁed rates ( in percent ) from the classiﬁcation Feed - Forward Neural Network trained on representations of the Restricted Boltzmann Machine ( RBM ) and the Gated Autoencoder ( GAE ) in diﬀerent architecture sizes and for diﬀerent transformation types . Random denotes the random guessing baseline dependent on the number of classes . 4 . 3 . 3 Results and Discussion 4 . 3 . 3 . 1 Discriminative Performance Table 4 . 2 shows the mis - classiﬁcation rates of the FFNN , trained on representations of the GAE and the RBM . The models are trained separately for each transfor - mation type , chromatic transposition ( TransC ) , diatonic transposition ( TransD ) , half - time / double - time ( Tempo ) , and retrograde ( Retro ) where for each type , there are several classes . For example , for chromatic transposition , the classes are the 24 transposition distances of the set [ − 12 , 11 ] ( see Section 4 . 3 . 2 . 1 ) . The GAE is clearly better at learning discriminative representations of musical relations . The misclassiﬁcation rates of the largest architecture ( 512 / 256 ) are be - low 0 . 3 % for all relations . In contrast , the RBM is less suitable for unsupervised learning of relations—in the case of Retro , it does not even outperform random guessing . Note furthermore that in contrast to the GAE , increasing the capacity of the RBM does not always improve results . This suggests that the architecture itself is a limiting factor : As the activations of units in an RBM only depend on the additive accumulation of evidence , it attempts to learn all combinations of mutually transformed data instances it is presented with during training . For transposition relations , learning such combinations results in representations which are informa - tive enough that the classiﬁcation FFNN can infer the transformation class from it . Retrograde and Tempo , however , are too complex transformations to grasp for 75 content - dependent representation learner . In contrast , the GAE separates the problem in representing characteristics of the input n - grams with additive input connections on the one hand , and modeling transformations using multiplicative interactions on the other hand . Also , when training the GAE on reconstructing an input , the other input provides content information allowing the mapping units to represent only the content - invariant transformation . It is still a non - trivial result that the GAE can learn all pre - deﬁned transformation types . In particular diatonic transposition ( TransD ) is a very relevant transformation in music and it is a valuable ﬁnding that the GAE is capable of learning it . Table 4 . 1 shows a confusion matrix for the GAE TransD 128 / 64 relation . It is instructive for interpretation of the table to realize that two diatonic transpositions of the same n - gram yield another pair of n - grams that are also diatonically trans - posed versions of each other . For instance , the diatonic transposition pairs 0 and - 7 , 1 and - 6 , 2 and - 5 , and so on , all yield a resultant transposition of - 7 ( a downward octave ) . Analog equivalences hold for resultant transpositions of an upward octave , the upward / downward ﬁfth , third , and so on . The equivalent resultant transposi - tions have been marked by colored arrows in the table . The arrows clearly highlight that the model is biased toward confusions by one octave from the correct shift ( red arrows ) . For example , when the actual shift between n - gram pairs is + 5 , the clas - siﬁer frequently estimates a shift of − 2 , which is 7 scale steps or one octave away from the correct target . Similarly , frequently confused targets are a third ( green arrows ) , a ﬁfth ( blue arrows ) , and an octave plus a third ( orange arrows ) away from the correct target . Since the data pairs are uniformly distributed over all classes , it follows that these confusions are caused by covariances within single n - grams , where thirds , ﬁfths , and octaves occur frequently . Similarly , the misclassiﬁcations of second interval shifts ( i . e . , the entries directly above and below to the diagonal ) most likely result from second intervals in melodies . Many of the ﬁlters learned by the model are also receptive to intervals of notes which occur sequentially in time ( see Figure 4 . 4 , where ﬁlters are mostly horizontally oriented ) , therefore also intervals in melodies are represented in the latent space of the model . 4 . 3 . 3 . 2 Reconstruction Table 4 . 3 lists the reconstruction cross - entropies for each architecture versus trans - formation type . The error of the largest GAE architecture is about an order of magnitude smaller than that of the best RBM architecture . Furthermore , the cross - entropies do not substantially improve when increasing the size of the RBM , again suggesting an architectural advantage of the GAE over the RBM . Olshausen et al . [ 2007 ] suggest that the advantage consists in the ability to separate content and its manifestation ( the “what” and the “where” ) . Mapping units only have to encode 76 Transformation Model TransC TransD Tempo Retro RBM 128 / 64 0 . 131 0 . 122 0 . 098 0 . 110 256 / 128 0 . 122 0 . 119 0 . 084 0 . 095 512 / 256 0 . 113 0 . 112 0 . 075 0 . 086 GAE 128 / 64 0 . 026 0 . 032 0 . 025 0 . 033 256 / 128 0 . 018 0 . 024 0 . 015 0 . 017 512 / 256 0 . 012 0 . 016 0 . 007 0 . 009 Table 4 . 3 : Reconstruction cross - entropies for the Restricted Boltzmann Machine ( RBM ) and the Gated Autoencoder ( GAE ) in diﬀerent sizes and for diﬀerent transformation types . the " where " components leading to more eﬃcient use of the model parameters . 4 . 3 . 3 . 3 Generation Figure 4 . 5 shows analogy - making examples for the GAE . Mappings are inferred from template n - gram pairs ( A ) and applied to single instances which were not part of the training corpus . The resulting pairs ( B ) should exhibit the same trans - formation as the template pairs from which the transformation was inferred . For diatonic transposition ( 2 ) , the mapping is applied only to instances in the same key as the source instance ( i . e . left instance ) of the template pairs , and valid pitches are marked with green lines . The transformations of the template pairs shown in Figure 4 . 5 are chromatic transposition by − 7 semitones ( 1 ) , diatonic transposition by + 5 scale steps in C major ( 2 ) , halftime ( 3 ) , and retrograde ( 4 ) . Depending on the type of transforma - tion , the results vary in their overall quality . The examples shown in Figure 4 . 5 have been selected to illustrate both high and low - quality transformations . We found that diatonic transposition ( TransD ) transformation was generally of lower qual - ity than the other transformation types . In low - quality transformations , notes are frequently missing in generated counterparts , which is a result of learned represen - tations being not fully content - invariant . Figure 4 . 4 shows that content - dependence in the ﬁrst principal component , as this component is not correlated with content - invariant classes . Note that the GAE is “conservative” when it comes to generating analogies , in that the reconstruction errors are caused by omitting existing notes , but very rarely by incorrectly introduced notes . The reason is that factors only take on large values when inputs comply with their transformations ( see [ Memise - vic , 2012 ] for more details ) . 77 4 . 3 . 4 Conclusion and Future Work We have evaluated the performance of two connectionist models in learning trans - formations from pairs of musical n - grams in an unsupervised manner . We found that the Gated Autoencoder was more eﬀective than a standard RBM , both in an input reconstruction task and in a discriminative task in which the represen - tations learned by the models were used to classify n - gram pairs in terms of the transformation they exhibit . Ideally , transformations learned by a GAE are fully content - invariant . We found that this is not the case in practice when training the GAE using the classical objective function . In other recent work , we show that the content - invariance of learned representations can be improved by a regularization term that explicitly penalizes content - variance [ Lattner and Grachten , 2017a ] . The results reported in this section show that when given enough n - gram pairs exhibiting transformations of a speciﬁc type , the GAE can learn these transforma - tions . Future work should assess the suitability of the models to automatically infer transformations from a music corpus . This poses the further challenge of select - ing appropriate n - gram pairs for training the model , as the majority of randomly selected n - gram pairs in a corpus would be unrelated . A possible way to address this issue is to use the GAE itself to make musical representations locally invariant towards some properties , in order to better detect similar fragments . For example , local invariance towards transposition can unveil mutually transposed texture - pairs on a global scale ( see Section 4 . 4 ) . Another way is to use a bootstrapping approach in which the selection of n - gram pairs for training the model is based on some measure deﬁned by the model itself , for example by greedily selecting the pairs for which the score is high ( i . e . , the energy of the pair is low [ Im and Taylor , 2015 ] ) . 78 4 . 3 Learning Transformations of Musical Material A B 1 2 3 4 Figure 4 . 5 : Results of the analogy - making task . Transformations are inferred by the GAE from pairs of 8 - grams ( A ) and applied to new n - grams , not seen during training ( B , left parts ) to generate counterparts with analogous transformations ( B , right parts ) , where the level of blackness indicates the certainty of the model that the respective note is part of the transformed result : 1 ) Chromatic transposition , 2 ) diatonic transposition , 3 ) tempo change ( halftime ) , and 4 ) retrograde . Green horizontal lines mark scale notes in diatonic transposition , for visual guidance . 79 4 . 4 Learning Interval Representations from Polyphonic Music Sequences In the previous section , we tested a common Gated Autoencoder ( GAE ) archi - tecture on n - gram pairs and found that it is well - suited for learning musical transformations—an important step towards modeling musical structure . In or - der to achieve a controlled setting , the n - gram pairs were constructed , rather than sampled from real music pieces . Therefore , it was known which transformations they exhibit and that the pairs always exhibit a systematic transformation . In a real - world setting , there is the chicken - egg situation that for selecting related train - ing pairs in a music piece , an already trained GAE would be necessary to score the relatedness of pairs ( see Section 4 . 3 . 4 ) . A diﬀerent approach is based on the observation that music is a sequential phe - nomenon , and thus we can rely on some local correlations . A GAE can learn such regularities and provide us with a transformational view on the musical surface . In this study , the architecture and loss function of the GAE is modiﬁed to obtain a predictive setting , where events are represented as the result of learned transfor - mation functions applied to preceding events . It turns out that learning such local transformations of musical notes leads to very useful features : pitch intervals . Most concepts in music theory ( e . g . , scale types and modes , cadences , chord types ) are deﬁned in terms of intervals ( i . e . , relative distances ) to reference pitches . Therefore , when computer models are employed in music tasks , it can be useful to operate on interval representations rather than on the raw musical surface . More - over , interval representations are transposition - invariant , valuable for tasks like audio alignment , cover song detection , and music structure analysis . In this sec - tion , we employ a GAE to learn ﬁxed - length , invertible and transposition - invariant interval representations from polyphonic music both in the symbolic domain and in audio . Applying a k - nearest neighbor ( k - nn ) classiﬁer in the mapping space , we show that the mappings ( i . e . , latent representations ) learned by the model encode the intervals contained in the polyphonic input data . Furthermore , we examine the organization of intervals in the mapping space and show that the model organizes them in a musically meaningful manner . Finally , based on the learned mappings , transposition - invariant self - similarity matrices are constructed and employed in the MIREX task “Discovery of Repeated Themes and Sections” , yielding competitive results . The contributions of this study include ( 1 ) adapting the GAE architecture for sequence learning , in particular using diﬀerent dimensionality for input and predic - tion , as well as two mapping layers , ( 2 ) proposing a novel cost function to support the learning of transposition - invariant interval representations , ( 3 ) showing the use - fulness of the learned interval representations for the discovery of musical sections . 80 Furthermore , to our knowledge , this is the ﬁrst time that it is shown that a GAE can represent multiple transformations ( i . e . , multiple intervals ) within a data pair at once . This section is structured as follows . Section 4 . 4 . 1 provides some additional moti - vation for relative pitch processing in music . In Section 4 . 4 . 2 , the used architecture is described and in Section 4 . 4 . 3 , data is introduced on which the GAE is trained . The training procedure , including the novel method to support the emergence of transposition - invariance , is proposed in Section 4 . 4 . 4 . The experiments conducted to examine the properties of learned mappings are introduced in Section 4 . 4 . 5 , and results and a discussion are given in Section 4 . 4 . 6 . 4 . 4 . 1 Motivation for Modeling Relative Pitch Processing The notion of relative pitch is important in music understanding . Many music - theoretical concepts , such as scale types , modes , chord types , and cadences , are deﬁned in terms of relations between pitches or pitch classes . But relative pitch is not only a music - theoretical construct . It is common for people to perceive and memorize melodies in terms of pitch intervals ( or in terms of contours , the upward or downward direction of pitch intervals ) rather than sequences of absolute pitches . This characteristic of music perception also has ramiﬁcations for the perception of form in musical works . It implies that transposition of some musical fragment along the pitch dimension ( such that the relative distances between pitches remain the same ) does not alter the perceived identity of the musical material , or at least establishes a sense of similarity between the original and the transposed material . As such , adequate detection of musical form in terms of ( approximately ) repeated structures presupposes the ability to account for pitch transposition—one of the most common types of transformations found in music . Relative pitch perception in humans is currently not well - understood [ McDer - mott and Oxenham , 2008 ] . For example , there are no established theories on how the human brain derives a relative representation of pitch from the tonotopic rep - resentations formed in the cochlea , neither is it clear whether there is a connection between the perception of pitch relations in simultaneous versus consecutive pitches . Computational approaches to address tasks of music understanding ( such as de - tecting patterns and form in music ) often circumvent this issue by representing musical stimuli as sequences of monophonic pitches , after which simply diﬀerencing consecutive pitches yields a relative pitch representation . This approach also works for polyphonic music , to the extent that the music can be meaningfully segregated into monophonic pitch streams . A drawback of this approach is that it presupposes the ability to segregate musical streams , which is often far from trivial due to the ambiguity of musical contexts . To take an analogous approach on acoustical repre - sentations of musical stimuli is even more challenging since it further depends on 81 the ability to detect pitches and onsets in sound . In this study , we take a diﬀerent approach altogether . We train a Gated Autoen - coder ( GAE ) to learn representations that represent the relation between the music at some time point t and the preceding musical context . During training , these representations are adapted to minimize the reconstruction error of the music at t given the preceding context and the representation itself . A crucial aspect of the GAE is its bilinear architecture involving multiplicative connections , which facilitates the formation of relative pitch representations . We stimulate such representations more explicitly using an altered training procedure in which we transpose the training data using arbitrary transpositions . The results are two models ( for symbolic music and audio ) that can map both monophonic and polyphonic music to a sequence of points in a vector space—the mapping space —in a way that is invariant to pitch transpositions . This means that a musical fragment will be projected to the same mapping space trajectory independently of how it is transposed . We validate our approach experimentally in several ways . First , we show that musical fragments that are nearest neighbors in the mapping space have many pitch intervals in common ( as opposed to nearest neighbors in the input space ) . Then we show that the topology of the learned mapping space reﬂects musically meaningful relations between intervals ( such as the tritone being dissimilar to other intervals ) . Lastly , we use mapping space representations to detect musical form both for sym - bolic and audio representations of music , showing that it yields competitive results , and in the case of audio even improves the state - of - the - art . A re - implementation of the transposition - invariant GAE for audio is publicly available 1 . 4 . 4 . 2 Model Let x j be a vector representing pitches of currently sounding notes ( in the symbolic domain ) or the energy distributed over frequency bands ( in the audio domain ) , in a ﬁxed - length time interval . Given a temporal context x tt − n = x t − n . . . x t as the input and the next time step x t + 1 as the target , the goal is to learn a mapping m t which does not change when shifting x t + 1 t − n up - or downwards in the pitch dimension . A GAE ( depicted in Figure 4 . 6 ) is well - suited for this task , modeling the intervals between reference pitches in the input and pitches in the target , encoded in the latent variables of the GAE as mapping codes m j . In Section 4 . 3 , we trained the GAE by minimizing the symmetric error when reconstructing the output from the input and vice versa . In the proposed architecture , we use predictive training and just learn to infer the output from the input . More precisely , the goal of the training is to ﬁnd a mapping m t for any input / target pair which transforms the input into 1 see https : / / github . com / SonyCSLParis / cgae - invar 82 Figure 4 . 6 : Schematic illustration of the Gated Autoencoder ( GAE ) architecture used in the experiments . the given target . The mapping at time t is calculated as m t = σ h ( W 1 σ h ( W 0 ( Ux tt − n · Vx t + 1 ) ) ) , ( 4 . 6 ) where U , V and W k are weight matrices , σ h is the hyperbolic tangent non - linearity , and we will refer to the learnt mappings m j as the mapping space of the input / target pairs . The operator · ( depicted as a triangle in Figure 4 . 6 ) depicts the Hadamard ( or element - wise ) product of the ﬁlter responses Ux tt − n and Vx t + 1 , denoted as factors . This operation allows the model to relate its inputs , making it possible to learn interval representations . We found that using two mapping layers W 0 and W 1 improves the results of our experiments . In the previous section ( see Section 4 . 3 ) , we use only one layer , as we simply adopt the vanilla GAE architecture from the literature to perform a proof - of - concept for learning musical transformations . Having said that , we hypothesize that more layers would also improve the results in Section 4 . 3 . In fact , in our most recent publication [ Lattner and Grachten , 2019 ] we use stacks of several ( convolutional ) layers for the input , target , and mapping pathways ( depicted as U , V , and W k in Figure 4 . 6 ) . The target of the GAE can be reconstructed as a function of the input x t t − n and a mapping m t : ˜x t + 1 = σ g ( V > ( W > 0 W > 1 m t · Ux tt − n ) ) , ( 4 . 7 ) where σ g is the sigmoid non - linearity for binary input and the identity function for real - valued input . The cost function is deﬁned to penalize the error of reconstructing the target x t + 1 given the input x tt − n and the mapping m t . For real - valued sequences , the 83 mean - square error L MSE ( x , ˜x ) = 1 N N X n = 1 ( x n − ˜ x n ) 2 ( 4 . 8 ) is used , while the cross - entropy loss L CE ( x , ˜x ) = − 1 N N X n = 1 (cid:20) x n log 2 ˜ x n + ( 1 − x n ) log 2 ( 1 − ˜ x n ) (cid:21) ( 4 . 9 ) is minimized for binary sequences . A temporal context n > 0 is chosen to provide the GAE with more context to relate its input with the output . As shown in Figure 4 . 10 , besides focusing on t , the model will turn out to be speciﬁcally sensitive to onsets a quarter note ( t − 3 ) and a half note ( t − 7 ) before the prediction target at t + 1 . 4 . 4 . 3 Data We train the model both on symbolic music representations and on audio spectro - grams . For the symbolic data , the Mozart / Batik data set [ Widmer , 2003 ] is used , consisting of 13 piano sonatas containing more than 106 , 000 notes . The dataset is encoded as successive 60 dimensional binary vectors ( encoding MIDI note number 36 to 96 ) , each representing a single time step of 1 / 16th note duration . The pitch of an active note is encoded as a corresponding on - bit , and as multiple voices are encoded simultaneously , a vector may have multiple active bits . The result is a piano roll - like representation . The audio dataset consists of 100 random piano pieces of the MAPS dataset [ Emiya et al . , 2010 ] ( subset MUS ) , at a sampling rate of 22 . 05 kHz . We choose a constant - Q transformed spectrogram using a hop size of 1984 , and Hann windows with diﬀerent sizes depending on the frequency bin . The range comprises 120 frequency bins ( 24 per octave ) , starting from a minimal frequency of 65 . 4 Hz . Each time step is contrast - normalized to zero mean and unit variance . 4 . 4 . 4 Training The model is trained with stochastic gradient descent in order to minimize the cost function ( cf . Equations 4 . 8 and 4 . 9 ) using the data described in Section 4 . 4 . 3 . However , rather than using the data as is , we use data - augmentation in combination with an altered training procedure to explicitly aim at transposition invariance of the mapping codes . 84 4 . 4 . 4 . 1 Enforcing Transposition - Invariance As described in Section 4 . 4 . 2 the classical GAE training procedure derives a map - ping code from an input / target pair , and subsequently penalizes the reconstruction error of the target given the input and the derived mapping code . Although this procedure naturally tends to lead to similar mapping codes for input target pairs that have the same interval relationships , the training does not explicitly enforce such similarities , and consequently , the mappings may not be maximally transpo - sition invariant . Under ideal transposition invariance , by deﬁnition the mappings would be iden - tical across diﬀerent pitch transpositions of an input / target pair . Suppose that a pair ( x tt − n , x t + 1 ) leads to a mapping m ( by Equation 4 . 6 ) . Transposition invari - ance implies that reconstructing a target x 0 t + 1 from the pair ( x 0 tt − n , m ) should be as successful as reconstructing x t + 1 from the pair ( x tt − n , m ) when ( x 0 tt − n , x 0 t + 1 ) can be obtained from ( x tt − n , x t + 1 ) by a single pitch transposition . Our altered training procedure explicitly aims to achieve this characteristic of the mapping codes by penalizing the reconstruction error using mappings obtained from transposed input / target pairs . More formally , we deﬁne a transposition function shift ( x , δ ) , shifting the values of a vector x of length M by δ steps ( MIDI note numbers and CQT frequency bins for symbolic and audio data , respectively ) : shift ( x , δ ) = ( x ( 0 + δ ) mod M , . . . , x ( M − 1 + δ ) mod M ) > , ( 4 . 10 ) and shift ( x tt − n , δ ) denotes the transposition of each single time step vector before concatenation and linearization . The training procedure is then as follows . First , the mapping code m t of an input / target pair is inferred as shown in Equation 4 . 6 . Then , m t is used to recon - struct a transposed version of the target , from an equally transposed input ( modi - fying Equation 4 . 7 ) as ˜x 0 t + 1 = σ g ( V > ( W > 0 W > 1 m t · U shift ( x tt − n , δ ) ) ) , ( 4 . 11 ) with δ ∈ [ − 30 , 30 ] for the symbolic , and δ ∈ [ − 60 , 60 ] for the audio data . Finally , we penalize the error between the reconstruction of the transposed target and the actual transposed target ( i . e . , employing Equations 4 . 8 and 4 . 9 ) as L ( shift ( x t + 1 , δ ) , ˜x 0 t + 1 ) . ( 4 . 12 ) The transposition distance δ is randomly chosen for each training batch . This method amounts to both , a form of guided training and data augmentation . Some weights ( i . e . , ﬁlters ) in U and V resulting from that training are depicted in Figure 4 . 7 . 85 Figure 4 . 7 : Some ﬁlter pairs ∈ { U , V } of a GAE trained on polyphonic Mozart piano pieces . 4 . 4 . 4 . 2 Architecture and Training Details The architecture and training details of the GAE are as follows : A temporal context length of n = 8 is used ( the choice of n > 1 leads to higher robustness of the mapping codes to diatonic transposition ) . The factor layer has 1024 units for the symbolic data , and 512 units for the spectrogram data . Furthermore , for all datasets , there are 128 neurons in the ﬁrst mapping layer and 64 neurons in the second mapping layer ( resulting in m t ∈ R 64 ) . L2 weight regularization for weights U and V is applied , as well as sparsity regularization [ Lee et al . , 2007 ] on the topmost mapping layer . The deviation of the norms of the columns of both weight matrices U and V from their average norm is penalized . Furthermore , we restrict these norms to a maximum value . We apply 50 % dropout on the input and no dropout on the target , as proposed in [ Memisevic , 2011 ] . The learning rate ( 1e - 3 ) is gradually decremented to zero throughout the training . 4 . 4 . 5 Experiments In this Section we describe several experimental analyses to validate the pro - posed approach . They are intended to test the degree of transposition - invariance of the learned mappings , and to assess their musical relevance ( Sections 4 . 4 . 5 . 1 and 4 . 4 . 5 . 3 ) . Furthermore , we put the learned representations to practice in a repeated section discovery task for symbolic music and audio ( Section 4 . 4 . 5 . 2 ) . 86 4 . 4 . 5 . 1 Classiﬁcation and Cluster Analysis Our hypothesis is that the model learns relative pitch representations ( i . e . intervals ) from polyphonic absolute pitch sequences . In order to test this hypothesis , we conduct two experiments using symbolic data . In the ﬁrst experiment a ten - fold k - nn classiﬁcation of intervals is performed ( where k = 10 ) , where the task is to identify all pitch intervals between notes in the input and the target of an input / target pair . If the learned mappings actually represent intervals , the classiﬁer will perform substantially better on the mappings than on the input space . As intervals in music are transposition - invariant , the inter - val labels do not change when performing transposition in the input space . Thus , we perform the classiﬁcation on the mappings of the original data and of randomly transposed data , to test if the mappings are indeed transposition - invariant . We label the symbolic train data input / target pairs according to all intervals which occur between any two notes from the pair , independent of the temporal distance of the notes exhibiting the intervals . Thus , each pair can have multiple labels . For each pair in the test set , the k - nn classiﬁer predicts the set of interval labels that are present in the k neighbors of that pair . Using the Euclidean distance , the classiﬁcation is performed in the input space ( i . e . , on concatenated input / target pairs ) , and in the mapping space . From the resulting predictions , we determine the precision , recall , and F - score over the test set ( cf . Table 4 . 4 ) . For example , when a pair contains 6 intervals and the classiﬁer estimate yield 4 true - positive and 4 false - positive interval occurrences , that pair is assigned a precision of 0 . 5 and a recall of 0 . 67 . In the second part of the experiment , the cluster centers of all intervals in the mapping space are determined . Again , each pair projected into the mapping space accounts for all intervals it exhibits and can , therefore , participate in more than one cluster . The mutual Euclidean distances between all cluster centers are displayed as a matrix ( cf . Figure 4 . 8 ) . An interpretation of the results follows in Section 4 . 4 . 6 . 4 . 4 . 5 . 2 Discovery of Repeated Themes and Sections The MIREX Task for Discovery of Repeated Themes and Sections for Symbolic Music and Audio 2 tests algorithms for their ability to identify repeated patterns in music . The commonly used JKUPDD dataset [ Collins , 2017 ] contains 26 motifs , themes , and repeated sections annotated in 5 pieces by J . S . Bach , L . v . Beethoven , F . Chopin , O . Gibbons , and W . A . Mozart . We use the MIDI and the audio versions of the dataset and pre - process them as described in Section 4 . 4 . 3 . We calculate the reciprocal of the Euclidean distances between all representations m t of a song , resulting in a transposition - invariant similarity matrix X . Then , the 2 http : / / www . music - ir . org / mirex / wiki / 2017 : Discovery _ of _ Repeated _ Themes _ & _ Sections 87 4 Learning Musical Structure by Learning Transformations Figure 4 . 8 : Distance matrix of cluster centers of intervals represented in mapping space . Darker cells indicate higher distances between respective clusters , brighter cells indicate closeness . 88 Data Precision Recall F1 Original input Mapping space 91 . 27 70 . 25 76 . 66 Input space 65 . 58 46 . 05 50 . 59 Transposed input Mapping space 90 . 78 71 . 44 77 . 31 Input space 51 . 81 32 . 99 37 . 43 All 26 . 40 100 . 0 40 . 05 None 0 . 0 0 . 0 0 . 0 Table 4 . 4 : Results of the k - nn classiﬁcation in the mapping space and in the input space for the original symbolic data and data randomly transposed by [ − 24 , 24 ] semitones . “All” is a lower bound ( always predict all intervals ) , “None” returns the empty set . values of the main diagonal are set to the minimum value of the matrix . Subse - quently , the matrix is normalized and convolved with an identity matrix of size 15 × 15 to emphasize and smooth diagonals ( Figure 4 . 9 shows a resulting matrix ) . The method used to determine repeated parts based on diagonals of high values in the self - similarity matrix is adopted from [ Nieto and Farbood , 2014 ] , with a diﬀerent method to identify diagonals , as described below . The function s ( i , j , N ) = N X k = N − m X ( i + k , j + k ) w k m ( 4 . 13 ) returns the score for a diagonal starting at X ( i , j ) with length N , and diagonals with high score are considered to be repeated sections . For each i , j , we iteratively evaluate the score with N increasing from 1 in integer steps , until the score under - cuts a threshold γ . Only the last m values , m = min ( 10 , N ) , of the diagonal are taken into account , because those values indicate when to stop tracing . The factor w k = 1 + k + m − N m ( 4 . 14 ) linearly weights the last m values of the diagonal so that later values have more impact on the overall score . Three empirically determined parameters inﬂuence the functioning of the method : ( 1 ) from the diagonals found , we only keep those spanning more than 2 whole notes , ( 2 ) when aligning the repetitions of a section with the ﬁrst occur - rence of the section , their start and end may diﬀer by not more than a half note to be still considered repetitions of each other , ( 3 ) the thresholds γ determining if a diagonal should be considered a repetition in the symbolic and the audio data are set to 0 . 9 and 0 . 81 , respectively . The results are shown in Table 4 . 5 and are discussed in Section 4 . 4 . 6 . 89 4 Learning Musical Structure by Learning Transformations Figure 4 . 9 : Symbolic music and corresponding self - similarity matrix calculated from transposition - invariant mapping codes . Warmer colors indicate similarity , colder colors indicate dissimilarity . 90 4 . 4 Learning Interval Representations from Polyphonic Music Sequences A l go r i t h m F e s t P e s t R e s t F o ( . 5 ) P o ( . 5 ) R o ( . 5 ) F o ( . 75 ) P o ( . 75 ) R o ( . 75 ) F 3 P 3 R 3 T i m e ( s ) S y m b o li c R e l a t i v e P i t c h 59 . 07 77 . 60 58 . 30 68 . 92 80 . 24 67 . 46 77 . 51 91 . 38 73 . 29 50 . 44 60 . 36 53 . 23 127 V M O s y m b o li c * 60 . 79 74 . 57 56 . 94 71 . 92 79 . 54 68 . 78 75 . 98 75 . 98 75 . 99 56 . 68 68 . 98 53 . 56 4333 S I A R CT - C F P * 33 . 70 21 . 50 78 . 00 76 . 50 78 . 30 74 . 70 - - - - - - - C O S I A T E C * 50 . 20 43 . 60 63 . 80 63 . 20 57 . 00 71 . 60 68 . 40 65 . 40 76 . 40 44 . 20 40 . 40 54 . 40 7297 A ud i o R e l a t i v e P i t c h 57 . 67 67 . 46 59 . 52 58 . 85 61 . 89 56 . 54 68 . 44 72 . 62 64 . 86 51 . 61 59 . 60 55 . 13 194 V M O d e a dp a n * 56 . 15 66 . 80 57 . 83 67 . 78 72 . 93 64 . 30 70 . 58 72 . 81 68 . 66 50 . 60 61 . 36 52 . 25 96 S I A R CT - C F P * 23 . 94 14 . 90 60 . 90 56 . 87 62 . 90 51 . 90 - - - - - - - N i e t o * 49 . 80 54 . 96 51 . 73 38 . 73 34 . 98 45 . 17 31 . 79 37 . 58 27 . 61 32 . 01 35 . 12 35 . 28 454 T a b l e 4 . 5 : D i ﬀ e r e n t p r ec i s i o n , r ec a ll a nd f - s c o r e s ( d e t a il s o n t h e m e a s u r e s a r e g i v e n i n [ C o lli n s , 2017 ] ) o f d i ﬀ e r e n t m e t h o d s i n t h e D i s c o v e r y o f R e p e a t e d T h e m e s a nd S ec t i o n s M I R E X t a s k , f o r s y m b o li c m u s i c a nd a ud i o . T h e F 3 s c o r e c o n s t i t u t e s a s u mm a r i z a t i o n o f a ll m e a s u r e s . * R e s u l t s o f V M O s y m b o li c a nd V M P d e a dp a n f r o m [ W a n g e t a l . , 2015 ] , S I A R CT - C F P f r o m [ C o lli n s e t a l . , 2013 ] , C O S I A T E C f r o m [ M e r e d i t h , 2013 ] a nd V M O d e a dp a n f r o m [ N i e t o a nd F a r b oo d , 2014 ] . 91 7 6 5 4 3 2 1 0 Time ( t ) 0 . 9 1 . 0 1 . 1 1 . 2 R e l a t i v e s e n s i t i v i t y Figure 4 . 10 : Absolute sensitivity of the model when looking backwards on the temporal context , averaged over the whole dataset . 4 . 4 . 5 . 3 Sensitivity Analysis The sensitivity of the model to speciﬁc context information provides important insights into the functioning of the model . A common way of determining the sensitivity of a network is by calculating the absolute value of the gradients of the network’s predictions with respect to the input , holding the network parame - ters ﬁxed [ Simonyan et al . , 2014 ] . Figure 4 . 10 shows the sensitivity of the model with respect to the temporal context . The model is particularly sensitive to note occurrences at t ∈ { 0 , − 3 , − 7 } . This shows that the most informative notes for a prediction are direct predecessors ( t = 0 ) , and notes which occur a quarter ( t = − 3 ) and a half note ( t = − 7 , i . e . , eight sixteenth notes ) before the prediction . 4 . 4 . 6 Results and Discussion The results of the k - nn classiﬁcation on the raw data and representations learned by the model are shown in Table 4 . 4 . Classiﬁcation in the mapping space appre - ciably outperforms classiﬁcation in the input space and obtains similar values for mappings of the original data and the randomly transposed data . In contrast , when performing classiﬁcation in the input space , the results deteriorate for the randomly transposed input and do not exceed the theoretical lower bound ( i . e . , always predict all intervals ) . As the register and keys of the original data are limited , correlations 92 between absolute and relative pitch exist . When transposing the input , the classi - ﬁer cannot make use of these absolute cues for relative pitch anymore and performs weakly in the input space . Figure 4 . 8 indicates which intervals are close to each other in the mapping space . An obvious regularity are the slightly brighter k - diagonals ( i . e . , parallels to the main diagonal ) with k ∈ { − 24 , − 12 , 12 , 24 } , showing that two pitch intervals lead to similar mapping codes when they result in the same pitch class , such as the intervals + 8 and - 4 semitones , or - 7 and - 19 semitones . This is an indication that the model learns the phenomenon of octave equivalence , even if the input to the model represents only absolute pitch . Another distinct feature is the stripe which is orthogonal to the main diagonal ( i . e . , where y = − x ) . This indicates that the model develops some notion of relative distances , by positioning intervals of the same distance ( but diﬀerent signs ) close to each other . Note also that the mappings of certain intervals , notably 6 and − 6 , are distant to those of most other intervals ( dark horizontal and vertical lines ) . This likely reﬂects the fact that tritone intervals are rare in diatonic music , and is further evidence of the musical signiﬁcance of the learned mappings . Table 4 . 5 shows results of the repeated themes and section discovery task , where the F 3 score is a good indicator for the overall performance of the models ( see [ Collins , 2017 ] for a thorough explanation on the respective measures ) . For the audio data , the current state - of - the - art F 3 score was raised from 50 . 60 to 51 . 61 by our proposed method . The method performs slightly worse on the symbolic data , which is counterintuitive at ﬁrst sight , given that results of other models suggest that this task is easier . We hypothesize that for the discovery of repeated sections , approximate matching leads to better results than exact comparison , simply be - cause musical variation goes beyond chromatic transposition ( towards which our model is invariant ) . For approximate matching , a spectrogram representation is better suited than symbolic vectors , as notes are blurred over more than one fre - quency bin , and harmonics may provide additional cues for similarity estimation . The proposed approach is computationally eﬃcient because the diagonal detector ( cf . Equations 4 . 13 and 4 . 14 ) is rather simple and the transposition - invariance of the representations does not require explicit comparison of mutually transposed musical textures . 4 . 4 . 7 Conclusion and Future work In this section , we have presented a computational approach to deriving ( pitch ) transposition - invariant vector space representations of music both in the symbolic and the audio domain . The representations encode pitch intervals that occur in the music in a musically meaningful way , with tritone intervals—a rare interval in diatonic music—leading to more distinct representations , and octaves leading to 93 more similar representations . Furthermore , the temporal sensitivity of the model reveals a beat pattern that shows increased sensitivity to pitch intervals occurring at beat multiples of each other . The transposition - invariance of the representations makes it possible to detect transposed repetitions of musical sections in the symbolic and in the spectral do - main of audio . We have demonstrated that this is beneﬁcial in tasks such as the MIREX task Discovery of Repeated Themes and Sections . A simple diagonal ﬁnd - ing approach on a transposition - invariant self - similarity matrix produced by our model is suﬃcient to outperform the state - of - the - art in the audio version of the task . We believe it is worthwhile to further explore the utility of transposition - invariant music representations for other applications , including speech recognition , music summarization , music classiﬁcation , transposition - invariant music alignment ( in - cluding a cappella voices with pitch drift ) , query by humming , fast melody - based retrieval in large audio collections , and music generation . First results show that the proposed representations are useful for audio - to - score alignment [ Arzt and Lattner , 2018 ] and for music prediction tasks ( see Section 4 . 5 ) . In the next section , we show that interval representations which take into account reference pitches at a constant time lag in the past ( e . g . , one measure ) are a useful feature to encode ( transposed ) repetitions , which gives rise to modeling repetition structure in music . Furthermore , by combining the proposed GAE architecture with an RNN , which operates on the interval representations provided by the GAE , generalization in sequence learning tasks can be improved by reducing sparsity in the input data . 94 4 . 5 Learning Sequences of Intervals and Repetition Structure In this section , we further modify the GAE architecture proposed in Section 4 . 4 by combining it with a Recurrent Neural Network ( RNN ) , to obtain a “relative pitch” sequence model—the Recurrent Gated Autoencoder ( RGAE ) . The objective of sequence models for music prediction is to predict ( the probability of ) musical events at the next time step , given some prior musical context . In the ( most common ) case of predicting note events , this task involves ﬁnding relationships between past and future occurrences of absolute pitch values . However , many music theoretical constructs that might help to ﬁnd such relationships are deﬁned in relative terms , such as diatonic scale steps and cadences . The discrepancy between the relative nature of musical constructs and the absolute pitch representation is problematic for modeling tasks , because it leads to high sparsity in the input data , bigger models , and altogether reduced generalization in music modeling . To remedy these problems , musical input sequences can be transposed to a com - mon key before training , augmented by random transpositions during training , or , in case of symbolic monophonic music , transformed into interval representations before training . In this Section , we propose a sequence model which learns both interval representations from absolute pitch sequences and temporal dependencies between such intervals . By not only learning the intervals between two successive notes but all intervals within a window of n pitches , the model is more robust to dia - tonic transposition and can also learn repetition structure . To that end , a recurrent neural network ( RNN ) is employed on top of a gated autoencoder ( GAE ) , which we refer to as Recurrent Gated Autoencoder ( RGAE ) . The GAE portion learns the intervals between its input and target pitches and represents them in its latent space . The RNN portion operates on these interval representations , to learn their temporal dependencies . The implicit transformation to intervals allows this archi - tecture to operate directly on absolute musical textures , without the need for data pre - processing . Besides , relative pitch modeling reduces the sparsity in the data , and the representations learned by the GAE are transposition - invariant . Therefore , the RGAE requires less temporal connections than a common RNN while achieving higher prediction accuracy . Also , operating on the intervals of input sequences brings added value to sequence modeling . By relating its prediction with events using speciﬁc time lags , the RGAE can learn copy - and - transpose operations of short melodic fragments . More precisely , a transposed copy of a fragment is generated by repeatedly applying a constant interval ( i . e . , the transposition distance ) to events occurring a constant time lag ( i . e . , the length of the fragment ) in the past . Moreover , the RGAE can learn sequences of such copy - and - transpose operations ( i . e . , “structure schemes” ) . As a 95 result , an arbitrary melodic fragment can be used as a building block from which transposed copies are generated over and over to create a new sequence obeying a learned scheme ( see Section 4 . 5 . 3 . 2 ) . Learning copy - and - transpose operations can be useful for music modeling , where repeated sections often occur as a transposed version of the initial section . With common sequence models , like RNNs , it is challenging to learn such self - similarity relationships . Common RNNs are specialized in learning the statistics of musical textures and are “blind” towards similarity and ( transposed ) repetition ( i . e . , there is no content - independent “repetition neuron” ) . As a result , when sampling music us - ing such models , repeated fragments occur either due to chance or as a phenomenon of an entanglement with a learned texture . In contrast , when RGAEs learn copy - and - transpose operations , they separate self - similarity structure from the actual content with which the structure is instantiated , leading to improvements in music prediction and music generation tasks . In conclusion , we show in this Section that the RGAE is competitive in a music prediction task . Furthermore , by combining the predictions of absolute pitch models with predictions of the ( relative pitch ) RGAE , we can further improve the prediction accuracy . This shows that the RGAE is complementary to common absolute pitch models . Lastly , we show that the RGAE is particularly suited for learning sequences of copy - and - transpose operations . It can learn to recognize and continue pre - deﬁned “structure schemes” , abstracted from the actual texture , with which the scheme is realized . In Section 4 . 5 . 1 , the GAE and the proposed extensions yielding the RGAE are described , as well as the baseline RNN used for comparison and combined predic - tion . General training details concerning the GAE are given in Section 4 . 5 . 2 . The experiments conducted , including data pre - processing , training details and discus - sions , are introduced in Section 4 . 5 . 3 . A ﬁnal discussion is given in Section 4 . 5 . 3 . 2 and Section 4 . 5 . 4 concludes this section and provides further directions . 4 . 5 . 1 Models 4 . 5 . 1 . 1 Gated Autoencoder For the GAE portion of our architecture , we largely adopt the GAE model described in Section 4 . 4 . 2 , but we only use one mapping layer for simplicity ( see Figure 4 . 11 , GAE portion ) , and diﬀerent non - linearities . Accordingly , the GAE learns to encode intervals in its latent mapping space m t + 1 = σ q ( W m ( Qx tt − n · Vx t + 1 ) ) , ( 4 . 15 ) where Q , V and W m are weight matrices , and σ q is the softplus non - linearity . The operator · ( indicated as a triangle in Figure 4 . 11 ) depicts the Hadamard product of the ﬁlter responses Qx tt − n and Vx t + 1 , denoted as factors . 96 The reconstruction of the target x t + 1 is deﬁned as a function of the input x tt − n and the mapping m t + 1 as ˜x t + 1 = σ g ( V > ( W > m m t + 1 · Qx tt − n ) ) , ( 4 . 16 ) where σ g is the sigmoid non - linearity . The GAE portion of the RGAE is pre - trained by minimizing the binary cross - entropy loss of the reconstruction ( see Equation 4 . 9 ) . 4 . 5 . 1 . 2 Recurrent Gated Autoencoder The proposed model is a combination of a gated autoencoder ( GAE ) and a recurrent neural network ( RNN ) as depicted in Figure 4 . 11 . The GAE learns relative pitch ( i . e . , interval ) representations of the musical surface , and the RNN learns their temporal dependencies . We use gated recurrent units ( GRUs ) [ Cho et al . , 2014 ] for the RNN portion of the RGAE . This type of units has been shown to be often as eﬃcient as long short - term memory units ( LSTMs , [ Hochreiter and Schmidhuber , 1997 ] ) while being conceptually simpler [ Chung et al . , 2014 ] . It is intuitively clear that any RNN variant can be potentially attached on a GAE . The input to the RNN at time t is the GAE’s mapping m t , resulting in the following speciﬁcation : z t = σ g ( W z m t + U z h t − 1 + b z ) , ( 4 . 17 ) r t = σ g ( W r m t + U r h t − 1 + b r ) , ( 4 . 18 ) h t = z t · h t − 1 + ( 1 − z t ) · σ h ( W h m t + U h ( r t · h t − 1 ) + b h ) , ( 4 . 19 ) where h t is the hidden state at time t , z t is the update gate vector , r t is the reset gate vector , and W , U and b are parameter matrices and vectors . The RNN predicts the next mapping of the GAE as f m t + 1 = σ q ( U o h t ) , ( 4 . 20 ) which is used to reconstruct the target conﬁguration at t + 1 as ˜x t + 1 = σ s ( V > ( W > m f m t + 1 · Qx tt − n ) ) . ( 4 . 21 ) Here , we use the softmax non - linearity σ s , as the data the RGAE is trained on is monophonic . The full architecture is trained with Backpropagation through time ( BPTT ) to minimize the categorical cross - entropy loss for the reconstructed target as L ( x , ˜x ) = − 1 N N X n = 1 x n log 2 ˜ x n . ( 4 . 22 ) When the RGAE is applied to polyphonic music , in Equation 4 . 16 the sigmoid non - linearity , together with the binary cross - entropy loss ( see Equation 4 . 9 ) has to be used . 97 R NN G A E Figure 4 . 11 : Schematic illustration of the proposed Recurrent Gated Autoencoder archi - tecture . Arrows represent weight matrices , rounded rectangles represent vectors . The triangles depict the Hadamard product . The speciﬁcs of the Gated Recurrent Unit are omitted for better clarity . 4 . 5 . 1 . 3 Baseline RNN As a baseline , we employ an RNN with GRUs to directly operate on the data . Accordingly , Equations 4 . 17 , 4 . 18 , and 4 . 19 are adapted to consume x t instead of m t as input . Consequently , the prediction of the baseline RNN amounts to ˜x t + 1 = σ s ( U o h t ) , ( 4 . 23 ) where the softmax non - linearity is applied , making the categorical cross - entropy loss ( see Equation 4 . 22 ) applicable in training . 4 . 5 . 2 Gated Autoencoder Pre - Training Due to the relatively high number of parameters in its GAE portion , the RGAE is prone to overﬁtting . To circumvent this , and to establish robust interval represen - tations , we pre - train the GAE ﬁrst , using the cross - entropy of the reconstruction as the cost function ( cf . Equation 4 . 9 ) . In the second training iteration , we train the RNN portion of the GAE to minimize the cross - entropy error of the architec - ture’s prediction ( cf . Equation 4 . 22 ) . The datasets may diﬀer between the training iterations as long as the included relations are identical ( e . g . , “intervals of western tonal music” ) . Consequently , the GAE parameters trained on one dataset can be used for prediction tasks on several datasets . Fine - tuning the whole architecture in the last few epochs of predictive training can make up for possible bias . 98 In the following , we describe how the GAE is pre - trained in our experiments . Details varying between the experiments are given later in the experiments section ( see Section 4 . 5 . 3 ) . Pre - training details The GAE portion is pre - trained using the method to support the learning of intervals , as introduced in Section 4 . 4 . 4 . 1 . The training details are as follows . We use 512 units in the factor layer and 64 units in the mapping layer of the GAE . On the latter , sparsity regularization [ Lee et al . , 2007 ] is applied . The deviation of the norms of the columns of both weight matrices U and V from their average norm is penalized . Furthermore , we restrict these norms to a maximum value . The learning rate is reduced from 1e - 3 to 0 throughout training , and RMSProp [ Hinton et al . , 2012 ] is used . 4 . 5 . 3 Experiments 4 . 5 . 3 . 1 Experiment 1 : Folk Song Prediction We test the RGAE and RNN in a sequence learning task using the data described in Section 4 . 5 . 3 . 1 . In order to make the results comparable , we use the same experimental setup as in [ Pearce and Wiggins , 2004 , Cherla , 2016 ] . Data The EFSC subset ( comprising a total of 54 , 308 note events ) of the Essen Folk Song Collection ( EFSC ) [ Schaﬀrath , 1995 ] constitutes the data for the actual training and evaluation . It consists of 119 Yugoslavian folk songs , 91 Alsatian folk songs , 93 Swiss folk songs , 104 Austrian folk songs , the German subset kinder ( 213 songs ) , and 237 songs of the Chinese subset shanxi . The melodies are represented as a series of pitches ignoring note durations . For pre - training the GAE portion of the RGAE , we again use the polyphonic Mozart piano music dataset described in Section 4 . 3 . 2 . 1 ( [ Widmer , 2003 ] , compris - ing 13 piano sonatas with more than 106 , 000 notes ) in piano - roll representation ( i . e . , using a regular time grid of 1 / 8th note resolution , and an active note can span several time steps ) . We pre - train on that data because polyphonic music acts as a better regularizer for learning interval representations than monophonic music . Training and Architecture We use only 16 hidden units in the RNN portion of the RGAE . The lookback window of the GAE is n = 8 pitches , and we apply 50 % dropout on the input in pre - training and when training the whole architecture . We pre - train the GAE for 250 epochs on the Mozart piano pieces ( cf . Section 4 . 5 . 3 . 1 ) . Subsequently , the RNN portion is trained for 110 epochs on the interval representations ( i . e . , mappings provided by the GAE ) of the EFSC datasets . In the last 10 epochs the whole architecture is ﬁne - tuned . 99 The baseline RNN with 50 hidden units is trained for 70 epochs on the EFSC data . The learning rate scheme is adopted from that described in Section 4 . 5 . 2 for all models . Combining Model Predictions We hypothesize that the RNN and the RGAE are complementary in how they process musical sequences . For example , the RNN may have better stability in remembering absolute reference pitches , like the tonic of a piece , and is superior in modeling prior probabilities , to keep predictions in a plausible pitch range . In contrast , the RGAE can make use of structural cues indi - cating repetitions and can generalize better due to relative pitch processing . There are several possibilities to combine the predictions of statistical models . Next to the ad - hoc approach of merely averaging their outputs , we can also use information about the certainty of the models and weight their outputs accordingly . A measure for the certainty of a prediction is given by the Shannon entropy [ Shannon , 1948 ] : H ( p ) = − X a ∈ A p ( a ) log 2 p ( a ) , ( 4 . 24 ) where p ( a ∈ A ) = P ( X = a ) is a probability mass function over a discrete alphabet A . The method which worked best in our experiments is calculating the entropy - weighted geometric mean of both predictions , as proposed in [ Pearce et al . , 2004 ] : p ( t ) = 1 R Y m ∈ M p m ( t ) w m , ( 4 . 25 ) where p m ( t ) is the predicted distribution of model m at time t , w m = H relative ( p m ) − b is the weight of model m , non - linearly scaled using a bias b ( set to 0 . 5 in our experiments ) , and R is a normalization constant . The relative entropy H relative ( p m ) for model m is given by H relative ( p m ) = H ( p m ) H max ( p m ) , ( 4 . 26 ) where H max ( p m ) > 0 is the entropy of the probability mass uniformly distributed over the alphabet ( indicating maximal uncertainty of the model ) . Evaluation Since the datasets are rather small , a ﬁxed training / test set split would lead to a poor estimation of the performance of the models . Therefore , and in accordance with [ Pearce and Wiggins , 2004 , Cherla , 2016 ] , a 10 - fold cross validation is performed for each dataset and the categorical cross - entropy loss ( cf . Equation 4 . 22 ) is reported . 100 RNN RTDRBM RGAE RNN + RNN + RTDRBM + Data ( GRU ) [ Cherla , 2016 ] RTDRBM RGAE RGAE Alsatian folk songs 2 . 890 2 . 897 2 . 872 2 . 844 2 . 788 2 . 771 Yugoslavian folk songs 2 . 717 2 . 655 2 . 676 2 . 617 2 . 586 2 . 530 Swiss folk songs 2 . 954 2 . 932 2 . 895 2 . 851 2 . 831 2 . 769 Austrian folk songs 3 . 185 3 . 259 3 . 171 3 . 163 3 . 070 3 . 085 German folk songs 2 . 358 2 . 301 2 . 305 2 . 257 2 . 233 2 . 184 Chinese folk songs 2 . 725 2 . 685 2 . 752 2 . 612 2 . 650 2 . 595 Average 2 . 805 2 . 788 2 . 779 2 . 724 2 . 693 2 . 656 Table 4 . 6 : Cross - Entropies of the 10 - fold cross validation in the prediction task for diﬀer - ent data sets and diﬀerent models . When combining the RGAE with an absolute pitch model ( i . e . , RNN , RTDRBM ) , results improve substantially . The results suggest that ab - solute and relative pitch models are complementary in the aspects they learn about music and can be eﬀectively used in an ensemble method . Results and Discussion The results are shown in Table 4 . 6 . The current state - of - the - art results for general connectionist sequence models on the datasets are achieved by the RTDRBM model introduced in [ Cherla , 2016 ] . The results show that the RGAE slightly outperforms the RTDRBM and is clearly superior to the baseline RNN . Note that , as stated in Section 4 . 5 . 3 . 1 , the RGAE needs only 16 hid - den units for learning temporal dependencies ( the GAE portion mainly transforms absolute pitch input to relative pitch representations ) . This compactness suggests that the relative processing of music indeed supports generalization by reducing the sparsity in the data . When combining the predictions of the RGAE with an absolute pitch model ( i . e . , RNN or RTDRBM ) based on the entropy - weighted geometric mean ( cf . Section 4 . 5 . 3 . 1 ) , a more substantial improvement is achieved than when combining the two absolute pitch models . This result shows that absolute and relative processing of music are complementary and can , therefore , be eﬀectively used together in an ensemble method . 4 . 5 . 3 . 2 Experiment 2 : Copy - and - Transpose Operations This experiment is intended as a proof - of - concept for the RGAE’s ability to learning sequences of copy - and - transpose operations ( i . e . , structure schemes ) . We oppose our model to an RNN with GRUs , which is known to have diﬃculties in learning tasks in the form “whatever has been generated before , now create a ( transposed ) copy of it” . The hypothesis is that the RGAE , due to its modeling of intervals , is su - perior in solving this task . It has shown in previous studies that it can learn content - invariant transformations between data instances [ Memisevic and Exarchakis , 2013 ] , a necessary capability for learning content - invariant structure schemes . 101 Transposition Schemes { + 5 , + 5 , + 5 , . . . } { + 7 , + 7 , + 7 , . . . } { − 5 , − 5 , − 5 , . . . } { − 7 , − 7 , − 7 , . . . } { + 12 , − 12 , + 12 , . . . } { + 3 , − 3 , + 3 , . . . } { + 4 , − 4 , + 4 , . . . } { + 9 , − 9 , + 9 , . . . } { + 4 , − 8 , + 4 , − 8 , . . . } { − 4 , + 8 , − 4 , + 8 , . . . } Table 4 . 7 : The diﬀerent relative transposition schemes used in the “learning copy - and - transpose operations” experiment . Data In order to obtain a controlled setup for testing the model performances , we construct data obeying diﬀerent recurring ( chromatic ) transposition patterns . To this end , the EFSC dataset is transformed into a piano - roll representation with a resolution of 1 / 8th note . From that , short fragments of length 4 , 8 , and 16 ( ≤ the input length of the models ) are randomly sampled ( rests are omitted ) . It is necessary that the RGAE has access to all past events with which the prediction should be related . Choosing longer fragment lengths than the lengths of the re - ceptive ﬁelds yields considerably worse results , also for the baseline RNN , which already performs weakly in this setup . The fragments are copied and transposed according to some pre - deﬁned transposition schemes ( cf . Table 4 . 7 ) . For each of the 10 schemes and fragment lengths , 26 sequences ( 512 time steps each , resulting in 133 120 time steps ) are generated , where 20 sequences are used for training , 5 sequences are used for testing and 1 for evaluation . This results in a total of 600 sequences for training , 150 sequences for testing and 30 sequences for evaluation . Training and Architecture The lookback window of the RGAE is n = 16 time steps , the RNN portion has 64 units , and we do not use dropout on the input . For the baseline RNN , we also input the 16 preceding time steps , as this supports copy operations by freeing up memory in the hidden units ( i . e . , that way , we obtain a direct path from the relevant information to the output ) . The baseline RNN model size ( 512 units ) is selected by starting from 64 units and always doubling that number until no substantial improvement occurs on the evaluation set . The GAE portion of the RGAE is pre - trained for 50 epochs on the structured sequences described above . Subsequently , the RGAE is trained for 50 epochs , hold - ing the parameters of the GAE ﬁxed . As the data of the pre - training does not diﬀer 102 Model Pr ( % ) > 99 % CE # Params RNN 41 . 38 6 . 67 0 . 287 ∼ 2 300 000 RGAE 99 . 43 92 . 00 0 . 033 ∼ 600 000 Table 4 . 8 : Results of the “learning copy - and - transpose operations” task . Average precision ( Pr ) , percentage of continuations above 99 % precision , cross - entropy ( CE ) and number of parameters of the respective model . from the sequences in the prediction task , ﬁne - tuning is not necessary . The baseline RNN is trained for 60 epochs . Again , for both models , the learning rate scheme described in Section 4 . 5 . 2 is employed . Note that in this task , we always randomly transpose the input to the models in all training phases . Therefore , we need no dropout on the input of the RGAE , and the baseline RNN does not overﬁt , despite its high number of parameters . Evaluation The models have to learn to continue sequences from the test set after exposition to the ﬁrst 64 time steps of each sequence . The experiment is diﬀerent from typical prediction tasks in that possibly incorrect predictions are fed back to the models , causing errors to accumulate . To obtain more stable continuations , we do not sample from the predicted distributions of the models , but instead , treat the experiment as a classiﬁcation task and choose the pitch with the highest pre - dicted probability . Accordingly , the precision is merely the percentage of correctly predicted pitches over time . Also , we quantify how many sequences are correctly continued until the end by considering all sequences with an overall precision above 99 % as correctly continued . Furthermore , like in Experiment 1 , the categorical cross - entropy loss ( cf . Equation 4 . 22 ) is computed . Results and Discussion Table 4 . 8 shows the quantitative results of the ex - periment , and Figure 4 . 12 shows a box plot comparing the precisions of the two models . With an average precision of 99 . 43 % percent , where 92 % of all examples are ﬂawlessly continued , the RGAE shows remarkable stability in continuing the structure scheme realizations . The cross - entropy of the RGAE is about two orders of magnitude lower than that of the RNN . In Figure 4 . 13 , a speciﬁc example of this sequence continuation task is depicted . Note that the hidden unit activations of the RGAE are more regular because they only represent copy - and - transpose opera - tions instead of the musical texture itself ( as it is the case for the RNN ) . The most challenging part for the RGAE is counting , in order to change the copy operation ( i . e . , transposition distance ) at the right time ( in fact , at most of the incorrectly continued sequences , the RGAE miscounted by a time step ) . It is important to note that the hidden unit activations of the RNN portion are identical for identical 103 RGAE RNN 0 20 40 60 80 100 P r e c i s i o n ( % ) Precisions vs . Model Figure 4 . 12 : Distribution of precisions for continuation of highly structured sequences in the test set of size 150 . The median is marked with a orange line , the boxes indicate the interquartile range , and circles indicate outliers . schemes because they operate on transformations between events , rather than on the events themselves ( i . e . , they are largely content - invariant ) . 4 . 5 . 4 Conclusion and Future Work The principle of modeling sequences of ﬁrst - order derivatives in music is a com - pelling concept with the potential to solve two persistent problems in MIR : Learn - ing transposition - invariant interval representations , and learning representations of ( chromatically transposed ) repetition structure . The proposed model is conceptu - ally simple and can be trained as a generative model in sequence learning tasks . Moreover , the RGAE could act as a building block for more complex architec - tures , in order to extend its capabilities . For example , the temporal lookback window could be greatly extended by employing the RGAE on top of a ( dilated ) convolutional network , enabling it to learn higher - level repetition structure . In an - other variant , an RGAE could be employed on top of an RNN . Applied to music , the RNN would provide the RGAE with representations of important , absolute reference pitches ( e . g . , the tonic of a scale , or the root note of a chord ) , and the RGAE could learn sequences of intervals in relation to them . Another interesting architecture would involve stacking more than one RGAE on top of one another to learn higher - order derivatives , for example , variations between mutually transposed parts in music . The RGAE , however , is not limited to the symbolic , monophonic , domain of music . As shown in Section 4 . 4 , a GAE can also operate in the spectral domain of audio and in polyphonic symbolic music . Finally , we note that the RGAE is general enough to apply to other domains where the derivatives of functions are of 104 Figure 4 . 13 : Generated structure schemes and hidden unit activations of the RGAE and the RNN models after input of a primer indicating the { − 4 , + 8 , − 4 , + 8 , . . . } scheme , realized with melodies of length 16 not contained in the train set . Black notes indi - cate correct continuation , green notes indicate false negatives , red notes indicate false positives . Hidden units activations of the RNN are pruned due to space limitation . higher importance than their absolute course . Possible applications include model - ing temporal progressions of changes in loudness , tempo , mood , information density curves , and other musical properties , modeling moving or rotating objects , camera movements in video recordings , and signals in the time domain . 105 5 Conclusion and Future Work In this section , the chapters of this thesis are brieﬂy reviewed , and an outlook on possible future research is given . I will focus mainly on two research directions that I consider to be promising for follow - up in musical structure learning : Information Theory and Transformation Learning . The conclusion starts by pointing out the potential of Information Theory in music structure analysis and structured gener - ation , referring to some examples in linguistics . Then , I will review my work on transformation and invariance learning , including a brief discussion on future work and on two of my very recent publications , which are related but not part of this thesis . 5 . 1 Information Theory and Structure Analysis In Chapter 2 , I perform segmentation of monophonic melodies using a probabilistic approach . Probabilistic segmentation is based on the assumption that the proba - bility of events within a segment is higher than that of events that mark segment boundaries . The metric employed in practice is the information content ( IC ) , de - ﬁned as the negative log probability of a note . IC can be interpreted as the " level of surprise " a listener would experience when encountering a note event . We con - ﬁrm prior studies ( see [ Pearce et al . , 2010b ] ) that the IC , in combination with a threshold , provides a valid proxy for determining segment boundaries . Other computational models of segmentation are based on Gestalt Principles . We discuss in Chapter 2 that while there is a direct link between IC and Gestalt Principles , it is unclear which is more parsimonious and might have given rise for the other to emerge as a perceptual mechanism . In any case , the competitiveness of the probabilistic approach is evidence for the usefulness of Information Theory in musical structure analysis . In light of this observation , it seems worthwhile to consider other metrics of Information Theory for music structure modeling . In Chapter 3 , I evaluate the generated musical sequences x 0 . . . x T of length T based on Information Rate ( IR ) . IR , in sequences also known as the mutual information between the current event x t and its predecessors x < t , is deﬁned as H ( x t ) − H ( x t | x < t ) , where H ( · ) is the entropy . The equation yields high IR , when the conditional entropy of all events H ( x t | x < t ) is minimal ( i . e . , when all events in a sequence are highly expected given the past ) , and when the prior entropy of the events H ( x t ) is maximal ( i . e . , when all events are equally likely to occur ) . The 107 IR is therefore high , when a sequence is well predictable ( highly structured ) , but not trivial ( i . e . , not composed of , for example , just a single , constantly repeating event ) . IR can also be interpreted as the relative reduction of uncertainty of the present when the past is known [ Dubnov et al . , 2011 ] . In music , IR is a measure for “well - structuredness” and has been applied , for instance , in parameter selection for musical pattern discovery [ Wang and Dubnov , 2015 ] . Another Information Theory metric , which has been less studied in the musical domain , is Uniform Information Density ( UID ) . UID is based on the proposal of Shannon that for optimal data ﬂow through a noisy channel , the information density ( i . e . , the IC over time ) should be as uniform as possible [ Shannon , 1948 ] . This means that in order to minimize comprehension diﬃculty in language and music processing , the sensory information should be structured so as to avoid peaks and troughs in information density [ Levy and Jaeger , 2006 ] . It has been shown that speakers intuitively construct utterances in order to satisfy UID ( e . g . , by modifying the talking speed [ Bell et al . , 2003 , Aylett and Turk , 2004 ] , or by omitting words which carry little information [ Levy and Jaeger , 2006 ] ) . In order to avoid troughs in information density , the expectedness of some phrases may be deliberately reduced . For example , when a phrase is semantically expected due to the preceding context , it can have a more complex syntactic form [ Genzel and Charniak , 2002 ] . In music , a repetition usually causes a trough in information density , because it is easy to predict the events of a sequence when the same sequence has already been encountered . In order to balance the overall predictability of such a repeated musi - cal phrase , some less predictable events ( e . g . , unlikely intervals ) may be introduced . Evidence for this is given by Temperley [ 2014 ] , who shows that “there is a tendency that when an intervallic pattern is repeated with alterations , the alterations tend to lower the probability of the pattern rather than raising it . ” As we have seen , Information Theory metrics can make predictions about lan - guage usage ( UID ) , musical segmentation boundaries ( IC ) , choices in music com - position ( UID ) , and parameters for musical pattern discovery systems ( IR ) . Con - sidering the apparent links between Information Theory and structural properties in language and music , it seems obvious to further examine possible applications of Information Theory metrics to music composition and analysis . As generative sequence models become ever more powerful [ Payne , Website accessed July 19th , 2019 ] , their precision in estimating Information Theory metrics is naturally increas - ing , too . With the current state of the art , it would already be possible to perform large - scale corpus studies on audio data . Future work could investigate the variance of information in diﬀerent genres and in diﬀerent time periods ( where higher vari - ance suggests less UID , as proposed for language in [ Collins , 2014 ] ) . Such a study , besides its potential to yield interesting insights on its own , could also help to quantify the degree of human - acceptable violations of UID , which in turn could be used for automatic music composition systems . Analysis of musical works regarding 108 UID could also be used in assisted composition systems to point out “problematic” sections ( i . e . , such with too high or too low variance in information ) . Likewise , IR could inform automatic composition systems regarding the “well - structuredness” of their output . Overall , it seems that the full potential of Information Theory in music generation and analysis has not yet been suﬃciently exploited—a situation which should be improved in the future . 5 . 2 Learning Transformations and Invariances in Music In Section 4 . 3 , I explore a novel way of musical representation learning . To that end , I do not aim to learn the musical patterns themselves , but some “rules” deﬁning how a given pattern can be transformed into another pattern . Transformation Learning ( TL ) was initially proposed for image processing and had not yet been applied to music . In this section , I summarize our experiments in TL for music and suggest possible future research directions . The models used throughout this thesis are based on Gated Autoencoders ( GAE ) which learn orthogonal transformations between data pairs . We show that a GAE can learn chromatic transposition , tempo - change , and the retrograde movement in music , but also more complex musical transformations , like diatonic transposition ( see Section 4 . 3 ) . Transformation Learning ( TL ) provides us with a diﬀerent view on music data , and yields features complementary to other music descriptors ( e . g . , such as obtained by autoencoder learning or hand - crafted features ) . There are dif - ferent possible research directions regarding TL in music . They involve using the transformation features themselves , using transformation - invariant features com - puted from TL models , and using TL models for music generation . The most obvious usage of TL is to represent transformations between n - gram pairs which stem from a song or a corpus , as shown in Section 4 . 3 . In the future , statistics of the thus obtained features could be used for comparison of musical works ( e . g . , regarding the occurrence probability of certain transformations ) . In intra - opus analysis , all possible pairwise transformations between the n - grams of a song could be computed . Such a representation would be informative for structure analysis , but also for music generation . Given a complete description of the trans - formations in a song , a new song could be found obeying these transformations but instantiated with diﬀerent content . In Chapter 3 , I propose a constrained sampling approach to transfer structural properties from an existing piece to newly generated material . It would be straight - forward to use TL features as constraints in order to transfer the “transformation structure” between musical works . A further application of Transformation Learning ( TL ) in music is to represent musical events as the result of transformation rules applied to preceding events . In Section 4 . 4 , I show that this approach yields features which behave like interval 109 representations ( i . e . , their organization in the latent space is musically meaningful , and they are transposition - invariant ) . In such a setting , again , TL is useful for analysis and generation . Due to the invariance property of the interval features , a self - similarity analysis in the feature space of musical works is suﬃcient to ﬁnd mutually transposed sections . Furthermore , in audio - to - score and audio - to - audio alignment , conventional dynamic time - warping approaches can be applied to mutu - ally transposed tracks , if the alignment is performed in the transposition - invariant feature space [ Arzt and Lattner , 2018 ] . When applying sequence models to the invariant interval space , the accuracy in predicting musical events can be improved compared to using common input representations ( see Section 4 . 5 ) . Furthermore , in music generation , using these features I show that it is easier for sequence models to generate copies of musical fragments , where a literal copy is just a special case of the transposed copy operation ( see Section 4 . 5 . 3 . 2 ) . Importantly , learning transformations and learning invariances are two sides of the same coin ( as speciﬁc invariances are deﬁned with respect to speciﬁc trans - formations ) . This becomes even clearer when considering the functioning of the Gated Autoencoder ( GAE ) . When learning a particular orthogonal transformation between data pairs , a GAE internally performs an eigenvector - decomposition of that transformation . In general , the eigenvectors of orthogonal transformations are complex - valued . When projecting data points onto a complex eigenvector pair of a particular transformation , the norm of the resulting coeﬃcients is invariant to that transformation . A well - known example of this property is the Fourier transform . As sine and cosine represent the complex eigenvectors of translation , the amplitude spectrum of a signal is invariant to translation ( i . e . , phase shift ) of the signal . Even though they do not emerge through conventional GAE training , it has been shown in [ Memisevic and Exarchakis , 2013 ] , that a GAE can be constrained to obtain such complex eigenvector pairs . Very recently , I introduced the Complex Autoencoder [ Lattner et al . , 2019 ] , a model which explicitly learns such complex eigenvectors from data pairs and showed that the features thus obtained are competitive in alignment tasks and repeated section discovery . A further example of the usefulness of transformation learning in music is an - other recent study on conditional rhythm generation [ Lattner and Grachten , 2019 ] . Following a similar intuition as in Section 4 . 4 , where a GAE learns the pitch inter - vals between temporally successive events , we now learn the inter - onset - intervals between diﬀerent rhythm instruments in a song . More precisely , a GAE is extended to a convolutional architecture and used for the generation of kick drum rhythms , conditioned on snare drum , bass , and estimated beat and downbeat information . Like in the experiments mentioned above , the model beneﬁts from the invariance properties of Transformation Learning ( TL ) . In fact , it is possible to represent a large part of the dynamic rhythmic relationships in a song by a single style vector . This style vector is invariant to time ( i . e . , the model produces plausible output when 110 the style vector is kept constant over time ) , and invariant to the tempo ( i . e . , the output tempo is determined by the input tempo and not the style vector ) . These properties render the approach very useful for high - level user control because a desired style vector can be found by manual exploration of a “style space” and can then be kept constant for the whole song or section while obtaining an output rhythm which dynamically adapts to the rhythmic context . The idea of conditional generation of rhythmic structures using TL could be further extended to conditional generation of tonal structures in the future . Instead of learning the pitch intervals between successive events like in Section 4 . 4 , one could learn the pitch intervals between concurrent events of diﬀerent instruments . In that case , a transposition - invariant style vector would , for example , yield the root note with respect to the tonal context , while another style vector conﬁguration would yield the third or the ﬁfth . While keeping the style vector ﬁxed , the output pitch would still change , when the tonal context changes . This would considerably reduce the amount of information in the representation space of a model and could lead to more compact music generation systems . A disadvantage of Transformation Learning ( TL ) models like the GAE is their restriction to orthogonal transformations . Learning more general transformations in an unsupervised manner with neural network architectures could be relevant in music and other domains and should be investigated in the future . A further problem is the selection of data pairs to feed to the model . GAEs do not learn well if a considerable portion of the input pairs is not related . Preliminary experiments suggest that this problem can be overcome by using methods from curriculum learning , which can choose to ignore data pairs which cause high losses [ Jiang et al . , 2015 ] . The given examples show that TL can provide us with a diﬀerent view on musical data , leading to improvements or advantageous behaviors in music analysis and generation systems . I also pointed out some promising future research directions and challenges . As TL in music is still a new ﬁeld , it oﬀers a high potential for novel ﬁndings and applications . 111 Bibliography Edward H Adelson and James R Bergen . Spatiotemporal energy models for the perception of motion . JOSA A , 2 ( 2 ) : 284 – 299 , 1985 . Kat Agres , Carlos Cancino , Maarten Grachten , and Stefan Lattner . Harmon - ics co - occurrences bootstrap pitch and tonality perception in music : Evidence from a statistical unsupervised learning model . In Proceedings of the 37th An - nual Meeting of the Cognitive Science Society , CogSci 2015 , Pasadena , Cali - fornia , USA , July 22 - 25 , 2015 . cognitivesciencesociety . org , 2015 . URL https : / / mindmodeling . org / cogsci2015 / papers / 0018 / index . html . Droniou Alain and Sigaud Olivier . Gated autoencoders with tied input weights . In Proceedings of the 30th International Conference on Machine Learning ( ICML - 13 ) , volume 28 , pages 154 – 162 , 2013 . Andreas Arzt and Stefan Lattner . Audio - to - score alignment using transposition - invariant features . In Proceedings of the 19th International Society for Music Information Retrieval Conference , ISMIR 2018 , Paris , France , September 23 - 27 , 2018 , pages 592 – 599 , 2018 . URL http : / / ismir2018 . ircam . fr / doc / pdfs / 166 _ Paper . pdf . Matthew Aylett and Alice Turk . The smooth signal redundancy hypothesis : A functional explanation for relationships between redundancy , prosodic promi - nence , and duration in spontaneous speech . Language and speech , 47 ( 1 ) : 31 – 56 , 2004 . Matthew Aylett and Alice Turk . Language redundancy predicts syllabic duration and the spectral characteristics of vocalic syllable nuclei . The Journal of the Acoustical Society of America , 119 ( 5 ) : 3048 – 3058 , 2006 . doi : 10 . 1121 / 1 . 2188331 . A . D . Baddeley . Short - term memory for word sequences as a function of acoustic , semantic and formal similarity . Quarterly Journal of Experimental Psychology , 18 ( 4 ) : 362 – 365 , 1966 . doi : 10 . 1080 / 14640746608400055 . Nicola Barbieri . Regularized Gibbs sampling for user proﬁling with soft con - straints . In International Conference on Advances in Social Networks Analy - sis and Mining , ASONAM 2011 , Kaohsiung , Taiwan , 25 - 27 July 2011 , pages 113 129 – 136 . IEEE Computer Society , 2011 . doi : 10 . 1109 / ASONAM . 2011 . 92 . URL https : / / doi . org / 10 . 1109 / ASONAM . 2011 . 92 . Alan Bell , Daniel Jurafsky , Eric Fosler - Lussier , Cynthia Girand , Michelle Gregory , and Daniel Gildea . Eﬀects of disﬂuencies , predictability , and utterance position on word form variation in english conversation . The Journal of the Acoustical Society of America , 113 ( 2 ) : 1001 – 1024 , 2003 . Yoshua Bengio . Learning deep architectures for AI . Foundations and Trends in Machine Learning , 2 ( 1 ) : 1 – 127 , 2009 . Nicolas Boulanger - Lewandowski , Yoshua Bengio , and Pascal Vincent . Modeling temporal dependencies in high - dimensional sequences : Application to polyphonic music generation and transcription . In Proceedings of the 29th International Conference on Machine Learning , ICML 2012 , Edinburgh , Scotland , UK , June 26 - July 1 , 2012 , 2012 . URL http : / / icml . cc / 2012 / papers / 590 . pdf . A . S . Bregman . Auditory Scene Analysis . MIT Press , Cambridge , MA , 1990 . Michael R . Brent . An eﬃcient , probabilistically sound algorithm for segmentation and word discovery . 34 ( 1 – 3 ) : 71 – 105 , 1999 . Jane Bromley , James W . Bentz , Léon Bottou , Isabelle Guyon , Yann LeCun , Cliﬀ Moore , Eduard Säckinger , and Roopak Shah . Signature veriﬁcation using a " siamese " time delay neural network . IJPRAI , 7 ( 4 ) : 669 – 688 , 1993 . E Cambouropoulos . The local boundary detection model ( LBDM ) and its applica - tion in the study of expressive timing . In Proceedings of the 2006 International Computer Music Conference , pages 17 – 22 , San Francisco , 2001 . Carlos Eduardo Cancino Chacón , Stefan Lattner , and Maarten Grachten . Develop - ing tonal perception through unsupervised learning . In Hsin - Min Wang , Yi - Hsuan Yang , and Jin Ha Lee , editors , Proceedings of the 15th International Society for Music Information Retrieval Conference , ISMIR 2014 , Taipei , Taiwan , October 27 - 31 , 2014 , pages 195 – 200 , 2014 . URL http : / / www . terasoft . com . tw / conf / ismir2014 / proceedings / T036 _ 230 _ Paper . pdf . Srikanth Cherla . Neural Probabilistic Models for Melody Prediction , Sequence La - belling and Classiﬁcation . PhD thesis , City , University of London , 2016 . Kyunghyun Cho , Bart van Merriënboer , Dzmitry Bahdanau , and Yoshua Bengio . On the properties of neural machine translation : Encoder – decoder approaches . Syntax , Semantics and Structure in Statistical Translation , page 103 , 2014 . 114 Keunwoo Choi , George Fazekas , and Mark B . Sandler . Text - based LSTM networks for automatic music composition . CoRR , abs / 1604 . 05358 , 2016 . URL http : / / arxiv . org / abs / 1604 . 05358 . Sumit Chopra , Raia Hadsell , and Yann LeCun . Learning a similarity metric discrim - inatively , with application to face veriﬁcation . In IEEE Computer Society Con - ference on Computer Vision and Pattern Recognition , CVPR 2005 . , volume 1 , pages 539 – 546 . IEEE , 2005 . Junyoung Chung , Caglar Gulcehre , Kyunghyun Cho , and Yoshua Bengio . Empir - ical evaluation of gated recurrent neural networks on sequence modeling . arXiv preprint arXiv : 1412 . 3555 , 2014 . Michael Xavier Collins . Information density and dependency length as comple - mentary cognitive models . Journal of psycholinguistic research , 43 ( 5 ) : 651 – 681 , 2014 . Tom Collins . Discovery of repeated themes and sections . http : / / www . music - ir . org / mirex / wiki / 2017 : Discovery _ of _ Repeated _ Themes _ % 26 _ Sections , 2017 . Tom Collins , Andreas Arzt , Sebastian Flossmann , and Gerhard Widmer . SIARCT - CFP : improving precision and the discovery of inexact musical patterns in point - set representations . In Proceedings of the 14th International Society for Music Information Retrieval Conference , ISMIR 2013 , Curitiba , Brazil , November 4 - 8 , 2013 , pages 549 – 554 , 2013 . URL http : / / www . ppgia . pucpr . br / ismir2013 / wp - content / uploads / 2013 / 09 / 161 _ Paper . pdf . Tom Collins , Robin C . Laney , Alistair Willis , and Paul H . Garthwaite . De - veloping and evaluating computational models of musical style . Artiﬁcial In - telligence for Engineering Design , Analysis and Manufacturing , 30 ( 1 ) : 16 – 43 , 2016 . doi : 10 . 1017 / S0890060414000687 . URL https : / / doi . org / 10 . 1017 / S0890060414000687 . Darrell Conklin . Chord sequence generation with semiotic patterns . Journal of Mathematics and Music , 10 ( 2 ) : 92 – 106 , 2016 . doi : 10 . 1080 / 17459737 . 2016 . 1188172 . URL https : / / doi . org / 10 . 1080 / 17459737 . 2016 . 1188172 . David Cope . Experiments in musical intelligence , volume 12 . Madison , WI : AR editions , 1996 . Sally Jo Cunningham , Zhiyao Duan , Xiao Hu , and Douglas Turnbull , editors . Pro - ceedings of the 18th International Society for Music Information Retrieval Con - ference , ISMIR 2017 , Suzhou , China , October 23 - 27 , 2017 , 2017 . ISBN 978 - 981 - 11 - 5179 - 8 . 115 Afshin Dehghan , Enrique G Ortiz , Ruben Villegas , and Mubarak Shah . Who do i look like ? determining parent - oﬀspring resemblance via gated autoencoders . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni - tion , pages 1757 – 1764 , 2014 . Alain Droniou , Serena Ivaldi , and Olivier Sigaud . Learning a repertoire of actions with deep neural networks . In IEEE International Joint Conferences on Devel - opment and Learning and Epigenetic Robotics ( ICDL - Epirob ) , pages 229 – 234 . IEEE , 2014 . Alain Droniou , Serena Ivaldi , and Olivier Sigaud . Deep unsupervised network for multimodal perception , representation and classiﬁcation . Robotics and Au - tonomous Systems , 71 : 83 – 98 , 2015 . Shlomo Dubnov , Gérard Assayag , and Arshia Cont . Audio oracle analysis of musical information rate . In Proceedings of the 5th IEEE International Conference on Semantic Computing ( ICSC 2011 ) , Palo Alto , CA , USA , September 18 - 21 , 2011 , pages 567 – 571 . IEEE Computer Society , 2011 . doi : 10 . 1109 / ICSC . 2011 . 106 . URL https : / / doi . org / 10 . 1109 / ICSC . 2011 . 106 . Douglas Eck and Juergen Schmidhuber . A ﬁrst look at music composition using LSTM recurrent neural networks . Technical report , Istituto Dalle Molle Di Studi Sull Intelligenza Artiﬁciale , 2002 . URL www . iro . umontreal . ca / ~ eckdoug / blues / IDSIA - 07 - 02 . pdf . Arne Eigenfeldt and Philippe Pasquier . Evolving structures for electronic dance music . In Genetic and Evolutionary Computation Conference , GECCO ’13 , Am - sterdam , The Netherlands , July 6 - 10 , 2013 , pages 319 – 326 . ACM , 2013 . doi : 10 . 1145 / 2463372 . 2463415 . URL http : / / doi . acm . org / 10 . 1145 / 2463372 . 2463415 . Valentin Emiya , Roland Badeau , and Bertrand David . Multipitch estimation of piano sounds using a new probabilistic spectral smoothness principle . IEEE Transactions on Audio , Speech , and Language Processing , 18 ( 6 ) : 1643 – 1654 , 2010 . Bradley W Frankland and Annabel J Cohen . Parsing of Melody : Quantiﬁcation and Testing of the Local Grouping Rules of Lerdahl and Jackendoﬀ’s A Generative Theory of Tonal Music . Music Perception , 21 ( 4 ) : 499 – 543 , 2004 . Leon A . Gatys , Alexander S . Ecker , and Matthias Bethge . Image style transfer using convolutional neural networks . In 2016 IEEE Conference on Computer Vision and Pattern Recognition , CVPR 2016 , Las Vegas , NV , USA , June 27 - 30 , 2016 , pages 2414 – 2423 . IEEE Computer Society , 2016 . doi : 10 . 1109 / CVPR . 2016 . 265 . URL https : / / doi . org / 10 . 1109 / CVPR . 2016 . 265 . 116 Dmitriy Genzel and Eugene Charniak . Entropy rate constancy in text . In Proceed - ings of the 40th annual meeting of the Association for Computational Linguistics , 2002 . Arit Glicksohn and Asher Cohen . The role of Gestalt grouping principles in visual statistical learning . Attention Perception & Psychophysics , 73 : 708 – 713 , 2011 . doi : 10 . 3758 / s13414 - 010 - 0084 - 4 . F . Gobet and H . Simon . Expert chess memory : Revisiting the chunking hypothesis . Memory , 6 : 225 – 255 , 1998 . F . Gobet , P . C . R . Lane , S . Croker , P . C - H . Cheng , G . Jones , I . Oliver , and J . M . Pine . Chunking mechanisms in human learning . Trends in Cognitive Sciences , 5 ( 6 ) : 236 – 243 , 2001 . ISSN 1364 - 6613 . doi : 10 . 1016 / S1364 - 6613 . H . Goh , N . Thome , and M . Cord . Biasing restricted Boltzmann machines to ma - nipulate latent selectivity and sparsity . NIPS workshop on deep learning and unsupervised feature learning , 2010 . M . Grachten and F . Krebs . An assessment of learned score features for modeling expressive dynamics in music . IEEE Transactions on Multimedia , 16 ( 5 ) : 1211 – 1218 , 2014 . doi : 10 . 1109 / TMM . 2014 . 2311013 . URL http : / / dx . doi . org / 10 . 1109 / TMM . 2014 . 2311013 . Yves Grandvalet and Yoshua Bengio . Semi - supervised Learning by Entropy Mini - mization . Advances in Neural Information Processing Systems , 17 : 529 – 536 , 2004 . Alex Graves . Generating sequences with recurrent neural networks . CoRR , abs / 1308 . 0850 , 2013 . URL http : / / arxiv . org / abs / 1308 . 0850 . Gaëtan Hadjeres and Frank Nielsen . Anticipation - RNN : enforcing unary constraints in sequence generation , with application to interactive music generation . Neural Computing and Applications , pages 1 – 11 , 2018 . Gaëtan Hadjeres , François Pachet , and Frank Nielsen . DeepBach : A steerable model for bach chorales generation . In Doina Precup and Yee Whye Teh , editors , Proceedings of the 34th International Conference on Machine Learning , ICML 2017 , Sydney , NSW , Australia , August 6 - 11 , 2017 , volume 70 of Proceedings of Machine Learning Research , pages 1362 – 1371 . PMLR , 2017 . URL http : / / proceedings . mlr . press / v70 / hadjeres17a . html . Dorien Herremans and Elaine Chew . MorpheuS : Automatic music generation with recurrent pattern constraints and tension proﬁles . In Proceedings of the IEEE Region 10 Conference ( TENCON ) , Singapore , November 22 - 25 , 2016 , pages 282 – 285 . IEEE , 2016 . doi : 10 . 1109 / TENCON . 2016 . 7848007 . 117 G . E . Hinton , S . Osindero , and Y . Teh . A fast learning algorithm for deep belief nets . Neural Computation , 18 : 1527 – 1554 , 2006 . Geoﬀrey Hinton , Nitish Srivastava , and Kevin Swersky . Neural networks for ma - chine learning Coursera Lecture 6a overview of mini - batch gradient descent , 2012 . Geoﬀrey E Hinton . Training products of experts by minimizing contrastive diver - gence . Neural Computation , 14 ( 8 ) : 1771 – 1800 , July 2002 . Geoﬀrey E Hinton , Alex Krizhevsky , and Sida D Wang . Transforming auto - encoders . In Proceedings of the 21th International Conference on Artiﬁcial Neural Networks , ICANN’11 . Springer - Verlag , June 2011 . Geoﬀrey E Hinton , Oriol Vinyals , and Jeﬀ Dean . Distilling the Knowledge in a Neural Network . In NIPS 2014 Deep Learning and Representation Learning Workshop , December 2014 . Sepp Hochreiter and Jürgen Schmidhuber . Long short - term memory . Neural Computation , 9 ( 8 ) : 1735 – 1780 , 1997 . doi : 10 . 1162 / neco . 1997 . 9 . 8 . 1735 . URL https : / / doi . org / 10 . 1162 / neco . 1997 . 9 . 8 . 1735 . Douglas R . Hofstadter and Melanie Mitchell . The copycat project : A model of mental ﬂuidity and analogy - making . In Douglas R . Hofstadter and the Fluid Analogies Research group , editors , Fluid Concepts and Creative Analogies , pages 205 – 267 . Basic Books , 1995 . Cheng - Zhi Anna Huang , Tim Cooijmans , Adam Roberts , Aaron C . Courville , and Douglas Eck . Counterpoint by convolution . In Cunningham et al . [ 2017 ] , pages 211 – 218 . ISBN 978 - 981 - 11 - 5179 - 8 . URL https : / / ismir2017 . smcnus . org / wp - content / uploads / 2017 / 10 / 187 _ Paper . pdf . Cheng - Zhi Anna Huang , Ashish Vaswani , Jakob Uszkoreit , Ian Simon , Curtis Hawthorne , Noam Shazeer , Andrew M Dai , Matthew D Hoﬀman , Monica Din - culescu , and Douglas Eck . Music transformer : Generating music with long - term structure . In 7th International Conference on Learning Representations , ICLR 2019 , New Orleans , USA , May 6 - May 9 , 2019 , Conference Track Proceedings . OpenReview . net , 2019 . Daniel Jiwoong Im and Graham W Taylor . Scoring and classifying with gated auto - encoders . In Joint European Conference on Machine Learning and Knowledge Discovery in Databases , pages 533 – 545 . Springer , 2015 . Sergey Ioﬀe and Christian Szegedy . Batch normalization : Accelerating deep net - work training by reducing internal covariate shift . In Proceedings of the 32nd International Conference on Machine Learning ( ICML - 15 ) , pages 448 – 456 , 2015 . 118 Max Jaderberg , Karen Simonyan , Andrew Zisserman , and Koray Kavukcuoglu . Spatial transformer networks . In Corinna Cortes , Neil D . Lawrence , Daniel D . Lee , Masashi Sugiyama , and Roman Garnett , editors , Advances in Neu - ral Information Processing Systems 28 : Annual Conference on Neural In - formation Processing Systems 2015 , December 7 - 12 , 2015 , Montreal , Que - bec , Canada , pages 2017 – 2025 , 2015 . URL http : / / papers . nips . cc / paper / 5854 - spatial - transformer - networks . Lu Jiang , Deyu Meng , Qian Zhao , Shiguang Shan , and Alexander G . Hauptmann . Self - paced curriculum learning . In Blai Bonet and Sven Koenig , editors , Proceed - ings of the Twenty - Ninth AAAI Conference on Artiﬁcial Intelligence , January 25 - 30 , 2015 , Austin , Texas , USA . , pages 2694 – 2700 . AAAI Press , 2015 . URL http : / / www . aaai . org / ocs / index . php / AAAI / AAAI15 / paper / view / 9750 . Anna Jordanous . A standardised procedure for evaluating creative systems : Com - putational creativity evaluation based on what it is to be creative . Cogni - tive Computation , 4 ( 3 ) : 246 – 279 , 2012 . doi : 10 . 1007 / s12559 - 012 - 9156 - 1 . URL https : / / doi . org / 10 . 1007 / s12559 - 012 - 9156 - 1 . H Kirchmeyer . On the historical constitution of a rationalistic music , volume 8 , pages 11 – 24 . Die Reihe , 1968 . Carol L Krumhansl . Cognitive foundations of musical pitch , volume 17 of Oxford Psychology Series , pages 77 – 110 . Oxford University Press , New York , 1990 . Jonas Langhabel , Robert Lieck , Marc Toussaint , and Martin Rohrmeier . Feature discovery for sequential prediction of monophonic music . In Sally Jo Cunning - ham , Zhiyao Duan , Xiao Hu , and Douglas Turnbull , editors , Proceedings of the 18th International Society for Music Information Retrieval Conference , ISMIR 2017 , Suzhou , China , October 23 - 27 , 2017 , pages 649 – 656 , 2017 . URL https : / / ismir2017 . smcnus . org / wp - content / uploads / 2017 / 10 / 163 _ Paper . pdf . Stefan Lattner and Maarten Grachten . Improving content - invariance in gated au - toencoders for 2d and 3d object rotation . CoRR , abs / 1707 . 01357 , 2017a . URL http : / / arxiv . org / abs / 1707 . 01357 . Stefan Lattner and Maarten Grachten . Learning transformations of musical ma - terial using gated autoencoders . In Proceedings of the 2nd Conference on Computer Simulation of Musical Creativity , CSMC 2017 , Milton Keynes , UK , September 11 - 13 , 2017 , 2017b . URL https : / / drive . google . com / open ? id = 0B12H6lLCqzLWdVgwMFZON25UM3c . Stefan Lattner and Maarten Grachten . High - level control of drum track genera - tion using learned patterns of rhythmic interaction . In 2019 IEEE Workshop on 119 Applications of Signal Processing to Audio and Acoustics , WASPAA 2019 , New Paltz , NY , USA , October 20 - 23 , 2019 . IEEE , 2019 . Stefan Lattner , Carlos Eduardo Cancino Chacón , and Maarten Grachten . Pseudo - supervised training improves unsupervised melody segmentation . In Proceedings of the Twenty - Fourth International Joint Conference on Artiﬁcial Intelligence , IJCAI 2015 , Buenos Aires , Argentina , July 25 - 31 , 2015 , pages 2459 – 2465 , 2015a . URL http : / / ijcai . org / Abstract / 15 / 348 . Stefan Lattner , Maarten Grachten , Kat Agres , and Carlos Eduardo Cancino Chacón . Probabilistic segmentation of musical sequences using restricted Boltz - mann machines . In Proceedings of the 5th International Conference on Math - ematics and Computation in Music , MCM 2015 , London , UK , June 22 - 25 , 2015 , pages 323 – 334 , 2015b . doi : 10 . 1007 / 978 - 3 - 319 - 20603 - 5 _ 33 . URL https : / / doi . org / 10 . 1007 / 978 - 3 - 319 - 20603 - 5 _ 33 . Stefan Lattner , Maarten Grachten , and Gerhard Widmer . Learning transposition - invariant interval features from symbolic music and audio . In Proceedings of the 19th International Society for Music Information Retrieval Conference , ISMIR 2018 , Paris , France , September 23 - 27 , 2018a . URL http : / / ismir2018 . ircam . fr / doc / pdfs / 172 _ Paper . pdf . Stefan Lattner , Maarten Grachten , and Gerhard Widmer . A predictive model for music based on learned interval representations . In Proceedings of the 19th International Society for Music Information Retrieval Conference , ISMIR 2018 , Paris , France , September 23 - 27 , 2018b . URL http : / / ismir2018 . ircam . fr / doc / pdfs / 179 _ Paper . pdf . Stefan Lattner , Maarten Grachten , and Gerhard Widmer . Imposing higher - level structure in polyphonic music generation using convolutional restricted Boltz - mann machines and constraints . Journal of Creative Music Systems , 2 ( 2 ) , 2018c . URL https : / / www . jcms . org . uk / article / id / 522 / . Stefan Lattner , Monika Dörﬂer , and Andreas Arzt . Learning complex basis func - tions for invariant representations of audio . In Proceedings of the 20th Interna - tional Society for Music Information Retrieval Conference , ISMIR 2019 , Delft , The Netherlands , November 4 - 8 , 2019 . Yann LeCun , Bernhard E . Boser , John S . Denker , Donnie Henderson , Richard E . Howard , Wayne E . Hubbard , and Lawrence D . Jackel . Backpropagation applied to handwritten zip code recognition . Neural Computation , 1 ( 4 ) : 541 – 551 , 1989 . doi : 10 . 1162 / neco . 1989 . 1 . 4 . 541 . URL https : / / doi . org / 10 . 1162 / neco . 1989 . 1 . 4 . 541 . 120 Dong - Hyun Lee . Pseudo - Label : The Simple and Eﬃcient Semi - Supervised Learn - ing Method for Deep Neural Networks . In ICML Workshop Challenges in Rep - resentation Learning WREPL , pages 1 – 6 , Atlanta , Georgia , 2013 . Honglak Lee , Chaitanya Ekanadham , and Andrew Y . Ng . Sparse deep be - lief net model for visual area V2 . In John C . Platt , Daphne Koller , Yoram Singer , and Sam T . Roweis , editors , Proceedings of the Twenty - First Annual Conference on Neural Information Processing Systems , Van - couver , British Columbia , Canada , December 3 - 6 , 2007 , pages 873 – 880 . Curran Associates , Inc . , 2007 . URL http : / / papers . nips . cc / paper / 3313 - sparse - deep - belief - net - model - for - visual - area - v2 . Honglak Lee , Roger B . Grosse , Rajesh Ranganath , and Andrew Y . Ng . Convo - lutional deep belief networks for scalable unsupervised learning of hierarchical representations . In Andrea Pohoreckyj Danyluk , Léon Bottou , and Michael L . Littman , editors , Proceedings of the 26th Annual International Conference on Machine Learning , ICML 2009 , Montreal , Quebec , Canada , June 14 - 18 , 2009 , volume 382 of ACM International Conference Proceeding Series , pages 609 – 616 . ACM , 2009 . ISBN 978 - 1 - 60558 - 516 - 1 . doi : 10 . 1145 / 1553374 . 1553453 . URL http : / / doi . acm . org / 10 . 1145 / 1553374 . 1553453 . F . Lerdahl and R . A . Jackendoﬀ . A Generative Theory of Tonal Music . MIT Press , Cambridge , MA , 1983 . Roger Levy and T . Florian Jaeger . Speakers optimize information density through syntactic reduction . In Bernhard Schölkopf , John C . Platt , and Thomas Hof - mann , editors , Advances in Neural Information Processing Systems 19 , Proceed - ings of the Twentieth Annual Conference on Neural Information Processing Sys - tems , Vancouver , British Columbia , Canada , December 4 - 7 , 2006 , pages 849 – 856 . MIT Press , 2006 . ISBN 0 - 262 - 19568 - 2 . URL http : / / papers . nips . cc / paper / 3129 - speakers - optimize - information - density - through - syntactic - reduction . Feynman T . Liang , Mark Gotham , Matthew Johnson , and Jamie Shotton . Au - tomatic stylistic composition of bach chorales with deep LSTM . In Cunning - ham et al . [ 2017 ] , pages 449 – 456 . ISBN 978 - 981 - 11 - 5179 - 8 . URL https : / / ismir2017 . smcnus . org / wp - content / uploads / 2017 / 10 / 156 _ Paper . pdf . Corentin Louboutin and David Meredith . Using general - purpose compression al - gorithms for music analysis . Journal of New Music Research , 45 ( 1 ) : 1 – 16 , 2016 . doi : 10 . 1080 / 09298215 . 2015 . 1133656 . Qi Lyu , Zhiyong Wu , Jun Zhu , and Helen Meng . Modelling high - dimensional se - quences with LSTM - RTRBM : Application to polyphonic music generation . In 121 Qiang Yang and Michael Wooldridge , editors , Proceedings of the Twenty - Fourth International Joint Conference on Artiﬁcial Intelligence , IJCAI 2015 , Buenos Aires , Argentina , July 25 - 31 , 2015 , pages 4138 – 4139 . AAAI Press , 2015 . ISBN 978 - 1 - 57735 - 738 - 4 . URL http : / / ijcai . org / Abstract / 15 / 582 . Matija Marolt . A mid - level representation for melody - based retrieval in audio col - lections . IEEE Transactions on Multimedia , 10 ( 8 ) : 1617 – 1625 , 2008 . Andrew McCollough and Edward Vogel . Visual chunking allows eﬃcient allocation of memory capacity . Journal of Vision , 7 ( 9 ) : 861 , 2007 . doi : 10 . 1167 / 7 . 9 . 861 . Josh McDermott and Andrew Oxenham . Music perception , pitch , and the auditory system . Current Opinion in Neurobiology , 18 : 1 – 12 , 2008 . doi : 10 . 1016 / j . conb . 2008 . 09 . 005 . Roland Memisevic . Gradient - based learning of higher - order image features . In IEEE International Conference on Computer Vision ( ICCV ) , 2011 , pages 1591 – 1598 . IEEE , 2011 . Roland Memisevic . On multi - view feature learning . In John Langford and Joelle Pineau , editors , Proceedings of the 29th International Conference on Machine Learning ( ICML - 12 ) , ICML ’12 , pages 161 – 168 , New York , NY , USA , July 2012 . Omnipress . ISBN 978 - 1 - 4503 - 1285 - 1 . Roland Memisevic . Learning to relate images . IEEE transactions on pattern anal - ysis and machine intelligence , 35 ( 8 ) : 1829 – 1846 , 2013 . Roland Memisevic and Georgios Exarchakis . Learning invariant features by harness - ing the aperture problem . In Proceedings of the 30th International Conference on Machine Learning , ICML 2013 , Atlanta , GA , USA , 16 - 21 June 2013 , volume 28 of JMLR Workshop and Conference Proceedings , pages 100 – 108 . JMLR . org , 2013 . URL http : / / jmlr . org / proceedings / papers / v28 / memisevic13 . html . Roland Memisevic and Geoﬀrey Hinton . Unsupervised learning of image trans - formations . In IEEE Conference on Computer Vision and Pattern Recognition , 2007 . CVPR . , pages 1 – 8 . IEEE , 2007 . Roland Memisevic and Geoﬀrey E Hinton . Learning to represent spatial transfor - mations with factored higher - order Boltzmann machines . Neural Computation , 22 ( 6 ) : 1473 – 1492 , 2010 . David Meredith . COSIATEC and SIATECCompress : Pattern discovery by geo - metric compression . In International Society for Music Information Retrieval Conference , 2013 . 122 David Meredith , Kjell Lemström , and Geraint A Wiggins . Algorithms for discover - ing repeated patterns in multidimensional representations of polyphonic music . Journal of New Music Research , 31 ( 4 ) : 321 – 345 , 2002 . L . B . Meyer . Emotion and meaning in Music . University of Chicago Press , Chicago , 1956 . Vincent Michalski , Roland Memisevic , and Kishore Reddy Konda . Modeling deep temporal dependencies with recurrent " grammar cells " . In NIPS , pages 1925 – 1933 , 2014 . Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Corrado , and Jeﬀ Dean . Dis - tributed representations of words and phrases and their compositionality . In Advances in neural information processing systems , pages 3111 – 3119 , 2013 . Decebal Constantin Mocanu , Haitham Bou Ammar , Dietwig Lowet , Kurt Driessens , Antonio Liotta , Gerhard Weiss , and Karl Tuyls . Factored four way conditional restricted Boltzmann machines for activity recognition . Pattern Recognition Let - ters , 66 : 100 – 108 , 2015 . Michael C . Mozer . Neural network music composition by prediction : Exploring the beneﬁts of psychoacoustic constraints and multi - scale processing . Connect . Sci . , 6 ( 2 - 3 ) : 247 – 280 , 1994 . doi : 10 . 1080 / 09540099408915726 . URL https : / / doi . org / 10 . 1080 / 09540099408915726 . E . Narmour . The analysis and cognition of basic melodic structures : the Implication - Realization model . University of Chicago Press , 1990 . Eric Paul Nichols . Musicat : A Computer Model of Musical Listening and Analogy - Making . PhD thesis , faculty of the University Graduate School in partial fulﬁll - ment of the requirements for the degree Doctor of Philosophy in the Departments of Computer Science and Cognitive Science , Indiana University , 2012 . Oriol Nieto and Morwaread M Farbood . Identifying polyphonic patterns from audio recordings using music segmentation techniques . In Proc . of the 15th Interna - tional Society for Music Information Retrieval Conference , pages 411 – 416 , 2014 . Anders Nøklestad . A Machine Learning Approach to Anaphora Resolution Includ - ing Named Entity Recognition , PP Attachment Disambiguation , and Animacy Detection . PhD thesis , University of Oslo , 2009 . Bruno A Olshausen , Charles Cadieu , Jack Culpepper , and David K Warland . Bilin - ear models of natural images . In Electronic Imaging 2007 , pages 649206 – 649206 . International Society for Optics and Photonics , 2007 . 123 François Pachet and Pierre Roy . Markov constraints : steerable generation of Markov sequences . Constraints , 16 ( 2 ) : 148 – 172 , 2011 . doi : 10 . 1007 / s10601 - 010 - 9101 - 4 . URL https : / / doi . org / 10 . 1007 / s10601 - 010 - 9101 - 4 . François Pachet , Sony CSL Paris , Alexandre Papadopoulos , and Pierre Roy . Sam - pling variations of sequences for structured music generation . In Proceedings of the 18th International Society for Music Information Retrieval Conference , pages 167 – 173 , 2017 . C Palmer and Carol L Krumhansl . Mental representations for musical meter . Jour - nal of Experimental Psychology : Human Perception and Performance , 16 ( 4 ) : 728 – 741 , 1990 . Christine Payne . MuseNet , Website accessed July 19th , 2019 . https : / / openai . com / blog / musenet . M . T . Pearce . The Construction and Evaluation of Statistical Models of Melodic Structure in Music Perception and Composition . PhD thesis , Department of Computing , City University , London , UK . , 2005 . Marcus Pearce and Geraint Wiggins . Towards a framework for the evaluation of machine compositions . In Proceedings of the Symposium on Artiﬁcial Intelligence and Creativity in the Arts and Sciences , AISB’01 , York , UK , March 21 - 24 , 2001 , pages 22 – 32 . AISB Press , 2001 . Marcus Pearce and Geraint Wiggins . Improved methods for statistical modelling of monophonic music . Journal of New Music Research , 33 ( 4 ) : 367 – 385 , 2004 . Marcus Pearce , Darrell Conklin , and Geraint Wiggins . Methods for combining statistical models of music . In International Symposium on Computer Music Modeling and Retrieval , pages 295 – 312 . Springer , 2004 . Marcus Pearce , Daniel Müllensiefen , and Geraint A Wiggins . The role of ex - pectation and probabilistic learning in auditory boundary perception : a model comparison . Perception , 39 ( 10 ) : 1365 – 1389 , 2010a . doi : 10 . 1068 / p6507 . URL https : / / doi . org / 10 . 1068 / p6507 . Marcus Pearce , Daniel Müllensiefen , and Geraint A Wiggins . Melodic Grouping in Music Information Retrieval : New Methods and Applications . In Advances in Music Information Retrieval , pages 364 – 388 . Springer Berlin Heidelberg , Berlin , Heidelberg , January 2010b . Jeﬀrey Pennington , Richard Socher , and Christopher D Manning . Glove : Global vectors for word representation . In EMNLP , volume 14 , pages 1532 – 1543 , 2014 . 124 Scott E Reed , Yi Zhang , Yuting Zhang , and Honglak Lee . Deep visual analogy - making . In Advances in Neural Information Processing Systems , pages 1252 – 1260 , 2015 . Laura Rimell , Amandla Mabona , Luana Bulat , and Douwe Kiela . Learning to negate adjectives with bilinear models . EACL 2017 , page 71 , 2017 . Adam Roberts , Jesse Engel , and Douglas Eck . Hierarchical variational autoencoders for music . In NIPS Workshop on Machine Learning for Creativity and Design , Long Beach , California , USA , December 8 , 2017 , 2017 . Helmut Schaﬀrath . The Essen Folksong Collection in Kern Format . In David Huron , editor , Database containing , folksong transcriptions in the Kern format and a - page research guide computer database . Menlo Park , CA , 1995 . Jan Schlueter and Christian Osendorfer . Music similarity estimation with the mean - covariance restricted Boltzmann machine . In 10th International Conference on Machine Learning and Applications and Workshops ( ICMLA ) , 2011 , volume 2 , pages 118 – 123 . IEEE , 2011 . Claude Elwood Shannon . A mathematical theory of communication . Bell System Technical Journal , 27 : 379 – 423 , 623 – 656 , July 1948 . URL http : / / math . harvard . edu / ~ ctm / home / text / others / shannon / entropy / entropy . pdf . Karen Simonyan , Andrea Vedaldi , and Andrew Zisserman . Deep inside convolu - tional networks : Visualising image classiﬁcation models and saliency maps . In ICLR ( Workshop Poster ) , 2014 . Nicholas A Smith and Mark A Schmuckler . The perception of tonal structure through the diﬀerentiation and organization of pitches . Journal of Experimental Psychology : Human Perception and Performance , 30 ( 2 ) : 268 – 286 , 2004 . Nathan Srebro and Adi Shraibman . Rank , trace - norm and max - norm . In Pe - ter Auer and Ron Meir , editors , Proceedings of the 18th Annual Conference on Learning Theory , COLT 2005 , Bertinoro , Italy , June 27 - 30 , 2005 , , volume 3559 of Lecture Notes in Computer Science , pages 545 – 560 . Springer , 2005 . doi : 10 . 1007 / 11503415 _ 37 . URL https : / / doi . org / 10 . 1007 / 11503415 _ 37 . Nitish Srivastava , Geoﬀrey E Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdinov . Dropout : a simple way to prevent neural networks from overﬁt - ting . Journal of Machine Learning Research , 15 ( 1 ) : 1929 – 1958 , 2014 . Bob Sturm , João Felipe Santos , Oded Ben - Tal , and Iryna Korshunova . Music transcription modelling and composition using deep learning . In Proceedings of 125 the 1st Conference on Computer Simulation of Musical Creativity , 2016 . URL https : / / goo . gl / uEzGeQ . Joshua Susskind , Geoﬀrey Hinton , Roland Memisevic , and Marc Pollefeys . Model - ing the joint density of two images under a variety of transformations . In IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 2793 – 2800 . IEEE , 2011 . Ilya Sutskever , Geoﬀrey E . Hinton , and Graham W . Taylor . The recur - rent temporal restricted Boltzmann machine . In Daphne Koller , Dale Schuurmans , Yoshua Bengio , and Léon Bottou , editors , Advances in Neu - ral Information Processing Systems 21 , Proceedings of the Twenty - Second Annual Conference on Neural Information Processing Systems , Vancou - ver , British Columbia , Canada , December 8 - 11 , 2008 , pages 1601 – 1608 . Curran Associates , Inc . , 2008 . URL http : / / papers . nips . cc / paper / 3567 - the - recurrent - temporal - restricted - boltzmann - machine . Graham W . Taylor , Geoﬀrey E . Hinton , and Sam T . Roweis . Modeling human motion using binary latent variables . In Bernhard Schölkopf , John C . Platt , and Thomas Hofmann , editors , Advances in Neural Information Processing Sys - tems 19 , Proceedings of the Twentieth Annual Conference on Neural Informa - tion Processing Systems , Vancouver , British Columbia , Canada , December 4 - 7 , 2006 , pages 1345 – 1352 . MIT Press , 2006 . URL http : / / papers . nips . cc / paper / 3078 - modeling - human - motion - using - binary - latent - variables . David Temperley . The Cognition of Basic Musical Structures . MIT Press , Cam - bridge , MA , 2001 . David Temperley . Information ﬂow and repetition in music . Journal of Music Theory , 58 ( 2 ) : 155 – 178 , 2014 . doi : 10 . 1215 / 00222909 - 2781759 . Joshua B Tenenbaum and William T Freeman . Separating style and content with bilinear models . Neural Computation , 12 ( 6 ) : 1247 – 1283 , 2000 . James Tenney and Larry Polansky . Temporal Gestalt Perception in Music . Journal of Music Theory , 24 ( 2 ) : 205 – 241 , 1980 . Theano Development Team . Theano : A python framework for fast computation of mathematical expressions . CoRR , abs / 1605 . 02688 , 2016 . URL http : / / arxiv . org / abs / 1605 . 02688 . T . Tieleman and G . E . Hinton . Using Fast Weights to Improve Persistent Con - trastive Divergence . In Proceedings of the 26th international conference on Ma - chine learning , pages 1033 – 1040 . ACM New York , NY , USA , 2009 . 126 Tijmen Tieleman . Training restricted Boltzmann machines using approximations to the likelihood gradient . In William W . Cohen , Andrew McCallum , and Sam T . Roweis , editors , Machine Learning , Proceedings of the Twenty - Fifth International Conference ( ICML 2008 ) , Helsinki , Finland , June 5 - 9 , 2008 , volume 307 of ACM International Conference Proceeding Series , pages 1064 – 1071 . ACM , 2008 . doi : 10 . 1145 / 1390156 . 1390290 . URL http : / / doi . acm . org / 10 . 1145 / 1390156 . 1390290 . P M Todd . A connectionist approach to algorithmic composition . Computer Music Journal , 13 ( 4 ) : 27 , 1989 . doi : 10 . 2307 / 3679551 . Edgard Varèse and Chou Wen - Chung . The liberation of sound . Perspectives of new music , 5 ( 1 ) : 11 – 19 , 1966 . Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N . Gomez , Lukasz Kaiser , and Illia Polosukhin . Attention is all you need . In Isabelle Guyon , Ulrike von Luxburg , Samy Bengio , Hanna M . Wal - lach , Rob Fergus , S . V . N . Vishwanathan , and Roman Garnett , editors , Ad - vances in Neural Information Processing Systems 30 : Annual Conference on Neural Information Processing Systems 2017 , 4 - 9 December 2017 , Long Beach , CA , USA , pages 6000 – 6010 , 2017 . URL http : / / papers . nips . cc / paper / 7181 - attention - is - all - you - need . Pascal Vincent , Hugo Larochelle , Isabelle Lajoie , Yoshua Bengio , and Pierre - Antoine Manzagol . Stacked denoising autoencoders : Learning useful represen - tations in a deep network with a local denoising criterion . Journal of Machine Learning Research , 11 ( Dec ) : 3371 – 3408 , 2010 . Christoph Von Der Malsburg . The correlation theory of brain function . In Models of neural networks , pages 95 – 119 . Springer , 1994 . Hermann von Helmholtz . Treatise on physiological optics , volume 3 . Courier Cor - poration , 2005 . Thomas C Walters , David A Ross , and Richard F Lyon . The intervalgram : an audio feature for large - scale melody recognition . In Proc . of the 9th International Symposium on Computer Music Modeling and Retrieval ( CMMR ) . Citeseer , 2012 . Cheng - i Wang and Shlomo Dubnov . Pattern discovery from audio recordings by variable Markov oracle : A music information dynamics approach . In Proceedings of the IEEE International Conference on Acoustics , Speech and Signal Processing , ICASSP 2015 , South Brisbane , Queensland , Australia , April 19 - 24 , 2015 , pages 683 – 687 . IEEE , 2015 . doi : 10 . 1109 / ICASSP . 2015 . 7178056 . URL https : / / doi . org / 10 . 1109 / ICASSP . 2015 . 7178056 . 127 Cheng - i Wang , Jennifer Hsu , and Shlomo Dubnov . Music pattern discovery with variable markov oracle : A uniﬁed approach to symbolic and audio representa - tions . In Meinard Müller and Frans Wiering , editors , Proceedings of the 16th International Society for Music Information Retrieval Conference , ISMIR 2015 , Málaga , Spain , October 26 - 30 , 2015 , pages 176 – 182 , 2015 . ISBN 978 - 84 - 606 - 8853 - 2 . URL http : / / ismir2015 . uma . es / articles / 78 _ Paper . pdf . Max Wertheimer . Laws of organization in perceptual forms . A source book of Gestalt psychology , pages 71 – 88 , 1938 . Raymond Whorley and Darrell Conklin . A transformational method for chorale gen - eration . In Proceedings of the 9th International Workshop on Machine Learning and Music , MML 2016 , Riva del Garda , Italy , September 23 , 2016 , pages 71 – 75 , 2016 . URL http : / / www . ehu . eus / cs - ikerbasque / conklin / papers / rw16mml . pdf . Gerhard Widmer . Discovering simple rules in complex data : A meta - learning al - gorithm and some surprising musical discoveries . Artiﬁcial Intelligence , 146 ( 2 ) : 129 – 148 , 2003 . doi : 10 . 1016 / S0004 - 3702 ( 03 ) 00016 - X . URL https : / / doi . org / 10 . 1016 / S0004 - 3702 ( 03 ) 00016 - X . 128 Curriculum Vitae of the Author Personal data Name : Stefan Lattner Date of birth : February 22nd , 1984 Place of birth : Kirchdorf a . d . Krems , Austria Email : me @ stefanlattner . at Website : www . stefanlattner . at Education 2014 – 2019 PhD in Computer Science , Johannes - Kepler University Linz . Thesis : “Modeling Musical Structure with Artiﬁcial Neural Networks’ 2010 – 2014 Master of Science in Pervasive Computing , Johannes - Kepler University Linz , Austria . Thesis : “Hierarchical Temporal Memory - Investigations , Ideas , and Experiments” 2013 Study abroad at the University of Tasmania , Hobart , Australia 2006 – 2009 Bachelor of Science in Media Technology and - Design , University of Applied Sciences , Hagenberg Campus , Austria . Thesis : “Konzepte al - gorithmischer Komposition : Ein vergleichender Überblick” 2005 – 2006 Bioinformatics , University of Applied Sciences , Hagenberg Campus , Austria 129 Experience 2018 – present Associate Researcher at the Sony Computer Science Laboratories , Paris , France 2017 – 2018 Research Assistant at the Johannes - Kepler University , Linz , Austria 2016 – 2017 Lectures in Generative Music ( Markov Models and Neural Networks ) , Technical University , Vienna 2014 – 2017 Research Assistant at the Austrian Research Institute for Artiﬁcial Intel - ligence ( OFAI ) , Project Lrn2Cre8 , European Union Seventh Framework Programme 2009 – 2014 Chief Developer and Project Manager for the Music AI Application Liq - uid Notes at Re - Compose GmbH , Vienna , Austria 2008 – 2009 Tutor in Algorithms and Data Structures , University of Applied Sci - ences , Hagenberg Campus , Austria Scientiﬁc Services Reviewer Society for Music Information Retrieval Conference ( 2015 – 2019 ) Special Issue on Deep Learning for Music and Audio in Springer’s Neural Computing and Applications ( 2018 ) Association for the Advancement of Artiﬁcial Intelligence ( 2017 )