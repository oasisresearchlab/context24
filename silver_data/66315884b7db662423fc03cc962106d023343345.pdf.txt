Artificial ELSEVIER Artificial Intelligence 91 ( 1997 ) 177 - 181 Intelligence Scientific discovery and simplicity of method Editorial Herbert A . Simon ‘l * , Ra61 E . ValdCs - PCrez a , * , Derek H . Sleeman b 1 Computer Science Department , Carnegie Mellon University , Pittsburgh , PA 15213 . USA h Computing Science Department , King’s College , University of Aberdeen , Old Aberdeen AB9 2UE , Scotland , United Kingdom 1 . Goals of research on scientific discovery The purpose of this special issue of Artificial Intelligence is to report some recent research on computer systems that can discover scientific laws or concepts . For several decades , the processes of discovering new scientific laws and concepts have been a topic of AI research , directed at either or both of two goals : namely , understanding human scientific reasoning , and secondly developing systems which au - tonomously , or collaboratively with the expert , contribute to scientific knowledge . The two goals can be pursued independently , but in fact , they have been intermingled ; ideas derived from research on human scientific discovery have been applied in systems that are not committed to humanoid methods ; and the design of discovery systems have contributed ideas about the processes that might be used by humans when performing these tasks . There has been extensive mutually beneficial interaction between these two activities , A great variety of processes are used in scientific discovery . Research problems must be identified and representations that describe them and their potential solutions must be formulated . Data must be gathered by making observations and carrying out exper - iments . Instruments must be developed for carrying out observations and experiments . Regularities and patterns , that is to say , generalizations ( laws ) and concepts , must be sought to describe the data parsimoniously ; once the initial laws have been formed , the observations are often then explained at a more detailed level where each of the substages are sometimes given a causal explanation . Laws and concepts must be gener - alized for use in creating more comprehensive theories . Inferences must be drawn from theories and empirical predictions made that can be tested by new observations and experiments . * Corresponding author . E - mail : vaIdes @ cs . cmu . edu . 0004 - 3702 / 97 / $ 17 . 00 @ 1997 Published by Elsevier Science B . V . All rights reserved . PIISOOO4 - 3702 ( 97 ) 00019 - 2 178 H . A . Simon et al . / Artijicial Intelligence 91 ( 1997 ) 177 - 181 A substantial period of the professional life of a single scientist or laboratory may be devoted to one or a few of these activities ; alternatively , the scientist or laboratory may range over the complete range of activities . In any event , a single scientific report , recorded on the public blackboard known as “publication” , generally covers only a rather circumscribed aspect . Each published report provides potential inputs to one or more of the other activities , so that the blackboard becomes a principal instrument for cumulation and coordination of knowledge . A human scientist or a computer system programmed to engage in discovery activities may be viewed as a multi - stage process that uses information on the blackboard plus the empirical data it gathers as its inputs , and subsequently contributes new information to the same blackboard . Additionally , humans and computers may engage in the meta - scientific activity of building general theories of the scientific discovery process itself - either in the abstract , or as a basis for new discovery systems that can contribute to particular processes and specific sciences . The decentralized organization of science mentioned above , coordinated by the “black - board” of publication , is tolerant of simplicity in the individual components of the sys - tem . Of course , the price of this very loose form of coordination of relatively simple components is that the component systems must be capable of delivering their outputs in representations that are decodable by the other components . Research on data - mining methods has burgeoned in the past few years given the availability of large data bases , and their accessibility on the World Wade Web . However , this subfield is just beginning to address the questions of mutual accessibility and interpretation in a sophisticated way . Hence , such research may be expected subsequently to play an important role in the general advance in the application of computers to scientific discovery , and to be an important area of research in scientific discovery itself . 2 . Brief overview of recent and past work Work on the modelling of scientific discovery began very early in the history of artificial intelligence . Hunt and Hovland [ l ] built a system that induced simple concepts from examples presented to it ; Simon and Kotovsky [ 5 ] showed how patterns could be discovered in sequences , and the sequences could thereby be extrapolated - both programs exhibiting simple forms of law discovery . The DENDRAL program [ 41 of the late 1960s and 1970s used mass spectrogram data to discover the chemical formulae of complex molecules , while META - DENDRAL induced new constraints for incorporating into the DENDRAL search program . Lenat [ 3 ] built the AM system , which , given basic knowledge about a domain , was able to construct “interesting” new concepts in that domain , and this was followed by the EURISKO system that had capabilities for extending its repertory of discovery heuristics . Langley , Simon , Zytkow and Bradshaw [ 2 ] devised a series of programs which could derive scientific laws from data ( BACON , GLAUBER , STAHL , DALTON , and others ) , which in turn suggested a theory of human scientific discovery . As noted earlier , efforts at modelling discovery processes have sometimes been aimed at developing a theory of human discovery , sometimes at constructing systems that can , H . A . Simon et al . / Art $ cial Intelligence 91 ( 1997 ) 177 - IRI 179 in collaboration with scientists or autonomously , engage in discovery work . Examples of autonomous or collaborative systems are MECHEM [ 7 ] , which undertakes to elucidate the pathways of chemical reactions , and TETRAD [ 6 ] , which produces causal explana - tions that are consistent with the correlation matrices of sets of variables . Discovery tools have also been developed in various areas of molecular biology , such as the GRAIL pro - gram [ IO ] for detecting gene subsequences in anonymous DNA sequences , and various programs for inferring phylogenetic trees . Detailed reviews of recent scientific discovery research can be found in [ 111 and [ 81 . The current special issue provides a sample of recent research . The impetus for this issue came from an AAAI Spring Symposium held at Stanford University in the Spring of 1995 , although not all the work published here was presented there . Interestingly , four fifths of the articles of this special issue deal with biology or medicine , which departs somewhat from earlier published work that concentrated on chemistry , physics , and mathematics . 3 . The virtues of simplicity The discovery programs , both the programs already reported in the literature and those newly reported in this issue , are , for the most part , rather modest in size and simple in structure , as computer programs go . Because tasks as ambitious and as ill - structured as scientific discovery might be thought to call for very complex methods , we would like to offer a few comments on why large and complex programs have been the exception rather than the rule . A first reason for this simplicity has been mentioned earlier . Science is carried out in a very decentralized fashion , the “blackboard” of publication serving as a principal coordinating mechanism . Thus , generally any component of the system of science , human or machine , carries out only a relatively circumscribed range of activities . Its greatest complexities may reside in its capabilities for accepting and interpreting outputs from the blackboard , and for producing interpretable inputs to the blackboard . Thus , the BACON system requires as input , sets of values for the dependent and independent variables , and provides as output a law that describes the relation between the dependent and independent variables in the form of an algebraic equation . The input to the MECHEM program [ 7 ] is a set of starting substances and a partial set of the product substances observed in a chemical reaction described in standard chemical notation ; it also can accept a large variety of constraints that reflect a user’s prior understanding [ 9 ] . MECHEM outputs hypotheses about the reaction pathway in the form of standard descriptions of a set of component reactions . Given the ability to use the available inputs and to produce intelligible outputs , not only do discovery programs tend to be simple , but they tend to produce rather simple results . This is partly a consequence of the way in which they generate solutions to the problems posed ; the generators embedded in the programs generally start with the simplest hypotheses , then produce , if necessary , more and more complex ones that are created combinatorially , under the guidance of constraints and other selective heuristics , from the previous ones . 180 H . A . Simon et al . / Artifcial Intelligence 91 ( 1997 ) 177 - 181 In any event , complexity of programs or of their outputs is not a measure of their “intelligence” . Given very complex tasks , complex algorithms may be a necessity , but they are clearly not a virtue . A central lesson of artificial intelligence , and of computing in general , is that if a task domain has strong structure and if sufficient domain informa - tion can be obtained , either a priori or in the course of computation , then rather simple programs may suffice . ( The simplex method for solving linear programming problems is a classical example of simplicity achieved by efficient exploitation of a strong problem structure . ) In science itself , a major goal is to describe nature by means of laws that are as parsimonious as possible . The same is true of a science of discovery , whose goal is not to awe with the complexity of the means employed , but to produce efficient and parsimonious procedures for achieving discoveries . Complex systems may sometimes be necessary ; however , simplicity , when attainable , is always to be preferred . It is therefore gratifying that , during the initial decades of research on scientific discovery , it has been possible to construct a number of relatively simple programs , such as the examples mentioned above , which have powerful capabilities ; similar simplicity is evident in the programs reported in this issue . 4 . Emerging issues At this point in time , there is sufficient understanding of scientific discovery that efforts should be directed toward developing significant discovery tools in a variety of sciences . These efforts should not proceed in a vacuum , but instead should build on the substantial number of discovery systems that exist , as well as on fruitful analogies that can be drawn among discovery tasks from disparate scientific fields . Discovery systems which solve tasks cooperatively with a domain expert are likely to have an important role , because in any nontrivial domain , it will be virtually impossible to provide the system with a complete theory which is anyway constantly evolving . Finally , as the domains tackled become more complex , we shall need to pay more attention to how the system and the domain expert communicate , and to how the user’s relevant prior knowledge can be identified and built upon . Acknowledgements We thank the advisory editors for their substantial input to this special issue : Bruce Buchanan , Lindley Darden , Gerd Grasshoff , Pat Langley , and Jan Zytkow . References [ I J C . Hovland and E . Hunt , The computer simulation of concept attainment , Behavioral Science 5 ( 1960 ) 265 - 267 . [ 2 ] P . Langley , H . A . Simon , G . Bradshaw and J . Zytkow , Scientijk Discovery : Computational Explorations of the Creative Processes ( MIT Press , Cambridge , MA , 1987 ) . H . A . Simon et al . / Art $ cial Intelligence 91 ( 1997 ) 177 - 181 181 131 D . B . Lenat , AM : discovery in mathematics as heuristic search , in : R . Davis and D . B . Lenat , eds . , Knowledge - Based Systems in Art $ cial Intelligence ( McGraw Hill , New York , 1982 ) . [ 41 R . Lindsay , B . Buchanan , E . Feigenbaum and J . Lederbetg , DENDRAL : a case study of the first expert system for scientific hypothesis formation , Artificial intelligence 61 ( 1993 ) 209 - 261 , 151 H . A . Simon and K . Kotovsky , Human acquisition of concepts for sequential patterns , Psychological Review 70 ( 1963 ) 534 - 546 . [ 6 1 P Spirtes , C . Glymour and R . Scheines . Causation , Prediction and Search , Lecture Notes in Statistics ( Springer , New York , 1993 ) . [ 71 R . E . Valdes - Perez , Machine discovery in chemistry : new results , Artificial Intelligence 74 ( 1995 ) 191 - 201 . 18 ) R . E . Valdes - Perez , Computer science research on scientific discovery , Knowledge Engineering Review 11 ( 1996 ) 57 - 66 . [ 9 ] R . E . Valdts - Perez and A . V . Zeigamik , Interactive elucidation ( without programming ) of reaction mechanisms in heterogeneous catalysis , L Molecular Catalysis ( to appear ) . [ lo ] Y . Xu , R . Mural , . I . Einstein , M . Shah and E . Uberbacher , GRAIL : a multi - agent neural network system for gene identification , Proc . IEEE 84 ( 1996 ) 1544 . 1 11 1 J . Zytkow , ed . , Machine Discovery ( Kluwer , Dordrecht , 1996 ) .