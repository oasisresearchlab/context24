Scaling Creative Inspiration with Fine - Grained Functional Facets of Ideas Tom Hope tomh @ allenai . org Allen Institute for AI The University of Washington Ronen Tamari Hebrew University of Jerusalem Hyeonsu Kang Carnegie Mellon University Daniel Hershcovich University of Copenhagen , Denmark Joel Chan University of Maryland Aniket Kittur Carnegie Mellon University Dafna Shahaf dshahaf @ cs . huji . ac . il Hebrew University of Jerusalem ABSTRACT Large repositories of products , patents and scientific papers offer an opportunity for building systems that scour millions of ideas and help users discover inspirations . However , idea descriptions are typically in the form of unstructured text , lacking key structure that is required for supporting creative innovation interactions . Prior work has ex - plored idea representations that were limited in expressivity , required significant manual effort from users , or dependent on curated knowl - edge bases with poor coverage . We explore a novel representation that automatically breaks up products into fine - grained functional facets capturing the purposes and mechanisms of ideas , and use it to support important creative innovation interactions : functional search for ideas , and exploration of the design space around a focal problem by viewing related problem perspectives pooled from across many products . In user studies , our approach boosts the quality of creative search and inspirations , outperforming strong baselines by 50 - 60 % . 1 INTRODUCTION Human creativity often relies on detecting structural matches across distant ideas and adapting them by transferring mechanisms from one domain to another [ 16 , 17 , 29 , 30 ] . For example , microwave ovens were discovered by repurposing radar technology developed during World War II . Teflon , today chiefly used in non - stick cook - ware , was first used in armament development . Recognizing the potential of this innovation process , major organizations such as NASA and Procter & Gamble actively engage in searching for op - portunities to adapt existing technologies for new markets [ 22 ] . Online repositories of products , scientific papers , and patents present a significant opportunity to augment and scale this core pro - cess of innovation . The large scale and diversity of these repositories is important , because inspiration can be found in unexpected places – for example , a car mechanic recently invented a simple device to ease childbirths by adapting a trick for extracting a cork stuck in a wine bottle , which he discovered in a YouTube video [ 1 ] . However , the predominant way human problem - solvers currently interact with these repositories — via standard search engines — does not tap into their potential for augmenting and scaling human ingenuity . Core to this limitation is the representation of ideas in the form of unstructured textual descriptions . This representation hinders creative innovation interactions that require traversing multiple levels of granularity and abstraction around a focal problem , to “break out” of fixation on the details of a specific problem by exploring the design space and viewing novel perspectives on problems and solutions [ 15 , 20 , 43 , 47 ] . Toward addressing this challenge , our vision in this paper is to develop a novel representation of ideas that can support exploration and abstraction of fine - grained functional facets in large - scale idea repositories – facets such as the purposes and mechanisms of products . More specifically , our goal is to obtain a representation having two key capabilities : ( I ) The representation would be able to automatically disentangle raw descriptions into fine - grained func - tional “units” that support search and discovery of products that match on certain functions but not others . ( II ) This representation should also allow navigating the landscape of ideas at different resolutions — enabling users to “zoom” in and out at desired levels of abstraction of a given problem and connect to inspirations in seemingly distant areas . As an example , consider an inventor looking for a way to wash clothes without water ( e . g . , in space , or where water is scarce ) . Fig - ure 1 illustrates our vision . Breaking down product descriptions into fine - grained functions ( capability I above ) could allow an automated system to find ideas that match on certain purpose facets ( washing clothes ) but not certain mechanism facets ( usage of water ) . This could lead to solutions like cleaning mechanisms based on dry ice or chemical coating . Zooming out and abstracting the problem to a more general framing ( capability II ) might lead to broader ideas for the problem of cleaning such as techniques for removing dirt or odor – each resulting in novel problem perspectives and inspirations . In Figure 1 , each node represents a cluster of documents with a similar purpose and the user can explore neighboring clusters to find inspi - rations ( e . g . , dry shampoo ) . This can also expand the innovator’s conception of the problem space itself , such as the assumption that clothes should be cleaned and reworn ( vs . biodegradable ) . In this paper , we develop a scalable computational model of ideas that brings us closer to this vision . We train a neural network to extract spans of text describing purposes and mechanisms in prod - uct texts , and use them to build a span embedding representation that allows faceted matching between ideas . We then use this rep - resentation of individual ideas to automatically mine connections between problems and solutions across entire repositories and build a “functional network” that resembles functional ontologies used a r X i v : 2102 . 09761v2 [ c s . H C ] 10 S e p 2021 Hope et al . Purpose : wash clothes Mechanism : NOT water The garment deodorizer removes odor from clothing in only 40 minutes . Waterless washing machine scrubs clothes clean with dry ice . Dry shampoo uses starch - based active ingredients to soak up the oils and sweat from your hair , making it appear cleaner . Fragrance makes your hair smell fresh . A chemical coating that causes cotton materials to clean themselves of stains and remove odors when exposed to sunlight . Biodegradable yarn from cellulose fiber from citrus fruit peels . Can be spun into high - quality fabric that feels like silk . clean , remove dirt wash clothes , clean clothes remove bad odor get rid of dirty objects Figure 1 : Extracting fine - grained purposes and mechanisms at scale enables mapping the landscape of ideas . Suppose an inventor is looking for a way to wash clothes without water . Each node in the graph represents a cluster of documents with a similar purpose . Edges reflect abstract structural similarity , capturing partial purpose matches between documents across clusters with a rule - mining approach ( see Section 4 . 1 ) . Purpose / mechanism spans in documents are shown in pink / green , respectively . One can find direct matches to the query ( e . g . , waterless washing machine using dry ice ) or explore neighboring clusters , reformulating the problem as removing odor , removing dirt , or getting rid of the dirty clothes . in engineering and design ideation [ 31 , 38 ] , which are typically hand - crafted and limited in scale . Our approach could facilitate many downstream applications in creative innovation arising from its ability to decompose idea texts into functional facets , and to surface related problem perspectives at multiple levels of abstraction — two fundamental drivers of creativ - ity support . In this paper we instantiate the approach in two prototype systems , probing its value regarding each of these capabilities : • Functional faceted search for alternative uses . One important use of our representation is to enhance the expressivity of search engines over idea repositories . This way , our representation could support expressive search for alternative , atypical uses of products to identify potentially high - value adaptation opportunities . • Exploring alternative levels of problem perspectives . Recent work [ 32 ] showed that problem - solvers are often interested in exploring different reformulations of the problem when searching for inspiration . Our fine - grained span representations facilitate mining of recurring functional relations , such as purposes that are often mentioned together or mechanisms associated with purposes . This level of detail enables us to map the landscape of ideas with a network of purposes and mechanisms , allowing us to automati - cally traverse neighboring problems and solutions around a focal problem and surface novel inspirations to users . Previous work on creativity techniques and prototype systems highlighted the importance of functional representations [ 12 , 32 , 39 , 43 , 47 , 74 ] , but these methods require significant manual effort from the user , rely on resources with limited coverage , or have limited expressivity . For example , the WordTree method [ 47 ] – a prominent design method in creative engineering design – directs designers to break their problem into subfunctions , and then use the WordNet database [ 56 ] to explore abstractions and related functional facets . Likewise , a recent study [ 32 ] asked designers to select product as - pects to abstract using WordNet and Cyc [ 45 ] . These and other resources ( e . g . , NELL [ 57 ] and DBpedia [ 24 ] ) largely encode cate - gorical knowledge ( e . g . , is - a , has - a ) and rarely functional knowledge ( e . g . , used - for ) , and often suffer from poor coverage of concepts in real - world products [ 32 ] . Knowledge bases and ontologies that do fo - cus on functions , behaviors , and structures [ 6 , 38 , 71 ] have primarily been hand - crafted and are therefore even more limited in coverage . Work attempting to scale up has shown promise but is limited in expressivity or interpretability , such as modeling patents in terms of verbs and nouns [ 26 ] , using principal component analysis [ 21 ] , or learning coarse aggregate vectors that capture only one overall product purpose and mechanism [ 39 ] that cannot disentangle the different facets of a product . Separately , recent work on information extraction identified entities such as methods and tasks ( roughly cor - responding to mechanisms and purposes ) in computer science papers [ 48 ] ; this work focused on the technical aspects of extraction , not on developing a representation of ideas and exploring the interactions such a representation can facilitate as in this paper . We seek to advance the state of the art by developing a novel representation that is both expressive and scalable , and exploring the applications it unlocks . We believe our representation may serve as a useful building block for novel creativity support tools that can help users find and recombine the inspirations latent in unstructured idea repositories at a scale previously impossible . In summary , in this work we contribute : • A novel computational representation of ideas with fine - grained functional facets for purposes and mechanisms . Scaling Creative Inspiration with Fine - Grained Functional Facets of Ideas Figure 2 : Crowdsourcing interface for fine - grained purposes and mechanisms . • Empirical demonstrations of the flexibility and utility of the rep - resentation for computational support of core creative tasks : ( 1 ) searching for alternative , atypical product uses for potential adap - tation opportunities ; and ( 2 ) creating a functional concept graph that enables innovators to explore the design space around a fo - cal problem . Through two empirical user studies we demonstrate that our representation significantly outperforms both previous work and state - of - the art embedding baselines on these tasks . We achieve Mean Average Precision ( MAP ) of 87 % in the alternative product uses search , and 62 % of our inspirations for design space exploration are found to be useful and novel – a relative boost of 50 - 60 % over the best - performing baselines . 2 LEARNING A FINE - GRAINED FUNCTIONAL REPRESENTATION Our goal in this section is to construct a representation that can support the creative innovation tasks and interactions discussed in the Introduction . We propose to use span representations [ 44 ] . Given a product text description , we extract tagged spans of text corresponding to purposes and mechanisms ( see Figure 2 ) , and represent the product as a set of span embeddings . More technically , we use a standard sequence tagging formulation , with X 𝑁 = { x 1 , x 2 , . . . , x 𝑁 } a training set of 𝑁 texts , each a sequence of tokens x 𝑖 = ( 𝑥 1 𝑖 , 𝑥 2 𝑖 , . . . , 𝑥 𝑇𝑖 ) , and Y 𝑁 a corresponding set of label sequences , Y 𝑁 = { y 1 , y 2 , . . . , y 𝑁 } , y 𝑖 = { 𝑦 1 𝑖 , 𝑦 2 𝑖 , . . . , 𝑦 𝑇𝑖 } , where each 𝑦 𝑗 indicates token 𝑗 ’s label ( purpose / mechanism / other ) . In later sections , we represent each product 𝑖 as a set of purpose span embedding vectors and a set of mechanism span embedding vectors . 2 . 1 Data We use real - world product idea descriptions taken from crowd - sourced innovation website Quirky . com and used in [ 39 ] , including 8500 user - generated texts describing inventions across diverse do - mains ( e . g . , kitchen products , health and fitness , clean energy ) . Texts typically include multiple purposes and mechanisms . Texts in Quirky use very nonstandard language , including grammatical and spelling errors ( e . g . , “Folds Up Perfect For Carrying . you can walk - on , put your mouth on and or hands on . numbers in any configuration 4 learning to De / Composing Numbers . ” ) . Annotation . To create a dataset annotated with purposes and mecha - nisms , we collect crowdsourced annotations on Amazon Mechanical Turk ( AMT ) . In the similar annotation task of [ 39 ] workers were reported to annotate long , often irrelevant spans . We thus guided workers to focus on shorter spans . To further improve quality and encourage more fine - grained annotations , we limited maximal span length that could be annotated , and disabled the annotation of stop - words . Fig . 2 shows our tagging interface ; rectangles are taggable chunks . For quality control , we required US - based workers with approval rate over 95 % and at least 1000 approved tasks , and filtered unreasonably fast users . Workers were paid $ 0 . 1 per task . In total , we had 400 annotating workers . Median completion time was 100 seconds . While a manual inspection of the annotations revealed they are mostly satisfactory , we observe two main issues : First , there are of - ten multiple correct annotations . Second , workers provide partial tagging – in particular , if similar spans appear in different sentences , very few workers bother tagging more than one instance ( despite instructions ) . These issues would have made computing evaluation metrics problematic . We thus decided to use the crowdsourced an - notations as a bronze - standard for training and development sets only . For a reliable evaluation , we collected gold - standard test sets annotated by two CS graduate students . Annotators were instructed to mark all the relevant chunks , resulting in high inter - annotator agreement of 0 . 71 . We collect 22316 annotated training sentences and 512 gold sentences , for a total of 238 , 399 𝑡𝑜𝑘𝑒𝑛𝑠 ( tag proportions : 14 . 5 % mechanism , 15 . 9 % purpose , 69 . 6 % other ) . A note on related annotated data . There has been recent work on the related topic of information extraction from scientific papers by classifying sentences , citations , or phrases . Recent supervised approaches [ 9 , 41 , 49 ] use annotations which are often provided by either paper authors themselves , NLP experts , domain experts , or involve elaborate ( multi - round ) annotation protocols . Sequence tagging models are often trained and evaluated on ( relatively ) clean , succinct sentences [ 54 , 75 ] . When trained on noisy texts , results typically suffer drastically [ 2 ] . Our corpus of product descriptions is significantly noisier than scientific papers , and our training annota - tions were collected in a scalable , low - cost manner by non - experts . Using noisy crowdsourced annotation for training and development only is consistent with our quest for a lightweight annotation ap - proach that would still enable training useful models . 2 . 2 Extracting Spans After collecting annotations , we can now train models to extract the spans . We explore several models likely to have sufficient power to learn our proposed novel representation , with the goal of selecting the best performing one . In particular , we chose two approaches that are common for related sequence - tagging problems , such as named entity recognition ( NER ) and part - of - speech ( POS ) tagging : a common baseline and a recent state - of - the - art model . We also tried a model - enrichment approach with syntactic relational inputs . We briefly describe the models we used below , with full technical descriptions and implementation details , data and code appearing in the supplementary material ( Appendix A . 1 ) . We note that our goal in this section is to find a reasonable model whose output could support creative downstream tasks ; many other architectures are possible and could be considered in future work . Hope et al . 0 10 20 30 40 50 60 70 80 90100 Top K ( % ) 40 45 50 55 60 65 70 75 P r e c i s i on mechanism purpose 40 45 50 55 60 65 70 75 Configuration P R F 1 Enriched BiLSTM 45 . 24 39 . 01 41 . 90 Pooled - Flair 53 . 30 39 . 80 45 . 50 GCN 47 . 85 47 . 93 47 . 89 GCN self - train 49 . 00 52 . 00 50 . 50 Figure 3 : Left : Precision @ K results for the best performing model ( GCN + self - training ) . Right : Raw extraction accuracy evaluation . All approaches use CRF loss . GCN with syntactic edges outperforms baselines . Self - training further improves re - sults . Random - label achieves only 16 . 01 F 1 . • BiLSTM - CRF . A BiLSTM - CRF [ 40 ] neural network , a common baseline approach for NER tasks , enriched with semantic and syn - tactic input embeddings known to often boost performance [ 75 ] . We adopt the “multi - channel” strategy as in [ 75 ] , concatenating input word embeddings ( pretrained GloVe vectors [ 60 ] ) with part - of - speech ( POS ) and NER embeddings . A conditional random field ( CRF ) model over the BiLSTM outputs maximizes the tag sequence log likelihood under a pairwise transition model between adjacent tags [ 5 ] . • Pre - trained Language Model ( Pooled Flair ) . A pre - trained lan - guage model [ 4 ] based on contextualized string embeddings , re - cently shown to outperform other powerful models such as BERT [ 19 ] in NER and POS tagging tasks and achieve state - of - art results . • GCN . We also explore a model - enrichment approach with syn - tactic relational inputs . We employ a graph convolutional network ( GCN ) [ 42 ] over dependency - parse edges [ 75 ] . GCNs are known to be useful for propagating relational information and utilizing syn - tactic cues [ 54 , 75 ] . The linguistic cues are of special relevance and interest to us , as they are known to exist for purpose / mechanism mentions in texts [ 26 ] . 2 . 3 Evaluation of Extraction Accuracy In this section we assess extraction accuracy ( whether we are able to extract purpose and mechanism spans of text ) . In the next sections , we evaluate the utility of the extracted spans for enabling creative innovation tasks . To evaluate raw accuracy of the model’s predictions , we use the standard IOB label markup to encode the purpose and mechanism Figure 4 : Comparing our GCN model predictions ( right ) to human annotations ( left ) . Interestingly , our model managed to correct some annotator errors ( “it’s” , “heated” , “coffee warm” , “beverages” ) . Purposes in pink , mechanisms in green . spans ( 5 possible labels per token , { Beginning , Inside } x { Purpose , Mechanism } plus an " Outside " label ) . We conduct experiments using a train / development / test split of 18702 / 3614 / 512 . Due to our challenging setting , we train models on bronze - standard annotations with noisy and partial tagging done by non - experts ; for evaluation we use a curated gold - standard test set ( Section 2 ) . See Figure 3 ( right ) for results : GCN reaches an F 1 score of ∼ 48 % , out - performing the BiLSTM - CRF model ( enriched with multi - channel GloVe , POS , NER and dependency relation embeddings ) by 6 % . GCN also surpasses the strong Pooled - Flair pre - trained language model by nearly 2 . 5 % . A random baseline guessing each token by label frequencies ( Section 2 ) achieves 16 . 01 F 1 . We interpret these results as possibly attesting to the utility of graph representations and features capturing syntactic and semantic information when labels are noisy . As a sanity check , we also computed precision @ K ( Figure 3 , left ) . As expected , precision is higher with low values of 𝐾 , and gradually degrades . Precision for mechanisms is higher than for purposes . Interestingly , a manual inspection revealed many cases where despite the noisy training setting , our models managed to correct mistaken or partial annotations ( see Figure 4 ) . Self - Training . According to the results , we chose GCN as our best - performing model . We experimented adding self - training [ 63 ] to GCN . Self - training is a common approach in semi - supervised learn - ing where we iteratively re - label “O” tags in training data with model predictions . A large portion of our training sentences are ( erroneously ) un - annotated by workers , perhaps due to annotation fatigue , introducing bias towards the “O” label . Self - training with GCN shows an improvement in F 1 by an ad - ditional 2 . 6 % , substantially increasing recall ( more than 12 % over Flair ) , see Figure 3 , right . Self - training stopped after 2 iterations , following no gain in F 1 on the development set . 3 FINE - GRAINED FUNCTIONAL SEARCH FOR ALTERNATIVE USES In the previous section we suggested a model for extracting purpose and mechanism spans and assessed extraction accuracy . Our focus in this paper is to study the utility of the extracted purposes and mechanisms , in terms of the user interactions they enable . In the following sections we explore two tasks demonstrating the value of Scaling Creative Inspiration with Fine - Grained Functional Facets of Ideas our novel representation for supporting creative innovation . We start with a task involving search for alternative uses . Motivation . Our task is inspired by one of the most well - known divergent thinking tests [ 36 ] for measuring creative ability – the alternative uses test [ 37 ] , where participants are asked to think of as many uses as possible for some object . Aside from serving as a measure of creativity , the ability to find alternative uses for technolo - gies has important applications in engineering , science and industry . Technologies developed at NASA , the US space agency , have led to over 2 , 000 spinoffs , finding new uses in computer technology , agri - culture , health , transportation , and even consumer products 1 . Procter & Gamble , the multinational consumer goods company , has invested in systematic search for ideas to re - purpose and adapt from other industries , such as using a compound that speeds up wound healing to treat wrinkles - an idea that led to a new line of anti - wrinkle products [ 22 ] . And very recently , the COVID - 19 pandemic provided a stark example of human innovation , with many companies seeking to pivot and re - purpose existing products to fit the new climate [ 23 ] . One teaching story is that of John Osher , creator of the “Spin Pop” — a lollipop with a mechanism for twirling in your mouth . After selling his invention , Osher’s team systematically searched for new ideas — “rather than having an idea come to us” 2 . The group eventually landed on the ”Spin Brush” – a cheap electric toothbrush adapted from the same twirling mechanisms . This case of repurposing an existing technology involved a systematic search process rather than pure serendipity . Introducing automation could help accelerate the search process by scouring millions of relevant problems available online , but the task is challenging for existing search systems , requiring a fine - grained , multi - aspect understanding of products . Illustrative Example . Consider a company that manufactures light bulbs . The company is familiar with straightforward usages of their product ( lamps , flashlights ) , and wants to identify non - standard uses and expand to new markets . Finding uses for a lightbulb that are not about the standard purpose of illuminating a space would be difficult to do with a standard search query over an idea repository , as the term “lights” or “lighting” will bring back lots of results close to “lamps , ” “flashlight , ” and the like . In contrast , with our representation each idea is associated with mechanism and purpose facets , and one could form a query such as mechanism = “light bulb” , purpose = NOT “light” . Using our system , the searcher adds “light” as a mechanism and also adds “light” as a negative purpose ( i . e . , results should not include “light” as a purpose ) . Our prototype returns examples such as billiard laser instructor devices ( Table 1 ) , warning signs on food packages to get attention of kids with allergies , and lights attached to furniture to protect your pinky toes at night ( Figure 5 ) . 3 . 1 Study Design We have built a search engine prototype supporting our representa - tion . Figure 5 shows the top two results for the light bulb scenario : warning lights on food for kids with allergies , and lights attached to furniture to protect your pinky toe at night . These are non - standard 1 https : / / spinoff . nasa . gov / 2 https : / / www . allbusiness . com / the - man - the - legend - john - osher - inventor - of - the - spin - brush - part - i - 2 - 7665547 - 1 . html recombinations [ 25 ] ( light + allergies , light + furniture guard ) that could lead the company to new markets . We conduct an experiment simulating scenarios where users wish to find novel / uncommon uses of mechanisms . Table 1 shows the sce - narios and examples . To choose these scenarios for the experiment , we find popular / common mechanisms in the dataset and their most typical uses . For example , one frequent mechanism is RFID , which is typically used for purposes such as “locating” and “tracking” . We then create queries searching for different uses – purposes that do not include concepts related to the typical uses of a given mechanism . 3 Our Approach . We represent each product 𝑖 as a set of purpose vectors P 𝑖 (cid:66) { p 1 𝑖 , p 2 𝑖 , . . . , p 𝑃 𝑖 𝑖 } , and a set of mechanism vectors M 𝑖 (cid:66) { p 1 𝑖 , p 2 𝑖 , . . . , p 𝑀 𝑖 𝑖 } extracted with our GCN model . Simi - larly , we define a set of query vectors q 𝑝 (cid:66) q 1 , q 2 , . . . q 𝑄 𝑝 and q 𝑚 (cid:66) q 1 , q 2 , . . . q 𝑄 𝑚 . Each query chunk can be negated , meaning it should not appear . Finally , we define distance metrics 𝑑 𝑝 ( · , · ) , 𝑑 𝑚 ( · , · ) between sets of purposes and mechanisms . For example , to locate a dog using RFID but not GPS : argmin 𝑖 𝑑 𝑝 ( { q “locate dog” } , P ˜ 𝑖 ) 𝑠 . 𝑡 . 𝑑 𝑚 ( { q “GPS” } , M ˜ 𝑖 ) ≥ threshold 𝑑 𝑚 ( { q “RFID” } , M ˜ 𝑖 ) ≤ threshold ( 1 ) We explore two alternatives for computing distance metrics 𝑑 𝑚 , 𝑑 𝑝 : • FineGrained - AVG . 𝑑 𝑝 ( q 𝑝 , P 𝑖 ) is 1 minus the dot product be - tween average query and purpose vectors ( normalized to unit norm ) . We define 𝑑 𝑚 similarly . • FineGrained - MAXMIN . We match each element in q 𝑝 with its nearest neighbor in P 𝑖 , and then find the minimum over the distances between matches . 𝑑 𝑝 is defined as 1 minus the minimum . All vectors are normalized . We define 𝑑 𝑚 similarly . This captures cases where queries match only a small subset of product chunks , erring on the side of caution with a max - min approach . Baselines . We test our model against : • AvgGloVe . A weighted average of GloVe vectors of the entire text ( excluding stopwords ) , similar to standard NLP approaches for retrieval and textual similarity . We average query terms and normalize to unit norm . Distance is computed via the dot product . • Aggregate purpose / mechanism . Representing each document with the model in [ 39 ] . This model takes raw text as input , applies a BiLSTM neural network and produces two vectors correspond - ing to aggregate purpose and mechanism of the document . We average and normalize query vectors , and use the dot product . For all four methods , we handle negative purpose queries by filtering out all products whose distance is greater than 𝜆 , where lambda is a threshold selected to be the 90 th percentile of distances . 3 . 2 Results We recruited five engineering graduate students ( three female , two male ) to judge the retrieved product ideas . Each participant provided binary relevance ranking to the top 20 results from each of the 3 To automate scenario selection , we cluster mechanisms ( see Section 4 . 1 for details ) , select frequent mechanisms from the top 5 largest mechanism clusters , and identify purposes strongly co - occurring with them ( e . g . , “RFID” co - occurs with “locating” , “tracking” ) to avoid . Hope et al . Figure 5 : Applications for light where light is not in the purpose . Left : Interface . Right : Two of the results and their automatic annotations ( purposes in pink , mechanisms in green ) . Query Example results Mechanism : light . Purpose : NOT light Billiard laser instructor ( projector ) Mechanism : solar energy . Purpose : NOT generating power Light bulbs with built - in solar chips . Mechanism : water . Purpose : NOT cleaning , NOT drinking A lighter that burns hydrogen generated from water and sunlight . Mechanism : RFID . Purpose : NOT locating , NOT tracking A digital lock for your luggage with RFID access . Mechanism : light . Purpose : cleaning A UV box to clean and sanitize barbells at the gym . Table 1 : Scenarios and example results retrieved by our FineGrained - AVG method . Queries reflect non - trivial uses of mechanisms ( e . g . , using water not for drinking / cleaning , retrieving a lighter running on hydrogen from water and sunlight ) . Figure 6 : Results for search evaluation test case . Mean average precision ( MAP ) and Normalized Discounted Cumulative Gain ( NDCG ) by method , averaged across queries . Methods in bold use our model . four methods , shuffled randomly so that judges are blind to the condition . 4 4 Inter - rater agreement measured across all scenarios was at 50 % by both Fleiss kappa and Krippendorff’s alpha tests . See Figure 6 for results . We report Non Cummulative Discounted Gain ( NDCG ) and Mean Average Precision ( MAP ) , two common metrics in information retrieval [ 64 ] . Our FineGrained - AVG wins for both metrics , followed by FineGrained - MAXMIN . The baselines perform much worse , with the aggregate - vectors approach in [ 39 ] outperforming standard embedding - based retrieval with GloVe . Im - portantly , our approach achieves high MAP ( 85 % - 87 % ) in absolute terms , in addition to a large relative improvement over the baselines ( MAP of 40 % - 60 % ) . Qualitative Analysis . Table 1 shows example results of FineGrained - AVG . For instance , a query for using light not for lighting results in laser - based billiard instructions . A query for using RFID not for locating or tracking results in an idea for an RFID - based lock , or RFIDs used at supermarket checkouts . To give an intuition for what might be driving our quantitative findings , we examine examples of retrieved results . For instance , with the query for using light for the non - standard purpose of cleaning , the top ranked result retrieved by FineGrained - AVG is a UV Light Sterilizer , with extracted purposes including Sterilizes bacteria , Keep public and people healthy and Cleaner fresher air , and the top result from FineGrained - MAXMIN is sim - ilarly a Standalone bug zapper bulb that uses uv light / black light . Conversely , the top result for both baselines ( standard search and aggregate - vectors ) is a Toilet / Bathroom Light , with “a sensor light Scaling Creative Inspiration with Fine - Grained Functional Facets of Ideas that glows around your toilet” and “extra batteries if you lose elec - tricity in the bathroom” . It appears that both baselines were not able to accurately capture and disentangle purposes and mechanisms , despite the aggregate - vector being explicitly designed for that . More generally , it appears that the aggregate - vector approach squashes multiple purposes together by design into one soft , ag - gregate vector , which in this case includes concepts like toilet and bathroom that are somewhat topically related to cleaning . The aggre - gate approach had similar issues in the next product ideas it retrieved ( e . g . , Switch that glows in the dark , a Dash Light to illuminate ash trays ) . Overall , our results demonstrate that fine - grained purposes and mechanisms lead to better functional search expressivity than ap - proaches based on distributional representations or coarse purpose - mechanism vectors . 4 EXPLORING THE DESIGN SPACE WITH A FUNCTIONAL CONCEPT GRAPH In this section we test the value of our novel representation for supporting users in exploring the design space for solving a given problem . We use our span - based representation to construct a corpus - wide graph of purpose / mechanism concepts . We demonstrate the utility of this approach in an ideation task , helping users identify useful inspirations in the form of problems that are related to their own . Our goal is to help users “break out” of fixation on a certain domain , a well - known hindrance to innovation [ 15 , 43 ] . Doing so is challenging because it requires some level of abstraction : being able to go beyond the details of a concrete problem to connect to a part of the design space that may look dissimilar on the surface , but has abstract similarity . Numerous studies in engineering and cognitive psychology have shown the benefits of problem abstractions for ideation [ 26 , 28 , 35 , 43 , 46 , 72 , 73 ] . However , these studies either involve non - scalable methods ( relying on highly - structured annota - tions , or on crowd - sourcing ) or simple , syntactical pattern - matching heuristics incapable of capturing deeper abstract relations . In [ 39 ] ( aggregate - vectors baseline from the previous section ) , crowdwork - ers were given a product description from the Quirky database , and asked to come up with ideas for products that solve the same prob - lem in a different way . Aggregate vectors representing purpose and mechanism were used to find near - purpose , far - mechanism analo - gies . Thus , finding analogies relied on having a given mechanism to control for structural distance . Unlike [ 39 ] , in our setup we assume a more realistic scenario where we are given only a short problem description – e . g . , generat - ing power for a phone , reminding someone to take medicine , folding laundry – and aim to find inspirational stimuli [ 35 ] in the “sweet spot” for creative ideation – structurally related to the given problem , not too near yet also not too far [ 27 ] . Functional Concept Graph . To address this challenge , we build a tool inspired by functional modeling [ 38 ] , which we call a Func - tional Concept Graph . A functional model is , roughly put , a hierar - chical ontology of functions and ways to achieve them , and is a key concept in engineering design . Such models are especially useful for innovation , allowing problem - solvers to break out of a fixed overly - concrete purpose or mechanism and move up and down the Figure 7 : An example of our learned functional concept graph extracted from texts . Mechanism in green , purpose in pink . Ti - tles are tags nearest to cluster centroids ( redacted to fit ) . hierarchy . Despite their great potential , today’s functional models are constructed manually , and thus do not scale . We thus construct a ( crude ) approximation of a functional representation that would still be useful for exploring the design space and suggesting potentially useful inspirations to users . In our approach , Functional Concept Graphs consist of nodes cor - responding to purposes or mechanisms , and edges encoding semantic ( not necessarily hierarchical ) relations . Our span - based representa - tion enables us to build this graph , as we can look at fine - grained co - occurrences of concepts appearing together in products and thus infer relations between them , finding connections across ideas at different levels of abstraction . For example , Figure 7 shows an actual subgraph from our auto - matically constructed functional concept graph related to electricity , power and charging . Products that mention certain purposes ( e . g . , “charge your phone " ) will often mention other , structurally related problems that could be more general / abstract ( e . g . , “generate power " ) or more specific ( “wireless phone charging " ) , resulting in edges in our graph ( only high - confidence edges are shown ) . A designer could go from the problem of charging batteries to the more general prob - lem of generating power , and from there to another branch ( e . g . , solar power and mechanical stored energy ) , to get inspired by structurally related ideas . 4 . 1 Building a Functional Concept Graph We develop a method to infer this representation from co - occurrence patterns of the fine - grained spans of text . Naively looking for co - occurrences of problems may yield inspirations too near to the orig - inal 𝑝 𝑖 , as many frequently co - occurring purposes tend to be very similar , while we are interested discovering the more abstract re - lations . In addition , raw chunks of text extracted from our tagging model have countless variants that are not sufficiently abstract and are thus sparsely co - occurring . We thus design our approach to en - courage abstract inspirations . Broadly , we take the following two steps 5 : 5 This is meant as an overview of the method . See more implementation details in the next section . Hope et al . I . Concept discretization . Intuitively , nodes in our graph should correspond to groups of related spans ( “charging” , “charging the battery” , “charging a laptop” ) . To achieve this , we take all purpose and mechanism spans ˆ P , ˆ M in the corpus , extracted using our GCN model , and cluster them ( separately ) , using pre - trained vector representations . We refer to the clusters C 𝑝 , C 𝑚 as concepts . II . Relations . We employ rule - mining [ 59 ] to discover a set of rela - tions R between concepts . Relations are Antecedent = ⇒ Consequent , with weights corresponding to rule confidence . To illustrate our intu - ition , suppose that when “prevent head injury” appears in a product description , the conditional probability of “safety” appearing too is large ( but not the other way around ) . In this case , we can ( weakly ) infer that preventing head injuries is a sub - purpose of “safety” . Indeed , manually observing the purpose - purpose edges , the one - directional relations captured are often sub - purpose , and the bi - directional ones often encode abstract similarity . Similarly , for mechanism concepts the one - directional relations are often part of ( “cell phone” and “battery” ) , and bi - directional are mechanisms that co - occur often . For pairs of purpose and mechanism concepts , the relation is often functionality ( “charger” , “charge” ) . Exploring more relations is left for future work . 4 . 2 Study Design Next , we set out to test the utility of the functional concept graph in an ideation task . In our setup participants are given problems ( e . g . , reminding people to take their medication in the morning ) and are asked to think of creative solutions . Participants were also given a list of potential inspirations , grouped into boxes , and were instructed to mark whether each was novel and helpful . They were also encouraged to explain the solution it inspired . Figure 8 shows our interface . In this example , seeing inspirations about health monitors caused one user to suggest monitoring the person to find the best time to remind them to take medicine ; see - ing inspirations about coffee caused them to suggest integrating medicine reminders into coffee machines . To create a set of seed problems , a graduate student mapped between problems from WikiHow . com ( a website of how - to guides ) to purposes in our data . Using this source allowed us to collect real - world problems that are broadly familiar , with succinct and self - explanatory titles that do not require further reading to understand . The student was tasked with confirming that our Quirky dataset contains idea descriptions that mention these problems . For a given problem in WikiHow ( how to remember to take medication ) , they performed keyword search over 17 𝐾 purpose spans gleaned by our model from Quirky , and found matching spans ( morning medicine reminder ) . We use those matching spans as our seed problem descrip - tion given to users ( purple text in Figure 8 ) . We collect 25 problems this way . Table 2 shows more examples , such as Tracking distance walked , folding laundry or sensing dryness level . Inspirations are other purpose spans from our dataset ( see Table 2 ) , selected automatically using our approach or baseline approaches . Our Method . For our approach , we construct a functional concept graph as in Section 4 . 1 . To cluster related spans into concept nodes , we explore two common and powerful vector representations of spans to capture semantic similarity : • GloVe [ 60 ] pre - trained word embeddings , averaged across tokens . Figure 8 : A snippet from our ideation interface for “morning medicine reminder” . Users see inspirations grouped into boxes ( each box is supposed to represent a concept ) . User indicate which inspirations were useful , and what ideas they inspired . For example , seeing “real time health checker” inspired one user to suggest a monitoring device for finding the best time for reminding to take the medicine . • BERT - based [ 62 ] contextualized vectors that have been fine - tuned for semantic similarity tasks 6 . We cluster the spans using K - Means + + 7 [ 8 ] . We then apply the Apriori algorithm 8 to automatically mine association rules between clusters , [ 59 ] and use the confidence metric to select the top rules 9 . To use the mined rules between purpose nodes ( clusters ) for select - ing inspirations shown to users , we start from the purpose node corresponding to the given problem and take its consequents ; as explained earlier , this captures a weak signal of abstract similarity . Some of these nodes contain tens of spans in them . Thus , we also explore two approaches to “summarize” each concept cluster with representative spans displayed to users – one that attempts to summarize the cluster independently of the seed problem , and one that takes the seed problem into account : • TextRank [ 55 ] . We construct a graph where nodes are the spans in a cluster and edges represent textual similarity . We run PageRank [ 58 ] on this graph , selecting the top 𝐾 spans to present . • Nearest spans . Following the findings in [ 27 ] , select the top 𝐾 spans in C 𝑝 that are nearest to the query 𝑝 𝑖 . ( For both approaches , we use 𝐾 = 5 ) . Baselines . • Purpose span similarity . Given a problem 𝑝 𝑖 , we find the 𝐾 = 5 nearest purpose spans of text in our corpus ( out of 17 𝐾 purposes ) . We experiment with the same two vector representations used by our approach : GloVe and BERT . This method is similar to applying the methodology in [ 39 ] to our setting , where in our setting we are given only a problem 𝑝 𝑖 and no mechanism 𝑚 𝑖 is available to control for structural distance . While this approach relies on our model for extracting purpose spans , we consider it a baseline to study the added value of our hierarchy . 6 We use RoBERTa - large - STS - SNLI , available at github . com / UKPLab / sentence - transformers . 7 𝐾 = 250 selected automatically with elbow - based criteria on silhouette scores . 8 http : / / www . borgelt . net / pyfim . html . 9 We use the top 3 rules in our experiment . Scaling Creative Inspiration with Fine - Grained Functional Facets of Ideas Problem Inspirations Rater explanation Track distance walked Protect children Get ideas from devices that keep track of children Folding laundry Store toilet paper Roll laundry around a tube instead of folding Dispense medicine Pet bowl that keeps ants away Based on pet bowls that can dispense food during the day Sense dryness level Voltage reading Use electric current to measure water level ( safely ) Waterproof Ideas from sensors in waterproof devices Temperature reading Morning medicine reminder Schedule coffee , coffee alarm Alarm clock with coffee and medicine reminders Send vital data , real - time health checker Health trackers to tell if medicine not taken , alert accordingly Heart rate monitoring , continuously monitor glucose Find the best time to take medicine Table 2 : Example inspirations and explanations given by human evaluators . • Linguistic abstraction . We use the WordNet [ 56 ] lexical data - base to extract hypernyms ( for each token in 𝑝 𝑖 ) , in order to capture potential abstractions . WordNet is often used in similar fashion for design - by - analogy studies [ 32 , 35 , 46 ] . • Random concepts . Random inspirations are often considered as a baseline in ideation studies since diversity of examples is a known booster for creative ability [ 39 ] . For each task , we select a random cluster from C 𝑝 and display its TextRank summary . Rating Collection . In our study , each method generated 𝐾 = 5 spans ( concept summaries ) , which are grouped and displayed together in a box ( see Figure 8 ) . For each problem a rater views 8 boxes in randomized order , to avoid bias . We recruit 10 raters ( 8 graduate students , a senior engineering professor , and an architect ) . Raters were instructed to mark inspirations they consider useful and relevant for solving a given problem , while being not about the same problem . Raters were also encouraged to write comments , especially for non - trivial cases which they found of interest ( see Table 2 ) . In total , raters viewed 2584 boxes , or 12920 purpose descriptors . 4 . 3 Results Analyzing inspirations . Table 2 and Figure 8 show examples of problems , inspirations and user explanations from our study . For in - stance , users facing the “morning medicine reminder” problem were presented with nearby concepts in the Functional Concept Graph that included health monitoring and coffee machines . To explore why these concepts are connected in our graph and why they are potentially useful as inspirations , we make use of the direct inter - pretability of our approach . We examine the purpose co - occurrences from which the Functional Concept Graph was constructed . Figure 9 shows the subgraph with concept nodes of Making hot drinks , alerting / reminding , health monitoring , medicine delivery , and edges representing products in which two adjacent purposes were co - mentioned ( e . g . , a “coffee machine alarm” product that men - tioned the purposes of making hot drinks and alerting / reminding , or a “smart medicine injector " that mentioned both alerting / reminding and medicine delivery ) . This explains why the concepts are nearby in the graph , as there are multiple products in our dataset that share purposes from both concepts . For example , a “pill reminder” product refers to the problem of forgetting to take medicine at prescribed times ( Sends notification if you forgot to take your AM or PM meds ) , while a “smart injec - tor” device administers medicine on set time intervals . At the same time , both of these products mention purposes of medicine delivery . When our graph construction algorithm observes enough similar co - occurrence patterns between the concepts of alerting and medicine delivery , across multiple products , an edge is added between the two in the graph . Similarly , an “Alarm coffee maker” product mentions the purposes of time management and making coffee at a set time as well as alerting when the coffee is ready , explaining how it emerges as a potential inspiration in our graph . This type of linkage or overlap between an original problem space and inspiration problems helps get at a sweet - spot of innovation [ 16 ] by finding ideas that are not too near and not too far from the original problem , helping users break out of fixation as discussed earlier in this section . Users in our study used these inspirations to come up with a tracker that alerts the user at the best time to take a medicine and a coffee machine reminding the user to take their medication with their morning coffee . Those creative directions demonstrate the utility of the Functional Concept Graph for exploring the design space . Quantitative results . Figure 10 shows the results of the user study . On the left , we show the proportion of inspirations ( individual spans ) selected by at least two raters , for each method . Our approach sig - nificantly outperforms all the baselines . The effect is particularly pronounced for the BERT - based approach , with 51 % of inspirations found useful , while the best baseline reaches less than 30 % . Interest - ingly , for both BERT and GloVe representations , the Nearest - span summarization approach fares better , potentially due to striking a balance between being too far / near the initial problem 𝑝 𝑖 . Figure 10 ( right ) shows the proportion of inspiration boxes that got at least 2 individual inspirations marked ( by at least 2 raters ) . This metric measures the effect of a box as one unit , as each box is meant to represent a coherent cluster . Our method is able to reach 62 % , while the best baseline ( GloVe search on purpose spans ) yields only 39 % . Again , the nearest - span summarization is prefered to TextRank . Importantly , for both individual inspiration spans and inspirations boxes , 51 % - 62 % are rated as useful – high figures considering the challenging nature of the task . 5 DISCUSSION AND CONCLUSION In this paper we introduced a novel span - based representation of ideas in terms of their fine - grained purposes and mechanisms and used it to develop new tools for creative ideation . We trained a model to extract spans from a noisy , real - world corpus of products . We used this representation to support human creativity in two applications : expressive search for alternative , uncommon uses of products , and generating a graph to help problem - solvers explore the design space Hope et al . Alert / remind Making hot drinks Medicine delivery Medical monitoring Coffee machine alarm Smart medicine injector Smart medicine injector pill reminder Smart medicine injector Figure 9 : Example from our Functional Concept Graph , explaining the inspirations shown to users in Figure 8 . Nodes represent concepts ( clusters of purposes ) , named by us for readability . Edges are annotated with products containing spans from both concepts . The problem of “medicine morning reminder” is mapped ( via embedding ) to the Alert / remind concept , which is linked to the concepts of medical monitoring and making hot drinks through products such as “smart medicine injector” and “coffee machine alarm” ( among others , not displayed in the figure ) . These links serve as inspirations in our study . Figure 10 : Inspiration user study results . Left : Proportion of inspirations selected by at least 2 raters , per condition . Right : Proportion of boxes ( clusters ) with at least 2 spans marked by ≥ 2 raters . around their problem . In both ideation studies , we were able to achieve high accuracies , significantly outperform baselines and help boost user creativity . 5 . 1 Limitations While our results showed the promise of a functional facet representa - tion , and demonstrated potentially feasible technical approaches for extracting this representation from unstructured text , the approach has several limitations . Challenging Annotation Task for Crowds . First , the annotation task proved to be somewhat difficult for crowdworkers , and the out - puts were noisy . One direction for future work would be to explore weak supervision approaches to augment annotation . One issue that Scaling Creative Inspiration with Fine - Grained Functional Facets of Ideas might exacerbate the problem is that sometimes the boundary be - tween purpose and mechanism is fuzzy , and it is genuinely difficult to tell how to annotate the span . Limited Functional Schema . In a similar vein , it might be interest - ing to explore more expressive schemas , containing elements other than just purpose and mechanism ( similar to [ 12 ] ) . One particularly useful element to add might be context / constraints ( e . g . , nanoscale ) , to restrict the search space to feasible solutions . Surface Form Abstraction . Another limitation of our approach is that our functional facets ( and resulting embeddings ) remain quite closely anchored to the original texts . This limits their ability to be used to match across domains , to make connections such as inspiring new optimization approaches by analogy to " heating / cooling " sched - ules in metallurgy . Achieving abstraction to match across distant domains without burdening the user with a combinatorial explosion of noisy matches remains an open problem . We wonder if abstracting key objects or entities in a purpose functional facet — such as a more automated approach to replacing objects with their " commonsense " properties — might be more feasible than attempting to abstract from an entire product description or abstract , given that the chunk is already a rich signal of the product’s functional meaning . 5 . 2 Future Work and Broader Implications Moving to future directions , we are excited about the potential of functional facets to lead to advances in the interpretability of content - based recommender systems in these complex domains . Keywords are inherently interpretable , but are limited in their capacity to sup - port crossing knowledge boundaries ; and until now , embedding - based approaches ( e . g . , [ 39 ] ) have not always led to interpretable justifications for matches . Functional facets could provide the basis of not just more powerful search operators , but also more inter - pretable results and feedback loops . Deeper Functional Graph Exploration . A key component for the above might be expanding on our use of functional graphs , built from the extracted functional facets . In our experiments , we used our functional concept graph to retrieve inspirations from " around " the problem . But what would it take to be able to explore this graph ? Could we identify and optimize for latent coordinates in the func - tional space , moving " up " and " down abstraction levels , or " across " sibling nodes in a functional graph ? Taking inspiration from network perspectives on ideation [ 11 , 34 , 69 ] , could we retrieve interesting " lineages " of ideas , or compute the potential inspiration value of functional facets based on network connectivity metrics ? Could we combine these content - based functional facets with measures of use ( e . g . , citations ) , to enrich approaches that combine content - and social - based signals , such as literature - based discovery [ 68 , 70 ] ? Identifying Overlap and Gaps Across Fields . These approaches rely on identifying interesting overlaps in concepts that simultane - ously coincide with disjunctions in citations , as signals of potentially impactful " undiscovered public knowledge " : a persistent challenge is how to define " concepts " - keyword or unstructured text approaches can lead to combinatorial explosions of noise to sift through , and con - trolled vocabulary ( e . g . , MEDLine ) can help increase the signal to noise ratio , but are only available in specialized circumstances [ 65 ] . Functional facets might be a useful bridge between unstructured text and controlled vocabularies for identifying points of overlap and disjunction between different fields , accelerating the discovery of gems hidden in plain sight . Functional Facets for Collaborative Ideation . Future work could also explore new interactions in collaborative and crowd innovation that might be enabled by the ability to quickly extract functional facets in idea corpora . The source of our primary dataset here , Quirky , was actually a crowd innovation platform . HCI research on these plat - forms have begun to emphasize moving away from mere " selection " of best ideas from large samples of ideas , towards supporting gener - ative collaboration over ideas . Open problems include synthesizing major themes in large - scale corpora of user - generated ideas and identifying gaps in the idea space [ 13 , 51 , 66 ] , as well as supporting intelligent matching and structuring ways for crowd innovators to collaborate and build on each others’ ideas [ 14 , 52 ] between crowd innovators . We are excited about the potential for functional facets to assist with these functions , as a complement to other approaches like crowd - powered synthesis [ 7 , 18 , 33 , 67 ] . Here , too , the potential for functional facets to be highly interpretable could power novel ex - plorations of mixed - initiative systems for augmenting collaborative ideation at scale [ 50 , 67 ] . Mapping of Design Spaces . Beyond supporting richer search for creative inspiration , a data - driven approach to extracting functional facets and learning relationships between the facets could power much more expansive approaches to mapping out design spaces for entire domains or problem areas , identifying key subproblems and constraints and novel paths through the design space . Mapping approaches like this , such as technological roadmapping [ 10 ] , have already shown significant promise for reinvigorating research and development in real - world applications such as neural recording [ 53 ] . However , these mapping exercises are still highly manual and labor - intensive processes ; computational support for such tasks could have transformative impacts on innovation . REFERENCES [ 1 ] The car mechanic who uncorked a childbirth revolution . BBC News , 2013 . [ 2 ] G . Aguilar , S . Maharjan , A . P . L . Monroy , and T . Solorio . A multi - task approach for named entity recognition in social media data . In 3rd Workshop on Noisy User - generated Text , 2017 . [ 3 ] A . Akbik , T . Bergmann , D . Blythe , K . Rasul , S . Schweter , and R . Vollgraf . Flair : An easy - to - use framework for state - of - the - art nlp . In NAACL - HLT , 2019 . [ 4 ] A . Akbik , T . Bergmann , and R . Vollgraf . Pooled contextualized embeddings for named entity recognition . In NAACL , 2019 . [ 5 ] A . Akbik , D . Blythe , and R . Vollgraf . Contextual string embeddings for sequence labeling . In International Conference on Computational Linguistics , 2018 . [ 6 ] G . Altshuller . 40 principles : TRIZ keys to innovation . 2002 . [ 7 ] P . André , A . Kittur , and S . P . Dow . Crowd Synthesis : Extracting Categories and Clusters from Complex Data . In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing , CSCW ’14 , pages 989 – 998 , New York , NY , USA , 2014 . ACM . [ 8 ] D . Arthur and S . Vassilvitskii . k - means + + : The advantages of careful seeding . In ACM - SIAM symposium on Discrete algorithms , 2007 . [ 9 ] I . Augenstein , M . Das , S . Riedel , L . Vikraman , and A . McCallum . Semeval 2017 task 10 : Scienceie - extracting keyphrases and relations from scientific publications . arXiv preprint arXiv : 1704 . 02853 , 2017 . [ 10 ] E . S . Boyden and A . H . Marblestone . Architecting Discovery : A Model for How Engineers Can Help Invent Tools for Neuroscience . Neuron , 102 ( 3 ) : 523 – 525 , May 2019 . Publisher : Elsevier . [ 11 ] H . Cai , E . Y . Do , and C . M . Zimring . Extended linkography and distance graph in design evaluation : An empirical study of the dual effects of inspiration sources in creative design . Design Studies , 31 ( 2 ) : 146 – 168 , 2010 . [ 12 ] J . Chan , J . Chang , T . Hope , D . Shahaf , and A . Kittur . Solvent : A mixed initiative system for finding analogies between research papers . CSCW , 2018 . Hope et al . [ 13 ] J . Chan , S . Dang , and S . P . Dow . Comparing Different Sensemaking Approaches for Large - Scale Ideation . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems , pages 2717 – 2728 . ACM , 2016 . [ 14 ] J . Chan , S . Dang , and S . P . Dow . Improving Crowd Innovation with Expert Facilitation . In Proceedings of the 19th ACM Conference on Computer - Supported Cooperative Work & Social Computing , CSCW ’16 , pages 1223 – 1235 , New York , NY , USA , 2016 . ACM . [ 15 ] J . Chan , S . P . Dow , and C . D . Schunn . Do The Best Design Ideas ( Really ) Come From Conceptually Distant Sources Of Inspiration ? Design Studies , 2015 . [ 16 ] J . Chan , K . Fu , C . Schunn , J . Cagan , K . Wood , and K . Kotovsky . On the benefits and pitfalls of analogies for innovative design : Ideation performance based on ana - logical distance , commonness , and modality of examples . Journal of mechanical design , 2011 . [ 17 ] J . Chan , T . Hope , D . Shahaf , andA . Kittur . Scalingupanalogywithcrowdsourcing and machine learning . In ICCBR - 16 . [ 18 ] L . B . Chilton , G . Little , D . Edge , D . S . Weld , and J . A . Landay . Cascade : Crowd - sourcing taxonomy creation . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems , pages 1999 – 2008 . ACM , 2013 . [ 19 ] J . Devlin , M . - W . Chang , K . Lee , and K . Toutanova . Bert : Pre - training of deep bidirectional transformers for language understanding . In NAACL - HLT , 2019 . [ 20 ] K . Dorst . The core of “design thinking” and its application . Design Studies , 2011 . [ 21 ] J . R . Duflou and P . - A . Verhaegen . Systematic innovation through patent based product aspect analysis . CIRP Annals - Manufacturing Technology , 2011 . [ 22 ] K . Essick . Technology scouts : hoping to find the next big thing . Science Business , Feb . 2006 . [ 23 ] K . Essick . Innovation and creativity in a time of crisis . Science Business , 2020 . [ 24 ] M . Färber , F . Bartscherer , C . Menne , and A . Rettinger . Linked data quality of dbpedia , freebase , opencyc , wikidata , and yago . Semantic Web , 2018 . [ 25 ] L . Fleming . Recombinant uncertainty in technological search . Management science , 47 ( 1 ) : 117 – 132 , 2001 . [ 26 ] K . Fu , J . Cagan , K . Kotovsky , and K . L . Wood . Discovering Structure In Design Databases Through Functional And Surface Based Mapping . JMD , 2013 . [ 27 ] K . Fu , J . Chan , J . Cagan , K . Kotovsky , C . Schunn , and K . Wood . The Meaning of Near and Far : The Impact of Structuring Design Databases and the Effect of Distance of Analogy on Design Output . JMD , 2013 . [ 28 ] K . Fu , J . Chan , C . Schunn , J . Cagan , and K . Kotovsky . Expert representation of design repository space : A comparison to and validation of algorithmic output . Design Studies , 2013 . [ 29 ] D . Gentner and K . J . Kurtz . Relational Categories . In Categorization inside and outside the laboratory : Essays in honor of Douglas L . Medin , APA decade of behavior series . American Psychological Association , Washington , DC , US , 2005 . [ 30 ] D . Gentner and A . B . Markman . Structure mapping in analogy and similarity . American psychologist , 1997 . [ 31 ] K . Gericke and B . Eisenbart . The integrated function modeling framework and its relation to function structures . AI EDAM , 2017 . [ 32 ] K . Gilon , J . Chan , F . Y . Ng , H . Lifshitz - Assaf , A . Kittur , and D . Shahaf . Analogy mining for specific design needs . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , CHI ’18 , pages 121 : 1 – 121 : 11 . ACM , 2018 . [ 33 ] V . Girotto , E . Walker , and W . Burleson . The Effect of Peripheral Micro - tasks on Crowd Ideation . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems , CHI ’17 , pages 1843 – 1854 , New York , NY , USA , 2017 . ACM . [ 34 ] M . Gonçalves and P . Cash . The life cycle of creative ideas : Towards a dual - process theory of ideation . Design Studies , 72 : 100988 , Jan . 2021 . 00000 . [ 35 ] K . Goucher - Lambert and J . Cagan . Crowdsourcing inspiration : Using crowd generated inspirational stimuli to support designer ideation . Design Studies , 2019 . [ 36 ] J . P . Guilford . Three faces of intellect . American psychologist , 1959 . [ 37 ] J . P . Guilford . The nature of human intelligence . 1967 . [ 38 ] J . Hirtz , R . Stone , D . A . McAdams , S . Szykman , and K . Wood . A functional basis for engineering design : reconciling and evolving previous efforts . Research in engineering Design , 2002 . [ 39 ] T . Hope , J . Chan , A . Kittur , and D . Shahaf . Accelerating innovation through analogy mining . In KDD , 2017 . [ 40 ] Z . Huang , W . Xu , and K . Yu . Bidirectional lstm - crf models for sequence tagging . arXiv preprint arXiv : 1508 . 01991 , 2015 . [ 41 ] D . Jin and P . Szolovits . Hierarchical neural networks for sequential sentence classification in medical scientific abstracts . arXiv preprint arXiv : 1808 . 06161 , 2018 . [ 42 ] T . N . Kipf and M . Welling . Semi - Supervised Classification with Graph Convolu - tional Networks . sep 2016 . [ 43 ] A . Kittur , L . Yu , T . Hope , J . Chan , H . Lifshitz - Assaf , K . Gilon , F . Ng , R . E . Kraut , and D . Shahaf . Scaling up analogical innovation with crowds and ai . PNAS , 2019 . [ 44 ] T . Kuribayashi , H . Ouchi , N . Inoue , P . Reisert , T . Miyoshi , J . Suzuki , and K . Inui . An empirical study of span representations in argumentation structure parsing . In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4691 – 4698 , Florence , Italy , July 2019 . Association for Compu - tational Linguistics . [ 45 ] D . B . Lenat . Cyc : a large - scale investment in knowledge infrastructure . In Communications of the ACM , 1995 . [ 46 ] J . Linsey , A . Markman , and K . Wood . Design by analogy : a study of the wordtree method for problem re - representation . JMD , 2012 . [ 47 ] J . Linsey , A . B . Markman , and K . L . Wood . WordTrees : A method for design - by - analogy . In Proceedings of the 2008 ASEE Annual Conference , 2008 . [ 48 ] Y . Luan , L . He , M . Ostendorf , and H . Hajishirzi . Multi - task identification of entities , relations , and coreference for scientific knowledge graph construction . In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3219 – 3232 , 2018 . [ 49 ] Y . Luan , L . He , M . Ostendorf , and H . Hajishirzi . Multi - task identification of entities , relations , and coreferencefor scientific knowledge graph construction . In Proc . Conf . Empirical Methods Natural Language Process . ( EMNLP ) , 2018 . [ 50 ] M . Mackeprang , C . Müller - Birn , and M . T . Stauss . Discovering the Sweet Spot of Human - Computer Configurations : A Case Study in Information Extraction . Proc . ACM Hum . - Comput . Interact . , 3 ( CSCW ) : 195 : 1 – 195 : 30 , Nov . 2019 . [ 51 ] N . Mahyar , D . V . Nguyen , M . Chan , J . Zheng , and S . P . Dow . The Civic Data Deluge : Understanding the Challenges of Analyzing Large - Scale Community Input . In Proceedings of the 2019 on Designing Interactive Systems Conference , DIS ’19 , pages 1171 – 1181 , New York , NY , USA , June 2019 . Association for Computing Machinery . 00012 . [ 52 ] T . W . Malone , J . V . Nickerson , R . J . Laubacher , L . H . Fisher , P . de Boer , Y . Han , and W . B . Towne . Putting the Pieces Back Together Again : Contest Webs for Large - Scale Problem Solving . In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing , CSCW ’17 , pages 1661 – 1674 , New York , NY , USA , 2017 . ACM . [ 53 ] A . H . Marblestone , B . M . Zamft , Y . G . Maguire , M . G . Shapiro , T . R . Cybulski , J . I . Glaser , D . Amodei , P . B . Stranges , R . Kalhor , D . A . Dalrymple , D . Seo , E . Alon , M . M . Maharbiz , J . M . Carmena , J . M . Rabaey , E . S . Boyden , G . M . Church , and K . P . Kording . Physical Principles for Scalable Neural Recording . Frontiers in Computational Neuroscience , 7 , 2013 . arXiv : 1306 . 5709 . [ 54 ] D . Marcheggiani and I . Titov . Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling . 1 , 2017 . [ 55 ] R . Mihalcea and P . Tarau . Textrank : Bringing order into text . In EMNLP , 2004 . [ 56 ] G . A . Miller . Wordnet : a lexical database for english . Communications of the ACM , 1995 . [ 57 ] T . Mitchell , W . Cohen , E . Hruschka , P . Talukdar , B . Yang , J . Betteridge , A . Carlson , B . Dalvi , M . Gardner , B . Kisiel , et al . Never - ending learning . Communications of the ACM , 2018 . [ 58 ] L . Page , S . Brin , R . Motwani , and T . Winograd . The pagerank citation ranking : Bringing order to the web . Technical report , Stanford InfoLab , 1999 . [ 59 ] N . Pasquier , Y . Bastide , R . Taouil , and L . Lakhal . Discovering frequent closed itemsets for association rules . In Database Theory—ICDT’99 . 1999 . [ 60 ] J . Pennington , R . Socher , and C . D . Manning . Glove : Global vectors for word representation . In EMNLP , 2014 . [ 61 ] M . E . Peters , S . Ruder , andN . A . Smith . Totuneornottotune ? adaptingpretrained representations to diverse tasks . In RepL4NLP @ ACL , 2019 . [ 62 ] N . Reimers and I . Gurevych . Sentence - bert : Sentence embeddings using siamese bert - networks . In EMNLP , 2019 . [ 63 ] M . Sachan and E . Xing . Self - training for jointly learning to ask and answer questions . In NAACL - HLT , 2018 . [ 64 ] H . Schütze , C . D . Manning , and P . Raghavan . Introduction to information re - trieval . In International communication of association for computing machinery conference , 2008 . [ 65 ] Y . Sebastian , E . - G . Siew , and S . O . Orimaye . Emerging approaches in literature - based discovery : techniques and performance review . The Knowledge Engineering Review , 32 , Jan . 2017 . [ 66 ] P . Siangliulue , K . C . Arnold , K . Z . Gajos , and S . P . Dow . Toward Collaborative Ideation at Scale : Leveraging Ideas from Others to Generate More Creative and Diverse Ideas . In Proceedings of the 18th ACM Conference on Computer Sup - ported Cooperative Work & Social Computing , CSCW ’15 , pages 937 – 945 , New York , NY , USA , 2015 . ACM . [ 67 ] P . Siangliulue , J . Chan , S . P . Dow , and K . Z . Gajos . IdeaHound : Improving Large - scale Collaborative Ideation with Crowd - powered Real - time Semantic Modeling . In UIST’16 , 2016 . [ 68 ] N . R . Smalheiser . Rediscovering Don Swanson : The Past , Present and Future of Literature - based Discovery . Journal of Data and Information Science , 2 ( 4 ) : 43 – 64 , Dec . 2017 . [ 69 ] R . Sosa . Accretion theory of ideation : evaluation regimes for ideation stages . Design Science , 5 : e23 , 2019 . [ 70 ] D . R . SwansonandN . R . Smalheiser . UndiscoveredPublicKnowledge : aTen - Year Update . In Proceedings of KDD ’96 , page 4 , 1996 . [ 71 ] S . Vattam , B . Wiltgen , M . Helms , A . K . Goel , and J . Yen . DANE : Fostering Creativity in and through Biologically Inspired Design . In Design Creativity 2010 . 2011 . [ 72 ] L . Yu , A . Kittur , and R . E . Kraut . Searching for analogical ideas with crowds . In CHI , 2014 . Scaling Creative Inspiration with Fine - Grained Functional Facets of Ideas [ 73 ] L . Yu , B . Kraut , and A . Kittur . Distributed analogical idea generation : innovating with crowds . In CHI’14 , 2014 . [ 74 ] L . Yu , R . E . Kraut , and A . Kittur . Distributed analogical idea generation with multiple constraints . In Proceedings of the 19th ACM Conference on Computer - Supported Cooperative Work & Social Computing . ACM , 2016 . [ 75 ] Y . Zhang , P . Qi , and C . D . Manning . Graph convolution over pruned dependency trees improves relation extraction . In EMNLP , 2018 . A TECHNICAL APPENDIX A . 1 Model Details • BiLSTM - CRF . A BiLSTM - CRF [ 40 ] neural network , a common baseline approach for NER tasks , enriched with semantic and syn - tactic input embeddings known to often boost performance [ 75 ] . We first pass the input sentence x = ( 𝑥 1 , 𝑥 2 , . . . , 𝑥 𝑇 ) through an embedding module resulting in v 1 : 𝑇 , v 𝑖 ∈ R 𝑑 𝑒 , where 𝑑 𝑒 is the embedded space dimension . We adopt the “multi - channel” strat - egy as in [ 75 ] , concatenating input word embeddings ( pretrained GloVe vectors [ 60 ] ) with part - of - speech ( POS ) and NER embed - dings . We additionally add an embedding corresponding to the incoming dependency relation . The sequence of token embeddings is then processed with a BiLSTM layer to obtain contextualized word representations h ( 0 ) 1 : 𝑇 , h 𝑖 ∈ R 𝑑 ℎ , where 𝑑 ℎ is the hidden state dimension . The outputs are fed into a linear layer 𝑓 to obtain per - word tag scores 𝑓 (cid:16) h ( 𝐿 ) 1 (cid:17) , 𝑓 (cid:16) h ( 𝐿 ) 2 (cid:17) , . . . , 𝑓 (cid:16) h ( 𝐿 ) 𝑇 (cid:17) . These are used as inputs to a conditional random field ( CRF ) model which maximizes the tag sequence log likelihood under a pairwise transition model between adjacent tags [ 5 ] . • Pooled Flair . A pre - trained language model [ 4 ] based on contex - tualized string embeddings , recently shown to outperform powerful approaches such as BERT [ 19 ] in NER and POS tagging tasks and achieve state - of - art results . Flair 10 uses a character - based language model pre - trained over large corpora , combined with a memory mechanism that dynamically aggregates embeddings of each unique string encountered during training and a pooling op - eration to distill a global word representation . We follow [ 4 ] and concatenate pre - trained GloVe vectors to token embeddings , add a CRF decoder , and freeze the language - model weights rather than fine - tune them [ 19 , 61 ] . • GCN . We also explore a model - enrichment approach with syn - tactic relational inputs . We employ a graph convolutional network ( GCN ) [ 42 ] over dependency - parse edges [ 75 ] . GCNs are known to be useful for propagating relational information and utilizing syn - tactic cues [ 54 , 75 ] . The linguistic cues are of special relevance and interest to us , as they are known to exist for purpose / mechanism mentions in texts [ 26 ] . We used a GCN with same token embeddings as in the BiLSTM - CRF baseline , with a BiLSTM layer for sequential context and a CRF decoder . For the graph fed into the GCN , we use a pre - computed syntactic edges with dependency parsing : For sentence x 1 : 𝑇 , we convert its dependency tree to A 𝑠𝑦𝑛 where A 𝑠𝑦𝑛𝑖𝑗 = 1 for any two tokens 𝑥 𝑖 , 𝑥 𝑗 connected by a dependency edge . We also add self - loops A 𝑠𝑒𝑙𝑓 = 𝐼 ( to propagate from h ( 𝑙 − 1 ) 𝑖 to h ( 𝑙 ) 𝑖 [ 75 ] ) . Following [ 75 ] , we normalize activations to reduce bias toward high - degree nodes . For an 𝐿 - layer GCN , denoting h ( 𝑙 ) 𝑖 ∈ R 𝑑 ℎ to be the 𝑙 - th layer output node , the GCN operation can be written as ℎ ( 𝑙 ) 𝑖 = 𝜎 (cid:169) (cid:173) (cid:171) ∑︁ 𝑟 ∈R    𝑛 ∑︁ 𝑗 = 1 A 𝑟𝑖𝑗 W ( 𝑙 ) 𝑟 ℎ ( 𝑙 − 1 ) 𝑗 / 𝑑 𝑟𝑖 + b ( 𝑙 ) 𝑟    (cid:170) (cid:174) (cid:172) 10 https : / / github . com / flairNLP / flair Hope et al . Figure 11 : Schema of our GCN model . where R = { syn , self } , 𝜎 is the ReLU activation function , W ( 𝑙 ) 𝑟 is a linear transformation , b ( 𝑙 ) 𝑟 is a bias term and 𝑑 𝑟𝑖 = (cid:205) 𝑇𝑗 = 1 A 𝑟𝑖𝑗 is the degree of token 𝑖 w . r . t 𝑟 . In the GCN architecture , 𝐿 layers corre - spond to propagating information across 𝐿 - order neighborhoods . We set the contextualized word vectors h ( 0 ) 1 : 𝑇 to be the input to the GCN , and use h ( 𝐿 ) 1 : 𝑇 as the output word representations . Figure 11 illustrates the GCN model architecture . Similarly to [ 54 ] , we do not model edge directions or dependency types in the GCN layers , to avoid over - parameterization in our data - scarce setting . We also attempted edge - wise gating [ 54 ] to mitigate noise propagation but did not see improvements , similarly to [ 75 ] . In our experiments , we followed standard GCN training proce - dures . Specifically , we base our model on the experimental setup detailed in [ 75 ] ( see also the authors’ code which we adapt for our architecture , at https : / / github . com / qipeng / gcn - over - pruned - trees ) . We pre - process the data using the spaCy ( https : / / spacy . io ) package for tokenization , dependency parsing , and POS / NER - tagging . We use pretrained GloVE embeddings of dimension 300 , and NER , POS and dependency relation embeddings of size 30 each , giving a total embedding dimension 𝑑 𝑒 = 390 . The bi - directional LSTM and GCN layers’ hidden dimension is 𝑑 ℎ = 200 , with 1 hidden layer for the LSTM . We find that the setting of 2 hidden layers works best for the GCNs . We also tried training with edge label information based on syntactic relations , but found this hurts performance . The training itself was carried out using SGD with gradient clipping ( cutoff 5 ) for 100 epochs , selecting the best model on the development set . For the Pooled - Flair approach [ 4 ] , we use the FLAIR framework [ 3 ] , with the settings obtaining SOTA results for CONLL - 2003 as in [ 4 ] ( see https : / / github . com / flairNLP / flair / blob / master / resources / docs / EXPERIMENTS . md ) . We also experiment with non - pooled embeddings and obtain similar results . We experiment with initial learning rate and batch size settings described in [ 4 ] , finding 0 . 1 and 32 to work best , respectively .