a r X i v : 1607 . 01422v1 [ phy s i c s . e d - p h ] 5 J u l 2016 Examining and contrasting the cognitive activities engaged in undergraduate research experiences and lab courses N . G . Holmes ∗ Department of Physics , Stanford University , Stanford , CA Carl E . Wieman Department of Physics , Stanford University , Stanford , CA and Graduate School of Education , Stanford University , Stanford , CA ( Dated : October 6 , 2016 ) While the positive outcomes of undergraduate research experiences ( UREs ) have been extensively categorized , the mechanisms for those outcomes are less understood . Through lightly structured focus group interviews , we have extracted the cognitive tasks that students identify as engaging in during their UREs . We also use their many comparative statements about their coursework , especially lab courses , to evaluate their experimental physics - related cognitive tasks in those envi - ronments . We ﬁnd there are a number of cognitive tasks consistently encountered in physics UREs that are present in most experimental research . These are seldom encountered in lab or lecture courses , with some notable exceptions . Having time to reﬂect and ﬁx or revise , and having a sense of autonomy , were both repeatedly cited as key enablers of the beneﬁts of UREs . We also iden - tify tasks encountered in actual experimental research that are not encountered in UREs . We use these ﬁndings to identify opportunities for better integration of the cognitive tasks in UREs and lab courses , as well as discussing the barriers that exist . This work responds to extensive calls for science education to better develop students’ scientiﬁc skills and practices , as well as calls to expose more students to scientiﬁc research . I . INTRODUCTION A recent focus of science education has been on build - ing scientiﬁc skills and practices , beyond content mas - tery [ 1 , 2 ] . Instructional laboratory courses ( labs ) and undergraduate research experiences ( UREs ) are key plat - forms through which undergraduate students can experi - ence these practices in a “hands - on / minds - on” way . The American Physical Society , in particular , has recently called upon higher education institutions to increase stu - dent access to UREs [ 3 ] , citing the positive outcomes of such experiences . Given the costs of these experiences ( as well as other resource requirements ) , providing UREs to all students seems logistically impractical , especially given that it has been estimated that only 50 % of under - graduate science students are currently accessing UREs [ 4 ] . Many attempts have been made , therefore , to in - corporate aspects of UREs into course - based activities [ e . g . 5 – 7 ] , despite a lack of evidence towards the partic - ular mechanisms that are responsible for the observed outcomes [ 8 ] . This has emphasized the need to better understand what about UREs leads to positive student outcomes . To this aim , this paper aims to explore the activities in which students engage during their UREs , using the cognitive tasks of experimental physics [ 9 ] as a lens for that exploration ( Fig . 1 ) . We also begin to identify the uniqueness of these tasks in UREs by comparing them to the tasks in which students engage during instructional labs and other coursework . We use the comparisons to ∗ ngholmes @ stanford . edu Cognitive Task Analysis Elements 1 . Establishing research goals 2 . Deﬁning criteria for suitable evidence 3 . Determining feasibility of experiment 4 . Experimental design 5 . Construction and testing of apparatus / code 6 . Analyzing data 7 . Evaluating results and analyzing implications 8 . Presenting the work FIG . 1 : List of experimental physics cognitive task anal - ysis ( EPCTA ) elements from [ 9 ] . Each task element con - tains a number of sub - tasks and there is extensive itera - tion between tasks and sub - tasks , as well as cycling back to earlier elements . Note that element seven combines two tasks from the original work . suggest what aspects of UREs could best be incorporated into course activities . Prior research probing the outcomes of UREs have highlighted clarifying career aspirations , understanding the scientiﬁc research process , learning how to think like a scientist , broadening disciplinary content knowledge , and developing self - conﬁdence and self - eﬃcacy [ 10 – 15 ] . While these elements emerge as common themes in stu - dents’ reported gains , the extensive variability in each student’s experience relates to large variability in an indi - vidual student’s gains [ 16 ] . Developing experimentation skills and ways of thinking has been studied a limited This paper has been accepted for publication . TABLE I : Demographics of the sample of students included in the interviews . Curricular level Number interviewed Number in URE Rising sophomores ( completed freshman / ﬁrst year ) 16 19 Rising juniors ( completed sophomore / second year ) 10 19 Rising seniors ( completed junior / third year ) 4 12 Number of faculty members of students interviewed 21 24 Number of women 9 ( 30 % ) 13 amount , notably in the work of Hunter et al [ 13 ] . Here we are building on this past work by digging more deeply into the particular skills and activities in which students engage in UREs . We then examine how well these overlap with the full range of cognitive tasks involved in carrying out actual experimental physics research , and contrast this to students’ course - based experiences . This examination uses the recent analysis of the cogni - tive tasks used by physicists to successfully carry out an experiment from conception to completion [ 9 ] . Wieman deﬁnes the Experimental Physics Cognitive Task Analy - sis ( EPCTA ) items as the “mental tasks or types of think - ing ( ‘cognitive task analysis’ ) associated with a physicist doing tabletop experimental research” [ p . 1 9 ] . Figure 1 lists the overarching cognitive tasks , each of which in - volves a number of sub - tasks . Though the list suggests a linear process , Wieman [ 9 ] speciﬁes that moving through the EPCTA often occurs in a highly iterative and cyclic process . While these tasks were developed in the context of experimental physics , many of these elements are fun - damental skills and practices that will beneﬁt students in a variety of contexts . As lab courses involve carrying out experiments , it is natural to consider the extent to which they emulate ac - tual research . It should be acknowledged , however , that this is not necessarily the sole , or even primary , goal of all lab courses . The goals of lab courses have been highly debated and without consensus for many years [ 17 – 19 ] . There is suggestive evidence , however , that traditionally - taught lab courses provide little added value for develop - ing or improving content understanding beyond lectures or tutorials [ 18 , 20 – 22 ] . The hands - on experimentation in lab courses provides a unique opportunity , however , for developing students’ scientiﬁc and experimentation skills . In addition , limited resources provide challenges to consistently oﬀering research opportunities to under - graduate students , and so it is important to examine the extent to which lab courses can compensate for these lim - itations . Furthermore , this analysis will provide insight into the uniqueness of UREs at the cognitive level , to shed light on what may be lost in replacing UREs with course - based activities . Here we are considering the traditional lab course and not course - based undergraduate research experiences ( CUREs ) [ e . g . 5 – 7 , 23 , 24 ] . While they oﬀer an interest - ing solution to these issues , none of our study population had experience in CUREs and so we did not consider them in this work . The research questions we are trying to answer are : • What cognitive tasks do students engage in during UREs , as contrasted to their experiences in course - based activities ( especially through lab courses ) ? • What barriers limit this engagement in UREs , coursework , and lab coursework ? • In what ways could coursework and lab course - work better incorporate these tasks ( with the aim of potentially facilitating larger scale participation and / or richer UREs ) ? II . METHODS Nine hour - long focus group interviews were conducted with summer URE students in the physics department at an elite university . All 51 students conducting research in the department were invited to participate through several email notices and short recruitment presentations during professional development events . Students signed up to participate after each such call and 32 of the 51 individual students participated in an interview by the end of the program . The physics department at this institution guarantees funding for one summer of research for every physics ma - jor and all students that apply to work each summer are accepted . This provides a unique measurement op - portunity in that our sample provides the perspective of students at a range of seniorities and class standings ( Table I ) . Most studies of undergraduate research expe - riences are limited to juniors or seniors ( upper - division students ) with high grade - point averages [ 4 ] . It must still be recognized , however , that the physics majors in our sample are somewhat unusual ; for example , several had had research experience while in secondary school . A . Interviews Interviews were conducted across eight weeks of the 8 - 10 week summer research program . The number of par - This paper has been accepted for publication . 1 . What year are you and what is your background experience in research ? 2 . Why did you sign up to do an URE project this summer ? 3 . How are you enjoying your research so far ? Can you give a little background about why this project is interesting to you ? 4 . What has been most enjoyable or rewarding to you about research so far ? 5 . What has been most disappointing ? 6 . What has been most diﬀerent from your expecta - tions ( good or bad ) ? 7 . What are the most important things you have learned ? 8 . What are some things you wish you had known be - fore starting out your research project ? 9 . What are some things that you did know starting out your research project that you think have helped you succeed ? 10 . Have your views about how research works and how much you enjoy it changed , and if so how and why ? 11 . Which scheduled development sessions did you ﬁnd most / least useful / enjoyable ? 12 . Tell me about your progress on the goals that you set at the beginning of the summer . 13 . How do you know that you have been making progress towards those goals ? 14 . Tell me about how you came up with your goals . Who was responsible for deciding on the goals ? 15 . In what ways has your experience in lab courses been similar to , or diﬀerent from , your research experi - ence ? 16 . In what ways has your experience in lab courses pre - pared you to do your research project ? 17 . Is there anything else you would like to tell me about your research experience ? FIG . 2 : List of questions used in each interview , though the order of questions varied . ticipants in a given interview varied from two to eight stu - dents . In addition , 12 students were interviewed twice . No two interviews , however , were made up of the same set of students . That is , the composition of each interview diﬀered , so students were interacting with diﬀerent peers and , therefore , diﬀerent ideas . In addition , students in - terviewed twice were interviewed towards the start and end of the summer , so the topics they focused on natu - rally shifted . Interviews were semi - structured . There was a ﬁxed set of targeted questions to be asked during each interview ( Fig . 2 ) . As can be seen from the list , most questions were explicitly centered on students’ URE . The order of the questions diﬀered and particular questions were probed more deeply depending on student answers . In addition , since there were multiple participants in each session , ideas evolved and expanded as students elabo - rated on or provided counter arguments to their peers’ statements . All questions were discussed in each of the interviews . The interviewer asked every student to an - swer the ﬁrst couple of questions by going around the ta - ble . For subsequent questions , students were participat - ing freely in the discussion . While diﬀerent students par - ticipated to varying degrees , the interviewer attempted to encourage all students to participate throughout , and in no interview did a single student ( or two ) dominate the discussion , and in all interviews all students contributed to the discussion at some point . B . Courses At the introductory level , students enrolled in three 10 - week physics courses , each of which involved optional associated lab courses . The lecture components of the introductory courses included some interactive engage - ment , but , in general , their courses were relatively tra - ditional in their pedagogical approaches . The associated lab courses , however , varied in their pedagogical designs and learning goals . A design lab course engaged students in a single , extended , student - designed experiment . The other two courses were structured lab courses that en - gaged students in highly structured lab exercises that changed weekly . The structured labs aimed to reinforce or develop concepts related to the lecture course con - tent , with some focus on exposing students to a variety of equipment or teaching data analysis and statistics . All students had taken at least one structured lab course and about 90 % of students interviewed had taken the intro - ductory design lab course . A few advanced students in the interviews discussed a senior - level project lab course , where students work in groups to design and carry out their own experimental project in low - temperature physics . This course will be discussed in the context of design lab courses . An upper division electronics course involved some troubleshoot - ing of circuits and experimental set ups , but was other - wise relatively structured and traditional . This course is therefore discussed in the context of the structured lab courses . C . Interview analysis The interviews were ﬁrst analyzed for emergent themes . A number of the themes that emerged were con - sistent with previous research on outcomes of UREs ( e . g . learning about career choices , the process of science , the life of academics ) . Evidence of this consistency can be found in the sample quotes embedded . Our questions did , however , elicit extensive discussions that probed comparisons between the URE and their prior lecture and lab courses . Primarily , these responses were related to what kinds of activities students were This paper has been accepted for publication . and were not doing in each context , what aﬀordances one provided over the other , and students’ opinions of these activities . Some of the most prominent topics were troubleshooting , experimental design , opportunities and time for reﬂection and iteration , the lack of a single or clear correct answer to a problem , autonomy , and com - parisons with course work . The EPCTA provided an appropriate framework for characterizing students’ cog - nitive processes . Interviews were coded based on the EPCTA . Instances of a particular cognitive task were categorized ( or coded ) for whether students were referring to the task in re - search , lecture courses , or the structured or design lab courses . The context of the discussion was also recorded as to whether students explicitly discussed that they were doing it , were not doing it , or whether discussion was mixed . From those notes , the coded discussions were further reﬁned for common themes . Our methodology places limitations on the analysis we perform and the conclusions we can draw . We cannot use the frequency of particular statements or cognitive tasks in a particular interview to quantitatively repre - sent importance or value . That is , a cognitive task being discussed more times or for a longer period of time in one interview is more representative of the nature of the interview than of the value students placed on that idea . We also cannot decisively say that because a code did not come up in an interview that it was not relevant in their research or classwork . The use of repeated inter - views , however , does provide some quantitative insight . Namely , importance can be inferred if a task element was discussed in multiple interviews . In contrast , a lack of relative value ( compared with the other tasks ) can be inferred if a task element was discussed in few interviews or if many interviews explicitly identiﬁed not engaging in that task . It would be desirable , therefore , to follow this work with targeted interviews or surveys with diﬀerent samples of students in diﬀerent contexts to better estab - lish the generalizability and consistency of the results . III . RESULTS In what follows , we describe the details of discussion surrounding each of the cognitive tasks independently . The summary of the number of interviews discussing each cognitive task element , as well as the context of that discussion , can be found in Fig . 3 . We provide sample quotes throughout the text . These use speaker identiﬁers NH to reﬂect the initial of the interviewer and S to reﬂect the student ( with S1 and S2 and so on to distinguish multiple speakers ) . A . Establishing research goals Students unanimously reported that the goals of their URE projects were provided to them by their research mentor . Related to predicting whether the goal was ahead of current knowledge , most of the discussion cen - tered on whether the goal was ahead of their knowl - edge . That is , students generally felt that their class - work had not prepared them to deal with the content in their projects . Students did identify that their courses laid groundwork for the research content . This was ei - ther in core courses that introduced basic terminology or through broad perspective courses that introduce current research topics . Few students discussed deciding whether the project was feasible , presumably because the supervisor had al - ready chosen the project . In two interviews , students discussed some lack of knowledge regarding goal setting . For example , they wondered how these projects come to fruition : how do we know this is worth doing ? Another student wanted to know more about how to apply infor - mation from research papers to solve new problems . Students also unanimously reported that the speciﬁc experimentation goals were always given in their struc - tured lab courses . Students described that , often in labs , the goal was to “obtain this number” ( e . g . gravitational acceleration as 9 . 8 m / s 2 ) . They directly contrasted this to their URE , where the goal was rarely to obtain a sin - gle number , and never to obtain a given number . In the design lab course , in contrast , students described hav - ing some control over the research question , though the space was constrained to a particular mechanics context ( at the intro level ) or low - temperature physics ( at the ad - vanced level ) . Only one student diﬀerentiated the URE as uniquely producing new knowledge . B . Deﬁning criteria for suitable evidence In UREs , the notion of identifying which variables were important arose in the context of the importance of un - derstanding every process and piece of their project or apparatus . This topic was picked up in one focus group with three diﬀerent students contributing to the discus - sion , all of whom were upper - division students who had completed more than one summer URE . One student ex - plicated how the fact that there was no ‘right’ answer required them to carefully evaluate all the relevant vari - ables . Related to the design lab courses , students simi - larly described evaluating the relevant variables and how to measure or control them , though they described the process as much simpler and more constrained . In the URE , the students found that the sheer number of rele - vant variables required them to integrate many diﬀerent areas of physics . This was contrasted to the lack of in - tegration of diﬀerent topics of physics in their typical homework assignments . A student also expressed surprise at realizing diﬀer - ent researchers took diﬀerent approaches to model the same system , which reinforced the need to carefully jus - tify their decisions : This paper has been accepted for publication . URE Design labs Structured labs Coursework 02468 02468 02468 02468 1 . Goals 2 . Criteria 3 . Feasibility 4 . Design 5 . Testing 6 . Analyzing 7 . Evaluating 8 . Presenting Section N u m be r o f i n t e r v i e w s No Mixed Yes FIG . 3 : Fraction of interviews in which the EPCTA elements were discussed in the context of URE , classwork , or the structured or design lab courses . Comments were categorized as Yes ( students were performing this task ) , No ( students were not performing this task ) , or mixed ( some students were and others were not ) . Student ( S ) : “When you’re building this trap and . . . you’re supposed to try to levitate this tiny bead . And the diﬃcult part is that even when you have all your optics perfectly aligned and everything , there are still so many parameters that go into whether your trap can trap or not . And ﬁguring that out is such a frus - trating and long process because basically you’re just bashing at a black box . . . . But over time you sort of build this sensitive understanding of each of the dif - ferent components of this black box . And at the end , when you understand the big picture , I think that’s the most satisfying . ” Students brieﬂy touched on the notion of determining what evidence would be convincing and for whom . A stu - dent reﬂected on how the goal of a previous URE project shifted signiﬁcantly when they did preliminary measure - ments and the eﬀect was not as strong as expected . An - other student described how researchers need to “see stuﬀ to show that it’s there . . . no argument matters until you have a plot of a 5 - sigma eﬀect . ” This student described learning about how the goal of research was to convince a “highly skeptical community” of researchers . This was explicitly contrasted to their classwork , where there was no such element of argumentation . Students did not speciﬁcally discuss activities related to identifying necessary controls or checks . Students also did not discuss evaluating criteria with regards to their structured lab courses , because , as they described , they simply “followed the instructions and got to the end , and turned it in . ” C . Determining feasibility of experiment The main topic of discussion about evaluating feasi - bility , in all contexts , was that of time . Students said that having a ﬁxed amount of time in their URE caused their goals to change , especially because timescales for completion were much longer than they expected : S : “There was a big sort of shift for me about two days ago , but it was sort of coming on for the past few weeks . Originally I was planning on getting a lot more done this summer and actually doing the trapping of atoms , but now I’m realizing I’m just going to have built a vacuum chamber and that’s not such a terrible thing to have done . Like , such a small amount of stuﬀ I’ve accomplished in , you know , 2 and a half months . ” Interviewer ( NH ) asks what caused the shift . S : “Well , at the very beginning of the summer . . . [ my mentor ] said we’d probably have time in 10 weeks to do this but everything always takes longer than you think it will and you have to order parts and those take a while to get there and you have to take them to the shop and the shop takes 4 days to do whatever you need done . ” This paper has been accepted for publication . Regarding contingency plans , some students identiﬁed that they had back - up project goals if things did not go as planned . One student emphasized the importance of having several actionable goals so that , even if large roadblocks are met , the student can still make progress on some pieces . There was little discussion of feasibility in the context of labs or coursework . The design lab course was high - lighted in that it gave students the ﬂexibility to shift their goal if things were not working out as planned , similar to UREs . Historical experiments included in the structured labs ( such as a Millikan oil - drop experiment ) were de - scribed as providing appreciation for how diﬃcult these classic experiments had been , though the results are fun - damental to our knowledge now . D . Experimental design While the URE project goals were generally given , stu - dents felt they had signiﬁcant autonomy to decide ele - ments of experimental design . The speciﬁcs of what de - cisions students made varied greatly between groups , but there was extensive dialogue regarding the overall sense of autonomy and independence : S1 : “We get to choose how we want to accomplish diﬀerent things . . . . they could tell us to simulate something on Matlab or go build something on the [ scanning tunneling microscope ] , . . . they don’t direct us that closely - but they do . It’s not like we’re choos - ing what we want to research in the ﬁeld so much . Which , I feel like I wouldn’t be able to do . ” NH : “So the question is given to you , but the approach is up to ? ” S1 : “Yeah , and we can ask for advice and stuﬀ , but it’s been a good balance , I think . ” S2 : “Yeah , I feel like , primarily , we need them to tell us what to do because we’re not at a level where we can actually understand enough to choose what we want to do . So at the most basic level of , like , simulate exactly what happens under these conditions , we choose line - by - line what code to write , but they’re telling us what they want the program to do . ” Only two students expressed the dissenting view that in their UREs they had very limited autonomy in the decision making process . In both cases , much of the de - cision making was done for them and so it was unclear why they were doing what they were doing . One of the students attributed this to a project whose goal was to replicate an existing process . A group of students discussed how the design space demonstrated that physics is much more creative than they had previously thought . Students drew compar - isons between exploring many possible designs with try - ing diﬀerent approaches to solving homework problems . This was contrasted , however , with the fact that having a single correct answer or a single way to solve a prob - lem removed much of the creative space in coursework . The fact that there was no single correct answer in their URE projects also led students to value the process of carefully analyzing the relevant variables in the system to check for systematic errors ( their language was to “see if you’re missing something” ) . The sense of autonomy was also evident in the design lab course . For example , students described having the opportunity to decide what they needed to do the next day . One group highlighted that developing their own procedure taught them that the procedure you take is the most important element of an experiment , rather than the result produced—a conclusion that was also discussed in the context of their URE . In contrast , students explicitly discussed how the structured lab courses did not allow autonomy . They saw no decisions to be made ; they simply had to follow the instructions provided . Students contrasted this to be - ing able to “ﬁddle around” in their UREs to make sense of equipment . Students also mentioned having to refer to equipment manuals in their URE , with one student explicitly contrasting that they would never do this in a lab course . Students described often not paying atten - tion to the process of the lab experiment because of the sheer amount of information given . Their focus on the experimental designs was also limited by time , because each week often involved a new lab activity that must be completed in a single two hour session . E . Construction and testing of apparatus ( or code ) In almost every interview , the majority of time was spent discussing this cognitive task , and every interview group had something to say about it . Troubleshoot - ing was a signiﬁcant part of the students’ URE , from searching for bugs in code or ﬁnding leaks in vacuum equipment . One student described how “everything is troubleshooting” and everything breaks . Troubleshoot - ing was both frustrating and rewarding , and often cited as an element where the most learning occurred . When asked about content students felt was needed to prepare them for their URE , troubleshooting was the most com - mon response . Most students said that no single course taught them troubleshooting , but some said it was picked up along the way to some degree . We will ﬁrst discuss elements of construction . Building or purchasing equipment occupied a signiﬁ - cant amount of students’ time early in the summer . One student said that they found making things very reward - ing while others described building equipment as not yet ‘doing science’ . The amount of building ( whether code or equipment ) in students’ projects varied from building new systems , to tweaking or optimizing an existing sys - tem . Some were shocked , but always very grateful , that they were able to build equipment or code themselves , independently , during their URE . In this area , students found that there was much more This paper has been accepted for publication . ‘engineering - like’ work than they had expected . They enjoyed being able to tinker with equipment : S1 : “There’s a lot of manual , random tweaking you do , which I guess makes sense . But I feel like when I’m in the lab , I’m more of a car mechanic than like a sci - entist with a pristine white lab coat . That’s actually interestingly fun - like one of the most fun things I did in the lab was change the oil in the vacuum pump . ” S2 : “That’s very true ! I’ve learned a lot about , like , torque wrenches and . . . I know so much about how screws are measured . ” S1 : “Yeah . ” S2 : “Like , what a quarter - twenty screw is and , like , all that sort of thing . ” S1 : “I feel like I can ﬁx my car now . I feel like I can ﬁgure that out . ” Students were surprised to learn about the enterprise , outside of the university , involved in building and sup - plying equipment to physics researchers . For example , one student mentioned that this presented career paths of which they had been previously unaware . Another noted the variety of personnel involved in building an ex - perimental set - up with relatively speciﬁc expertise . This message conﬂicted with that of their degree program : the fact that students need to excel in a variety of courses in a variety of areas suggested to them that they needed expertise in all areas . In using existing equipment , students discussed how there were no instructions to follow . Instead , the student had to ﬁgure out why each piece was where it was . Stu - dents contrasted this with their experience in the struc - tured labs ; they were given instructions on how to use the equipment , and felt that they did not learn what the equipment was or how it worked . Working with oscillo - scopes was extensively referenced in this context . Students whose URE projects involved constructing computer programs expressed a desire for the physics curriculum to include more explicit emphasis on learn - ing how to code . They highlighted , however , that the content in existing programming - focused courses ( both provided by the computer science and physics depart - ments ) was often insuﬃcient or irrelevant to the speciﬁc tasks they needed in their URE . While some students said they did not know how to check whether things were correct , other students dis - cussed testing their designs and codes in a variety of ways in their UREs . One student discussed using order of magnitude calculations to check for mistakes early . They discussed the skills required to be able to think , qualita - tively or intuitively , on your feet , and how they encoun - tered that in their URE . Another student discussed the process of breaking the system into smaller pieces to test individual elements and narrow down the problem . One group discussed how this process was much more ﬁxed or rigid than the design process , in that there were speciﬁc tasks that needed to be to carried out to test each part of the system . This was contrasted with structured lab courses , however , where one student described explicitly that they would not collect data on the performance of their equipment . While troubleshooting was a source of much learning , it was also a source of frustration . Students were dis - heartened if their equipment broke early in the project , because signiﬁcant use of valuable time was spent ﬁx - ing it ( again , rather than ‘doing science’ ) . Nonetheless , many students found this process to be very rewarding , expressing a sense of satisfaction when they ﬁnally ﬁg - ured out how to ﬁx their problem : S1 : “When you break a machine in the one way that the professor said , ‘Do not break the machine because they don’t make spare parts for this thing anymore . ’ But then you manage to ﬁx it anyways and then the thing starts working again , that’s good . . . Overcoming obstacles . ” S2 : “I completely agree with that . Yesterday I was struggling all day long with how to ﬁt this one graph a certain way and I was so upset and this morning I came in early and then it magically worked and I got it to work and I was so happy and it’s carried me through the whole day . ” This feeling of reward and satisfaction was even stronger if students had the independence to solve the problem themselves . Students described having to learn that one could try small modiﬁcations ﬁrst , compared with their initial instinct to start over from the beginning when things went wrong . One student described eventu - ally developing the courage to say “I have no clue what’s going on , but I can probably ﬁgure out why it’s not work - ing . ” Others said they learned not to get “stressed about getting things perfect the ﬁrst time . ” Regarding the iterative nature of experimentation , one student described how troubleshooting surprisingly led them to try to do things better . Another student de - scribed abandoning a project , instead of iterating to im - prove , because results were not promising enough to con - tinue . A student highlighted the need for patience and that sometimes it was important to leave , go have lunch , and come back . They appreciated that research gave them time to step back and make sense of things ( to be compared with structured lab courses ) . Iteration also came in the form of cycles of feedback and revisions with a professor or mentor . One student mentioned that , once they achieved data that suggested that the system was were working properly , the mentor would help ﬁgure out what to do next . Students noted that the independence and control over the project in the design lab courses made troubleshoot - ing and ﬂexibility necessary and much like their URE . Students also described having to suggest and ﬁnd ways to improve their experiment . There was much less discussion about ﬁnally collecting data in the URE . Many students did not quite reach this stage until the very end of their project , since building and troubleshooting took so much time . Students who This paper has been accepted for publication . had functioning systems described the process of collect - ing data to be very rewarding . As suggested , students’ descriptions of the structured lab courses paint a diﬀerent picture . They described many of the structured lab experiments as having “in - stant gratiﬁcation” with results that came out as ex - pected , with no need for iteration or reﬂection . Several interview groups highlighted a particular experiment in the introductory structured lab course when discussing troubleshooting : a Millikan oil drop experiment . Stu - dents described how , in this experiment , many students struggled to collect meaningful data . Students expressed frustration , however , at the contrast between this sit - uation and their URE . Students said they did not have suﬃcient time or resources in the lab experiment to make sense of their messy data , identify all the sources of error , or ﬁgure out how to resolve them . From their URE , they recognized the importance of taking a step back from the activity , as well as focusing on the process instead of ob - taining a speciﬁc result . They felt that the lack of time with each experiment created a missed opportunity . Another reason students did not engage in trou - bleshooting or spend time making sense of equipment in structured labs had to do with motivation . Students described the limitations when they were performing the experiment for a class assignment , rather than an au - thentic purpose . For example , a student said that , in research , they took care with their process because ev - erything is ‘real’ in research : systems break and it is their job to get them working again in order to move forward . In lab courses , you just need the machine to “spit out numbers that agree with your calculations . ” We should note here that the notion of tackling cutting edge research questions was not mentioned in this use of authentic or real experimentation . That is , it was the process of the experimentation that they found to be particularly inau - thentic , not the subject . F . Analyzing data Most students had not collected data until the end of the summer , and then only to a very limited extent . Re - gardless , there was extensive discussion about the anal - ysis process in their URE . Many students spoke about data analysis in the context of the required statistics and programming knowledge . There was disagreement about whether targeted courses were helpful or suﬃcient preparation for the programming and statistics needs in their UREs . Others described the process of modeling data , especially identifying and understanding approxi - mations and assumptions . There was also limited dis - cussion about analysis related to producing graphs and using data to make statistical or qualitative arguments . In the structured lab courses , the Millikan oil drop ex - periment was raised as one that required students to eval - uate sources of error and uncertainty , though with lim - ited time to act on that evaluation . Regarding the design lab courses , students described engaging in modeling and analyzing their data . One student described seeing little emphasis on the modeling process in their coursework , beyond just applying equations to solve problems . G . Evaluating results and analyzing implications Once again , there was little discussion about eval - uating results because few students had obtained re - sults . Students did describe , however , the questions that needed to be asked when interpreting results . In relation to checking results that come out the same as or diﬀer - ent than expected , one student described not knowing how to check whether the data were correct . Another student described the exciting process of interpreting re - sults with their mentor , asking questions such as “are the data what we expected ? Is something wrong or is it something new ? ” One student described their surprise at the subjectivity involved in making sense of results , recognizing that there may be multiple reasonable expla - nations for data . This was contrasted to coursework with a single correct answer . The notion of checking results came up more frequently with regards to lab courses . As mentioned earlier , stu - dents described that their structured lab course exper - iments typically involved obtaining a known or given value . A student described that , it was okay if their re - sult came out diﬀerently than expected , and that they would just submit the wrong result . Other students de - scribed asking the TAs to tell them what result they were supposed to get and how they should get it , so that they could ﬁnish . Other students described getting frustrated when the experiment did not work , leading to tempta - tions to “massage data” to obtain the speciﬁc , known value . In their URE , these students had described learn - ing to value data and their process because it might tell you what went wrong . In structured labs , students did not have time or incentive to ﬁgure that out : S1 : “I mean , it’s like , very much like you follow these - [ the design lab course ] is kind of diﬀerent - but it’s like you follow these steps and then here’s , ‘Today we’re learning about the charge of the electron . ’ And then you run this little experiment and you get a number that’s , like , kind of close to right . Maybe it’s , like , a multiple of 10 . . . ” S2 ( Interrupting ) : “No no - when you do [ that exper - iment ] and you’re , just like , no where close ! I think I wrote in my lab report that ‘It is very disturbing that so many people have previously measured the electron charge incorrectly . We can clearly see from this one that it should be equal to that . ’” Discussions of the design lab course reﬂected the sat - isfaction , also described in the context of their UREs , that any result was a result . In this course , students also described being guided to evaluate what they can and cannot conclude from their data . This paper has been accepted for publication . H . Presenting the work Regarding presentations , students primarily discussed the ﬁnal poster presentation , which was a required part of the URE program . Students expressed that the ﬁxed date of the poster presentation applied pressure to obtain presentable results . Presenting their work at group meet - ings was also raised as motivating them to make more progress . The notion of publishing did arise , but stu - dents did not expect to obtain a publication after only a summer of work . Some students mentioned learning about the process of publishing and some expressed sur - prise by the associated politics and pressure . Group meetings were also highlighted as helping stu - dents see the broader context of their and others’ work . This was mentioned as being helpful to students who sometimes felt lost in the details of their URE project . They contrasted that this broader perspective was even harder to achieve in coursework . Students explicitly described not presenting results of their experiments from structured lab courses . One stu - dent did mention that labs could provide some prac - tice for writing reports on experiments , but that the lab courses they took did not . A couple of students noted that the design lab course helped them communicate their work and progress , through the use of lab note - books . IV . DISCUSSION AND CONCLUSIONS In this paper , we used focus group interviews with summer URE students in a physics department to ﬁrst evaluate the cognitive tasks students engaged in during their UREs , with comparisons to students’ experiences in typical coursework and lab coursework ( Table II ) . Stu - dents generally engaged with most of the cognitive tasks required in physics research during their UREs and de - sign lab courses . The exception was initial goal setting and evaluation of feasibility in UREs . In coursework and structured lab courses , however , there was little engage - ment in many of the EPCTA elements . Our second research question regarded the barriers to engagement in these tasks in these diﬀerent settings . The primary barrier to engaging in cognitive tasks in their URE , extracted from student comments , was time . The limited term of the URE restricted the scope of the project . The amount of time taken up by build - ing and testing equipment took over the other aspects of the EPCTA . The second signiﬁcant barrier was stu - dents’ content knowledge . As highlighted in the EPCTA [ 9 ] , many of the tasks require an extensive understanding of the current state of knowledge in the ﬁeld , as well as technical expertise . As a result of these barriers , initial goal setting , eval - uating criteria , and evaluating feasibility were typically conducted by the research mentors before the students began their projects . This seems necessary for students to engage with the second half of the cognitive tasks and be able to present results at the end of the work pe - riod . Both of these barriers are logistically impractical to overcome . One potential partial remedy would be for the research mentor to explicitly expose the student to the steps and decisions that led to the research question . It is important to remember that not all undergraduate science majors , however , can engage in UREs . How well do our courses ( especially lab courses ) expose students to the EPCTA elements ? The conclusion of this work is that they do not do this well , at least as they are experienced by these students in courses with relatively conventional designs . While it is understandable that lecture courses would not engage students in cognitive tasks involved in physics experimentation , the lack of exposure in traditional lab courses where students are conducting experiments may seem surprising . Time , once again , becomes particularly problematic in lab courses in that carrying out an experi - ment and analyzing the data in just a few hours , often in a single week , makes the autonomy , reﬂection , and itera - tion that is fundamental to most elements of the EPCTA impossible . The structuring and framing of the activi - ties , in addition , discourage engaging in many cognitive tasks . Our third research question was to identify ways to overcome the barriers in coursework in order to bet - ter prepare students for research experiences . From our data , two small manipulations might allow structured labs to better prepare students for the cognitive tasks involved in UREs : providing time for testing and trou - bleshooting equipment ( for example , spreading experi - ment across multiple weeks ) and placing emphasis on the quality of students’ process rather than the product they obtain . It was clear that when the goal was to obtain a known result , this corrupted the process . It did not emerge from our work that it was important that the experimental outcome was producing new knowledge . We see two prominent future research questions from these results . First , would engagement and preparation in EPCTA elements in early lab courses lead to more ful - ﬁlling engagement in URE research activities , further im - proving the beneﬁts of UREs ? Because research on UREs have discussed relatively little about the role UREs play in developing these skills , perhaps lab courses should fo - cus on explicit skills development beyond simple engage - ment . While an open design lab course , where students choose their research goal and design , seems to include all EPCTA elements , we must recognize the necessary constraints and scaﬀolding for students to engage pro - ductively in and learn from those activities . Further re - search should probe the quality of engagement beyond the quantity found here . Second , could engagement and preparation in EPCTA elements in early lab courses replace UREs in terms of the non - cognitive beneﬁts they aﬀord ? Understanding this relationship would also require understanding the role of authenticity and community in these experiences . This paper has been accepted for publication . TABLE II : Broad summary of elements that are and are not included in undergraduate research , design lab courses , or structured lab courses . Cognitive Task Element Research Design lab courses Structured lab courses Establishing research goals No Yes No Deﬁne criteria for suitable evidence Partially Partially No Determine feasibility of experiment Partially No No Experimental design Yes Yes No Construction & testing of apparatus Yes Yes No ( except collect data ) Analyzing data Yes Yes Yes Evaluating results & Analyzing implications No No No Presenting the work Yes No No Although not part of the data set of this paper , students discussed elsewhere in the interviews how the commu - nity and collaboration between the undergraduate stu - dents and their graduate student , post - doc , or profes - sorial mentors were signiﬁcant contributions to the re - warding experiences during their UREs . Their contribu - tions to an existing body of knowledge , however , were mentioned much less so ; mostly students noted how fun - damentally small their contributions were . In addition , the authentic contributions were discussed much less and seemed less important to them than the authentic deci - sion making that was involved . This was contrasted to ﬁnding correct answers through a single correct proce - dure as in the structured labs and coursework . This work provides a foundational characterization of the types of skills and cognitive activities students engage in related to experimentation in physics . Beyond those listed above , we see new research questions that these data and results elicit regarding the role of lab courses and UREs in training STEM majors to understand and develop the tools and activities of an experimental scien - tist . ACKNOWLEDGMENTS We would like to thank the undergraduate volunteers who participated in this work , and Rick Pam and Lauren Tompkins for their support in recruiting these students . [ 1 ] H . Quinn , H . A . Schweingruber , and T . Keller , eds . , A Framework for K - 12 Science Education : Prac - tices , Crosscutting Concepts , and Core Ideas ( National Academies Press , 2012 ) . [ 2 ] American Association of Physics Teachers , Tech . Rep . ( 2014 ) , URL http : / / www . aapt . org / Resources / upload / LabGuidlinesDocument _ EBendorsed _ nov10 . pdf . [ 3 ] American Physical Society , Undergraduate research state - ment , http : / / www . aps . org / policy / statements / 14 - 1 . cfm ( 2014 ) . [ 4 ] S . H . Russell , M . P . Hancock , and J . McCullough , Science 316 , 548 ( 2007 ) . [ 5 ] S . E . Brownell , D . S . Hekmat - Scafe , V . Singla , P . Chan - dler Seawell , J . F . Conklin Imam , S . L . Eddy , T . Stearns , and M . S . Cyert , CBE Life Sci . Educ . 14 , ar21 ( 2015 ) . [ 6 ] L . C . Auchincloss , S . L . Laursen , J . L . Branchaw , K . Ea - gan , M . Graham , D . I . Hanauer , G . Lawrie , C . M . McLinn , N . Pelaez , S . Rowland , et al . , CBE Life Sci . Educ . 13 , 94 ( 2014 ) . [ 7 ] S . E . Brownell , M . J . Kloser , T . Fukami , and R . Shavel - son , J . Coll . Sci . Teach . 41 , 36 ( 2011 ) . [ 8 ] S . L . Laursen , in Physics Education Research Conference , edited by A . D . Churukian , D . L . Jones , and L . Ding ( College Park , MD , 2015 ) , pp . 18 – 21 . [ 9 ] C . Wieman , Phys . Teach . 53 , 349 ( 2015 ) . [ 10 ] T . D . Sadler , S . Burgin , L . McKinney , and L . Ponjuan , J . Res . Sci . Teach . 47 , 235 ( 2010 ) . [ 11 ] H . Thiry , S . L . Laursen , and A . - B . Hunter , J . High . Educ . 82 , 357 ( 2011 ) . [ 12 ] H . Thiry , T . J . Weston , S . L . Laursen , and A . - B . Hunter , CBE Life Sci . Educ . 11 , 260 ( 2012 ) . [ 13 ] A . - B . Hunter , S . L . Laursen , and E . Seymour , Sci . Educ . 91 , 36 ( 2007 ) . [ 14 ] E . Seymour , A . - B . Hunter , S . L . Laursen , and T . DeAn - toni , Sci . Educ . 88 , 493 ( 2004 ) . [ 15 ] D . Lopatto , Cell Biol . Educ . 3 , 270 ( 2004 ) . [ 16 ] D . I . Hanauer , J . Frederick , B . Fotinakes , and S . A . Stro - bel , CBE Life Sci . Educ . 11 , 378 ( 2012 ) . [ 17 ] A . Hofstein and V . N . Lunetta , Rev . Educ . Res . 52 , 201 ( 1982 ) . [ 18 ] A . Hofstein and V . N . Lunetta , Sci . Educ . 88 , 28 ( 2004 ) . [ 19 ] Committee on High School Science Laboratories : Role and Vision , Tech . Rep . , National Research Council , Washington , D . C . ( 2005 ) . [ 20 ] C . Wieman and N . G . Holmes , Am . J . Phys . 83 , 972 ( 2015 ) . [ 21 ] C . E . Wieman and N . G . Holmes , in Physics Education Research Conference 2015 , edited by A . D . Churukian , D . L . Jones , and L . Ding ( College Park , MD , 2015 ) . [ 22 ] M . - G . S´er´e , Sci . Educ . 86 , 624 ( 2002 ) . This paper has been accepted for publication . [ 23 ] I . Makarevitch , C . Frechette , and N . Wiatros , CBE Life Sci . Educ . 14 , ar27 ( 2015 ) . [ 24 ] C . Shapiro , J . Moberg - Parker , S . Toma , C . Ayon , H . Zim - merman , E . A . Roth - Johnson , S . P . Hancock , M . Levis - Fitzgerald , and E . R . Sanders , J . of Microbiol . Biol . Educ . 16 , 186 ( 2015 ) .