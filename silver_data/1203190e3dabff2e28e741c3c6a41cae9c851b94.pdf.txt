161 Critique Me : Exploring How Creators Publicly Request Feedback in an Online Critique Community RUIJIA CHENG , University of Washington , USA ZIWEN ZENG , University of California , San Diego , USA MAYSNOW LIU , University of California , Davis , USA STEVEN DOW , University of California , San Diego , USA Creative workers frequently turn to online critique communities for feedback on their work . While past research has focused primarily on how to yield better feedback from providers , less is known about the strategies feedback seekers use to engage providers and request feedback . We present two studies to explore the feedback exchange dynamics between feedback requesters and providers in the subreddit community , r / design _ critiques . In Study 1 , we interviewed 12 community members and found that while creators have strategies to request feedback , they expressed uncertainty about whether and how to include details about the design context , personal background , and specific feedback needs . In Study 2 , through a mixed - method analysis , we identified how specific request strategies impact the quantity and quality of community feedback , and found several key , but undervalued strategies : signaling as a novice , critiquing one’s own design , and providing design variants . These strategies led to better community response , but were rarely used . We offer design implications around how to leverage these insights to improve online feedback exchange platforms . CCS Concepts : • Human - centered computing → Empirical studies in collaborative and so - cial computing . Additional Key Words and Phrases : feedback exchange ; critique ; creativity ; online community ; mixed methods ACM Reference Format : Ruijia Cheng , Ziwen Zeng , Maysnow Liu , and Steven Dow . 2020 . Critique Me : Exploring How Creators Publicly Request Feedback in an Online Critique Community . Proc . ACM Hum . - Comput . Interact . 4 , CSCW2 , Article 161 ( October 2020 ) , 24 pages . https : / / doi . org / 10 . 1145 / 3415232 1 INTRODUCTION Seeking feedback online has become increasingly common and important for creative workers , especially those with limited access to formal feedback resources [ 20 , 29 ] . Creators often post their creative work to public online communities to seek opinions and suggestions from strangers [ 7 , 35 , 42 , 55 ] . Compared to formal feedback exchanges that occur in classrooms and workplaces , some argue that online communities can provide “more equal , collaborative , and interactive” [ 35 ] critique since it averts the traditional power dynamics between seekers and providers . However , feedback exchange in the wild is also unstructured . Often , the whole community can see feedback Authors’ addresses : Ruijia Cheng , rcheng6 @ uw . edu , University of Washington , Seattle , Washington , 98195 , USA ; Ziwen Zeng , ziwenzeng98 @ gmail . com , University of California , San Diego , La Jolla , California , 92093 , USA ; Maysnow Liu , mayliu @ ucdavis . edu , University of California , Davis , Davis , California , 95616 , USA ; Steven Dow , spdow @ ucsd . edu , University of California , San Diego , La Jolla , California , 92093 , USA . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . © 2020 Association for Computing Machinery . 2573 - 0142 / 2020 / 10 - ART161 $ 15 . 00 https : / / doi . org / 10 . 1145 / 3415232 Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . 161 : 2 Ruijia Cheng et al . requests as an open call . No one is guaranteed to get feedback , let alone high - quality feedback [ 55 ] . Prior research indicates that seekers often face social - psychological challenges when seeking feedback [ 20 ] . Novice designers , in particular , often feel apprehensive about even sharing their work for feedback online [ 42 ] . Prior studies of social Q & A and professional development communities have indicated that people generally get more help from online communities when their requests are concise and exhibit positive emotion [ 32 , 43 ] . However , we know less about the strategies used by creators to solicit design feedback or how the community responds to such requests . Unlike health - support networks or professional development help - seeking , online critique communities offer a unique context to study feedback exchange where creators share their work - in - progress and solicit input . Specifically , how do feedback seekers motivate providers to invest time and effort in critiquing their work ? How do these feedback request strategies affect the quality of feedback ? This paper extends prior work on help - seeking and online feedback exchange by identifying and evaluating practices that creators use to seek feedback on creative work . As suggested by Foong et al . ’s framework [ 20 ] , we begin by seeking to understand feedback request strategies — as well as their pros and cons — in a real online community where we can study a range of naturalistic , end - to - end feedback exchange activities . Equipped with a better understanding of the practices , challenges and opportunities of feedback seeking , we discuss the implications for systems designed to support feedback exchange . To better understand feedback exchange in the wild , we conducted two studies within a large and active open online critique community dedicated to design , r / design _ critiques subreddit , 1 which has been active since 2013 and includes some 25k requests for feedback . We chose this community because it allows us to observe a range of feedback interactions performed by users with different levels of experience levels across a variety of design genres . In Study 1 , we interviewed 12 community members to understand the particular strategies creators use to petition for feedback . We learned that many seekers were uncertain about how to best request feedback : what aspects of design context to include , whether to include personal information , and how best to convey specific feedback needs . Based on these insights , in Study 2 , we iteratively developed a qualitative coding scheme to analyze how request strategies affect the feedback provided by the community . We manually coded a random subset of 900 feedback requests from a six - year period of community activity ( 150 posts per year ) for the presence of specific request strategies . Then we created a regression model to investigate the effects of these strategies on the quantity , length , timeliness , actionability and justification of feedback . This analysis indicates that certain strategies , such as signaling one’s newbie status , critiquing one’s own work , and offering design variants to compare , generally yielded a response that was more actionable or justified from the feedback community . While these strategies led to better responses , they only appeared in a small portion of feedback requests ( 6 % , 22 % and 11 % , respectively ) . Findings from both studies suggest implications for future feedback systems . This paper offers the following contributions to the CSCW community : 1 ) We extend the literature on online feedback exchange by shedding light on the strategies creators use to request feedback from an open critique community . 2 ) We conduct two empirical studies , including an interview study that revealed that many seekers have uncertainty about their request strategies , and a qualitative coding and regression analysis that unpacked the relative importance of these strategies . 3 ) Finally , building on these findings , we propose design implications for improving systems to support online feedback exchange . 1 https : / / www . reddit . com / r / design _ critiques Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . Critique Me : Exploring How Creators Publicly Request Feedback in an Online Critique Community 161 : 3 2 RELATED WORK Feedback exchange is an important practice for improving design work and other creations . In project - based learning , feedback not only improves creative work , but also extends both the creators’ and the feedback providers’ domain knowledge and creative skills [ 1 , 12 , 17 , 49 ] . Feedback seeking is also considered a crucial component of self - regulated learning [ 5 , 48 ] , where creators actively test out their work with target audiences . However , in many learning contexts , instructors can struggle to give every student timely and personalized feedback [ 3 ] . Many instructors have adopted peer feedback methods [ 39 ] but have noted that peers often lack the experience to provide meaningful feedback like experts [ 23 , 31 , 56 ] . 2 . 1 Systems to support feedback at scale To help address the growing demand for feedback , the HCI community has offered a number of novel research systems designed to facilitate critique between peers or from an external crowd . Peer feedback systems for classrooms and MOOCs have focused on scaffolding feedback providers with rubrics [ 40 ] and prompts [ 53 ] , reusing feedback from experts [ 46 ] , and having peers compare multiple submissions [ 6 ] . However , research indicates that students often get fatigued and raise questions of fairness when engaging in peer feedback exchange [ 40 ] . To avoid some of these issues , some researchers have explored the potential of tapping into online crowds to produce feedback for learners . Systems to support crowd feedback have focused on guiding the crowd — who are typically not experts in design — to generate effective feedback through a variety of means , including offering micro tasks [ 56 ] , embedding domain knowledge ( e . g . , visual design wisdom ) as pre - authored critique statements [ 41 ] , evaluating feedback using algorithms [ 37 ] and leveraging experts’ inputs [ 62 ] . While most research systems strive to scaffold feedback providers — typically for peers or crowd workers — few focus on supporting seekers to appropriately frame requests for feedback . Foong et al . offer a theoretical framework on online feedback exchange that describes the key socio - psychological factors that affect interactions between feedback seekers and providers [ 20 ] . The framework argues that seekers need more support throughout the feedback exchange process , including at the onset , to help them feel confident enough to make a request and to effectively communicate intent to providers [ 20 ] . Akin to Foong et al . , the learning science literature suggests that feedback seekers in classrooms need to invest effort in crafting a message to solicit feedback , in order to draw attention from providers [ 22 , 45 ] . While it is clear that feedback solicitation plays an important role in open critique communities , less is known about what strategies people use and what challenges people face when seeking feedback . To better understand the practices , challenges , and opportunities of feedback seeking , we analyzed a large , distributed , unstructured open critique community and explored how creators solicit feedback and how different solicitation strategies affect feedback . Insights from this nat - uralistic feedback setting can inform practices for existing online communities as well as future systems that support end - to - end feedback exchange . 2 . 2 Seeking feedback in online communities Online critique communities provide an opportunity to study feedback exchange in the wild . Despite recent innovations on peer feedback and crowd - feedback systems ( e . g . , [ 40 , 41 ] ) , many creators turn to online communities for feedback , in part due to limited access to formal feedback resources , like course instructors and teaching assistants [ 15 , 29 ] . Creators often publicly present their work in online forums and obtain feedback from distributed providers with similar interests . These communities help creators reach a wider audience [ 42 , 55 ] and obtain encouragement from Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . 161 : 4 Ruijia Cheng et al . diverse providers [ 7 ] . Some argue that online communities , compared to other feedback exchange contexts , enable more equal , collaborative and interactive exchanges and focus more on process rather than outcome [ 35 , 60 ] . However , creators also face many challenges when trying to obtain feedback from online com - munities [ 20 ] . Since feedback requests are visible to the whole community , novices creators often feel anxious about sharing early - stage work and self - conscious about their abilities [ 11 , 33 , 34 , 42 ] . While creators have expectations about the quantity and quality of feedback , the resulting feedback is often too little , arrives too late , and fails to provide concrete suggestions for improvement [ 55 ] . Many online critique communities lack details about the expertise of feedback providers that could help seekers trust and assess the relative importance of different points of feedback [ 30 , 60 ] . Although these past studies have revealed limitations on the outcomes of online feedback exchange , little is known about the solicitation process , that is , how creators communicate with feedback providers about their work and their expectations of feedback . Researchers have started exploring how different types of guiding questions ( such as prompting people to brainstorm , critique , improve , and share stories ) affect feedback exchange in classrooms [ 10 ] and how guidance on critique influences feedback in controlled experiments [ 27 ] . However , we know less about how seekers generate prompts for feedback and how the prompts affect providers in more naturalistic settings . This paper focuses on investigating the social dynamics of online critique communities and the strategies seekers use to engage providers . 2 . 3 How help - seeking requests affect community response In other online help - seeking contexts , such as crowdfunding and social questioning and answering ( Q & A ) forums , studies have shown that the request itself can have a big impact on the community’s response . For example , language in help requests that exhibit positive emotions typically yields more positive help from online members [ 43 ] . Polite and concise introductory messages recognizing shared social connections are more likely to solicit help from experts [ 32 ] . Studies on social Q & A sites show that questions edited by human mentors to fit community norms will result in more and higher quality answers [ 21 ] . Respondents tend to have different perceptions of the quality of a question based on its rhetorical type ( e . g . , asking for advice or asking for information ) [ 25 ] . Non - textual features , such as the length of a question , can be used to predict whether people will respond [ 57 ] . All these studies show that strategies concerning the valence and framing of help requests can affect whether and how help providers will respond . While these studies provide key insights , it is still unclear how such help - seeking strategies apply to creative contexts . On top of presenting an explicit help message and question , feedback exchange usually requires additional information , such as design artifacts , sketches , or prototypes [ 18 ] . One of the key challenges of online feedback exchange is the lack of shared context between seekers and providers : providers may not know the background of the artifact , the seeker’s design background , or the kind of feedback that will benefit the seekers the most [ 15 ] . Therefore , seekers have to spend effort communicating the design context and their needs to a whole community of potential feedback providers . We seek to extend the prior work on help - seeking strategies to understand and support online feedback requests on creative work by unpacking how seekers communicate their context and needs around design artifacts , and exploring how these strategies affect feedback outcomes . 2 . 4 Research Questions Specifically , building on this backdrop of online feedback exchange and help - seeking , this paper explores the following research questions : Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . Critique Me : Exploring How Creators Publicly Request Feedback in an Online Critique Community 161 : 5 • RQ1 : what information do community members consider important to present when request - ing feedback and responding to others’ requests ? • RQ2 : how do specific strategies for requesting feedback relate to the quality , quantity and timeliness of feedback responses ? We explored RQ1 through an interview study with 12 diverse participants in r / design _ critiques to shed light on emerging strategies for requesting feedback . We investigated RQ2 through a qualitative coding analysis to categorize and explore the prevalence of different request strategies . Then , we apply a regression analysis to examine the relationship between specific strategies and the resulting feedback . 3 EMPIRICAL SETTING We chose to investigate the r / design _ critiques subreddit 2 , which is part of Reddit . com , a general - purpose website comprised of social news and discussion forums on different topics . This community allows anyone to publicly share their work through text - based forum posts ( and by including embedded URLs to their work ) and receive feedback in the form of public comments . Created in 2010 , the community has been popular and active , with 45 , 557 followers as of the time of writing this paper , and more than 3 , 500 new posts per year since 2013 ( Fig . 2 ) . The community is open to all levels of members at no cost , allowing us to observe a variety of feedback practices . An example thread of feedback exchange from the community is included in Fig . 1 . Fig . 1 . Example thread of feedback exchange in r / design _ critiques . The top post is a feedback request , containing a title ( bolded ) , a url to the design , and description . The request is followed by two comments , which are feedback responses from two different feedback providers . To protect users’ privacy , we masked the identities and slightly modified the wording in each post so they cannot be web - searched . 3 . 1 r / design _ critiques as representative of feedback exchange around creative work A number of features make this community an ideal site to study emerging feedback request strategies . Notably , this community is dedicated to feedback exchange across a range of design 2 https : / / www . reddit . com / r / design _ critiques Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . 161 : 6 Ruijia Cheng et al . domains . Our analysis in Section 5 . 2 verified that members post a wide range of different design artifacts , including web and mobile applications , graphic designs , physical prototypes , animations , etc . We chose it over other communities that focus on specific design domains ( e . g . , UX design [ 35 ] ) because it gave us a chance to study feedback exchange across a wider range of design genres . We recognized that , despite the plurality of its design genres , the community still largely revolves around designs that have a visual component . Our findings and resulting implications may not generalize to feedback exchange activities on all domains of design . Furthermore , we chose this community because it is not a membership - based platform nor a professional site for designers . Professional - oriented communities ( e . g . , dribbble . com ) tend to be used more for showcasing designs , resulting in rare instances of constructive feedback ( usually just praise ) and less active contributions from novice designers [ 42 ] . r / design _ critiques supports more meaningful exchanges and feedback activities among all levels of users . Its community guidelines , in particular , advocate supporting amateur designers to get feedback on works - in - progress . 3 With all these unique features , r / design _ critiques allows us to observe a range of feedback interactions between users with different experience levels in a variety of design genres . 3 . 2 Community data collection We collected and analyzed feedback exchange in r / design _ critiques from six years of activity . We used the Reddit API for Python to collect all the posts created after January 1 , 2013 and before October 4 , 2018 and their associated comments and metadata in the r / design _ critiques subreddit . Reddit has a policy that posts will be “closed” to comments and edits after 180 days . 4 Our dataset only includes posts that have been “closed” to activity ; newer posts are still active and getting responses , which could skew our results . While the community was founded in 2010 , we only analyze posts since 2013 , when the community reached a stable level of posting activity , as shown in Fig . 2 . Since r / design _ critiques only allows users to share designs through external URLs ( no embedded videos and pictures are allowed in posts ) , we excluded posts and associated comments without any URLs ( 1 . 3 % of the posts or 339 total ) , as those posts were likely not requesting feedback on a design artifact . This helped to ensure our analyses focus on actual feedback requests , reducing the risk of including false positives . We also removed all reply comments made by the post authors themselves to focus on feedback from others in the community . This results in a dataset with 24857 posts ( including a title , body , author , and date ) and 98475 comments ( body , author , and date ) . 3 . 3 Preliminary observation : less feedback per request over time The community is generally active and responsive to feedback requests . Of all 24857 feedback requests in this analysis , the majority ( 90 . 2 % ) received some feedback . On average , requests for feedback received 4 . 0 pieces of feedback . Only 2438 ( 9 . 8 % ) requests did not receive any feedback from the community ( unanswered ) . During 2013 to 2018 ( Fig . 2 ) , the community was steadily active with on average 4142 . 83 requests per year ( sd = 448 . 5 ) . Despite the large number of requests , we found a continuous decrease in the amount of feedback received by a single request on average over the six years as seen in Fig . 3 . Compared to 2013 , when a request received 4 . 63 pieces of feedback on average , in 2018 , a request only received 3 . 57 pieces of feedback ( a decrease of 23 % ) . Furthermore , the percentage of unanswered requests each year doubled from 7 . 24 % in 2014 to 14 . 74 % in 2018 . Notably , there are no obvious changes in requesting behaviors over the years : the length of requests and the percentage that include additional details beyond a URL remained the same . All these changes point to the importance of developing strategies 3 https : / / www . reddit . com / r / design _ critiques 4 https : / / www . reddit . com / r / help / comments / az320v / what _ is _ an _ archived _ post Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . Critique Me : Exploring How Creators Publicly Request Feedback in an Online Critique Community 161 : 7 to petition for feedback within a community that is becoming steadily less responsive to feedback requests . Fig . 2 . The r / design _ critiques community started in 2010 and plateaued at around 4000 new posts per year . Our analysis focuses on the years since the initial growth period ( in the orange frame ) . Fig . 3 . Average number of feedback responses per request from 2013 - 2018 . Seekers are getting less feed - back over the years . 4 STUDY 1 : COMMUNITY MEMBERS’ OPINIONS ABOUT REQUESTING FEEDBACK To study how seekers compose feedback requests , we first interviewed 12 active members of r / design _ critiques . We focused on exploring RQ1 to understand what members regard as important when requesting feedback and when responding to others’ requests . 4 . 1 Method We conducted semi - structured interviews with 12 users ( 3 females , 9 males ) who are active on the r / design _ critiques subreddit . We recruited them through purposeful sampling from users who have posted feedback requests to the community . Each participant was compensated $ 20 for participating in the study . Among the 12 participants , the average number of posts is 12 . 64 ( min = 8 , max = 23 ) and the average years of experience in this subreddit is 3 . 46 years ( min = 0 . 5 years , max = 5 years ) . Our participants consist of 6 professional designers , 5 freelancers , and 1 student , of whom 4 self - reported as expert designers , 6 as intermediate designers , and 2 as novices in design . We report more detailed characteristics in Table 1 . All participants have both provided feedback to others and requested feedback from the community . Most participants reported that they began using the platform early in their design career due to a lack of feedback resources in real life . Over the years , they moved from mainly posting requests to providing more feedback for other members in the community . Each interview lasted 30 - 60 minutes through either online conference calls or text - based synchro - nous chat ( as requested by 1 participant ) . Interview questions focused on participants’ experiences , both for requesting and generating feedback in the community . Participants were asked to 1 ) reflect on strategies of asking for feedback by reviewing their previous requests , 2 ) to recall previous experience of providing feedback to others’ requests , and 3 ) to critique the most recent posts to the community . We followed a thematic analysis procedure to analyze the interview data . Three researchers transcribed the interviews and then collaboratively constructed an affinity diagram to identify common themes across interviews . In this process , they first annotated lines of transcripts with notes for possible themes , then iteratively merged and synthesized a final set of themes , which are presented in the interview results . Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . 161 : 8 Ruijia Cheng et al . Table 1 : Profile of interview participants . 4 . 2 Results In this section , we first reveal how creators in the community present contextual information and convey needs in feedback requests , which we later compare and contrast with what they like to see in feedback requests when playing the role of feedback providers . 4 . 2 . 1 Seekers often provide design ( but not personal ) details . Participants ( 9 / 12 — 9 out of 12 participants ) usually prefer to provide some background information about their designs when requesting feedback in the community , commonly what the design will be used for . They believe that stating the purpose addresses the importance of the project , attracts attention from the community , and indicates expectations for the depth and scope of feedback . One participant described how they stated the purpose of a personal design practice as an album cover for an actual band in order to make his project look important to the community : “If you tell them I’m making this for fun , people don’t take it as seriously as an actual item that consumers are going to see . ” ( P9 ) Notably , this participant perceives only designs set for use in professional settings as important and attention - grabbing in the community . On the contrary , participants ( 9 / 12 ) seldom talk about themselves in the requests . A big concern regarding additional information , particularly personal information , is that it may prevent them from receiving “general neutral feedback without being influenced by my skill level or anything . ” ( P3 ) Exposing feedback providers to the feedback seeker’s personal information may alter how they approach the feedback : “ ( Personal information ) changes the way that people frame the content in their mind . Instead of thinking about the questions they are trying to answer and the quality of the work , they are thinking about ‘who is this person ? ’” ( P11 ) Seeker’s disclosure of personal information such as demographics and experience level may distract providers and create the potential for biased feedback . 4 . 2 . 2 Seekers prompt for specific feedback , yet still want comprehensive critique . When posting designs in the community , some participants ( 6 / 12 ) will point to specific parts of the design Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . Critique Me : Exploring How Creators Publicly Request Feedback in an Online Critique Community 161 : 9 that they need feedback on . They believe specific questions lead to helpful feedback by guiding feedback providers to pay special attention to the parts of the design that need the most help . Some participants talked about the strategy of “self - critique” , or critiquing their own design , with the hope of guiding and eliciting thoughtful reaction from providers : “ ( I’ll include ) what I think some issues are , and what might be or might not be working . Because if you were to just post an image with no content , nobody would know what they’re looking at or what they’re supposed to be looking for . ” ( P3 ) Conversely , others ( 5 / 12 ) believe in keeping their feedback request general to the community , refraining from asking specific questions or pointing to any specific aspect of their design . They believe that avoiding prompts “doesn’t rule out any ideas people might want to give” ( P3 ) , and will lead feedback providers to express “natural first impressions” ( P11 ) . They hope that allowing feedback providers to explore every aspect without any guidance or constraints will result in a comprehensive evaluation of their designs by allowing people “to be able to maneuver within them to create as much value as they think that they can . ” ( P2 ) 4 . 2 . 3 Seekers want expert feedback , but avoid explicitly requesting it . Participants generally desire feedback from people with deep expertise in design ( 8 / 12 ) . They want feedback from experts who have the skills and capability to provide knowledgeable and reliable comments . They also believe that people with more experience in the community can deliver their critiques in a more meaningful and constructive manner : “Constructive feedback usually comes from people who’ve been around a bit longer in the community and also have a lot more experience themselves . ” ( P12 ) However , despite this desire for experienced community members to critique their designs , none of the participants would explicitly ask for expertise or experience in their request . Many expressed how asking for expert feedback goes against community values that no one should be excluded from presenting opinions : “I hope to get an expert in the field , but I don’t say it in the post explicitly . That would be super rude to do and would likely make the post heavily downvoted . ” ( P4 ) Therefore , participants would not target a specific group in requests on the belief that this will lower their chances of getting feedback . As a result , their feedback requests may not reach the specific audience that could potentially give them the most effective input . 4 . 2 . 4 Providers wanted more contextual information . When providing feedback for others’ designs , many participants ( 7 / 12 ) sought more details around the design context , including how the design was made and the desired outcomes , so that they will be able to give more tailored and actionable suggestions . Participants like to seek and give critiques in areas that they feel qualified to provide feedback on , based on their own levels of confidence with their own design skills . As one participant stated , “If I’m good at Illustrator and I see his issue with Illustrator , I can help him” ( P8 ) . Background information , such as what techniques were used in making the design , can attract feedback providers correspondingly . Besides basic background , participants want to see the seekers’ rationale for design choices , in order to provide more effective and justified feedback : “I need to see what their reasons are and then counter - argue their reason if I’m going to apply my opinion” ( P1 ) . Also , participants want to know the seeker’s thought process on the design in order to put themselves in the shoes of the seekers : “I just want to find what was this participant thinking while making this because I might want to go through that process myself ( when writing feedback ) ” ( P8 ) . By looking into the other person’s thought process , participants project themselves into the situation and imagine what direction they would like to go , thus generating more tailored feedback . Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . 161 : 10 Ruijia Cheng et al . 4 . 2 . 5 Providers try to empathize with seekers . Participants discussed how they are more responsive to requests that elicit empathetic emotion by containing personal experiences . Humanizing a post contributed to a feeling of an increased connection to the seeker : “It does help people empathize or at least see where you’re coming from . . . I think that if you describe who you are , that lets people step into your shoes . ” ( P11 ) Participants described how they are more receptive to self - proclaimed novice designers in particular ( 5 / 12 ) . Experienced participants relate back to their own experience as someone new to the field and desperate for feedback . Giving feedback to novices is also easier for less experienced members , because the feedback standard for novices is lowered by default . For example , P8 , who self - identified as a novice but slightly above a beginner’s level , is willing to provide opinions to people who are “very early on , ” because “that’s the kind of people I can help the most because they’re close to my level . ” ( P8 ) On the other hand , providers do adjust their feedback based on the proclaimed personal experi - ence of the seeker . For example , more experienced providers may treat seeker who claimed to be a novice with extra patience : “If the post is from someone who’s kind of new , they’re not confident in their work , I’ll be a little more positive and I’ll be less critical . ” ( P9 ) In part , this confirmed the seekers’ concern about disclosing their personal status in requests : providers do get influenced by what kind of person they are , and this bias will be reflected in feedback . 4 . 2 . 6 Providers prefer specific , not vague feedback prompts . When giving feedback , participants value specific feedback requests , while vague and broad questions are less preferable . Participants like to see descriptions of specific design problems : “A lot of people get a lot of super open - ended questions . . . What would create the most value for ( them ) ? What are some areas that they specifically want feedback on ? I am much more prone to click on the specific questions for sure . ” ( P2 ) Often times , a design has multiple components . Participants like to have pointers to specific parts of the design that need feedback because navigating through every aspect of the design requires a lot of time and effort . This allows for more focused critique : “People who will just post like , ‘critique my portfolio’ , and the portfolio is 15 different things . It’s tough to critique and it’s not defined [ clearly ] what they want . ” ( P8 ) Participants prefer responding to feedback requests that contain specific and actionable tasks , such as choosing between variations of a design : “If someone just says , ‘please critique my website’ , I probably won’t critique your website . If someone says , ‘what do you think about these two variations of a home feed ? ’ Then I’m like , sure , I’ll weigh in on that . ” ( P2 ) By knowing exactly what is needed , feedback providers seem more willing to invest in someone else’s design . 4 . 2 . 7 Providers avoid lengthy and complicated requests . When giving feedback , participants reported that they spend as little as “10 to 15 seconds” ( P1 ) processing information about the design and deciding whether they will invest in giving feedback . Requests that appear complicated and rambling at a first glance will deter feedback providers ( 8 / 12 ) : “I think it works better if you very quickly let people know what the logo is about instead of them having to read six or seven lines of text to know what it’s about . ” ( P12 ) Participants may , in fact , avoid responding to requests if they are not clear and easy to understand . Many mentioned that well - structured requests with listed information help them obtain the essential information quickly and stay focused . Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . Critique Me : Exploring How Creators Publicly Request Feedback in an Online Critique Community 161 : 11 4 . 3 Study 1 Discussion Through an interview study with 12 members of this subreddit community , we surfaced a set of factors that members deemed important when requesting feedback and responding to requests . Our analysis has uncovered four key tensions that seekers grapple with when requesting feedback : 1 ) how to present the design context , 2 ) whether to include personal details , 3 ) whether to request specific or general feedback , and 4 ) whether to explicitly request expert input . Here we describe each tension and unpack why participants feel uncertainty about the effectiveness of using these strategies in an open critique community : 4 . 3 . 1 How to present design context . Both feedback seekers and providers expressed a need for detailed design context ( background and design rationale ) in order to support effective feedback exchange . This echoes prior literature that stresses the importance of shared context for supporting effective online critique [ 15 ] . In particular , we found that while seekers focused more on the design outcome ( e . g . , information on what the design should be ) , providers wanted more context around the designer’s process . They appreciated information about how a design was made and the seeker’s own reflection on design choices . Learning science research suggests that feedback about process is more helpful than feedback about the outcome [ 26 ] , but we found in practice , most seekers do not describe their design process or provide process - related prompts as part of their request . While rich information about design context is generally appreciated , lengthy descriptions may preclude providers from engaging in the first place . This finding is corroborated by a design principle from Kraut & Resnick ( 2012 ) [ 38 ] , where a concise call for contribution will result in more compliance from a community than a complex one , because most members of an online community do not have the patience to comprehend a complex message . Therefore , tension exists between what and how much context to provide that can lead to effective feedback . 4 . 3 . 2 Whether to include personal details . Although self - disclosure of academic and professional background has been found to be common and helpful in a community where members seek advice for career development or social support [ 36 ] , we found that when seeking design feedback , people prefer to de - emphasize their personal background . Participants expressed a desire for quality , unbiased feedback and worried that revealing their identity and experience would detract from focusing on the design . While our finding echos previous work that indicates social anxiety can deter novices from even asking for feedback [ 42 ] , we also heard that seekers worry about the negative impact of disclosing personal details for fear of biasing providers . On the other hand , from a provider’s perspective , participants wanted to see more information about the seeker in requests , especially indicators of experience level , in part to emotionally connect with the seeker . This perceived connection is important for motivational reasons [ 32 , 38 ] , since the community operates on a volunteer model , and providers need an incentive to help out strangers . 4 . 3 . 3 Whether to request specific or general feedback . Several participants expressed concern about scaffolding providers with specific instructions for feedback or including a critique of their own design , worried that this might preclude more comprehensive feedback . Previous studies also show that breaking down reviews into micro tasks can result in both positive and negative impacts on feedback [ 27 , 41 ] . On the other hand , from a provider’s perspective , participants talked about how they prefer to respond to very specific requests , where seekers include an explicit task . 4 . 3 . 4 Whether to explicitly request expert input . Our interview participants expressed desire for feedback from more experienced designers , hoping to get constructive , thoughtfully delivered feedback . Prior work also found that getting the attention and commitment from specialists in a large community with diverse design topics can be difficult [ 42 , 55 ] , even if they want to help . Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . 161 : 12 Ruijia Cheng et al . However , our participants also expressed concerns that explicitly requesting an expert would go against community norms , as it seems to elevate opinions from some members over others . Together these uncertainties indicate that community members have nascent strategies about how to request feedback , but they are not certain how the strategies actually play out . Each strategy may lead to feedback that is effective in some aspects , yet unsuccessful in other ways . 5 STUDY 2 : HOW REQUEST STRATEGIES AFFECT COMMUNITY FEEDBACK Study 1 identified a set emerging themes of strategies for requesting feedback as well as uncertainty around their perceived effectiveness . In Study 2 , to further explore the prevalence and impact of these strategies on the community’s critique behavior , we conducted qualitative coding and prepared a regression analysis . We focused on exploring RQ2 to understand how request strategies affect the quality , quantity , and timeliness of feedback responses . First , we qualitatively coded strategies presented in request posts , building on the themes that emerged in Study 1 . We then conducted a regression analysis to study relationships between the request strategies and the resulting feedback . 5 . 1 Method 5 . 1 . 1 Development of Coding Scheme . In order to create a dataset that would be pragmatic for our team to analyze and yet sufficient for a detailed content analysis , we selected 900 posts from the corpus of 24857 posts used in Study 1 by randomly sampling 150 posts from each year of the 6 - year time span . Then , we manually deleted those that were unrelated to design feedback - seeking or had invalid external links to designs . The remaining 879 posts ( 3 . 5 % of total posts ) and their corresponding 3632 feedback comments served as the dataset for this analysis . We iteratively developed a coding scheme that describes the feedback request strategies in the community based on themes emerging from Study 1 . Two researchers first independently looked for strategies from 60 sampled posts , specifically on how feedback seekers communicate their personal details , the context of the design , and prompts for guiding the feedback providers . Then the two researchers got together to compare and discuss the strategies they identified , merged and updated the definitions of some strategies , then re - coded the 60 posts again . They repeated these steps for three rounds until they reached a high inter - rater reliability ( Cohen’s Kappa > = 85 % ) on coding all the strategies . This iterative approach resulted in 7 strategies for feedback request ( see Table 2 ) : for including personal details , we coded for whether request includes Signaling Novice status ; for presenting the design context , we coded for whether the request provides Description of design outcome , Rationales for design choices , and Self - Critique on the design ; for prompting for feedback , we coded for whether the request presents Variants of a design for comparison , and gives either Specific Pointers to focus the critique or General Prompts . We did not see requests for expert input in our sample , so we did not code it despite uncovering this potential strategy in study 2 . Table 2 shows the description of the 7 strategies and example posts that use each strategy . After identifying and coding strategies in the 60 posts , the two researchers then divided and coded the remaining posts independently . All 7 strategies were binary - coded in each post for their presence or absence . We also coded the type of design presented for feedback in each post ( i . e . , Websites , graphic design ) . 5 . 1 . 2 Regression Variables and Analysis . To investigate how the features of feedback requests influence the resulting feedback , we built regression models with the binary - coded strategies as independent variables ( IVs ) . We also added the length of the request as a covariate in the model . Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . Critique Me : Exploring How Creators Publicly Request Feedback in an Online Critique Community 161 : 13 Table 2 : Coding scheme for feedback request strategies As suggested by Q & A literature [ 58 ] , the length of a question can affect the length and quality of answers , so we controlled for the factor to help isolate the effects of request strategies . We analyzed several features of the resulting feedback as dependent variables ( DVs ) and created a different regression model for each . In this study , we consider the request posts as the unit of analysis and assume all replies to a post contain feedback . We examined a range of surface features : average length of feedback in characters , quantity of feedback ( number of replies ) , and timeliness of feedback as the waiting time for the first feedback ( in hours ) . In addition , building on prior work , we calculated two text - based content measures , actionability and justification . Actionable feedback offers concrete suggestions for how to improve the design , while justified feedback backs up the suggestions with evidence and reasoning . These two metrics are commonly used to evaluate design feedback in prior studies [ 10 , 37 , 47 , 51 , 62 ] , and also build on our interview findings in Study 1 . Descriptive statistics for the DVs are shown in Table 3 . Since we had 3632 feedback comments , we used a computational approach to evaluate feedback actionability and justification , based on work by Krause et al . [ 37 ] . We calculate actionability based on the extent the feedback provides directive and suggestive content , operationalized as the ratio of non - indicative ( command or suggestions ) and indicative ( only stating a fact ) sentences determined Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . 161 : 14 Ruijia Cheng et al . Table 3 : Descriptive statistics of the 5 DVs ( per feedback request post ) . The unit of analysis is a post . Measures of feedback length , actionability , and justification are the aggregated averages on all feedback comments received by a post Table 4 : Example feedback replies with calculated scores for actionability and justification by the pattern . en natural language toolkit . 5 Justification is operationalized by calculating the ratio of sentences containing words indicating reasoning ( e . g . , because ) over all words in a comment [ 24 ] . We determined the aggregated average for both measures on all feedback comments for a particular request post ranging from 0 ( not actionable or justified at all ) to 1 ( most actionable or justified ) . In Table 4 , we provide some examples of feedback with different actionability and justification scores . For our regression analysis , we used a Generalized Linear Regression with Gaussian distribution and log link [ 2 ] for the continuous and lognormal DVs ( including delta lognormal [ 19 ] ) : length of feedback , waiting time , actionability , and justification . We applied a Negative Binomial regression model for the count DV ( quantity of feedback ) . We also analyzed the relationship between strategies and the length of the request ( Request Length in Table 5 ) . In all the models , we included random 5 https : / / github . com / clips / pattern Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . Critique Me : Exploring How Creators Publicly Request Feedback in an Online Critique Community 161 : 15 effect variables for the year of the post and the type of design ( web / mobile application , graphic design , animation , physical design or other ) , as well as fixed effects for the binary values of the codes . A correlation test indicated no obvious correlation ( | r | < 0 . 3 ) between codes , so we can assume that all the codes influence the DVs independently . 5 . 2 Results First , we summarize what kinds of design artifacts are most commonly posted to the community for feedback . Web / mobile application and graphic design ( poster , visual , logo , etc . ) were the two most common designs presented for feedback in the community ( 49 . 4 % and 41 . 8 % respectively ) , with animation ( videos , GIFs , etc . ) , physical design ( clothing , 3D models , etc . ) and other designs covering the remaining 8 . 8 % . Table 5 summarizes each DV for each of the request strategies . We found that a majority of requests ( 89 . 0 % ) present design context , but rarely contain reasoning or narratives about the design process ( 13 . 4 % ) . While many explicitly prompted for feedback in the request ( 87 . 7 % ) , more than half of them only included general prompts without any specific scaffolds for feedback . In Table 5 , we report the effect sizes of the fixed effects ( strategies as DVs and length of request as control ) of our regression models . The effect sizes are the ratio change of the DV by increasing an IV by a unit while holding other variables constant , calculated by exp ( regression coefficients ) ( effect size > 1 means the presence of the code leads to an increase in the DV , effect size < 1 means a decrease in the DV . The percentage of increase / decrease is equal to the absolute difference with 1 ) . We only regard IVs as meaningful ( bolded ) if they led to statistical significance and where we see at least a 10 % change in the DVs . Table 5 : Effect Size ( ratio change of the DV when a strategy is presented ) and significance of each strategy for each DV , including the quantity of responses , the timeliness of responses , the length of an average response , as well as the degree of Actionability and Justification in responses ( calculated by an automated language model ) . Significance codes : 0 . 001 * * * , 0 . 01 * * , 0 . 05 * , 0 . 1· ( marginally significant ) Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . 161 : 16 Ruijia Cheng et al . 5 . 2 . 1 Signaling novice led to better justified feedback . Seekers that explicitly mention their beginner status received 86 % ( p = 0 . 001 ) more indicators of justification in the feedback . In addition , they also waited 41 % ( p = 0 . 091 ) less time for the first comment — only marginally significant but notable . Despite the value of this strategy , seekers only disclosed a novice status in 6 . 1 % of posts . 5 . 2 . 2 Critiquing one’s own design in request resulted in more actionable feedback . When a seeker included a critique of their own design in the request , it yielded 14 % ( p = 0 . 025 ) more actionable feedback . This strategy led to 20 % ( p = 0 . 067 ) shorter wait times for the first response — also marginally significant . Despite its benefits to feedback , seekers only offered self - critique on a small number of requests ( 21 . 8 % ) . While we found community members commonly include descriptive information on what the design is about in their feedback requests , elaborating about the design itself resulted in 31 % ( p = 0 . 007 ) fewer sentences devoted to justifying the feedback . Similarly , the strategy of providing process - level reasoning behind design choices did not lead to better feedback either . 5 . 2 . 3 Showing variants yielded faster and more justified feedback . Strategies for directing feedback provides , such as providing design variants , also provided dividends . When multiple variations were presented for critique , the first feedback came in 41 % faster ( p = 0 . 017 ) , and the feedback contained 65 % more justifications ( p = 0 . 002 ) . However , it also led to feedback that was 21 % shorter in length ( p = 0 . 04 ) and possibly less actionable , as indicated by marginal significance . Nevertheless , scaffolding providers with specific instructions on feedback is rare : only 11 . 2 % provided design variants for choice and critique . Similarly , only 14 . 8 % requests provided a specific pointer to some aspect ( e . g . , style , color scheme etc . ) that needs feedback , although this particular strategy did not yield measurable impacts on the community’s feedback . 5 . 3 Study 2 Discussion The literature on pedagogical help - seeking [ 22 , 45 ] and online feedback exchange [ 20 ] suggest that not only the feedback provider , but also the feedback seeker plays an important role in guiding the feedback exchange process . This paper deepens this insight by exploring the strategies creators use to seek feedback in an open critique community . Through interviews with community members in Study 1 , we heard members express uncertainty about how certain request strategies affected community response . Study 2 developed a qualitative analysis to code for the presence or absence of request strategies ( e . g . , providing a prompt ) and then built a regression model to measure the relative impact on the speed , quantity , and quality of feedback from the community . Our results show that the language people use to request feedback within an open critique community can significantly affect the speed and quality of responses . On the whole , requests fare better when signaling newbie status , offering self - critique as a way to provide context around design choices , and prompting the community with specific design variants to compare . However , these specific strategies are only used by a small portion of the community ( 6 . 1 % , 21 . 8 % , and 11 . 2 % respectively ) . Additional details on design description and rationales , though commonly present in requests , do not significantly or positively impact the quality of the community response . 5 . 3 . 1 Designers get better feedback when they claim to be a newbie , but most hesitate to share their status . We found that seekers who highlighted their novice status in the request got more justified feedback . This triangulates with insights from Study 1 where providers discussed how they pay more attention to and invest time for novices , largely because they feel they had been at the same position in the past . This relates to prior work that shows people are more likely to make contributions for people that share an identity - based bond [ 38 ] . The marginally faster responses could signal the community’s desire to help up - and - coming members . The more elaborate Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . Critique Me : Exploring How Creators Publicly Request Feedback in an Online Critique Community 161 : 17 justifications for the feedback perhaps indicate an effort to provide additional scaffolding for less experienced designers . While these effects can be seen as indicative of a healthy critique community , we find only about 6 % of requests mention the seeker’s status . We learned from Study 1 that novices worry that signaling their newbie status will lead to a lower standard or watered - down critique . This very low rate of novice - signaling in design feedback request messages , opposed to common self - disclosure behaviors in communities focusing on career development [ 36 ] , surfaces a potential perception gap between different roles in a feedback community . Experienced providers feel a connection and willingness to help novices , however , novices feel socially anxious to share their newbie status , especially in an open forum [ 42 ] . 5 . 3 . 2 Self - critiques establish context and surface meta - cognitive thinking . Study 2 found that most creators used a strategy where they presented context around the design . Despite prior literature that points to the importance of providing a description [ 20 ] and articulating design rationale [ 8 ] , we find that adding this information may actually preclude the community from deeply engaging with feedback requests . Nearly 70 % of requests included basic descriptive information about the design . However , the regression analysis discovered that descriptive backgrounds did not seem to improve the community response and actually lead to less justified feedback . A possible explanation is that such surface - level information does not sufficiently prompt reflective thinking among the providers , which is essential for the generation of well - reasoned feedback [ 26 ] . Similarly , requests that discuss the underlying rationales for design choices do not lead to any positive impacts on feedback . Perhaps factual information on the design does not spark deep thinking among providers ; instead , they need to be guided by the designers’ own opinions . On the contrary , self - critiques seem to work better than other strategies for establishing a context for feedback providers . This strategy of reflecting on and surfacing doubts about one’s own design led to more actionable feedback , as well as marginally faster responses from the community . This strategy seems to help feedback providers direct their attention towards issues that are most salient to creators , allowing them to better understand the seeker’s motivation and challenges in their design , thus providing more actionable suggestions . Not only do we observe a better community response , but also this strategy may help seekers interpret community feedback by instigating meta - cognitive thinking around the design space [ 59 ] . On the other hand , presenting self - critique in public requires humility and bravery on behalf of the seeker and may be counter - intuitive to many feedback seekers . While nearly 70 % of seekers included a descriptive account of the design context , only 22 % included self - critique as part of their request . Many researchers have suggested that self - critique practices need to be an ongoing part of the design process [ 52 , 54 ] , and that technology could be designed to guide people to critique one’s own design [ 13 , 44 ] . 5 . 3 . 3 Prompting the community with design variants fuels concrete discussion . Previous work on online groups demonstrated that prompting people with a specific question , rather than an open - ended one , increases the likelihood of getting a response by 50 % [ 4 ] . To explore this strategy more deeply , we compared three ways of promoting feedback providers : offering a very general high - level call for feedback , pointing to specific parts of the design for feedback , and providing design variations to compare and contrast . The regression analysis revealed that the strategy of letting providers choose and critique multiple design variants results in faster and better justified feedback than other ways of promoting . Since providing feedback is an open - ended task , providers might struggle to know where to get started with only a general question . Design variants help to narrow the possible angles for feedback and give providers something substantial to start with , thus helping them to come up Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . 161 : 18 Ruijia Cheng et al . with deeper and better justified feedback . This is corroborated by research that suggests people perform better in problem - solving tasks when they have concrete scaffolds [ 50 ] . Although pointers to specific parts of the design could also help narrow down the angles for critique , design variants may offer the providers with stronger empowerment over their critique , thus resulting in faster and more engaged responses . Research on parallel prototyping [ 16 ] indicates that presenting multiple versions of a design improves how designers communicate because it signals to feedback providers that the creator is open to possibilities and because it helps to frame a conversation around key open dimensions of the design space [ 14 ] . In this online critique context , presenting multiple design variants also seems to lower the barrier for community participation . This strategy led to faster responses , but also shorter and marginally less actionable feedback , which indicates that providers may see these requests as an opportunity to quickly provide meaningful input that does not require a significant time investment . We also see that presenting design variants is an uncommon strategy in this community , only present in 11 % of requests . On one hand , creators may not be aware of the value of this approach , but on the other , creating alternative designs requires additional investment of time and effort , and it may be more difficult to generate alternatives in some domains or to document a design process where different versions may be generated [ 28 ] . 6 DESIGN IMPLICATIONS FOR FEEDBACK SYSTEMS Across two studies , our mixed method investigation has revealed a number of insights on the r / design _ critiques community , as an illustrative case study of open online critique forums . We see that over the course of 5 + years of feedback exchange , participation numbers have reached a steady state , and in the same period , there has been a steady decline in responses to requests for feedback . Coinciding with these overall trends , our interviews explored how community members have developed strategies to help their posts stand out among a long list of other open requests . We heard members express doubts about the efficacy of these strategies and worried about potential counter effects . In our second study , a regression analysis helped to unpack the relationship between strategies and feedback quality , and revealed that the specific strategies of signaling one’s newbie status , critiquing one’s own work , and providing design variants tended to yield a better response from the feedback community . Our findings suggest many opportunities for designing better feedback cultures and systems for online critique . We hope the following guidelines could be useful for developers when designing interfaces to support feedback ( e . g . , [ 40 , 41 ] ) . We also hope our design implications could be adopted by moderators of online communities where feedback exchange happens organically , such as helping them develop prompts and community guidelines for members to post effective feedback - seeking messages . For example , feedback requests that attract high - quality feedback could be curated as example posts in the community ; tips on showing self - reflection and design variants in feedback request could be embedded in the community guidelines ; regular community critique events where novices submit their work anonymously to a panel of experts could be organized to address novices’ concerns about self - disclosure and their needs for expert input . 6 . 1 Online feedback systems could support how seekers compose feedback requests Echoing Foong et al . ’s framework about online feedback exchange , our results show that a key component for providers to generate quality feedback is that seekers need to clearly communicate their feedback needs , as well as motivate feedback providers to pay attention to their message and invest effort in their project . However , our results from the two studies show that the current feedback request input interface — typically a free text box — does little to assist seekers in structuring their requests for feedback . Many end up requesting feedback through a very general Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . Critique Me : Exploring How Creators Publicly Request Feedback in an Online Critique Community 161 : 19 call ( as shown in 41 . 5 % posts in our Study 2 ) , which does not help to guide providers at all . In general , feedback systems should take seekers’ perspectives into account and provide them with scaffolds for framing clear feedback requests . For example , feedback systems could provide hints and reviews — building on approaches like question framing [ 10 ] , cognitive apprenticeship [ 9 ] , and help - seeking message composition [ 32 ] — that offer key principles as creative workers compose their requests . 6 . 2 Online feedback systems could support how seekers reflect on their designs Our investigation found that many of the most effective request strategies are rarely used by seekers . As we identified , including self - critiques in feedback requests is an effective strategy that leads to better feedback , but most seekers do not necessarily know how to carry out such reflection ; future feedback systems could build on this finding to prompt designers to " reflect - in - action " [ 52 ] when writing feedback requests . The request interface could engage with interactive mechanisms , such as an assistant chatbot that provides step - wise instruction on reflective practices , guiding seekers to elaborate their design process and generate opinions on their own design work . Moreover , since reflection about design is usually a messy process [ 61 ] , the request interface could prompt designers to sort out their thinking process and communicate their reflection effectively . Systems could display examples for integrating reflective information into a feedback request , and provide automatic evaluation mechanisms . 6 . 3 Online feedback systems could support how seekers generate and present variations of their design Future feedback systems could also encourage seekers to explore " what - ifs " in their design and effectively present those alternatives in their feedback requests . Request composition interfaces could be designed so that the seekers can easily upload , organize and add explanations for multiple versions of the same design . Furthermore , the system could innovate on helping seekers generate variants on media components of their designs . An example could be an AI - powered feature that allows seekers to easily create multiple versions of a visual component ( e . g . , different color themes ) , and select a subset of them to present for feedback . At the same time , since designers often feel anxious to show their intermediate drafts [ 33 ] , systems could innovate in lowering designers’ psychological burden of presenting the evolving process , perhaps through cultivating community norms or enhancing anonymity of feedback requesting . 6 . 4 Online feedback system could initiate private feedback exchange between novices and experts We learned from Study 1 that novices desire feedback from experts , and experts are usually more than willing to respond to a novice’s request . However , both studies also suggest that novices are reluctant to publicly share their status of being a newbie , nor do they feel it to be appropriate to directly call out to experts for help . Future system could innovate to bridge the gap between novices and experts . Systems could empower seekers , especially those who identified as novices , with a safer , perhaps private or semi - private , channel to share their personal details along side their designs . Systems could go further to manage the attention of feedback providers by more carefully managing who sees a feedback request on a design . Maybe providers only see designs that align with their motivational profile , e . g . , only novice participants who want feedback on web design . This more personalized approach to matching feedback seekers and providers could help lower the barrier to participation , especially for those who might be anxious about sharing design with a large open community [ 42 ] . Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . 161 : 20 Ruijia Cheng et al . 7 LIMITATIONS Our analysis only focuses on a single online critique community . We take caution to generalize our insights too broadly but feel confident in the results , due to the internal consistency across our two investigations and the overlap with prior work . We dove deep into r / design _ critiques because of its popularity and representativeness of many critique forums . Although we intentionally selected this community due to the plurality of designs genres ( website , graphic design , physical prototype , animation , etc ) , it still has primarily focuses on designs with visual components and surely does not cover all creative genres . Therefore , our findings and implications may not generalize to all kinds of designs . For example , the implication about supporting seekers with design variants may not be applicable to domains where variants are difficult to create [ 35 ] . Future work should explore whether similar trends , request strategies , and feedback dynamics occur in other communities . Our interviews asked a range of new and old members of r / design _ critiques to reflect on their experience as a novice coming into the community . For these longer - term participants , their retrospective accounts of being a novice may have faded and may not be as fresh as memories provided by current novices . Our regression analysis offered insights about the relationships between request strategies and resulting feedback , but only correlational . Further experimentation , perhaps via the design and deployment of novel feedback systems , will be needed to isolate key variables and explain causal pathways . Furthermore , a key assumption in our quantitative analysis is that all comments contained feedback . We made this assumption based on the community’s guidelines and initial observations that show that the community is dedicated to constructive feedback exchange . Comments could also contain things that are not explicitly about feedback , such as questions and social interactions . We tried to eliminate threads irrelevant to feedback exchange by only including request posts with URLs ( the only way to link a design artifact ) , but we recognize that some non - feedback comments are part of this analysis . Also , the validity of timeliness as a DV could be affected by the fact that posts are made in different time zones . Since we do not have information on the feedback seekers’ time zones , the timing analysis should be considered preliminary . In addition , although we included hand - coded design types as random effects in our regression analysis , we acknowledged that we could not eliminate all possible confounds related to the characteristics of the designs ; for example , in our analysis we did not consider potential effects of the designer’s current stage of design . While we conducted an extensive data analysis , we did not collect data about members’ status , social ties , and prior interactions with other members [ 38 ] , all known to affect community dynamics and likely also impact feedback exchange . Finally , while our analysis explores key indicators of critique ( e . g . , length , speed , actionability ) , we did not explicitly measure feedback quality or study how designers integrated the feedback into their work . 8 CONCLUSION This paper presented two studies to enrich understanding about feedback exchange in a large online critique community , namely the subreddit r / design _ critiques . In Study 1 , we interviewed 12 community members to understand their strategies and misgivings , revealing uncertainties around how much design context to include , whether seekers should self - disclose , and how best to prompt providers . In Study 2 , we manually coded for specific strategies and used a regression analysis to explore relationships between request strategies and community responses . We find that strategies involving signalling novice status , critiquing one’s own design , and presenting design variants generally result in better feedback , but are also rarely adopted in the community . These insights Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . Critique Me : Exploring How Creators Publicly Request Feedback in an Online Critique Community 161 : 21 offer a number of implications for improving systems and practices to support online feedback exchange communities . ACKNOWLEDGMENTS We thank Brian McInnis , Stephen MacNeil , Xiaotong Xu , Lu Sun , Zijian Ding , Gus Umbelino , Benjamin Mako Hill , Mark Zachry , Jenna Frens , Kaylea Champion and Yinan Xuan for providing feedback to this study . We thank all the participants who participated in our study . This material is based upon work supported by the National Science Foundation under Grant No . 1821618 and Grant No . 1122320 . REFERENCES [ 1 ] Bernadette Blair . 2006 . Perception interpretation impact ; an examination of the learning value of formative feedback to students through the design studio critique . Ph . D . Dissertation . Institute of Education , University of London . [ 2 ] Benjamin M Bolker , Mollie E Brooks , Connie J Clark , Shane W Geange , John R Poulsen , M Henry H Stevens , and Jada - Simone S White . 2009 . Generalized linear mixed models : a practical guide for ecology and evolution . Trends in Ecology & Evolution 24 , 3 ( 2009 ) , 127 – 135 . https : / / doi . org / 10 . 1016 / j . tree . 2008 . 10 . 008 [ 3 ] Donald J Boyd . 2015 . Public research universities : Changes in state funding . In American Academy of Arts and Sciences . [ 4 ] Moira Burke , Robert Kraut , and Elisabeth Joyce . 2010 . Membership claims and requests : Conversation - level newcomer socialization strategies in online groups . Small group research 41 , 1 ( 2010 ) , 4 – 40 . [ 5 ] Deborah L Butler and Philip H Winne . 1995 . Feedback and self - regulated learning : A theoretical synthesis . Review of educational research 65 , 3 ( 1995 ) , 245 – 281 . [ 6 ] Julia Cambre , Scott Klemmer , and Chinmay Kulkarni . 2018 . Juxtapeer : Comparative Peer Review Yields Higher Quality Feedback and Promotes Deeper Reflection . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI ’18 ) . Association for Computing Machinery , New York , NY , USA , 1 – 13 . https : / / doi . org / 10 . 1145 / 3173574 . 3173868 [ 7 ] Julie Campbell , Cecilia Aragon , Katie Davis , Sarah Evans , Abigail Evans , and David Randall . 2016 . Thousands of Positive Reviews : Distributed Mentoring in Online Fan Communities . In Proceedings of the 19th ACM Conference on Computer - Supported Cooperative Work & Social Computing ( San Francisco , California , USA ) ( CSCW ’16 ) . Association for Computing Machinery , New York , NY , USA , 691 – 704 . https : / / doi . org / 10 . 1145 / 2818048 . 2819934 [ 8 ] J . M . Carroll . 2012 . Creativity and Rationale : Enhancing Human Experience by Design . Springer London . https : / / books . google . com / books ? id = _ dZJxlxK0NgC [ 9 ] Allan Collins , John Seely Brown , and Ann Holum . 1991 . Cognitive apprenticeship : Making thinking visible . American educator 15 , 3 ( 1991 ) , 6 – 11 . [ 10 ] Amy Cook , Jessica Hammer , Salma Elsayed - Ali , and Steven Dow . 2019 . How Guiding Questions Facilitate Feedback Exchange in Project - Based Learning . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems ( Glasgow , Scotland Uk ) ( CHI ’19 ) . Association for Computing Machinery , New York , NY , USA , 1 – 12 . https : / / doi . org / 10 . 1145 / 3290605 . 3300368 [ 11 ] Patrick A . Crain and Brian P . Bailey . 2017 . Share Once or Share Often ? Exploring How Designers Approach Iteration in a Large Online Community . In Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition ( Singapore , Singapore ) ( C & C ’17 ) . Association for Computing Machinery , New York , NY , USA , 80 – 92 . https : / / doi . org / 10 . 1145 / 3059454 . 3059476 [ 12 ] Deanna Dannels , Amy Housley Gaffney , and Kelly Norris Martin . 2008 . Beyond Content , Deeper than Delivery : What Critique Feedback Reveals about Communication Expectations in Design Education . International Journal for the Scholarship of teaching and Learning 2 , 2 ( 2008 ) , n2 . [ 13 ] Jelle van Dijk , Jirka van der Roest , Remko van der Lugt , and Kees C . J . Overbeeke . 2011 . NOOT : A Tool for Sharing Moments of Reflection during Creative Meetings . In Proceedings of the 8th ACM Conference on Creativity and Cognition ( Atlanta , Georgia , USA ) ( C & C ’11 ) . Association for Computing Machinery , New York , NY , USA , 157 – 164 . https : / / doi . org / 10 . 1145 / 2069618 . 2069646 [ 14 ] Steven Dow , Julie Fortuna , Dan Schwartz , Beth Altringer , Daniel Schwartz , and Scott Klemmer . 2011 . Prototyping Dynamics : Sharing Multiple Designs Improves Exploration , Group Rapport , and Results . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Vancouver , BC , Canada ) ( CHI ’11 ) . Association for Computing Machinery , New York , NY , USA , 2807 – 2816 . https : / / doi . org / 10 . 1145 / 1978942 . 1979359 [ 15 ] Steven Dow , Elizabeth Gerber , and Audris Wong . 2013 . A Pilot Study of Using Crowds in the Classroom . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( Paris , France ) ( CHI ’13 ) . Association for Computing Machinery , New York , NY , USA , 227 – 236 . https : / / doi . org / 10 . 1145 / 2470654 . 2470686 Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . 161 : 22 Ruijia Cheng et al . [ 16 ] Steven P . Dow , Alana Glassco , Jonathan Kass , Melissa Schwarz , Daniel L . Schwartz , and Scott R . Klemmer . 2011 . Parallel Prototyping Leads to Better Design Results , More Divergence , and Increased Self - Efficacy . ACM Trans . Comput . - Hum . Interact . 17 , 4 , Article 18 ( Dec . 2011 ) , 24 pages . https : / / doi . org / 10 . 1145 / 1879831 . 1879836 [ 17 ] Mukaddes Fasli and Badiossadat Hassanpour . 2017 . Rotational critique system as a method of culture change in an architecture design studio : urban design studio as case study . Innovations in Education and Teaching International 54 , 3 ( 2017 ) , 194 – 205 . [ 18 ] Gerhard Fischer , Kumiyo Nakakoji , Jonathan Ostwald , Gerry Stahl , and Tamara Sumner . 1993 . Embedding critics in design environments . The knowledge engineering review 8 , 4 ( 1993 ) , 285 – 307 . [ 19 ] David Fletcher . 2008 . Confidence intervals for the mean of the delta - lognormal distribution . Environmental and Ecological Statistics 15 , 2 ( 2008 ) , 175 – 189 . [ 20 ] Eureka Foong , Steven P . Dow , Brian P . Bailey , and Elizabeth M . Gerber . 2017 . Online Feedback Exchange : A Framework for Understanding the Socio - Psychological Factors . In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems ( Denver , Colorado , USA ) ( CHI ’17 ) . Association for Computing Machinery , New York , NY , USA , 4454 – 4467 . https : / / doi . org / 10 . 1145 / 3025453 . 3025791 [ 21 ] Denae Ford , Kristina Lustig , Jeremy Banks , and Chris Parnin . 2018 . “We Don’t Do That Here” : How Collaborative Editing with Mentors Improves Engagement in Social Q & A Communities . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI ’18 ) . Association for Computing Machinery , New York , NY , USA , 1 – 12 . https : / / doi . org / 10 . 1145 / 3173574 . 3174182 [ 22 ] Sharon Nelson - Le Gall . 1985 . Chapter 2 : Help - Seeking Behavior in Learning . Review of Research in Education 12 , 1 ( 1985 ) , 55 – 90 . [ 23 ] Elizabeth M Gerber , Jeanne Marie Olson , and Rebecca LD Komarek . 2012 . Extracurricular design - based learning : Preparing students for careers in innovation . International Journal of Engineering Education 28 , 2 ( 2012 ) , 317 . [ 24 ] Trudy Govier . 2013 . A practical study of argument . Cengage Learning . [ 25 ] F Maxwell Harper , Joseph Weinberg , John Logie , and Joseph A Konstan . 2010 . Question types in social Q & A sites . First Monday 15 , 7 ( 2010 ) . [ 26 ] John Hattie and Helen Timperley . 2007 . The power of feedback . Review of educational research 77 , 1 ( 2007 ) , 81 – 112 . [ 27 ] Catherine M . Hicks , Vineet Pandey , C . Ailie Fraser , and Scott Klemmer . 2016 . Framing Feedback : Choosing Review Environment Features That Support High Quality Peer Assessment . In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems ( San Jose , California , USA ) ( CHI ’16 ) . Association for Computing Machinery , New York , NY , USA , 458 – 469 . https : / / doi . org / 10 . 1145 / 2858036 . 2858195 [ 28 ] John Horner and Michael E Atwood . 2006 . Effective design rationale : understanding the barriers . In Rationale management in software engineering . Springer , 73 – 90 . [ 29 ] Julie S . Hui , Amos Glenn , Rachel Jue , Elizabeth M Gerber , and Steven P . Dow . 2015 . Using Anonymity and Communal Efforts to Improve Quality of Crowdsourced Feedback . In Proceedings of the Third AAAI Conference on Human Computation and Crowdsourcing , Elizabeth Gerber and Panos Ipeirotis ( Eds . ) . AAAI Press , 72 – 82 . [ 30 ] Julie S Hui , Matthew W Easterday , and Elizabeth M Gerber . 2019 . Distributed Apprenticeship in Online Communities . Human – Computer Interaction 34 , 4 ( 2019 ) , 328 – 378 . [ 31 ] Julie S . Hui , Elizabeth M . Gerber , and Steven P . Dow . 2014 . Crowd - Based Design Activities : Helping Students Connect with Users Online . In Proceedings of the 2014 Conference on Designing Interactive Systems ( Vancouver , BC , Canada ) ( DIS ’14 ) . Association for Computing Machinery , New York , NY , USA , 875 – 884 . https : / / doi . org / 10 . 1145 / 2598510 . 2598538 [ 32 ] Julie S . Hui , Darren Gergle , and Elizabeth M . Gerber . 2018 . IntroAssist : A Tool to Support Writing Introductory Help Requests . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems ( Montreal QC , Canada ) ( CHI ’18 ) . Association for Computing Machinery , New York , NY , USA , 1 – 13 . https : / / doi . org / 10 . 1145 / 3173574 . 3173596 [ 33 ] Joy Kim , Maneesh Agrawala , and Michael S . Bernstein . 2017 . Mosaic : Designing Online Creative Communities for Sharing Works - in - Progress . In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing ( Portland , Oregon , USA ) ( CSCW ’17 ) . Association for Computing Machinery , New York , NY , USA , 246 – 258 . https : / / doi . org / 10 . 1145 / 2998181 . 2998195 [ 34 ] Yasmine Kotturi and McKayla Kingston . 2019 . Why Do Designers in the “Wild” Wait to Seek Feedback until Later in Their Design Process ? . In Proceedings of the 2019 on Creativity and Cognition ( San Diego , CA , USA ) ( C & C ’19 ) . Association for Computing Machinery , New York , NY , USA , 541 – 546 . https : / / doi . org / 10 . 1145 / 3325480 . 3326580 [ 35 ] Yubo Kou and Colin M Gray . 2017 . Supporting distributed critique through interpretation and sense - making in an online creative community . Proceedings of the ACM on Human - Computer Interaction 1 , CSCW ( 2017 ) , 60 . [ 36 ] Yubo Kou and Colin M . Gray . 2018 . “What Do You Recommend a Complete Beginner like Me to Practice ? ” : Professional Self - Disclosure in an Online Community . Proc . ACM Hum . - Comput . Interact . 2 , CSCW , Article 94 ( Nov . 2018 ) , 24 pages . https : / / doi . org / 10 . 1145 / 3274363 [ 37 ] Markus Krause , Tom Garncarz , JiaoJiao Song , Elizabeth M . Gerber , Brian P . Bailey , and Steven P . Dow . 2017 . Critique Style Guide : Improving Crowdsourced Design Feedback with a Natural Language Model . In Proceedings of the 2017 CHI Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . Critique Me : Exploring How Creators Publicly Request Feedback in an Online Critique Community 161 : 23 Conference on Human Factors in Computing Systems ( Denver , Colorado , USA ) ( CHI ’17 ) . Association for Computing Machinery , New York , NY , USA , 4627 – 4639 . https : / / doi . org / 10 . 1145 / 3025453 . 3025883 [ 38 ] Robert E Kraut and Paul Resnick . 2012 . Building successful online communities : Evidence - based social design . Mit Press . [ 39 ] Chinmay Kulkarni , Koh Pang Wei , Huy Le , Daniel Chia , Kathryn Papadopoulos , Justin Cheng , Daphne Koller , and Scott R . Klemmer . 2013 . Peer and Self Assessment in Massive Online Classes . ACM Trans . Comput . - Hum . Interact . 20 , 6 , Article 33 ( Dec . 2013 ) , 31 pages . https : / / doi . org / 10 . 1145 / 2505057 [ 40 ] Chinmay E . Kulkarni , Michael S . Bernstein , and Scott R . Klemmer . 2015 . PeerStudio : Rapid Peer Feedback Emphasizes RevisionandImprovesPerformance . In ProceedingsoftheSecond ( 2015 ) ACMConferenceonLearning @ Scale ( Vancouver , BC , Canada ) ( L @ S ’15 ) . Association for Computing Machinery , New York , NY , USA , 75 – 84 . https : / / doi . org / 10 . 1145 / 2724660 . 2724670 [ 41 ] Kurt Luther , Jari - Lee Tolentino , Wei Wu , Amy Pavel , Brian P . Bailey , Maneesh Agrawala , Björn Hartmann , and Steven P . Dow . 2015 . Structuring , Aggregating , and Evaluating Crowdsourced Design Critique . In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing ( Vancouver , BC , Canada ) ( CSCW ’15 ) . Association for Computing Machinery , New York , NY , USA , 473 – 485 . https : / / doi . org / 10 . 1145 / 2675133 . 2675283 [ 42 ] Jennifer Marlow and Laura Dabbish . 2014 . From Rookie to All - Star : Professional Development in a Graphic Design Social Networking Site . In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing ( Baltimore , Maryland , USA ) ( CSCW ’14 ) . Association for Computing Machinery , New York , NY , USA , 922 – 933 . https : / / doi . org / 10 . 1145 / 2531602 . 2531651 [ 43 ] Tanushree Mitra and Eric Gilbert . 2014 . The Language That Gets People to Give : Phrases That Predict Success on Kickstarter . In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing ( Baltimore , Maryland , USA ) ( CSCW ’14 ) . Association for Computing Machinery , New York , NY , USA , 49 – 61 . https : / / doi . org / 10 . 1145 / 2531602 . 2531656 [ 44 ] KumiyoNakakoji , YasuhiroYamamoto , ShingoTakada , andBrentN . Reeves . 2000 . Two - DimensionalSpatialPositioning as a Means for Reflection in Design . In Proceedings of the 3rd Conference on Designing Interactive Systems : Processes , Practices , Methods , and Techniques ( New York City , New York , USA ) ( DIS ’00 ) . Association for Computing Machinery , New York , NY , USA , 145 – 154 . https : / / doi . org / 10 . 1145 / 347642 . 347697 [ 45 ] Richard S Newman . 1994 . Adaptive help seeking : A strategy of self - regulated learning . ( 1994 ) . [ 46 ] Tricia J Ngoon , C Ailie Fraser , Ariel S Weingarten , Mira Dontcheva , and Scott Klemmer . 2018 . Interactive Guidance Techniques for Improving Creative Feedback . In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . ACM , 55 . [ 47 ] David J Nicol and Debra Macfarlane - Dick . 2006 . Formative assessment and self - regulated learning : A model and seven principles of good feedback practice . Studies in higher education 31 , 2 ( 2006 ) , 199 – 218 . [ 48 ] Minna Puustinen and Jean - François Rouet . 2009 . Learning with new technologies : Help seeking and information searching revisited . Computers & Education 53 , 4 ( 2009 ) , 1014 – 1019 . https : / / doi . org / 10 . 1016 / j . compedu . 2008 . 07 . 002 Learning with ICT : New perspectives on help seeking and information searching . [ 49 ] Ken Reily , Pam Ludford Finnerty , and Loren Terveen . 2009 . Two Peers Are Better than One : Aggregating Peer Reviews for Computing Assignments is Surprisingly Accurate . In Proceedings of the ACM 2009 International Conference on Supporting Group Work ( Sanibel Island , Florida , USA ) ( GROUP ’09 ) . Association for Computing Machinery , New York , NY , USA , 115 – 124 . https : / / doi . org / 10 . 1145 / 1531674 . 1531692 [ 50 ] Brian J Reiser . 2004 . Scaffolding complex learning : The mechanisms of structuring and problematizing student work . The Journal of the Learning sciences 13 , 3 ( 2004 ) , 273 – 304 . [ 51 ] D . Royce Sadler . 1989 . Formative assessment and the design of instructional systems . Instructional Science 18 , 2 ( 01 Jun 1989 ) , 119 – 144 . https : / / doi . org / 10 . 1007 / BF00117714 [ 52 ] Donald A Schön . 2017 . The reflective practitioner : How professionals think in action . Routledge . [ 53 ] Amy Shannon , Jessica Hammer , Hassler Thurston , Natalie Diehl , and Steven Dow . 2016 . PeerPresents : A Web - Based Systemfor In - ClassPeerFeedback duringStudentPresentations . In Proceedingsof the2016ACMConferenceonDesigning Interactive Systems ( Brisbane , QLD , Australia ) ( DIS ’16 ) . Association for Computing Machinery , New York , NY , USA , 447 – 458 . https : / / doi . org / 10 . 1145 / 2901790 . 2901816 [ 54 ] Oscar Tomico , Joep W . Frens , and C . J . Overbeeke . 2009 . Co - Reflection : User Involvement for Highly Dynamic Design Processes . In CHI ’09 Extended Abstracts on Human Factors in Computing Systems ( Boston , MA , USA ) ( CHI EA ’09 ) . Association for Computing Machinery , New York , NY , USA , 2695 – 2698 . https : / / doi . org / 10 . 1145 / 1520340 . 1520389 [ 55 ] Anbang Xu and Brian Bailey . 2012 . What Do You Think ? A Case Study of Benefit , Expectation , and Interaction in a Large Online Critique Community . In Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work ( Seattle , Washington , USA ) ( CSCW ’12 ) . Association for Computing Machinery , New York , NY , USA , 295 – 304 . https : / / doi . org / 10 . 1145 / 2145204 . 2145252 [ 56 ] Anbang Xu , Shih - Wen Huang , and Brian Bailey . 2014 . Voyant : Generating Structured Feedback on Visual Designs Using a Crowd of Non - Experts . In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 . 161 : 24 Ruijia Cheng et al . Social Computing ( Baltimore , Maryland , USA ) ( CSCW ’14 ) . Association for Computing Machinery , New York , NY , USA , 1433 – 1444 . https : / / doi . org / 10 . 1145 / 2531602 . 2531604 [ 57 ] Lichun Yang , Shenghua Bao , Qingliang Lin , Xian Wu , Dingyi Han , Zhong Su , and Yong Yu . 2011 . Analyzing and Predicting Not - Answered Questions in Community - Based Question Answering Services . In Proceedings of the Twenty - Fifth AAAI Conference on Artificial Intelligence ( San Francisco , California ) ( AAAI’11 ) . AAAI Press , 1273 – 1278 . [ 58 ] Lichun Yang , Shenghua Bao , Qingliang Lin , Xian Wu , Dingyi Han , Zhong Su , and Yong Yu . 2011 . Analyzing and Predicting Not - Answered Questions in Community - Based Question Answering Services . In Proceedings of the Twenty - Fifth AAAI Conference on Artificial Intelligence ( San Francisco , California ) ( AAAI’11 ) . AAAI Press , 1273 – 1278 . [ 59 ] Yu Chun Grace Yen , Steven P . Dow , Elizabeth Gerber , and Brian P . Bailey . 2017 . Listen to others , listen to yourself : Combining feedback review and reflection to improve iterative design . In C and C 2017 - Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition ( C and C 2017 - Proceedings of the 2017 ACM SIGCHI Conference on CreativityandCognition ) . AssociationforComputingMachinery , Inc , 158 – 170 . https : / / doi . org / 10 . 1145 / 3059454 . 3059468 [ 60 ] Yu - Chun ( Grace ) Yen , Steven P . Dow , Elizabeth Gerber , and Brian P . Bailey . 2016 . Social Network , Web Forum , or Task Market ? Comparing Different Crowd Genres for Design Feedback Exchange . In Proceedings of the 2016 ACM Conference on Designing Interactive Systems ( Brisbane , QLD , Australia ) ( DIS ’16 ) . Association for Computing Machinery , New York , NY , USA , 773 – 784 . https : / / doi . org / 10 . 1145 / 2901790 . 2901820 [ 61 ] Yu - Chun Grace Yen , Joy O . Kim , and Brian P . Bailey . 2020 . Decipher : An Interactive Visualization Tool for Interpreting Unstructured Design Feedback from Multiple Providers . In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems ( Honolulu , HI , USA ) ( CHI ’20 ) . Association for Computing Machinery , New York , NY , USA , 1 – 13 . https : / / doi . org / 10 . 1145 / 3313831 . 3376380 [ 62 ] Alvin Yuan , Kurt Luther , Markus Krause , Sophie Isabel Vennix , Steven P Dow , and Bjorn Hartmann . 2016 . Almost an Expert : The Effects of Rubrics and Expertise on Perceived Value of Crowdsourced Design Critiques . In Proceedings of the 19th ACM Conference on Computer - Supported Cooperative Work & Social Computing ( San Francisco , California , USA ) ( CSCW ’16 ) . Association for Computing Machinery , New York , NY , USA , 1005 – 1017 . https : / / doi . org / 10 . 1145 / 2818048 . 2819953 Received January 2020 ; revised June 2020 ; accepted July 2020 Proc . ACM Hum . - Comput . Interact . , Vol . 4 , No . CSCW2 , Article 161 . Publication date : October 2020 .