Variance Reduction in Deep Learning : More Momentum is All You Need . Lionel Tondji 1 2 Sergii Kashubin 3 Moustapha Cisse 3 Abstract Variance reduction ( VR ) techniques have con - tributed signiﬁcantly to accelerating learning with massive datasets in the smooth and strongly con - vex setting ( Schmidt et al . , 2017 ; Johnson & Zhang , 2013 ; Roux et al . , 2012 ) . However , such techniques have not yet met the same success in the realm of large - scale deep learning due to various factors such as the use of data augmenta - tion or regularization methods like dropout ( De - fazio & Bottou , 2019 ) . This challenge has re - cently motivated the design of novel variance re - duction techniques tailored explicitly for deep learning ( Arnold et al . , 2019 ; Ma & Yarats , 2018 ) . This work is an additional step in this direction . In particular , we exploit the ubiquitous clustering structure of rich datasets used in deep learning to design a family of scalable variance reduced opti - mization procedures by combining existing opti - mizers ( e . g . , SGD + Momentum , Quasi Hyperbolic Momentum , Implicit Gradient Transport ) with a multi - momentum strategy ( Yuan et al . , 2019 ) . Our proposal leads to faster convergence than vanilla methods on standard benchmark datasets ( e . g . , CI - FAR and ImageNet ) . It is robust to label noise and amenable to distributed optimization . We provide a parallel implementation in JAX . 1 . Introduction Given a data generating distribution P , a parameterized func - tion f θ ( e . g . a deep network ) and a loss function (cid:96) ( · ) , we consider the traditional risk minimization problem represen - tative of most settings in ML ( Shalev - Shwartz & Ben - David , 2014 ) : θ ∗ = arg min θ E ( x ∼ P ) (cid:96) ( f θ ( x ) ) ( 1 ) The celebrated optimization algorithm for solving this prob - lem in large - scale machine learning is Stochastic Gradient 1 Institute of Analysis and Algebra , TU Braunschweig 2 This work was done during the AI Resideny at Google Brain 3 Google Research , Brain Team . Correspondence to : Lionel Tondji < l . ngoupeyou - tondji @ tu - braunschweig . de > . Descent ( SGD ) ( Robbins , 2007 ; Bottou et al . , 2018 ) . In fa - vorable cases ( e . g . , smooth and strongly convex functions ) , SGD converges to a good solution by iteratively applying the following ﬁrst - order update rule : θ t + 1 ← θ t − µ t g ( x t , θ t ) where x t is an instance drawn from P , µ t is the learning rate and g ( x t , θ t ) is the ( approximate ) gradient . SGD enjoys several desirable properties . Indeed , It is fast : when one has access to the actual gradient , its convergence rate is O ( e − νt ) for some ν > 0 . Also , it is memory efﬁcient , robust , and amenable to distributed optimization ( Bottou et al . , 2018 ; Nemirovski et al . , 2009 ; Dean et al . , 2012 ) . However , we generally do not know the data - generating distribution P ; consequently , we do not have access to the actual gradient . Instead , we resort to empirical risk minimization and rely on a stochastic approximation of the gradient for a given x t and θ t . Unfortunately , the gradient noise due to using the approximate instead of the exact gradient slows down the convergence speed ( which becomes O ( 1 / t ) instead of O ( e − νt ) ) ( Bottou et al . , 2018 ) . It also makes the algorithm harder to tune . Variance Reduction ( VR ) techniques allow us to mitigate the impact of using noisy gradients in stochas - tic optimization ( Roux et al . , 2012 ; Defazio et al . , 2014 ; Johnson & Zhang , 2013 ; Mairal , 2013 ) . Variance reduction methods have been successfully applied to convex learning problems ( Roux et al . , 2012 ; Defazio et al . , 2014 ; Shalev - Shwartz & Zhang , 2013 ; Johnson & Zhang , 2013 ) . However , this success does not readily trans - late to large - scale non - convex settings such as deep learning due to colossal memory requirements ( Roux et al . , 2012 ; De - fazio et al . , 2014 ; Shalev - Shwartz & Zhang , 2013 ) and the use of normalization and data augmentation procedures ( De - fazio & Bottou , 2019 ) . This has motivated the design of novel VR strategies that are better suited for deep learn - ing ( Cutkosky & Orabona , 2019 ; Arnold et al . , 2019 ; Ma & Yarats , 2018 ) . In the next section , we present in more detail Implicit Gradient Transport ( Arnold et al . , 2019 ) and Quasi Hyperbolic Momentum ( Ma & Yarats , 2018 ) , two instances of successful application of VR to deep learning . Large scale datasets used in deep learning problems come with a rich clustering structure ( Deng et al . , 2009 ) . For example , the data can result from the aggregation of several datasets collected by different individuals or organizations ( e . g . , this is typical in the federated learning setting ( Li et al . , 2019 ) ) . When there is an underlying clustering structure a r X i v : 2111 . 11828v1 [ c s . L G ] 23 N ov 2021 Discover in the data , the variance due to gradient noise decomposes into an in - cluster variance and a between - cluster variance . Yuan et al . ( 2019 ) exploit this insight to reduce the between cluster variance in the convex setting by considering every data point as a separate cluster . Unfortunately , such an approach requires storing as many models as data points and therefore has prohibitive memory requirements . Also , it does not consider the use of mini - batches during training . Consequently , it is not directly applicable to deep learning . In this work , we make the following contributions : • We leverage the idea of using multiple momentum terms and introduce a family of variance reduced op - timizers for deep learning . These new approaches ( termed Discover 1 ) improve upon widely used meth - ods such as SGD with Momentum , IGT , and QHM . They are theoretically well - motivated and can exploit the clustering structures ubiquitous in deep learning . • Using simple clustering structures , we empirically val - idate that Discover optimizers lead to faster conver - gence and sometimes ( signiﬁcantly ) improved gener - alization on standard benchmark datasets . We provide parallel implementations of our algorithms in JAX 2 . In the next section , we provide a brief overview of variance reduction methods with an emphasis on Implicit Gradient Transport ( Arnold et al . , 2019 ) and Quasi Hyperbolic Mo - mentum ( Ma & Yarats , 2018 ) , two VR optimizers speciﬁ - cally designed for deep learning . We then discuss ( section 3 ) the clustering structure present in most deep learning prob - lems and how we can leverage it to design scalable variance reduction strategies . The experiments section demonstrates our proposals’ effectiveness and provides several insights regarding their behavior . 2 . Variance Reduction in Deep Learning We consider Variance Reduction ( VR ) in the realm of large - scale deep learning ( Goodfellow et al . , 2016 ) , i . e . when we are in the presence of signiﬁcant amounts of ( streaming ) data , and the parameterized function f θ is a massive deep neural network . Most existing approaches do not naturally scale to this setting . Indeed , dual methods for VR , such as SDCA ( Shalev - Shwartz & Zhang , 2013 ) , have prohibitive memory requirements due to the necessity of storing large dual iterates . Primal methods akin to SAGA ( Roux et al . , 2012 ) are also memory inefﬁcient because they need to store past gradients . Other approaches like SARAH ( Nguyen et al . , 2017 ) are computationally demanding ; they entail a full snapshot gradient evaluation at each step and two mini - batch evaluations . While some recent approaches , such as 1 Deep SCalable Online Variance Reduction 2 https : / / github . com / google / jax stochastic MISO ( Bietti & Mairal , 2017 ) , can handle inﬁ - nite data in theory . However , their memory requirement scales linearly with the number of examples . In addition to these limitations , Defazzio & Bottou ( Defazio & Bottou , 2019 ) have shown that several other factors such as data augmentation ( Krizhevsky et al . , 2012 ) , batch normaliza - tion ( Ioffe & Szegedy , 2015 ) , or dropout ( Srivastava et al . , 2014b ) impede the success of variance reduction methods in deep learning . Despite these negative results , some recent approaches such as Quasi Hyperbolic Momentum ( Ma & Yarats , 2018 ) and Implicit Gradient Transport with tail av - eraging ( IGT ) ( Arnold et al . , 2019 ) have shown promising results in variance reduction in the context of large scale deep learning . In the sequel , we present them in more detail . We ﬁrst present the Momentum or Heavy Ball method since the above approaches and our proposal rely on it . The Momentum or Heavy Ball ( Polyak , 1964 ; Sutskever et al . , 2013 ) method uses the following update rule : v t = βv t − 1 + ( 1 − β ) · g ( θ t , x t ) θ t + 1 = θ t − µv t where v t is called the momentum buffer and β t balances the buffer and the approximate gradient compute at iteration t . SGD with Momentum is widely used in deep learning due to its simplicity and its robustness to variations of hyper - parameters ( Sutskever et al . , 2013 ; Zhang & Mitliagkas , 2017 ) . It is an effective way of alleviating slow convergence due to curvature ( Goh , 2017 ) and has been recently shown also perform variance reduction ( Roux et al . , 2018 ) . The Quasi Hyperbolic Momentum ( QHM ) ( Ma & Yarats , 2018 ) method uses the following update rule : v t = βv t − 1 + ( 1 − β ) · g ( θ t , x t ) θ t + 1 = θ t − µ · [ νv t + ( 1 − ν ) g ( θ t , x t ) ] QHM extends Momentum by using in the update a combina - tion of the approximate gradient and the momentum buffer . When ν = 1 ( resp . ν = 0 ) , it reduces to Momentum ( resp . SGD ) . QHM inherits the efﬁciency of Momentum and also performs VR . In addition , it alleviates the potential staleness of the momentum buffer ( when choosing ν < 1 ) . The Implicit Gradient Transport ( IGT ) ( Arnold et al . , 2019 ) method uses the following update rule : γ t = t / ( t + 1 ) v t = γ t · v t − 1 + ( 1 − γ t ) · g (cid:18) θ t + γ t 1 − γ t ( θ t − θ t − 1 ) , x t (cid:19) w t = β · w t − 1 − µ · v t θ t + 1 = θ t + w t IGT effectively achieves variance reduction by combining several strategies maintaining a buffer of past gradients ( us - Discover ing iterative tail averaging ) and transporting them to equiva - lent gradients at the current point . The resulting update rule can also be combined with Momentum as shown above . 3 . Exploiting Clusters in Variance Reduction The Ubiquitous Clustering Structure Large - scale ma - chine learning datasets come with a rich clustering structure that arises in different ways depending on the setting . For example , when training deep neural networks , we combine various data augmentation strategies , resulting in a mixture with clusters deﬁned by the transformations ( Zhang et al . , 2017 ; Yun et al . , 2019b ; Krizhevsky et al . , 2012 ) . In multi - class classiﬁcation problems , one can consider each class as a separate cluster . Therefore , the overall dataset becomes a mixture deﬁned by the classes . In all these cases , the data is not independent and identically distributed across clus - ters . Indeed , different data augmentation strategies lead to different training data distributions . To make the clustering structure apparent , we can rewrite the minimization prob - lem in equation 1 as a combination of risks on the different clusters . To this end , we denote P n the data distribution cor - responding to the n - th cluster and p n the probability with which we sample from that cluster , with (cid:80) Ni = 1 p n = 1 . We also denote x nt the realization of the data point x t belonging to the cluster n for clarity . The risk minimization problem 1 becomes : θ ∗ = arg min θ N (cid:88) n = 1 p n E ( x ∼ P n ) (cid:96) ( f θ ( x ) ) ( 2 ) While one can pool all the data to apply traditional empir - ical risk minimization 1 , this would ignore valuable prior clustering information . In the next section , we show how , when solving the equivalent problem 2 , we can leverage the clustering information to speed the learning procedure by reducing the variance due to gradient noise . We start with stochastic gradient descent and present a decomposition of such variance , which considers the clustering structure . Gradient noise for SGD with clusters Here , we re - derive the variance of the gradient noise for SGD in presence of clustered data ( Sayed , 2014b ; Yuan et al . , 2019 ) . To this end , we introduce the ﬁltration F t = { θ i < t + 1 } and denote g n ( θ ) = E ( x n ∼ P n ) g ( θ , x n ) for convenience . We assume the loss (cid:96) ( f ( x n ) ) is δ - Lipschitz with respect to θ and the clus - tered risk (cid:96) n ( θ ) = E ( x ∼ P n ) (cid:96) ( f θ ( x n ) ) is ν - strongly convex , that is for any θ 1 , θ 2 it holds : ( g n ( θ 1 ) − g n ( θ 2 ) ) T ( θ 1 − θ 2 ) ≥ ν (cid:107) θ 1 − θ 2 (cid:107) 2 For a given example x nt , the update rule for stochastic gradi - ent descent is θ t + 1 ← θ t − µ t g ( x nt , θ t ) . The gradient noise resulting from this update depends both on the probability distribution on the clusters and the data distribution P n for the considered cluster . For a given example , the gradient noise writes : s t + 1 ( θ t ) = g ( x nt , θ t ) − g ( θ t ) and the gradient noise within the cluster n is : s nt + 1 ( θ t ) = g ( x nt , θ t ) − g n ( θ t ) . The following result bounds the ﬁrst and second moment of the within - cluster variance of the gradient noise for SGD . Lemma 1 The ﬁrst and second order moments of the gra - dient noise s t + 1 ( θ t ) satisfy : E (cid:0) s t + 1 ( θ t ) | F t (cid:1) = 0 and E (cid:0) (cid:107) s t + 1 ( θ t ) (cid:107) 2 | F t (cid:1) ≤ β 2 (cid:107) ˜ θ t (cid:107) 2 + σ 2 where β 2 = 2 δ 2 , ˜ θ t = θ t − θ ∗ and σ 2 = 2 E (cid:0) (cid:107) g ( x nt , θ ∗ ) (cid:107) 2 | F t (cid:1) . We can now decompose the variance of the gradient noise into a sum of within - cluster and between - cluster variance . Lemma 2 Assuming the gradient is unbiased and the vari - ance of the gradient noise within the cluster n is bounded as in shown in lemma 1 , the following inequality holds : E ( (cid:107) s t + 1 ( θ ∗ ) (cid:107) 2 | F t ) ≤ N (cid:88) n = 1 p n σ 2 n (cid:124) (cid:123)(cid:122) (cid:125) in - clustervariance + N (cid:88) n = 1 p n (cid:107) g n ( θ ∗ ) (cid:107) 2 (cid:124) (cid:123)(cid:122) (cid:125) between - clustervariance ( 3 ) Lemma 2 captures the structure of the problem . The LHS represents the variance of the gradient noise . The ﬁrst term of the RHS is the within - cluster variance σ 2 in , and the second term is the between - cluster variance σ 2 bet . In the limit , the mean square deviation of the steady - state de - pends on the in - cluster and between - cluster variances as follows lim sup t →∞ (cid:107) θ t − θ ∗ (cid:107) = O ( µ ( σ 2 in + σ 2 bet ) ) . There - fore when the clustering structure is known and σ 2 in (cid:28) σ 2 in + σ 2 bet , which is our working assumption , we can signif - icantly reduce the overall variance by reducing the between - cluster variance . In the next section , we present an update rule exploiting this fact and generalizing the approach pre - sented in ( Yuan et al . , 2019 ) to the minibatch setting 3 . We also prove its gradient noise properties and convergence 4 4 . Discover Algorithms To re - iterate , we assume we know the clustering structure and the probability p n of observing data from a given cluster n such that (cid:80) n p n = 1 . As we will show in the experiments , this is a realistic assumption , and straightforward design choices such as using labels or data augmentation strategies as clusters lead to improved results . We do not have access to the data distribution given a cluster n . We consider a 3 It is worth noting that Yuan et al . ( 2019 ) have assumed the results in lemma 1 and state lemma 2 without proof . We provide full proofs in the appendix . 4 The proof assumes a smooth and strongly convex setting . Though we consider in the experiments non - convex problems , this assumption may be valid locally in a basin of attraction of the loss landscape of a deep neural network Discover learning setting where at each round t , we observe a batch of examples B t = { x nt } coming from the different groups and sampled according to the mixture distribution induced by the clustering structure . We propose to achieve between cluster variance reduction in minibatch stochastic gradient descent by recursively applying the following update rule : θ t + 1 = θ t − µ | B t | · (cid:88) x nt ∈B t (cid:18) g ( x nt , θ t ) − g ( n ) t + N (cid:88) k = 1 p k g ( k ) t (cid:19) ( 4 ) The update rule stems from the traditional use of control variates for variance reduction ( Fishman , 1996 ) with the ad - ditional trick to use one control variate for each cluster given that the clustering structure is known . In Equation ( 4 ) , each g ( n ) t is an approximation of the actual cluster gradient g n ( θ t ) to which the example x nt belongs , and ¯ g t = (cid:80) Nk = 1 p k g ( k ) t is the average cluster gradient . The gradient noise resulting from the update rule 4 depends both on the probability dis - tribution on the clusters and the data distribution P n of each considered cluster . It can be written as follows : u t + 1 ( θ t ) = 1 | B t | · (cid:88) x nt ∈B t (cid:18) g ( x nt , θ t ) − g ( n ) t + ¯ g t (cid:19) − g ( θ t ) In the sequel , we show how a recursive application of the above update rule ultimately leads to reduced between - cluster variance of the gradient noise u t + 1 ( θ t ) . Before , we ﬁrst state a lemma exposing the properties of the gra - dient noise u t + 1 ( θ t ) . Similarly to Lemma 2 , this results highlights how the variance of the gradient noise decom - poses into an in - cluster and a between - cluster variance . We provide a proof of the lemma in the appendix Section 9 . 1 . Lemma 3 ( gradient noise properties ) Under the same assumptions as in Section 3 , for a batch of size | B t | the gradient is unbiased E [ u t + 1 ( θ t ) | F t ] = 0 . Denoting ˜ θ t : = θ ∗ − θ t , C 1 = 4 δ 2 and σ 2 n = 2 · E ( (cid:107) g ( x nt , θ ∗ ) (cid:107) 2 ) the variance of the gradient noise is bounded as : E (cid:32) (cid:107) u t + 1 ( θ t ) (cid:107) 2 | F t (cid:33) ≤ 1 | B t | · C 1 (cid:107) ˜ θ t (cid:107) 2 + 1 | B t | · N (cid:88) n = 1 p n σ 2 n (cid:124) (cid:123)(cid:122) (cid:125) in - clustervariance + 2 | B t | · N (cid:88) n p n (cid:107) g ( n ) t − g n ( θ ∗ ) (cid:107) 2 (cid:124) (cid:123)(cid:122) (cid:125) between - clustervariance ( 5 ) Remark The second and the last term of the right - hand side of this bound are respectively the within - cluster and the between - cluster variance . If the approximate cluster gradi - ent converges to the actual one g ( n ) t → g n ( θ ∗ ) as t → ∞ , the between - cluster variance vanishes . It is worth noting Algorithm 1 Discover Initialization : ¯ g 0 = 0 , α ∈ ( 0 , p min ) , g ( n ) 0 = 0 , α n = α / p n for t = 0 , . . . , T − 1 do Get the cluster indexes C in the current batch B t and update θ t + 1 , { g ( n ) t + 1 } Nn = 1 and ¯ g t + 1 : θ t + 1 = θ t − µ | B t | · (cid:80) x nt ∈B t (cid:18) g ( x nt , θ t ) − g ( n ) t + ¯ g t (cid:19) for n ∈ C do (cid:46) in parallel B nt = { x kt ∈ B t | k = n } g ( n ) t + 1 = ( 1 − α n ) g ( n ) t + α n | B nt | (cid:80) x nt ∈B nt g ( x nt , θ t ) end for g ( n ) t + 1 = g ( n ) t for each n / ∈ C ¯ g t + 1 = ¯ g t − α | B t | (cid:80) x nt ∈B t (cid:16) g ( n ) t − g ( x nt , θ t ) (cid:17) end for return θ T that when we use the same approximate gradient buffer g M for all the clusters , the update rule 4 reduces to that of SGD with Momentum ( Polyak , 1964 ; Goh , 2017 ) . Con - sequently , the between cluster variance term in the above bound becomes ( 2 / | B t | ) · (cid:107) g Mt − g n ( θ ∗ ) (cid:107) 2 and may van - ish as t → ∞ . Therefore , Momentum also can perform between - cluster variance reduction and can be seen as a special case of the method proposed here , albeit operating at a coarser level ( maintaining one general approximate cluster gradient buffer instead of one for each cluster ) . Momen - tum has been mainly considered as a method for ﬁghting curvature ( Goh , 2017 ) . Recent work has shown its variance reduction capabilities in speciﬁc cases ( Roux et al . , 2018 ) . We show in our experiments that Momentum indeed per - forms between - cluster variance reduction . We now show that recursive applications of the update rule 4 indeed leads to vanishing between - cluster variance . The full proof of Theorem 1 is provided in the appendix Section 9 . 3 . Theorem 1 Under the same assumptions as in Section 3 , we denote | B t | as the batch size and σ 2 in = (cid:80) Nn = 1 p n σ 2 n as the in - cluster variance with σ 2 n = 2 · E ( (cid:107) g ( x nt , θ ∗ ) (cid:107) 2 ) , p min = min { p 1 , . . . , p N } . For any step size satisfying µ ≤ min (cid:40) ν | B t | 3 δ 2 ( | B t | + 5 ) , α 6 ν (cid:41) where α ∈ ( 0 , p min ) and γ = 3 µ 2 α | B t | , the iterate θ t from Discover 1 converge in expectation to the solution θ ∗ with the contraction factor q = 1 − µν < 1 with G 0 = (cid:80) Nn = 1 p n (cid:107) g n ( θ ∗ ) (cid:107) 2 . it holds : E ( (cid:107) θ t − θ ∗ (cid:107) 2 ) ≤ q t (cid:32) E ( (cid:107) ˜ θ 0 (cid:107) 2 ) + γG 0 (cid:33) + 4 µ ν | B t − 1 | σ 2 in ( 6 ) lim sup t → + ∞ E ( (cid:107) θ t + 1 − θ ∗ (cid:107) 2 ) = O ( µ · σ 2 in / | B t | ) ( 7 ) Theorem 1 shows that when the step size is small , Discover 1 eliminates the between - cluster variance in the limit , hence Discover reducing the overall variance of the gradient noise to the in - cluster variance . Therefore , when the latter is signiﬁ - cantly smaller than the between - cluster variance , Discover can be an effective variance reduction strategy . We show in the experiments section that when the clustering struc - ture to exploit is carefully chosen , Discover leads to faster convergence and sometimes results in improved general - ization thanks to the cluster information . We now show how to leverage the idea of using multiple momentum terms based on a given clustering to improve Implicit Gradient Transport ( Arnold et al . , 2019 ) and Quasi Hyperbolic Mo - mentum ( Ma & Yarats , 2018 ) ( both relying on a single mo - mentum in their vanilla version ) . The update rules for such extensions ( respectively called Discover - IGT and Discover - QHM ) follow . Our experiments will demonstrate the effec - tiveness of these methods . Discover - IGT ( D - IGT ) update rule : γ t = t / ( t + 1 ) v t = γ t · v t − 1 + ( 1 − γ t ) · g (cid:18) θ t + γ t 1 − γ t ( θ t − θ t − 1 ) , x t (cid:19) g ( n ) t + 1 = (cid:40) ( 1 − α n ) g ( n ) t + α n v t if n ∈ C , g ( n ) t otherwise θ t + 1 = θ t − µ | B t | · (cid:88) x nt ∈B t (cid:18) v t − g ( n ) t + ¯ g t (cid:19) The Discover - QHM ( D - QHM ) method uses the following update rule : g ( n ) t + 1 =   ( 1 − α n ) g ( n ) t + α n | B nt | (cid:80) x nt ∈B nt g ( x nt , θ t ) if n ∈ C , g ( n ) t otherwise g ( n ) t + 2 = νg ( n ) t + 1 + ( 1 − ν ) · 1 | B nt | (cid:88) x nt ∈B nt g ( x nt , θ t ) θ t + 1 = θ t − µ | B t | · (cid:88) x nt ∈B t (cid:18) g ( x nt , θ t ) − g ( n ) t + 1 + ¯ g t + 1 (cid:19) where B nt = { x kt ∈ B t | k = n } is the batch at iteration t , C is the cluster indexes present in the batch B t , and g ( n ) t are updated in parallel for n ∈ C . Discover - IGT combines gradient transport and the multi - momentum strategy . D - QHM extends QHM and Discover by using in the update a combination of the approximate gradient and the momentum buffer , together with a multi - momentum approach similar to Discover . When ν = 1 , and N = 1 it reduces to Momentum . When N = 1 , it reduces to QHM . When ν = 1 , it reduces to Discover . Choosing ν < 1 alleviates the potential staleness of the buffer . 5 . Experiments We validate that the Discover algorithms coupled with a careful clustering structure lead to faster convergence com - pared to their vanilla counterparts ( SGD + Momentum , IGT , and QHM 5 ) . We provide additional insights and show that the runtime is comparable to competing methods thanks to a careful parallel implementation . We implement all the methods used in our experiments using JAX ( Bradbury et al . , 2018 ) , and FLAX ( Flax Devel - opers , 2020 ) and perform our experiments on Google Cloud TPU ( Jouppi et al . , 2017 ) devices . We compare Discover with widely used minibatch SGD with Momentum , and Adam ( Kingma & Ba , 2014 ) , as well as with the more re - cently introduced IGT and QHM on ImageNet ( Deng et al . , 2009 ) and CIFAR - 10 ( Krizhevsky et al . , 2009 ) datasets . Dis - cover is amenable to parallelization . For example , at each round , we can update the cluster gradients in parallel , as shown in Algorithm 1 . Our implementation exploits this feature and groups clusters by cores to enable parallel up - dates . That is , examples of each core belong to the same cluster . In this section we treat α n as an independent hyper - parameter instead of using an exact equation α n = α / p n to simplify the parallel implementation . We provide more details about the practical implementation in appendix Sec - tion 7 . 1 . We conﬁrm that using the same strategy does not make a practical difference for all the other optimizers we consider . For each optimizer , we select the hyperparameters either as suggested by its authors or by running a sweep to obtain the highest accuracy at the end of training across ﬁve random seeds . The details of the training setup , HP search , and the best values found for each optimizer are given in appendix Section 7 . ImageNet : Data augmentation methods as clusters We ﬁrst consider an image classiﬁcation and train a ResNet - v1 - 50 ( He et al . , 2016 ) model on the ImageNet ( Deng et al . , 2009 ; Russakovsky et al . , 2015 ) dataset for 31200 steps ( 90 epochs ) . We use cosine learning rate schedule with mini - batches of a batch size of 4096 , weight decay regularization of 0 . 001 , group normalization ( Wu & He , 2018 ) , and weight standardization ( Qiao et al . , 2019 ) . We use a standard pre - processing pipeline consisting of cropping ( Szegedy et al . , 2015 ) with size 224x224 , pixel value scaling , and a random horizontal ﬂipping . We train our Resnet - 50 using three data augmentation methods : random ﬂipping , Mixup ( Zhang et al . , 2017 ) , and CutMix ( Yun et al . , 2019a ) . For each image , we ap - ply all transformations separately , producing three differ - ently augmented examples ( as shown in Figure 5 ) . Conse - quently , each of the augmentation methods induces a differ - 5 we restrict QHM to ν (cid:54) = 0 and ν (cid:54) = 1 so it is not reduced to SGD or SGD + Momentum Discover ent cluster and the probability of each is 1 / 3 . We compare the multi - momentum strategies ( Discover , Discover - IGT , and Discover - QHM ) , exploiting this clustering information , with their vanilla counterparts ( SGD with Momentum , IGT , and QHM ) . For the sake of completeness , we also add Adam to the comparison as a baseline . Figure 1 shows the results for the different methods . Using all these data augmenta - tion methods can make the learning problem more difﬁcult because the resulting cluster data distributions can differ sig - niﬁcantly . In this setting , we observe that Discover variants initially converge faster than their corresponding vanilla op - timizers . Discover , and Discover - IGT reach similar ﬁnal performance to SGD with Momentum and IGT , respectively , while Discover - QHM outperforms the vanilla QHM . CIFAR : Classes as clusters . Next , we consider the CIFAR - 10 ( Krizhevsky et al . , 2009 ) classiﬁcation and use the classes as the clusters , therefore having ten different clusters . We train a WideResNet26 - 10 ( Zagoruyko & Ko - modakis , 2016 ) on CIFAR - 10 for 400 epochs using cosine learning rate schedule , batch size of 256 , group normal - ization , L2 - regularization of 5 × 10 − 4 and dropout rate of 0 . 3 . The preprocessing consists of 4 - pixel zero - padding , a random crop of size 32x32 , scaling the pixels to [ 0 , 1 ] range and random horizontal ﬂip . Figure 2 shows the results for the different optimizers . CIFAR - 10 is an easy task and all the methods eventually con - verge to high accuracy . IGT converges faster than Discover - IGT and reaches higher ﬁnal accuracy ( 95 . 3 % vs 94 . 2 % ) . Discover ( resp . Discover - QHM ) converge similarly to SGD with Momentum ( resp . QHM ) . These single momentum strategies also reach a similar ( for QHM , 95 . 1 % vs 95 . 4 % ) or higher ( for Momentum , 95 . 3 % vs 94 . 2 % ) ﬁnal perfor - mance . To highlight the importance of carefully choosing the clustering structure , we also performed an experiment on CIFAR - 10 where we assigned each point to 1 of 10 clus - ters uniformly at random and train the model using discover and such clustering . The resulting test accuracy is 91 . 09 % down from 94 . 2 % when using classes as clusters . Therefore it is essential to use a good clustering structure . Learning with noisy labels We now consider the more challenging ImageNet and CIFAR - 10 classiﬁcation task in the presence of label noise : At each round , the label of the considered example is ﬂipped with probability p ( the noise level ) and assigned to a different class selected uniformly at random among the other classes . In this setting , the variance of the gradient noise is increased due to the presence of la - bel noise ( see Appendix Section 8 ) . We compare Discover , Discover - QHM , and Discover - IGT with their vanilla coun - terparts ( SGD with Momentum , QHM , IGT ) and Adam . We use Data augmentation methods as clusters for ImageNet and classes as clusters for CIFAR - 10 . We use the same Table 1 . Training performance ( in steps / second ) with different opti - mizers . On ImageNet a single number is reported , while on CIFAR - Mean and std deviation across 5 runs . Optimizer ImageNet , CIFAR - 10 ( µ ± σ ) , 64 TPUv3 4 TPUv2 SGD − . − 14 . 3 ± 3 . 8 Adam 8 . 2 9 . 8 ± 0 . 4 Momentum 8 . 7 10 . 4 ± 0 . 7 IGT 7 . 8 9 . 3 ± 0 . 6 QHM 8 . 6 12 . 5 ± 2 . 7 Discover 7 . 4 6 . 3 ± 0 . 3 Discover - QHM 7 . 6 5 . 8 ± 0 . 02 Discover - IGT 7 . 3 5 . 6 ± 0 . 02 setting as in the previous ImageNet and CIFAR - 10 exper - iments . Still , We perform a hyperparameter ( HP ) search de novo to determine the best learning for each method in the presence of noisy labels . The details of the HP search , and the best values found for each optimizer are given in appendix Section 7 . When the label noise is low ( p = 0 . 2 ) , all the methods achieve high accuracy on the non - corrupted training exam - ples and the test examples with only a slight deterioration compared to a clean setting p = 0 ( exact results are given in appendix Section 7 . 5 ) . Figure 3 and Figure 4 show the results of this experiment in the high noise level setting ( p = 0 . 8 ) on ImageNet and CIFAR - 10 , respectively . On CIFAR - 10 , the multi - momentum optimizers’ loss curves are almost superimposed with the curves of SGD with Mo - mentum , QHM , and IGT . However , the former general - ize signiﬁcantly better at each time step and reach higher ﬁnal performance than all their single momentum coun - terparts . It is worth noting that Discover outperforms all the multi - momentum optimizers by a large margin on the validation accuracy , achieving 85 . 9 % . The superiority of multi - momentum optimizers over the single momentum ones translates to ImageNet , suggesting that our Discover algorithms ﬁnd better local optima even in challenging set - tings . The other interesting ﬁnding is that while Adam converges faster in training loss , it generalizes poorly . Impact of clustering structure . We also performed an experiment on CIFAR - 10 where we compare three cluster - ing structures : ( 1 ) Classes as clusters : This is the setting we have considered so far for CIFAR - 10 . ( 2 ) Transformations as clusters . ( 3 ) Random clusters : assigning each point to 1 of 10 clusters uniformly at random . In each case , we train the model using Discover and the chosen clustering . When the clusters are selected uniformly at random , the result - ing test accuracy is 91 . 09 % . When we use transformation as clusters instead , the test accuracy improves to 93 . 6 % . The best accuracy with Discover on CIFAR - 10 ( 94 . 2 % ) is achieved when using classes as clusters . However , using this Discover ( a ) ( b ) ( c ) Figure 1 . Results of training ResNet - v1 - 50 model on ImageNet dataset : train loss ( a ) , validation accuracy ( b ) and validation accuracy on the last step in % ( c ) . Discover consistently stays on par with vanilla optimizers ( IGT , Momentum ) or outperforms them ( QHM ) in the end of the training while always converging faster in the beginning . ( a ) ( b ) ( c ) Figure 2 . Results of training WideResNet26 - 10 model on CIFAR - 10 dataset ( classes as clusters ) : train loss ( a ) , validation accuracy ( b ) and validation accuracy on the last step in % ( c ) . For each step a mean value across 5 random seeds is plotted , black whiskers in ( c ) indicate standard deviation . Discover variants for IGT and Momentum are slightly worse than vanilla optimizers , while being on par for QHM . clustering structure comes at the cost of a higher memory . Between - Cluster Variance Reduction In this experi - ment , we assess the between - cluster variance reduction ca - pabilities of Discover compared to SGD and SGD with Momentum on CIFAR - 10 ( with classes as clusters ) both in the clean ( p = 0 ) and noisy ( p = 0 . 8 ) settings . We measure at each step of the optimization an approximation of the between - cluster variance obtained by substituting g n ( θ t + 1 ) to g n ( θ ∗ ) in the between - cluster variance formula . For SGD we use Equation ( 3 ) . For Discover and SGD with Momen - tum – Equation ( 5 ) . However , for SGD with Momentum , one global gradient buffer is used for all the clusters . Equa - tion ( 5 ) shows that in both the clean and the noisy setting , Discover reduces the between - cluster variance faster . The ﬁgure also highlights the between - cluster variance reduction capabilities of SGD with Momentum , which , though not as fast as Discover , also leads to quickly vanishing variance . This explains the good performance of Momentum in our previous experiments . 6 . Conclusion and Perspectives We introduced a set of scalable VR algorithms exploiting the ubiquitous clustering structure in the data and relying on a multi - momentum strategy . We demonstrated that simple choices of clustering structure ( e . g . , transformations ) can lead to improved results . Our framework gives the designer signiﬁcant ﬂexibility allowing them to leverage prior infor - mation to select good clustering structure . The experiments demonstrated that the proposed strategy often leads to faster convergence and sometimes to improved generalization , es - pecially in challenging settings ( e . g . , label noise ) . Table 1 shows there are only minor differences between the multi - momentum strategies and their vanilla counter - parts thanks to our efﬁcient implementation that exploits the parallel structure of the algorithms . Discover ( a ) ( b ) ( c ) Figure 3 . Results of training ResNet - v1 - 50 model on ImageNet dataset in a high noise setting p = 0 . 8 : train loss ( a ) , validation accuracy ( b ) and validation accuracy on the last step in % ( c ) . Discover variants outperforms all vanilla optimizers by a large margin . ( a ) ( b ) ( c ) Figure 4 . Results of training WideResNet26 - 10 model on CIFAR - 10 dataset ( classes as clusters ) in a high noise setting p = 0 . 8 : train loss ( a ) , validation accuracy ( b ) and validation accuracy on the last step in % ( c ) . For each step a mean value across 5 random seeds is plotted , black whiskers in ( c ) indicate standard deviation . Discover modiﬁcations outperform all vanilla optimizers by a large margin , once again suggesting high noise robustness . Adam achieves only 19 . 8 % mean ( 27 . 7 % max ) ﬁnal accuracy and thus not shown on ( c ) for clarity . ( a ) ( b ) ( c ) Figure 5 . Between - cluster variance estimate ( mean per 100 train steps ) for CIFAR - 10 with clean labels ( a ) and noisy labels ( b ) with p = 0 . 8 . Example augmentations ( c ) used as clusters for ImageNet experiments . Discover References TensorFlow Datasets , a collection of ready - to - use datasets . https : / / www . tensorflow . org / datasets . Abadi , M . , Agarwal , A . , Barham , P . , Brevdo , E . , Chen , Z . , Citro , C . , Corrado , G . S . , Davis , A . , Dean , J . , Devin , M . , Ghemawat , S . , Goodfellow , I . , Harp , A . , Irving , G . , Isard , M . , Jia , Y . , Jozefowicz , R . , Kaiser , L . , Kudlur , M . , Lev - enberg , J . , Mané , D . , Monga , R . , Moore , S . , Murray , D . , Olah , C . , Schuster , M . , Shlens , J . , Steiner , B . , Sutskever , I . , Talwar , K . , Tucker , P . , Vanhoucke , V . , Vasudevan , V . , Viégas , F . , Vinyals , O . , Warden , P . , Wattenberg , M . , Wicke , M . , Yu , Y . , and Zheng , X . TensorFlow : Large - scale machine learning on heterogeneous systems , 2015 . URL https : / / www . tensorflow . org / . Software available from tensorﬂow . org . Arnold , S . , Manzagol , P . - A . , Harikandeh , R . B . , Mitliagkas , I . , and Le Roux , N . Reducing the variance in online opti - mization by transporting past gradients . In Advances in Neural Information Processing Systems , pp . 5392 – 5403 , 2019 . Bietti , A . and Mairal , J . Stochastic optimization with vari - ance reduction for inﬁnite datasets with ﬁnite sum struc - ture . In Advances in Neural Information Processing Sys - tems , pp . 1623 – 1633 , 2017 . Bottou , L . , Curtis , F . E . , and Nocedal , J . Optimization methods for large - scale machine learning . Siam Review , 60 ( 2 ) : 223 – 311 , 2018 . Bradbury , J . , Frostig , R . , Hawkins , P . , Johnson , M . J . , Leary , C . , Maclaurin , D . , and Wanderman - Milne , S . JAX : com - posable transformations of Python + NumPy programs , 2018 . URL http : / / github . com / google / jax . Cutkosky , A . and Orabona , F . Momentum - based variance reduction in non - convex sgd . In Advances in Neural Information Processing Systems , pp . 15236 – 15245 , 2019 . Dean , J . , Corrado , G . , Monga , R . , Chen , K . , Devin , M . , Mao , M . , Ranzato , M . , Senior , A . , Tucker , P . , Yang , K . , et al . Large scale distributed deep networks . In Advances in neural information processing systems , pp . 1223 – 1231 , 2012 . Defazio , A . and Bottou , L . On the ineffectiveness of vari - ance reduced optimization for deep learning . In Ad - vances in Neural Information Processing Systems , pp . 1753 – 1763 , 2019 . Defazio , A . , Bach , F . , and Lacoste - Julien , S . Saga : A fast incremental gradient method with support for non - strongly convex composite objectives . In Advances in neural information processing systems , pp . 1646 – 1654 , 2014 . Deng , J . , Dong , W . , Socher , R . , Li , L . - J . , Li , K . , and Fei - Fei , L . Imagenet : A large - scale hierarchical image database . In 2009 IEEE conference on computer vision and pattern recognition , pp . 248 – 255 . Ieee , 2009 . Fishman , G . S . Monte Carlo : Concepts , Algorithms and Applications . Springer Verlag , New York , NY , USA , 1996 . Flax Developers . Flax : A neural network library for jax designed for ﬂexibility , 2020 . URL https : / / github . com / google - research / flax / tree / prerelease . Goh , G . Why momentum really works . Distill , 2 ( 4 ) : e6 , 2017 . Goodfellow , I . , Bengio , Y . , and Courville , A . Deep Learning . MIT Press , 2016 . http : / / www . deeplearningbook . org . He , K . , Zhang , X . , Ren , S . , and Sun , J . Deep residual learning for image recognition . In The IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , June 2016 . Ioffe , S . and Szegedy , C . Batch normalization : Accelerating deep network training by reducing internal covariate shift . arXiv preprint arXiv : 1502 . 03167 , 2015 . Johnson , R . and Zhang , T . Accelerating stochastic gradient descent using predictive variance reduction . In Advances in neural information processing systems , pp . 315 – 323 , 2013 . Jouppi , N . P . , Young , C . , Patil , N . , Patterson , D . , Agrawal , G . , Bajwa , R . , Bates , S . , Bhatia , S . , Boden , N . , Borchers , A . , et al . In - datacenter performance analysis of a tensor processing unit . In Proceedings of the 44th Annual Inter - national Symposium on Computer Architecture , pp . 1 – 12 , 2017 . Kingma , D . P . and Ba , J . Adam : A method for stochastic optimization . arXiv preprint arXiv : 1412 . 6980 , 2014 . Krizhevsky , A . , Hinton , G . , et al . Learning multiple layers of features from tiny images . 2009 . Krizhevsky , A . , Sutskever , I . , and Hinton , G . E . Imagenet classiﬁcation with deep convolutional neural networks . In Advances in neural information processing systems , pp . 1097 – 1105 , 2012 . Krogh , A . and Hertz , J . A . A simple weight decay can im - prove generalization . In Moody , J . E . , Hanson , S . J . , and Lippmann , R . P . ( eds . ) , Advances in Neural Information Processing Systems 4 , pp . 950 – 957 . Morgan - Kaufmann , 1992 . URL http : / / papers . nips . cc / paper / Discover 563 - a - simple - weight - decay - can - improve - generalization . pdf . Li , T . , Sahu , A . K . , Talwalkar , A . , and Smith , V . Feder - ated learning : Challenges , methods , and future directions . arXiv preprint arXiv : 1908 . 07873 , 2019 . Loshchilov , I . and Hutter , F . Decoupled weight decay reg - ularization . In International Conference on Learning Representations , 2019 . URL https : / / openreview . net / forum ? id = Bkg6RiCqY7 . Ma , J . and Yarats , D . Quasi - hyperbolic momentum and adam for deep learning . arXiv preprint arXiv : 1810 . 06801 , 2018 . Mairal , J . Optimization with ﬁrst - order surrogate functions . In International Conference on Machine Learning , pp . 783 – 791 , 2013 . Nemirovski , A . , Juditsky , A . , Lan , G . , and Shapiro , A . Ro - bust stochastic approximation approach to stochastic pro - gramming . SIAM Journal on optimization , 19 ( 4 ) : 1574 – 1609 , 2009 . Nguyen , L . M . , Liu , J . , Scheinberg , K . , and Takáˇc , M . Sarah : A novel method for machine learning problems using stochastic recursive gradient . In Proceedings of the 34th International Conference on Machine Learning - Volume 70 , pp . 2613 – 2621 . JMLR . org , 2017 . Polyak , B . T . Some methods of speeding up the convergence of iteration methods . USSR Computational Mathematics and Mathematical Physics , 4 ( 5 ) : 1 – 17 , 1964 . Qiao , S . , Wang , H . , Liu , C . , Shen , W . , and Yuille , A . Weight standardization . arXiv preprint arXiv : 1903 . 10520 , 2019 . Robbins , H . E . A stochastic approximation method . Annals of Mathematical Statistics , 22 : 400 – 407 , 2007 . Roux , N . L . , Schmidt , M . , and Bach , F . R . A stochastic gradient method with an exponential convergence _ rate for ﬁnite training sets . In Advances in neural information processing systems , pp . 2663 – 2671 , 2012 . Roux , N . L . , Babanezhad , R . , and Manzagol , P . - A . Online variance - reducing optimization , 2018 . URL https : / / openreview . net / forum ? id = r1qKBtJvG . Russakovsky , O . , Deng , J . , Su , H . , Krause , J . , Satheesh , S . , Ma , S . , Huang , Z . , Karpathy , A . , Khosla , A . , Bernstein , M . , Berg , A . C . , and Fei - Fei , L . ImageNet Large Scale Visual Recognition Challenge . International Journal of Computer Vision ( IJCV ) , 115 ( 3 ) : 211 – 252 , 2015 . doi : 10 . 1007 / s11263 - 015 - 0816 - y . Sayed , A . H . Adaptation , learning , and optimization over networks . Foundations and Trends in Machine Learning , 7 ( ARTICLE ) : 311 – 801 , 2014a . Sayed , A . H . Adaptive networks . Proceedings of the IEEE , 102 ( 4 ) : 460 – 497 , 2014b . Schmidt , M . , Le Roux , N . , and Bach , F . Minimizing ﬁnite sums with the stochastic average gradient . Mathematical Programming , 162 ( 1 - 2 ) : 83 – 112 , 2017 . Shalev - Shwartz , S . and Ben - David , S . Understanding ma - chine learning : From theory to algorithms . Cambridge university press , 2014 . Shalev - Shwartz , S . and Zhang , T . Stochastic dual coordinate ascent methods for regularized loss minimization . Jour - nal of Machine Learning Research , 14 ( Feb ) : 567 – 599 , 2013 . Srivastava , N . , Hinton , G . , Krizhevsky , A . , Sutskever , I . , and Salakhutdinov , R . Dropout : A simple way to prevent neural networks from overﬁtting . J . Mach . Learn . Res . , 15 ( 1 ) : 1929 – 1958 , January 2014a . ISSN 1532 - 4435 . Srivastava , N . , Hinton , G . , Krizhevsky , A . , Sutskever , I . , and Salakhutdinov , R . Dropout : a simple way to prevent neural networks from overﬁtting . The journal of machine learning research , 15 ( 1 ) : 1929 – 1958 , 2014b . Sutskever , I . , Martens , J . , Dahl , G . , and Hinton , G . On the importance of initialization and momentum in deep learn - ing . In International conference on machine learning , pp . 1139 – 1147 , 2013 . Szegedy , C . , Liu , W . , Jia , Y . , Sermanet , P . , Reed , S . , Anguelov , D . , Erhan , D . , Vanhoucke , V . , and Rabinovich , A . Going deeper with convolutions . In Computer Vision and Pattern Recognition ( CVPR ) , 2015 . URL http : / / arxiv . org / abs / 1409 . 4842 . Wu , Y . and He , K . Group normalization . In The Euro - pean Conference on Computer Vision ( ECCV ) , September 2018 . Yuan , K . , Ying , B . , and Sayed , A . H . Cover : A cluster - based variance reduced method for online learning . In ICASSP 2019 - 2019 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , pp . 3102 – 3106 . IEEE , 2019 . Yun , S . , Han , D . , Oh , S . J . , Chun , S . , Choe , J . , and Yoo , Y . Cutmix : Regularization strategy to train strong classiﬁers with localizable features . In The IEEE International Conference on Computer Vision ( ICCV ) , October 2019a . Yun , S . , Han , D . , Oh , S . J . , Chun , S . , Choe , J . , and Yoo , Y . Cutmix : Regularization strategy to train strong classiﬁers Discover with localizable features . In Proceedings of the IEEE International Conference on Computer Vision , pp . 6023 – 6032 , 2019b . Zagoruyko , S . and Komodakis , N . Wide residual networks . arXiv preprint arXiv : 1605 . 07146 , 2016 . Zhang , H . , Cisse , M . , Dauphin , Y . N . , and Lopez - Paz , D . Mixup : Beyond empirical risk minimization . arXiv preprint arXiv : 1710 . 09412 , 2017 . Zhang , J . and Mitliagkas , I . Yellowﬁn and the art of mo - mentum tuning . arXiv preprint arXiv : 1706 . 03471 , 2017 . Discover 7 . Appendix A : Experimental details 7 . 1 . Implementation All models , training and evaluation pipelines are imple - mented using JAX ( Bradbury et al . , 2018 ) and FLAX ( Flax Developers , 2020 ) . The data loading and preprocessing is implemented in Tensorﬂow v2 ( Abadi et al . , 2015 ) and TFDS ( TFD ) . The SGD , Momentum and Adam optimizers are taken from FLAX libraries , QHM is implemented following ( Ma & Yarats , 2018 ) and IGT is written in FLAX following an example at https : / / github . com / google - research / google - research / tree / master / igt _ optimizer ( Arnold et al . , 2019 ) . Discover optimizers are implemented with single - program multiple - data training in mind . Our implementation expects a training data processed during a single training step by a given device core to contain examples from the same cluster . After shufﬂing the dataset , our input pipeline selects the next batch _ size / num _ devices examples from the same randomly picked cluster to satisfy this condition . This translates into a single cluster per batch on one - host one - core training ( e . g . a workstation with one CPU ) , but easily scales to more clusters by using multiple cores even within a single - host setting . For examples , the CIFAR experiments use one host machine with 4 Google Cloud TPUv2 ( Jouppi et al . , 2017 ) devices , thus having 8 cores in total , so each batch can contain examples from up to 8 different clusters . Discover optimizer averages g ∆ , t = g ( x nt , θ t ) − g ( n ) t across devices to perform identical ¯ g t updates locally on each device and performs local g ( n ) t updates in parallel , synchronizing across devices at the end of each training step ( see Algorithm 1 for notation ) . This averaging makes effective α n value depend on the distributed training setup and to account for it we chose to tune α n as a direct hyperparameter instead of trying to incorporate the training setup into the formula from Algorithm 1 . This implementation is well suited to a good balance of clusters within a single global batch , which is the usual case for a small number of clusters and a randomly shufﬂed dataset . We have not tested the performance when the cluster distribution in a global batch is signiﬁcantly skewed . 7 . 2 . Datasets The datasets used in this work are ImageNet - 1000 ( Deng et al . , 2009 ; Russakovsky et al . , 2015 ) and CIFAR - 10 ( Krizhevsky et al . , 2009 ) . The datasets were downloaded from https : / / www . tensorflow . org / datasets / catalog / imagenet2012 and https : / / www . tensorflow . org / datasets / catalog / cifar10 respectively . The detailed informa - tion about the size and format of the train and validation splits are available at the corresponding web pages . 7 . 3 . ImageNet ( augmentations as clusters ) We trained a ResNet - v1 - 50 ( He et al . , 2016 ) model on the ImageNet ( Deng et al . , 2009 ; Russakovsky et al . , 2015 ) dataset for 31200 steps , corresponding to 100 epochs on the unaugmented dataset . We used : • Cosine learning rate schedule with 5 warmup steps . • Batch size of 4096 . • Weight decay regularization setting of λ = 0 . 001 . We used decoupled weight decay inspired by ( Loshchilov & Hutter , 2019 ) and each step update all network pa - rameters by Θ t + 1 = Θ t ∗ ( 1 − λµ ) where µ is the learning rate . • Group normalization with 32 groups ( Wu & He , 2018 ) . • Weight standardization . The preprocessing consists of Inception - style cropping ( Szegedy et al . , 2015 ) with size 224x224 , scaling the pixel values to [ − 1 ; 1 ] range and random horizontal ﬂip . In this setting the model achieves 0 . 76 accuracy with Momen - tum ( Polyak , 1964 ; Sutskever et al . , 2013 ) optimizer when µ = 0 . 1 , β = 0 . 9 . We extend the setting to use Mixup ( Zhang et al . , 2017 ) and Cutmix ( Yun et al . , 2019a ) aug - mentations instead of random ﬂipping , choosing the same mixing ratio for all examples in the single local batch of each distributed training host . We correspondingly assign all examples augmented with random horizontal ﬂip to clus - ter 0 , with Mixup - to cluster 1 , Cutmix - to cluster 2 . The clusters are used for Discover optimizer . The hyperparameters are selected from a hyperparameter sweep to obtain the highest mean accuracy at 31200 steps across 5 random seeds . The learning rate µ is chosen from a set of { 0 . 03 , 0 . 1 , 0 . 3 } for all optimizers . The further hyperparameters are selected from the following sets : • Discover : α , α n ∈ { 0 . 001 , 0 . 01 , 0 . 1 , 0 . 9 } • Momentum : β ∈ { 0 . 85 , 0 . 9 , 0 . 95 , 0 . 99 } • QHM : β = 0 . 999 , γ = 0 . 7 ( as reported default for ImageNet in ( Ma & Yarats , 2018 ) ) • IGT : β = 0 . 9 , tail _ fraction = 90 ( as reported to be selected for ImageNet in ( Arnold et al . , 2019 ) ) • Adam : µ = 0 . 001 , β 1 = 0 . 9 , β 2 = 0 . 999 • Discover - QHM : α ∈ { 0 . 1 , 0 . 6 , 0 . 7 , 0 . 8 , 0 . 9 , 0 . 99 } , α n ∈ { 0 . 01 , 0 . 1 , 0 . 9 } , β ∈ { 0 , 1 , 0 . 6 , 0 . 7 , 0 . 8 } • Discover - IGT : α ∈ { 0 . 1 , 0 . 6 , 0 . 7 , 0 . 8 , 0 . 9 , 0 . 99 } , α n ∈ { 0 . 01 , 0 . 1 , 0 . 9 } , tail _ fraction ∈ { 18 , 45 , 50 , 60 , 90 , 180 , 360 } The exact hyperparameters selected for each optimizer from the hyperparameters sweep are : Discover • Discover : µ = 0 . 1 , α = 0 . 1 , α n = 0 . 01 • Momentum : µ = 0 . 1 , β = 0 . 9 • QHM : µ = 0 . 1 • IGT : µ = 0 . 1 • Adam : µ = 0 . 001 , β 1 = 0 . 9 , β 2 = 0 . 999 • Discover - QHM : µ = 0 . 1 , α = 0 . 9 , α n = 0 . 1 , γ = 0 . 9 • Discover - IGT : µ = 0 . 1 , α = 0 . 9 , α n = 0 . 1 , tail _ fraction = 180 The training is setup in the multi - device data - parallel fashion on a pod of 8x8 Google Cloud TPUv3 ( Jouppi et al . , 2017 ) . There are 16 host machines , each hav - ing access to 4 TPUv3 devices . The ImageNet ( Deng et al . , 2009 ; Russakovsky et al . , 2015 ) training and valida - tion splits obtained from https : / / www . tensorflow . org / datasets / catalog / imagenet2012 are di - vided into equal parts and each host has access to its own separate part . At most num _ hosts = 16 examples are ex - cluded this way to ensure the equal division . The training examples inside each batch are arranged such that each de - vice core receives only examples from a single cluster ( see Section 7 . 1 ) . We conﬁrmed that this does not make a prac - tical difference for any other optimizer examined : the loss and accuracy curves look identical and the ﬁnal accuracy is the same independent of whether this technique is applied or not . 7 . 4 . CIFAR ( classes as clusters ) We trained a WideResNet26 - 10 ( Zagoruyko & Komodakis , 2016 ) model on the CIFAR10 dataset ( Krizhevsky et al . , 2009 ) for 400 epochs . We used : • Cosine learning rate schedule with 5 warmup steps . • Batch size of 256 . • L2 - regularization ( Krogh & Hertz , 1992 ) set to 0 . 0005 . • Dropout ( Srivastava et al . , 2014a ) rate of 0 . 3 . • Group normalization ( Wu & He , 2018 ) with 16 groups in the ﬁrst layer in each block and 32 groups in the rest of the layers . The preprocessing consists of 4 pixel zero - padding followed by a random crop of size 32x32 , scaling the pixels to [ 0 ; 1 ] range and random horizontal ﬂip . We correspondingly as - sign all examples with the same class to the same cluster ( for Discover optimizer ) . The hyperparameters are selected from a hyperparameter sweep to obtain the highest mean accuracy at 400 epochs across 5 runs with different random seeds . The learning rate µ is chosen from a set of { 0 . 001 , 0 . 01 , 0 . 03 , 0 . 1 , 0 . 175 } for all optimizers . The further hyperparameters are selected from the following sets : • Discover : α , α n ∈ { 0 . 001 , 0 . 01 , 0 . 015 , 0 . 02 , . . . 0 . 095 , 0 . 1 , 0 . 15 , 0 . 2 , . . . 0 . 95 , 1 . 0 } • Momentum : β = 0 . 9 ( as used for WideResNet on CIFAR - 10 in ( Zagoruyko & Komodakis , 2016 ) ) • QHM : β ∈ { 0 . 8 , 0 . 9 } , γ ∈ { 0 . 1 , 0 . 2 , . . . , 0 . 8 , 0 . 9 } • IGT : β = 0 . 9 , tail _ fraction = 18 ( as determined to be best for CIFAR - 10 in ( Arnold et al . , 2019 ) ) • Adam : β 1 = 0 . 9 , β 1 = 0 . 999 , (cid:15) = 1 e − 8 ( as a good default settings reported in ( Kingma & Ba , 2014 ) ) • Discover - QHM : α ∈ { 0 . 6 , 0 . 7 , 0 . 8 , 0 . 9 } , α n ∈ { 0 . 01 , 0 . 1 , 0 . 9 } , β ∈ { 0 . 6 , 0 . 7 , 0 . 8 , 0 . 9 , 0 . 99 } • Discover - IGT : α ∈ { 0 . 095 , 0 . 1 , 0 . 6 , 0 . 7 , 0 . 8 , 0 . 9 , 0 . 99 } , α n ∈ { 0 . 01 , 0 . 1 , 0 . 9 } , tail _ fraction ∈ { 18 , 45 , 50 , 60 , 90 , 180 , 360 } The exact hyperparameters selected for each optimizer from the hyperparameters sweep are : • SGD : µ = 0 . 01 • Discover : µ = 0 . 01 , α = 0 . 095 , α n = 0 . 1 • Momentum : µ = 0 . 03 , β = 0 . 9 • QHM : µ = 0 . 1 , γ = 0 . 9 , β = 0 . 9 • IGT : µ = 0 . 01 , β = 0 . 9 , tail _ fraction = 18 • Adam : µ = 0 . 001 • Discover - QHM : µ = 0 . 01 , α = 0 . 9 , α n = 0 . 1 , β = 0 . 9 • Discover - IGT : µ = 0 . 01 , α = 0 . 095 , α n = 0 . 1 , tail _ fraction = 18 The training setup is the same as for ImageNet experiments ( see appendix Section 7 . 3 ) but using only one host machine with 4 Google cloud TPUv2 ( Jouppi et al . , 2017 ) . The CIFAR - 10 ( Krizhevsky et al . , 2009 ) training and test splits are obtained from https : / / www . tensorflow . org / datasets / catalog / cifar10 . 7 . 5 . CIFAR ( nosiy labels ) We also run the above - mentioned CIFAR - 10 ( Krizhevsky et al . , 2009 ) experiments with partially corrupted labels , where the label of each image is ﬂipped to a random differ - ent class independently with probability p every time the example is seen during training . E . g . the same example might get different labels in different training epochs . We have tuned the hyperparameters anew in a same way as the clean CIFAR - 10 experiment above Section 7 . 4 each value of p ∈ { 0 . 2 , 0 . 8 } . The results for the low - noise setting ( p = 0 . 2 ) and high - noise setting ( p = 0 . 8 ) are reported in Figure 6 and Figure 4 respectively . The exact hyperparameters selected for p = 0 . 2 setting are : • SGD : µ = 0 . 175 Discover ( a ) ( b ) ( c ) Figure 6 . Results of training WideResNet26 - 10 model on CIFAR - 10 dataset ( classes as clusters ) in a low noise setting p = 0 . 2 : train loss ( a ) , validation accuracy ( b ) and validation accuracy on the last step in % ( c ) . For each step a mean value across 5 random seeds is plotted , black whiskers in ( c ) indicate standard deviation . • Discover : µ = 0 . 01 , α = 0 . 095 , α n = 0 . 1 • Momentum : µ = 0 . 1 , β = 0 . 9 • QHM : µ = 0 . 175 , γ = 0 . 9 , β = 0 . 9 • IGT : µ = 0 . 1 , β = 0 . 9 , tail _ fraction = 18 • Adam : µ = 0 . 001 • Discover - QHM : µ = 0 . 1 , α = 0 . 9 , α n = 0 . 1 , γ = 0 . 7 • Discover - IGT : µ = 0 . 1 , α = 0 . 095 , α n = 0 . 1 , tail _ fraction = 18 The exact hyperparameters selected for p = 0 . 8 setting are : • SGD : µ = 0 . 1 • Discover : µ = 0 . 01 , α = 0 . 095 , α n = 0 . 1 • Momentum : µ = 0 . 01 , β = 0 . 9 • QHM : µ = 0 . 1 , γ = 0 . 6 , β = 0 . 9 • IGT : µ = 0 . 01 , β = 0 . 9 , tail _ fraction = 18 • Adam : µ = 0 . 001 • Discover - QHM : µ = 0 . 1 , α = 0 . 6 , α n = 0 . 1 , γ = 0 . 6 • Discover - IGT : µ = 0 . 1 , α = 0 . 095 , α n = 0 . 1 , tail _ fraction = 18 7 . 6 . ImageNet ( nosiy labels ) We also run the above - mentioned ImageNet ( Deng et al . , 2009 ; Russakovsky et al . , 2015 ) experiments with partially corrupted labels , where the label of each image is ﬂipped to a random different class independently with probability p = 0 . 8 every time the example is seen during training . E . g . the same example might get different labels in different training epochs . We used the same hyperparameters as selected for the clean ImageNet experiment , except the learning rate which was selected again for each optimizer value of p as described in Section 7 . 3 . The best values were exactly the values chosen in Section 7 . 3 . 7 . 7 . CIFAR ( augmentations as clusters ) We also run the above - mentioned CIFAR - 10 experiments when using Mixup , Cutmix and left - right ﬂipping aug - mented examples as clusters to mirror the setting used for ImageNet experiments above . We did not tune any hyper - parameters anew , instead for both clean and noisy labels setting we have used the same hyperparameters as for the ex - periments above ( when classes were used as clusters ) . When using clean labels the results for Discover modiﬁcations shown on Figure 7 were very similar to those of vanilla opti - mizers as expected . When using noisy labels ( with p = 0 . 8 ) Discover modiﬁcations performed signiﬁcantly worse than vanilla optimizers as shown on Figure 8 . These results highlight the importance of the careful clustering structure choice and suggest that Discover hyperparameters chosen for one clustering structure does not necessarily transfer to a different clustering choice in case of noisy labels . Discover ( a ) ( b ) ( c ) Figure 7 . Results of training WideResNet26 - 10 model on CIFAR - 10 dataset ( Mixup , Cutmix and left - right ﬂipping augmented examples as clusters ) in a clean labels setting : train loss ( a ) , validation accuracy ( b ) and validation accuracy on the last step in % ( c ) . ( a ) ( b ) ( c ) Figure 8 . Results of training WideResNet26 - 10 model on CIFAR - 10 dataset ( Mixup , Cutmix and left - right ﬂipping augmented examples as clusters ) in a high noise ( p = 0 . 8 ) setting : train loss ( a ) , validation accuracy ( b ) and validation accuracy on the last step in % ( c ) . 8 . Variance due to label noise Suppose that we optimize the loss (cid:96) ( f θ ( x ) ) : = (cid:96) ( θ ; x , f θ ( x ) ) with gradient equal to g ( x , θ ) . Let denote by ˜ g ( x , θ ) the gradient of the loss (cid:96) ( F θ ( x ) ) where F ( x ) is the true labeling function . On an instance x , we can write it as : (cid:96) ( f θ ( x ) ) = a + b , with a = (cid:96) ( F θ ( x ) ) and b = (cid:96) ( f θ ( x ) ) − (cid:96) ( F θ ( x ) ) . E (cid:107) g ( x , θ ) (cid:107) 2 = E (cid:107) ˜ g ( x , θ ) + g ( x , θ ) − ˜ g ( x , θ ) (cid:107) 2 ≤ 2 E (cid:107) ˜ g ( x , θ ) (cid:107) 2 + 2 E (cid:107) g ( x , θ ) − ˜ g ( x , θ ) (cid:107) 2 The ﬁrst term is the traditional variance of the gradient noise , and the second term is due to the label noise the larger the noise the larger it is . 9 . Appendix B : Proofs Proof 1 Before proving , we denote E [ . ] = E n , t [ . | x nt ] and E t = E [ . | F t ] where the superscript n indicate the cluster index x arises from . In this part we want to give a proof of Lemma 1 . We want to show that the ﬁrst and second order moments of the gradient noise s t + 1 ( θ t ) satisfy : E (cid:32) s t + 1 ( θ t ) | F t (cid:33) = 0 ( 8 ) E (cid:32) (cid:107) s t + 1 ( θ t ) (cid:107) 2 | F t (cid:33) ≤ β 2 (cid:107) ˜ θ t (cid:107) 2 + σ 2 ( 9 ) where ˜ θ t = θ t − θ ∗ , β 2 = 2 δ 2 and : σ 2 = 2 E (cid:32) (cid:107) g ( x nt , θ ∗ ) (cid:107) 2 (cid:33) = 2 N (cid:88) n = 1 p n E t (cid:32) (cid:107) g ( x n t , θ ∗ ) (cid:107) 2 (cid:33) where σ 2 is referred to as the magnitude of gradient noise . In case all the clusters have the same probability that is Discover p 1 = p 2 , . . . , p N = 1 N , we have that : σ 2 = 2 E (cid:32) (cid:107) g ( x nt , θ ∗ ) (cid:107) 2 (cid:33) = 2 N (cid:88) n = 1 p n E t (cid:32) (cid:107) g ( x nt , θ ∗ ) (cid:107) 2 (cid:33) = 2 E t (cid:32) (cid:107) g ( x t , θ ∗ ) (cid:107) 2 (cid:33) which correspond to the magnitude of gradient noise without considering the clustering structure . we recall that the gradient noise for SGD is given by : s t + 1 ( θ t ) = g ( x nt , θ t ) − g ( θ t ) . E (cid:32) s t + 1 ( θ t ) | F t (cid:33) = E n , t (cid:32) g ( x nt , θ t ) | F t (cid:33) − g ( θ t ) = N (cid:88) n = 1 p n g n ( θ t ) − g ( θ t ) = 0 Where we use the fact that : g n ( θ t ) = E t (cid:32) g ( x nt , θ t ) | F t (cid:33) g ( θ t ) = E n ( g n ( θ t ) ) Using the following Jensen’s inequality : E ( (cid:107) a + b (cid:107) 2 ) = 4 E ( (cid:107) 1 2 a + 1 2 b (cid:107) 2 ) ≤ 2 E ( (cid:107) a (cid:107) 2 ) + 2 E ( (cid:107) b (cid:107) 2 ) It holds that : E (cid:32) (cid:107) s t + 1 ( θ t ) (cid:107) 2 | F t (cid:33) = E (cid:32) (cid:107) g ( x nt , θ t ) − g ( θ t ) (cid:107) 2 | F t (cid:33) ≤ 2 E (cid:32) (cid:107) g ( x nt , θ t ) − g ( θ t ) − g ( x nt , θ ∗ ) (cid:107) 2 | F t (cid:33) + 2 E (cid:32) (cid:107) g ( x nt , θ ∗ ) (cid:107) 2 | F t (cid:33) Using the fact that for any random variable x , E (cid:107) x − E [ x ] (cid:107) 2 = E (cid:107) x (cid:107) 2 − (cid:107) E [ x ] (cid:107) 2 ≤ E (cid:107) x (cid:107) 2 and g ( θ ∗ ) = 0 , we have : E (cid:32) (cid:107) g ( x nt , θ t ) − g ( θ t ) − g ( x nt , θ ∗ ) (cid:107) 2 | F t (cid:33) ≤ E (cid:32) (cid:107) g ( x nt , θ t ) − g ( x nt , θ ∗ ) (cid:107) 2 | F t (cid:33) ≤ δ 2 (cid:107) ˜ θ t (cid:107) 2 We recall that the gradient noise within the cluster n is given by : s nt + 1 ( θ t ) = g ( x nt , θ t ) − g n ( θ t ) Assuming the gradient noise within the cluster n is fol - lowing assumptions similar to the result in Lemma 1 meaning that it is unbiased and it variance is bounded : E ( (cid:107) s nt + 1 ( θ t ) (cid:107) 2 | F t ) ≤ γ 2 n (cid:107) ˜ θ t (cid:107) 2 + σ 2 n , where ˜ θ t = θ ∗ − θ t , and γ n , σ n > 0 , where σ 2 n is referred to as the magnitude of gradient noise in cluster n . we want to prove Lemma 2 : E ( (cid:107) s t + 1 ( θ ∗ ) (cid:107) 2 | F t ) ≤ N (cid:88) n = 1 p n σ 2 n (cid:124) (cid:123)(cid:122) (cid:125) in - clustervariance + N (cid:88) n = 1 p n (cid:107) g n ( θ ∗ ) (cid:107) 2 (cid:124) (cid:123)(cid:122) (cid:125) between - clustervariance ( 10 ) Discover Proof 2 E ( (cid:107) s t + 1 ( θ ∗ ) (cid:107) 2 | F t ) = E ( (cid:107) g ( x nt , θ ∗ ) (cid:107) 2 | F t ) = E ( (cid:107) g ( x nt , θ t ) − g n ( θ ∗ ) + g n ( θ ∗ ) (cid:107) 2 | F t ) = E ( (cid:107) s nt + 1 ( θ ∗ ) + g n ( θ ∗ ) (cid:107) 2 | F t ) = E ( (cid:107) s nt + 1 ( θ ∗ ) (cid:107) 2 | F t ) + E ( (cid:107) g n ( θ ∗ ) (cid:107) 2 | F t ) ≤ N (cid:88) n = 1 p n ( γ 2 n (cid:107) 0 (cid:107) 2 + σ 2 n ) + N (cid:88) n = 1 p n (cid:107) g n ( θ ∗ ) (cid:107) 2 E ( (cid:107) s t + 1 ( θ ∗ ) (cid:107) 2 | F t ) ≤ N (cid:88) n = 1 p n σ 2 n + N (cid:88) n = 1 p n (cid:107) g n ( θ ∗ ) (cid:107) 2 where the expectation is also taken over the different clusters with probabilities p n . 9 . 1 . Proof of Lemma 3 Under the same assumptions as in Section 3 , for a batch of size | B t | the gradient is unbiased E [ u t + 1 ( θ t ) | F t ] = 0 . Denoting ˜ θ t : = θ ∗ − θ t , C 1 = 4 δ 2 , C 2 = (cid:80) Nn = 1 p n σ 2 n and σ 2 n = 2 · E ( (cid:107) g ( x nt , θ ∗ ) (cid:107) 2 ) . The gradient noise of DISCOVER is given by the following : u t + 1 ( θ t ) = 1 | B t | · (cid:88) x nt ∈B t (cid:18) g ( x nt , θ t ) − g ( n ) t + ¯ g t − g ( θ t ) (cid:19) = 1 | B t | (cid:88) x nt ∈B t u nt + 1 ( θ t ) where u nt + 1 ( θ t ) = g ( x nt , θ t ) − g ( n ) t + ¯ g t − g ( θ t ) To this end , we introduce the ﬁltration F t = { θ i < t + 1 } . Since θ t ∈ F t and the cluster n is selected with probability p n , it holds that E [ u nt + 1 ( θ t ) | F t ] = E (cid:32) g ( x nt , θ t ) − g ( n ) t + ¯ g t − g ( θ t ) | F t (cid:33) = E (cid:32) g ( x nt , θ t ) − g ( θ t ) | F t (cid:33) + E (cid:32) ¯ g t − g ( n ) t | F t (cid:33) = E (cid:32) s t + 1 ( θ t ) | F t (cid:33) + ¯ g t − N (cid:88) n = 1 p n g ( n ) t = 0 Because s t + 1 ( θ t ) = g ( x nt , θ t ) − g ( θ t ) and ¯ g t = (cid:80) Nn = 1 p n g ( n ) t . From ( Yuan et al . , 2019 ) it holds that : E (cid:32) (cid:107) u nt + 1 ( θ t ) (cid:107) 2 | F t (cid:33) ≤ C 1 (cid:107) ˜ θ t (cid:107) 2 + C 2 + 2 N (cid:88) n = 1 p n (cid:107) g ( n ) t − g n ( θ ∗ ) (cid:107) 2 Now we can give properties of DISCOVER gradient noise by proving Lemma 3 : Discover E [ u t + 1 ( θ t ) | F t ] = E (cid:32) 1 | B t | (cid:88) x nt ∈B t u nt + 1 ( θ t ) | F t (cid:33) = 1 | B t | (cid:88) x nt ∈B t E (cid:32) u nt + 1 ( θ t ) | F t (cid:33) = 0 E (cid:32) (cid:107) u t + 1 ( θ t ) (cid:107) 2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) F t (cid:33) = E (cid:32) (cid:107) 1 | B t | (cid:88) x nt ∈B t u nt + 1 ( θ t ) (cid:107) 2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) F t (cid:33) = 1 | B t | 2 · E (cid:32) (cid:107) (cid:88) x nt ∈B t u nt + 1 ( θ t ) (cid:107) 2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) F t (cid:33) = 1 | B t | 2 · E (cid:32) (cid:88) x nt ∈B t (cid:13) (cid:13)(cid:13)(cid:13)(cid:13) u nt + 1 ( θ t ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:12) (cid:12)(cid:12)(cid:12)(cid:12) F t (cid:33) + 1 | B t | 2 · E (cid:32) (cid:88) x nt ∈B t (cid:88) x mt (cid:54) = x nt (cid:32) u nt + 1 ( θ t ) (cid:33) T (cid:32) u mt + 1 ( θ t ) (cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) F t (cid:33) = 1 | B t | 2 · E (cid:32) (cid:88) x nt ∈B t (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) u nt + 1 ( θ t ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) F t (cid:33) = 1 | B t | 2 · (cid:88) x nt ∈B t E (cid:32)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) u nt + 1 ( θ t ) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) F t (cid:33) In the above proof we use the fact that the gradient noise are independent from one cluster to another meaning that for n (cid:54) = m : E (cid:18) u nt + 1 ( θ t ) u mt + 1 ( θ t ) | F t (cid:19) = E (cid:18) u nt + 1 ( θ t ) | F t (cid:19) E (cid:18) u mt + 1 ( θ t ) | F t (cid:19) = 0 So we have : E (cid:32) (cid:107) u t + 1 ( θ t ) (cid:107) 2 | F t (cid:33) = 1 | B t | 2 · (cid:88) x nt ∈B t E (cid:32) (cid:107) u nt + 1 ( θ t ) (cid:107) 2 | F t (cid:33) ≤ 1 | B t | 2 · (cid:88) x nt ∈B t (cid:32) C 1 (cid:107) ˜ θ t (cid:107) 2 + C 2 + 2 N (cid:88) n = 1 p n (cid:107) g ( n ) t − g n ( θ ∗ ) (cid:107) 2 (cid:33) ≤ 1 | B t | · C 1 (cid:107) ˜ θ t (cid:107) 2 + 1 | B t | · C 2 + 2 | B t | · N (cid:88) n = 1 p n (cid:107) g ( n ) t − g n ( θ ∗ ) (cid:107) 2 Discover Lemma 4 Under the same assumptions as in Section 3 , for a batch of size | B t | , deﬁning G t to be : G t = N (cid:88) n = 1 p n E (cid:32) (cid:107) g ( n ) t − g n ( θ ∗ ) (cid:107) 2 (cid:33) The two inequalities hold : E (cid:32) (cid:107) ˜ θ t + 1 (cid:107) 2 (cid:33) ≤ (cid:32) 1 − 2 µν + µ 2 ( δ 2 + C 1 | B t | ) (cid:33) E ( (cid:107) ˜ θ t (cid:107) 2 ) + µ 2 | B t | C 2 + 2 µ 2 | B t | · G t G t + 1 ≤ ( 1 − α ) · G t + 3 αδ 2 · E ( (cid:107) ˜ θ t (cid:107) 2 ) + αC 2 ( 11 ) 9 . 2 . Proof of Lemma 4 Before proving Lemma 4 , we are going to use the following lemma from ( Sayed , 2014a ) : Lemma 5 ( Mean - value theorem : Real arguments ) Consider a real - valued and twice - differentiable function g ( z ) ∈ R , where z ∈ R M is real - valued . Then for any M - dimensional vectors z 0 and ∆ z , the following increment equalities hold : g ( z 0 + ∆ z ) − g ( z 0 ) = (cid:32) (cid:90) 1 0 ∇ z g ( z 0 + t ∆ z ) dt (cid:33) ∆ z ( 12 ) ∇ z g ( z 0 + ∆ z ) −∇ z g ( z 0 ) = ( ∆ z ) T (cid:32) (cid:90) 1 0 ∇ 2 z g ( z 0 + r ∆ z ) dr (cid:33) ( 13 ) From Lemma 5 , denoting ˜ θ t : = θ ∗ − θ t , g ( θ ) = ∇ (cid:96) ( f ( θ , x n ) ) , we have : g ( θ t ) = − (cid:32) (cid:90) 1 0 ∇ θ g ( θ ∗ − t ˜ θ t ) dt (cid:33) ˜ θ t : = − H t ˜ θ t where we are introducing the symmetric time - variant matrix which is deﬁned in terms of the Hessian of the cost function . H t : = (cid:90) 1 0 ∇ θ g ( θ ∗ − t ˜ θ t ) dt Considering DISCOVER recursion in Algorithm 1 under the same assumptions as in Section 3 , for a batch of size | B t | and using the particular result of Lemma 5 we have : θ t + 1 = θ t − µ | B t | · (cid:88) x nt ∈B t (cid:18) g ( x nt , θ t ) − g ( n ) t + ¯ g t (cid:19) − θ t + 1 = − θ t + µ | B t | · (cid:88) x nt ∈B t (cid:18) g ( x nt , θ t ) − g ( n ) t + ¯ g t (cid:19) = − θ t + µ | B t | · (cid:88) x nt ∈B t (cid:18) g ( x nt , θ t ) − g ( n ) t + ¯ g t − g ( θ t ) + g ( θ t ) (cid:19) = − θ t + µ | B t | · (cid:88) x nt ∈B t (cid:18) g ( x nt , θ t ) − g ( n ) t + ¯ g t − g ( θ t ) (cid:19) + µ · g ( θ t ) = − θ t + µ · g ( θ t ) + µ · u t + 1 ( θ t ) ˜ θ t + 1 = ˜ θ t − µ · H t · ˜ θ t + µ · u t + 1 ( θ t ) ˜ θ t + 1 = ( I − µ · H t ) ˜ θ t + µ · u t + 1 ( θ t ) Since : 0 < νI d < ∇ g ( θ ) < δI d where d is the dimension of θ and ∇ g ( θ ) is the Hessian Matrix , we can derive : ( 1 − µδ ) I d < I d − µH t < ( 1 − µν ) I d for all t . Using the fact that I d − µH t is a symmetric matrix , we have that its 2 - induced norm is equal to its spectral radius so that : (cid:107) I − µ · H t (cid:107) 2 = (cid:18) ρ ( I − µ · H t ) (cid:19) 2 ≤ max (cid:40) ( 1 − µδ ) 2 , ( 1 − µν ) 2 (cid:41) = max (cid:40) 1 − 2 µδ + µ 2 δ 2 , 1 − 2 µν + µ 2 ν 2 (cid:41) (cid:107) I − µ · H t (cid:107) 2 ≤ 1 − 2 µδ + µ 2 δ 2 Discover We got the last inequality because ν ≤ δ Using the previous result , we have : (cid:107) ˜ θ t + 1 (cid:107) 2 ≤ (cid:107) I − µ · H t (cid:107) 2 · (cid:107) ˜ θ t (cid:107) 2 + µ 2 · (cid:107) u t + 1 ( θ t ) (cid:107) 2 E (cid:32) (cid:107) ˜ θ t + 1 (cid:107) 2 | F t (cid:33) ≤ (cid:107) I − µ · H t (cid:107) 2 · E ( (cid:107) ˜ θ t (cid:107) 2 | F t ) + µ 2 E (cid:32) (cid:107) u t + 1 ( θ t ) (cid:107) 2 | F t (cid:33) ≤ ( 1 − 2 µν + µ 2 δ 2 ) E ( (cid:107) ˜ θ t (cid:107) 2 | F t ) + µ 2 (cid:32) 1 | B t | C 1 E ( (cid:107) ˜ θ t (cid:107) 2 | F t ) + 1 | B t | C 2 + 2 | B t | N (cid:88) n = 1 p n (cid:107) g ( n ) t − g n ( θ ∗ ) (cid:107) 2 (cid:33) ≤ (cid:32) 1 − 2 µν + µ 2 ( δ 2 + C 1 | B t | ) (cid:33) E ( (cid:107) ˜ θ t (cid:107) 2 | F t ) + µ 2 | B t | C 2 + 2 µ 2 | B t | · N (cid:88) n = 1 p n (cid:107) g ( n ) t − g n ( θ ∗ ) (cid:107) 2 Then taking the expectation on both side give us : E (cid:32) (cid:107) ˜ θ t + 1 (cid:107) 2 (cid:33) ≤ (cid:32) 1 − 2 µν + µ 2 ( δ 2 + C 1 | B t | ) (cid:33) E ( (cid:107) ˜ θ t (cid:107) 2 ) + µ 2 | B t | C 2 + 2 µ 2 | B t | · N (cid:88) n = 1 p n E ( (cid:107) g ( n ) t − g n ( θ ∗ ) (cid:107) 2 ) which conclude the proof . The second equation of Lemma 4 is obtained from Lemma 2 of ( Yuan et al . , 2019 ) . Discover 9 . 3 . Proof of Theorem 1 From Lemma 4 , we have : E ( (cid:107) ˜ θ t + 1 (cid:107) 2 ) + γG t + 1 ≤ (cid:32) 1 − 2 µν + µ 2 ( δ 2 + C 1 | B t | ) (cid:33) E ( (cid:107) ˜ θ t (cid:107) 2 ) + µ 2 | B t | C 2 + 2 µ 2 | B t | · G t + γ (cid:32) ( 1 − α ) · G t + 3 αδ 2 · E ( (cid:107) ˜ θ t (cid:107) 2 ) + αC 2 (cid:33) = (cid:34) 1 − 2 µν + µ 2 ( δ 2 + C 1 | B t | ) + 3 αγδ 2 (cid:35) E ( (cid:107) ˜ θ t (cid:107) 2 ) + ( µ 2 | B t | + αγ ) C 2 + ( 2 µ 2 | B t | + γ ( 1 − α ) ) · G t ≤ (cid:34) 1 − 2 µν + K (cid:35) E ( (cid:107) ˜ θ t (cid:107) 2 ) + ( µ 2 | B t | + αγ ) C 2 + ( 2 µ 2 | B t | + γ ( 1 − α ) ) · G t ≤ (cid:34) 1 − 2 µν + K (cid:35)(cid:32) E ( (cid:107) ˜ θ t (cid:107) 2 ) + (cid:18) 2 µ 2 | B t | + γ ( 1 − α ) (cid:19) 1 − 2 µν + K · G t (cid:33) + ( µ 2 | B t | + αγ ) C 2 ≤ (cid:34) 1 − 2 µν + K (cid:35)(cid:32) E ( (cid:107) ˜ θ t (cid:107) 2 ) + (cid:18) 2 µ 2 | B t | + γ ( 1 − α ) (cid:19) 1 − 2 µν · G t (cid:33) + ( µ 2 | B t | + αγ ) C 2 ≤ ( 1 − µν ) (cid:32) E ( (cid:107) ˜ θ t (cid:107) 2 ) + (cid:18) 2 µ 2 | B t | + γ ( 1 − α ) (cid:19) 1 − 2 µν · G t (cid:33) + ( µ 2 | B t | + αγ ) C 2 ≤ ( 1 − µν ) (cid:32) E ( (cid:107) ˜ θ t (cid:107) 2 ) + γ · G t (cid:33) + ( µ 2 | B t | + αγ ) C 2 E ( (cid:107) ˜ θ t + 1 (cid:107) 2 ) + γG t + 1 ≤ ( 1 − µν ) (cid:32) E ( (cid:107) ˜ θ t (cid:107) 2 ) + γ · G t (cid:33) + 4 µ 2 | B t | · C 2 where K = 3 δ 2 ( µ 2 + 2 µ 2 | B t | + αγ ) . Discover Since µ satisﬁes µ ≤ α 6 ν α − 6 µν ≥ 0 3 α − 6 µν ≥ 2 α 3 α ≥ 2 α − 2 µν γ = 3 µ 2 α | B t | ≥ 2 µ 2 | B t | ( α − 2 µν ) γ ≥ 2 µ 2 | B t | ( α − 2 µν ) ⇔ γ ≥ (cid:18) 2 µ 2 | B t | + γ ( 1 − α ) (cid:19) 1 − 2 µν ( 14 ) Because γ = (cid:18) 2 µ 2 | B | + γ ( 1 − α ) (cid:19) 1 − 2 µν ⇔ γ = 2 µ 2 | B t | ( α − 2 µν ) ( 15 ) Also since µ satisﬁes µ ≤ ν | B t | 3 δ 2 ( | B t | + 5 ) It implies that : µ ( 1 + 5 | B t | ) ≤ ν 3 δ 2 µ 2 ( 1 + 5 | B t | ) ≤ µν 3 δ 2 µ 2 + 2 µ 2 | B t | + 3 µ 2 | B t | ≤ µν 3 δ 2 µ 2 + 2 µ 2 | B t | + αγ ≤ µν 3 δ 2 3 δ 2 ( µ 2 + 2 µ 2 | B t | + αγ ) ≤ µν 1 − 2 µν + 3 δ 2 ( µ 2 + 2 µ 2 | B t | + αγ ) ≤ 1 − 2 µν + µν 1 − 2 µν + K ≤ 1 − µν So in conclusion : 1 − 2 µν + K ≤ 1 − µν ( 16 ) In summary we have : E ( (cid:107) ˜ θ t + 1 (cid:107) 2 ) + γG t + 1 ≤ ( 1 − µν ) · (cid:32) E ( (cid:107) ˜ θ t (cid:107) 2 ) + γG t (cid:33) + 4 µ 2 | B t | C 2 Discover Iterating recursion above we get : E ( (cid:107) ˜ θ t + 1 (cid:107) 2 ) ≤ E ( (cid:107) ˜ θ t + 1 (cid:107) 2 ) + γG t + 1 ≤ ( 1 − µν ) · (cid:32) E ( (cid:107) ˜ θ t (cid:107) 2 ) + γG t (cid:33) + 4 µ 2 | B t | C 2 ≤ ( 1 − µν ) t + 1 · (cid:32) E ( (cid:107) ˜ θ 0 (cid:107) 2 ) + γG 0 (cid:33) + 4 µ 2 | B t | C 2 t (cid:88) k = 0 ( 1 − µν ) k ≤ ( 1 − µν ) t + 1 · (cid:32) E ( (cid:107) ˜ θ 0 (cid:107) 2 ) + γG 0 (cid:33) + 1 1 − ( 1 − µν ) 4 µ 2 | B t | C 2 ≤ ( 1 − µν ) t + 1 · (cid:32) E ( (cid:107) ˜ θ 0 (cid:107) 2 ) + γG 0 (cid:33) + 4 µ ν | B t | C 2 That implies : E ( (cid:107) ˜ θ t + 1 (cid:107) 2 ) ≤ ( 1 − µν ) t + 1 · (cid:32) E ( (cid:107) ˜ θ 0 (cid:107) 2 ) + γG 0 (cid:33) + 4 µ ν | B t | C 2 ( 17 ) So that : lim sup t → + ∞ E ( (cid:107) θ ∗ − θ t + 1 (cid:107) 2 ) = O ( µ | B t | C 2 ) = O ( µ · σ 2 in / | B t | ) ( 18 )