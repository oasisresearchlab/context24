Proceedings of the Second Workshop on Statistical Machine Translation , pages 228 – 231 , Prague , June 2007 . c (cid:13) 2007 Association for Computational Linguistics Meteor : An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments Alon Lavie and Abhaya Agarwal Language Technologies Institute Carnegie Mellon University Pittsburgh , PA , 15213 , USA { alavie , abhayaa } @ cs . cmu . edu Abstract Meteor is an automatic metric for Ma - chine Translation evaluation which has been demonstrated to have high levels of corre - lation with human judgments of translation quality , signiﬁcantly outperforming the more commonly used Bleu metric . It is one of several automatic metrics used in this year’s shared task within the ACL WMT - 07 work - shop . This paper recaps the technical de - tails underlying the metric and describes re - cent improvements in the metric . The latest release includes improved metric parameters and extends the metric to support evalua - tion of MT output in Spanish , French and German , in addition to English . 1 Introduction Automatic Metrics for MT evaluation have been re - ceiving signiﬁcant attention in recent years . Evalu - ating an MT system using such automatic metrics is much faster , easier and cheaper compared to human evaluations , which require trained bilingual evalua - tors . Automatic metrics are useful for comparing the performance of diﬀerent systems on a common translation task , and can be applied on a frequent and ongoing basis during MT system development . The most commonly used MT evaluation metric in recent years has been IBM’s Bleu metric ( Papineni et al . , 2002 ) . Bleu is fast and easy to run , and it can be used as a target function in parameter op - timization training procedures that are commonly used in state - of - the - art statistical MT systems ( Och , 2003 ) . Various researchers have noted , however , var - ious weaknesses in the metric . Most notably , Bleu does not produce very reliable sentence - level scores . Meteor , as well as several other proposed metrics such as GTM ( Melamed et al . , 2003 ) , TER ( Snover et al . , 2006 ) and CDER ( Leusch et al . , 2006 ) aim to address some of these weaknesses . Meteor , initially proposed and released in 2004 ( Lavie et al . , 2004 ) was explicitly designed to im - prove correlation with human judgments of MT qual - ity at the segment level . Previous publications on Meteor ( Lavie et al . , 2004 ; Banerjee and Lavie , 2005 ) have described the details underlying the met - ric and have extensively compared its performance with Bleu and several other MT evaluation met - rics . This paper recaps the technical details underly - ing Meteor and describes recent improvements in the metric . The latest release extends Meteor to support evaluation of MT output in Spanish , French and German , in addition to English . Furthermore , several parameters within the metric have been opti - mized on language - speciﬁc training data . We present experimental results that demonstrate the improve - ments in correlations with human judgments that re - sult from these parameter tunings . 2 The Meteor Metric Meteor evaluates a translation by computing a score based on explicit word - to - word matches be - tween the translation and a given reference trans - lation . If more than one reference translation is available , the translation is scored against each refer - ence independently , and the best scoring pair is used . Given a pair of strings to be compared , Meteor cre - ates a word alignment between the two strings . An alignment is mapping between words , such that ev - ery word in each string maps to at most one word in the other string . This alignment is incrementally produced by a sequence of word - mapping modules . The “exact” module maps two words if they are ex - actly the same . The “porter stem” module maps two words if they are the same after they are stemmed us - ing the Porter stemmer . The “WN synonymy” mod - ule maps two words if they are considered synonyms , based on the fact that they both belong to the same “synset” in WordNet . The word - mapping modules initially identify all 228 possible word matches between the pair of strings . We then identify the largest subset of these word mappings such that the resulting set constitutes an alignment as deﬁned above . If more than one maxi - mal cardinality alignment is found , Meteor selects the alignment for which the word order in the two strings is most similar ( the mapping that has the least number of “crossing” unigram mappings ) . The order in which the modules are run reﬂects word - matching preferences . The default ordering is to ﬁrst apply the “exact” mapping module , followed by “porter stemming” and then “WN synonymy” . Once a ﬁnal alignment has been produced between the system translation and the reference translation , the Meteor score for this pairing is computed as follows . Based on the number of mapped unigrams found between the two strings ( m ) , the total num - ber of unigrams in the translation ( t ) and the total number of unigrams in the reference ( r ) , we calcu - late unigram precision P = m / t and unigram recall R = m / r . We then compute a parameterized har - monic mean of P and R ( van Rijsbergen , 1979 ) : F mean = P · R α · P + ( 1 − α ) · R Precision , recall and Fmean are based on single - word matches . To take into account the extent to which the matched unigrams in the two strings are in the same word order , Meteor computes a penalty for a given alignment as follows . First , the sequence of matched unigrams between the two strings is di - vided into the fewest possible number of “chunks” such that the matched unigrams in each chunk are adjacent ( in both strings ) and in identical word or - der . The number of chunks ( ch ) and the number of matches ( m ) is then used to calculate a fragmenta - tion fraction : frag = ch / m . The penalty is then computed as : Pen = γ · frag β The value of γ determines the maximum penalty ( 0 ≤ γ ≤ 1 ) . The value of β determines the functional relation between fragmentation and the penalty . Finally , the Meteor score for the align - ment between the two strings is calculated as : score = ( 1 − Pen ) · F mean In all previous versions of Meteor , the values of the three parameters mentioned above were set to be : α = 0 . 9 , β = 3 . 0 and γ = 0 . 5 , based on experimen - tation performed in early 2004 . In the latest release , we tuned these parameters to optimize correlation with human judgments based on more extensive ex - perimentation , as reported in section 4 . 3 Meteor Implementations for Spanish , French and German We have recently expanded the implementation of Meteor to support evaluation of translations in Spanish , French and German , in addition to English . Two main language - speciﬁc issues required adapta - tion within the metric : ( 1 ) language - speciﬁc word - matching modules ; and ( 2 ) language - speciﬁc param - eter tuning . The word - matching component within the English version of Meteor uses stemming and synonymy modules in constructing a word - to - word alignment between translation and reference . The re - sources used for stemming and synonymy detection for English are the Porter Stemmer ( Porter , 2001 ) and English WordNet ( Miller and Fellbaum , 2007 ) . In order to construct instances of Meteor for Span - ish , French and German , we created new language - speciﬁc “stemming” modules . We use the freely available Perl implementation packages for Porter stemmers for the three languages ( Humphrey , 2007 ) . Unfortunately , we have so far been unable to obtain freely available WordNet resources for these three languages . Meteor versions for Spanish , French and German therefore currently include only “exact” and “stemming” matching modules . We are investi - gating the possibility of developing new synonymy modules for the various languages based on alterna - tive methods , which could then be used in place of WordNet . The second main language - speciﬁc issue which required adaptation is the tuning of the three parameters within Meteor , described in section 4 . 4 Optimizing Metric Parameters The original version of Meteor ( Banerjee and Lavie , 2005 ) has instantiated values for three pa - rameters in the metric : one for controlling the rela - tive weight of precision and recall in computing the Fmean score ( α ) ; one governing the shape of the penalty as a function of fragmentation ( β ) and one for the relative weight assigned to the fragmenta - tion penalty ( γ ) . In all versions of Meteor to date , these parameters were instantiated with the values α = 0 . 9 , β = 3 . 0 and γ = 0 . 5 , based on early data ex - perimentation . We recently conducted a more thor - ough investigation aimed at tuning these parameters based on several available data sets , with the goal of ﬁnding parameter settings that maximize correlation with human judgments . Human judgments come in the form of “adequacy” and “ﬂuency” quantitative scores . In our experiments , we looked at optimizing parameters for each of these human judgment types separately , as well as optimizing parameters for the sum of adequacy and ﬂuency . Parameter adapta - 229 Corpus Judgments Systems NIST 2003 Ara - to - Eng 3978 6 NIST 2004 Ara - to - Eng 347 5 WMT - 06 Eng - to - Fre 729 4 WMT - 06 Eng - to - Ger 756 5 WMT - 06 Eng - to - Spa 1201 7 Table 1 : Corpus Statistics for Various Languages tion is also an issue in the newly created Meteor instances for other languages . We suspected that parameters that were optimized to maximize corre - lation with human judgments for English would not necessarily be optimal for other languages . 4 . 1 Data For English , we used the NIST 2003 Arabic - to - English MT evaluation data for training and the NIST 2004 Arabic - to - English evaluation data for testing . For Spanish , German and French we used the evaluation data provided by the shared task at last year’s WMT workshop . Sizes of various corpora are shown in Table 1 . Some , but not all , of these data sets have multiple human judgments per translation hypothesis . To partially address human bias issues , we normalize the human judgments , which trans - forms the raw judgment scores so that they have sim - ilar distributions . We use the normalization method described in ( Blatz et al . , 2003 ) . Multiple judgments are combined into a single number by taking their average . 4 . 2 Methodology We performed a “hill climbing” search to ﬁnd the parameters that achieve maximum correlation with human judgments on the training set . We use Pear - son’s correlation coeﬃcient as our measure of corre - lation . We followed a “leave one out” training proce - dure in order to avoid over - ﬁtting . When n systems were available for a particular language , we train the parameters n times , leaving one system out in each training , and pooling the segments from all other systems . The ﬁnal parameter values are calculated as the mean of the n sets of trained parameters that were obtained . When evaluating a set of parameters on test data , we compute segment - level correlation with human judgments for each of the systems in the test set and then report the mean over all systems . 4 . 3 Results 4 . 3 . 1 Optimizing for Adequacy and Fluency We trained parameters to obtain maximum cor - relation with normalized adequacy and ﬂuency judg - Adequacy Fluency Sum α 0 . 82 0 . 78 0 . 81 β 1 . 0 0 . 75 0 . 83 γ 0 . 21 0 . 38 0 . 28 Table 2 : Optimal Values of Tuned Parameters for Diﬀerent Criteria for English Adequacy Fluency Sum Original 0 . 6123 0 . 4355 0 . 5704 Adequacy 0 . 6171 0 . 4354 0 . 5729 Fluency 0 . 6191 0 . 4502 0 . 5818 Sum 0 . 6191 0 . 4425 0 . 5778 Table 3 : Pearson Correlation with Human Judg - ments on Test Data for English ments separately and also trained for maximal corre - lation with the sum of the two . The resulting optimal parameter values on the training corpus are shown in Table 2 . Pearson correlations with human judgments on the test set are shown in Table 3 . The optimal parameter values found are somewhat diﬀerent than our previous metric parameters ( lower values for all three parameters ) . The new parame - ters result in moderate but noticeable improvements in correlation with human judgments on both train - ing and testing data . Tests for statistical signiﬁcance using bootstrap sampling indicate that the diﬀer - ences in correlation levels are all signiﬁcant at the 95 % level . Another interesting observation is that precision receives slightly more “weight” when op - timizing correlation with ﬂuency judgments ( versus when optimizing correlation with adequacy ) . Recall , however , is still given more weight than precision . Another interesting observation is that the value of γ is higher for ﬂuency optimization . Since the frag - mentation penalty reﬂects word - ordering , which is closely related to ﬂuency , these results are consistent with our expectations . When optimizing correlation with the sum of adequacy and ﬂuency , optimal val - ues fall in between the values found for adequacy and ﬂuency . 4 . 3 . 2 Parameters for Other Languages Similar to English , we trained parameters for Spanish , French and German on the available WMT - 06 training data . We optimized for maximum corre - lation with human judgments of adequacy , ﬂuency and for the sum of the two . Resulting parameters are shown in Table 4 . 3 . 2 . For all three languages , the parameters that were found to be optimal were quite diﬀerent than those that were found for English , and using the language - speciﬁc optimal parameters re - 230 Adequacy Fluency Sum French : α 0 . 86 0 . 74 0 . 76 β 0 . 5 0 . 5 0 . 5 γ 1 . 0 1 . 0 1 . 0 German : α 0 . 95 0 . 95 0 . 95 β 0 . 5 0 . 5 0 . 5 γ 0 . 6 0 . 8 0 . 75 Spanish : α 0 . 95 0 . 62 0 . 95 β 1 . 0 1 . 0 1 . 0 γ 0 . 9 1 . 0 0 . 98 Table 4 : Tuned Parameters for Diﬀerent Languages sults in signiﬁcant gains in Pearson correlation levels with human judgments on the training data ( com - pared with those obtained using the English opti - mal parameters ) 1 . Note that the training sets used for these optimizations are comparatively very small , and that we currently do not have unseen test data to evaluate the parameters for these three languages . Further validation will need to be performed once ad - ditional data becomes available . 5 Conclusions In this paper we described newly developed language - speciﬁc instances of the Meteor metric and the process of optimizing metric parameters for diﬀerent human measures of translation quality and for diﬀerent languages . Our evaluations demonstrate that parameter tuning improves correlation with hu - man judgments . The stability of the optimized pa - rameters on diﬀerent data sets remains to be inves - tigated for languages other than English . We are currently exploring broadening the set of features used in Meteor to include syntax - based features and alternative notions of synonymy . The latest re - lease of Meteor is freely available on our website at : http : / / www . cs . cmu . edu / ~ alavie / METEOR / Acknowledgements The work reported in this paper was supported by NSF Grant IIS - 0534932 . References Satanjeev Banerjee and Alon Lavie . 2005 . ME - TEOR : An Automatic Metric for MT Evalua - tion with Improved Correlation with Human Judg - ments . In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures 1 Detailed tables are not included for lack of space . for Machine Translation and / or Summarization , pages 65 – 72 , Ann Arbor , Michigan , June . John Blatz , Erin Fitzgerald , George Foster , Simona Gandrabur , Cyril Goutte , Alex Kulesza , Alberto Sanchis , and Nicola Ueﬃng . 2003 . Conﬁdence Es - timation for Machine Translation . Technical Re - port Natural Language Engineering Workshop Fi - nal Report , Johns Hopkins University . Marvin Humphrey . 2007 . Perl In - terface to Snowball Stemmers . http : / / search . cpan . org / creamyg / Lingua - Stem - Snowball - 0 . 941 / lib / Lingua / Stem / Snowball . pm . Alon Lavie , Kenji Sagae , and Shyamsundar Jayara - man . 2004 . The Signiﬁcance of Recall in Auto - matic Metrics for MT Evaluation . In Proceedings of the 6th Conference of the Association for Ma - chine Translation in the Americas ( AMTA - 2004 ) , pages 134 – 143 , Washington , DC , September . Gregor Leusch , Nicola Ueﬃng , and Hermann Ney . 2006 . CDER : Eﬃcient MT Evaluation Using Block Movements . In Proceedings of the Thir - teenth Conference of the European Chapter of the Association for Computational Linguistics . I . Dan Melamed , Ryan Green , and Joseph Turian . 2003 . Precision and Recall of Machine Transla - tion . In Proceedings of the HLT - NAACL 2003 Conference : Short Papers , pages 61 – 63 , Edmon - ton , Alberta . George Miller and Christiane Fellbaum . 2007 . Word - Net . http : / / wordnet . princeton . edu / . Franz Josef Och . 2003 . Minimum Error Rate Train - ing for Statistical Machine Translation . In Pro - ceedings of the 41st Annual Meeting of the Asso - ciation for Computational Linguistics . Kishore Papineni , Salim Roukos , Todd Ward , and Wei - Jing Zhu . 2002 . BLEU : a Method for Auto - matic Evaluation of Machine Translation . In Pro - ceedings of 40th Annual Meeting of the Association for Computational Linguistics ( ACL ) , pages 311 – 318 , Philadelphia , PA , July . Martin Porter . 2001 . The Porter Stem - ming Algorithm . http : / / www . tartarus . org / mar - tin / PorterStemmer / index . html . Matthew Snover , Bonnie Dorr , Richard Schwartz , Linnea Micciulla , and John Makhoul . 2006 . A Study of Translation Edit Rate with Targeted Hu - man Annotation . In Proceedings of the 7th Confer - ence of the Association for Machine Translation in the Americas ( AMTA - 2006 ) , pages 223 – 231 , Cam - bridge , MA , August . C . van Rijsbergen , 1979 . Information Retrieval . Butterworths , London , UK , 2nd edition . 231