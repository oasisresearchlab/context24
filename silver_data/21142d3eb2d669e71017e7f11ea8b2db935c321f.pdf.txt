Understanding International Benchmarks on Student Engagement Awareness and Research Alignment from a Computer Science Perspective Michael Morgan Monash University Melbourne , Australia michael . morgan @ monash . edu Jane Sinclair University of Warwick Coventry , UK J . E . Sinclair @ warwick . ac . uk Matthew Butler Monash University Melbourne , Australia matthew . butler @ monash . edu Neena Thota University of Massachusetts Amherst , USA nthota @ cs . umass . edu Janet Fraser Monash University Melbourne , Australia Janet . Fraser @ monash . edu Gerry Cross Mount Royal University Calgary , Canada gcross @ mtroyal . ca Jana Jackova Matej Bel University Banska Bystrica , Slovakia jana . jackova @ umb . sk ABSTRACT There is an increasing trend to use national survey instruments to measure student engagement . Unfortunately , Computer Science ( CS ) rates poorly on a number of measures in these surveys , even when compared to related STEM disciplines . Initial research sug - gests reasons for this poor performance may include a lack of awareness by CS academics of these instruments and the student engagement measures on which they are based , and a misalign - ment between these measures and the research focus ( and teaching practice ) of CS educators . This working group carried out an inves - tigation of major engagement instruments to examine the measures they embody and track the achievement of CS with respect to the major international benchmarks . A comprehensive research map - ping exercise was then conducted to examine the focus of current CS education research and its alignment to student engagement measures on which the instruments are based . The process enabled identification of examples of best practice in student engagement research in CS education . In order to better understand CS aca - demics’ perspectives on engagement a series of interviews were also conducted with CS staff . Our findings indicate that CS engagement results are , if any - thing , declining further . Analysis of CS education research liter - ature shows that many authors refer to “engagement” ( and their aim to increase it ) but few attach a clear meaning to the term or offer evidence to support a link to improved engagement . Further , many initiatives reported would be unlikely to tick the boxes of Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM mustbehonored . Abstractingwithcreditispermitted . Tocopyotherwise , orrepublish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . ITICSE - WGR’17 , July 3 – 5 , 2017 , Bologna , Italy © 2017 Association for Computing Machinery . ACM ISBN 978 - 1 - 4503 - 5627 - 5 / 17 / 07 . . . $ 15 . 00 https : / / doi . org / 10 . 1145 / 3174781 . 3174782 the narrow , behaviourally - focussed measures covered by the ma - jor instruments . Staff interviews revealed a wide variety of beliefs about what student engagement means and what should be done to promote it in CS , including the view that many activities measured in the instruments are counter - productive for CS . This work aims to promote a greater awareness of the international benchmarks and the aspects of student engagement they measure . The results reported here can be used by CS educators to inform decisions on strategies to improve engagement and how these might relate to existing survey measures . CCS CONCEPTS • Social and professional topics → Computing education ; KEYWORDS Student engagement ; international benchmarks ; computing educa - tion ; higher education ACM Reference Format : Michael Morgan , Jane Sinclair , Matthew Butler , Neena Thota , Janet Fraser , Gerry Cross , and Jana Jackova . 2017 . Understanding International Bench - marks on Student Engagement : Awareness and Research Alignment from a Computer Science Perspective . In ITICSE - WGR’17 : ITiCSE 2017 Working Group Reports , July 3 – 5 , 2017 , Bologna , Italy . ACM , New York , NY , USA , 24 pages . https : / / doi . org / 10 . 1145 / 3174781 . 3174782 1 INTRODUCTION Student disengagement in Higher Education is a major concern internationally , with many staff and institutions worried by is - sues such as poor attendance , substandard academic performance and student drop - out . In essence student engagement relates to the extent to which students become involved in activities that support improved educational outcomes [ 50 ] . A growing body of research points to the link between better engagement and im - proved attainment [ 16 , 26 ] . In recent years , student self - report Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 1 survey instruments have become the predominant means by which engagement information is gathered . Cheap to administer to many thousands of students and easy to analyse , prominent examples of these instruments include the National Survey of Student Engage - ment ( NSSE ) [ 36 ] in North America , Australia’s Student Experience Survey ( SES ) [ 8 ] and the United Kingdom Engagement Survey ( UKES ) [ 35 ] . These surveys are generally administered to first and final year students and examine the wider university experience rather than individual units . Because student engagement surveys seek to measure engagement in educationally productive activities they are different from student satisfaction surveys that tend to focus on student preferences . The headline news for Computer Science ( CS ) is that our stu - dents are reporting low levels of engagement on many measures in these surveys [ 6 , 46 ] . The obvious question is - why ? Are the survey results genuinely reflecting the engagement levels of CS students ? If so , is CS teaching internationally deficient in its atten - tion to engagement ? Or are our students different in some way ? Alternately , the construction and interpretation of questions in the surveys could be an issue . A lack of understanding of these issues hinders attempts to interpret the low scores reported or to decide appropriate recommendations for action in response to them . Since results from these student engagement surveys are widely publicised [ 8 , 35 , 36 ] and may be used by students for course ( degree program ) selection and by administrators to assess courses , it is im - portant for the CS education discipline to have a greater awareness of the instruments and their design . We need to better understand how current computing education research relates to engagement , in particular mapping its relationship to the instruments and the engagement measures they use . This , in addition to exploring why the measures are considered important and how they align to our teaching and research practice , is crucial if we are to improve the performance of CS in these international benchmarks . However , the fundamental issue is not about ensuring boxes are ticked so that CS achieves good survey scores : revealing the picture of engage - ment in CS teaching and CS education research ( whether or not this is accurately reflected by current surveys ) paves the way for genuine understanding of current best practice and general areas for improvement . This can then lead to suggestions of ways that CS teaching can better engage students , as well as a deeper un - derstanding of how to interpret CS results on student engagement measures . Given the global nature of these benchmarks on student engage - ment , an ITiCSE working group was formed to obtain some of the perspectives needed to address these challenges . The research investigated the conceptual alignment between international bench - marks on student engagement and the CS academic community’s educational research and practice . The working group facilitated international input to the following activities : Stage 1 - A study examining the trends and variations in the data for the computing discipline from several international student engagement instruments ( NSSE , SES , UKES ) . Stage 2 - An analysis of current CS education research literature with specific focus on initiatives to promote student engagement . Stage 3 - Interviews examining the perceptions of CS academics regarding student engagement and their perspectives on the various survey instrument questions . Stage 4 - Deriving suggestions for ways in which the CS disci - pline can respond to the findings . 2 BACKGROUND AND CONTEXT In order to understand the possible implications of computing stu - dents’ performance in international student engagement bench - marks and to assess the nature of computing education research relating to engagement , it is important to form a clear understand - ing of the concept and how it is used in general education literature . 2 . 1 What is engagement ? It is often possible to spot students who are failing to engage by their lack of attendance , unresponsiveness and general apathy : a state referred to by Krause [ 25 ] as “inertia” since it lacks the positive nature of active disengagement . However , despite being a widely used term in education , student engagement has been defined in many , often conflicting , ways [ 2 , 3 , 19 , 23 , 26 , 27 , 50 ] . Despite this , there is a growing body of evidence that a high level of student engagement is associated with many positive and desirable out - comes including increased learning , lower attrition and increased personal development [ 2 , 16 , 50 ] . Kuh ( instrumental in the development and spread of NSSE ) views engagement as the extent to which students participate in “edu - cational practices that are strongly associated with high levels of learning and personal development” [ 26 , p 12 ] . According to this view , engagement relates to the extent to which students spend time on educationally purposeful or “high impact” activities . It is related to earlier work , for example , that by Astin who believed “It is not so much what the individual thinks or feels , but what the individual does , how he or she behaves , that defines and identifies involvement” [ 2 , p 519 ] . This emphasis has been reflected in the de - velopment of the NSSE survey instrument which assumes a strong link between institutional teaching ( that is , the activities in which students are led to engage ) and actual student engagement . Definitions based on student activity relate to a behavioural con - ception of engagement . However , students’ compliant behaviour does not necessarily indicate real engagement with their studies or suggest that learning is taking place . At a most basic level , students may be physically present at a teaching session but concentrating on other things , such as their social media accounts . Participation in more active learning situations may be a better indicator of the “student’s willingness , need , desire and compulsion to participate in , and be successful in , the learning process promoting higher level thinking for enduring understanding” [ 4 , p . 294 ] which is widely associated with engagement . Definitions based purely on students’ behaviour may be contrasted with those that refer to cognitive aspects of engagement characterised as “a student’s psychological investment in and effort directed towards learning , understanding , or mastering the knowledge , skills or craft” [ 28 , p12 ] . Cognitive en - gagement is often associated with reflective , self - regulated learning and the effective use of deep learning strategies . A third dimension of engagement highlights students’ emotional investment in learning [ 32 ] . Referred to as affective or emotional engagement , it may be evidenced by enthusiasm and interest in a subject and has also been linked to students’ sense of belonging [ 29 ] . Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 2 A number of studies have noted the inadequacy of characterising student engagement on a single dimension [ 20 , 50 ] . It is now often conceptualised as a meta - construct which incorporates behavioural , cognitive and emotional aspects [ 16 ] . While some studies suggest further dimensions should be added , these three form a consistent basis for defining engagement . Conceptualisations of engagement incorporating multiple dimensions are referred to as ‘holistic’ . In a comprehensive survey of student engagement literature Trowler [ 50 ] points to the fact that the behavioural , cognitive and emotional ( affective ) dimensions of student engagement can be expressed in positive or negative forms of engagement . While the objective of many learning activities are expressed in positive terms , in practice some students may be withdrawn , apathetic , avoid en - gagement , or actively disruptive or obstructive . So student engage - ment can be viewed as a continuum from positive engagement through non - engagement to negative engagement . Trowler’s definition of student engagement is also of note : Student engagement is concerned with the interac - tion between the time , effort and other relevant re - sources invested by both students and their institu - tions intended to optimise the student experience and enhance the learning outcomes and development of students and the performance , and reputation of the institution . [ 50 , p3 ] This is a view of engagement which emphasises the role of the institution . It is interesting that teaching is positioned not just as something which can promote and enable student learning - it is seen as part of the very definition of engagement . Even more strik - ing is the inclusion of reference to institutional performance and reputation . Again , this is not referred to as an outcome of evidenc - ing student engagement , but as something which is fundamental to the consideration of engagement . 2 . 2 Engagement , instruments and institutional responses Despite a general recognition of the multi - dimensional nature of engagement and the inadequacy of self - report instruments to mea - sure the concept [ 17 ] , the majority of education research literature relates to the NSSE - style benchmark surveys . Proponents of the approach claim that the measures are all linked by research to high levels of learning and may be used as proxy measures of learning gain , although some researchers have questioned the strength of this link [ 18 ] . Further criticisms of the approach include the validity and reliability of the instrument [ 41 ] , ambiguity of questions [ 39 ] , inaccuracy of self - reporting [ 10 ] and the inapplicability of certain questions to particular subjects [ 46 ] . While mainly behavioural in its conceptual basis , NSSE and other major surveys do incorporate questions which relate to cognitive aspects such as higher order learning . However , by focusing on aspects which the institution can control , it is questionable whether they can represent true affective or cognitive dimensions in a meaningful way [ 23 ] . Trowler’s definition quoted above exemplifies a view in which there is a role for both the student and the institution in co - creating the students’ learning experiences : the student by seeking to ac - tively engage with the experience through educationally effective practices and the institution by creating suitable opportunities to engage in such practices [ 50 ] . This may be helpful in apportioning responsibility and identifying the need for action to improve student engagement . However , as Kahu notes , fundamentally entwining institutional practice and student behaviour “has resulted in a lack of clear distinction between the factors that influence engagement , the measurement of engagement itself , and the consequences of engagement” [ 23 , p760 ] . Many authors ( even those with doubts about the process ) ac - knowledge benefits in the NSSE - style self - reporting survey ap - proach to measuring engagement . It can provide part if not all of the complex picture relating to student engagement . The survey measures certainly focus on many teaching activities which have long been acknowledged as good practice , such as Chickering and Gamson’s well - known “Seven Principles For Good Practice in Un - dergraduate Education” [ 9 ] . From an administrative perspective , a survey allows data to be gathered from thousands of students . Sta - tistical analysis indicates student performance against benchmarks and this can be tracked against previous years’ data [ 46 ] . However , there is a danger that figures may be produced and used with little understanding of their real significance . For example , should a CS department with low scores in Academic Challenge incorporate more 20 page essays in its coursework just to increase its score on this contributing question ? On what basis should such decisions be made ? Which low scores represent areas of concern and which might be deemed less appropriate to the subject ? Further , there are potential funding implications associated with these surveys . In Australia the SES ( formally the Australasian Sur - vey of Student Engagement or AUSSE ) was originally set up to determine funding allocation to institutions but the government has since stepped back from this . In the UK , the government has only temporarily shelved its plans to link student tuition fees to teaching quality measures in part formed by surveys . Therefore it becomes imperative to understand the implications of CS results obtained in these surveys . It is not far - fetched to imagine depart - ments trying to add or modify components to their courses not for educational purposes but to increase survey scores . A better understanding of the surveys and how to interpret results could allow , firstly , an appreciation of appropriate institution response and beneficial adjustment to teaching and , secondly , clarity on the limitations of these surveys in measuring engagement ( in general and for particular groups ) . 2 . 3 Differentiating related concepts One common problem in discussing student engagement is confu - sion caused by its obvious close relationship to a variety of other concepts . For example , there is often lack of clarity concerning the distinction between motivation and engagement - indeed they are sometimes found to be used interchangeably . Other concepts , such as expectation , identity , satisfaction , experience , achievement and so on have obvious connection to engagement . Although this continues to be a matter of some debate , a concep - tual framework proposed by Kahu provides a useful clarification of the distinction between the terms , underlining the fact that they are not the same but that relationships can be distinguished between them [ 23 ] . Kahu distinguishes six elements : socio - cultural context , structural influences ( both from the university and the student Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 3 Figure 1 : Conceptual framework of engagement : Adapted from Kahu ( 2013 ) . themselves ) , psychosocial influences ( again from both university and student ) , actual student engagement ( along the various dimen - sions previously noted ) , proximal consequences ( both academic and social ) , and distal consequences ( further removed than the proximal ones ) . The ordering is important since the earlier generally pre - cede the latter in the manner of ‘antecedents’ and ‘consequences’ . However , it is not a linear process . So , motivation may be seen as an antecedent to engagement , but engagement can help reinforce motivation . In this view , student engagement is seen as a dynamic process that involves multiple inputs , processes and outcomes . We have found Kahu’s framework , shown in Figure 1 , to provide a very helpful guide to clarifying and distinguishing the various terms and relationships which are often conflated both by CS aca - demics when discussing engagement and in CS education research literature . As noted by Kahu , it can also be of use in deciding where to target interventions aimed at increasing student engagement . Fur - thermore , the interlinked nature of psychosocial influences strongly suggests that a student’s major , and the culture and identity associ - ated with it , affects engagement . Indeed , Kahu states , “the need for in - depth study of particular student populations is self - evident” [ 23 , p 766 ] . The variety of different conceptualisations of student engage - ment leads to an obvious discussion of the dimensions on which stu - dent engagement research can be categorised . As previously noted , multiple dimensions including behavioural , cognitive , affective ( or psychological ) and socio - cultural engagement are commonly ac - knowledged [ 23 ] . Reschly and Christenson provide a division of the first of these into academic ( such as time on task , assignments completed and credits earned ) and behavioural ( which includes at - tendance , class participation and extra - curricular participation ) [ 43 ] . Other authors provide further distinctions which may be viewed as discriminators of different aspects of behavioural engagement . For example , Krause and Coates [ 25 ] introduce seven separate ‘en - gagement scales’ which include peer engagement , staff - student engagement and beyond - class engagement . Aspects of all of these are reflected in instruments such as NSSE . The additional engage - ment scale of work - integrated learning was initially represented in the AUSSE instrument ( now known as the SES ) . 2 . 4 Investigating student engagement While ‘engagement’ is a term often seen in education research , a number of authors point to differences in the way it is tackled in the research literature . Trowler highlights the issue of focus in student engagement studies , ranging from the individual student , to specific student groups ( such as under - represented minorities ) and to the institutional level [ 50 ] . Studies targeted at the individual tend to be small scale investigations of specific teaching practices or educational tools set in a specific instructional context , while studies at the institutional level tend to focus on more substantial national or international benchmarks . Similarly , Ashwin and McVitty [ 1 ] define three levels of focus for engagement studies : Formation of Understanding ( about helping students to improve their learning outcomes ) , Formation of Curric - ula ( student involvement in shaping the courses they study ) and Formation of Communities ( students influencing institutions and societies they participate in ) . Also described by Trowler [ 50 ] and evident in the literature is the range of objectives in studies on stu - dent engagement including : improving student learning ; improving completion times and retention ; widening participation and equity ; improving curricular relevance ; institutional quality assurance , rep - utation and funding ; and return on educational investment from funding bodies and fee - paying students . As noted by many , the multi - dimensional nature of engagement indicates that it is insufficient to rely on the assessment of any sin - gle aspect as an evaluation of student engagement [ 5 ] . The resulting holistic view [ 23 ] characterises engagement as multi - faceted , fluid , and highly dependent on many other factors , in particular , how the student feels ( emotion ) [ 5 ] . However , as pointed out by Kahu , the difficulty with such an all - encompassing approach is that the con - cept of engagement lacks definition and becomes indistinguishably merged with other concepts ( such as motivation and expectation ) which , although undeniably linked are antecedents to engagement rather than engagement itself [ 23 ] . It also becomes more difficult to evaluate . However , given the multi - faceted conceptualisation of engagement , it is clear that any attempt to categorize research re - lated to student engagement should include behavioural , cognitive and affective dimensions . A further aspect of comparison of student engagement research , theevaluationmethod , isdiscussedbyFredricksandMcColskey [ 17 ] . In practice , the most commonly used method in Higher Education is the standardised self - report engagement survey instrument ( such as NSSE and SES ) . However , in addition to the wide variety of different survey instruments there are many other possible approaches for assessing and evaluating engagement . These include : experience sampling , instructor rating , interviews and observations [ 17 , 47 , 51 ] . Many of these have commonly been observed in school - level re - search so it might be expected that they would also be reported in literature on Higher Education and provide a further classification on which to assess the research identified . The importance and relevance of the methods is in particular related to the conceptual dimensions noted above , since different methods have been shown Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 4 to relate more closely to different aspects of engagement . For exam - ple , teacher reports agree more closely with students’ reports on behavioural engagement but less so for emotional engagement [ 47 ] . However , in practice , within the current study insufficient work was observed which employed any direct consideration or assess - ment of engagement with respect to such methods . Hence this was not used to categorise candidate papers in the present work . Given the variety of meanings , aims and methods associated with the term ‘student engagement’ it is interesting to speculate how this concept is viewed and used by computing education practition - ers and researchers . This is especially topical given the relatively poor performance of CS education in terms of international stu - dent engagement benchmarks . With respect to academics : are com - puting academics familiar with international student engagement benchmarks and the theory on which they are based ? Does estab - lished literature on student engagement shape their educational practice ? In terms of researchers : what research related to student engagement is carried out by computing education researchers ? Do computing education researchers articulate a coherent view of student engagement that is well - grounded in established research literature in the field ? When computing education research studies claim to observe increased student ‘engagement’ , do they in fact use established student engagement practices and metrics ? There are certainly existing studies which investigate engagement in different aspects of CS , such as Naps et al . ’s 2002 study of engagement with a focus on visualisation technology [ 34 ] . In the current research we explore the diversity of recent perspectives on engagement in CS and the theoretical stances that underpin them . 2 . 5 Engagement as it relates to this report It is important to note that it is not the intention of the authors to provide their definitive definition of the term engagement . The purpose of our research is to explore CS academic understandings of engagement and how this aligns with its conceptualisation within international benchmarks . The current discussion of engagement serves to provide context for the following analysis and reporting of the international benchmarks , research literature , and academic interviews . The authors acknowledge the diversity of views and understand - ings of engagement , and the presentation of research regarding student engagement serves to highlight the myriad definitions and inherent complexity that it brings . While there are well - understood formal definitions of engagement , such as those discussed in this section , its behavioural , cognitive , and affective dimensions mean that the work of academic researchers and teachers can meaning - fully relate to a subset of one dimension , or indeed all three . We note however that the current understanding within more general educa - tion and psychology research literature is that a holistic view across the various dimensions is needed to conceptualise engagement fully . Further , we note the need to maintain a distinction between engage - ment and related concepts and we follow Kahu’s framework [ 23 ] where appropriate in terms of considering influences , engagement and consequences . This paper analyses and presents engagement research and academic practice in a ‘non - judgemental’ manner in order to identify the alignment with the benchmark instruments and the opportunities for improving student engagement . 3 INTERNATIONAL BENCHMARKS OF STUDENT ENGAGEMENT This section introduces the specific international survey instru - ments and gives an overview of their use . Recent changes to the surveys will also be discussed . It should be noted that these sur - veys do not measure learning outcomes or quantify the educational resources available to students directly , rather they seek to give an insight into the extent to which students make use of these resources and the impact this has on their learning experience . National Survey of Student Engagement ( NSSE ) . The North Amer - ican National Survey of Student Engagement ( NSSE ) is the largest of its kind and also the longest running of the current crop of higher education student engagement instruments . In the most recent round ( 2016 ) , 322 , 582 students from across 560 colleges and universities were surveyed [ 36 ] . NSSE was conceived in 1998 , and first piloted in 1999 . It arose from a working group of higher education leaders within North America that sought to better understand “investments that institu - tions make to foster proven instructional practices and the kinds of activities , experiences , and outcomes that their students receive as a result” [ 36 ] . The most significant outcome was the intention to establish a set of benchmarks for student engagement that were not associated with pre - established measures of institutional reputation , resulting in the NSSE . First - year and senior bachelor degree students are surveyed . Results of the survey are made available to institutions , while ag - gregated data is published openly to the wider public in both report form and through a tool that allows users to access more detailed information relating to specific disciplines or engagement factors . Student Experience Survey ( SES ) . The Australian Student Experi - ence Survey ( SES ) has undergone a number of changes and renam - ing since being commissioned by the Australian Government in 2011 . It was originally intended that results would provide informa - tion for allocation of funding , but this idea was later dropped [ 42 ] . At its inception as AUSSE it was based predominately on the NSSE , however since then it has become less focused on specific engage - ment activities , becoming the University Experience Survey ( UES ) and then the SES in 2015 . In the 2016 offering , data was collected from over 178 , 000 students from all 40 Australian universities , and 55 non - university higher education institutions [ 8 ] . First and later year undergraduate students are surveyed . Results of the SES are made available to institutions . A report of results and a facility for interrogating aggregated data is also made available to the general public . Queries can select disciplines and / or institutions and view the results for specific engagement factors . 3 . 0 . 1 Other International Benchmarks . A number of other coun - tries undertake national benchmarking of student engagement , however none are as extensive as the NSSE or SES . One of the most notable is the UK Experience Survey ( UKES ) , which is optional rather than a compulsory national instrument . Administered by the Higher Education Academy , it was piloted in 2013 and 2014 , and is now a nationwide ( albeit voluntary ) instrument . In 2016 , 29 UK institutions took part , with 23 , 198 students responding [ 35 ] . Data obtained from the survey is used by institutions and is available Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 5 to the general public on request through a report and tabulated aggregated data that can be broken down by discipline . Other countries that undertake surveys of this nature include South Africa and China , although as yet they do not have the reach , history , or richness of data of NSSE or SES . Therefore data analysis presented in this report on student engagement will focus on the results of NSSE and SES , with some limited discussion of UKES . 3 . 1 International benchmark design and engagement measures Both the NSSE and SES address and explore student engagement , however , they differ in terms of survey structure , length , and ques - tions asked of students . At their heart , both rely predominantly on Likert scale questions , however the questions are grouped on different dimensions of engagement . The term ‘student experience’ ( as embodied in SES ) indicates a broader , more encompassing per - spective than engagement . Relating this to Kahu’s framework , the measures explored would investigate elements beyond the strict ‘engagement’ aspect . That is , some antecedent factors ( for example , psychosocial consideration of support ) and some consequent fac - tors ( such as the proximal consequence of satisfaction ) are explored . Although SES throws a wider conceptual net , the construction of its engagement aspects is still firmly based in the NSSE perspective . Thus the results from those parts are directly comparable to NSSE results , while the additional factors provide some broader insight into the student experience . 3 . 1 . 1 NSSE . The NSSE presents approximately 10 pages of ques - tions . Some questions capture basic descriptive and demographic information , but most focus on the surveys specific engagement measures . In the initial version of NSSE five specific benchmarks were used [ 27 ] : 1 . Level of Academic Challenge , 2 . Student - Faculty Interaction , 3 . Active and Collaborative Learning , 4 . Enriching Ed - ucational Experiences , and 5 . Supportive Campus Environment . These benchmark areas have subsequently been condensed into four main themes : 1 . Academic Challenge ( 17 questions ) , covering reflective and integrated learning , higher order learning , quantita - tive reasoning , and learning strategies ; 2 . Learning with Peers ( 8 questions ) , examines collaborative learning and discussions with diverse others ; 3 . Experiences with Faculty ( 9 questions ) , covers student - faculty interaction , and effective teaching practices ; and 4 . Campus Environment ( 13 questions ) , looking at quality of interac - tions , and supportive environment . Students are not made aware of which questions relate to which benchmark . Many questions work on a 4 - point Likert scale ( Very Often , Often , Sometimes , Never ) , while others ask students to rate interactions on a 7 point scale ( Poor to Excellent ) , or to approximate the number of hours spent on a range of different activities . No qual - itative questions were asked in the 2016 survey . The benchmarks are then assessed by the different question sets and a standardised measure ( out of 60 ) calculated for each benchmark indicator . A full copy of the survey can be found at the NSSE website [ 36 ] . 3 . 1 . 2 SES . While there have been significant ties to the NSSE survey in the past , the SES incorporates aspects of work experience into learning , such as industry placements , industry experience studios and capstone units . This has evolved from the earlier AUSSE survey which incorporated a further benchmark of work - integrated learning . Later versions of this survey also condensed the categories and the SES currently uses five different benchmarks for student engagement . A range of different questions evidence each bench - mark : 1 . Skills Development ( 8 questions ) looking at development of general skills such as critical thinking , ability to work with oth - ers , communication skills , and knowledge of the field ; 2 . Learner Engagement ( 7 questions ) , covering belonging to the university , participation , and interactions with other students ; 3 . Teaching Quality ( 11 questions ) , focusing on rating overall educational expe - rience quality , as well as expected aspects such as quality of in - class experiences and feedback ; 4 . Student Support ( 14 questions ) , relat - ing primarily to the university services provided ; and 5 . Learning Resources ( 7 questions ) , which rates a wide range of physical and virtual academic resources . Responses are on a five - point Likert scale and are used to calcu - late an overall figure for each benchmark . There is also a noticeable lack of qualitative questions . A full copy of the survey can be found in the SES national final report [ 8 ] . 3 . 1 . 3 UKES . The UKES is developed under licence from NSSE but has developed its own distinctive flavour during several years of trial and updates . In particular , it aims to provide students with an opportunity to reflect on their learning . It is offered as a survey suitable for students at all years of study , however the organisers recognise that fewer institutions will choose to administer it to final year students because the UK already has a compulsory Na - tional Student Satisfaction survey ( NSS ) which takes precedence and is used in league tables . UKES includes the following sections : 1 . Areas of Engagement ( 29 question ) , covering critical thinking , learning with others , interacting with staff , reflecting and con - necting , course challenge , independent learning , engagement with research and inquiry , and staff / student partnerships ; 2 . Skills De - velopment ( 12 questions ) , examining academic skills , career skills , active learner skills , civic skills ; 3 . Time Spent on Academic Work ( 2 questions ) ; and 4 . Extra - Curricular Activity ( 5 questions ) . Only the engagement items are regarded as ‘core’ with other aspects labelled ‘optional’ . Many questions have Likert scale responses ( ‘definitely agree’ , ‘mostly agree’ and ‘neither agree nor disagree’ ) . 3 . 2 Analysis of Computer Science performance It is clear that each of the instruments describes uses slightly differ - ent student engagement measures and question formats . Despite the variation , there is a remarkable consistency in the results of CS relative to other other disciplines . This section describes the performance of CS in 2016 against international student engage - ment benchmarks in general and also in comparison to other STEM disciples . Due to the fact that these benchmarks have changed over time it is difficult to track performance across years , but previous work [ 6 , 46 ] has reported on the poor CS performance in 2014 and 2015 . The NSSE results are reported first , followed by SES and then the UKES . For each instrument , the nature of the data available and the methods used to extract the data is discussed . We highlight a number of key indicators rather than attempting to report all the data available . Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 6 3 . 2 . 1 NSSE . NSSE provides a number of summary reports for each year of results , made available to the general public via their website . The reports are broken into results for First Year ( FY ) students and separately for Seniors ( SR ) . Engagement scores are calculated from question responses and given a score out of 60 . Reports are available that provide calculated scores for the general engagement indicators , brokeninto 10 ‘related - majors’ ( for example , CS is grouped with Science and Mathematics ) , along with reports that provide mean answers to specific questions for each subject grouping . While individual institutions are provided with more detailed results relating to their students , an online report builder tool [ 37 ] is also made available to obtain more finely grained comparison data . Using this tool , anyone is able to obtain results for specific teaching disciplines , such as CS . The data presented in Tables 1 to 3 are a combination of summary reports and the CS results obtained using the online tool . Tables 1 and 2 present FY and SR results ( respectively ) from the 2016 NSSE survey , showing the mean scores for all teaching disci - plines combined for each engagement indicator . This is followed by the maximum and minimum of any of the ten related majors . This allows an assessment of the spread and a judgement to be formed as to where CS sits within this spread . Then the mean for the Physical Sciences , Maths and CS related major is shown , followed by the specific CS result . Table 1 : 2016 NSSE Results - First Years ( Scores out of 60 ) NSSEMean Max Min Phys Sci , Math , CS CS - FY Higher Order Learning 38 . 4 39 . 8 37 . 8 38 . 4 37 . 9 Reflective & Integrative Learning 35 . 5 39 . 2 32 . 6 33 . 7 33 . 7 LearningStrategies 38 . 7 40 . 6 36 . 4 36 . 7 36 . 2 QuantitativeReasoning 27 . 7 31 . 2 22 . 6 30 28 . 9 CollaborativeLearning 32 . 3 36 . 1 29 . 2 31 . 9 31 . 8 Discussions with Diverse Others 39 . 9 41 . 7 39 . 1 39 . 7 39 . 8 Student - Faculty Interaction 20 . 3 22 . 1 18 . 6 18 . 7 17 . 9 Effective Teaching Practices 39 . 3 40 . 6 37 . 5 39 . 7 39 . 2 Quality of Interactions 42 . 1 43 . 1 41 . 8 42 . 4 42 . 5 SupportiveEnvironment 36 . 7 37 . 6 35 . 9 36 . 3 36 As can be seen in both Tables 1 and 2 , CS performs considerably below the average in many categories , most notably Higher Order Learning , Reflective & Integrative Learning , Learning Strategies , and Student - Faculty Interaction . Quantitative Reasoning and Ef - fective Teaching Practices are also significantly lower for Senior Table 2 : 2016 NSSE Results - Seniors ( Scores out of 60 ) NSSEMean Max Min Phys Sci , Math , CS CS - SR Higher Order Learning 40 . 7 42 . 7 38 . 4 38 . 4 36 . 5 Reflective & Integrative Learning 38 . 6 42 . 8 32 . 7 33 . 2 32 . 2 LearningStrategies 39 . 6 42 . 2 35 . 6 36 . 3 34 . 3 QuantitativeReasoning 30 . 2 36 . 2 21 . 2 34 . 1 28 . 9 CollaborativeLearning 32 . 2 39 . 8 28 . 2 33 . 9 32 . 4 Discussions with Diverse Others 40 . 9 42 . 6 39 . 9 39 . 9 38 . 5 Student - Faculty Interaction 23 . 5 26 . 7 20 . 9 23 . 8 19 . 8 Effective Teaching Practices 40 . 4 42 . 4 36 . 9 38 . 6 36 . 5 Quality of Interactions 42 . 9 44 41 . 4 42 42 SupportiveEnvironment 33 34 . 2 31 . 1 31 . 9 30 . 5 students relative to the wider NSSE results . The tables provide com - parative results for all students ( NSSE mean ) and for the STEM grouping in general ( including CS ) . For both first years and Senior students , CS is lower in the majority of categories compared to the overall average . The same holds for comparison of CS against the STEM group figures . It is also notable that while other STEM subjects generally demonstrate areas of strength , CS cannot point to any outstanding aspect of performance [ 46 ] . It is also notable that there is no evidence of improvement of the CS position from previous years . Exact comparison across years is hindered by changes in the survey instrument but , given that overall themes of comparison remain the same , the overwhelming message is that for both CS first years and Seniors , results in 2016 have remained broadly the same compared to 2013 results reported by Sinclair et al [ 46 ] . It seems that the poor performance cannot be blamed on a single unrepresentative year but demonstrates a fairly stable position representing genuine , persistent low attainment against NSSE measures . One other trend of interest is in the change in scores from First Year to Senior . This was the case in 2013 and is still the case now . Table 3 presents these changes for both CS and the NSSE means . While most engagement scores improve from First Year to Senior in the NSSE means , the opposite is true for CS . At the NSSE level , only one factor shifts downward significantly : Supportive Environment . This may be due to increased support often being provided to first year students to aid in transition and the tendency to reduce some layers of support as students move through their degree program . At the CS level , results in all but three of the factors fall ( and of the remaining three , Quantitative Reasoning remains the same ) . For example in Effective Teaching Practices , the overall NSSE change is a positive 1 . 1 while the CS change is negative 2 . 7 . While the absolute Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 7 Table 3 : 2016 NSSE Shifts in Scores from First Year to Senior CS - FY CS - SR CS Change NSSEChange Higher Order Learning 37 . 9 36 . 5 - 1 . 4 2 . 3 Reflective & Integrative Learning 33 . 7 32 . 2 - 1 . 5 3 . 1 LearningStrategies 36 . 2 34 . 3 - 1 . 9 0 . 9 QuantitativeReasoning 28 . 9 28 . 9 0 2 . 5 CollaborativeLearning 31 . 8 32 . 4 0 . 6 - 0 . 1 Discussions with Diverse Others 39 . 8 38 . 5 - 1 . 3 1 Student - Faculty Interaction 17 . 9 19 . 8 1 . 9 3 . 2 Effective Teaching Practices 39 . 2 36 . 5 - 2 . 7 1 . 1 Quality of Interactions 42 . 5 42 - 0 . 5 0 . 8 SupportiveEnvironment 36 30 . 5 - 5 . 5 - 3 . 7 value achieved by CS on these measures is of concern , of even greater concern is the drop in student perceptions of engagement from first to third year . 3 . 2 . 2 SES . The Australian SES provides the results in a detailed report format , with CS presented in the context of a broader report on the overall results . Within the report , sub - tables provide levels of detail , one of which gives the summarised engagement indicator results . Just like NSSE , the results from a number of questions are analysed to determine the final indicator score . Unlike NSSE , results are not broken down into First Year and Senior , rather a combined result is reported . The SES reports results broken into 21 ‘study areas’ of which CS is labelled ‘Computing and Information Systems’ . Regional termi - nology would equate this ( mostly ) with the term CS as it is under - stood in North America . Table 4 presents the results of the SES in a similar format to the NSSE to facilitate discussion . Overall means of all teaching disciplines combined are provided . Maximums and minimums refer to the highest and lowest scores recorded across any of the 21 study areas . For comparison , the study area of ‘Science and Mathematics’ is also provided , but this group does not include CS as in NSSE . Finally , CS specific results are also provided . Results are adapted from the SES report [ 8 ] . Like the NSSE , CS performs poorly in many key areas . Of note , CS has the lowest score in Skills Development , second lowest in Teaching Quality , and sadly lowest in the rating of Entire Educa - tional Experience . The authors of the SES report even provide a dedicated graphic showing the highest and lowest score in this indicator ( Figure 2 ) , placing CS’s poor performance squarely in the consciousness of the reader [ 8 ] . These results are consistent with those collected and published in 2015 . Table 4 : 2016 SES Results ( Scores out of 100 ) Mean Max Min Sci , Math CS Skills Development 81 90 75 81 75 Learner Engagement 62 75 54 65 60 Teaching Quality 81 89 75 84 76 Student Support 72 76 66 73 69 Learning Resources 85 89 76 89 85 Entire Experience 80 87 74 82 74 Figure 2 : SES Graphic Highlighting CS Poor Performance As with NSSE , these results also highlight the poor relative per - formance of CS when compared to other STEM disciplines . In all categories CS performance is worse , sometimes by a significant amount , for example for Teaching Quality 84 versus 76 . Clearly other disciplines with significant technical content are outperform - ing CS in terms of student engagement . Results in some areas are particularly concerning since CS might be expected to be scoring highly in certain aspects . Indeed , CS is often criticised for being too ‘skills focused’ , yet CS rates the lowest of all study areas . The measures used in the surveys are clearly not ones which students recognise spending time on . This raises obvious questions . Are CS students genuinely less engaged ( and our teaching less engaging ) ? Or might there be issues concerning the measures used ? For ex - ample , could the ‘skills focus’ referred to in CS literature point to skills which might equally be used as ‘high impact’ activities likely to be associated with learning gain ? 3 . 2 . 3 UKES . As discussed , the UKES is voluntary , with institu - tions opting in . As such , the results do not provide a complete view of the performance of CS in the UK . In 2016 results were obtained from 20 institutions , capturing 862 CS students ( 4 % of the UKES participants ) and 7061 STEM students providing sufficient results for comparison with the other instruments . Detailed results are Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 8 not made available to the public , however they can be requested from the instrument website and provided after approval . Table 5 presents results in the same format as the previous discussion , using data provided by the managing organisation . There are 19 subject areas for which minimum and maximums are shown . There are 4 clusters , such as STEM which includes CS . Table 5 : 2016 UK ES Results ( out of 100 ) Mean Max Min STEM CS Critical thinking 78 . 1 85 . 7 69 . 9 75 . 1 69 . 9 Learning with others 55 . 7 66 . 0 35 . 6 56 . 7 59 . 5 Interacting with staff 33 . 4 44 . 1 17 . 3 28 . 6 30 . 7 Reflecting and connecting 65 . 7 73 . 8 47 . 3 59 . 3 57 . 4 Course challenge 91 . 0 95 . 5 87 . 0 90 . 0 87 . 0 Research and inquiry 62 . 5 68 . 2 41 . 9 59 . 6 57 . 0 Staff - student partnerships 39 . 5 45 . 8 31 . 5 37 . 6 36 . 7 Sadly the results suggest issues similar to the other instruments . Of the engagement indicators provided , CS performs notably poorly in the areas of Critical Thinking and Course Challenge , ranked last in both categories . All other indicators also score below average , with the only exception being Learning With Others . The position of CS in Critical Thinking is made painfully apparent by the UKES report which singles out the category for illustration by means of a figure with CS at the bottom [ 35 ] . Of note is that the low score for Course Challenge appears directly at odds with both anecdotal and published evidence . The difficulty of learning to program is often cited as a factor leading to high fail rates and issues of retention in CS , however this does not seem to be the view of the students . 3 . 2 . 4 Summary . The over - arching message from all three sur - veys discussed is that CS is not scoring well . We may immediately seek to blame the surveys . However , even when we compare against groups which might be supposed to be similar ( as with the NSSE STEM comparison ) CS scores are low . Indeed , the STEM results are pulled down by CS . Other worrying aspects ( such as decline in results from first year to final year ) seem to be problematic for CS . So , while we may want to raise questions about issues within the survey instruments the results clearly indicate the need for a more in - depth understanding of why these results are being observed for CS , how students respond to surveys , how engagement is un - derstood and incorporated in CS teaching and how engagement is viewed in CS education research . 4 APPROACHES TO CATEGORISING COMPUTING EDUCATION RESEARCH The working group aims to examine the computing education litera - ture to determine the extent of alignment with international bench - marks on student engagement . It is therefore useful to examine how computing education research literature has been categorised by other researchers and , in particular , to determine whether a signifi - cant focus on student engagement has been identified previously . There have been a number of studies examining the nature of research in computing education , for example [ 7 , 15 , 22 , 24 , 30 , 31 , 40 , 44 , 45 ] . These classification papers range in objectives from categorising the nature of the topics researched to a more detailed analysis of the research methods used . They also range in terms of scope from the more general Education and Computing terms to an analysis of the papers from a single conference , such as ICER . A study published by Joy et al . [ 22 ] in 2009 examined the types of papers published in 42 computer science education publication outlets over a 12 month period , resulting in categorisation of 3 , 500 papers . After the process of evaluating the papers Joy et al . devel - oped a final categorisation scheme of eleven categories , the closest to the topic of ‘student engagement’ being Theoretical pedagogy and Social factors . A general finding of this study was that con - ferences in the field of computing education tended to focus on describing Systems and Technologies , while journals tended to fo - cus more heavily on Theoretical pedagogy , Practical pedagogy and Curriculum . Simon et al . [ 45 ] classified the content from three years of ICER conference proceedings ( from 2004 to 2007 ) totalling 43 papers . The classification system used in this case consisted of four dimensions , 1 . Content , 2 . Theme , 3 . Scope , and 4 . Nature . Results reported in this study related to the dimension of Context and Theme are most relevant to examine research studies related to student engagement . The authors found that 74 % of all papers were set in the program - ming context . Ten different research themes were identified , with the top five being : Teaching and learning theories and models ( 26 % ) , Ability and aptitude ( 26 % ) , Teaching and learning techniques ( 23 % ) , Teaching and learning tools ( 7 % ) , and Recruitment , progression , and pathways ( 7 % ) [ 45 ] . The dimension of Recruitment , progression , and pathways was perhaps the most relevant to a discussion of student engagement , however the theme of Ability and aptitude may also be relevant . Amoredetailedanalysisofcomputingeducationresearchsources related specifically to teaching of programming was carried out by Sheard et al . [ 44 ] who focused on the 2005 to 2008 period but included papers from the ICER , SIGCSE , ITiCSE , ACE , Koli Call - ing and NACCQ conferences . The total number of research pa - pers was 979 full research papers however only 164 ( 17 % ) were related to the specific programming context . Their analysis used four criteria : 1 . Type of research conducted , 2 . Data gathering tech - niques , 3 . Analysis techniques , and 4 . Aims and outcomes of the research . Eleven themes were identified from these papers , with the main examples being related to : Ability / aptitude / understanding ( 40 % ) , Teaching / learning / assessment techniques / ( 35 % ) , and Teach - ing / learning / assessmenttools ( 9 % ) [ 44 ] . ThethemeAbility / aptitude / understanding could include a number of papers related to student engagement but it is unclear if these are strongly supported by well established student engagement literature or if they are based on other considerations . The theme of Recruitment / progression / pathways could also contain papers related to student engagement but was represented by only two papers ( 1 % ) . In a 2010 paper Malmi et . al . [ 31 ] focused their analysis on 72 ICER papers from 2005 to 2009 . Their classification scheme ex - amined seven dimensions : Theory / Model / Framework / Instrument , Technology / Tool , Reference Discipline , Research Purpose , Research Framework , Data Source and Analysis Method . Their aim was to ex - amine the research processes documented in computing education research papers . Of particular interest to a consideration of student engagement is the Theory / Model / Framework / Instrument ( TMFI ) Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 9 dimension . This was used to represent the theoretical underpin - ning discussed in the research paper . Several well know TMFI were identified : Bloom’s Taxonomy , Cognitive apprenticeship , Cognitive load theory , General systems theory , Schema theory , Self - efficacy theory , Situated learning , SOLO taxonomy , and Threshold con - cepts [ 31 ] . While student engagement is not specifically referenced , Self - efficacy theory is perhaps most strongly related . It is clear from this analysis that for this particular conference venue a range of educational theories were cited but that student engagement theory was not prominent . In 2010 Kinnunen et al . [ 24 ] published a paper entitled ‘Have We Missed Something ? Identifying Missing Types of Research in Computing Education’ . The aim of this research was to discuss the goals and the present state of computing education research , with an emphasis on revealing novel areas of research that had received little attention . In a categorisation of ICER papers from 2005 to 2009 involving 62 papers , the authors found that most studies focused on course level issues with relatively few addressing the organisational / curriculum or society level of investigation . They also conclude that issues related to content and goals , and teacher attitudes and skills receive relatively little attention . Again , while Student actions , the results of students’ action and Self - efficacy in regard to CS1 were common themes , no categories identified specifically focused on student engagement . In summary , there are a number of research studies that seek to categorise and describe the nature of research in computing education . Most focus on a combination of the topics researched , the theoretical approach and the methodologies used . Several analyses reported that the majority of computing education research focuses on course - specific issues and the programming context , and that studies at organisational and wider society levels are relatively rare . None of the categorisation schemes examined addressed the issue of student engagement as a dedicated theme although some topic areas involved student centred concepts , such as Self - efficacy , aptitudeand progression . Amajor aimofthis studyisto examinethe prevalence and nature of research concerning student engagement in the computing education research literature . It is not possible from past studies of the nature of computing education research to determine the extent to which student engagement is addressed or if research in this area is well supported by the relevant engagement literature . 5 METHODOLOGY In order to examine the issue of the performance of Computer Science in international benchmarks on student engagement the methodology is divided into two main sections : the categorisation of CS research literature and academic interview protocols . The research questions developed by the working group are listed below and aim to explore student engagement from a CS perspective . In particular the issues of alignment between international bench - mark instruments and the educational research focus and teaching practice of the CS academic community are examined . Research Aim To investigate the conceptual alignment between international benchmarks on student engagement and the CS academic commu - nity’s educational research and practice . Research Questions : RQ 1 . How does current research in CS education align either to international engagement benchmark instruments or to broader conceptions of student engagement research ? RQ 2 . How do CS academics understand and foster student en - gagement ? RQ 3 . How aware are CS academics of international benchmark - ing instruments and how do they interpret specific questions from the surveys ? RQ 4 . What CS education research can be identified as exem - plifying ‘best practice’ relating either to benchmark engagement measures or to a broader conceptual understanding of engagement ? In examining the alignment between international benchmarks on student engagement and the CS academic community’s educa - tional research and practice , we were open to a range of possible explanations for the consistently poor performance of CS on a range of measures . These ranged from issues to do with the design of the benchmark instruments themselves to a focus on the practice of the CS community , as shown in Figure 3 . Figure 3 : A Range of Possible Factors 5 . 1 Categorising CS education literature Using the categorisation scheme described in this section , we have analysed computing education literature from the past five years . We aim to determine to what extent Computer Science Edu - cation ( CSE ) research literature focuses on research methods and teaching practices aligned with international benchmarks . Further , we aim to note engagement - focused activity based on a broader conceptualisation than that embodied by the surveys . A lack of alignment with the surveys may indicate some difference between the way the CSE community and other disciplines view and explore student engagement . For example , other disciplines might have a macro view of students engagement ( as demonstrated by large scale course level studies ) , while CSE research may be focused on a micro level , ( that is , usually focused on specific classroom practice or teaching specific computing techniques ) . Equally , it is possi - ble that CSE research takes a broader perspective , investigating ideas and initiatives which , while representing valuable engage - ment work , would not lead to high scores within the narrow focus of engagement surveys . 5 . 1 . 1 Selection of publication outlets . Based on past research in CSE paper categorisation the researchers compiled a list of journals and conferences related specifically to computing education . Well known conferences - ACE , ICER , ITiCSE , Koli Calling and SIGCSE , Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 10 and journals - Computer Science Education , ACM Transactions on Computing Education and IEEE Transactions on Education , were included . In order to include only high quality research in the anal - ysis , only journals and conferences well ranked in the ERA [ 13 ] Australian government research quality system and / or CORE [ 11 ] were selected . All papers in these publications were examined for a five year period ( generally years 2012 to 2017 depending on the pub - lication schedule at the time of the 2017 ITiCSE conference ) . In total , 1401 papers were considered . A full list of included conferences and journals is given in Appendix A . 1 . 5 . 1 . 2 Exclusion factors . Papers not related to higher education were excluded from the analysis process as we were focused on re - search related to the university sector . Papers outside the reference frame of 2012 to 2017 were also excluded to limit the scope of the analysis to recent research activity . This time period still resulted in a high number of relevant full research papers and therefore provided a good overview of research in this area . Only papers classified as full research papers were included , which eliminated those papers less likely to be strongly supported theoretically . Full research papers were also more likely to be based on rigorous methodologies and include more extensive evidence to support their findings . This excluded less well developed research papers , such as posters , works - in - progress , editorials and short papers . 5 . 1 . 3 Selection Methods . For the selected publications , the titles of papers , keyword and abstracts were examined to determine if the paper was related to student engagement . The process consisted of pairs of evaluators working independently to classify papers as related to student engagement or not , and then comparing results in order to reach a consensus . The three key criteria were used : 1 ) a focus on higher education ; 2 ) involving the computing education discipline ; and 3 ) with a significant focus on student engagement . The third criteria was the key factor and could include any pa - pers with a focus on the behavioural , cognitive or affective aspects of student engagement . In addition studies were included if they related to any of the key engagement scales included in interna - tional benchmarks on student engagement . These scales include : NSSE specific measures such as Academic Challenge , Learning with Peers , Experiences with Faculty , and Campus Environment ; and SES specific measures of Skills Development , Learner Engage - ment , Teaching Quality , Student Support and Learning Resources . Specifically , we also include papers with a stated aim to improve student engagement , or closely related terms such as student moti - vation . We aimed to assess whether these papers were in fact well supported by appropriate theory and if methodologies related to student engagement were employed . This resulted in an inclusive initial selection of 335 computing education research papers related to student engagement , from a total of 1401 papers , which could then be analysed further . A summary of the publication outlets and the numbers of papers found is shown in Table 6 . Overall , 22 . 8 % of CSE papers were cate - gorised as relating to student engagement in higher education in some form , despite no previous categorisations of computing edu - cation research highlighting this area . Computer Science Education ( 34 . 7 % ) , Koli Calling and SIGCSE published the greatest proportion of papers initially assessed as being related . Table 6 : Paper Selection Outcomes ( * Journal , + Conference ) Outlet TotalPapers InitialSelec - tion Removed Analyse % CSEd * 75 27 - 1 26 34 . 7 % KOLI + 79 22 22 27 . 9 % SIGCSE + 531 136 - 5 131 24 . 7 % ACE + 86 22 - 2 20 23 . 3 % TOCE * 83 21 - 2 19 22 . 9 % ITiCSE + 256 61 - 5 56 21 . 9 % ICER + 110 22 - 1 21 19 . 1 % IEEETE * 181 24 24 13 . 3 % Totals 1401 335 - 16 319 22 . 8 % The resulting papers were then collated for further analysis as to the nature of the content , theoretical underpinnings and methodol - ogy . The papers were examined in detail to assess if they contained a significant focus on the issue of student engagement , for exam - ple reference to research literature and theory relevant to student engagement , the introduction of teaching practices and pedagogy related to engagement , use of established student engagement mea - sures and benchmarks , and / or collection and analysis of data related to student engagement . A form was created listing the items on which each paper was assessed . The initial assessment looked at the motivation or aim of the research : Did the paper aim to improve or investigate en - gagement in some way ? This enabled us to examine the purpose of the research . We then examined if a formal definition of stu - dent engagement was given and what terminology was used to describe engagement . This factor related to the alignment of the research with existing literature in the field of student engagement . The methodology and rigour of the research process used were ex - amined , including whether formal research questions were stated and the sources of data . The dimension ( s ) of engagement covered ( behavioural , cognitive , emotional ) were also noted . The areas of focus of each paper were mapped against engagement measures from the survey instruments . Consideration of the exact questions evidencing survey themes allowed us to determine whether a paper addressing a theme aligned to the survey conception or whether the theme was addressed in a different way . For example , a paper might examine a teaching initiative that clearly developed students’ higher order learning skills , but yet not be covered by the survey questions evidencing that theme . The final aspect of the assessment related to identifying examples of good practice in CSE research on student engagement . Here we aimed to briefly describe examples of best practice in engagement , either aligning to international bench - mark instruments or going beyond what these surveys measure . As a group we tested and refined the evaluation form to ensure it captured the required level of detail to thoroughly analyse the research papers selected . Definitions for each of the items and measures were created ( see Appendix A . 2 ) . The group went through a joint process of analysing and discussing a number of papers until a consistent analysis of papers was achieved . From this point , papers were analysed individually , however if a coder was uncertain Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 11 one member of the team was assigned as the final arbiter . After analysing 335 papers an additional 16 papers were removed from the analysis resulting in 319 papers ( corresponding to the ‘Removed’ column in Table 6 ) . 5 . 2 Academic Interview Protocol In order to elicit the views of academics regarding their under - standing of the term student engagement , what teaching practices are related to student engagement and their response to selected questions extracted from the survey instruments , a semi - structured interview protocol was developed ( see Appendix A . 3 ) . The inter - view was divided into two sections : the first contained nine general questions related to the academics experiences and understanding of student engagement ; the second part presented nine questions ex - tracted from the international benchmark instruments for comment and discussion . Seven general themes were explored in relation to the questions extracted from the surveys . Further , if academics raised issues of interest relevant to student engagement the inter - viewer had the flexibility to probe further to clarify the issue . All interviews were recorded and made available within the working group . Each interview was listened to in full by at least two members of the group . In addition , an audio compilation was made of the section concerning understanding of engagement from all interviews and this was studied by all members . The methodology followed was for each listener to make notes of themes of interest emerging in the interviews . These could be either inductive ( relating to the areas of questioning already noted ) or deductive ( emergent from the interview ) . Themes were then drawn together across all interviews allowing aspects of comparison and contrast to emerge . Results are reported by theme below . Interviews of 17 academics from a range of countries were obtained and analysed , providing a broad range of responses from international academics in the field . 6 ANALYSIS AND RESULTS In the following sections we first report the results of the analysis of CSE literature , followed by the results from the academic interview data and finally we report several examples of ‘best practice’ in computing education research . 6 . 1 Alignment of CSE literature Below is a general overview of the results obtained from the anal - ysis of engagement related literature ( 319 papers ) that seeks to address the research question : RQ 1 . How does current research in CS education align either to international engagement benchmark instruments or to broader conceptions of student engagement re - search ? Figures 4 to 7 show the overall position with respect to the nature of the 319 papers , the methods used and the way student engagement is approached . For each paper the stated aim of the research was assessed . As shown in Figure 4 a high proportion of the papers ( 43 . 8 % ) clearly stated an aim to improve student engagement . An additional 32 . 5 % used a proxy term for the main aim of the paper , such as to improve motivation . Only a small number of studies 7 . 2 % focused on learning outcomes or content . The other category 16 . 6 % related to items such as improving industry links , gender and academic perspectives . On face value a high proportion of papers aimed to improve student engagement either directly or indirectly . Figure 4 : Motivation for the Research There is widespread use of the term ‘engagement’ throughout the CS education literature . There is also extensive use of words that are strongly related ( but not equivalent ) to ‘engagement’ ( for ex - ample , terms such as ‘motivation’ , proximally positioned in Kahu’s framework presented in Figure 1 ) . Although the papers discuss en - gagement , very few ( 8 . 2 % , Figure 5 ) give a formal definition of the concept or provide evidence of a clear understanding of the theories and factors surrounding it . The way in which related terms such as ‘motivation’ often appear to be used as proxy words for engagement indicates a lack of understanding of the distinction between these terms and hence a lack of a clear definition of student engagement . In general , there appears to be a shallow understanding of formal terms within engagement and a conflation of terminology within the broader conceptual framework . Figure 5 : Assessing the Theory Underpinning the Research It is possible that authors assumed a shared understanding within the CSE community of what ‘engagement’ encompasses and did not feel the need to define it or to highlight their conceptual viewpoint or the dimensions of engagement addressed . However , considering how the word is used within the literature surveyed , it is apparent that there are many different implicit interpretations including : stu - dent satisfaction , introducing novel content , attendance , involving Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 12 students in the process of governance and making links to industry . These all bear some relationship to engagement but are not the same . In fact , as shown in in Figure 5 formal ‘student engagement’ terminology is only used in 35 . 4 % of the papers assessed . It is perhaps not surprising that ‘student engagement’ is most often used in the very general sense of improved interest in a topic area or activity , or of increased attendance . This is observed in many papers reporting cases where the development of a new teaching technique , tool or area of novel content and reporting surface level outcomes are seen as representing evidence of improved engage - ment . Of course it is to be expected that on some occasions the word ‘engagement’ may be used in a more informal way . However , the dearth of papers which do include a more formally conceptu - alised treatment of the issue is concerning . One reason is that by taking an a - conceptual approach , ignoring current understandings and general educational research on student engagement , the CS community is failing to benefit from and build on advances which may be useful within our subject area . As shown in Figure 5 only 58 . 7 % of the papers made reference to a formal theory or aligned to formal concepts of educational research in any way . It is of concern that 41 . 3 % of these papers are not well grounded in educational theory , particularly given the fact that the journals and conferences reviewed were the most highly - rated in the field . The papers generally reported initiatives involving individual classes , for example introducing a novel cur - riculum area or a tool for automated assessment , without clear frameworks for understanding and assessing the impact of these changes . It was evident that many papers claiming to be focused on ‘student engagement’ were in fact addressing related concepts such as motivation , self - efficacy and learning concept inventories . A lack of definition of and distinction between these concepts was also often associated with a failure to state a clear research aim or provide focussed and convincing ( or indeed any ) evaluation . These aspects are discussed further below . Part of the analysis examined the issue of rigour in the research process of papers related to student engagement . As can be seen in Figure 6 formal research questions ( or hypotheses ) were used in only 36 . 4 % of the papers . This may indicate some lack of focus on formally evaluating the results of changes to practice intended to improve ‘student engagement’ . More disturbing was the finding that in 37 . 9 % of papers no formal research method was stated or followed . These papers tended to describe changes to curriculum , educational tools or teaching practices in great detail but offered little evidence of the efficacy of their approach . Although the papers claimed to address engagement or stated that their results indicated improved engagement , in many cases the evidence related to general student satisfaction with the class ( for example , students saying they found a particular teaching method useful or data showing that students had used a new tool ) . In some cases , no evidence at all was provided . Hence the claims were mostly not well supported with data that actually measured engagement . Hence , while many of the papers reported work and initiatives that may lead to improved student engagement , the fact was that it was not clearly evidenced . Another aspect of research rigour related to the number and nature of the participants involved in the study . As can be seen from Figure 6 , high level or large scale studies focused at a degree level in CS educational research were rare ( 5 papers constituting 1 . 6 % ) . In 40 . 7 % Figure 6 : Assessing the Rigour of the Research ( 129 ) of papers multiple classes were involved in the study but this figure included studies using multiple versions of the same class over several instances of the course . The majority of the research was based on data from one class 39 . 1 % ( 124 ) or a class subset 11 % ( 35 ) indicating that small scale or one off research efforts are the norm . These small scale studies tend to result in limited impact to the CS discipline overall and change in practice emerging from them is hard to sustain beyond the staff involved . This analysis indicates a general lack of large scale systematic studies on student engagement in CS . Of concern is the fact that 7 . 6 % ( 24 ) papers made claims about student engagement without providing any data from research participants . We also looked at how the research outcomes were reported and the results are shown in Figure 7 . Rigorous results directly related to student engagement were reported in 24 . 8 % ( 79 ) of pa - pers . This indicates the appropriate application of methodology and measures directly related to student engagement . The majority of papers 49 . 8 % ( 159 ) reported results in a rigorous manner but these results were indirectly related to student engagement . This indicates some misalignment between the stated purpose of the research and the results reported . Of concern is the fact that some papers only reported anecdotal evidence 9 . 7 % ( 31 ) to support their results and some papers 14 . 7 % ( 47 ) reported no evidence to support their results . Of the papers that addressed a single dimension of student engagement the highest number did so from a behavioural perspective 22 . 6 % ( 72 ) , followed by emotional / affective 16 % ( 51 ) and cognitive 14 . 1 % ( 45 ) ( see Figure 8 ) . However , many papers addressed multiple dimensions of student engagement 37 % ( 128 ) , for example behaviourally - focused papers that also reported cognitive benefits . Unfortunately for some papers 10 . 3 % ( 33 ) we could not find any reference to a specific dimension of student engagement , and in fact most of these papers related to general enthusiasm or satisfac - tion in a CS topic or tool use . Given the behaviour - focused nature of the international benchmark surveys on student engagement , the behaviourally - focused research might have a better chance of leading to initiatives which improve survey scores , however from Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 13 Figure 7 : How Results Were Reported the perspective of genuine , multi - dimensional engagement it is encouraging to see evidence of work relating to other dimensions . Figure 8 : Dimension of Engagement Addressed Figure 9 : Measures of Engagement Addressed Figure 9 shows the results concerning which specific engagement measures used in the surveys were tackled by the papers reviewed . Each bar shows the number of papers that ‘aligned’ with the mea - sure ( shown in blue ) and the number of papers that related to the measure but did not directly align to the questions the surveys use ( shown in red as a stacked bar ) . The results clearly indicate that the four most frequently referenced engagement measures are Active learning , Collaborative learning , Skills development and Effective teaching practices . Under - represented themes were Quantitative reasoning , Quality of Interactions , Student - Faculty Interaction and Discussion with Diverse Others . The common theme with these under represented measures of student engagement in the research is how our students form relationships with others , from their peers to academics . Given the poor rating of CS in these four areas it is interesting to note that there has been virtually no consideration in the CS education research community of these issues over the past five years . Perhaps this goes some way to explaining the perception of CS students and CS in general as being socially isolating . These factors have significant implications for broadening participation and retention . In two areas ( Effective teaching practices and Higher order learning ) a significant proportion of the research targets these themes but work is not well aligned with the actual benchmark measures . This suggests that those measures are being considered by CS researchers but in ways which are possibly subject - specific and which would not translate to improved outcomes in the inter - national survey instruments . As part of the analysis , reviewers were asked to nominate papers as being suitable for case studies of ‘best practice’ in student engage - ment studies in computing education . Generally , those studies that showed a clear focus on student engagement , were well supported theoretically and adopted a rigorous research methodology , were considered . Approximately , 16 % of papers ( 51 ) were identified as providing significant insights and perspectives on student engage - ment in the CS context . Some of these papers validated the efficacy in the CS context of student engagement approaches that were de - veloped in other disciplines . Others highlighted novel approaches to student engagement specific to the subject area . These papers demonstrate a variety of research methods applied in a CS context that are appropriate to rigorously evaluate the efficacy of interven - tions from a student engagement perspective . Specific examples of ‘best practice’ are highlighted in Section 6 . 3 . Overall in our analysis of computing education research litera - ture , although there was some evidence of papers presenting ex - cellent research and interesting case studies relating to student engagement , the focus and rigour of the papers varied widely . As noted in the Methodology section , we selected only the top - ranked journals and conferences in CS education with the intention of cap - turing the most rigorous work rather than early stage or practice reports . Issues regarding rigour include many papers not posing for - mal research questions , some having no formal methodology , and in many cases obtaining poor evidence to support their outcomes . Where papers reported collecting data , this was often in the form of a student satisfaction survey soliciting their impressions of a teaching initiative or how useful a tool was . In many cases surveys were not based on validated engagement instruments ( either the international instruments or more widely from the education liter - ature ) and in some cases they were clearly measuring something other than engagement . Interviews were sometimes used to obtain supporting evidence , but in nearly all cases this was presented as a limited number of selected comments rather than as the results of any clear , systematic analysis of interview data . While conducting the literature review analysis of 319 papers we were interested in the alignment of computing education research Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 14 with the current international student engagement surveys . How - ever , we are well aware that these international instruments are limited by virtue of being primarily based on behavioural perspec - tives and contain few qualitative type questions . We were careful also to note and map computing education research on student engagement which goes beyond the current instruments , either by its nature as relating to CS - specific practices or indeed because it goes beyond the narrow behavioural confines of the surveys . Over - all , despite widespread use of the word ‘engagement’ and research aims supposedly related to improving engagement , there was a general lack of evidence of an appreciation of engagement from the computing education research community . Indeed , the widespread conflation of engagement with other terms and the tacit assump - tion that evidence of something related is evidence of engagement belies an underlying confusion that is unhelpful to understanding and address student engagement issues within CS education . 6 . 2 Academic perspectives on engagement Interview data was collected from 17 CS academics , with interviews averaging approximately 44 minutes . Detailed notes were taken for each interview . Note that for interview 17 audio recording was not available due to technical issues but detailed notes were taken . Academics were recruited from the IT Faculty of Monash Univer - sity , the Department of Computer Science at Warwick University and from ITiCSE participants . Of the 17 participants , 10 were in Australia , 3 in North America and 4 from Europe ( including 3 from the UK ) . Participants had an average of 21 . 5 years experience of teaching CS at tertiary level , ranging from 4 to 43 years . 6 . 2 . 1 CS academics’ understanding of student engagement and relevant practice . The research question addressed is : RQ 2 . How do CS academics understand and foster student engagement ? Indi - vidual interviewees are referred to below as I01 - I17 . After discussing the context of the study and their past experi - ence of CS teaching , academics were asked what the term ‘student engagement’ meant to them . Most struggled to define the term , with significant pauses in many cases and obvious difficulty in avoiding simply repeating the word ‘engaged’ . There was a notable diversity of views on the meaning of student engagement . Several partici - pants felt student engagement related mainly to student motivation or enthusiasm . Words used by a number of interviews include “en - thusiastic , eager , proactive , interested , excited” . For example , I02 is typical of this group in characterising engaged students as being interested , self - motivated learners who do not need pushing . These qualities relate to student attitude and feeling , and may be seen as aligning to the affective dimension of engagement . For many interviewees the important features of engagement were linked to what students do , clearly indicating a more be - havioural view . For example , I03 points to expected activities typi - cally including attendance , involvement in forums and discussions , communication and submission of work . Even where an intervie - wee referred initially to enthusiasm and interest , most went on to note more behavioural examples as indications of engagement . Characteristics linking to a cognitive perspective were noticeably less frequently mentioned in the interviews . Cognitive engagement includes self - regulation in learning and the employment of deep learning strategies . However , while most interviewees referred to the need to develop students’ abilities in subject - specific topics and also in more skills - based areas , few talked about enhancing students cognitive skills such as aspects of self - regulated learning or more generally ‘learning to learn’ . Only one interviewee ( I02 ) talked about the importance of learners’ maturity and its impact on their ability to engage . It may be that others might see such things as important but not connected to engagement and so did not raise them in the discussion . However , at least one interview subject was firmly opposed to anything that would take away valuable curriculum time from CS subject content . Exemplifying this view , I10 stated : “If you can’t master CS competencies you’re out , no matter how critical or reflective you are” . The content area taught and student level had some impact on the academics’ view of engagement . Those academics focused in techni - cal domains and to some extent undergraduate introductory units , such as programming , database and networking , tended to express a more behavioural view of engagement ( I01 , I03 , I04 , I10 ) . Activi - ties associated with engagement related to completion of tutorial exercises and in - class quizzes , a skills - based focus on the content , and building incremental technical expertise . While those in less technical area and / or post - graduate units , such as information man - agement , project management and Industry Experience / Capstone ( I05 , I07 , I08 , I17 ) , tended to include more of the cognitive and / or affective / emotional aspects of engagement in the discussion . Ac - tivities associated with engagement included a deeper conceptual engagement with the content , in class discussions , interacting with peers and clients , links to industry , self - direction of learning , and soft skills . A more instrumental view of engagement was held by some interviewees who saw the focus of CS as relating to the efficient acquisition of high level technical skills in programming . In this view , reflective practices , a focus on transferable skills , active learn - ing strategies and enrichment activities took away time from more relevant technical content and in fact detracted from learning by distracting students . I10 expressed the opinion that CS students do not need to think analytically : they “don’t need to compare theories and opinions” . This interviewee went on to point out the high demand for graduates with technical IT skills and indicated that students who wanted to gain other skills , such as soft skills , could gain these in other faculties . This opinion is somewhat at odds with the assumption that engagement and its measures are universally viewed as being a ‘good thing’ ( even if hard to pin down ) , for example , the claim by Trowler and Trowler that “the value of engagement is no longer questioned” [ 49 , p . 9 ] . The dep - recation by CS staff of aspects viewed in the literature as central engagement indicators demonstrates that the value of engagement ( as most widely conceptualised ) is indeed questioned by some CS academic staff . At this point , the limited number of interviews con - ducted allows us only to raise possibilities , but it is interesting to speculate on how widely a similarly sceptical perspective might be encountered amongst CS academics generally and whether this might be different in other disciplines . Engagement was seen by some academics as being intrinsic to the student and therefore something that teaching initiatives had little power to influence ( I01 , I03 ) . However , the most prevalent view was that there was some degree of joint responsibility between teacher and student : “We have to create activities in their learning Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 15 environment so that the student can benefit maximum” ( I04 ) . It was seen as a two - way street in which the teacher provides resources and guidance , and the student contributes effort to learn ( I06 , I08 ) . I12 felt that the instructor initiates that engagement , suggesting that it “starts in the classroom : I am the facilitator” . Similarly , I10 referred to an implicit “social contract” between teacher and student which had to be tacitly agreed for successful learning . However , it was clear that views on the nature of both sides of the bargain were very different amongst the academics interviewed . A similar difference was observed in staff attitudes on what would engage students in the affective dimension , although en - richment activities such as hack - a - thons and initiatives relating engagement to industry practice were often cited as approaches that could stimulate interest and enjoyment ( I02 , I08 , I17 ) . I13 stated that it was a requirement for all capstone projects to be community - related . However , from a contrary perspective , I10 was convinced that links to ‘social good’ had no beneficial effect , categorically stating “there’s no data - I looked it up” . Mostly , when interviewees pointed to particular activities in an affective context they did not raise the issue of whether these would increase engagement for all students or would be appealing only to some . However , I12 ob - served the difference in engagement that would occur because of students’ natural interest in different topics . She felt that although she would try to provide engaging activities , she would not expect all students to be engaged . The same interviewee also raised the issue of differences in engagement depending on subject and year group , making it clear that a variety of strategies were needed and that a ‘one size fits all’ approach was not appropriate . A more nuanced interpretation from I02 related the potential engagement benefits of the activities we provide to the maturity of the learner . Having observed poor take - up on an exciting , games industry visit he had organised , he links the failure to learners’ maturity concluding that “we need to encourage this mentality in students that they are responsible adult learners” . More immature learners are seen as grades - focused , and the benefits of transferable skills and enrichment activities may be appreciated only as students complete capstone units and move out into industry . One question explored whether staff thought there were particu - lar characteristics of CS students which might relate to engagement . While some thought there was little difference , most expressed the view that CS students tended to be more individual learners , less keen to communicate and less inclined to form a learning com - munity . Indeed , I09 stated ( perhaps somewhat provocatively ) that “many of our IT students , they don’t like people” . In this intervie - wee’s opinion , the most important thing staff could do to improve engagement would be to foster a more cohesive community in which students would communicate and support each other better . The benefits of this approach were also highlighted by I17 . A further strand pointed to a ‘loner’ effect in CS as being a con - sequence of many of the activities such as programming , databases and networking being inherently solo activity and that this tended to isolate students from engaging with their peers ( I03 ) . This content tended to be located in the early stages of undergraduate degrees , with more pair or team programming being introduced later on . Thus , content at the earlier stage of the curriculum might be less suitable for collaborative activities and hence communicating with others was not necessary in order to learn it . Group work in these areas was viewed by some as being less successful . It was acknowl - edged that these ‘soft skills’ were more important later in the course , specifically in capstone or industry experience units . However , con - cerns were expressed that leaving certain aspects to be introduced later on in the course may not be effective as expectations and practice become entrenched early on . A large variety of strategies to facilitate engagement by teaching practices were discussed . Approaches stated included : breaking up longer sessions with activities such as quizzes and discussions ; providing problem code for students to debug together ; flipped class - room ; blended learning ; peer review ; team work ; brain - storming ; use of analogies ; links to industry ; and links to society . However there were often conflicting views with , for example , one academic expressing scepticism that flipped classroom teaching engaged stu - dents ( I09 ) and another worrying that some students become pas - sengers in group work . A common theme was the inclusion of as much practical , hands - on activity as possible ( I04 ) . Some intervie - wees noted the benefits of more personal , interactive approaches . I10 felt it was important to know the students individually and contact them if they did not attend . I09 thought the most important factor was to treat students as equals and as adults . A further strand that emerged was the importance some staff placed on the strength of their ‘performance’ in front of a class . I10 said they felt they should be humorous . I12 noted : “It’s my job to not make it boring . ” Otherwise : “Even for me it’s not interesting - I’m not engaged ! ” Several staff noted the need for staff themselves to be engaged , expressing the view that students will not engage if they see their instructors are not showing interest and enthusiasm . With large international enrolments in some courses , the issue of student expectations and the applicability of active learning techniques for all students , such as flipped classrooms , was men - tioned as an issue ( I17 ) . Some academics felt that some groups of international students might have expectations based on a more traditional classroom delivery format and be less well equipped to take advantage of in - class discussion activities . Some academics saw a number of areas assessed by engagement surveys as being beyond the scope of individual academics . For example several felt that discussion of careers is not their job . I05 said they “felt daunted” at the prospect and I10 stated “It’s very hard to know what a good career is in CS” , making it clear that they feel ill equipped for the task . Several academics noted organisa - tional constraints , such as growing class sizes , online courses , and a lack of resources , as impacting on their ability to facilitate engage - ment with and by students . Growing class size was seen by some academics as reducing options for engaging students individually . Some academics commented on the scalability of active learning techniques and providing timely high quality feedback to students . Restrictions in curriculum flexibility were also mentioned as being a factor in reducing the ability of individual academics to foster increased student engagement . With regard to enrichment activ - ities and a focus on industry engagement , fostering an inclusive and supportive learning environment might require an institutional response by the university and the faculty to support individual academics , as noted by I17 . Many academics used terminology associated with engagement , active learning and motivating students . However they did not display an in - depth understanding of these terms and had difficulty Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 16 nominating specific strategies to facilitate student engagement . En - gagement activities cited tended to relate to monitoring attendance , setting assessments to promote attendance and deploying frequent quiz activities . Conversely , several academics discussed activities that would normally be described as targeting student engagement factors but did not classify them as such . For example , one academic ( I06 ) had implemented a flipped classroom active learning strategy with the use of student response systems in lectures but did not associate these practices with student engagement measures . In general a lack of familiarity with the terminology associated with student engagement by CS academics was evident . It is likely that a clearer understanding of engagement and related issues could help staff target their teaching initiatives more effectively . Several indicated that they would appreciate guidance on reliable strate - gies for increasing CS student engagement . Interviewees reported that many of their teaching strategies were motivated by the need to engage students more . However , none mentioned monitoring the effect on engagement in any formal way or by means of any evaluation beyond the usual class satisfaction surveys . 6 . 2 . 2 Academics’ awareness of international benchmarks and responses to specific survey questions . This section considers the interview data further to address RQ 3 : How aware are CS aca - demics of international benchmarking instruments and how do they interpret specific questions from the surveys ? Academics were asked about their awareness of the relevant student engagement survey in their country and whether results were disseminated to academic staff . Further , they were asked to respond to specific questions extracted from student engagement survey instruments . Most interviewees were unaware of the international surveys and none could say what survey would cover their university . Even when academic staff knew such surveys existed , they did not know if participation by their institution was optional , or whether or not their institution engaged . Results were not being disseminated to academic staff in individual departments . One interviewee raised the issue of ‘critical mass’ since some CS departments may not have enough respondents to be reported as an individual unit . As many academics were not aware of these survey instruments or the results obtained by CS , they also had no knowledge of the specific engagement measures that were included in the instruments . This is of concern for two reasons . Firstly , academics were unaware of specific measures of engagement and therefore would be less likely to develop teaching practices to facilitate such engagement . Secondly , even where academics were implementing practices to increase student engagement , they were unlikely to communicate them to students in such terms . Therefore CS students would be less likely to relate their experiences to items mentioned in student engagement surveys . It was clear that a greater familiarity with stu - dent engagement terminology and techniques would be of benefit to CS academics . This would be an important first step to changing CS teaching practice and hence to raising students’ understanding of the relevance and importance of some of the engagement - related teaching and learning activities they encounter . Staff were shown questions from the surveys and asked to discuss their reactions to these as measures of engagement for CS students , and how they thought their students would interpret and respond to the questions . Staff pointed to a variety of aspects that they found difficult to interpret and which they thought would cause misunderstanding amongst their students . One survey question ( number 3 in the interview protocol shown in Appendix A . 3 ) asks students how many papers of different lengths they have written . There was considerable diversity of opinion over how students would interpret this . I01 believed that students would definitely think it meant length of programs written . Others were equally sure that their students would think it means prose assignments . I05 thought that students would be unsure of how to calculate pages because assignments would not be expressed in that way . Given the difference of opinion amongst staff it seems highly likely that CS students’ interpretations would also differ widely . This question was certainly contentious but it should be noted that each measure was derived from multiple questions . While some staff thought writing substantial reports was something CS students should be doing more of , others felt that the length should not be viewed as the important factor . Most thought their CS students would do less writing than students from other disciplines , but I04 stated their students did as much . Several interviewees ( such as I01 ) felt that writing essays was not relevant to CS , with I09 going so far as to say the question was “meaningless” . The issue of writing illustrates the point that some activities included in the surveys , and designed as universally good indicators of engagement , are seen by CS academics as anything from desirable to completely meaningless in the CS context . Other questions were also noted as being open to misinterpreta - tion by students . Words and phrases thought likely to cause this include , “learning community” , “complex project” , “two or more drafts of a paper” , “co - op” , “field experience” , “clinical placement” and “community - based project” . Questions thought by some to be too vague to receive valid answers include 8a concerning asking questions in class . A further issue was that CS students may do some of the activities in the questions but would not interpret them in that way . For example , I12 noted that their students do a good deal of peer code review and that this activity and the discussions it involved clearly constitute ‘evaluating a point of view’ , mentioned in Question 9d . But the academic thought that students would not recognise such CS activities as being related to this question . The relevance of some activities was brought into question , al - though in each case there were also staff who thought the activity was relevant to CS students . In addition to writing , these problem - atic questions included : linking to societal issues ( 1b ) ; considering diverse perspectives ( 1c ) ; examine strengths and weaknesses of views ( 1d ) ; tried to better understand someone else’s point of view ( 1e ) ; learned something that changed understanding ( 1f ) ; leader - ship ( 4b ) ; study abroad ( 4d ) ; understanding people from other back - grounds ( 6h ) ; being an informed citizen ( 6j ) ; preparing two or more drafts ( 8b ) ; attending an art exhibition ( 8d ) . Given the conflicting views of staff on the relevance of these questions it is likely that there would be a great deal of variation in relation to how each of these activities is reflected and supported in teaching practice . However , despite the lack of agreement , staff reported incorporat - ing support for a good deal of the aspects raised by the questions in their own practice . Examples commonly offered relate to pair activities or teamwork , breaking up lectures with short activities or discussions , and providing online quizzes . There still seems to be a good deal of large group lecturing and several interviewees Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 17 felt constrained in their teaching by the class sizes , timetabling and rooms provided by their institution . Several were aware of initiatives at a departmental or institu - tional level , but I17 felt more needed to be done to inform staff and get them involved in such initiatives . However , academics did not always feel these initiatives were helpful . For example , I09 reported that one of her modules had been chosen as part of an “enhance - ment” programme to encourage engagement . This involved use of a flipped classroom but she remained sceptical about its benefits . In summary the above discussion highlights the diversity of views on student engagement expressed by a small sample of CS academics . There was considerable lack of awareness of this con - cept , terminology and the specific measures of engagement applied . Teaching practices aimed at increasing student engagement were varied and many academics felt constrained in their ability to ad - dress engagement issues by issues such as class size . Interpretation of survey questions was also highly varied . Many questioned the relevance to CS of specific questions and expressed doubt that CS students would interpret the questions as intended by the designers of the surveys . It would be interesting to collect similar data from students themselves and compare this to the academics’ views . 6 . 3 Examples of best practice in CS research on student engagement During the literature review process , reviewers were asked to indi - cate papers which might illustrate best practice in different dimen - sions and measures of student engagement . The research question addressed is : RQ 4 . What CS education research can be identified as exemplifying ‘best practice’ relating either to benchmark en - gagement measures or to a broader conceptual understanding of engagement ? To be marked as ‘best practice’ papers had to be well aligned with engagement issues , there needed to be a clear research question or objective and rigorous engagement - related evaluation of the work . Overall , 51 such papers were identified . Although this is a relatively small number , it does show that strong CS education research is being carried out in many dimensions and measures of student engagement . In some cases , CS - specific and innovative aspects of student engagement are evidenced . Here we briefly in - troduce a sample of the papers identified to illustrate the coverage of engagement dimensions , topics and approaches used . Case 1 : Illustrating performance indicators and course characteristics to support students’ self - regulated learning in CS1 Student engagement dimensions behavioural and cognitive Themes aligned effective teaching practices Themes not aligned learning strategies , reflective learning Ott et al’s 2015 paper [ 38 ] in Computer Science Education fo - cuses on the importance of quality feedback to foster learner self regulation but the example demonstrates the issue of alignment with specific engagement measures . The authors aim to increase students’ awareness of the impact of different study behaviours . In this study , 200 CS1 students were provided with infographic information about the course using learning analytics , and projec - tions for future attainment were provided based on the student’s current position . The study provided clear evidence of change in students’ thinking about their own learning ( for example , helped students think about and balance their workload better ) as well as influencing behavioural aspects ( as in affecting students’ decision to carry out the required pre - class reading ) . The former is clearly reflective practice , but its cognitive approach does not align with any survey question . It is also clearly helping students develop learning strategies , but again not in a way that would allow stu - dents to answer positively to engagement survey question on this theme . However , students would probably see the relationship of this initiative to several of the survey questions contributing to the effective teaching theme . Case 2 : Defining and Evaluating Conflictive Animations for Programming Education : The Case of Jeliot ConAn Student engagement dimensions cognitive Themes aligned skills development Themes not aligned higher order learning The previous case reported an intervention that could have been used for any subject area , but this paper by Morino et al [ 33 ] is an example of an engagement activity which is specific to CS . Using an adapted scale for engagement with algorithm animations , the authors investigate the effect of producing animations that delib - erately produce cognitive conflict by showing something which is not the same as the algorithm under investigation . This is clearly a situation which demands higher order learning skills from students as , rather than passively watching an animation or even extend - ing something which has been clearly and correctly presented to them , they must develop a deep understanding of the algorithm to work out the discrepancies between it and the animation . It is un - likely that students would recognise this as falling within the more narrowly - presented , ideas analysis perspective of survey questions evidencing higher order learning . Case 3 : Team Project Experiences in Humanitarian Free and Open Source Software ( HFOSS ) Student engagement dimensions behavioural and cognitive Themes aligned reflective learning , collaborative learning , work - integrated learning Themes not aligned N / A One area of particular relevance to CS is the final year ( or cap - stone ) project and this example links skills development to societal issues . Although projects are not unique to CS , the computing do - main lends itself well to developing desirable skills and connecting both with other subjects and with high impact associations . In this paper by Ellis et al , students’ projects are directed towards humani - tarian aims , with a resulting increase in students’ motivation and engagement [ 12 ] . Not only does the approach increase students’ interest in computing and improve their subject - related skills and knowledge , but it is also shown to improve other engagement - related skills areas , notably skills of communication and distributed teamwork . Studies such as these point to the opportunities afforded by the CS curriculum and make it even more surprising that the subject does not score more highly on related survey measures . Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 18 Case 4 : A Review of Generic Program Visualization Systems for Introductory Programming Education Student engagement dimensions behavioural and cognitive Themes aligned effective teaching practices , active Learning Themes not aligned N / A This paper describes a meta - analysis of CS literature on program visualization systems aimed at introducing the execution - time dy - namics of computer programs to novice programmers in introduc - tory courses . The review provides an example of a higher level systematic examination of CS educational tools and practice that is more likely to result in systemic change , as it includes reviews of 24 systems developed between 1983 and 2009 . The analysis reveals that many papers are evaluations of short - lived prototype systems , indi - cating a need for larger scale and more rigorous research in this area . This paper is also notable in its focus on specific student engage - ment literature and measures . After reviewing existing measures of engagement the authors propose their own 2DET engagement taxonomy [ 48 ] . Learner activity is measured on two dimensions , di - rect engagement and content ownership , and is primarily focussed on the behavioural and cognitive aspects of learner activity . This attention to specific engagement measures is relatively rare in the CS literature examined . Also of note is the systematic approach applied to review the systems examined , including a discussion of the empirical research methods used to evaluate the systems . Case 5 : Interactions of Individual and Pair Programmers with an Intelligent Tutoring System for Computer Science Student engagement dimensions behavioural Themes aligned collaborative learning , skills development Themes not aligned N / A This is a small scale but well - focused example of examining learning activity and engagement in fine detail . The study exam - ines the ChiQat - Tutor Intelligent Tutoring System [ 21 ] as students solve coding problems either in pairs or individually . The study , run over two semesters of the same course , involved 173 participants and presents data from : pre and post tests , extensive activity track - ing data from the system itself , and a brief post treatment survey of student satisfaction . Specific focus was placed on fine - grained differences in activity between individual and pair programming conditions . For example the pair programmers required less time to solve problems , relied less on examples , showed more persistence in working through problems , compiled more successfully and coded more efficiently . Student satisfaction was also reported in this study but mainly to support the other data presented rather than as the primary source of evidence as to the efficacy of the approach . Case 6 : Supporting and structuring “contributing student pedagogy” in Computer Science curricula Student engagement dimensions behavioural , cognitive , emo - tional / affective Survey themes aligned reflective learning , higher order learning , collaborative learning , active learning , skills development Survey themes not aligned N / A This study discusses the introduction of Contributing Student Pedagogy ( CSP ) across the curriculum in a CS degree program with the specific aim to foster learning communities , collaboration and deeper engagement in learning . The study is well supported theo - retically using principles of social constructivism and community - based learning . The key element is “valuing student contributions , including an awareness by students that their contribution will be used by others to facilitate their learning , and assessed as to its suitability for that purpose . ” [ 14 , p . 414 ] . Due to the wide ranging changes made to teaching practice impacts were anticipated in all the behavioural , cognitive , emotional / affective dimensions of stu - dent engagement . Using an Action Research methodology the study authors describe the impact of reshaping their teaching practice to be more student driven , practice - based , social and reflective . Three cycles of evaluation were implemented , using data such as student attendance and participation in course activities , and student per - ceptions of their engagement and motivation , to demonstrate the efficacy of their approach . While these projects are just selected examples , they indicate that CS education research is considering aspects of student en - gagement , albeit in a modest number of papers . Indeed , the variety of aspects considered and the ways in which engagement themes are addressed show a much wider perspective than some student engagement surveys . Some of the activities and approaches noted were subject - specific and are applicable only to CS . These are signs of healthy research activity . However , many initiatives examined were small scale so unlikely to influence CS teaching more widely . 7 RECOMMENDATIONS This section presents recommendations based on the analysis car - ried out on the computing education literature and the views of CS academics . While further work is generally needed to increase understanding of CS and student engagement , several recommenda - tions emerge from this report relating to the following categories : • Ways to facilitate discussion of student engagement in rela - tion to computer science . • Ways of increasing staff awareness of international bench - marks and student engagement measures . • Suggestions for focussing future CS education research to increase understanding of engagement in the CS context . • Identifying problematic issues in benchmark instruments in terms of measuring student engagement in CS . Recommendation 1 : Computer Science departments should analyse the performance of CS in their relevant national student engagement benchmark and distribute the results to their academic staff . This will help identify areas of strength and weakness and also increase staff awareness of relevant benchmark indicators ( Evi - dence : Interview data shows general lack of awareness of academic staff regarding survey instruments and results ) . Recommendation 2 : Computer Science departments should develop a specific plan to address areas of perceived weakness . This may include academic staff training in the following areas : 1 ) discussing the terminology and definitions for students engagement , 2 ) discussion of relevant benchmark indicators associated with their national survey , and 3 ) reviews of teaching practices associated with increased student engagement , including examples of best practice Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 19 in the computing discipline ( Evidence : Interview and computing education research data show a lack of awareness of engagement issues but a desire to improve practice ) . Recommendation 3 : That institutions show leadership in cer - tain areas of engagement as academics have limited influence in the overall shape of some aspects of student experience at a course and university level ( Evidence : Interview data related to in - class vs out of class engagement ) . Recommendation 4 : Communications with students should be revised to include relevant student engagement terminology and interactions with students should focus where possible on promoting student engagement measures ( Evidence : Interview data in terms of a lack of awareness of terminology ) . Recommendation5 : CSdepartmentsshouldconsiderre - framing their curriculum to provide contexts that are personally relevant to students , for example Computing for Social Good and Service Lead Teaching , in order to broaden the appeal of the computing discipline , as in Case 3 ( Evidence : Computing education research data related to Emotional / Affective dimensions , gender , identity and motivation ) . Recommendation 6 : CS teaching practice and research should focus more on student engagement and ways to formally measure outcomes in terms of student engagement measures ( Evidence : Interview data and computing education research data related to the rigour of assessing outcomes ) . Recommendation 7 : The CS community should publicize ex - amples of best teaching practice in terms of increasing student engagement ( Evidence : Computing education research data relat - ing to examples of high quality research activity ) . Recommendation 8 : The CS community should consider spe - cific funding for larger scale research into CS relevant educational practices to increase student engagement ( Evidence : Computing education research data related to the current focus on small scale studies involving a single class or class sub - sets ) . Recommendation 9 : Research should be carried out to deter - mine the impact on engagement issues of large class sizes and high levels of international enrolments . Techniques that are appropriate for smaller groups may not scale well . Also international students may have divergent views on engagement and teaching techniques ( Evidence : Interview data related to comments of enrolment num - bers and student background and expectations ) . Recommendation10 : CSconferenceandeditorialboardsshould consider strengthening editorial policies in relation to research with a stated aim to focus on student engagement , in order to improve the quality of the research accepted and to ensure rigour in terms of methodology . Conference chairs and editorial boards might also consider creating student engagement tracks and issues in view of the finding that approximately 23 % of the literature in the field has some focus on student engagement ( Evidence : Computing educa - tion research data on the rigour of publications ) . Recommendation 11 : CS as a body should provide feedback to the designers of Student Engagement Benchmark instruments as to : 1 ) the use of inclusive language in the questions , 2 ) the need to rephrase specific ambiguous questions , 3 ) the inclusion of a wider range of examples , for example including hack - a - thons or cyber - challenges in discussion of enrichment activities , 4 ) the inclusion of CS specific ways of engaging , for example debugging and testing as opposed to drafts of papers , and 5 ) discussion of the interpretation and reporting of results in the context of CS ( Evidence : Interview data on the CS academic response to specific survey questions ) . The above recommendations are intended to facilitate discussion of the performance of CS in international benchmarks on student engagement and how we might improve that performance . 8 CONCLUSIONS AND FURTHER RESEARCH Many intriguing themes have emerged from this Working Group report . Firstly , the analysis of recent computing education literature has highlighted the widespread but a - conceptual use of the term ‘engagement’ within the CS education research community . This is of concern because it may assume a shared understanding while actually masking the very opposite . Further , failure to build on the extensive engagement literature available in more general educa - tion research means we are in danger of missing out on findings and best practice from other disciplines , which are performing much better on these measures , and that could benefit CS educational practice . Considering the alignment of research areas to engagement sur - vey measures it was found that some measures are addressed by few papers in the CS education research literature . Well aligned measures included Active Learning and Collaborative Learning . However , some research which does address the themes is often of a nature which does not align to the specific student engage - ment measures and questions contained in the surveys , for example Higher Order Learning . Despite many papers stating that the work aims to improve engagement , very little evaluation of engagement is presented . Overall , the rigour and focus of CS research on student engagement could be improved . However there are a number of examples of outstanding CS research in this area . The wide variety in academics’ conception of student engage - ment is striking . Debate over the appropriate level of , for example , written assessments is perhaps to be expected . However , some - what more surprising was the view that any explicit support for non - technical aspects is a waste of CS teaching time . Staff with such beliefs are unlikely to introduce teaching activities that relate to many of the skills - related measures of engagement or which explicitly scaffold more cognitive , reflective approaches that help students ‘learn to learn’ . When considering the spectrum of possible causes for the poor performance of CS presented in Figure 3 , it is clear from the anal - ysis above that inappropriate instrument design and a lack of CS discipline specific measures / questions is only a small part of the problem , as CS performance is poor across a range of instruments with different question designs and engagement measures . Evidence gained from a analysis of recent computing education literature and interviews with CS academics points to a lack of awareness of student engagement issues and a need to improve our research and teaching practice . Future work will include further interviews to widen the pool of CS academics . In particular the views of CS academics from North America and also from more CS academics not specifically interested in education research would enhance the data set . We also intend to collect data from CS students to examine the issue of engagement from a student perspective . Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 20 ACKNOWLEDGMENTS Support for this project has been provided by the Monash Education Academy and also by KEGA , The Ministry of Education , Science , Research and Sport of the Slovak Republic ( KEGA 009KU - 4 / 2017 ) . REFERENCES [ 1 ] Paul Ashwin and Debbie McVitty . 2015 . The meanings of student engagement : implications for policies and practices . In The European Higher Education Area . Springer , 343 – 359 . [ 2 ] Alexander W Astin . 1984 . Student involvement : A developmental theory for higher education . Journal of college student personnel 25 , 4 ( 1984 ) , 297 – 308 . [ 3 ] Rick D Axelson and Arend Flick . 2010 . Defining student engagement . Change : The magazine of higher learning 43 , 1 ( 2010 ) , 38 – 43 . [ 4 ] Lisa Bomia , Lynne Beluzo , Debra Demeester , Keli Elander , Mary Johnson , and Betty Sheldon . 1997 . The Impact of Teaching Strategies on Intrinsic Motivation . ( 1997 ) . [ 5 ] C Bryson , C Hardy , and L Hand . 2009 . Student expectations of higher education . Learning and Teaching Update : Innovation and Excellence in the Classroom 27 ( 2009 ) , 4 – 6 . [ 6 ] Matthew Butler , Jane Sinclair , Michael Morgan , and Sara Kalvala . 2016 . Com - paring international indicators of student engagement for computer science . In Proceedings of the Australasian Computer Science Week Multiconference . ACM , 6 . [ 7 ] Angela Carbone , Michael de Raadt , Raymond Lister , Margaret Hamilton , Judy Sheard , et al . 2008 . Classifying computing education papers : process and results . In Proceedings of the Fourth international Workshop on Computing Education Research . ACM , 161 – 172 . [ 8 ] Social Research Centre . 2017 . 2016 Student Experience Survey - National Report . ( 2017 ) . Available from https : / / www . qilt . edu . au / docs / default - source / gos - reports / 2017 / 2016 - ses - national - report - final . pdf . [ 9 ] Arthur W Chickering and Zelda F Gamson . 1987 . Seven principles for good practice in undergraduate education . AAHE bulletin 3 ( 1987 ) , 7 . [ 10 ] James S Cole and Robert M Gonyea . 2010 . Accuracy of self - reported SAT and ACT test scores : Implications for research . Research in Higher Education 51 , 4 ( 2010 ) , 305 – 319 . [ 11 ] CORE . 2016 . CORE Rankings Portal . ( 2016 ) . http : / / www . core . edu . au / conference - portal [ 12 ] Heidi JC Ellis , Gregory W Hislop , Stoney Jackson , and Lori Postner . 2015 . Team Project Experiences in Humanitarian Free and Open Source Software ( HFOSS ) . ACM Transactions on Computing Education ( TOCE ) 15 , 4 ( 2015 ) , 18 . [ 13 ] ERA . 2017 . Excellence in Research for Australia . ( 2017 ) . http : / / www . arc . gov . au / excellence - research - australia [ 14 ] Katrina Falkner and Nickolas JG Falkner . 2012 . Supporting and structuring “contributing student pedagogy” in computer science curricula . Computer Science Education 22 , 4 ( 2012 ) , 413 – 443 . [ 15 ] Sally Fincher and Marian Petre . 2004 . Computer Science Education Research . [ 16 ] Jennifer A Fredricks , Phyllis C Blumenfeld , and Alison H Paris . 2004 . School engagement : Potential of the concept , state of the evidence . Review of educational research 74 , 1 ( 2004 ) , 59 – 109 . [ 17 ] Jennifer A Fredricks and Wendy McColskey . 2012 . The measurement of student engagement : A comparative analysis of various methods and student self - report instruments . In Handbook of research on student engagement . Springer , 763 – 782 . [ 18 ] Jonathan Gordon , Joe Ludlum , and J Joseph Hoey . 2008 . Validating NSSE against student outcomes : Are they related ? Research in Higher Education 49 , 1 ( 2008 ) , 19 – 39 . [ 19 ] Coates Hamish . 2005 . The value of student engagement for higher education qualityassurance . QualityinHigherEducation 11 , 1 ( 2005 ) , 25 – 36 . https : / / doi . org / 10 . 1080 / 13538320500074915 arXiv : http : / / dx . doi . org / 10 . 1080 / 13538320500074915 [ 20 ] LoisRuthHarris . 2008 . Aphenomenographicinvestigationofteacherconceptions of student engagement in learning . The Australian Educational Researcher 35 , 1 ( 2008 ) , 57 – 79 . [ 21 ] Rachel Harsley , Davide Fossati , Barbara Di Eugenio , and Nick Green . 2017 . Inter - actions of Individual and Pair Programmers with an Intelligent Tutoring System for Computer Science . In Proceedings of the 2017 ACM SIGCSE Technical Sympo - sium on Computer Science Education . ACM , 285 – 290 . [ 22 ] Mike Joy , Jane Sinclair , Shanghua Sun , Jirarat Sitthiworachart , and Javier López - González . 2009 . Categorising Computer Science Education Research . Education and Information Technologies 14 , 2 ( June 2009 ) , 105 – 126 . https : / / doi . org / 10 . 1007 / s10639 - 008 - 9078 - 4 [ 23 ] Ella R Kahu . 2013 . Framing student engagement in higher education . Studies in higher education 38 , 5 ( 2013 ) , 758 – 773 . [ 24 ] Päivi Kinnunen , Veijo Meisalo , and Lauri Malmi . 2010 . Have we missed some - thing ? : identifying missing types of research in computing education . In Proceed - ings of the Sixth international workshop on Computing education research . ACM , 13 – 22 . [ 25 ] Kerri - LeeKrauseandHamishCoates . 2008 . StudentsâĂŹengagementinfirst - year university . Assessment & Evaluation in Higher Education 33 , 5 ( 2008 ) , 493 – 505 . [ 26 ] George D Kuh . 2001 . Assessing what really matters to student learning inside the national survey of student engagement . Change : The Magazine of Higher Learning 33 , 3 ( 2001 ) , 10 – 17 . [ 27 ] George D Kuh . 2003 . What we’re learning about student engagement from NSSE : Benchmarks for effective educational practices . Change : The Magazine of Higher Learning 35 , 2 ( 2003 ) , 24 – 32 . [ 28 ] S Lamborn , F Newmann , and G Wehlage . 1992 . The significance and sources of student engagement . Student engagement and achievement in American secondary schools ( 1992 ) , 11 – 39 . [ 29 ] Heather P Libbey . 2004 . Measuring student relationships to school : Attachment , bonding , connectedness , and engagement . Journal of school health 74 , 7 ( 2004 ) , 274 – 283 . [ 30 ] Alex Lishinski , Jon Good , Phil Sands , and Aman Yadav . 2016 . Methodological Rigor and Theoretical Foundations of CS Education Research . In Proceedings of the 2016 ACM Conference on International Computing Education Research ( ICER ’16 ) . ACM , NewYork , NY , USA , 161 – 169 . https : / / doi . org / 10 . 1145 / 2960310 . 2960328 [ 31 ] LauriMalmi , JudySheard , Simon , RomanBednarik , JuhaHelminen , AriKorhonen , Niko Myller , Juha Sorva , and Ahmad Taherkhani . 2010 . Characterizing Research in Computing Education : A Preliminary Analysis of the Literature . In Proceedings of the Sixth International Workshop on Computing Education Research ( ICER ’10 ) . ACM , New York , NY , USA , 3 – 12 . https : / / doi . org / 10 . 1145 / 1839594 . 1839597 [ 32 ] Raymond B Miller , Barbara A Greene , Gregory P Montalvo , Bhuvaneswari Ravin - dran , andJoeDNichols . 1996 . Engagementinacademicwork : Theroleoflearning goals , future consequences , pleasing others , and perceived ability . Contemporary educational psychology 21 , 4 ( 1996 ) , 388 – 422 . [ 33 ] Andrés Moreno , Erkki Sutinen , and Mike Joy . 2014 . Defining and evaluating conflictive animations for programming education : The case of Jeliot ConAn . In Proceedings of the 45th ACM technical symposium on Computer science education . ACM , 629 – 634 . [ 34 ] Thomas L . Naps , Guido Rossling , Vicki Almstrum , Wanda Dann , Rudolf Fleischer , Chris Hundhausen , Ari Korhonen , Lauri Malmi , Myles McNally , Susan Rodger , and J . Ángel Velázquez - Iturbide . 2002 . Exploring the Role of Visualization and Engagement in Computer Science Education . SIGCSE Bull . 35 , 2 ( June 2002 ) , 131 – 152 . https : / / doi . org / 10 . 1145 / 782941 . 782998 [ 35 ] Jonathan Neves . 2017 . Student Engagement and Skills Development : The UK Engagement Survey 2016 . ( 2017 ) . [ 36 ] NSSE . 2017 . NSSE Home . ( 2017 ) . http : / / nsse . indiana . edu / [ 37 ] NSSE . 2017 . NSSE Report Builder . ( 2017 ) . http : / / nsse . indiana . edu / html / report _ builder . cfm [ 38 ] ClaudiaOtt , AnthonyRobins , PatriciaHaden , andKerryShephard . 2015 . Illustrat - ing performance indicators and course characteristics to support studentsâĂŹ self - regulated learning in CS1 . Computer Science Education 25 , 2 ( 2015 ) , 174 – 198 . [ 39 ] Stephen L Payne , Karynne LM Kleine , Jim Purcell , and Ginger Rudeseal Carter . 2005 . Evaluating academic challenge beyond the NSSE . Innovative Higher Educa - tion 30 , 2 ( 2005 ) , 129 – 146 . [ 40 ] Arnold Pears , Stephen Seidman , Crystal Eney , Päivi Kinnunen , and Lauri Malmi . 2005 . Constructing a Core Literature for Computing Education Research . SIGCSE Bull . 37 , 4 ( Dec . 2005 ) , 152 – 161 . https : / / doi . org / 10 . 1145 / 1113847 . 1113893 [ 41 ] StephenRPorter , CoreyRumann , andJasonPontius . 2011 . Thevalidityofstudent engagement survey questions : can we accurately measure academic challenge ? New Directions for Institutional Research 2011 , 150 ( 2011 ) , 87 – 98 . [ 42 ] Ali Radloff , Hamish Coates , Richard James , and Kerri - Lee Krause . 2011 . Report on the development of the University Experience Survey . ( 2011 ) . [ 43 ] Amy L Reschly and Sandra L Christenson . 2012 . Jingle , jangle , and concep - tual haziness : Evolution and future directions of the engagement construct . In Handbook of research on student engagement . Springer , 3 – 19 . [ 44 ] Judy Sheard , S Simon , Margaret Hamilton , and Jan Lönnberg . 2009 . Analysis of research into the teaching and learning of programming . In Proceedings of the fifth international workshop on Computing education research workshop . ACM , 93 – 104 . [ 45 ] Simon . 2007 . A classification of recent Australasian computing education publi - cations . Computer Science Education 17 , 3 ( 2007 ) , 155 – 169 . [ 46 ] Jane Sinclair , Matthew Butler , Michael Morgan , and Sara Kalvala . 2015 . Measures of student engagement in computer science . In Proceedings of the 2015 ACM Conference on Innovation and Technology in Computer Science Education . ACM , 242 – 247 . [ 47 ] Ellen Skinner , Carrie Furrer , Gwen Marchand , and Thomas Kindermann . 2008 . Engagement and disaffection in the classroom : Part of a larger motivational dynamic ? Journal of Educational Psychology 100 , 4 ( 2008 ) , 765 . [ 48 ] Juha Sorva , Ville Karavirta , and Lauri Malmi . 2013 . A review of generic pro - gram visualization systems for introductory programming education . ACM Transactions on Computing Education ( TOCE ) 13 , 4 ( 2013 ) , 15 . [ 49 ] Paul Trowler and Vicki Trowler . 2010 . Student engagement evidence summary . ( 2010 ) . Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 21 [ 50 ] Vicki Trowler . 2010 . Student engagement literature review . The higher education academy 11 ( 2010 ) , 1 – 15 . [ 51 ] GadYair . 2000 . EducationalbattlefieldsinAmerica : Thetug - of - waroverstudents’ engagement with instruction . Sociology of Education ( 2000 ) , 247 – 269 . A APPENDICES A . 1 Publication Outlets Table 7 : Computing Education Conferences 1 . ACM Special Interest Group on Computer Science Education Conference ( SIGCSE ) 2 . Annual Conference on Innovation and Technology in Com - puter Science Education ( ITiCSE ) 3 . Australasian Computing Education Conference ( ACE ) 4 . International Computing Education Research Workshop ( ICER ) 5 . International Conference on Computing Education Research ( Koli Calling ) Table 8 : Computing Education Journals 1 . Computer Science Education 2 . ACM Transactions on Computing Education ( TOCE ) 3 . IEEE Transactions on Education A . 2 Literature Review Criteria Top level : distinguish between pre - factors , direct engage - ment , affects ( cf Kahu ) . • Structural influences ( university culture , policies and curricu - lum ; student background ) Psychosocial influence ( support , workload , motivation ) . • Student engagement . • Consequences ( learning achievement , satisfaction , well - being , employment citizenship ) . Student Engagement • Behaviour ( Time and effort , interaction , participation ) . • Cognition ( Deeplearning , self - regulation , studentsâĂŹlearn - ing strategies , reflection ) • Emotional / affective ( enthusiasm , interest , identity ) . Does paper address one or more of these ? Is it explicit or implicit ? Does it relate to a stated concept or definition of SE ( if so - what ? ) Is it measured - how ? outcome ? Specific categories Within each of the following , questions might be : • alignment to actual survey measures ; • evidence of other CS measure of the property ; • explicit measurement or more intended outcome ; • does this paper provide a good case study exemplifying the factor ? Reflective learning ( NSSE Academic challenge benchmark ) Evidenced in surveys by : • Examined the strengths and weaknesses of your own views on a topic or issue • Tried to better understand someone else’s views by imagin - ing how an issue looks from his or her perspective • Learned something that changed the way you understand an issue or concept Integrative learning ( NSSE Academic challenge benchmark ) Evidenced in surveys by : • Combined ideas from different courses when completing assignments • Connected your learning to societal problems or issues • Includeddiverseperspectives ( political , religious , racial / ethnic , gender , etc . ) in course discussions or assignments • Connected ideas from your courses to your prior experiences and knowledge Higher order learning ( NSSE Academic challenge benchmark ) Evidenced in surveys by : • Applying facts , theories , or methods to practical problems or new situations • Analysing an idea , experience , or line of reasoning in depth by examining its parts • Evaluating a point of view , decision , or information source • Forming a new idea or understanding from various pieces of information Skill development ( SEQ skills question ) Evidenced in surveys by : • a ) critical thinking skills ? • b ) ability to solve complex problems ? • c ) ability to work with others ? • d ) confidence to learn independently ? • e ) written communication skills ? • f ) spoken communication skills ? • g ) knowledge of the field ( s ) you are studying ? • h ) development of work - related knowledge and skills ? Learning strategies ( NSSE Academic challenge benchmark ) Evidenced in surveys by : • Identified key information from reading assignments • Reviewed your notes after class • Summarized what you learned in class or from course mate - rials Quantitativereasoning ( NSSEAcademicchallengebenchmark ) Evidenced in surveys by : • Reached conclusions based on your own analysis of numeri - cal information ( numbers , graphs , statistics , etc . ) • Used numerical information to examine a real - world problem or issue ( unemployment , climate change , public health , etc . ) • Evaluated what others have concluded from numerical in - formation Collaborative learning ( NSSE Learning with peers benchmark and SEQ learner engagement ) Evidenced in surveys by : • Asked another student to help you understand course mate - rial • Explained course material to one or more students Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 22 • Prepared for exams by discussing or working through course material with other students • Worked with other students on course projects or assign - ments Active learning ( SEQ Teaching quality , NSSE some qs ! ) Evidenced in surveys by : • Has course engaged you actively in learning ? Discussions with diverse others ( NSSE Learning with peers benchmark ) Evidenced in surveys by : • People from a race or ethnicity other than your own • People from an economic background other than your own • People with religious beliefs other than your own • People with political views other than your own Student faculty interaction ( NSSE Experiences with faculty benchmark ) Evidenced in surveys by : • Talked about career plans with a faculty member • Worked with a faculty member on activities other than coursework ( committees , student groups , etc . ) • Discussed course topics , ideas , or concepts with a faculty member outside of class • Discussed your academic performance with a faculty mem - ber Effective teaching practices ( NSSE Experiences with faculty benchmark and SEQ teaching quality ) Evidenced in surveys by : • Clearly explained course goals and requirements • Taught course sessions in an organized way • Used examples or illustrations to explain difficult points • Provided feedback on a draft or work in progress • Provided prompt and detailed feedback on tests or completed assignments • Demonstrated concern for student learning ? • Provided clear explanations on coursework and assessment ? • stimulated you intellectually ? • Commented on your work in ways that help you learn ? • set assessment tasks that challenge you to learn ? Quality of interactions ( NSSE Learning Environment bench - mark and SEQ Teaching Quality ) Evidenced in surveys as interactions with : • Students ; • Academic advisors ; • Faculty ; • Student services staff ( career services , student activities , housing , etc . ) ; • Other administrative staff and offices ( registrar , financial aid , etc . ) • Staff seemed helpful and approachable ? Supportive environment ( NSSE Learning Environment bench - mark and SEQ student support items ) Evidenced in surveys by : • Providing support to help students succeed academically • Using learning support services ( tutoring services , writing center , etc . ) • Encouraging contact among students from different back - grounds ( social , racial / ethnic , religious , etc . ) • Providing opportunities to be involved socially • Providing support for your overall well - being ( recreation , health care , counseling , etc . ) • Helping you manage your non - academic responsibilities ( work , family , etc . ) • Attending campus activities and events ( performing arts , athletic events , etc . ) • Attending events that address important social , economic , or political issues Belonging ( SEQ Learner engagement question ) • Hadasenseofbelongingtoinstitution / department / discipline . Work - integrated learning ( Was formerly in AUSSIE - seems like a good one ) OTHER - categories we want to add In or Out Does the paper relate to student engagement ? Motivation for the research What is the motivation for the paper ? Engagement ( explicitly mentioned ) Learning Outcomes ? Some other goalâĂę student mo - tivation , experience , etc Is a research question explicitly stated ? Links to Theory Is a theoretical construct used for the approach taken ? What is it ? Evaluation and Results Are student outcomes measured ? How are they measured ? How are they reported ? Is engagement specifically reported ? Rigour of the Work Suitable as an example of best practice ? A . 3 Interview Questions Interview Protocol General Questions To Ask All Participants ( 1 ) What country do you teach in ? ( 2 ) What units / topics / subjects do you teach ? ( 3 ) How many years have you been teaching ? ( 4 ) What does student engagement mean to you ? ( 5 ) What have you done in your teaching practice to promote student engagement ? ( 6 ) Are you aware of any initiatives in your department / faculty / university to promote student engagement ? ( 7 ) Are you familiar with the Student Experience / Engagement survey used in your country ? ( 8 ) Do you believe Computer Science students in general have any specific characteristics compared to other students ? ( 9 ) Do you believe there is any Computer Science specific issues related to student engagement ? Obtaining Insight Into International Experience Surveys Please provide comment on a series of questions extracted from international surveys of student experience . As a set comment on any of the following aspects : • How would you interpret these question ? Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 23 • How do you think CS students would interpret / answer these questions ? • How relevant are these questions to the CS domain ? • How much of this do you include in your own teaching ? Provide an example if possible . • Do you think that CS students do less of this than other students - and is that legitimate ? • Is this something CS faculty should be doing ( or why not ) ? • Can you suggest more relevant areas for CS to assess student engagement ? Knowing that these questions are used to measure and report on the engagement of our students , would this prompt you to reconsider your teaching methods or content or context ? Questions Extracted From International Surveys of Student Ex - perience : 1 . During the current school year , about how often have you done the following ? • a . Combined ideas from different courses when completing assignments • b . Connected your learning to societal problems or issues • c . Includeddiverseperspectives ( political , religious , racial / ethnic , gender , etc ) in course discussions or assignments • d . Examined the strengths and weaknesses of your own views on a topic or issue • e . Tried to better understand someone else’s views by imag - ining how an issue looks from his or her perspective • f . Learned something that changed the way you understand an issue or concept • g . Connected ideas from your courses to your prior experi - ences and knowledge 2 . During the current school year , about how often have you done the following ? • a . Talked about career plans with a faculty member • b . Discussed your academic performance with a faculty mem - ber 3 . During the current school year , about how many papers , re - ports , or other writing tasks of the following lengths have you been assigned ? ( Include those not yet completed . ) • a . Up to 5 pages • b . Between 6 and 10 • c . 11 pages or more 4 . Which of the following have you done or do you plan to do before you graduate ? • a . Participate in an internship , co - op , field experience , stu - dent teaching , or clinical placement • b . Hold a formal leadership role in a student organization or group • c . Participate in a learning community or some other formal program where groups of students take two or more classes together • d . Participate in a study abroad program • e . Work with a faculty member on a research project • f . Completeaculminatingseniorexperience ( capstonecourse , senior project or thesis , comprehensive exam , portfolio , etc . ) 5 . About how many of your courses at this institution have included a community - based project ( service - learning ) ? 6 . How much has your experience at this institution contributed to your knowledge , skills , and personal development in the follow - ing areas ? • a . Writing clearly and effectively • b . Speaking clearly and effectively • c . Thinking critically and analytically • d . Analyzing numerical and statistical information • e . Acquiring job - or work - related knowledge and skills • f . Working effectively with others • g . Developing or clarifying a personal code of values and ethics • h . Understanding people of other backgrounds ( economic , racial / ethnic , political , religious , nationality , etc . ) • i . Solving complex real - world problems • j . Being an informed and active citizen 7 . During 2015 , to what extent have the lecturers , tutors and demonstrators in your < course > : • a . engaged you actively in learning ? • b . demonstrated concern for student learning ? • c . provided clear explanations on coursework and assess - ment ? • d . stimulated you intellectually ? • e . commented on your work in ways that help you learn ? • f . seemed helpful and approachable ? • g . set assessment tasks that challenge you to learn ? 8 . During the current school year , about how often have you done the following ? • a . Ask questions or contribute to course discussion in other ways • b . Prepared two or more drafts of a paper or assignment before turning it in • c . Come to class without completing readings or assignments • d . Attended an art exhibit , play , other arts performance ( dance , music , etc . ) • e . Worked with other students on course projects or assign - ments • f . Given a course presentation 9 . During the current school year , how much has your course - work emphasized the following ? • a . Memorizing course material • b . Applying facts , theories , or methods to practical problems or new situations • c . Analyzing an idea , experience , or line of reasoning in depth by examining its parts • d . Evaluating a point of view , decision , or information source • e . Forming a new idea or understanding from various pieces of information Session : Working Group Presentations ITiCSE - WGR ' 17 , July 3 – 5 , 2017 , Bologna , Italy 24