The CAPIO 2017 Conversational Speech Recognition System Kyu J . Han , Akshay Chandrashekaran , Jungsuk Kim , Ian Lane Capio Inc . , Belmont , CA , USA { kyu , akshay , jungsuk , ian } @ capio . ai Abstract In this paper we show how we have achieved the state - of - the - art performance on the industry - standard NIST 2000 Hub5 En - glish evaluation set . We explore densely connected LSTMs , inspired by the densely connected convolutional networks re - cently introduced for image classiﬁcation tasks . We also pro - pose an acoustic model adaptation scheme that simply averages the parameters of a seed neural network acoustic model and its adapted version . This method was applied with the CallHome training corpus and improved individual system performances by on average 6 . 1 % ( relative ) against the CallHome portion of the evaluation set with no performance loss on the Switchboard portion . With RNN - LM rescoring and lattice combination on the 5 systems trained across three different phone sets , our 2017 speech recognition system has obtained 5 . 0 % and 9 . 1 % on Switchboard and CallHome , respectively , both of which are the best word error rates reported thus far . According to IBM in their latest work to compare human and machine transcriptions , our reported Switchboard word error rate can be considered to surpass the human parity ( 5 . 1 % ) of transcribing conversational telephone speech . Index Terms : Densely connected LSTM , neural network acoustic model adaptation , conversational speech recognition 1 . Introduction We have recently observed a series of leap - frog advancements in deep learning based acoustic and language modeling for con - versational speech recognition . With the contributions mainly from convolutional neural networks ( CNNs ) and recurrent neu - ral networks ( RNNs ) , multiple research groups have contin - ued to improve their system performance , usually on the well - known , industry - standard NIST 2000 Hub5 English evaluation set 1 [ 1 , 2 , 3 , 4 , 5 ] . Achieving human parity has now become the topic of the speech recognition community , which nurtured in - teresting research works of contrasting transcriptions from hu - man transcribers and conversational speech recognition systems [ 1 , 2 , 6 ] . It is reported in [ 6 ] that similar error patterns were found between human and machine transcriptions , hinting that the quality of machine transcriptions gets closer to that of hu - man transcribers . Conversational speech recognition , however , is still consid - ered challenging , and we in [ 3 ] performed a comparative analy - sis on how vulnerable even the state - of - the - art conversational speech recognition system would be against real - world tele - phone conversations in the wild . One of the causes to make it hard for speech recognition systems to perform well against real - world data must be the acoustic and lexical variability of the real - world data that has not been exposed in a training phase . To overcome this mismatch , neural network models need to be adapted , but they are widely known to be not easy to be adapted 1 As known as Switchboard , but it actually consists of the two testsets of Switchboard and CallHome . due to a large number of parameters to be tuned . Most of the research effort on neural network adaptation thus has been fo - cused either to update a part of parameters while ﬁxing the rest [ 7 , 8 , 9 , 10 ] or to append domain - speciﬁc features ( e . g . , i - vectors [ 11 ] in case of speaker adaptation ) for a better fea - ture transformation [ 12 , 13 , 14 ] . In this paper , we propose a simple but generalized adaptation method for deep neural net - works such that it obtains expected adaptation beneﬁts as well as avoids overﬁtting . We applied this scheme with the Call - Home training corpus to our individual systems and observed the performance improvement of on average 6 . 1 % ( relative ) on the CallHome subset of the NIST 2000 Hub5 English evaluation set without any loss on the Switchboard subset . We also propose a new neural network based acoustic model structure with dense connections between long short - term memory ( LSTM ) layers . Densely connected neural net - works were originally introduced to avoid layer - wise vanish - ing gradient problems when CNNs are stacked in a very deep fashion , e . g . , more than 100 - layers , for image recognition tasks [ 15 ] . One can view dense connection as a variant from residual learning [ 16 ] or highway networks [ 17 , 18 ] . In speech recog - nition , residual or highway connection have been applied to LSTMs , only between adjacent layers [ 19 , 20 , 21 , 22 ] . Our dense networks connect ( almost ) every layer to one another to mitigate vanishing gradient effect between LSTM layers and help error signals propagated even back to the very ﬁrst layer during back propagation in training . With a lattice - based sys - tem combination , the densely connected LSTMs signiﬁcantly contributed us to reach the marks of 5 . 0 % and 9 . 1 % in word error rate ( WER ) for the Switchboard and CallHome testsets , respectively . This paper is organized as follows . Section 2 describes the proposed densely connected LSTMs , accompanying empirical analysis on residual and dense LSTMs . Section 3 highlights the proposed acoustic model adaptation scheme , and shares the performance of individual systems before and after model adap - tation using the CallHome training data . In Section 4 , we de - tail the other components constituting our 2017 conversational speech recognition system , such as language models and system combination . After presenting experimental results in a broader scale across individual systems in Section 5 , we conclude this paper in Section 6 with the remarks on the contributions and future directions . 2 . Densely Connected LSTM Dense connection [ 15 ] was introduced for CNNs to yield the state - of - the - art performance on the CIFAR - 10 / 100 data sets for image classiﬁcation [ 23 ] , outperforming residual networks [ 16 , 24 ] which had been the best performing neural network architecture in the domain . Like skip connections in resid - ual learning , dense connections let error signals further back - propagated with less gradient vanishing effect between layers in a deep network . One notable difference between dense net - a r X i v : 1801 . 00059v1 [ c s . C L ] 29 D ec 2017 1 5 10 15 20 25 30 1516 17 . 5 20 25 30 Number of LSTM Layers W o r d E rr o r R a t e ( % ) LSTM ResidualLSTM DenseLSTM ( 5 / block ) DenseLSTM ( 10 / block ) Figure 1 : WER comparison between residual and dense con - nection for LSTMs with the cell dimension of 128 . The systems were trained with a small portion of our entire training data , i . e . , 300hr Switchboard - 1 Release 2 ( LDC97S62 ) , and tested against the NIST 2000 Hub5 English evaluation set . works and residual networks is a connectivity pattern . Consid - ering that H (cid:96) ( · ) is a general composite function of operations in the (cid:96) th layer of a given neural network , a residual connectivity for the output of the (cid:96) th layer , x (cid:96) , can be written as x (cid:96) = H (cid:96) ( x (cid:96) − 1 ) + x (cid:96) − 1 , ( 1 ) while a dense connectivity can be represented as x (cid:96) = H (cid:96) ( [ x 1 , x 2 , · · · x (cid:96) − 1 ] ) , ( 2 ) where [ x 1 , x 2 , ··· x (cid:96) − 1 ] is a concatenated vector of outputs from the ﬁrst layer to the ( (cid:96) − 1 ) th layer . The dense connectivity pat - tern accommodates more direct connections throughout layers while residual connections are only made between adjacent lay - ers . More direct connections from dense connectivity could fur - ther help information ﬂow not completely vanished during back propagation in training deep neural networks . We propose densely connected LSTMs in this paper , in - spired by the success of dense connection for CNNs . In speech recognition , there has been a limited number of effort to ex - ploit residual connection or its variants , e . g . , highway con - nection , to LSTMs with minor differences in implementation [ 19 , 20 , 21 , 22 ] , but none using dense connection yet . To peek how dense LSTMs would work as layers get deeper , let us take a look at Figure 1 . For the experiments , we trained ( uni - directional ) LSTMs with the cell dimension of 128 using a small portion of our entire training data , i . e . , 300hr Switchboard - 1 Release 2 corpus from LDC ( LDC97S62 ) , and tested them against the NIST 2000 Hub5 English evaluation set ( Switch - board and CallHome combined ) . The red curve indicates that normal LSTMs would not obtain any beneﬁt after the 6th layer where the lowest WER of 19 . 1 % is reached . The performance of residual LSTMs , depicted as the orange curve , seems further improved until the 10th layer where 17 . 7 % is marked and then continues to degrade thereafter as more layers are added . This validates that residual learning makes LSTMs perform better with more layers as has been reported in other research works , but we also see that there is a clear limitation . The U - shaped curve might be the reason why most of the residual LSTMs for Figure 2 : Structure of a dense TDNN - LSTM acoustic model . Each layer outputs 1 , 024 dimensional non - linear activation vectors . Table 1 : WER comparison for TDNN - LSTMs with and without dense connection . Acoustic Model SWBD CH TDNN - LSTM 7 . 3 % 13 . 8 % Dense TDNN - LSTM 7 . 3 % 13 . 0 % speech recognition so far have been stuck with only up to 15 layers with marginal improvement in WER . In contrast , dense LSTMs are shown continuously beneﬁted as more layers are added even after the 10th layer , further pulling the lowest possi - ble WER down to around 16 % at the number of LSTM layers of 20 ( light blue curve ) . There are a couple of notes on the dense LSTMs experimented . Due to the connectivity pattern in Eq . ( 2 ) of concatenating vectors coming from the previous layers , the dimension of an input vector for the (cid:96) th LSTM layer with the cell dimension of d would be ( (cid:96) − 1 ) × d , which keeps increasing as (cid:96) goes larger . Thus we made LSTM layers into blocks where dense connections are applied only within the same block , and linked blocks by a transitional layer . This dense block concept has been also exploited in the original paper for densely con - nected CNNs [ 15 ] , but with other purposes . The green curve in Figure 1 is based on the dense LSTMs where every group of 5 LSTM layers belong to one block while the light blue curve comes from the dense LSTMs with 10 LSTM layers per block . Dense connection can be easily applied to existing LSTM - based neural network architectures for speech recognition , thanks to a simple connectivity pattern and can improve perfor - mance as we add more layers since it can help alleviate layer - wise vanishing gradient effect . Based on our experimental val - idation from Figure 1 , we propose two dense LSTM architec - tures for conversational speech recognition , which are detailed in the next subsections . 2 . 1 . Dense TDNN - LSTM The ﬁrst proposed network with dense connection is dense TDNN - LSTM . It has the common network skeleton with the model conﬁguration used for the Switchboard testset in [ 25 ] , consisting of 7 - layer time delay neural networks ( TDNNs ) com - bined with 3 - layer LSTMs . The details of the model architec - Figure 3 : Structure of a dense CNN - bLSTM acoustic model . ture is depicted in Figure 2 , where 3 TDNNs are followed by a couple of dense blocks and 1 LSTM in the ﬁnal layer before the softmax layer . Each green - highlighted dense block contains 1 LSTM and 2 TDNNs with the dense connectivity pattern . The ﬁnal layer in each block is to concatenate all the outputs of the neural layers inside the block . Table 1 shows a WER comparison between the original TDNN - LSTM [ 25 ] and the proposed dense TDNN - LSTM . For this experiment , we utilized the entire training data of Fisher English Training Part 1 and 2 ( LDC2004S13 , LDC2005S13 ) and the aforementioned Switchboard corpus . The total amount of the data used for training is approximately 2 , 000hrs . We re - port the performance of the trained models on Switchboard and CallHome separately . It is noticeable in the table that there is a statistically meaningful improvement ( by around 5 % , relative ) on the CallHome testset . 2 . 2 . Dense CNN - bLSTM We propose another densely connected neural network archi - tecture in dense CNN - bLSTM , as shown in Figure 3 . As we explore several dense CNN - bLSTMs in this paper , the ﬁgure is presented as general as possible . This architecture has 3 CNN layers followed by N dense blocks ( blue - highlighted ) , each of which contains M LSTM layers being connected densely to one another . The ﬁnal layer in each block concatenates the output vectors from all the layers inside to deliver to the next block . Table 2 presents the performance of a few dense CNN - bLSTMs with different conﬁgurations . Like the experiments for Table 1 , all the dense CNN - bLSTMs in the table were trained on the 2000hr Switchboard / Fisher data . In the conﬁgurations ( a ) , ( b ) and ( c ) , the dense CNN - bLSTMs have the total 15 LSTM layers . The dense CNN - bLSTM - ( a ) and ( b ) have one transi - tional layer between the two blocks where 7 LSTM layers are allocated each ( N = 2 , M = 7 ) , while the three blocks having 5 LSTM layers each ( N = 3 , M = 5 ) are tightly connected without a transitional layer in the conﬁguration ( c ) . In the dense CNN - bLSTM - ( c ) , the cell dimension in each block gets smaller from 512 to 128 to make the entire neural network shape narrower as we go deeper . In the conﬁguration ( d ) , the dense CNN - bLSTM has the total 30 LSTM layers with a smaller cell dimension of 128 . The dense CNN - bLSTM - ( a ) , ( b ) and ( c ) all exceeded the performance of the dense TDNN - LSTM introduced in Section Table 2 : WER comparison for dense CNN - bLSTMs . N : number of dense blocks , M : number of LSTM layers per dense block , d : LSTM cell dimension . Dense CNN - bLSTM SWBD CH ( a ) N = 2 , M = 7 , d = 512 6 . 7 % 12 . 5 % ( b ) N = 2 , M = 7 , d = 256 7 . 1 % 12 . 5 % ( c ) N = 3 , M = 5 , d = 512 , 256 , 128 7 . 2 % 12 . 6 % ( d ) N = 2 , M = 15 , d = 128 7 . 6 % 13 . 4 % 2 . 1 for both of the Switchboard and CallHome testset . The per - formance gap between the dense CNN - bLSTMs seems to be largely contributed by the LSTM cell dimension . 2 . 3 . Acoustic Model Training The Kaldi toolkit [ 26 ] was used to train these dense networks . Lattice - free maximum mutual information ( LF - MMI ) was cho - sen as an objective function for network training . The cross entropy objective function was also applied as an extra regular - ization as well as leaky HMM to avoid overﬁtting [ 27 ] . The learning rate was gradually adjusted from 10 − 3 to 10 − 5 over the course of 4 epochs . Prior to neural network acoustic modeling , we trained Gaussian mixture models ( GMMs ) within the framework of 3 - state hidden Markov models ( HMMs ) . The conventional 39 - dimensional MFCC features were spliced over 9 frames and LDA was applied to project the spliced features onto a 40 - dimensional subspace . Further projection was conducted through MLLT for better orthogonality . Speaker adaptive train - ing ( SAT ) was applied with feature - space MLLR ( fMLLR ) to further reﬁne mixture parameters in GMMs [ 28 ] . The total 140K word tokens to cover the entire words con - tained in the training data were mapped to the PronLex pronuci - ation lexicon 2 . The phone dictionary consists of 42 non - silence phones with lexical stress markers on vowels as well as two hesitation phones , making the total phones to 44 . 3 . Acoustic Model Adaptation Neural networks are well known to be hard for adaptation due to a huge number of parameters to be tuned , unlike statistical frameworks such as GMMs . As a result there have been alter - native approaches to update only a small part of a neural net - work model [ 7 , 8 , 9 , 10 ] to obtain adaptation beneﬁts . In this paper , we propose a simple model adaptation scheme exploiting parameter averaging . The idea is similar to how Kaldi’s NNET3 acoustic model training handles updated models across multiple GPUs through - out iterations [ 29 ] . Kaldi’s NNET3 training strategy lets each GPU do stochastic gradient descent ( SGD ) separately with dif - ferent randomized subsets of a training data and , after process - ing a ﬁxed number of samples , averages the parameters of the models across all the jobs and re - distribute the result to each GPU . We borrow this concept of parameter averaging to av - erage the parameters of a seed neural network model and its adapted version . In order to update a seed model with adaptation data before parameter averaging , we applied the same training technique in Section 2 . 3 with the LF - MMI objective function . We found it 2 https : / / catalog . ldc . upenn . edu / LDC97L20 Table 3 : Acoustic model adaptation results in WER . Before pa - rameter averaging . The conﬁguration indexes ( a ) , ( b ) and ( c ) for the dense CNN - bLSTMs are inherited from Table 2 . Dense Model SWBD CH TDNN - LSTM 7 . 3 % → 7 . 7 % 13 . 0 % → 12 . 2 % CNN - bLSTM - ( a ) 6 . 7 % → 7 . 3 % 12 . 5 % → 12 . 2 % CNN - bLSTM - ( b ) 7 . 1 % → 7 . 5 % 12 . 5 % → 12 . 1 % CNN - bLSTM - ( c ) 7 . 2 % → 7 . 9 % 12 . 6 % → 12 . 2 % Table 4 : Acoustic model adaptation results in WER . After pa - rameter averaging . The conﬁguration indexes ( a ) , ( b ) and ( c ) for the dense CNN - bLSTMs are inherited from Table 2 . Dense Model SWBD CH TDNN - LSTM 7 . 7 % → 7 . 2 % 12 . 2 % → 12 . 1 % CNN - bLSTM - ( a ) 7 . 3 % → 6 . 9 % 12 . 2 % → 12 . 0 % CNN - bLSTM - ( b ) 7 . 5 % → 7 . 1 % 12 . 1 % → 11 . 9 % CNN - bLSTM - ( c ) 7 . 9 % → 7 . 2 % 12 . 2 % → 12 . 1 % more beneﬁcial to disable the cross entropy loss function while keeping leaky HMM for regularization . The learning rate was set to be gradually decreased from 10 − 5 to 10 − 7 over the course of 4 epochs . We took an advantage of the CallHome American English Speech corpus ( LDC97S42 ) for our experiments on acoustic model adaptation . According to the 2000 Hub5 Evaluation re - sult report by NIST [ 30 ] , this corpus is listed as one of publicly available training materials . We only used a training portion of the corpus which contains 80 telephone conversations between native English speakers of around 13 speech hours . There is no overlap in data itself as well as speaker between this adaptation data and the CallHome portion of the NIST 2000 Hub5 English evaluation set 3 , but it is expected for adapted models to perform better than before adaptation , at least against the CallHome test - set . Tables 3 and 4 show the experimental results from the proposed model adaptation scheme , with and without param - eter averaging . We tested the four dense LSTM models dis - cussed in the previous section . Table 3 speciﬁcally presents the WERs from the updated models before parameter averag - ing . Although there exists a consistent improvement for the CallHome testset across the updated dense LSTM models , the performance against the Switchboard testset is all degraded . This indicates that the models updated with the adaptation data from the CallHome corpus have the parameters shifted toward CallHome - speciﬁc regions in a parameter space , but farther from a Switchboard - speciﬁc domain . The proposed parameter averaging method is shown in Table 4 to balance out the biases in the updated models to settle down the Switchboard WERs back to the range before the model adaptation and keep the ben - eﬁt for the CallHome testset . For the dense TDNN - LSTM and the dense CNN - bLSTM - ( a ) , the slight changes in the SWBD WER are observed ( 7 . 3 % → 7 . 2 % & 6 . 7 % → 6 . 9 % ) from the 3 Unlike CallHome , it was reported in [ 30 ] that 36 out of 40 speakers in the Switchboard portion of the NIST 2000 Hub5 English evaluation set appeared in the conversations of the Switchboard corpora available for training . However , it was also reported that this would have a limited effect in terms of enhancing performance . proposed model adaptation scheme . The overall improvement for the CallHome testset across the updated models is approxi - mately 5 % ( relative ) . 4 . System Descriptions 4 . 1 . Other Acoustic Models To achieve the state - of - the - art performance on the NIST 2000 Hub5 English evaluation set , we consider more acoustic models in addition to the aforementioned dense LSTM models . With more systems with various conﬁgurations from acoustic and lexical perspectives , we could have a bigger boost when com - bining such systems to obtain a better performance . For phoneset diversity , we consider two more phonesets in addition to PronLex mentioned in Section 2 . 3 . According to [ 3 ] , systems being trained across different phonesets can pro - vide more diversities to system combination . We thus exploited the CMU 4 and MSU 5 phoneset . The CMU phoneset consists of 39 phones with three lexical stress markers . The MSU phone - set has 36 phones with no stress distinctions . With these three phonesets ( CMU , MSU , PronLex ) , we trained CNN - bLSTMs with 3 CNN layers and 7 bLSTM layers , respectively . Different trees were formed for the 3 CNN - bLSTMs during the acoustic model training stage , which can provide diversity to a combined system later . We also applied a different hesitation modeling for these CNN - bLSTMs from the dense LSTM models in Section 2 . We used 11 distinct hesitation phones to better distinguish some hesitation utterances , such as ‘uh - huh’ and ‘um - hum’ , in - stead of 2 hesitation phones in the dense LSTM models . On top of the 3 CNN - bLSTM models with 11 hesitation phones across the three phonesets , we built another PronLex - based CNN - bLSTM with 2 hesitations , totaling our individual acoustic model lineup to 8 ( 4 CNN - bLSTMs and 4 dense LSTM models ) shown in Table 5 . For all the CNN - bLSTMs , log - mel cepstra were fed into the three convolutional layers and a 3 × 3 kernel was applied with the ﬁlter size of 32 throughout the layers . The ﬁltered signals were then passed to the 7 - layer bLSTMs with the cell dimension of 1 , 024 after being appended with 100 - dimensional i - vectors . Each neural network layer is followed by non - linear ReLU ac - tivation . 4 . 2 . Language Models The 4 - gram language model ( LM ) was trained with the open - source library of SRILM [ 31 ] on a combination of pub - licly available data , including Fisher English Training Part 1 and 2 ( LDC2004T19 , LDC2005T19 ) , Switchboard - 1 Re - lease 2 ( LDC97S62 ) , CallHome American English Speech ( LDC97T14 ) , Switchboard Cellular Part 1 ( LDC2001T14 ) , TED - LIUM [ 32 ] , British Academic Spoken English ( BASE ) [ 33 ] , Michigan Corpus of American Spoken English ( MICASE ) [ 34 ] and English Gigaword ( LDC2003T05 ) . We used this LM for the 2nd - pass LM rescoring . For the 1st pass decoding , we pruned the trained 4 - gram LM with the pruning thresholds of 1 . 0e - 8 , 1 . 0e - 7 , and 1 . 0e - 6 for bigrams , trigrams , and 4 - grams , respectively . The RNN LM built with the CUED - RNNLM tookit [ 35 ] was trained on a subset of the aforementioned text data , con - sisting of only Fisher , Switchboard and CallHome with 2M sen - 4 http : / / svn . code . sf . net / p / cmusphinx / code / trunk / cmudict 5 http : / / www . isip . piconepress . com / projects / switchboard / r eleases / sw - ms98 - dict . text Table 5 : Experimental evaluation in WER for the 8 individual systems and their combinations . The conﬁguration indexes ( a ) , ( b ) and ( c ) for the dense CNN - bLSTMs are inherited from Table 2 . d : LSTM cell dimension . Acoustic Model d Phoneset HES SWBD CH N - gram RNN N - gram RNN CNN - bLSTM 1 , 024 CMU 11 6 . 8 % 5 . 9 % 11 . 5 % 10 . 7 % CNN - bLSTM 1 , 024 MSU 11 6 . 8 % 5 . 9 % 11 . 3 % 10 . 5 % CNN - bLSTM 1 , 024 PronLex 11 6 . 7 % 5 . 8 % 11 . 6 % 10 . 7 % CNN - bLSTM 1 , 024 PronLex 2 6 . 4 % 5 . 6 % 11 . 4 % 10 . 7 % Dense CNN - bLSTM - ( a ) 512 PronLex 2 6 . 9 % 6 . 0 % 12 . 0 % 11 . 5 % Dense CNN - bLSTM - ( b ) 256 PronLex 2 7 . 1 % 6 . 1 % 11 . 9 % 11 . 1 % Dense CNN - bLSTM - ( c ) 512 , 256 , 128 PronLex 2 7 . 2 % 6 . 1 % 12 . 1 % 11 . 2 % Dense TDNN - LSTM 1 , 024 PronLex 2 7 . 2 % 6 . 1 % 12 . 1 % 11 . 0 % 4 CNN - bLSTMs Combined - - - - 5 . 2 % - 9 . 5 % 4 Dense LSTMs Combined - - - - 5 . 1 % - 9 . 6 % System Combination - - - - 5 . 0 % - 9 . 1 % tences and 24M word tokens . We used variance regularization [ 36 ] as the optimization criterion of the objective function for the RNN LM with 1 , 000 nodes in each of two hidden layers . We trained two separate RNN LMs as we used the two different hesitation modeling approaches in 11 hesitations versus 2 hesi - tations , which resulted in differently normalized transcripts for model training . These RNN LMs are also expected to provide a different level of diversity when combining the systems . 4 . 3 . System Combination In order to combine the total 8 systems , we applied a lattice combination which conducts a union of lattices from individual systems and searches the best path from the extended lattices using minimum Bayes risk decoding [ 37 ] . Due to the different mix ups across the systems in terms of hesitation modeling and phoneset , we had to relabel the word list of each individual sys - tem before the lattice combination . The combination weights were found through a hyper - parameter optimization algorithm , called a tree - structured Parzen estimator [ 38 ] , using a held - out development set . 5 . Experimental Results We evaluated the performance of the total 8 individual sys - tems in 4 CNN - bLSTMs , 3 dense CNN - bLSTMs and 1 dense TDNN - LSTM across the three different phonesets against the Switchboard and CallHome testset of the NIST 2000 Hub5 En - glish evaluation set . The performance in terms of WER , before and after RNN LM rescoring , is shown in Table 5 . Among the acoustic models , the PronLex - based CNN - bLSTM with 2 hesitation phones outperformed the other mod - els , marking 5 . 6 % for SWBD , while the MSU - based CNN - bLSTM with 11 hesitation phones reached the lowest 10 . 5 % for CH . These two numbers are the best reported WERs achieved by any single system so far in the industry . The CNN - bLSTM models obtained WERs 0 . 1 % - 0 . 5 % ( absolute ) for SWBD and 0 . 4 % - 1 . 0 % ( absolute ) for CH lower than the dense acoustic models . Having 2 hesitations when using the PronLex phone - set appears to be a better choice to improve the robustness of the CNN - bLSTM model , but we didn’t have the same pattern for the other phonesets in CMU and MSU ( WERs not reported in the table ) . We observed that RNN LM rescoring provides consistent improvement in all the cases with absolute improve - ments between 0 . 5 % and 1 . 1 % , and a maximum reduction in WER of up to 8 % relative in the case of the dense TDNN - LSTM model on SWBD ( from 7 . 2 % to 6 . 1 % ) . As brieﬂy mentioned in Section 2 . 2 , the cell dimension of LSTMs turns out to be a dominating factor to decide how the models perform . It is noticeable that any CNN - bLSTM model with the cell dimension of 1 , 024 has lower WER than any dense model with lesser cell dimension . As for the dense TDNN - LSTM model , its lower WER than those of the dense CNN - bLSTMs ( although the gap is not huge ) can also be explained by the larger cell dimension of 1 , 024 as compared to a max - imum of 512 in the dense CNN - bLSTMs , even though it has uni - directional LSTMs , not bi - directional . In a system combination perspective , the proposed dense models achieved a similar performance for SWBD and CH with the combined CNN - bLSTMs ( 5 . 1 % vs 5 . 2 % for SWBD & 9 . 6 % vs 9 . 5 % for CH ) , signiﬁcantly contributed our combined system to reach the state - of - the - art performance of 5 . 0 % and 9 . 1 % in WER for SWBD and CH , respectively . We achieved the same results from the combination of the 5 systems as well , from the 3 CNN - bLSTMs with CMU , MSU and PronLex ( with 2 hesita - tion phones ) , dense CNN - bLSTM - ( b ) and dense TDNN - LSTM . To date these are the best reported numbers on these two test - sets . 6 . Conclusions In this paper we have proposed several densely connected LSTM architectures , bringing the dense connectivity that was successful for CNNs in image classiﬁcation tasks to the LSTM framework for conversational speech recognition . This allowed LSTMs to have more direct connections between layers such that layer - wise vanishing gradient effect is alleviated even as more layers are stacked in a deep neural network model . Also we have introduced parameter averaging for acoustic model adaptation that averages the parameters of a seed neural net - work acoustic model and its adapted one , in order to balance out between domain adaptation and generalization . We note that in a comaprison of the reported numbers of 5 . 1 % and 9 . 9 % from [ 5 ] our combined system has made a sig - niﬁcant improvement on the CallHome portion of the NIST 2000 Hub5 English evaluation set , mainly due to the acoustic model adaptation using the CallHome training data of approxi - mately 13 hours of speech . This shows that more domain spe - ciﬁc data which has similar acoustics and lexical information would have direct impact on performance improvement . As dis - cussed in [ 3 ] , even the best conversational speech recognition system could suffer from higher error rates when it is tested against real - world data with a number of unseen dynamics in data characteristics . To have systems more robust to unseen testing conditions , given limited resources of audio data and the corresponding reference transcripts , unsupervised learning that can continuously improve its recognition coverage would be required . In addition , various testing materials beyond the NIST 2000 Hub5 English evaluation set would be able to pro - vide deeper insights on how systems could generalize against real - world data . 7 . References [ 1 ] W . Xiong , J . Droppo , X . Huang , F . Seide , M . Seltzer , and A . Stol - cke , “Achieving human parity in conversational speech recogni - tion , ” in MSR - TR - 2016 - 71 , 2016 . [ 2 ] G . Saon , G . Kurata , T . Sercu , K . Audhkhasi , S . Thomas , D . Dim - itriadis , X . Cui , B . Ramabhadran , M . Picheny , L . Lim , B . Roomi , and P . Hall , “English conversational telephone speech recognition by humans and machines , ” in Proc of INTERSPEECH 2017 , 2017 , pp . 132 – 136 . [ 3 ] K . J . Han , S . Hahm , B . Kim , J . Kim , and I . Lane , “Deep learning - based telephony speech recognition in the wild , ” in Proc of IN - TERSPEECH 2017 , 2017 , pp . 1323 – 1327 . [ 4 ] W . Xiong , L . Wu , F . Alleva , J . Droppo , X . Huang , and A . Stolcke , “The Microsoft 2017 conversational speech recognition system , ” in MSR - TR - 2017 - 39 , 2017 . [ 5 ] G . Kurata , B . Ramabhadran , G . Saon , and A . Sethy , “Language modeling with highway LSTM , ” in Proc of ASRU 2017 , 2017 . [ 6 ] A . Stolcke and J . Droppo , “Comparing human and machine er - rors in conversational speech transcription , ” in Proc of INTER - SPEECH 2017 , 2017 , pp . 137 – 141 . [ 7 ] B . Li and K . C . Sim , “Comparison of discriminative input and output transformations for speaker adaptation in the hybrid NN / HMM systems , ” in Proc of INTERSPEECH 2010 , 2010 . [ 8 ] H . Liao , “Speaker adaptation of context dependent deep neural networks , ” in Acoustics , Speech and Signal Processing ( ICASSP ) , 2013 IEEE International Conference on . IEEE , 2013 , pp . 7947 – 7951 . [ 9 ] I . Himawan , P . Motlicek , M . F . Font , and S . Madikeri , “Towards utterance - based neural network adaptation in acoustic modeling , ” in Proc of ASRU 2015 . IEEE , 2015 , pp . 289 – 295 . [ 10 ] L . Lu , “Sequence training and adaptation of highway deep neu - ral networks , ” in Spoken Language Technology Workshop ( SLT ) , 2016 IEEE . IEEE , 2016 , pp . 461 – 466 . [ 11 ] N . Dehak , P . Kenny , R . Dehak , P . Dumouchel , and P . Ouellet , “Front - end factor analysis for speaker veriﬁcation , ” IEEE Trans . Audio , Speech and Lang . Process . , vol . 19 , no . 4 , pp . 788 – 797 , 2011 . [ 12 ] G . Saon , H . Soltau , D . Namahoo , and M . Picheny , “Speaker adap - tation of neural network acoustic models using i - vectors , ” in Proc of ASRU 2013 , 2013 , pp . 55 – 59 . [ 13 ] V . Gupta , P . Kenny , P . Ouellet , and T . Stafylakis , “I - vector - based speaker adaptation of deep neural networks for French broadcast audio transcription , ” in Proc of ICASSP 2014 , 2014 , pp . 6334 – 6338 . [ 14 ] Y . Miao , L . Jiang , H . Zhang , and F . Metze , “Towards speaker adaptive training of deep neural network acoustic models , ” in Proc of INTERSPEECH 2014 , 2014 . [ 15 ] G . Huang , Z . Liu , K . Q . Weinberger , and L . van der Maaten , “Densely connected convolutional networks , ” in arXiv : 1608 . 06993 , 2016 . [ 16 ] K . He , X . Zhang , S . Ren , and J . Sun , “Deep residual learning for image recognition , ” in Proc of CVPR 2016 , 2016 , pp . 770 – 778 . [ 17 ] R . K . Srivastava , K . Greff , and J . Schmidhuber , “Training very deep networks , ” in Proc of NIPS 2015 , 2015 , pp . 2377 – 2385 . [ 18 ] —— , “Highway networks , ” in Proc of ICML 2015 , 2015 . [ 19 ] Y . Zhang , G . Chen , D . Yu , K . Yao , S . Khudanpur , and J . Glass , “Highway long short - term memory RNNs for distant speech recognition , ” in Proc of ICASSP 2016 , 2016 , pp . 5755 – 5759 . [ 20 ] G . Pundak and T . Sainath , “Highway - LSTM and recurrent high - way networks for speech recognition , ” in Proc of INTERSPEECH 2017 , 2017 . [ 21 ] J . Kim , M . El - Khamy , and J . Lee , “Residual LSTM : Design of a deep recurrent architecture for distant speech recognition , ” in Proc of INTERSPEECH 2017 , 2017 . [ 22 ] L . Huang , J . Sun , J . Xu , and Y . Yang , “An improved residual LSTM architecture for acoustic modeling , ” in Proc of ICCCS 2017 , 2017 . [ 23 ] A . Krizhevsky and G . Hinton , “Learning multiple layers of fea - tures from tiny images , ” in Tech Report , 2009 . [ 24 ] S . Zagoruyko and N . Komodakis , “Wide residual networks , ” in arXiv : 1605 . 07146 , 2009 . [ 25 ] G . Cheng , V . Peddinti , D . Povey , V . Manohar , S . Khudanpur , and Y . Yan , “An exploration of dropout with LSTMs , ” in Proc of IN - TERSPEECH 2017 , 2017 . [ 26 ] D . Povey , A . Ghoshal , G . Boulianne , L . Burget , O . Glembek , N . Goel , M . Hannemann , P . Motlicek , Y . Qian , P . Schwarz , J . Silovsky , G . Stemmer , and K . Vesely , “The Kaldi speech recog - nition toolkit , ” in Proc of ASRU 2011 , 2011 . [ 27 ] D . Povey , V . Peddinti , G . Galvez , P . Ghahrmani , V . Manohar , X . Na , Y . Wang , and S . Khudanpur , “Purely sequence - trained neu - ral networks for asr based on lattice - free MMI , ” in Proc of INTER - SPEECH 2016 , 2016 . [ 28 ] M . J . F . Gales , “Maximum likelihood linear transformations for HMM - based speech recognition , ” Comp . Speech and Lang . , vol . 12 , pp . 75 – 98 , 1997 . [ 29 ] D . Povey , X . Zhang , and K . Sanjeev , “Parallel training of DNNs with natural gradient and paramter averaging , ” in Proc of ICLR 2015 , 2015 . [ 30 ] J . F . William , W . M . Fisher , A . F . Martin , M . A . Przybocki , and D . S . Pallett , “NIST evaluation of conversational speech recogni - tion over the telephone : English and mandarin performance re - sults , ” in NIST , 2000 . [ 31 ] A . Stolcke , “SRILM – An extensible language modeling toolkit , ” in Proc of ICSLP 2002 , 2002 , pp . 901 – 904 . [ 32 ] A . Rousseau , P . Del´eglise , and Y . Esteve , “TED - LIUM : an auto - matic speech recognition dedicated corpus . ” in LREC , 2012 , pp . 125 – 129 . [ 33 ] P . Thompson and H . Nesi , “Research in progress , the British Aca - demic Spoken English ( base ) corpus project . ” Language Teaching Research , vol . 5 , no . 3 , 2001 . [ 34 ] R . C . Simpson , S . L . Briggs , J . Ovens , and J . M . Swales , “The Michigan Corpus of Academic Spoken English . ” Ann Arbor , MI : The Regents of the University of Michigan , 2002 . [ 35 ] X . Chen , X . Liu , Y . Qian , M . J . F . Gales , and P . C . Woodland , “CUED - RNNLM - An open - source toolkit for efﬁcient training and evaluation of recurrent neural network language models , ” in Proc of ICASSP 2016 , 2016 , pp . 6000 – 6004 . [ 36 ] Y . Shi , W . Zhang , M . Cai , and J . Liu , “Variance regularization of RNNLM for speech recognition , ” in Proc of ICASSP 2014 , 2014 . [ 37 ] H . Xu , D . Povey , L . Mangu , and J . Zhu , “Minimum bayes risk decoding and system combination based on a recursion for edit distance , ” Comp . Speech and Lang . , vol . 4 , pp . 802 – 828 , 2011 . [ 38 ] J . Bergstra , R . Bardenet , Y . Bengio , and B . K ´ egl , “Algorithms for hyper - parameter optimization , ” in Proc of NIPS 2011 , 2011 .