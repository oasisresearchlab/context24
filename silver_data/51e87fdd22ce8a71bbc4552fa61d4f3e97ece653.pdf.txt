Skill Progression in Scratch Revisited J . Nathan Matias MIT Media Lab Cambridge , MA , 02139 jnmatias @ mit . edu Sayamindu Dasgupta MIT Media Lab Cambridge , MA , 02139 sayamindu @ media . mit . edu Benjamin Mako Hill University of Washington Seattle , WA , 98195 makohill @ uw . edu ABSTRACT This paper contributes to a growing body of work that attempts to measure informal learning online by revisit - ing two of the most surprising ﬁndings from a 2012 study on skill progression in Scratch by Scaﬃdi and Chambers : users tend to share decreasingly code - heavy projects over time ; and users’ projects trend toward using a less diverse range of code concepts . We revisit Scaﬃdi and Cham - bers’s work in three ways : with a replication of their study using the full population of projects from which they sampled , a simulation study that replicates both their analytic and sampling methodology , and an alter - native analysis that addresses several important threats . Our results suggest that the population estimates are opposite in sign to those presented in the original work . ACM Classiﬁcation Keywords H . 5 . 3 Information Interfaces and Presentation ( e . g . , HCI ) : Group and Organization Interfaces— Evalua - tion / methodology ; K . 3 . 2 Computers and Education : Com - puter and Information Science Education— Information systems education Author Keywords learning ; online communities ; computers and children ; creativity support tools ; replication INTRODUCTION In recent years , informal learning with digital tools has become increasingly prevalent in contexts including online communities , maker - spaces , and after - school computer clubs . One challenge in these informal learning envi - ronments is the measurement of learning outcomes , as there are rarely pre - speciﬁed learning goals and learning can occur in many ways . As these environments become increasingly widespread , it is important to not only for - mulate appropriate measures of learning but also validate and replicate ﬁndings . Moreover , within the larger sphere of human - computer interaction research , replication has been cited as important as it can help conﬁrm previous ﬁndings and lead to better methodologies and measures [ 16 ] . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for third - party components of this work must be honored . For all other uses , contact the Owner / Author . Copyright is held by the owner / author ( s ) . CHI’16 , May 07 - 12 , 2016 , San Jose , CA , USA ACM 978 - 1 - 4503 - 3362 - 7 / 16 / 05 . http : / / dx . doi . org / 10 . 1145 / 2858036 . 2858349 Figure 1 . Example Scratch code with six blocks in the ﬁrst “script” and three blocks in the second script . The ﬁrst script makes a sprite move back and forth 10 times . The second script makes a sprite play a sound and show a speech bubble upon being clicked . This paper re - examines part of the ﬁrst quantitative eval - uation of learning in the Scratch community , published in 2012 by Scaﬃdi and Chambers [ 14 ] . Scratch is a vi - sual , block - based programming language designed for children aged 8 – 15 , which young programmers have used to create a vast range of projects including animations , games , interactive stories , simulations , and science ex - periments [ 13 ] . In Scratch , programs are constructed by dragging and dropping visual blocks —similar to tokens in other programming languages—to deﬁne the behav - ior of on - screen graphical objects called sprites ( Figure 1 ) . The Scratch language is supported by a large on - line community launched in March 2007 and publicly announced in May 2007 [ 10 ] , where creators can share their projects , comment on each others’ work , and remix projects created by their peers . 1 In particular , we revisit two of Scaﬃdi and Chambers’s ﬁndings that ran counter to their hypotheses : that users tend to share decreasingly code - heavy projects over time ; and that users’ projects trend toward using a less diverse range of blocks . We revisit these ﬁndings as they are often cited in the literature on learning in informal online settings ( e . g . , [ 8 , 12 ] ) , because their study uses a relatively small sample of 2 , 195 Scratch projects shared by 250 users from a population where a full dataset has been made available , and because the original results are surprising in terms of other research . In particular , these results seem to run counter to earlier work on Scratch suggesting that young learners in a computer club broadened their block usage and project count over time even without teacher intervention [ 9 ] as well as several qualitative studies that suggested that Scratch users grew in their learning of 1 https : / / scratch . mit . edu programming concepts over time [ 4 , 11 , 2 ] . Drawing from the same population as Scaﬃdi and Chambers but using diﬀerent measures of learning , more recent work has also suggested that Scratch users tend to learn over time [ 17 , 5 ] . This paper attempts to explore this apparent contradic - tion in three ways . First , we replicate Scaﬃdi and Cham - bers’s relevant analyses with the full Scratch dataset . Second , we conduct a simulation study to estimate the chances of the earlier work’s ﬁnding given their sample size . Third , we oﬀer an alternative modeling approach that addresses important issues highlighted by our at - tempt at replication . By using the full dataset of 643 , 246 public non - remix projects shared by 138 , 321 users at approximately the time of data collection for the ear - lier work , we show that the actual relationships between these measures of skill and time are reversed in sign from Scaﬃdi and Chambers’s original ﬁndings . By simulating small samples using their methodology , we are able to reproduce Scaﬃdi and Chambers’s results in a way that suggests that the earlier ﬁndings are most likely the result of a small sample and an unlucky random draw . Finally , using the larger dataset , we are also able to present more detailed within - person estimates that support our new ﬁnding of a positive relationship . We argue that these new results help bring Scaﬃdi and Chambers’s study in line with other research on learning and skill progression in similar settings . SCAFFIDI AND CHAMBERS ( 2012 ) Scaﬃdi and Chambers provide a detailed analysis of learning in Scratch , developing and testing a series of models of users’ skill progression and levels of activity . In this paper , we focus on two of the paper’s ﬁndings that contradict the authors’ stated hypotheses . They ﬁnd that there is a negative relationship between the depth and breadth of projects shared by users on Scratch and the time since users created their accounts . The authors deﬁne depth as the “amount with which people used [ Scratch’s programming ] features , ” which they measure as the total number of programming blocks used in a project ( see Figure 1 ) , excluding any measure of graphical elements or sounds . Breadth is deﬁned as , “the range of diﬀerent features people could use , ” measured as the total number of distinct categories of programming blocks used within a project . For their categorization , Scaﬃdi and Chambers group Scratch’s 120 unique pro - gramming primitives into a set of 17 categories . Scaﬃdi and Chambers sampled 250 users from the Scratch website by using a “Surprise Me” feature that displayed a random published project . 2 The authors then collected a sample of projects from each user’s list of projects . Users’ projects on the Scratch website are displayed in ascending order of time created and are pagi - nated into groups of 15 . Scaﬃdi and Chambers’s dataset 2 Because a small proportion of users produce most projects , very active users will be overrepresented . included each user’s ﬁrst project and one project selected randomly from each subsequent page . For example , if a user published 35 projects , the sample would include three projects : their ﬁrst project , a project between 16 and 30 , and a project between 30 and 35 . Scaﬃdi and Chambers selected a total of 2 , 195 projects , of which 403 could not be analyzed due to version incompatibilities and errors . They also discarded one additional project from the remaining set as the creator of the project in question had simply copied a number of blocks repeatedly . In their study of project depth and breadth , they omitted remixed projects from their analysis . To motivate their analyses , Scaﬃdi and Chambers hypoth - esized that “online Scratch users would demonstrate an increase in sophistication over time , with rising breadth and depth demonstrated by Scratch animations . ” To test this hypothesis , they created a measure of expe - rience represented by the number of months that had elapsed since each project’s creator shared their ﬁrst project , rounded to the nearest month . They regressed this measure of creator experience on their measures of depth and breadth . Despite their expectations , they found a well - estimated negative relationship between depth and months ( β = − 1 . 51 , p < 0 . 01 ) and a similarly well - estimated negative relationship between breadth and months ( β = − 0 . 07 , p < 0 . 01 ) . Because learning happens within individuals , Scaﬃdi and Chambers also considered variation in a person - level version of their dataset . Among 145 users with projects shared in more than one month , they calculate the average depth and average breadth of a user’s projects in their ﬁrst and last months , testing the diﬀerence with two - tailed t - tests . In both cases , they found positive relationships between time on Scratch and breadth and depth , but a small sample size meant that these were poorly estimated . They could not reject the null hypothesis that there was no relationship . REPLICATION USING FULL DATASET Although there is some debate over terminology , we at - tempt a “replication” of Scaﬃdi and Chambers’s work as deﬁned by Bollen et al . [ 1 ] in that we use the original authors methods with new data . We deviate from Scaf - ﬁdi and Chambers’s methods in three ways : we include all Scratch projects instead of a sample ; we include all projects from every user instead of sub - sample ; and we include data on nearly every project , while Scaﬃdi and Chambers’s analytic software fails to parse 18 % due to errors and incompatibilities . In each sense , we include more data , increase internal validity , and work around limitations in the previous work . In all other ways , we at - tempt to ensure that our methods are identical to Scaﬃdi and Chambers’s . Our replication analysis began with the dataset of 1 , 925 , 054 public projects shared between March 2007 and April 2012 . Following Scaﬃdi and Chambers , we ﬁrst remove 506 , 072 projects that are remixes . Next , we remove 775 , 736 projects that were shared after July 1 , Depth Breadth ( Intercept ) 74 . 72 ∗∗∗ 2 . 68 ∗∗∗ ( 0 . 95 ) ( 0 . 00 ) months 2 . 57 ∗∗∗ 0 . 01 ∗∗∗ ( 0 . 17 ) ( 0 . 00 ) R 2 0 . 00 0 . 00 Adj . R 2 0 . 00 0 . 00 Num . obs . 643246 643246 ∗∗∗ p < 0 . 001 Table 1 . Regression models on measures of project depth and breadth . The unit of analysis is average month of user experience . 2010 , which appears to be near the date that Scaﬃdi and Chambers collected their dataset . 3 Our ﬁnal dataset includes 643 , 246 projects . Following Scaﬃdi and Chambers’s methodology , we calcu - lated measures of depth , breadth , and months of creator “experience” for every non - remix project in Scratch from March 2007 to July 2010 . Our linear models are presented in Table 1 , where the regression results are opposite in sign to those reported by Scaﬃdi and Chambers . We also estimate t - tests between users’ ﬁrst and last months and ﬁnd that , like Scaﬃdi and Chambers , our results are positive in sign . That said , because we use a much larger sample ( 23 , 092 users instead of 145 ) , we are easily able to reject the null hypothesis of no relationship for both depth ( ∆ µ = 39 ; t = 13 . 04 ) and breadth ( ∆ µ = 0 . 15 ; t = 8 . 24 ) . Because these analyses involve multiple projects from the same users , whose projects’ breadth and depth may be correlated over time , they violate regression’s assumption of independent observations . Although this error aﬀects Scaﬃdi and Chambers’s original analysis , it is aggravated in our analysis which includes all projects . We address this important threat in our alternative analysis in the section following the next . SIMULATION : REPLICATION USING SAMPLE −0 . 04 0 . 00 0 . 04 0 . 009 0 . 012 0 . 015 Standard Error E s t i m a t e SignificanceThreshold p < 0 . 01 p < 0 . 05 p > = 0 . 05 Figure 2 . Plot of estimate vs . standard error from simu - lating samples using the Scaﬃdi and Chambers method - ology . The fact that our regression results are opposite in sign to the well estimated results reported by Scaﬃdi and Chambers is surprising . One explanation may be due to one of the other major ﬁndings in Scaﬃdi and Chambers paper : a high “dropout” rate in the Scratch community , where most Scratch users share only a very small number of projects . Even though the “Suprise Me” feature over - samples on very active users , Scratch’s steep dropout rate means that only a small number of users contribute data for later months and that , as a result , the results are very sensitive to the particular users that are sampled . This is further aggravated by the small sample size which previous work has shown can lead to incorrect estimates [ 7 ] . To test this theory , we implemented the sampling technique used by Scaﬃdi and Chambers ( e . g . , sampling on random projects , choosing 250 unique users ; taking the ﬁrst project and then one randomly selected project from each subsequent “page” of 15 projects , etc ) . Be - cause Scratch is open source software , we reviewed the Scratch code from the time of Scaﬃdi’s paper to verify that our random project selection matches the “Surprise Me” functionality used in the original study . Using this methodology , we created 2 , 000 samples of projects from 250 users and estimated the linear regression models for each sample . A visualization of the resulting estimates for the size of standard errors versus estimates for breadth across all 2 , 000 simulated samples is shown in Figure 2 . We ﬁnd that in 70 % of our samples , there is no statistically signiﬁcant relationship at the conventional level of α = 0 . 05 . That said , in 13 . 5 % of the simulations , we estimate a positive association at the α = 0 . 01 level . In 3 . 8 % of the cases , we estimate a negative relationship at the same level . Results for depth are substantively similar but even less likely to result in estimates for which we can reject the null hypothesis of no relationship . In the models that show negative associations between user experience and both depth and breadth at below the α = 0 . 01 level reported by Scaﬃdi and Chambers , our parameter estimates are very similar to those reported by them . Although it occurs in less than 4 % of our simulated samples for breadth and less than 1 % for depth , we were able to reproduce the earlier work’s results . It seems likely that Scaﬃdi and Chambers’s surprising results are due to one of these atypical samples . ALTERNATIVE ANALYSIS : WITHIN - USER ESTIMATES With a much larger sample , we can also ﬁt types of mod - els that would require more statistical power than was available to Scaﬃdi and Chambers and that might take into account additional threats to validity . In particular , we use ﬁxed eﬀects multilevel models [ 15 ] to estimate variation in depth and breadth associated with changes within users’ experiences over time , addressing the is - sue of non - independent observations present in Scaﬃdi and Chambers’s analysis and in both sets of analyses 3 Sensitivity analyses showed similar results with or without remixes or projects created after the study by Scaﬃdi and Chambers . ln Depth Breadth ln Months Experience 0 . 02 ∗∗∗ 0 . 10 ∗∗∗ ( 0 . 00 ) ( 0 . 00 ) ln Comments Since Last Project 0 . 01 ∗∗∗ 0 . 01 ∗ ( 0 . 00 ) ( 0 . 00 ) R 2 0 . 0002 0 . 0011 Adj . R 2 0 . 0001 0 . 0008 Num . obs . 643246 643246 ∗∗∗ p < 0 . 001 , ∗ p < 0 . 05 Table 2 . Regression models for Scratch users’ depth and breadth . All models use user - level ﬁxed eﬀects and reﬂect within - user estimates . presented above . These models are equivalent to ﬁtting dummy variables for every user and control for every variable—observed or unobserved—that has a consistent eﬀect on the outcome across a user’s projects . In the case of depth , we also log - transform Scaﬃdi and Chambers’s depth measure to better meet parametric assumptions of the model . By adopting a regression framework instead of relying on t - tests , we can add additional controls . For example , we can control for the number of comments received since the last project—a variable shown to be an important predictor of learning in Scratch [ 5 ] . Results from ﬁxed eﬀects models estimated using the full population of non - remix projects are presented in Table 2 . In our breadth model we estimate that a 1 % increase in the amount of user experience is associated with a 0 . 02 % diﬀerence in the number of blocks . In our breadth model , a 1 % increase in the amount of the user’s experience is associated with a 0 . 001 unit diﬀerence in the number of categories used in a project , holding all else constant . Similar to Scaﬃdi and Chambers’s t - tests , these results suggest small , positive relationships between experience and measures of project depth and breadth . Goodness of ﬁt statistics suggest that neither model describes much of the within - user variation in breadth or depth , suggesting the need for better predictors and better measures . DISCUSSION Taken together , our replication , simulation , and within - user model oﬀer one explanation for the surprising contra - diction between ﬁndings by Scaﬃdi and Chambers and other research on learning in Scratch . Although Scaﬃdi and Chambers’s 2012 paper suggested that Scratch users tend to share decreasingly code - heavy projects over time and that users’ projects trend toward using a less diverse range of code concepts , our replication of their ﬁndings in the full dataset of Scratch users from 2007 to 2010 suggest that the actual relationship is reversed . 4 Our simulation results suggest that Scaﬃdi and Chambers drew an unlucky random sample . Finally , using within - user models , we found consistently positive relationships between experience and breadth and depth . One potential takeaway is the need for improved quan - titative measures of learning in Scratch . For example , 4 In results not reported here , we found that our pattern of results is similar using a complete dataset from 2007 to 2012 . subsequent work has oﬀered a category system for blocks similar to Scaﬃdi and Chambers’s measure of breadth , where Scratch blocks correspond to computational think - ing concepts ( e . g . loops , operators , and events ) [ 3 ] . Other work has analyzed when users adopt these individual concepts in their Scratch code as a way to measure learn - ing [ 5 ] . Scaﬃdi and Chambers’s measures each require continued performance and consider that a user is not progressing if their projects do not include more code and a wider variety of code over time . More recent work has used trajectory - based measures of the cumulative reper - toire of programming concepts [ 17 , 5 ] which we believe is a promising avenue for future research . Although Scaﬃdi and Chambers’s surprising results on depth and breadth seem to have been driven by a small dataset and an unlucky sample , their paper includes many other ﬁndings and detailed analyses that remain important for researchers of learning and programming in informal environments . Of particular importance is their description of a high dropout rate in the Scratch community . While high attrition is common to many on - line communities , it has serious implications for learning outcomes when , as Scaﬃdi and Chambers rightfully point out , very few users stick around over a signiﬁcant period of time . Whatever the modeled estimate of learning over time may be , many ( or most ) users do not participate long enough to receive any beneﬁts . We hope that our work helps demonstrate the value of replication in human - computer interaction research . The majority of replications in HCI have been described as unplanned replications of previous ﬁndings [ 6 ] . In this paper , we explicitly set out to conduct a replication , and our ﬁndings are opposite to those in the original study . For designers of informal learning communities such as Scratch , this positive evidence of skill progression repre - sents good news , provides an answer to the puzzle of Scaf - ﬁdi and Chambers’s surprising results , and brings their study into harmony with other research . Of course , our revised estimates only reinforce Scaﬃdi and Chambers’s parting argument calling for research to help decrease high levels of “attrition” among users of informal learning environments . When users’ skills progress over time , it is even more important that designers understand how to support and increase engagement . ACKNOWLEDGMENTS We would like to thank Christopher Scaﬃdi , who oﬀered graceful , valuable feedback on this paper . We would like to thank the Lifelong Kindergarten group at the MIT Media Lab for creating Scratch as well as the millions of Scratch users who create and participate on the Scratch website . We would also like to acknowledge Mitchel Resnick , Andr´es Monroy Hern´andez , and our anonymous reviewers for their thoughtful feedback . Financial support for this work came from the National Science Foundation ( grants DRL - 1417663 and DRL - 1417952 ) . REFERENCES 1 . Kenneth Bollen , John T . Cacioppo , Robert M . Kaplan , Jon A . Krosnick , James L . Olds , and Heather Dean . 2015 . Social , Behavioral , and Economic Sciences Perspectives on Robust and Reliable Science . Technical Report . National Science Foundation . http : / / www . nsf . gov / sbe / AC _ Materials / SBE _ Robust _ and _ Reliable _ Research _ Report . pdf 2 . Karen Brennan . 2013 . Best of both worlds : Issues of structure and agency in computational creation , in and out of school . Ph . D . Dissertation . Massachusetts Institute of Technology . http : / / dspace . mit . edu / handle / 1721 . 1 / 79157 3 . Karen Brennan and Mitchel Resnick . 2012 . New frameworks for studying and assessing the development of computational thinking . In Proceedings of the 2012 annual meeting of the American Educational Research Association , Vancouver , Canada . http : / / scratched . gse . harvard . edu / ct / files / AERA2012 . pdf 4 . Aniket Dahotre , Yan Zhang , and Christopher Scaﬃdi . 2010 . A Qualitative Study of Animation Programming in the Wild . In Proceedings of the 2010 ACM - IEEE International Symposium on Empirical Software Engineering and Measurement ( ESEM ’10 ) . ACM , New York , NY , USA , 29 : 1 – 29 : 10 . DOI : http : / / dx . doi . org / 10 . 1145 / 1852786 . 1852825 5 . Sayamindu Dasgupta , William Hale , Andr´es Monroy - Hern´andez , and Benjamin Mako Hill . 2016 ( forthcoming ) . Remixing as a Pathway to Computational Thinking . In Proceedings of the 19th Conference on Computer Supported Cooperative Work & Social Computing ( CSCW ’16 ) . ACM . 6 . Kasper Hornbæk , Søren S . Sander , Javier Andr´es Bargas - Avila , and Jakob Grue Simonsen . 2014 . Is Once Enough ? : On the Extent and Content of Replications in Human - computer Interaction . In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ( CHI ’14 ) . ACM , New York , NY , USA , 3523 – 3532 . DOI : http : / / dx . doi . org / 10 . 1145 / 2556288 . 2557004 7 . John P . A . Ioannidis . 2005 . Why Most Published Research Findings Are False . PLoS Med 2 , 8 ( 2005 ) , e124 . DOI : http : / / dx . doi . org / 10 . 1371 / journal . pmed . 0020124 8 . Jay Lemke , Robert Lecusay , Michael Cole , and Vera Michalchik . 2015 . Documenting and assessing learning in informal and media - rich environments . MIT Press . 9 . John H Maloney , Kylie Peppler , Yasmin Kafai , Mitchel Resnick , and Natalie Rusk . 2008 . Programming by choice : urban youth learning programming with Scratch . ACM SIGCSE Bulletin 40 , 1 ( 2008 ) , 367 – 371 . DOI : http : / / dx . doi . org / 10 . 1145 / 1352135 . 1352260 10 . Andr´es Monroy - Hern´andez . 2007 . ScratchR : sharing user - generated programmable media . In Proceedings of the 6th international conference on Interaction design and children ( IDC ’07 ) . ACM , New York , NY , USA , 167 – 168 . DOI : http : / / dx . doi . org / 10 . 1145 / 1297277 . 1297315 11 . Kylie A . Peppler and Mark Warschauer . 2011 . Uncovering Literacies , Disrupting Stereotypes : Examining the ( Dis ) Abilities of a Child Learning to Computer Program and Read . International Journal of Learning and Media 3 , 3 ( 2011 ) , 15 – 41 . DOI : http : / / dx . doi . org / 10 . 1162 / IJLM _ a _ 00073 12 . Alexander Repenning , David C . Webb , Kyu Han Koh , Hilarie Nickerson , Susan B . Miller , Catharine Brand , Ian Her Many Horses , Ashok Basawapatna , Fred Gluck , Ryan Grover , Kris Gutierrez , and Nadia Repenning . 2015 . Scalable Game Design : A Strategy to Bring Systemic Computer Science Education to Schools Through Game Design and Simulation Creation . Trans . Comput . Educ . 15 , 2 ( April 2015 ) , 11 : 1 – 11 : 31 . DOI : http : / / dx . doi . org / 10 . 1145 / 2700517 13 . Mitchel Resnick , John Maloney , Andr´es Monroy - Hern´andez , Natalie Rusk , Evelyn Eastmond , Karen Brennan , Amon Millner , Eric Rosenbaum , Jay Silver , Brian Silverman , and Yasmin Kafai . 2009 . Scratch : programming for all . Commun . ACM 52 , 11 ( 2009 ) , 60 – 67 . DOI : http : / / dx . doi . org / 10 . 1145 / 1592761 . 1592779 14 . Christopher Scaﬃdi and Christopher Chambers . 2012 . Skill Progression Demonstrated by Users in the Scratch Animation Environment . International Journal of Human - Computer Interaction 28 , 6 ( June 2012 ) , 383 – 398 . DOI : http : / / dx . doi . org / 10 . 1080 / 10447318 . 2011 . 595621 15 . Judith D . Singer and John B . Willett . 2003 . Applied Longitudinal Data Analysis : Modeling Change and Event Occurrence ( 1 ed . ) . Oxford University Press , USA . 16 . Max L . Wilson , Wendy Mackay , Ed Chi , Michael Bernstein , Dan Russell , and Harold Thimbleby . 2011 . RepliCHI - CHI Should Be Replicating and Validating Results More : Discuss . In CHI ’11 Extended Abstracts on Human Factors in Computing Systems ( CHI EA ’11 ) . ACM , New York , NY , USA , 463 – 466 . DOI : http : / / dx . doi . org / 10 . 1145 / 1979742 . 1979491 17 . Seungwon Yang , Carlotta Domeniconi , Matt Revelle , Mack Sweeney , Ben U . Gelman , Chris Beckley , and Aditya Johri . 2015 . Uncovering Trajectories of Informal Learning in Large Online Communities Of Creators . In Proceedings of the Second ( 2015 ) ACM Conference on Learning @ Scale . ACM , 131 – 140 . DOI : http : / / dx . doi . org / 10 . 1145 / 2724660 . 2724674