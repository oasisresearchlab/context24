Make It Make Sense ! Understanding and Facilitating Sensemaking in Computational Notebooks SOUTI CHATTOPADHYAY , University of Southern California , USA ZIXUAN FENG ∗ , Oregon State University , USA EMILY ARTEAGA , Oregon State University , USA AUDREY AU , Oregon State University , USA GONZALO RAMOS , Microsoft Research , USA TITUS BARIK , Microsoft , USA ANITA SARMA , Oregon State University , USA Reusing and making sense of other scientists’ computational notebooks . However , making sense of existing notebooks is a struggle , as these reference notebooks are often exploratory , have messy structures , include multiple alternatives , and have little explanation . To help mitigate these issues , we developed a catalog of cognitive tasks associated with the sensemaking process . Utilizing this catalog , we introduce Porpoise : an interactive overlay on computational notebooks . Porpoise integrates computational notebook features with digital design , grouping cells into labeled sections that can be expanded , collapsed , or annotated for improved sensemaking . We investigated data scientists’ needs with unfamiliar computational notebooks and investigated the impact of Porpoise adaptations on their comprehension process . Our counterbalanced study with 24 data scientists found Porpoise enhanced code comprehension , making the experience more akin to reading a book , with one participant describing it as It’s really like reading a book . CCS Concepts : • Human - centered computing → Human computer interaction ( HCI ) . Additional Key Words and Phrases : Design Probe , Jupyter notebook ACM Reference Format : Souti Chattopadhyay , Zixuan Feng , Emily Arteaga , Audrey Au , Gonzalo Ramos , Titus Barik , and Anita Sarma . 2018 . Make It Make Sense ! Understanding and Facilitating Sensemaking in Computational Notebooks . ACM Trans . Comput . - Hum . Interact . 37 , 4 ( August 2018 ) , 26 pages . https : / / doi . org / XXXXXXX . XXXXXXX 1 INTRODUCTION Sharing knowledge is an inevitable part of day - to - day organizational life ; building software and information artifacts of substantial size requires the expertise and effort of multiple people [ 1 , 2 ] . In addition to creating computational notebooks , data scientists also write and disseminate their analytical artifacts among a wide range of stakeholders ∗ The second author contributed equally to this work as the first author . Authors’ addresses : Souti Chattopadhyay , schattop @ usc . edu , University of Southern California , California , USA ; Zixuan Feng , fengzi @ oregonstate . edu , Oregon State University , Oregon , USA ; Emily Arteaga , arteagae @ oregonstate . edu , Oregon State University , Oregon , USA ; Audrey Au , auau @ oregonstate . edu , Oregon State University , Oregon , USA ; Gonzalo Ramos , goramos @ microsoft . com , Microsoft Research , Washington , USA ; Titus Barik , tbarik @ microsoft . com , Microsoft , California , USA ; Anita Sarma , anita . sarma @ oregonstate . edu , Oregon State University , Oregon , USA . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee . Request permissions from permissions @ acm . org . © 2018 Association for Computing Machinery . Manuscript submitted to ACM Manuscript submitted to ACM 1 a r X i v : 2312 . 11431v1 [ c s . H C ] 18 D ec 2023 2 Chattopadhyay , et al . such as developers , program managers , and marketing specialists [ 3 – 6 ] . However , making sense of these artifacts , like computational notebooks , requires significant effort to comprehend the code and intentions behind those lines of code . Data scientists frequently engage in exploratory programming , mixing and matching different solutions from their experiments or adopting approaches from other scientists’ work , predominantly utilizing computational notebooks as their tool of choice [ 7 – 12 ] . These notebooks are particularly well - suited for exploratory processes , as they adhere to the principles of literate programming , aiming to enhance code comprehension by effectively incorporating the programmer’s commentary within the code itself [ 13 ] . Notebooks allow data scientists to compile code cells , their commentary ( as markdown text ) , and the output of these cells into a single , cohesive document [ 14 , 15 ] . In an ideal situation where notebooks are simple , well - structured , and documented , data scientists can quickly under - stand the code in unfamiliar notebooks . But the reality is far different . Notebooks are complex ( lengthy with complicated , custom - made functions ) [ 16 ] , messy ( multiple additional analyses co - exist , some obsolete , some erroneous ) [ 7 , 10 ] , and confusing ( code cells don’t have documentation or obvious cues about the purpose of the code ) [ 17 , 18 ] . This makes it difficult for data scientists reviewing unfamiliar notebooks to understand the purpose . To bridge this gap , we investigate the sensemaking process and identify how to support sensemaking steps in computational notebooks . Leveraging the affordances that assist sensemaking in digital media , such as those found in e - books , we developed a design probe , Porpoise . This tool forms an interactive overlay on computational notebooks ( for example , Jupyter Notebook ) , enhancing the user’s ability to navigate and comprehend the notebook by providing a structured and easy - to - follow layout , much like the chapter and section divisions in e - books . It achieves this by drawing out and spotlighting comments on the underlying rationale of the code , thus offering users a more straightforward , more coherent narrative of the computational processes involved . We structured our research around these two questions : • RQ1 : How can we design computational notebooks to help sensemaking ? • RQ2 : How helpful are computational notebooks that support sensemaking ? Answering research questions , we followed a systematic approach as shown in Figure 1 . We began by reviewing the literature on the elements of sensemaking across psychology , computer science , and data science . We identified three common elements : comprehension [ 19 – 29 ] , mental modelling [ 30 , 30 – 37 ] , and situational awareness [ 21 , 24 , 38 – 40 , 40 ] . We then developed design features to enhance sensemaking in data science notebooks using Porpoise in Section 3 . We then operationalized our design features for Porpoise as an interactive overlay for Jupyter notebooks in Section 4 . To evaluate the effectiveness of Porpoise and how it could address data scientists’ sensemaking needs , we conducted a within - subject counterbalanced observation study with 24 data science practitioners . Our results showed that participants using Porpoise were enthusiastic about the affordances Porpoise provides to the navigation and sensemaking of notebooks . Porpoise provided a gradual information overlay that allowed them to progressively build insights and inferences as they made sense of the notebook . Moreover , participants found that Porpoise helped them find interesting and important areas in the notebook , share insights , and receive input from others . Our findings could inform future researchers and tool builders about the underlying cognitive tasks of sensemaking to inspire their design choices for future implementations of notebooks . 2 SENSEMAKING IN DATA SCIENCE 2 . 1 Literature Review about Sensemaking To support data scientists in their cognitive tasks within computational notebooks , it is essential to understand their needs . Our approach began with a review of related literature on sensemaking in psychology . Then , we expanded our Manuscript submitted to ACM Make It Make Sense ! 3 Transcripts & analysis Understand Sense - Making in Data Science ( Section 2 ) Redesigning Computational Notebooks ( Section 3 & 4 ) Evaluating Design Features of Sense - making ( Section 5 ) Review 42 related Literature In psychology and computer science Revisiting Literate Programming Paradigm Design motivations Within - subject counterbalanced observation study Enhancing Sense - Making Capabilities in Notebooks through Redesigned Features Operationalizing design features via design probe “Porpoise” - - - to parse the notebook’s “purpose Counter balance study  n = 24 Identified sense - making elements and tasks through open coding and negotiated agreement Porpoise Improves Notebook Sense - Making and Note - Taking for Data Scientists Fig . 1 . Study Overview . We study sensemaking literature across domains in Section 2 to form design motivations for our features in Section 3 and build Porpoise in Section 4 . Finally , we evaluate Porpoise in Section 5 using a within - subject study with 24 particpipants scope to include the terms : “Computer Science” and “Data Science” , to provide a more comprehensive understanding of sensemaking in these scopes . Additionally , we reviewed digital books and media content to investigate how their features enhance the comprehensibility of books and media content . We conducted a pilot search on Google Scholar by which we found relevant terms used in sensemaking , particularly in psychology . The final list of search keywords included “sensemaking , ” “sense making” , “sensemaking in data science” , “sensemaking in computer science” , “comprehending activities” , and “sensemaking psychology” , yielded 42 articles . After identifying relevant literature by the initial screening process , we selected 15 papers that discussed sensemaking in the abstract . Then , the first and second authors followed the guidelines recommended by Keele et al . [ 41 ] and performed a single iteration backward snowballing [ 42 ] to identify additional research on the topic . This led us to six additional studies related to education and computer science literature that provided detailed insights into the elements of sensemaking for computer science and data science , bringing the total number of papers in our final list to 21 . The first two authors independently read and analyzed the 21 papers following the open coding protocol [ 43 ] . We held weekly meetings to present our findings and discuss them until we reached an agreement . After the meetings , we agreed on classifying sensemaking into three main categories , each encompassing the elements of sensemaking , their definitions , and the cognitive tasks associated with each element . Through this analysis , we comprehensively understood the elements of sensemaking , their definitions , and the associated cognitive tasks . Sensemaking is an integral part of any creative process [ 44 ] . It is a complex activity that requires finding information , mapping the meaning to the larger context , and creating interpretations [ 45 ] . When complexity in collaborative Manuscript submitted to ACM 4 Chattopadhyay , et al . Understanding Behavior Comparing Information Broadening Knowledge Mental modelling Comprehension E1 Comprehension includes reading symbols and associating meaning to the symbols based on our existing knowledge to identify patterns and behavior of systems , compare information , and broaden knowledge and understanding . Mental models are internal memory representations depicting states of events related to ideas , declarative and procedural knowledge , and inferences drawn linked to or expressed in terms of concepts , principles , and knowledge . If no suitable schema is available , people might create mental models to generate descriptions of system purpose and form explanations of system functioning . E2 C1 C2 C3 Understanding Structure Comparing Information Mapping / Relating Elements and Concepts C4 C5 Contextualinferences Contextual inferences is a person’s perception of important information items in their present work environment and capacity to project the future condition of these elements . E3 Inferring hypothesis C6 C7 Inferring elements COGNITIVE TASKS : Fig . 2 . Defining Sensemaking and Its Related Cognitive Tasks . The sensemaking process has three elements : E1 . Comprehension , E2 . Mental Modelling , and E3 . contextualizing inferences . Each element is associated with a cognitive task listed below . environments increases , sensemaking develops an artifact or solution [ 46 ] . Collaboration then involves iterative sensemaking of another person’s program and adding knowledge this [ 47 ] . The process of sensemaking is a widely - discussed psychological perspective encompassing various elements . While some researchers associate sensemaking with comprehension alone [ 24 , 48 ] , others argue that it also entails mapping comprehended information to existing knowledge [ 39 ] . The sensemaking process involves integrating conjectured information with known data , linking an individual’s inferences to their observations , elucidating ambiguous data , diagnosing uncertain symptoms , and pinpointing problems [ 39 ] . In line with this , Pirolli and Card [ 49 ] emphasizes that the general sensemaking process includes gathering information , building a schema , and creating insight . These aspects align with the three elements discussed in Figure 2 . Sensemaking in the context of programming has been studied , but its application and implications for data scientists are still poorly understood . In the early 2000s , Pirolli and Card [ 49 ] investigated the meaning of sensemaking in programming and found that programmers followed a sensemaking loop when building programs . Researchers have observed that programmers constructed narratives when attempting to comprehend someone else’s code [ 50 ] , while frequently writing disorganized and ad - hoc code during exploration [ 9 ] . The computational notebook environ - ment doesn’t mandate a specific structure , and making code comments or Markdown explanations was optional and uncommon among data scientists [ 7 ] . A typical data science workflow , as described by Kandel et al . [ 51 ] , involves context - switching between raw data , wrangling tools , and visualization tools , suggesting an “ideal” solution would integrate these workflows into a single tool . Manuscript submitted to ACM Make It Make Sense ! 5 E1 . Comprehension Different definitions of the three sensemaking elements include different cognitive tasks . For instance , understanding behavior [ 19 – 22 ] , comparing information [ 23 , 24 ] , and expanding knowledge [ 25 – 29 ] are low - level cognition - related tasks associated with the process of comprehension . Comprehension refers to an individual’s awareness of facts important to the task and the broader purpose [ 23 ] , which includes reasoning , pattern recognition , and information comparison [ 24 ] . Combining these includes reading symbols and associating meaning to the symbols based on our existing knowledge to identify patterns and behavior of systems , compare information , and broaden knowledge and understanding . E2 . Mental models are internal memory representations that depict states of events related to ideas , declarative and procedural knowledge [ 30 – 33 ] , and inferences drawn connected to or expressed in terms of concepts , principles , and knowledge [ 30 , 34 – 37 ] . When no suitable schema is available , individuals might create mental models to generate descriptions of system purposes and formulate explanations of system functioning . E3 . Situational awareness is defined as a person’s perception of essential information items in their current work environment and their ability to project the future condition of these elements . The sensemaking process for information and structure involves generating hypotheses to explain discrepancies , searching for evidence to establish , support , or refute the hypotheses [ 38 – 40 ] , and seeking a structure to integrate all discovered information [ 21 , 24 , 40 ] . 2 . 2 Revisiting Literate Programming Paradigm To design a computational notebook that supports this process and enhances data scientists’ ability to engage in sense - making , we must incorporate additional features that specifically support the cognitive tasks underlying sensemaking . We then drew inspiration from the original literate programming paradigm behind computational notebooks [ 13 ] , which suggested that programs written as literary works are more understandable by humans . We also investigated features used in digital literature to make books and media more comprehensible . By combining insights from psychology and computer science , we developed a scaffolded notebook that provides users with a more structured and intuitive interface to empower them to engage in more effective sensemaking . Literate programming : In the early 1980s , Knuth [ 13 ] proposed a literate programming paradigm to make pro - gramming more explainable to human beings by writing it as a flow of thought rather than instructions to a computer . The earlier versions of literate programming interweaved code with programmers’ explanations to create printable documents [ 52 ] . Recently , computational notebook environments have adapted literate programming , e . g . , Sage Note - books [ 53 ] , Walter [ 54 ] , Jupyter [ 55 ] . A computational notebook can have " cells " that include code , output , table , visualizations , etc . Data scientists can manually interweave their explanations in markdown cells [ 7 ] . Such computa - tional notebooks were intended to help create and share computational narratives . Applying literate programming : However , data analysis is iterative and exploratory [ 10 ] , which creates messy computational notebooks with many erroneous code cells and throwaways [ 15 , 16 , 56 ] . Data scientists don’t want to manually add explanations to these exploratory notebooks as there is no assurance the analysis will be used [ 10 ] , which causes notebooks to stray away from the literate programming paradigm . Porpoise bridges this gap by grouping code cells logically and automatically annotating the groups with a description of the code’s purpose . 3 DESIGNING COMPUTATIONAL NOTEBOOKS TO SUPPORT SENSEMAKING To enhance data scientists’ ability to engage in sensemaking within computational notebooks , we aim to re - design the features that support these cognitive tasks for data scientists . In this section , we seek to the research question : RQ1 : How can we design computational notebooks to help sensemaking ? Manuscript submitted to ACM 6 Chattopadhyay , et al . 3 . 1 Mapping features from Digital literature into Computational Notebooks We aim to build a computational notebook design with simple affordances that blend how people interact with digital and computational books . These features were proposed by the author’s experiences with computational notebooks and HCI design , insights from prior research reporting data scientists’ challenges with notebooks [ 11 ] , and affordances found in related systems or books . These adaptations of a set of simple affordances : • A Navigation panel : Provides an overview of the notebook’s contents and structure through a side panel . • B Annotation : Enables data scientists to create annotations by highlighting text . • C Export : Allows users to create a snapshot of expanded chapters / sections by exporting a document . • D Chapter title and icon : provide an introductory overview of the chapter and its sections . • E Section title : As section headers that can be expanded to reveal their contents . Table 1 . Mapping Design Features and Cognitive Tasks in Sensemaking Cognitive tasks Deisgn Features Understanding Behavior ( C1 ) A B C D E Comparing Information ( C2 ) A D E Broadening Knowledge ( C3 ) D E Understanding Structure ( C4 ) A B C D E Mapping / Relating Elements and Concepts ( C5 ) A D E Inferring hypothesis ( C6 ) A D E Inferring elements ( C7 ) A B D E A : Navigation panel ; B : Annotation ; C : Export ; D : Chapter title and icon ; E : Section title ; Table 1 shows the relationship between cognitive tasks and design features . The table highlights seven cognitive tasks ( C1 to C7 ) and their corresponding design features , marked as A through E . Here , we present how we designed each feature based on existing literature and our experiences : A Navigation panel . A common feature of digitized books is the table of contents , typically on the left side of the main text area [ 57 ] . Visual representations of program segments rather than textual ones resulted in more quick and accurate program understanding [ 58 ] . A data science project’s lifetime may be divided into many stages , such as data acquisition , data readiness , etc [ 59 ] . Stakeholders and data scientists prefer presentation decks that are easily understood and visually represent high - level insights . [ 60 ] . Because of these findings , we chose to concentrate on the navigational structure as one of the primary design affordances of Porpoise . The content table provides a book’s entire structure and layout at a glance , making it easy to get an overview that helps place the contents in context [ 61 ] . A common configuration of this table is showing the chapters’ ( or sections ) titles and the associated page numbers [ 62 ] . Indexes , or page numbers , indicate the amount of information contained in each chapter [ 63 ] . To provide an overview of a notebook’s contents , we added a side panel displaying its major sections . The left panel of the notebook shows the chapter title and code cells included in the chapter . For example , in Figure 3 , the chapter navigation panel A shows the chapter “Exploratory Visualization” runs from cell 1 through 14 , while the last chapter , Ensemble Methods include cell 154 , and cells 165 and 166 . B Ability to take notes in notebook . Taking notes while reading a physical book , online , or PDF articles is a very common activity supported by most digital media forms . It allows readers to go through the documents to commit Manuscript submitted to ACM Make It Make Sense ! 7 http : / / localhost : 8888 / lab JupyterLab D Generic Modelling Model Selection - Feedback Based Model Evaluation # Load data  net CharRNN ( chars , n _ hidden , n _ layers ) = = = 256 3 [ 50 ] : # train plt . figure ( figsize ( , ) )  train ( net , data , epochs , n _ seqs , n _ steps , lr , device device , val _ frac ,  name , plot )  = = = = = = = = = 12 4500 768 512 0 . 001 0 . 1 True ' ml - train - data ' [ 51 ] : 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 0 25 50 75 100 125 150 175 Validation loss does not decrease further , stopping training . CHAPTER 04 - MODELING & EVALUATION  E Cancel  Save  B B Comments . . .  Code Python 3 File Edit View Run Kernel Tabs Settings Help ds . ipynb 50 - 51 Filter files by name Exploratory Visualization Data Cleaning Feature Engineering Modeling & Evaluation Ensemble Methods 1 - 14 14 - 23 24 - 49 154 165 - 66 Use the cursor to select the text you would like to annotate and the tool palette will appear Annotation Export / Import Save expanded cells / demo / Chapters Flags Cell Number Export In this chapter , the data scientists define cross - validation strategies and choose the models for training.  A B C Need further tuning Fig . 3 . Porpoise is an overlay interaction for computational notebooks that groups adjacent code cells automatically and conveys their purpose through five main interactive affordances . information to memory and remind themselves of past thoughts without editing the information [ 64 ] . While the most common way of allowing annotations is through comments and memos ( also known as sticky ) notes [ 65 ] , different characteristics of these annotation functions offer specific support . For instance , using color and highlighting in the text to improve reader comprehension by increasing reader engagement [ 66 ] . Annotations should also prevent covering over the raw data or parts of the text [ 63 ] and should be able to contain information in a compact and flexible format [ 65 ] . We adapted these findings to design the annotation feature as a compact , inexpensive , rectangular , lightweight , flexible , and intuitive media for recording words , thoughts , drawings , and models [ 65 , 67 ] data scientists wish to use . At the same time , they go through unfamiliar notebooks to remind themselves of future references without having to edit the information [ 64 ] . The annotations feature , marked as B in Figure 3 , shows the characteristics of the annotation features . Data scientists can highlight any part of the notebook to annotate , which pops up a widget to write their thoughts . To enable data scientists to post comments and highlight text with different colors , we included various color options for the highlight so that users may use multiple colors to emphasize different contexts for different purposes . Manuscript submitted to ACM 8 Chattopadhyay , et al . To prevent covering over the raw data by annotations such as sticky notes , we introduced the functionality that the annotated comments will only appear if the user hovers their cursor over the highlighted portion of the data [ 63 ] . C Improving shareability of computational narratives of choice . At the end of reading and making sense of a book and article , readers share or present their knowledge [ 49 ] . Sharing relevant information and the explanation of the information after sensemaking with coworkers improves the productivity of the team . Readers share digital media but email or host articles through the web . However , data scientists struggle with sharing notebooks , or parts of the notebook , easily with co - workers [ 11 ] , as it involves replicating packages , importing data and libraries , managing dependencies , reading and understanding someone’s explanations of the code or responding to them . Additionally , computational notebooks have multiple narratives depending on the ordering and inclusion of cells . To address these issues , we adapted the export feature marked as C in Figure 3 . The export feature exports only the parts of the notebook that are expanded and the contents within the expanded parts ( including any annotations ) . D E Chapters , sections , and icons to provide structural and logical understanding . To implement this navigation panel , we also need to identify within notebooks the set of structural features found in books and articles : chapters , sections and subsections . The information about the hierarchy of logical components , such as titles , abstracts , and sections , along with the physical components like pages , paragraphs , tables , figures , is useful in finding relevant information within textbook [ 68 , 69 ] and conveys the underlying logical structure of the data in the article or book [ 70 ] . We adapted the notebook interface to elucidate similar logical and physical components . Figure 3 , chapter D consists of the number , title , and a short description of the contents of the chapter akin to the introductory paragraph of a book or abstract of an article [ 71 ] . The sections within each chapter , marked as Figure 3 E , appear as highlighted bars containing explanations of the behavior of a group of code cells . These bars are interactive , expanding to show code , output , and comments when clicked once and collapsing when clicked twice . Similar to how physical components of books are defined as words , tables , figures [ 69 ] , we identified the components of computational notebooks are data , libraries , visualizations , tables , models , and author notes . We displayed this information through a set of six icons that explain what the icon stands for when hovered over . This creates an affordance to forage for the relevant information , for instance , when a data scientist is looking for how specific models are defined by identifying which parts of the notebook have those components from the navigation panel and section headers . The combined ability to navigate around sections and chapters along with getting high - level structural information is shown to aid comprehension in digital media [ 72 ] . This structured overview can help data scientists create a mental map of the knowledge contained in the notebook [ 73 ] . It also eliminates the need to scroll for a long time , which is associated with a sharp decrease in attention [ 74 ] and causes readers to lose their place in the document . 3 . 2 Operationalizing Design Features To implement the design features introduced above from A to E , we need to identify chapters and sections in notebooks for the navigation panel A , the chapters D , and the sections E . Therefore , before implementing these features , we must conduct additional analysis to define and automatically identify the chapters and sections in notebooks . However , it should be noted that implementing B and E is straightforward regarding user interaction design and thus doesn’t require this additional analysis . We defined the chapters to indicate broader sections of the analysis similar to steps in machine learning workflow like : “Data Processing” and “Modelling” . The sections indicated the purpose of a group of consecutive ( or one ) code cells . However , how can we define and extract chapters and sections ? How do data scientists structure their analyses into Manuscript submitted to ACM Make It Make Sense ! 9 sections similar to chapters and headers ? To answer these questions , we took a bottom - up approach by qualitatively analyzing existing notebooks from Open Source such as Jupyter Notebooks and Google Colaboratory . Qualitative analysis to identify notebook functionality . We selected 100 notebooks hosted on GitHub for competitions in the Kaggle platform , the world’s largest data science community [ 75 ] . These notebooks were related to domains like climate , sports , finance , traffic , and viruses with executed output . After removing incomplete notebooks or notebooks that are drastically different from previously studied practitioner’s notebooks [ 15 ] , we arrived at a set of 46 notebooks , of which two were not executable . Finally , we analyzed 44 notebooks from our set to understand how to define and extract chapters and sections . We will describe this analysis in the paragraphs below . Since chapters and sections are designed to capture the purpose of the code , we looked at the difference in functionality across the 44 notebooks . Two researchers classified every line of code ( LOC ) or function call in nine random notebooks ( 20 % of 44 rounded up ) based on their functionality following an open inductive coding approach . For example , 𝑝𝑎𝑛𝑑𝑎𝑠 . 𝑟𝑒𝑎𝑑 _ 𝑐𝑠𝑣 ( ) is a function call that loads data and categorizes it as a ‘Load ( L2 ) ’ functionality . Another example is 𝑡 . 𝑡𝑒𝑠𝑡 ( ) , which we categorized as the ‘Statistical Test ( ST4 ) ’ functionality . After the first round of inductive coding [ 76 ] , 39 such categories emerged . These categories naturally fell into seven broader categories : Load ( L ) , Domain Specific Functions ( S ) , Pre - Processing ( PP ) , Visualization ( V ) , Machine Learning ( ML ) , Statistics ( ST ) , and Others ( O ) . The two researchers independently coded the LOCs / function calls across 35 notebooks . Across all notebooks , we categorized 605 different function calls . The researchers then met to negotiate their disagreements [ 77 ] , which resulted in re - assigning categories to some functions and restructuring six categorizations ( L3 −→ PP5 , S3 −→ L3 , S4 −→ S3 , S5 −→ S4 , S6 −→ S5 , S0 −→ PP0 ) and removing the ‘Other’ category as it wasn’t indicating any meaningful grouping of functionality . The final categories of all observed functions are listed in Appendix ( Section 8 . 1 ) , and further definitions of each category and function are provided in the supplementary materials [ 78 ] . Encoding notebook functionality . We automated the transformation of each notebook into an encoded version using a mapping of function calls to categories . For each line of code , we extracted the sequence of categories ( 𝐻 1 ) based on their linear order . Then , we looked at permutations and nested functions , for instance , 𝑛𝑝 . 𝑚𝑒𝑎𝑛 ( 𝑎𝑐𝑐𝑢𝑟𝑎𝑐𝑦 ( 𝑦 𝑡𝑒𝑠𝑡 , 𝑝𝑟𝑒𝑑 ) performs a summary ( ST1 ) on the output of a machine learning model ( ML4 ) . We defined the rule to infer processes from nested calls in innermost - first order ( 𝐻 2 ) . We also looked at cases where custom functions were defined in the notebook and set the rule to get the sequence of processes at the point where they are called ( 𝐻 3 ) . We can transform notebook code into a list of categories using these three heuristics ( 𝐻 1 - 𝐻 3 ) . To uncover meaning patterns within the encoding , we must address questions like pattern initiation points and how to deal with overlapping lines of code . To facilitate identifying patterns in encoded notebooks , we defined the following heuristics that provide structural definitions to patterns . Loading data ( category L ) in any form indicates the start of a part of code written for a specific purpose ( 𝐻 4 ) , and getting any form of output like visualizations ( V ) or of a model ( ML4 ) indicates the end of a purpose ( 𝐻 5 ) . However , if a sub - sequence of processes repeats multiple times , they should be reviewed to see if the pattern is meaningful ( 𝐻 6 ) . In some cases , we found meaningful purpose in repeated processes . For example , notebooks with multiple ‘model and verify output’ loops were trying to perform the model selection by hand picking . Finally , if adjacent lines of code performed the same function ( e . g . , six lines of code / cells all doing sort ( ) on different data ) , they were all grouped into one process of formatting data ( PP3 ) , and the sequence of 6 : PP3 was instead counted as 1 : PP3 ( 𝐻 7 ) as we were interested in the purpose a part of the code performs ) . Using these heuristics ( 𝐻 4 - 𝐻 7 ) , we process the notebooks into a cleaner encoded list of categories . ( See Figure 7 in the Appendix Section 8 . 1 for the functional categories . ) Identifying code purpose from encoding patterns . Three researchers qualitatively analyzed repeating sequences of categories to identify which ones suggest “what the code is meant to do . ” These sequences capture a logical Manuscript submitted to ACM 10 Chattopadhyay , et al . purpose behind why multiple lines of code were written . For instance , when adjacent cells performed multiple data transformations followed by checking the summary of data operations , these cells’ purpose was marked as “summary - based transformation” . These code purposes were part of a step in the notebooks ( e . g . , summary - based transformation is part of data wrangling ) . Once the authors identified the patterns , we automated frequency counting of the patterns by identifying the location of each function in each notebook . The script takes in the processed encoded notebooks and consults a dataset where different sequences of categories associated with code purpose are stored . For example , a sequence of { ML1 , ML2 , ML3 } or { ML1 , ML3 , ML4 } or { ML1 , ML2 } all indicate the associated lines of code were meant to build a generic ML model . If the script detects any of these sequences in the encoded notebook , it shows the pattern’s location within the notebook . Porpoise uses these detected patterns and the location of the patterns to organize the code cells into sections . It uses the purposes of the code associated with each pattern as a section title . For example , in Figure 3 , The code in cells 50 - 51 are typical ML model defining and training activities , which relates to the section “Generic Modelling” as shown in Section 8 . 2 in Appendix Section 8 . 2 , for each code purpose , we presented explanations of each code purpose ( code sequences of each code purpose in supplementary [ 78 ] ) . Porpoise stops the clustering sections using header markdown cells as chapter titles . The chapter descriptions ( introductory paragraph ) were constructed by combining any text in the notebook by the original data scientist . To create an introductory paragraph ( like books ) that contains a summary of the chapter , we combined text sentences within each chapter into a paragraph and started this with : “In this chapter , the data scientist . . . ” . Note that the code purpose we identified is based on repeated patterns in code functionality found in the 44 notebooks . We don’t claim that these patterns are exhaustive , and other patterns may arise if the analyzed notebook uses functions other than the 605 functions we categorized . However , users can continue to use our method to identify meaningful patterns in previously unseen cases through two straightforward modifications . First , they must categorize and add the new functions to the list mapping stored in the configuration file . After using the updated configuration file to map the notebooks into a list of codes , users can use the pattern recognizer script to locate where the patterns exist . 3 . 3 Operationalize Design Features We used a design probe to operationalize design . Design probes are a valuable strategy used by the Human - Computer Interaction ( HCI ) and Design community to detect flaws and challenges early on , and learn about participants’ behavior . These probes are a design strategy that involves asking open - ended questions , presenting challenges , and observing participants’ responses to build systems that support human processes [ 79 ] . Although not standalone systems , probes require significant effort to build and put design at the heart of the process [ 80 ] . They are well - suited to verify the usefulness of theory - driven efforts and detect flaws and challenges early on [ 81 , 82 ] . Additionally , probes create an openness in the study to learn about participant behavior and provide an alternative approach to participatory design that starts with blank pages [ 83 , 84 ] . We have named our design probe Porpoise , as it is specifically designed to parse the notebook’s “purpose” and display it in interactive groups . At this stage , our methods are semi - automated , but we have the opportunity to improve automation and add intelligent parsing and rendering when the probe evolves into a full - scale system . There are two steps to transform a notebook into a Porpoise notebook—first , identify the patterns present in the notebook using the pattern mining script package ( provided in supplementary [ 78 ] ) . And then map those patterns as headers into the front end of the Porpoise overlay built using JavaScript . Once mapped , Porpoise allows users to interact with all the features , navigate across sections , annotate the notebook , and export the notebook or parts of it . Manuscript submitted to ACM Make It Make Sense ! 11 Generic Modelling # Load data  net CharRNN ( chars , n _ hidden , n _ layers ) = = = 256 3 [ 50 ] : # train plt . figure ( figsize ( , ) )  train ( net , data , epochs , n _ seqs , n _ steps , lr , device device , val _ frac ,  name , plot )  = = = = = = = = = 12 4500 768 512 0 . 001 0 . 1 True ' ml - train - data ' [ 51 ] : 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 0 25 50 75 100 125 150 175 Need further tunning Comments . . .  Cancel  Save  Validation loss does not decrease further , stopping training . CHAPTER 04 - MODELING & EVALUATION  In this chapter , the data scientists define cross - validation strategies and choose the models for training.  1 6 8 9 7 2 77 Use the cursor to select the text you would like to annotate and the tool palette will appear Annotation Export / Import Save expanded cells 1 3 4 5 2 Filter files by name Exploratory Visualization 1 - 14 / demo / Chapters Flags Cell Number Export http : / / localhost : 8888 / lab JupyterLab D Generic Modelling Model Selection - Feedback Based Model Evaluation # Load data  net CharRNN ( chars , n _ hidden , n _ layers ) = = = 256 3 [ 50 ] : # train plt . figure ( figsize ( , ) )  train ( net , data , epochs , n _ seqs , n _ steps , lr , device device , val _ frac ,  name , plot )  = = = = = = = = = 124500 768 512 0 . 001 0 . 1 True ' ml - train - data ' [ 51 ] : 1 . 01 . 52 . 02 . 53 . 0 0 25 50 75 100 125 150 175 Validation loss does not decrease further , stopping training . CHAPTER 04 - MODELING & EVALUATION  E Cancel  Save  B B Comments . . .  Code Python 3 FileEditView Run KernelTabs SettingsHelp ds . ipynb 50 - 135 Filter files by name Exploratory Visualization Data Cleaning Feature Engineering Modeling & Evaluation Ensemble Methods 1 - 13 14 - 23 24 - 49 135 - 166 Use the cursor to select the text you would like to annotate and the tool palette will appear Annotation Export / Import Save expanded cells / demo / Chapters Flags Cell Number ExportExport In this chapter , the data scientists define cross - validation strategies and choose the models for training.  A B C Need further tuning Fig . 4 . Overview of Design Features 4 PORPOISE WALK - THROUGH Charlie is a professional data scientist who uses computational notebooks in Python . She is exploring her team’s past analyses to find an appropriate model for her current task . She uses Porpoise to explore the notebook ( see Figure 4 ) . A bird’s eye view : Porpoise provides a story - like overview about the notebook through the “side panel” ( Green box in Figure 4 ) that is similar to a table of contents . Charlie can see from the chapter titles ( Figure 4 1 ) that the notebook includes multiple chapters ( e . g . , Feature Engineering , modeling & Evaluation ) , without having to read through the code . She also sees that the notebook uses ensemble methods , something she wants to use in her analyses . Therefore , she knows she is on the right path and decides to continue to explore the notebook . The side panel shows the amount of code per step , similar to chapter lengths , by displaying the associated cell numbers ( Figure 4 3 ) . Finally , “flags” in the side panel highlight the important operations like where data and libraries get imported , graphs and tables are displayed , and models are built ( Figure 4 2 ) . Deep dive into areas of interest . As Charlie wants to understand how the notebook implements the modeling , she “opens” Chapter 4 ( Modelling & Evaluation ) by clicking the chapter name in the side panel to open it on the right side canvas ( Figure 4 5 ) . The canvas shows the chapter title ( Figure 4 1 ) and a short introductory description of the chapter ( Figure 4 6 ) . Charlie clicked the “Generic Modelling” section to see what is inside it ( Figure 4 7 ) . Flags ( Figure 4 2 ) in the section headers provide cues about the modeling operations in each section , which helps her decide if the sections are of interest . So far , Porpoise has allowed Charlie to explore the notebook , its structure , and interesting operations without having to “crawl” through the code . Overlaying thoughts and comments . As Charlie reviews the code by expanding each section , she formulates a few hypotheses she wants to test . Porpoise allows data scientists to annotate parts of the notebook . Charlie jots down that the model “needs further tuning” without having to actually modify the notebook ( Figure 4 8 ) . Charlie selects a line of code , clicks on the annotate icon , which opens a comment ( Figure 4 9 ) , where she writes her note and saves it . Notes are visible when a Charlie hovers over the annotated line of code ( Figure 4 8 ) . After exploring the Manuscript submitted to ACM 12 Chattopadhyay , et al . relevant chapters and code cells , Charlie creates a snapshot of her exploration by exporting the notebook , which creates a document ( PDF ) ( Figure 4 5 ) with the expanded cells and annotations . 5 USER EVALUATION OF PORPOISE We conducted a within - subject counterbalanced observation study to learn how data scientists make sense of and explain notebooks using Porpoise’s interaction experience against baseline Jupyter . To compare their experiences and understand how the adaptations of Porpoise help improve sensemaking , we structure our analysis to answer the research question : RQ2 : How helpful are computational notebooks that support sensemaking ? 5 . 1 User Study Table 2 . Demographic Information of User Study Participants PID Gender Job Roles DS Experiences PID Gender Job Roles DS Experiences Group 1 ( Jupyter notebook - M * ; Porpoise - H * ) Group 2 ( Jupyter notebook - H * ; Porpoise - M * ) P1 Male Computer science PhD student 3 - 5 years CP1 Female Computer science MS student < 1 year P2 Male Data scientist / Staff researcher 1 - 3 years CP2 Female AI / ML - engineer < 1 year P3 Female Data scientist 3 - 5 years CP3 Male AI / ML - engineer < 1 year P4 Male Data scientist 3 - 5 years CP4 Female Data scientist 3 - 5 years P5 Male Computer science Post - doc 3 - 5 years CP5 Male Data scientist 1 - 3 years P6 Male Mathematics PhD student 3 - 5 years CP6 Male Computer science PhD student 3 - 5 years P7 Male Computer science PhD student 3 - 5 years CP7 Male AI - Ops / ML - Ops 1 - 3 years P8 Male AI / ML - engineer 1 - 3 years CP8 Female Computer science PhD student 1 - 3 years P9 Male Electrical engineering PhD student 3 - 5 years CP9 Female Computer science MS student 1 - 3 years P10 Male AI / ML - engineer 1 - 3 years CP10 Female Data scientist / Staff researcher 1 - 3 years P11 Male AI / ML - engineer 1 - 3 years CP11 Female Computer science PhD student 3 - 5 years P12 Male Data science researcher More than 5 years CP12 Male AI / ML - engineer 1 - 3 years * M : Original source code : https : / / github . com / alexattia / Data - Science - Projects / blob / master / KaggleMovieRating / Exploration . ipynb * H : Original source code : https : / / github . com / massquantity / Kaggle - HousePrices / blob / master / HousePrices % 20Kernel . ipynb Recruitment . We recruited 24 data science practitioners using convenience and snowball sampling from companies like Microsoft , RStudio , Amazon , and GitHub , and data science practitioners with advanced mathematics and computer science degrees . The participants were recruited through a sign - up survey . The Institutional Review Board ( IRB ) at our University approved the study protocol and materials and determined that the study had minimal risk . We compensated participants with $ 50 gift cards for their time . Participants reported working in various industries and roles , including data analysts , data scientists , and machine learning developers ( Table 2 ) . They primarily reported using Python with Jupyter Notebooks , motivating them to participate in the study . Once participants scheduled a time for a video call via Zoom , we forwarded the review board - approved consent form . The study was conducted entirely online through Zoom . All parts of the study were browser - based , and participants only needed access to a browser . The authors followed a script to introduce the study objectives and walked them through the parts of the task . Upon receiving verbal consent from the participants , the researcher recorded the voice and screen activity of the browser window . The study was conducted during the COVID - 19 pandemic when most people worked from home or remotely . We chose to make the study more accessible by selecting a browser - based approach . This lowered the barrier as participants did not need to install any apparatus on their devices . Protocol . We conducted a within - subject counterbalanced observation study , incorporating a think - aloud protocol to capture verbalized thoughts while performing a task [ 85 ] . We asked participants to go through two notebooks authored Manuscript submitted to ACM Make It Make Sense ! 13 by two anonymous data scientists and “explain the logic behind the analysis as [ they ] go through the code . Especially , focus on what [ they ] think the data scientist was trying to achieve or how they approached the problem” , which we will refer to as the task henceforth . We tentatively time - boxed each task to 25 minutes , and each participant performed the task twice , first explaining a notebook in Jupyter format followed by a notebook in Porpoise format . We selected Jupyter Notebooks since they are already ubiquitous in computational education and research among data scientists [ 86 , 87 ] . Notebook M is written to find the best model to predict movie ratings . Notebook H is an analysis to predict house pricing . The links to the source notebooks are provided in Table 2 . The Porpoise version of both notebooks is available here ( redacted for anonymity and added to supplementary [ 78 ] . ) For the first task , we provided Group 1 ( P1 - P12 in Table 2 ) with Jupyter Notebook M hosted on GitHub and asked them to perform the task . At the end of this , we asked participants “What are some of the pain points you faced when using this [ Jupyter ] format ? ” . Then we introduced the participants to Porpoise by demonstrating its features and allowing them to practice with a warm - up task . This involved expanding chapters and sections , highlighting code snippets , and leaving comments allowing them to familiarize with Porpoise . Once the warm - up was completed , the participants began the second task . The study protocol was the same for Group 2 ( CP1 - CP12 in Table 2 ) . However , we switched the notebooks to counterbalance any patterns we observed from the notebooks themselves . At the end of both tasks , we administered the participants with a post - study questionnaire [ 78 ] asking them about the advantages and disadvantages of Porpoise . Then , we asked six Likert scale questions where participants rated Porpoise’s helpfulness across different dimensions ( See [ 78 ] for post - study questionnaire ) . Analysis . We first transcribed the video and audio recordings of the participants . From the first task , we gathered observations from the participants using Jupyter notebooks to comprehend the purpose of the code . we analyzed the effect of using Porpoise . We performed a similar analysis using the verbalizations from participants and answers to the post - study questionnaires . The first three authors collaboratively mapped the participants’ experiences to the broader themes of challenges discussed above , to present how these experiences changed when using Porpoise . We also performed a descriptive analysis of the quantitative measures from the Likert scales to corroborate our findings . 5 . 2 Effect of Porpoise on sensemaking challenges A . Helping users comprehend analysis structure and purpose . Porpoise presents the notebook with chapters on the side panel , chapter and section titles , and chapter descriptions . The expandable sections of code and output were labeled with the code’s purpose to provide easier pathways to understand the components . Participants felt that chapter names helped make sense of code more efficiently by making “each chapter’s main purpose very clear " [ P7 ] . “Within this chapter , it was really helpful in terms of figuring out what really was going on in [ chapter name ] " [ P1 , CP4 , CP6 ] . Section titles within the chapters reduced participants’ efforts to understand the analysis components . Participants initially verified whether the section title is representative by reading the code . Once they felt confident , they would “skip " sections that don’t interest them [ P3 , P4 , P11 , CP6 ] . “I will see does it explain the title ? If it coincides with my understanding then I will tend to continue and not go through the detail of the code " [ CP11 ] . The collapsible property makes the length of the script not a concern , as sections break long parts into headers that can be hidden [ P3 , P5 , CP6 ] . Additionally , cell numbers in Porpoise’s side panel also provide a sense of how big the sections are , which can “give you an idea of how much time it may take you to read the chapter " [ CP2 ] . We observed participants could more easily compare alternate implementation pathways , e . g . , go to feature engi - neering to compare the different models included in the notebook [ P1 , P4 , P5 , P6 , P7 ] . Chapter and section titles can Manuscript submitted to ACM 14 Chattopadhyay , et al . help participants understand the functionality of specific libraries and functions , “I have guidelines here , I know you are doing data cleaning , feature engineering , etc " [ P9 ] . Overall , participants reported that features A , D , E helped them understand the analysis components and purpose of the code . “The chapter , chapter titles , chapter summaries , and then the notes within each section really helped me navigate and understand what was in the notebook that I was looking at " [ P3 ] . B . Helping notebook users find relevant information . Porpoise’s navigational affordances helped significantly reduce the need to scroll across long notebooks . Since the chapters and sections are collapsible , Participants could “simply click the section . . . I don’t need to go up and go down " [ P9 , P11 , CP8 ] . It also helped participants select to “look at what matters to [ them ] right now " [ P3 , P5 , P6 , P7 , CP3 , CP5 ] , and they could “focus on the content that is interesting” [ P5 , P9 , P11 , CP9 ] as the Porpoise affordances “guided where to go " [ P8 ] . These features made it “much easier to locate specific code " [ P11 , CP10 ] which helped participants orient themselves in the notebook and avoid getting lost [ P9 ] . Participants could look for a specific portion of data based on the section title E [ P4 ] or flags indicating what elements are included in the chapter and sections [ P1 , P4 , P12 ] . An interesting strategy participants tool was to evaluate which sections are most valuable and expand those , “I would go to a chapter and then go to the section that I think would provide the most information about what are we looking for , because like , I could skip like libraries . . . this limited the amount of code that I had to search through to find the piece of information I was looking for " [ P3 ] . Porpoise provides several UI affordances to help data scientists skip irrelevant or uninteresting information . As they already knew “what is included based on the title " [ P2 , P3 ] , the section headers describing the code purpose allowed participants to skip or “gloss over " [ P11 ] some sections . The collapsible property of the section headers made it “easier to navigate” [ P5 ] and allowed participants to “easily jump around” the different sections [ P4 , P11 ] [ C5 - I8 ] . “it was nice to be able to , like , skip over some things , because I knew that they weren’t necessary " [ P3 ] . We observed this in action as CP9 was thinking out loud while trying to understand the notebook and said , “Let’s go on to the data cleaning , but I don’t want to see this anymore . I’ll close that " [ CP9 ] . Porpoise’s navigational panel A and icons also helped to make the experience interactive chapters D locate components ( graphs , data , etc . ) . The overall clear demarcation in the analysis through chapters and sections made it “clearly much more easier to navigate " [ CP6 , CP7 , CP8 ] . “Since the menu is available to me on the left - hand side , I could easily fiddle around . So , the usability index piece of the interface was better " [ CP9 ] . Participants found that “flags are quite useful " [ P4 ] as it points to where certain phases of the analysis happen . CP6 explained , “it is helpful because you see , I can see the graph button , where the model has been written , and where there are tables , and where there are things related to data . . . that would help significantly . " As a consequence of these dynamic interactions and the enhanced ability to traverse through various sections of the notebook , participants reported being able to make more accurate assumptions about the code’s structure and purpose overall [ P1 , P2 , P3 , P4 , P5 , P6 , P12 , CP1 , CP2 , CP4 , CP9 , CP11 ] . C . Helping users build mental models of the notebook’s purpose . Porpoise helped participants to build a map of the structure and gave the participants “the big picture " of what the data scientist was A D E trying to do [ P3 , P9 , CP1 , CP2 ] . “Using this notebook , for each expanded section , at least I know like , oh , this , this operation is within this scope " [ CP11 ] . Participants could build and maintain mental models of the notebook by mapping the structure to the flow ; as CP2 described , his Porpoise experience was “really like reading a book " . The chapters on the side panel helped to “very clearly to show the flow of the analysis” [ P5 , P10 , P11 ] which in turn made it easy for the participants to understand how each component fits into the overall objective of the notebook [ P1 , CP2 ] . Manuscript submitted to ACM Make It Make Sense ! 15 The ability to “get a quick , top - down view” and “choose what you want to drill into " [ P12 ] helped connect the different components into chained events towards the larger objective [ CP1 ] . Participants could also “review back to make links between different sections to see the technical connection behind those sections " [ P9 ] . This top - down view also helped participants to not be distracted . “When it comes to some internal operation , for example , transformation verification , before this notebook , when I look at those portions of code , I will go to the wild , small details , but here I don’t I lose track of the big , big picture " [ CP11 ] . D . Allow notebook users to build and share explanations . Participants could leave annotations to themselves about hypotheses or their inferences , marking where to come back later if they needed to jump to another section [ P5 ] B . “I could track the code based on the sections , annotation is one of the best parts [ P9 ] . While making sense of the notebook , CP9 needed to review some part of the code from later in the notebook . Instead , she marked the place to revisit later and moved on ; “So maybe I’ll put a comment here , see you later " [ CP9 ] . Participants enjoyed this feature as it was “similar to what [ one ] would do in a while reading a PDF . [ One ] would like to annotate it’s , it’s the same for this . It is very helpful " [ CP6 ] . Participants wanted to use annotation for various purposes , from noting down small questions like “is this a typo ? " [ P6 ] to take detailed notes about steps in the pipeline to remind themselves [ P5 ] to “review back " [ P9 ] . Additionally , many participants agreed that commenting feature would be helpful in a collaborative setting [ P3 , CP10 ] . “I could also see the comment feature being really important if I was going to be handing my work products to another person or back to the data scientist . Then that would be helpful too . " [ P2 ] . Participants envision they would “leave comments for collaborators " [ P3 , P6 , P10 ] , “readers / managers could leave comments if they have any confusion " [ P6 ] , and “send this notebook to the senior , to check out the results , or to present the results " [ P4 , CP1 , CP2 ] . They also suggested annotations can get “help from the person who wrote the script " [ P11 ] . Furthermore , Porpoise’s annotation feature uses color highlights for annotated text / code , aiding participants in their reading and understanding . Participants discussed various strategies to use these features , like using separate colors for separate purposes [ P10 ] or using highlighting as markers to go find information [ CP8 ] . “You can label it with different colors . So I can see different colors organized by the tag . Oh , yellow will be my understanding or like my personal notes . And blue could be your to - do list . . . " [ P9 ] . Finally , participants also found Porpoise’s features to export annotated and expanded notebooks useful B C . Especially in collaborative settings , “it is really hard to share thoughts " [ P9 , CP4 ] . Sharing the annotated notebook would help communicate thoughts while working together [ P9 ] . In contrast to participants’ wanting to organize and drop down their thoughts for later on paper or external notepads [ P6 , P9 , P10 , P11 ] when using Jupyter notebooks , participants found Porpoise’s annotation feature “very helpful to indicate something for [ themselves ] ” [ P5 , P3 , P6 , P9 , P11 , CP1 , CP2 , CP4 , CP8 , CP9 , CP10 ] . An overview of mapping between design features and cognitive tasks . To make it evident how the design feature assists the data scientist in better understanding unfamiliar notebooks . The mapping design features developed based on cognitive activities with sensemaking needs assessed in our user studies are shown in Figure 5 . According to what we found , the cognitive tasks that are included in the comprehension element , the navigation panel , the chapter titles and icons , and the section titles are the three primary features that can assist data scientists in comprehending the objective , feature , and information of the analysis . As for structural - related cognitive tasks from the element of mental modeling , we found that in addition to the navigation panel , chapter / section titles , export , and annotations can provide additional help and support for data scientists who want to leave bookmarks for themselves and their collaborators . In addition , all of the features , with the exception of export , can assist data scientists in the process of inferring hypotheses and elements . Manuscript submitted to ACM 16 Chattopadhyay , et al . Understanding Behavior ( C1 ) Comparing Information ( C2 ) Broadening Knowledge ( C3 ) Understanding Structure ( C4 ) Mapping / Relating Elements and Concepts ( C5 ) Inferring hypothesis ( C6 ) Inferring elements ( C7 ) COGNITIVE TASK Helping users comprehendanalysisstructure and purpose Helping users find relevant information Helping users build mental models of notebookpurpose Helping users build and share explanations A D E A D E A B C D E A D E D E D E D E A D E A D E A B C D E A D E A D E A D E A D E A B D E A B D A B C D E Navigation panel Annotation Export Chapter title and icon Section title Fig . 5 . Design feature mapping . Multiple features contribute to support each cognitive task in the sensemaking process . Participants questionnaires . Figure 6 shows the distribution of responses from participants about the helpfulness of Porpoise . We asked the participants to rate the helpfulness of Porpoise by answering six questions . Four of these questions corresponded to cognitive tasks C1 - C7 , which are associated with three key elements in sensemaking : comprehension , mental modeling , and contextual inferences ( see Figure 2 ) : “Understanding the analysis” [ C1 - C3 ] “Identify the purpose of analysis” [ C1 - C3 ] , “Understand the flow of the analysis " [ C4 , C5 ] , and “Find decision points” [ C6 , C7 ] . The remaining two tasks , “explain the notebook” and “adapt the notebook” , aren’t the primary goals of our set of affordances ; these are tasks that require sensemaking and are two of the possible activities that come after sensemaking . We used this set of questions to understand how Porpoise’s affordances help the sensemaking process and beyond and also to drive the discussions on how to improve their experience further . Participants ranked Porpoise’s ability to “Understand the analysis” , “Identify the purpose of analysis” , and “Explain the analysis” as the three most helpful aspects , with 71 % , 79 % , and 80 % , of participants finding it very or extremely helpful . Figure 6 displays a bar chart illustrating the distribution of responses from six Likert - scale questions . These questions rank the helpfulness of Porpoise in making sense of notebooks , with responses ranging from 1 ( Not helpful at all ) to 5 ( Extremely helpful ) . The y - axis presents the six dimensions by which Porpoise’s helpfulness is evaluated . The chart employs a 5 - shade green color scheme . From left to right , the shades transition from darkest to lightest , representing the proportions of the 5 Likert scale responses received . At the chart’s bottom , a legend specifies each option : " Extremely helpful " , " Very helpful " , " Somewhat helpful " , " Slightly helpful " , and " Not at all helpful " , representing different levels of perceived helpfulness . The top section of the chart highlights the percentage of participants who agreed with the statement " Adapt the notebook . " On the left , a displayed percentage reads " 10 % . " As one moves to the right , the percentages increase : " 48 % " , " 19 % " , and culminating in a combined " 19 % and 5 % . " The subsequent bar represents the percentage of participants who concurred with " Explain the notebook , " accompanied by the respective percentages : Manuscript submitted to ACM Make It Make Sense ! 17 33 % 13 % 54 % 8 % 42 % 10 % 46 % 58 % 21 % 38 % 38 % 48 % 21 % 17 % 21 % 21 % 17 % 19 % 12 % 4 % 13 % 3 % 19 % 21 % 5 % 0 % 20 % 40 % 60 % 80 % 100 % Identify goal of notebook Understand the notebook Understand the flow of notebook Find decision points Explain the notebook Adapte the notebook Extremely helpful Very helpful Somewhat helpful Slightly helpful Not at all helpful Fig . 6 . The distribution of responses from six Likert - scale questions ranking the helpfulness of Porpoise in making sense of notebooks , measured from 1 ( Not helpful at all ) to 5 ( Extremely helpful ) . The y - axis presents the six dimensions along which Porpoise’s helpfulness is evaluated . " 42 % " , " 38 % " , and a combined " 17 % and 3 % " . These figures also seem to hint at the role of social media . Next , another bar indicates the percentages of participants who agreed with the statement " Find decision points , " with percentages displayed as 8 % , 38 % , 21 % , 13 % , and 21 % . Following that , another bar displays agreement on " Understand the flow of the notebook . " The listed percentages from left to right are 54 % , 21 % , 21 % , and 4 % . Another bar then shows agreement levels on " Understand the notebook , " with percentages of 13 % , 58 % , 17 % , and 12 % . The final bar showcases the percentage of participants who agreed with " Identify the goal of the notebook , " with percentages displayed as 33 % , 46 % , and 21 % . We discovered that the Porpoise is beneficial for data scientists to “understand the flow of the analysis , " with all participants finding Porpoise at least somewhat helpful . In comparison , almost a third of the participants did not find Porpoise helpful in finding decision points 34 % . Those who took part in the survey found Porpoise “very helpful to find the decision points” , as in the case of P2 , who identified modeling and evaluations as the decision points . These opposing perceptions can be a result of how data scientists define decisions . While the purpose of code ( shown in chapter and section titles on Porpoise ) based on Section 8 . 2 is indicative of analysis decisions at a higher level of functionality , some data scientists might need more granular information to identify decision points ( e . g . , using 𝑙𝑖𝑛𝑒𝑝𝑙𝑜𝑡 ( ) instead of 𝑏𝑜𝑥𝑝𝑙𝑜𝑡 ( ) ) . For example , CP3 asked “what is the decision point” . 24 % of the participants did not consider Porpoise helpful for adapting the code ( “slightly help & Not at all helpful” ) . Several participants said they would need to “develop [ analysis ] in this interface to answer this question” [ P2 , P5 , CP3 ] . They further elaborated that to evaluate adaptation , they would need to use Porpoise on a notebook that specifically belonged to them [ P5 ] or try adapting a notebook into their analysis : “I am not sure , that was not the task , I did not do it” [ P2 ] . Whereas 58 % of participants believed that using an Porpoise will be at least somewhat beneficial in adapting and reusing notebook sections across different scales , for example , “sections can serve as templates that be reused across the Manuscript submitted to ACM 18 Chattopadhyay , et al . team” [ P3 ] . “Because at least I don’t have to go through the whole notebook and waste my time and then see how I can reuse or adapt” [ CP8 ] . 5 . 3 Design opportunities to further improve Porpoise At the end of the study , we asked participants to rate the helpfulness of the affordances for sensemaking . We asked six questions ( See Figure 6 ) to evaluate how well Porpoise supports sensemaking and asked participants to elaborate on improving their experiences further . Following that , we engaged participants to discuss what support they ideally desire from their notebooks through the questions , “What features of Porpoise do you find not helpful ? What features would you like to have instead ? " . Adaptive definitions of Chapter and Sections . Participants suggested adding the feature to interactively adjust the number of details in the chapter titles and section headers [ P6 , P9 , P12 ] . Participants had different opinions about the level of detail to be captured in the chapter and section titles . For instance , P12 noted how it could sometimes be hard to “tease out different results because they are very subtle” and desired to be able to drill down into single code statement explanations . In other cases , P12 wanted coarser granularity in the section headers , for example , combining multiple sections into a single process - level section labeled “Data Wrangling . ” Allowing users to choose the information captured in titles will provide personalized support for their individual sensemaking needs . Interfaces can allow users to add section or chapter titles and select which cells they want to include in each section . Making features interactive . For the side panel , we provided cell ranges to help data scientists understand the size of the chapters . However , none of the participants found the cell range useful and ignored it , with P3 , P7 , P11 calling it “the least useful feature . ” In future implementation , presenting the number of cells within each chapter may be more useful . Participants generally found the flags for the structural components ( data , library , graph , table , model ) to be useful , and P5 and P12 suggested a capability that would allow them to add custom flags based on what information is relevant for specific tasks , for example , when a certain model is being used . While highlighting the part of the code or text tied to the annotation is helpful to locate the comments , the Porpoise provides “just too many colors . And they that is a little bit distracting from the task at hand " [ P2 ] . [ P2 , P5 ] further proposed adding search features to find and navigate between comments will improve usage ; as P2 said , “if there’s a find , you can just go back and forth between the comments . " Thus , future implementations can provide better support by highlighting the annotated parts with a single color , allowing users to navigate between comments , and allowing comments with different colors / icons to signify which user is commenting or replying to the comments . 6 LIMITATIONS Automating Porpoise : By building Porpoise , we had the opportunity to evaluate the helpfulness of such a tool and provide researchers with promising directions for future research . Porpoise parses the notebook’s purpose and displays it in interactive groups . While the parsing and the front - end display of Porpoise are implemented , Porpoise can’t automatically display parsed notebooks on the webpage , Section 7 discusses avenues for automating Porpoise . Selecting notebooks : We made some assumptions about notebooks : we assumed that notebooks are linear and read top - down and that cells are executed in the order they are created . Our participants mostly use Jupyter notebooks , but environments substantially differ from notebook architectures—such as RStudio [ 88 ] and Spyder [ 89 ] —may require different affordances than what Porpoise provides . Studying sensemaking individually : Given the lack of understanding in how people make sense of data science artifacts , this paper focuses on how individuals make sense of notebooks authored by other data scientists . We discuss Manuscript submitted to ACM Make It Make Sense ! 19 two features of Porpoise ( annotation ( B ) and export ( C ) ) in the context of collaborative data science . In the future , we aim to conduct additional research to gain insights into data scientists’ collaboration methods and to enhance Porpoise for better support in social sensemaking . User study : We conducted a counterbalance user study to investigate how data scientists use Porpoise to understand and explain notebooks . When encountering unfamiliar interfaces , participants behave differently as they need time to situate themselves in the context . To reduce the unfamiliarity effect [ 90 ] , we evaluated Porpoise by first asking participants to use baseline Jupyter to situate them with their typical sensemaking strategies and behavior . Additionally , all user studies are limited by participants’ response bias ( as good - participant role [ 91 ] ) . To reduce this bias , we engaged participants to discuss negative experiences with Porpoise , i . e . , the least beneficial features for Porpoise . 7 DISCUSSION Facilitating sensemaking in other domains . Sensemaking is an essential part of doing tasks in any domain . Our findings about the benefits of Porpoise and experiences with cases where Porpoise lacked open exciting research opportunities to study how systems like this can support sensemaking in physical sciences and mathematics , literature , journalism , or even help make sense of other research papers . Since Porpoise’s features are based on fundamental psychological principles on how humans make sense of any knowledge artifact , researchers can study how similar support systems can be designed in their domains of interest . Automatically elucidating code purpose to support code sensemaking and reuse . The findings from our design probe are encouraging and suggest that the Porpoise interaction experience is useful to data scientists . Thus , we discuss approaches towards designing a robust mechanism for building a purpose catalog . There are multiple potential approaches to automatically generating this catalog . One approach is to collect paraphrases of code comments and summaries describing code behavior from existing databases such as the MSR paraphrase corpus and phrase table [ 92 ] , ParaPhrase DataBase [ 93 ] , and DIRT [ 94 ] . Additional code purposes can be added to the catalog using machine - translation - inspired generative approaches , or variational auto - encoders with sequence - to - sequence models [ 95 ] that can learn from existing sequences of code syntax using a feed forward Deep Neural Network . Another approach is to use automated source code summarization [ 96 ] to describe the behavior of code cells using machine learning to discover the most important analysis steps [ 97 ] . Common methods to train machine learning models include collecting relevant keywords characterizing the program behavior using a Software Word Usage Model [ 98 ] and then using a Natural Language Generation system to generate natural language text describing the behavior [ 99 ] . However , the same analysis step ( and its code summarization ) can mean different things based on the surrounding code . For instance , the same function when called in a data model selection step as compared to a model refinement step can have different purposes . Thus , automated code summaries must also take into account the context of a particular cell slice [ 100 ] . Using Porpoise to bootstrap literate programming . While Jupyter notebooks have features like Markdown cells for data scientists to explain their notebooks , simply having the ability to do so doesn’t mean that data scientists will actually provide meaningful descriptions [ 17 ] . While Knuth desired programs to be “works of literature " [ 13 ] , even overcoming the initial inertia of having to document a notebook—especially an exploratory one—can be considerable . Porpoise helps data scientists to overcome this inertia by automatically bootstrapping the notebook with descriptions . Just as revising paragraphs can be often easier than writing one from scratch , our participants remarked that being able Manuscript submitted to ACM 20 Chattopadhyay , et al . to rename the automatic group descriptions—rather than having to come up with a description on on their own—made them more inclined to add descriptions ( P4 , P12 ) . For similar reasons , many participants reported the annotation capability as “one of the best parts of the experience” ( P3 , P5 , P6 , P8 , P11 ) . In contrast to explicit Markdown cells , Porpoise provided them a lightweight way to sprinkle their insights throughout the notebook . Consequently , the annotation experience was far less daunting when compared to more formal approaches to documenting the notebook . Finally , being able to reason about an unfamiliar notebook through the familiar experiences similar to physical books ( P9 ) makes “it easier to understand what to expect in each section through commonly understood terms like chapters and headings” ( P3 ) . In addition to the self - explanation benefits of using Porpoise , we postulate that this interaction experience will also encourage data scientists to incrementally adopt literate programming practices in their own day - to - day notebook explorations . 8 CONCLUSION From investigating sensemaking across psychology , computer science , and data science , we identified the components and cognitive tasks involved in sensemaking . We built a design probe of computational notebook overlay by blending five straightforward affordances that bring out the computational narratives by adding explanations about code structure and purpose . This design probe , Porpoise , structures programs as explanations to humans rather than instructions to the computer . Through a within - subject counterbalanced observation study with 24 data science practitioners , we found that even simple affordances to accentuate the computational narrative help cater to data scientists’ challenges when sensemaking . Data scientists enjoyed Porpoise as it helped them make sense of notebooks by getting a focused view and taking notes . P9 summarizes Porpoise’s benefits succinctly— “It’s just like reading a book ! ” ACKNOWLEDGMENTS We thank all the interviewees for their contributions to this research . We would also like to express our gratitude to Marjan Adeli for her contribution to Appendix , which involved designing the classification of code functionality and functionality patterns . This work is supported by the National Science Foundation ( NSF ) under Grant Nos . CCF : 2008089 and IUSE : 2235601 . REFERENCES [ 1 ] J . Whitehead , I . Mistrík , J . Grundy , and A . v . d . Hoek , “Collaborative software engineering : concepts and techniques , ” in Collaborative Software Engineering . Springer , 2010 , pp . 1 – 30 . [ 2 ] J . Whitehead , I . Mistrík , J . Grundy , and A . van der Hoek , Collaborative Software Engineering : Concepts and Techniques . Berlin , Heidelberg : Springer Berlin Heidelberg , 2010 , pp . 1 – 30 . [ Online ] . Available : https : / / doi . org / 10 . 1007 / 978 - 3 - 642 - 10294 - 3 _ 1 [ 3 ] A . Y . Wang , A . Mittal , C . Brooks , and S . Oney , “How data scientists use computational notebooks for real - time collaboration , ” Proceedings of the ACM on Human - Computer Interaction , vol . 3 , no . CSCW , pp . 1 – 30 , 2019 . [ 4 ] M . Kim , T . Zimmermann , R . DeLine , and A . Begel , “Data scientists in software teams : State of the art and challenges , ” IEEE Transactions on Software Engineering , vol . 44 , no . 11 , pp . 1024 – 1038 , 2017 . [ 5 ] K . Subramanian , N . Hamdan , and J . Borchers , “Casual notebooks and rigid scripts : Understanding data science programming , ” in 2020 IEEE Symposium on Visual Languages and Human - Centric Computing ( VL / HCC ) . IEEE , 2020 , pp . 1 – 5 . [ 6 ] A . X . Zhang , M . Muller , and D . Wang , “How do data science workers collaborate ? roles , workflows , and tools , ” Proceedings of the ACM on Human - Computer Interaction , vol . 4 , no . CSCW1 , pp . 1 – 23 , 2020 . [ 7 ] M . B . Kery , M . Radensky , M . Arya , B . E . John , and B . A . Myers , “The story in the notebook : Exploratory data science using a literate programming tool , ” Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , pp . 1 – 11 , 2018 . [ 8 ] M . B . Kery and B . A . Myers , “Exploring exploratory programming , ” in 2017 IEEE Symposium on Visual Languages and Human - Centric Computing ( VL / HCC ) . IEEE , 2017 , pp . 25 – 29 . Manuscript submitted to ACM Make It Make Sense ! 21 [ 9 ] M . B . Kery , A . Horvath , and B . A . Myers , “Variolite : Supporting exploratory programming by data scientists , ” CHI , vol . 10 , pp . 3025453 – 3025626 , 2017 . [ 10 ] A . Rule , A . Tabard , and J . D . Hollan , “Exploration and explanation in computational notebooks , ” in Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems , ser . CHI ’18 . New York , NY , USA : Association for Computing Machinery , 2018 , p . 1 – 12 . [ Online ] . Available : https : / / doi . org / 10 . 1145 / 3173574 . 3173606 [ 11 ] S . Chattopadhyay , I . Prasad , A . Z . Henley , A . Sarma , and T . Barik , “What’s wrong with computational notebooks ? pain points , needs , and design opportunities , ” p . 1 – 12 , 2020 . [ Online ] . Available : https : / / doi . org / 10 . 1145 / 3313831 . 3376729 [ 12 ] J . M . Perkel . ( 2018 , October ) Why jupyter is data scientists’ computational notebook of choice : An improved architecture and enthusiastic user base are driving uptake of the open - source web tool . [ Online ] . Available : https : / / www . nature . com / articles / d41586 - 018 - 07196 - 1 [ 13 ] D . E . Knuth , “Literate programming , ” The computer Journal , vol . 27 , no . 2 , pp . 97 – 111 , 1984 . [ 14 ] A . Rule , I . Drosos , A . Tabard , and J . D . Hollan , “Aiding collaborative reuse of computational notebooks with annotated cell folding , ” Proc . ACM Hum . - Comput . Interact . , vol . 2 , no . CSCW , nov 2018 . [ Online ] . Available : https : / / doi . org / 10 . 1145 / 3274419 [ 15 ] A . Head , F . Hohman , T . Barik , S . M . Drucker , and R . DeLine , “Managing messes in computational notebooks , ” in Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems , ser . CHI ’19 . New York , NY , USA : Association for Computing Machinery , 2019 , p . 1 – 12 . [ Online ] . Available : https : / / doi . org / 10 . 1145 / 3290605 . 3300500 [ 16 ] S . Kandel , A . Paepcke , J . M . Hellerstein , and J . Heer , “Enterprise data analysis and visualization : An interview study , ” IEEE Transactions on Visualization and Computer Graphics , vol . 18 , no . 12 , pp . 2917 – 2926 , 2012 . [ 17 ] J . F . Pimentel , L . Murta , V . Braganholo , and J . Freire , “A large - scale study about quality and reproducibility of jupyter notebooks , ” in 2019 IEEE / ACM 16th International Conference on Mining Software Repositories ( MSR ) , 2019 , pp . 507 – 517 . [ 18 ] A . Head , F . Hohman , T . Barik , S . M . Drucker , and R . DeLine , “Managing messes in computational notebooks , ” in Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems , ser . CHI ’19 . New York , NY , USA : Association for Computing Machinery , 2019 , p . 1 – 12 . [ Online ] . Available : https : / / doi . org / 10 . 1145 / 3290605 . 3300500 [ 19 ] D . Woods , N . Sarter , and C . Billings , “Automation surprises , handbook of human factors & ergonomics , ” 1997 . [ 20 ] A . von Mayrhauser and A . M . Vans , “From code understanding needs to reverse engineering tool capabilities , ” in Proceedings of 6th International Workshop on Computer - Aided Software Engineering . IEEE , 1993 , pp . 230 – 239 . [ 21 ] A . R . Bryant , R . F . Mills , G . L . Peterson , and M . R . Grimaila , “Software reverse engineering as a sensemaking task . ” Journal of Information Assurance & Security , vol . 6 , no . 6 , 2011 . [ 22 ] Y . Wang and D . Gafurov , “The cognitive process of comprehension , ” in The Second IEEE International Conference on Cognitive Informatics , 2003 . Proceedings . IEEE , 2003 , pp . 93 – 97 . [ 23 ] C . A . Ntuen , “Cognitive constructs and the sensemaking process , ” NORTH CAROLINA AGRICULTURE AND TECHNICAL STATE UNIV GREENS - BORO NC CENTER FOR . . . , Tech . Rep . , 2006 . [ 24 ] G . Klein , B . Moon , and R . R . Hoffman , “Making sense of sensemaking 1 : Alternative perspectives , ” IEEE intelligent systems , vol . 21 , no . 4 , pp . 70 – 73 , 2006 . [ 25 ] D . Snowden , “Naturalizing sensemaking , ” in Informed by Knowledge . Psychology Press , 2011 , pp . 237 – 248 . [ 26 ] —— , “Story telling : an old skill in a new context , ” Business information review , vol . 16 , no . 1 , pp . 30 – 37 , 1999 . [ 27 ] M . d . L . Borges and C . R . Gonçalo , “Learning process promoted by sensemaking and trust : a study related to unexpected events , ” Cadernos EBAPE . BR , vol . 8 , pp . 260 – 277 , 2010 . [ 28 ] C . M . Fiol and M . A . Lyles , “Organizational learning , ” Academy of management review , vol . 10 , no . 4 , pp . 803 – 813 , 1985 . [ 29 ] Y . Wang , Y . Wang , S . Patel , and D . Patel , “A layered reference model of the brain ( lrmb ) , ” IEEE Transactions on Systems , Man , and Cybernetics , Part C ( Applications and Reviews ) , vol . 36 , no . 2 , pp . 124 – 133 , 2006 . [ 30 ] P . N . Johnson - Laird , “Mental models . ” 1989 . [ 31 ] A . Rutherford and J . R . Wilson , “Models of mental models : an ergonomist - psychologist dialogue , ” in Human Factors in Information Technology . Elsevier , 1991 , vol . 2 , pp . 39 – 58 . [ 32 ] W . B . Rouse and N . M . Morris , “On looking into the black box : Prospects and limits in the search for mental models . ” Psychological bulletin , vol . 100 , no . 3 , p . 349 , 1986 . [ 33 ] N . Moray , “Identifying mental models of complex human – machine systems , ” International Journal of Industrial Ergonomics , vol . 22 , no . 4 - 5 , pp . 293 – 297 , 1998 . [ 34 ] N . Abel , H . Ross , and P . Walker , “Mental models in rangeland research , communication and management . ” The Rangeland Journal , vol . 20 , no . 1 , pp . 77 – 91 , 1998 . [ 35 ] J . Langan - Fox , J . Anglim , and J . R . Wilson , “Mental models , team mental models , and performance : Process , development , and future directions , ” Human Factors and Ergonomics in Manufacturing & Service Industries , vol . 14 , no . 4 , pp . 331 – 352 , 2004 . [ 36 ] S . Vosniadou , “16 universal and culture - specific properties of , ” Mapping the mind : Domain specificity in cognition and culture , p . 412 , 1994 . [ 37 ] N . J . Nersessian , The cognitive basis of model - based reasoning in science . na , 2002 . [ 38 ] T . Tiemens , “Cognitive model of program comprehension , ” Software Engineering Research Center Technical Report , 1989 . [ 39 ] G . Klein , J . K . Phillips , E . L . Rall , and D . A . Peluso , “A data – frame theory of sensemaking , ” in Expertise out of context . Psychology Press , 2007 , pp . 118 – 160 . Manuscript submitted to ACM 22 Chattopadhyay , et al . [ 40 ] M . R . Endsley , “Toward a theory of situation awareness in dynamic systems , ” in Situational awareness . Routledge , 2017 , pp . 9 – 42 . [ 41 ] S . Keele et al . , “Guidelines for performing systematic literature reviews in software engineering , ” Technical report , Ver . 2 . 3 EBSE Technical Report . EBSE , Tech . Rep . , 2007 . [ 42 ] C . Wohlin , “Guidelines for snowballing in systematic literature studies and a replication in software engineering , ” in Proceedings of the 18th international conference on evaluation and assessment in software engineering , 2014 , pp . 1 – 10 . [ 43 ] B . G . Glaser , “Open coding descriptions , ” Grounded theory review , vol . 15 , no . 2 , pp . 108 – 110 , 2016 . [ 44 ] R . Drazin , M . A . Glynn , and R . K . Kazanjian , “Multilevel theorizing about creativity in organizations : A sensemaking perspective , ” Academy of management review , vol . 24 , no . 2 , pp . 286 – 307 , 1999 . [ 45 ] G . Marchionini , “Search , sense making and learning : closing gaps , ” Information and Learning Sciences , vol . 120 , no . 1 / 2 , pp . 74 – 86 , 2019 . [ 46 ] Z . Wang , L . Chen , and T . Anderson , “A framework for interaction and cognitive engagement in connectivist learning contexts , ” International Review of Research in Open and Distributed Learning , vol . 15 , no . 2 , pp . 121 – 141 , 2014 . [ 47 ] P . Zhang and D . Soergel , “Towards a comprehensive model of the cognitive process and mechanisms of individual sensemaking , ” Journal of the Association for Information Science and Technology , vol . 65 , no . 9 , pp . 1733 – 1756 , 2014 . [ 48 ] M . S . Fitzgerald and A . S . Palincsar , “Teaching practices that support student sensemaking across grades and disciplines : A conceptual review , ” Review of Research in Education , vol . 43 , no . 1 , pp . 227 – 248 , 2019 . [ 49 ] P . Pirolli and S . Card , “The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis , ” in Proceedings of international conference on intelligence analysis , vol . 5 . McLean , VA , USA , 2005 , pp . 2 – 4 . [ 50 ] S . Srinivasa Ragavan , S . K . Kuttal , C . Hill , A . Sarma , D . Piorkowski , and M . Burnett , “Foraging among an overabundance of similar variants , ” Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems , pp . 3509 – 3521 , 2016 . [ 51 ] S . Kandel , A . Paepcke , J . Hellerstein , and J . Heer , “Wrangler : Interactive visual specification of data transformation scripts , ” in Proceedings of the sigchi conference on human factors in computing systems , 2011 , pp . 3363 – 3372 . [ 52 ] D . Knuth , “Literate Programming - CWEB , ” 2009 , [ Online ; accessed 2022 - 04 - 06 ] . [ 53 ] “Sign in – Sage , ” 2005 , [ Online ; accessed 2022 - 04 - 07 ] . [ 54 ] B . Walter , “Jupyterlab - Databricks Integration Bridges Local and Remote Workflows , ” dec 3 2019 , [ Online ; accessed 2022 - 04 - 07 ] . [ 55 ] Jupyter , “Jupyter : Free software , open standards , and web services for interactive computing across all programming languages , ” 2022 . [ 56 ] P . J . Guo and M . I . Seltzer , “Burrito : Wrapping your lab notebook in computational infrastructure , ” 2012 . [ 57 ] “Pdf reader | adobe acrobat reader . ” [ Online ] . Available : https : / / www . adobe . com / acrobat / pdf - reader . html [ 58 ] N . Cunniff and R . P . Taylor , “Graphics and learning : A study of learner characteristics and comprehension of programming languages , ” in Human – Computer Interaction – INTERACT’87 . Elsevier , 1987 , pp . 317 – 322 . [ 59 ] D . Wang , J . Andres , J . D . Weisz , E . Oduor , and C . Dugan , “Autods : Towards human - centered automation of data science , ” in Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems , 2021 , pp . 1 – 12 . [ 60 ] S . Kross and P . Guo , “Orienting , framing , bridging , magic , and counseling : How data scientists navigate the outer loop of client collaborations in industry and academia , ” Proceedings of the ACM on Human - Computer Interaction , vol . 5 , no . CSCW2 , pp . 1 – 28 , 2021 . [ 61 ] Z . Wu , P . Mitra , and C . L . Giles , “Table of contents recognition and extraction for heterogeneous book documents , ” in 2013 12th international conference on document analysis and recognition . IEEE , 2013 , pp . 1205 – 1209 . [ 62 ] C . C . Lin , Y . Niwa , and S . Narita , “Logical structure analysis of book document images using contents information , ” in Proceedings of the Fourth International Conference on Document Analysis and Recognition , vol . 2 . IEEE , 1997 , pp . 1048 – 1054 . [ 63 ] K . O’hara and A . Sellen , “A comparison of reading paper and on - line documents , ” in Proceedings of the ACM SIGCHI Conference on Human factors in computing systems , 1997 , pp . 335 – 342 . [ 64 ] A . Kalyanpur et al . , “A threaded interface for collaborative annotation of pdf documents . published jul . 15 , 2007 . ” [ 65 ] L . J . Ball and B . T . Christensen , “How sticky notes support cognitive and socio - cognitive processes in the generation and exploration of creative ideas , ” in Sticky creativity . Elsevier , 2020 , pp . 19 – 51 . [ 66 ] L . Guernsey and M . H . Levine , “Pioneering literacy in the digital age , ” Technology and digital media in the early years : Tools for teaching and learning , pp . 104 – 114 , 2015 . [ 67 ] I . V . T . García , “Diseño de actividades interactivas , complementarias al uso de los simuladores de negocios , para el desarrollo de competencias integrales . ” [ 68 ] S . Mao , A . Rosenfeld , and T . Kanungo , “Document structure analysis algorithms : a literature survey , ” Document Recognition and Retrieval X , vol . 5010 , pp . 197 – 207 , 2003 . [ 69 ] L . Gao , Z . Tang , X . Lin , Y . Liu , R . Qiu , and Y . Wang , “Structure extraction from pdf - based book documents , ” in Proceedings of the 11th annual international ACM / IEEE joint conference on Digital libraries , 2011 , pp . 11 – 20 . [ 70 ] E . Bart and P . Sarkar , “Information extraction by finding repeated structure , ” in Proceedings of the 9th IAPR International Workshop on Document Analysis Systems , 2010 , pp . 175 – 182 . [ 71 ] D . L . Sharp , J . D . Bransford , S . R . Goldman , V . J . Risko , C . K . Kinzer , and N . J . Vye , “Dynamic visual support for story comprehension and mental model building by young , at - risk children , ” Educational Technology Research and Development , vol . 43 , no . 4 , pp . 25 – 42 , 1995 . [ 72 ] P . F . Chong , Y . P . Lim , and S . W . Ling , “On the design preferences for ebooks , ” IETE Technical Review , vol . 26 , no . 3 , pp . 213 – 222 , 2009 . Manuscript submitted to ACM Make It Make Sense ! 23 [ 73 ] W . Ran and L . Jinglu , “The design and development of digital books for e - learning , ” in 2020 4th International Conference on Artificial Intelligence and Virtual Reality , 2020 , pp . 51 – 55 . [ 74 ] T . Fessenden , “Scrolling and attention , ” 2018 , [ Online ; accessed 2022 - 04 - 06 ] . [ 75 ] “Kaggle : Your machine learning and data science community . ” [ Online ] . Available : https : / / www . kaggle . com / [ 76 ] J . Saldaña , The Coding Manual for Qualitative Researchers . SAGE Publications , 2009 . [ 77 ] J . Forman and L . Damschroder , “Qualitative content analysis , ” in Empirical methods for bioethics : A primer . Emerald Group Publishing Limited , 2007 . [ 78 ] A . Author , “Make It Make Sense ! Understanding and Facilitating Sensemaking in Computational Notebooks , ” Jul . 2022 . [ Online ] . Available : https : / / doi . org / 10 . 5281 / zenodo . 6844387 [ 79 ] J . Wallace , J . McCarthy , P . C . Wright , and P . Olivier , “Making design probes work , ” in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems , 2013 , pp . 3441 – 3450 . [ 80 ] K . Boehner , J . Vertesi , P . Sengers , and P . Dourish , “How hci interprets the probes , ” in Proceedings of the SIGCHI conference on Human factors in computing systems , 2007 , pp . 1077 – 1086 . [ 81 ] I . Drosos , T . Barik , P . J . Guo , R . DeLine , and S . Gulwani , “Wrex : A unified programming - by - example interaction for synthesizing readable code for data scientists , ” in Proceedings of the 2020 CHI conference on human factors in computing systems , 2020 , pp . 1 – 12 . [ 82 ] M . B . Kery , D . Ren , F . Hohman , D . Moritz , K . Wongsuphasawat , and K . Patel , “mage : Fluid moves between code and graphical work in computational notebooks , ” in Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology , 2020 , pp . 140 – 151 . [ 83 ] B . Gaver , T . Dunne , and E . Pacenti , “Design : cultural probes , ” interactions , vol . 6 , no . 1 , pp . 21 – 29 , 1999 . [ 84 ] W . H . Gaver , B . Hooker , A . Dunne , and P . Farrington , “The presence project ( rca crd projects series ) , ” London : RCA Computer Related Design Research , pp . 21 – 52 , 2001 . [ 85 ] M . E . Fonteyn , B . Kuipers , and S . J . Grobe , “A description of think aloud method and protocol analysis , ” Qualitative health research , vol . 3 , no . 4 , pp . 430 – 441 , 1993 . [ 86 ] B . Granger and F . Pérez , “Jupyter : Thinking and storytelling with code and data , ” Authorea Preprints , 2021 . [ 87 ] J . M . Perkel , “Why jupyter is data scientists’ computational notebook of choice , ” Nature , vol . 563 , no . 7732 , pp . 145 – 147 , 2018 . [ 88 ] RStudios , “Rstudio : A stroke of innovation , ” 2022 , [ Online ; accessed 2022 - 04 - 06 ] . [ 89 ] Spyder , “Spyder : The scientific python development environment , ” 2018 , [ Online ; accessed 2022 - 04 - 06 ] . [ 90 ] R . Russo , G . Ward , H . Geurts , and A . Scheres , “When unfamiliarity matters : Changing environmental context between study and test affects recognition memory for unfamiliar stimuli . ” Journal of Experimental Psychology : Learning , Memory , and Cognition , vol . 25 , no . 2 , p . 488 , 1999 . [ 91 ] A . L . Nichols and J . K . Maner , “The good - subject effect : Investigating participant demand characteristics , ” The Journal of general psychology , vol . 135 , no . 2 , pp . 151 – 166 , 2008 . [ 92 ] C . Quirk , C . Brockett , and W . B . Dolan , “Monolingual machine translation for paraphrase generation , ” in Proceedings of the 2004 conference on empirical methods in natural language processing , 2004 , pp . 142 – 149 . [ 93 ] J . Ganitkevitch , B . Van Durme , and C . Callison - Burch , “Ppdb : The paraphrase database , ” in Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , 2013 , pp . 758 – 764 . [ 94 ] J . Guichard , E . Ruane , R . Smith , D . Bean , and A . Ventresque , “Assessing the robustness of conversational agents using paraphrases , ” in 2019 IEEE International Conference On Artificial Intelligence Testing ( AITest ) . IEEE , 2019 , pp . 55 – 62 . [ 95 ] A . Gupta , A . Agarwal , P . Singh , and P . Rai , “A deep generative framework for paraphrase generation , ” in Proceedings of the AAAI Conference on Artificial Intelligence , vol . 32 , 2018 . [ 96 ] S . Haiduc , J . Aponte , L . Moreno , and A . Marcus , “On the use of automated text summarization techniques for summarizing source code , ” in 2010 17th Working Conference on Reverse Engineering , 2010 , pp . 35 – 44 . [ 97 ] P . W . McBurney and C . McMillan , “Automatic source code summarization of context for java methods , ” IEEE Transactions on Software Engineering , vol . 42 , no . 2 , pp . 103 – 119 , 2015 . [ 98 ] E . Hill , L . Pollock , and K . Vijay - Shanker , “Automatically capturing source code context of nl - queries for software maintenance and reuse , ” in 2009 IEEE 31st International Conference on Software Engineering . IEEE , 2009 , pp . 232 – 242 . [ 99 ] E . Reiter and R . Dale , “Building applied natural language generation systems , ” Natural Language Engineering , vol . 3 , no . 1 , pp . 57 – 87 , 1997 . [ 100 ] J . Krinke , “Effects of context on program slicing , ” Journal of Systems and Software , vol . 2006 , 2006 . Manuscript submitted to ACM 24 Chattopadhyay , et al . APPENDIX 8 . 1 Classification of Code Functionality Load ( L ) Import / generate ( L1 ) String operations ( PP0 ) Summary ( ST1 ) Distribution ( V1 ) NLP operations ( S1 ) Fetch / Load ( L2 ) Tidying data ( PP1 ) Measure ( ST2 ) Relational ( V2 ) Querying ( S2 ) Parsing ( L3 ) Transforming data ( PP2 ) Plot ( ST3 ) Comparative ( V3 ) Math / sci ( S3 ) Export ( L4 ) Formatting data ( PP3 ) Test ( ST4 ) Modify visualization ( V4 ) Domain specific function ( S4 ) Pre - Process ( PP ) Statistics ( ST ) Visualization ( V ) Domain Specific Functions ( S ) Prep ( ML1 ) Train ( ML2 ) Test ( ML3 ) Verify ( ML4 ) Machine Leraning ( ML ) Summary ( PP4 ) Data inspection ( PP5 ) Summary ( PP4 ) Model ( ST5 ) ML visualization ( V5 ) Image processing ( S5 ) Clustering ( ML5 ) Featuring ( ML6 ) Tuning ( ML7 ) Special ( ML8 ) Fig . 7 . Classification of Code Functionality ( snippet ) : color cells indicate the category’s name ; grey cell shows examples of function calls under each category . Figure 7 displays the functional categories of encoding transforms . Each line of the notebook is converted into a list ( 𝐻 1 - 𝐻 7 ) , as discussed in Section 3 of the paper . The 39 categories emerged from the first round of inductive coding between two researchers across all 35 notebooks and 605 function calls . For example , 𝑝𝑎𝑛𝑑𝑎𝑠 . 𝑟𝑒𝑎𝑑 _ 𝑐𝑠𝑣 ( ) is a function call that loads data and categorizes it as a ‘Load ( L2 ) ’ functionality . Another example is 𝑡 . 𝑡𝑒𝑠𝑡 ( ) , which we categorized as the ‘Statistical Test ( ST4 ) ’ functionality . These categories naturally fell into seven broader categories : Load ( L ) , Domain Specific Functions ( S ) , Pre - Processing ( PP ) , Visualization ( V ) , Machine Learning ( ML ) , Statistics ( ST ) , and Others ( O ) . The researchers met to negotiate their disagreements [ 77 ] during inductive coding , which resulted in re - assigning categories to some functions and restructuring six categorizations ( L3 −→ PP5 , S3 −→ L3 , S4 −→ S3 , S5 −→ S4 , S6 −→ S5 , S0 −→ PP0 ) and removing the ‘Other’ category as it wasn’t indicating any meaningful grouping of functionality . The final categories of all observed functions are listed in Figure 7 , and definitions of each category and functions belonging to them are provided in the supplementary materials [ 78 ] . Manuscript submitted to ACM Make It Make Sense ! 25 8 . 2 Code Purpose from Functionality Patterns Code Purpose Description (cid:246) Libraries The process of configuring a programming system libraries .  system setup Managing system resources such as files , directories , and environment variables  Data loading Loading multiple data sources or generating a population , and tuning the data right away based on what they know about the data . N Data generation Making new data ( quering database ) N Initial wrangling Tuning the data based on internal knowledge / assumptions about the data . N Domain specific wrangling * Loading and processing specific types of data like image , audio , geographical , or text . Involves specific libraries and specific functional transformations . œ Saving intermediate progress Saving a model state with parameters , or saving transformed data for reuse . (cid:19) Visual Exploration of Data Space Exploratory visual inspection of the data right after loading . T Data transform Converting and modifying raw data into a structured format that is suitable for analysis and modeling . T Data transform - Inspection based transformation Data is processed / changed , inspected in textual or visual form , followed by more change to the data . T Data transform - Summary based transformation Inspection of summary measures that leads to certain transformations . T Data transform - Pre - model transformation Doing data transformation to fit upcoming model T Data transformation verification Building functions and transformations with interleaved data check along the way . T Summary based transformation Inspection of summary measures that leads to certain transformations .  Pre - model inspection of data Checking the shape and elements of data , in textual or graphical format , right before feeding it into the model .  Output verification Multiple format inspection of a model’s output or a transformation . Often also involves checking the statistical summary .  Visual inspection Looking at data / table / plot to inspect outpuit  Inspection based scientific coding Building functions and transformations with interleaved data check along the way .  Model output inspection Using just a print of the metrics or the visualization of the model’s output . 3 Model selection - AutoML by hand Selecting among multiple models . Typically either all models are initiated at the same time , or they are run one after the other . 3 Model selection - Feeback based Select a different model based on the output metrics of the model . (cid:1) Model refinement Improving the accuracy and performance of a machine learning model by adjusting its parameters , optimizing hyperparameters , selecting better features , or trying out different algorithms (cid:1) Model refinement - parameter turnning When the same model is refined by changing the parameters , or model structure , based on the result of the model’s output . (cid:1) Model refinement - Best practice When standard processes are followed to tackle specific performance issues with the model . These are handed down cookbook steps to take e . g . use ensemble to reduce error in regression . (cid:1) Model refinement - Prior result based Refining model based on previous results of the model (cid:1) Model refinement - Input tuning Standard processing of data after finding data related issues causing model performance to deteriorate ” Generic modeling Typical cycle of define , train , test of an ML model . ” Modelling with defaults Models based on the team / organization , or models that were pre - trained , or with default parameters with no tuning / training  Statistical Modeling Defining and executing a statistical model or performing statistical testing . Table 3 . Code Purpose from Functionality Patterns in Notebooks To identify functionality patterns , for each line of code , we extracted the sequence of categories ( 𝐻 1 ) based on their linear order . Then , we looked at permutations and nested functions , for instance , 𝑛𝑝 . 𝑚𝑒𝑎𝑛 ( 𝑎𝑐𝑐𝑢𝑟𝑎𝑐𝑦 ( 𝑦 𝑡𝑒𝑠𝑡 , 𝑝𝑟𝑒𝑑 ) performs a summary ( ST1 ) on the output of a machine learning model ( ML4 ) . We defined the rule to infer processes Manuscript submitted to ACM 26 Chattopadhyay , et al . from nested calls in execution order ( 𝐻 2 , 𝐻 3 ) . We can transform notebook code into a list of categories using these three heuristics ( 𝐻 1 - 𝐻 3 ) . Loading data ( category L ) in any form indicates the start of a part of code written for a specific purpose ( 𝐻 4 ) , and getting any form of output like visualizations ( V ) or of a model ( ML4 ) indicates the end of a purpose ( 𝐻 5 ) . Repeated subsequences were investigated to identify meaning ( 𝐻 6 ) . Finally , if adjacent lines of code performed the same function , they were all grouped into one process , and the sequence was instead counted as 1 : PP3 ( 𝐻 7 ) . Using these heuristics ( 𝐻 4 - 𝐻 7 ) , we process the notebooks into an encoded list of categories that reflect the ‘purpose’ or functionality of the code . Figure 7 lists all the patterns in functionally we identified across all the notebooks . For a detailed description of the process of the code purpose , please refer to the supplementary material [ 78 ] . Manuscript submitted to ACM