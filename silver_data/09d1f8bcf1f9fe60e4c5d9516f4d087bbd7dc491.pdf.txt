To appear in KR ' 96 Representing Sensing Actions : The Middle Ground Revisited Keith Golden Daniel Weld (cid:3) Department of Computer Science and Engineering University of Washington Seattle , WA 98195 fkgolden , weldg @ cs . washington . edu Abstract To build e(cid:11)ective planning systems , it is crucial to (cid:12)nd the right level of representa - tion : too impoverished , and important ac - tions and goals are impossible to express ; too expressive , and planning becomes in - tractable . Within the classical framework , Pednault ' s adl [ 24 ] provided a happy com - promise between the impoverished strips representation and the expensive situation calculus . Among languages handling sensing actions and information goals , there is a similar spec - trum of expressiveness . uwl , an extension of strips , can ' t express goals like \ Rename the (cid:12)le paper . tex to kr . tex . " Nor can it repre - sent universally quanti(cid:12)ed goals or e(cid:11)ects . At the other extreme are elegant languages [ 22 , 21 , 17 ] for which e(cid:11)ective planners do not ex - ist . In this paper , we combine elements of uwl and adl , to de(cid:12)ne sadl : a middle - ground representation for sensing actions . Under - lying our language are two insights , miss - ing from uwl : 1 ) Knowledge goals are in - herently temporal . 2 ) Knowledge precondi - tions are unnecessary for an important class of domains ( those obeying a Markov prop - erty ) . sadl is expressive enough to encode the rich domain theory of the Internet Soft - bot , including hundreds of UNIX and Inter - net operators ; yet it supports tractable infer - ence by planners such as xii [ 11 , 10 ] . (cid:3) Many thanks to Mark Boddy , Bob Doorenbos , Oren Etzioni , Marc Friedman , Robert Goldman , Neal Lesh , Greg Linden , Mike Perkowitz , Rich Segal , Jonathan Shakes and Ellen Spertus for helpful comments . This research was funded in part by O(cid:14)ce of Naval Research Grant N00014 - 94 - 1 - 0060 , by National Science Foundation Grant IRI - 9303461 , by ARPA / Rome Labs grant F30602 - 95 - 1 - 0024 , by a gift from Rockwell International Palo Alto Re - search , and by a Microsoft Graduate Fellowship 1 INTRODUCTION One of the stumbling blocks to past research in plan - ning with incomplete information has been inadequate or imprecisely de(cid:12)ned languages for representing infor - mation goals and sensing actions . Many researchers have devised formalisms for reasoning about knowl - edge and action [ 21 , 22 , 23 , 5 , 3 , 32 , 17 ] , but those languages are too expressive to be used in practical planning algorithms . uwl [ 9 ] o(cid:11)ered a more tractable representation ( based on strips ) that was tailored to current planning technology , but as Levesque [ 17 ] ob - serves , the semantics of uwl are unclear | the de(cid:12) - nitions were made relative to a speci(cid:12)c planning algo - rithm . In our e(cid:11)orts to de(cid:12)ne a semantics for uwl , we determined that uwl confused information goals with maintenance goals , and con(cid:13)ated knowledge goals with knowledge preconditions . Furthermore , years of experience with uwl convinced us that it wasn ' t ex - pressive enough to fully handle the real - world domains ( e . g . , unix and the Internet ) for which it was intended . Since uwl didn ' t support universal quanti(cid:12)cation or conditional e(cid:11)ects , it could not correctly represent the unix command ls , which lists all (cid:12)les in a directory , or rm * , which deletes all writable (cid:12)les . Information Expressiveness (cid:0) ! Complete strips adl Situation Calculus Incomplete uwl SADL Moore et al In this paper , we de(cid:12)ne a new action representation language , sadl , 1 that combines ideas from uwl with those from Pednault ' s adl [ 26 , 24 ] . Just as adl marked the \ middle ground " on the tractability spec - trum between strips and the situation calculus , sadl o(cid:11)ers an advantageous combination of expressiveness and e(cid:14)ciency . Since sadl supports universally quanti - (cid:12)ed information goals and universally quanti(cid:12)ed , con - ditional , observational e(cid:11)ects , it is expressive enough to represent hundreds of unix and Internet commands . 1 sadl ( pronounced \ Saddle " ) stands for \ Sensory Ac - tion Description Language . " Indeed , four years of painful experience writing and debugging the Internet Softbot [ 7 ] knowledge base forced us to uncover and remedy some subtle confu - sions about information goals : (cid:15) In a dynamic world , knowledge goals are inher - ently temporal | If proposition P is true at one time point and false in another , which time point do we mean when we ask about P ' s truth value ? Since uwl has limited provision to make tem - poral distinctions , it cannot encode an impor - tant class of goals . In particular , uwl cannot express goals that require causal change to at - tributes used to designate objects , e . g . \ Rename the (cid:12)le paper . tex to kr . tex . " ( See Sections 2 . 2 and 2 . 3 for the sadl solution ) (cid:15) We identify a large class of domains , called Markov domains , and argue that actions in these domains are best encoded without knowledge pre - conditions . The multiagent scenarios that in - spired Moore , Morgenstern , and others are not Markov , but UNIX and much of the Internet are . While sadl discourages knowledge precon - ditions it recognizes the need for knowledge sub - goals . ( Section 2 . 4 elaborates ) . 1 . 1 ROADMAP Section 2 describes problems with the uwl formula - tion of knowledge goals and presents the sadl so - lution . In Section 3 we discuss observational e(cid:11)ects of actions , and causal e(cid:11)ects , which can decrease the agent ' s knowledge about the world . We also demon - strate the representational adequacy of sadl by pre - senting an encoding of the UNIX ls - a command . In Section 4 we discuss temporal projection in sadl . In Section 5 we demonstrate that the sadl formalism is expressive enough to represent many interesting ac - tions . Section 6 argues that sadl ' s expressive power comes at a reasonable price | reasoning is tractable . We conclude with a discussion of related work in Sec - tion 7 and a summary in Section 8 . 2 KNOWLEDGE GOALS AND PRECONDITIONS In uwl , preconditions and goals were limited to con - junctions of literals , each annotated with one of three tags : satisfy , hands - o(cid:11) , and (cid:12)nd - out . The sadl action language is based on uwl , but uses a di(cid:11)er - ent set of annotations : satisfy , hands - o(cid:11) , and ini - tially , which provide a cleaner semantics for informa - tion goals and greater expressive power ; additionally , sadl uses unannotated literals to designate precon - ditions that don ' t depend on the agent ' s knowledge . Furthermore , sadl supports universal quanti(cid:12)cation and conditional e(cid:11)ects , both of which have interesting rami(cid:12)cations in the context of incomplete information . We proceed by reviewing uwl , uncovering some con - fusions , presenting the sadl solution , and sketching the formal semantics . In uwl ( and in sadl ) individual literals have truth values expressed in a three - valued logic : T , F , U ( un - known ) . Free variables are implicitly existentially quanti(cid:12)ed , and the quanti(cid:12)er takes the widest pos - sible scope . 2 For example , satisfy ( in . dir ( f , tex ) , T ) 3 means \ Ensure that there ' s at least one (cid:12)le in direc - tory tex . " Truth values can also be represented by variables . For example , satisfy ( in . dir ( myfile , tex ) , tv ) means \ Find out whether or not myfile is in tex . " Although the semantics of uwl was de(cid:12)ned procedu - rally [ 9 ] , we provide sadl ' s semantics in terms of the situation calculus . The situation calculus [ 19 ] is a (cid:12)rst - order logic used to capture changes to the world that come about by the execution of actions . A (cid:13)uent is a proposition whose truth value changes over time . Ev - ery (cid:13)uent , ' ( x ) , takes an additional argument , namely a situation , s . ' ( x ; s ) represents the statement that ' ( x ) holds in situation s . By convention , s is always the last argument of ' , so we will freely add or drop the s , depending on whether we are referring to ' in a particular situation . Thus , if in . dir ( f ; d ) means (cid:12)le f is in directory d , in . dir ( f ; d ; s ) means this fact holds in situation s . All state changes are assumed to result from the execution of actions . The special function DO is used to describe these changes : DO ( a ; s ) returns the situation resulting from executing action a in situation s . We use fag n 1 to represent the sequence of actions a1 ; a2 ; : : : ; an . DO ( fag n 1 ; s ) denotes nested applica - tion DO ( an ; DO ( an(cid:0)1 ; : : : ; DO ( a1 ; s ) ) ) , i . e . , the result of executing the entire sequence , starting in situation s . We use sn as a shorthand for DO ( fag n 1 ; s0 ) . Our formulation of sadl is based on Scherl and Levesque ' s [ 32 ] solution to the frame problem for knowledge - producing actions . We adopt their com - pleteness assumptions , and their formulation of incom - plete knowledge , and thus their results ( i . e . the per - sistence of knowledge and of ignorance ) hold for us as well . Incomplete knowledge is de(cid:12)ned in terms of the standard possible - worlds semantics , where K ( s 0 ; s ) means that if the situation is s , then it is con - sistent with the agent ' s knowledge to believe that the situation could in fact be s 0 . In other words , fs 0 jK ( s 0 ; s ) g denotes the set of all possible worlds con - sistent with the agent ' s knowledge in situation s . We assume that an agent ' s knowledge is correct , so the actual situation is always considered possible by the agent ( 8s : K ( s ; s ) ) , and we assume that situations only change when the agent executes an action . We de(cid:12)ne KNOW ( ' ; s ) def = 8s 0 : K ( s 0 ; s ) ) ' ( s 0 ) , i . e . , ' is true in all worlds consistent with the agent ' s knowledge . 2 Explicit quanti(cid:12)ers can be used to indicate a narrower scope . 3 For notational convenience , an omitted truth value de - faults to T , so this could be rewritten as satisfy ( in . dir ( f , tex ) ) . We use this shorthand in the remainder of the paper . Italicized lower - case symbols , such as f , denote variables . Symbols in typewriter font denote constants . Annota - tions are in bold . As we mentioned , sadl uses a three - valued logic ( T , F , U ) to represent knowledge . The relation between these truth values and KNOW is straightforward . If ' has the truth value T , then KNOW ( ' ) . If ' has the truth value F , then KNOW ( : ' ) . If the truth value is U , then : KNOW ( ' ) ^ : KNOW ( : ' ) . 2 . 1 SATISFACTION AND MAINTENANCE GOALS The goal satisfy ( P ) indicates a traditional goal ( as in adl ) : achieve P by whatever means possible . In the presence of incomplete information , we make the further requirement that the agent knows that P is true . We de(cid:12)ne GOAL ( G ; s0 ; fag n 1 ) to mean that goal G is achieved in the situation resulting from executing plan fag n 1 in situation s0 ; since we assume the agent ' s knowledge is correct , it is su(cid:14)cient to state that the agent knows P : GOAL ( satisfy ( P ; T ) ; s0 ; fag n 1 ) def = KNOW ( P ; sn ) ( 1 ) GOAL ( satisfy ( P ; F ) ; s0 ; fag n 1 ) def = KNOW ( : P ; sn ) ( 2 ) GOAL ( satisfy ( P ; tv ) ; s0 ; fag n 1 ) def = KNOW ( P ; sn ) _ KNOW ( : P ; sn ) ( 3 ) Note that when given an ( existentially quanti(cid:12)ed ) vari - able as truth value , a satisfy goal requires that the agent learn whether the proposition is true or false ( which could be achieved by making it true or false ) . Equation 3 is a slight simpli(cid:12)cation ; if several (cid:13)uents in a goal use the same variable , tv , then they should all have the same truth value . The above de(cid:12)nition fails to capture such correlations . We don ' t discuss corre - lated truth values in this paper , so for clarity , we omit these variable constraints in the remainder of the pa - per . However , we show them below for satisfy goals . Variable constraints in the other de(cid:12)nitions follow the same form : GOAL ( satisfy ( P ; tv ) ; s0 ; fag n 1 ) def = KNOW ( P ^ tv = T ; sn ) _ KNOW ( : P ^ tv = F ; sn ) ( 4 ) The hands - o(cid:11) annotation indicates a maintenance goal that prohibits the agent from changing the (cid:13)u - ent in question . GOAL ( hands - o(cid:11) ( P ) ; s0 ; fag n 1 ) def = 8s 2 State - History : [ P ( s ) , P ( s0 ) ] ( 5 ) By State - History we mean the set of n + 1 situations produced during execution of DO ( fag n 1 ; s0 ) ( including both s0 and sn ) . Thus , the de(cid:12)nition of hands - o(cid:11) requires that P not change value during execution of the plan . Etzioni et al [ 9 ] noted that together , satisfy + hands - o(cid:11) can be used to indicate a \ look but don ' t touch " goal : the agent may sense the (cid:13)uent ' s value , but is forbidden to change it . While hands - o(cid:11) goals are clearly useful , we argue that they are an overly restrictive way of specifying knowledge goals . In particular , they outlaw changing the value of a (cid:13)uent after it has been sensed . 2 . 2 KNOWLEDGE GOALS ARE INHERENTLY TEMPORAL Before explaining the sadl approach to knowledge goals , we discuss the uwl (cid:12)nd - out annotation . (cid:12)nd - out is problematic because the original de(cid:12)nition was in terms of a particular planning algorithm [ 9 ] . The motivation for (cid:12)nd - out was the existence of goals for which hands - o(cid:11) is too restrictive , but satisfy alone is too permissive . For example , given the goal \ Tell me what (cid:12)les are in directory tex , " executing rm tex / * and reporting \ None " would clearly be inappro - priate . But what about the conjunctive goal \ Free up some disk space and tell me what (cid:12)les are in directory tex " ? In this case excluding the rm seems inappro - priate , since it may be necessary in service of freeing disk space . Yet the knowledge that the directory is now empty is relevant to the information goal . Pro - ponents of (cid:12)nd - out argued that rm was unacceptable for the (cid:12)rst goal , but acceptable in service of the con - junction [ 9 ] . We contend that this de(cid:12)nition is unclear and unacceptable ; a plan that satis(cid:12)es the conjunction A ^ B should also be a solution to A . While the examples used to justify the original (cid:12)nd - out de(cid:12)nition are evocative , their persuasive powers stem from ambiguity . At what time point do we wish to know the directory contents ? Before freeing disk space , afterward , or in between ? Since (cid:13)uents are always changing , a general information goal requires two temporal arguments : the time a (cid:13)uent is sensed , and the time the sensed value is to be reported . E . g . , one can ask \ Who was president in 1883 , " or \ Tell me tomorrow who was president today . " Since planning with an explicit temporal representa - tion is slow , our quest for the \ middle ground " along the expressiveness / tractability spectrum demands a minimal notion of time that captures most common goals . We limit consideration to two time points : the time when a goal is given to the agent , and the time the agent gives his reply . Note that satisfy ( P ; tv ) ( Equa - tion 3 ) allows one to specify the goal of knowing P ' s truth value at this latter time point . To specify the goal of sensing a (cid:13)uent at the time the goal is given , we introduce the annotation initially . GOAL ( initially ( P ; tv ) ; s0 ; fag n 1 ) def = [ 8s2ORIGn : P ( s ) ] _ [ 8s2ORIGn : : P ( s ) ] ( 6 ) We use ORIGn ( Figure 1 ) to represent the agent ' s knowledge in sn about the past situation s0 , i . e . , the set of situations indistinguishable from s0 after execution of the plan : ORIGn = fs j K ( DO ( fag n 1 ; s ) ; DO ( fag n 1 ; s0 ) ) g . Thus the def - inition of initially states that when the agent has (cid:12)nished executing the plan , he will know whether P was true or false when he started . initially ( P ) is not achievable by an action that changes the (cid:13)uent P , since such an action only obscures the initial value of P . However , changing P after determining its initial value i i i i i i i i i - - - - - - - - - . . . . . . . . . a 1 a 2 an a 1 a 2 an a 1 a 2 an i i i i i i i i i i i i - - - - - - - - - - - - . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . s 0 a 1 a 2 an a 1 a 2 an a 1 a 2 an a 1 a 2 an sn ORIG K ( s ; s 0 ) K ( s ; sn ) Figure 1 : The region surrounded by dotted lines rep - resents the set ORIGn , the set of states indistinguish - able from s0 , based on the agent ' s knowledge in state sn . ORIGn is a subset of fs j K ( s ; s0 ) g , the states that were consistent with the agent ' s knowledge in s0 , since the agent has learned more about what originally held , but has not forgotten anything it knew originally . is (cid:12)ne . By combining initially with satisfy we can express \ tidiness " goals ( modify P at will , but restore its initial value by plan ' s end ) [ 35 ] . Furthermore , we can express goals such as \ Find the the (cid:12)le currently named paper . tex , and rename it to kr . tex , " which are impossible to express in uwl . Since uwl can ' t make temporal distinctions , there is no way to ask for the past value of a (cid:13)uent without also requiring that the (cid:13)uent have the same value when the reply is given , so any goal of the form \ Find some x such that P ( x ) , and make P ( x ) false " is inexpressible in uwl . 2 . 3 UNIVERSALLY QUANTIFIED GOALS When de(cid:12)ning universally quanti(cid:12)ed goals , one must again be speci(cid:12)c with respect to time points : does the designator specifying the Herbrand universe refer to s0 or sn ? Since sadl allows an arbitrary goal description to be used to scope a universally quanti(cid:12)ed goal , one can specify a wide range of requests . For example , suppose an agent is given the goal of seeing to it that all (cid:12)les in directory tex are compressed . What plans satisfy the goal ? It depends on what the request really means . In sadl , one can write one of the following precise versions , thus eliminating the ambiguity . 1 . Ensure that all (cid:12)les , which were initially in tex , end up being compressed : 8f initially ( in . dir ( f , tex ) ) ) satisfy ( compressed ( f ) ) . Executing compress tex / * solves this goal , as does execut - ing mv tex / * temp then compress temp / * . 2 . Ensure that all (cid:12)les , which end up in tex , end up being compressed : 8f satisfy ( in . dir ( f , tex ) ) ) satisfy ( compressed ( f ) ) . Executing compress tex / * solves this goal , but so does rm tex / * ! 3 . Determine if all (cid:12)les , initially in tex , were ini - tially compressed : 8f initially ( in . dir ( f , tex ) ) ) initially ( compressed ( f ) ) . 4 . Determine if all (cid:12)les , in tex at the end of execu - tion , were initially compressed : 8f satisfy ( in . dir ( f , tex ) ) ) initially ( compressed ( f ) ) . This is equivalent to 8f initially ( compressed ( f ) , F ) ) satisfy ( in . dir ( f , tex ) , F ) , i . e . ensure that all (cid:12)les not initially compressed do not end up in tex . The (cid:12)rst example seems the most likely interpreta - tion of the goal in this case , but it still leaves some - thing to be desired , since the user may not want the (cid:12)les moved from tex . We can easily state the additional requirement that the (cid:12)les not be moved ( hands - o(cid:11) ( in . dir ( f , tex ) ) ) , or that they be returned to tex by the end ( satisfy ( in . dir ( f , tex ) ) ) . We should be careful not to make goals overly restrictive , though . If the desire is that the agent should fail if there ' s no way to compress the (cid:12)les without mov - ing them , then adding such restrictions is correct . If the desire is merely that the agent should avoid mov - ing the (cid:12)les unnecessarily , then we want the original solution , with some background preference to mini - mize unnecessary changes . Such background prefer - ences could be expressed in terms of a utility function over world states [ 30 ] , a measure of plan quality [ 28 , 36 ] , or an explicit notion of harm [ 35 ] . Note that even if we decide to forbid moving the (cid:12)les from tex , there are still other actions , such as deleting all the (cid:12)les in important / papers , or sending threaten - ing email to president @ whitehouse . gov that haven ' t been excluded . This is a general problem with sat - is(cid:12)cing plans : anything goes as long as the goal is achieved . Specifying all the undesired outcomes with every goal would be tedious and error - prone . A better solution is to separate the criteria of goal satisfaction from background preferences , as is done in [ 37 , 13 , 35 ] . Given the appropriate annotations on (cid:13)uents , which provide temporal information , the semantics of 8 goals is straightforward : GOAL ( 8 ~ x : P ; s0 ; fag n 1 ) def = 8 ~ x : GOAL ( P ; s0 ; fag n 1 ) ( 7 ) GOAL ( P ) Q ; s0 ; fag n 1 ) def = GOAL ( P ; s0 ; fag n 1 ) ) GOAL ( Q ; s0 ; fag n 1 ) ( 8 ) Logical operators such as ^ , _ , and 9 follow the same form as above . 2 . 4 KNOWLEDGE PRECONDITIONS CONSIDERED HARMFUL Moore [ 21 ] identi(cid:12)ed two kinds of knowledge precon - ditions an agent must satisfy in order to execute an action in support of some proposition P : First , the agent must know a rigid designator ( i . e . , an unambigu - ous , executable description ) of the action . Second , the agent must know that executing the action will in fact achieve P . Subsequent work , e . g . [ 22 ] , general - ized this framework to handle scenarios where multiple agents reasoned about each other ' s knowledge . In the interest of tractability , we take a much narrower view , assuming away Moore ' s (cid:12)rst type of knowledge precondition and refuting the need for his second type . Our argument occupies the remainder of this section , but the summary is that there is a large class of do - mains , those obeying a Markov property , for which ac - tions are best encoded without knowledge precondi - tions . While the multiagent scenarios considered by Moore and Morgenstern are not Markov , UNIX and much of the Internet are . We start the argument by assuming away Moore ' s (cid:12)rst type of knowledge precondition . We de(cid:12)ne ac - tions as programs that can be executed by a robot or softbot , without the need for further reason - ing . In this view , all actions are rigid designators . dial ( combination ( safe ) ) is not an admissible action , but dial ( 31 - 24 - 15 ) is . Lifted action schemas , e . g . dial ( x ) , are not rigid designators , but it is easy to produce one by substituting a constant for x . Thus Moore ' s (cid:12)rst type of knowledge precondition vanishes . Moore ' s second type of knowledge precondition pre - supposes that an action in a plan must provably suc - ceed in achieving a desired goal . This is a standard assumption in classical planning , but is overly restric - tive given incomplete information about the world ; en - forcing this assumption by adding knowledge precon - ditions to actions is inappropriate . For example , if knowledge of the safe ' s combination is a precondition of the dial action , then it becomes impossible for a planner to solve the goal \ (cid:12)nd out whether the com - bination is 31 - 24 - 15 " by dialing that number , since before executing the dial action , it will need to sat - isfy that action ' s precondition of (cid:12)nding out whether 31 - 24 - 15 is the right combination ! 4 On the other hand , it is often necessary for an agent to plan to obtain information , such as the combination of a safe , either to reduce search or to avoid dangerous mistakes . These knowledge subgoals , naturally , have a temporal component , but the only time point of inter - est is the moment the action is executed . For example , the goal of knowing the safe ' s combination could be satis(cid:12)ed by watching another agent open the safe , but it might also be satis(cid:12)ed by changing the combination to some known value ( for instance , at some earlier time when the safe is open ) . We say that an action is Markov if its e(cid:11)ects depend only on the state of the world at the time of execution . Note that simple mechanical and software systems are naturally encoded as Markov , while multiagent sys - 4 Note that eliminating the knowledge precondition from the dial action also allows the unhurried agent to devise a plan to enumerate the possible combinations until he (cid:12)nds one that works . Indeed , the Internet Softbot [ 7 ] follows an analogous strategy when directed to (cid:12)nd a particular user , (cid:12)le or a web page , whose location is unknown . If finger and ls included knowledge preconditions , then the actions would be useless for locating users and (cid:12)les . tems are typically not , because it is useful to endow one ' s model of another agent with state ( i . e . , I know that Bill knew : : : ) . If all actions in a domain are Markov , then all knowledge sub - goals will be of the same form : 1 ) The agent needs to know the value of some (cid:13)uent at the time the action is to be executed , and 2 ) it doesn ' t matter if the agent a(cid:11)ects the (cid:13)u - ent while obtaining its value . 5 These requirements for knowledge sub - goals are met by the sadl de(cid:12)nition of satisfy ( Equation 3 ) , 6 if we regard the action sequence fag n 1 as a plan to achieve the preconditions of action an + 1 . The Markov assumption for actions yields a substan - tially simpler representation of change than those de - (cid:12)ned by Moore and Morgenstern . While their theo - ries are more appropriate for complex , multi - agent do - mains , sadl gains tractability while retaining enough expressive power to model many important domains . 3 EFFECTS Like uwl , sadl divides e(cid:11)ects into those that change the world , annotated by cause , and those that merely report on the state of the world , annotated by observe . Because it lacked universal quanti(cid:12)cation , uwl couldn ' t even correctly model UNIX ls . sadl goes beyond uwl by allowing both observational and causal e(cid:11)ects to have universal quanti(cid:12)cation and sec - ondary preconditions . 3 . 1 OBSERVATIONAL EFFECTS Executing actions with observational e(cid:11)ects assigns values to runtime variables that appear in those ef - fects . By using a runtime variable as a parameter to a later action ( or to control contingent execution ) , infor - mation gathered by one action can a(cid:11)ect the agent ' s subsequent behavior . Inside an e(cid:11)ect , runtime vari - ables ( syntactically identi(cid:12)ed with a leading an ex - clamation point , e . g . ! tv ) can appear as terms or as truth values . For example , ping twain has the ef - 5 The reader may object that ( nonrigid ) indexical ref - erences could appear as preconditions to actions . For ex - ample , suppose that running Netscape requires that the (cid:12)le netscape . bookmarks be in a given directory . It is not su(cid:14)cient that a (cid:12)le of that name be there , because re - naming paper . tex to netscape . bookmarks would cause Netscape to fail . But this example makes it clear that the proposed preconditions of Netscape are simply under - speci(cid:12)ed . They should be \ The directory contains a (cid:12)le named netscape . bookmarks , which is a valid bookmarks (cid:12)le , and : : : " This is just the quali(cid:12)cation problem [ 18 ] in disguise . Granted , it will usually be impossible ( or unde - sirable ) to model all such preconditions . 6 A justi(cid:12)cation that might be given for initially or hands - o(cid:11) preconditions is to minimize destructive actions used by an agent to satisfy a goal ( i . e . don ' t use mv to (cid:12)nd out the name of a (cid:12)le ) . We agree on the need for reasoning about plan quality , but an accurate theory of action should distinguish action preconditions from user preferences . fect of observe ( machine . alive ( twain ) , ! tv ) , i . e . deter - mining whether it is true or false that the machine named twain is alive , and wc myfile has the e(cid:11)ect observe ( word . count ( myfile , ! word ) ) , i . e . determin - ing the number of words in myfile . Before we de(cid:12)ne individual e(cid:11)ects , we discuss what it means to execute an action , with all its e(cid:11)ects . Let EFF ( E ; a ; s ) denote the fact that E becomes true after action a is executed in s , let (cid:25) a be the precondition of action a , and let " a be the e(cid:11)ects . An action ' s e(cid:11)ects will only be realized if the action is executed when its preconditions are satis(cid:12)ed . Furthermore , the agent always knows when it executes an action , and it knows the e(cid:11)ects of that action . Following Moore [ 21 ] : 8s : GOAL ( (cid:25) a ; s ; fg ) ) 8s 00 : [ K ( s 00 ; DO ( a ; s ) ) , 9s 0 : K ( s 0 ; s ) ^ s 00 = DO ( a ; s 0 ) ^ EFF ( " a ; a ; s ) ] ( 9 ) The fact that the agent knows the e(cid:11)ects of a doesn ' t imply that e(cid:11)ects are always certain . As we discuss in Section 3 . 3 , actions with conditional e(cid:11)ects can result in uncertainty . We now de(cid:12)ne the semantics of observe in terms of primitive situation calculus expressions : EFF ( observe ( P ; T ) ; a ; s ) def = 8s 0 : K ( s 0 ; DO ( a ; s ) ) ) 9si : K ( si ; s ) ^ s 0 = DO ( a ; si ) ^ P ( si ) ( 10 ) EFF ( observe ( P ; tv ) ; a ; s ) def = 8s 0 : K ( s 0 ; DO ( a ; s ) ) ) 9si : K ( si ; s ) ^ s 0 = DO ( a ; si ) ^ ( P ( si ) , P ( s ) ) ( 11 ) In other words if action a has an observe e(cid:11)ect and is executed in situation s , then in the resulting sit - uation , the agent knows more about the value that P had in s . For example , if in s the agent observes that the sky is blue , we would say that in situation s 0 = DO ( look ; s ) , the agent knows that the sky was blue in situation s . The double use of the K operator in Equations 9 and 10 is a tri(cid:13)e redundant given only a single observational e(cid:11)ect . Indeed , if we assume pos - itive introspection ( i . e . K is transitive ) , as in the S4 logic , the resulting equation can be greatly simpli(cid:12)ed . However , in more complex e(cid:11)ects , we wish to distin - guish between the agent knowing that the e(cid:11)ect as a whole took place , and knowing the value of a single (cid:13)uent . sadl supports universally quanti(cid:12)ed run - time vari - ables . By nesting universal and existential quanti(cid:12)ers , sadl can model powerful sensory actions that pro - vide several pieces of information about an unbounded number of objects . For example , ls - a , ( Figure 2 ) , reports several facts about each (cid:12)le in the current di - rectory . The universal quanti(cid:12)er indicates that , at ex - ecution time , information will be provided about all (cid:12)les ! f which are in directory d . Since the value of ! f is observed , quanti(cid:12)cation uses a run - time variable . The nested existential quanti(cid:12)er denotes that each (cid:12)le has action ls ( d ) precond : satisfy ( current . shell ( csh ) ) ^ satisfy ( protection ( d , readable ) ) ^ effect : 8 ! f when in . dir ( ! f , d ) 9 ! p , ! n observe ( in . dir ( ! f , d ) ) ^ observe ( pathname ( ! f , ! p ) ) ^ observe ( name ( ! f , ! n ) ) Figure 2 : UNIX action schema . The sadl ls ac - tion ( UNIX ls - a ) to list all (cid:12)les in the a directory . a distinct (cid:12)lename and pathname . The conditional when restricts the (cid:12)les sensed to those in directory d . The fact that the in . dir relation appears in two places may seem odd , but as we shall explain , the (cid:12)rst use of in . dir refers to the actual situation s , whereas the sec - ond refers to the agent ' s knowledge ( i . e . , all possible situations ) . It is useful to note that after executing ls - a tex , the agent not only knows all (cid:12)les in tex ; she knows that she knows all (cid:12)les ( i . e . , she has closed world knowledge on the contents of tex ) . Because of the 8 in the e(cid:11)ects of ls , and since she knows the ef - fects of ls , the agent can infer closed - world knowl - edge . Such inference would be costly if it were done using (cid:12)rst - order theorem - proving in the situa - tion calculus . We have devised e(cid:14)cient algorithms for doing this reasoning , which we describe in [ 6 , 8 ] . The translation of 8 e(cid:11)ects into the situation calculus is straightforward ( Other logical operators follow the same form ) : EFF ( 8 ~ x : E ; a ; s ) def = 8 ~ x : EFF ( E ; a ; s ) ( 12 ) This de(cid:12)nition of 8 e(cid:11)ects may seem anticlimactic . The magic , however , stems from the way in which when introduces secondary preconditions ; these are required for 8 e(cid:11)ects , where the when clause restricts the universe of discourse to a (cid:12)nite set , and indicates precisely the range of the quanti(cid:12)er . 3 . 2 CONDITIONAL EFFECTS A secondary precondition , i . e . one associated with an e(cid:11)ect [ 26 ] , de(cid:12)nes the conditions under which action execution will achieve that e(cid:11)ect . Unlike primary pre - conditions , secondary preconditions need not be true for the action to be executed . If p is the secondary precondition of e(cid:11)ect e , then the resulting conditional e(cid:11)ect is de(cid:12)ned as : EFF ( when ( p ; e ) ; a ; s ) def = GOAL ( p ; s ; fg ) ) EFF ( e ; a ; s ) We use GOAL in our de(cid:12)nition of when , but we have only de(cid:12)ned GOAL for annotations satisfy , hands - o(cid:11) and initially . How should we de(cid:12)ne when preconditions ? Since they need to hold , if at all , when the action is executed , they are di(cid:11)erent from initially preconditions . But satisfy requires that the agent know that the condition is true , which would lead to the faulty conclusion that the e(cid:11)ect only occurs if the agent knows that the secondary preconditions hold . So we add a new type of precondition , without any an - notation at all , to represent conditions that must hold at the time of execution , with or without knowledge of the agent : GOAL ( P ; s0 ; fag n 1 ) def = P ( sn ) ( 13 ) This ensures that whether the e(cid:11)ects occur depends only on the state of the world . It also makes it clear what is being quanti(cid:12)ed over in ls : The (cid:12)les really in d , at the time of execution . 3 . 3 UNCERTAIN EFFECTS In some cases , executing actions with causal e(cid:11)ects can decrease the agent ' s knowledge about the world . sadl provides two ways of encoding these actions : as conditional e(cid:11)ects whose secondary precondition is un - known , or by explicitly specifying the U truth value . As an example of the former , executing rm tex / * deletes all writable (cid:12)les in tex ; if the agent doesn ' t know which (cid:12)les are writable , then she won ' t know which (cid:12)les remain in tex even if she knew the contents be - fore executing the action . As an example of explicit creation of uncertainty , we encode compress myfile with the e(cid:11)ect 8n cause ( size ( myfile , n ) , U ) . 7 We de(cid:12)ne causal e(cid:11)ects for T and U truth values as follows : EFF ( cause ( P ; T ) ; a ; s ) def = P ( DO ( a ; s ) ) ( 14 ) EFF ( cause ( P ; U ) ; a ; s ) def = UnkP ( a ; DO ( a ; s ) ) , P ( DO ( a ; s ) ) ( 15 ) where , UnkP is a predicate such that : KNOW ( UnkP ( a ) ; DO ( a ; s ) ) ^ : KNOW ( : UnkP ( a ) ; DO ( a ; s ) ) ( 16 ) In other words , we represent an uncertain e(cid:11)ect as a deterministic function of hidden state . UnkP ( a ) de - notes a unique unknown predicate , which represents the hidden state responsible for the change in truth value of P . It must be unique to avoid biasing corre - lation of independent unknown e(cid:11)ects . It is clear from the above de(cid:12)nition how a cause ef - fect may make P unknown . What may not be clear is how a cause e(cid:11)ect can make P known . In fact , 7 In principle , we could represent all uncertain e(cid:11)ects as conditional e(cid:11)ects with unknown preconditions , but doing so would be cumbersome . However , we de(cid:12)ne the seman - tics of uncertain e(cid:11)ects in precisely this manner . it wouldn ' t , if not for the fact that the agent knows all the e(cid:11)ects of an action ( Equation 9 ) . However , knowledge of a conditional e(cid:11)ect does not necessarily mean knowledge of the consequent . For example , if an agent executes compress myfile , she only knows that if she had write permission prior to executing compress , then myfile is compressed afterward . 4 TEMPORAL PROJECTION & REGRESSION We have discussed the function DO , which maps a sit - uation and an action ( or sequence of actions ) to a new situation , but we haven ' t yet said how the two situa - tion terms relate to each other . If s 0 = DO ( fag n 1 ; s ) , we want to answer the following questions . (cid:15) Progression : What can we say about s 0 , given knowledge of the conditions that hold in s ? (cid:15) Regression : What must be true in s , to guarantee some desired condition in s 0 ? We treat each in turn . 4 . 1 PROJECTION & THE FRAME PROBLEM The de(cid:12)nitions for preconditions and e(cid:11)ects that we have given are insu(cid:14)cient to solve the temporal pro - jection problem . sadl e(cid:11)ects only list (cid:13)uents that an action a(cid:11)ects , but what about (cid:13)uents it doesn ' t af - fect ? Explicitly stating everything that doesn ' t change would be tedious | this is the well - known frame prob - lem . The standard approach to the frame problem , and the one we adopt , is to make the strips assump - tion : anything not explicitly said to change remains the same . To fully specify the sadl semantics , it is necessary to express the strips assumption in terms of the situation calculus . We use the formulation in - troduced in [ 31 ] , and augmented in [ 32 ] to account for sensing actions . This strategy consists of provid - ing a formula for each (cid:13)uent , called a successor state axiom , that speci(cid:12)es the value of the (cid:13)uent in terms of 1 ) the action executed , and 2 ) the conditions that held before the action was executed . By quantifying over actions , we can produce a single , concise formula for each (cid:13)uent that includes only the relevant information . Specifying update axioms for each (cid:13)uent indepen - dently requires (cid:13)uents to be logically independent of each other , so disjunction is not allowed . E(cid:11)ects con - sist of conjunctions of terms , each term being equiva - lent to one of the following when (cid:13) T P ( a ) cause ( P ; T ) ( 17 ) when (cid:13) F P ( a ) cause ( P ; F ) ( 18 ) when (cid:13) U P ( a ) cause ( P ; U ) ( 19 ) when (cid:20) tv P ( a ) observe ( P ; tv ) ( 20 ) where a is an action and P is a (cid:13)uent , which may contain universally quanti(cid:12)ed variables or constants , 8 and (cid:13) tv P ( a ) and (cid:20) tv P ( a ) represent arbitrary goal expressions . 9 For example , if compress tex / * changes the size of all writable (cid:12)les in directory tex , then (cid:13) U size ( f ) ( compress tex / * ) = indir ( f , tex ) ^ writable ( f ) . Clearly , all actions can be repre - sented by specifying the (cid:13) and (cid:20) preconditions for each (cid:13)uent in the domain theory . If a has a non - conditional e(cid:11)ect , cause ( P , tv ) , then (cid:13) tv P ( a ) = T . We can express the fact that action a doesn ' t a(cid:11)ect P at all by saying 8tv : (cid:13) tv P ( a ) = F . We don ' t list observe ( P ; T ) above , since it is subsumed by the con - junction observe ( P ; v ) ^ v = T ( similarly for F ) . Given these de(cid:12)nitions , we can state the conditions under which an action changes or preserves a (cid:13)uent ' s truth value . Following Pednault [ 24 ] , we de(cid:12)ne (cid:6) a ' to be the conditions under which an executable action a will establish ' , and (cid:5) a ' to be the conditions under which a will preserve ' . We have the following estab - lishment conditions : (cid:6) a ' , (cid:13) T ' ( a ) _ ( Unk ' ( a ) ^ (cid:13) U ' ( a ) ) ( 21 ) (cid:6) a : ' , (cid:13) F ' ( a ) _ ( : Unk ' ( a ) ^ (cid:13) U ' ( a ) ) ( 22 ) where Unk ' ( a ) is the unknown predicate introduced in Equations 15 and 16 . The presence of an e(cid:11)ect with a U truth value will make ' true or false , depending on the value of Unk ' ( a ) . Since Unk ' ( a ) is unknown by de(cid:12)nition , e(cid:11)ects with U truth values aren ' t gener - ally useful for goal establishment . We also have the following preservation conditions : (cid:5) a ' , : (cid:13) F ' ( a ) ^ ( : Unk ' ( a ) _ : (cid:13) U ' ( a ) ) ( 23 ) (cid:5) a : ' , : (cid:13) T ' ( a ) ^ ( Unk ' ( a ) _ : (cid:13) U ' ( a ) ) ( 24 ) For each (cid:13)uent , we can then generate an expression that speci(cid:12)es precisely when it is true or false , by quan - tifying over actions . For each (cid:13)uent P , there is a suc - cessor state axiom , which combines update axioms and frame axioums for P . The successor state axioms are straightforward statements of the strips assumption : a (cid:13)uent is true if and only if it was made true , or it was true originally and it wasn ' t made false : GOAL ( (cid:25) a ; s ; fg ) ) [ P ( DO ( a ; s ) ) , (cid:6) a P ( s ) _ P ( s ) ^ (cid:5) a P ( s ) ] ( 25 ) Similarly , there is a successor state axiom for K . GOAL ( (cid:25) a ; s ; fg ) ) [ K ( s 00 ; DO ( a ; s ) ) , 9s 0 : K ( s 0 ; s ) ^ ( s 00 = DO ( a ; s 0 ) ) ^ 8P : ( [ (cid:20) v P ( a ; s ) ^ P ( s ) ] ) [ P ( s 0 ) ^ v = s 0 T ] ^ ( [ (cid:20) v P ( a ; s ) ^ : P ( s ) ] ) ) [ : P ( s 0 ) ^ v = s 0 F ] ) ] ( 26 ) 8 Including variables that will resolve to constants . 9 with the restriction that e(cid:11)ects must be consistent , so , for example , (cid:13) T P ( a ) ^ (cid:13) F P ( a ) must always be false . We have stated this formula in second - order logic , but only because the formula depends on all of the actual (cid:13)uents in the domain theory . Given any speci(cid:12)c do - main , this second - order formula could be replaced with an equivalent (cid:12)rst - order formula by replacing P with each (cid:13)uent in the domain . The above de(cid:12)nition only speci(cid:12)es when information is gained , and seems to say nothing about when it is lost . However , information loss is indeed accounted for , through the successor state axiom for P . If P becomes true in some situations accessible from s , and false in others , then by de(cid:12)nition , P is unknown . For example , compress myfile compresses myfile if it is writable . If it is unknown whether myfile is writable , then in some accessible worlds , myfile is writable and will be compressed . In other worlds , myfile is not writable and won ' t be compressed . The result is that it becomes unknown whether myfile is compressed . Similarly , if P was known previously and not changed , then by the successor state axioms for P and K , P will continue to be known . [ 32 ] . The above formula correctly describes how K changes , but it is a little unwieldy if what we want to know about is KNOW ( ' ) . Intuitively , KNOW ( ' ) becomes true if ' is known to become true , or ' is observed . Additionally , ' continues to be known true until it possibly becomes false . The following formulas follow from the successor state axioms for ' and K . (cid:5) a KNOW ( ' ) , KNOW ( (cid:5) a ' ) ( 27 ) (cid:6) a KNOW ( ' ) , KNOW ( (cid:13) T ' ( a ) ) _ ( (cid:20) tv ' ( a ) ^ ' ^ (cid:5) a KNOW ( ' ) ) 10 ( 28 ) 4 . 2 REGRESSION Most modern planners build plans using goal regres - sion | starting with a goal and successively adding actions that achieve either part of the goal or precon - ditions of previously added actions . Once no precon - ditions remain that aren ' t true in the initial state , the plan is complete . It is therefore useful to have a for - mal speci(cid:12)cation of what conditions must be true for a 10 The additional requirement (cid:5) a KNOW ( ' ) may come as a surprise , since an action that simultaneously observes ' and causes ' to become false or unknown would seem to violate our rule against inconsistent actions . However , such e(cid:11)ects aren ' t inconsistent , since the observation pertains to situation s , whereas the update is to situation DO ( a ; s ) . Such destructive sensing actions are commonplace . By the Heisenberg Uncertainty Principle , they are inevitable , but examples can be found in macroscopic domains as well . Biologists (cid:12)nd out the number of insects living in a tree by placing containers under the tree and then fogging the tree with poison . The number of insects that fall into the containers provides an estimate of the number that were originally living there . given action sequence to achieve a given goal . Let a (cid:0)1 be a regression operator for action a . a (cid:0)1 ( ' ) is a con - dition that , if true immediately before the execution of a , results in ' being true after a is executed . We de(cid:12)ne ( fag n 1 ) (cid:0)1 ( ' ) to be an (cid:0)1 ( an(cid:0)1 (cid:0)1 ( : : : ( a1 (cid:0)1 ( ' ) ) ) ) . Nat - urally , regression on an action sequence of zero length is the identity function : fg (cid:0)1 ( ' ) = ' . Let (cid:11) be an axiomatization of the initial conditions , and let (cid:0) be some goal expression . The objective of planning is to produce an executable sequence of ac - tions , fag n 1 , such that (cid:11)j = ( fag n 1 ) (cid:0)1 ( (cid:0) ) . We discuss executability in Section 4 . 3 . We specify regression operators for satisfy , initially and hands - o(cid:11) goals below . Since some conditions could be true in the initial state , we also must specify when a condition is true after executing a plan of zero length . Since initially indicates something that must be true before the plan is executed , and satisfy in - dicates things true afterwards , it follows that if there is no plan , then initially and satisfy have the same interpretation : For all ' , (cid:11)j = KNOW ( ' ; S0 ) , fg (cid:0)1 ( initially ( ' ) ) = T ( 29 ) (cid:11)j = KNOW ( ' ; S0 ) , fg (cid:0)1 ( satisfy ( ' ) ) = T ( 30 ) hands - o(cid:11) is always true in the initial state , since it can only be violated by changing the proscribed (cid:13)uent : fg (cid:0)1 ( hands - o(cid:11) ( ' ) ) = T ( 31 ) We now consider how to regress a sadl goal formula through an action . A goal satisfy ( ' ) is achieved if the agent knows that ' is true ; i . e . , ' just became true , was just observed to be true , or was previously known to be true and wasn ' t subsequently a(cid:11)ected . The (cid:12)rst two conditions are captured by (cid:6) a KNOW ( ' ) . The latter holds when satisfy ( ' ) held in the previous state , and knowledge of ' was preserved : a (cid:0)1 ( satisfy ( ' ) ) = (cid:6) a KNOW ( ' ) _ ( satisfy ( ' ) ^ (cid:5) a KNOW ( ' ) ) ( 32 ) A hands - o(cid:11) goal holds if the state of ' always remains the same as it was in the initial state . hands - o(cid:11) ( ' ) doesn ' t forbid actions that a(cid:11)ect ' | just actions that change ' . For example , an action compress myfile doesn ' t violate the goal hands - o(cid:11) ( compressed ( myfile ) ) if myfile was al - ready compressed initially . 11 a (cid:0)1 ( hands - o(cid:11) ( ' ) ) = ( (cid:5) a : ' _ initially ( ' ) ) ^ ( (cid:5) a ' _ initially ( : ' ) ) ^ hands - o(cid:11) ( ' ) ( 33 ) 11 This is a departure from uwl ' s notion of hands - o(cid:11) , in which the compress would be a violation . However , un - compressing the (cid:12)le and then recompressing it does violate the goal , since the uncompress changes the (cid:13)uent . initially ( ' ) is satis(cid:12)ed after action a if it was already satis(cid:12)ed , or if ' was observed by action a , and wasn ' t a(cid:11)ected by any previous actions . Unlike other goals , we are interested in the (cid:12)rst time point at which an initially goal is achieved , as opposed to the last . The disjunct initially ( ' ) ensures that the (cid:12)rst occurrence is considered , because it is always regressed back . a (cid:0)1 ( initially ( ' ) ) = initially ( ' ) _ ( (cid:20) tv ' ( a ) ^ ' ^ hands - o(cid:11) ( ' ) ) ( 34 ) This de(cid:12)nition doesn ' t rule out using destructive sens - ing actions . All that matters is that ' be undisturbed before it is sensed . It ' s (cid:12)ne if the act of sensing the value of ' itself a(cid:11)ects ' . Unannotated preconditions merely need to be satis(cid:12)ed in the (cid:12)nal state , and it isn ' t necessary that they be known true . a (cid:0)1 ( ' ) = (cid:6) a ' _ ( ' ^ (cid:5) a ' ) ( 35 ) Logical operators are simply regressed back to the ini - tial state , since their interpretation is the same across all situations , as detailed in [ 25 ] . With these de(cid:12)nitions , we can show that regression is correct | that is , if the conditions returned by a (cid:0)1 ( (cid:0) ) are true , and fag n 1 is successfully executed , then (cid:0) will indeed be true . Theorem 1 ( Soundness of Regression ) Let fag n 1 be an executable action sequence . Let (cid:0) be a goal for - mula , and let (cid:11) be an axiomatization of the initial state , s0 . Then (cid:11)j = ( fag n 1 ) (cid:0)1 ( (cid:0) ) ) GOAL ( (cid:0) ; s0 ; fag n 1 ) We believe that the reverse is also true | i . e . , if (cid:0) is true after fag n 1 is executed , then a (cid:0)1 ( (cid:0) ) must have been true . 4 . 3 EXECUTABILITY Regression operators alone only tell part of the story about when an action , or sequence of actions , can achieve a goal . a (cid:0)1 ( ' ) consists of the conditions un - der which a will achieve ' assuming it is successfully executed . So to ensure that a brings about ' , we must also ensure that a can be executed . Action a is exe - cutable in situation s i(cid:11) the preconditions of a are true in s . A sequence of actions , fag n 1 , is executable in s i(cid:11) a1 is executable in s , a2 is executable in DO ( a1 ; s ) , a3 is executable in DO ( a2 ; DO ( a1 ; s ) ) , and so on . 5 EXPRESSIVENESS Although sadl is appropriate for any Markov domain ( e . g . , transportation logistics , manufacturing , mobile robotics , etc . ) , the language is best at modeling do - mains with accurate ( low noise ) sensors . We have concentrated our e(cid:11)orts on UNIX and the Internet , encoding hundreds of commands . Examples of sensory actions include finger , wc , grep , the netfind and inspec Internet sites , and actions to traverse the Web ; causal actions include cp , rm , and compress . Univer - sal quanti(cid:12)cation allows us to model actions that re - turn an unbounded amount of information , such as ls ( Figure 2 ) . As an illustration , consider the goal , (cid:0) , of (cid:12)nding a (cid:12)le named old and renaming it to new : (cid:0) = 9f : initially ( name ( f ; old ) ) ^ satisfy ( name ( f ; new ) ) . Recall that this goal is inexpressible in uwl . It can be achieved by executing ls in various directories un - til the desired (cid:12)le is found , and then executing mv to change the name to new . There is no single action sequence that will work in all situations , because the location of old is not necessarily known . Let ' s assume that old resides in the directory tex , and that its loca - tion is unknown . We also assume that the agent knows that tex is readable , and that current . shell ( csh ) is true . The shortest possible action sequence that would achieve the goal is ls tex then mv tex / old tex / new . For brevity , we abbreviate these actions as ls and mv , respectively . We show that this action sequence is ex - ecutable , and that it achieves the goal . For the sake of this example , we won ' t consider mv in its full glory . Rather , we assume a simpli(cid:12)ed version of mv , with the precondition (cid:25) mv = satisfy ( name ( f , old ) ) ^ satisfy ( in . dir ( f , tex ) ) , and the single ef - fect " mv = cause ( name ( f , new ) ) . This represen - tation ignores many details , such as whether tex is writable , old is readable , there is already a (cid:12)le named new , etc . To show that the plan is executable , we must (cid:12)rst show that the preconditions of ls hold in S0 , i . e . , S0j = (cid:25) ls , and then show that the preconditions of mv hold after ls is executed , i . e . , DO ( ls ; S0 ) j = (cid:25) mv : To show that the plan achieves the goal , we need show that DO ( mv ; DO ( ls ; S0 ) ) j = (cid:0) . We use regression to show that these results hold . We (cid:12)rst regress the two conjuncts of (cid:0) through mv . mv achieves the satisfy goal , with no secondary preconditions : mv (cid:0)1 ( satisfy ( name ( f ; new ) ) ) ( (cid:6) mv KNOW ( name ( f ; new ) ) ( KNOW ( (cid:13) T name ( f ; new ) ( mv ) ) ( KNOW ( T ) ( T . mv has no e(cid:11)ect on the initially goal : mv (cid:0)1 ( initially ( name ( f ; old ) ( initially ( name ( f ; old ) ) : Now we regress mv (cid:0)1 ( (cid:0) ) ^ (cid:25) mv through ls . That is , we regress initially ( name ( f ; old ) ) ^ satisfy ( name ( f ; old ) ) ^ satisfy ( in . dir ( f ; tex ) ) . We regress the (cid:12)rst two conjuncts through ls . The (cid:12)nal conjunct , satisfy ( in . dir ( f ; tex ) ) , follows the same pattern . The action ls tex has the e(cid:11)ect 8f9n observe ( name ( f , n ) ) ^ observe ( in . dir ( f ; tex ) ) , with the secondary precondition in . dir ( f , tex ) . This precondition does not require knowledge on the part of the agent . So ls (cid:0)1 ( initially ( name ( f ; old ) ) ^ satisfy ( name ( f ; old ) ) ) ( (cid:6) ls KNOW ( name ( f ; old ) ) ^ hands - o(cid:11) ( name ( f ; old ) ) ^ (cid:5) ls : name ( f ; old ) ( (cid:20) T KNOW ( name ( f ; old ) ) ( ls ) ^ KNOW ( (cid:5) ls name ( f ; old ) ) ^ name ( f ; old ) ^ hands - o(cid:11) ( name ( f ; old ) ) ^ : (cid:13) T name ( f ; old ) ( ls ) ^ : (cid:13) U name ( f ; old ) ( ls ) ( in . dir ( f , tex ) ^ KNOW ( : (cid:13) T name ( f ; old ) ( ls ) ) ^ : (cid:13) U name ( f ; old ) ( ls ) ) ^ name ( f ; old ) ( in . dir ( f , tex ) ^ KNOW ( T ) ^ name ( f ; old ) . This last formula is entailed by S0 . All that re - mains is to show that S0j = fg (cid:0)1 ( (cid:25) ls ) . By the de(cid:12) - nition of fg (cid:0)1 for satisfy goals , that follows i(cid:11) S0j = KNOW ( current . shell ( csh ) ^ KNOW ( protection tex , readable ) , which is true by assumption . 6 TRACTABILITY sadl is implemented by xii [ 11 , 10 ] , a partial - order planner whose performance is comparable to the ucpop / snlp family of classical planners . We analyze its performance in terms of the re(cid:12)nement paradigm described in [ 15 ] | xii has three re(cid:12)nement opera - tions : goal establishment , con(cid:13)ict resolution and ac - tion execution . Goal establishment involves possibly adding an action to the plan , and adding an interval protection constraint ( IPC ) to prevent the goal from being clobbered . In sadl , there are three possible in - tervals to consider . If sp is the situation in which the action will be executed , and sc is the situation in which the goal is to be ful(cid:12)lled , the intervals are [ sp + 1 ; sc ] , [ s0 ; sp ] or [ s0 ; sc ] , corresponding to satisfy , initially or hands - o(cid:11) , respectively . Ensuring that no actions violate the IPC requires O ( n ) time , where n is the number of steps in the plan , but maintaining a consis - tent ordering of actions requires O ( n 2 ) time . Con(cid:13)ict resolution and action execution also take O ( n 2 ) time . In contrast , note that goal establishment and con(cid:13)ict resolution are undecidable in the situation calculus . 7 RELATED WORK McCarthy and Hayes [ 19 ] (cid:12)rst argued that an agent needs to reason about its ability to perform an ac - tion . Moore [ 21 ] devised a theory of knowledge and action , based on a variant of the situation calculus with possible - worlds semantics . He provided an anal - ysis of knowledge preconditions , which we discussed earlier , and information - providing e(cid:11)ects . Morgen - stern [ 22 ] generalized Moore ' s results to express par - tial knowledge that agents have about the knowledge of other agents ( e . g . \ John knows what Bill said " ) , using a substantially more expressive logic , which is syntactic rather than modal . Davis [ 3 ] extended Moore ' s theory to handle contingent plans , though , like Moore , he doesn ' t discuss actions with indetermi - nate e(cid:11)ects . Levesque [ 17 ] o(cid:11)ers an elegant theory of when a plan , with conditionals and loops , achieves a satisfaction goal in the presence of incomplete infor - mation . However , Levesque doesn ' t discuss knowledge goals , and his sensory actions can return only T or F , and can ' t change the state of the world . Goldman and Boddy [ 12 ] present a clean language for contin - gent plans with context - dependent e(cid:11)ects and nonde - terminism . However , like Levesque , they don ' t allow variables in sensing actions : possible outcomes are rep - resented as a disjuction . Shoham [ 33 ] presents a lan - guage , with explicit time , for representing beliefs and communication among multiple agents . Agents can request other agents to perform actions , which can include ( nested ) communicative actions , but not ar - bitrary goals . A discrete temporal logic , without 8 , is used to represent beliefs . prs [ 14 ] is a procedural language that can represent a similar class of goals as sadl , but lacks temporal goals such as initially . prs has annotation achieve corresponding to sadl satisfy , preserve corresponding to hands - o(cid:11) , and test corresponding to satisfy + hands - o(cid:11) , as well as several procedural constructs that have no correspond - ing terms in the declarative sadl language . Partially - observable Markov Decision Processes [ 20 , 2 ] provide an elegant representation of sensing actions and actions with uncertain outcomes in Markov do - mains . However , they don ' t lend themselves to e(cid:14) - cient algorithms . With few exceptions , such as [ 1 ] , work in MDPs assumes that reward functions ( goals ) are Markov as well , so temporal goals like initially are inexpressible . A number of contingent planning systems have intro - duced novel representations of uncertainty and sensing actions . Warplan - C [ 34 ] tags actions as conditional , meaning they have two possible outcomes : P or : P . C - buridan [ 16 , 4 ] uses a probabilistic action language that can represent conditional , observational e(cid:11)ects , including noisy sensors , and e(cid:11)ects that cause infor - mation loss . Unlike sadl , the C - buridan language is propositional , and makes no distinction between knowledge goals and goals of satisfaction . C - buridan and Cassandra [ 29 ] ( and wcpl [ 12 ] ) can represent and reason with uncertain outcomes of actions as disjunc - tions , allowing them to deal with correlations between multiple unknown variables ( e . g . either it is raining and Fido is wet , or it is sunny and Fido is dry ) . By using the U truth value , sadl gives up the ability to represent these correlations ( i . e . as far as the agent knows , it is raining and (cid:12)do is dry ) . However , rea - soning with U truth values is more e(cid:14)cient than the possible - worlds representation used to handle disjunc - tion . cnlp [ 27 ] , like sadl , uses a three - valued logic to represent uncertainty . Another limitation of these other languages is an inability to represent actions , like ls that return information about an unbounded number of objects . 8 CONCLUSIONS We introduced sadl , a language for representing sens - ing actions and information goals , which embodies the lessons learned during four years of building and de - bugging Internet Softbot domain theories : 1 ) Since knowledge goals are temporal , sadl supports the tem - poral annotation initially . 2 ) In Markov domains , such as UNIX , knowledge preconditions for actions are inappropriate , but subgoaling to obtain knowledge is often necessary ; sadl handles this paradox by elimi - nating knowledge preconditions from actions , but us - ing secondary preconditions to clearly indicate when subgoaling to acquire knowledge could be useful . sadl is expressive enough to represent real - world domains , such as UNIX and the World Wide Web , yet restricted enough to be used e(cid:14)ciently by modern planning al - gorithms , such as xii [ 10 ] . References [ 1 ] Fahiem Bacchus , Craig Boutilier , and Adam Grove . Rewarding behaviors . In Proc . 14th Nat . Conf . on AI , 1995 . [ 2 ] A . R . Cassandra , L . P . Kaebling , and M . L . Littman . Algorithms for partially observable markov decision processes . Technical report 94 - 14 , Brown University , Providence , Rhode Island , 1994 . [ 3 ] E . Davis . Knowledge preconditions for plans . Technical Report 637 , NYU Computer Science Department , May 1993 . [ 4 ] D . Draper , S . Hanks , and D . Weld . A probabilis - tic model of action for least - commitment planning with information gathering . In Proc . 10th Conf . on Uncertainty in Arti(cid:12)cal Intelligence , 1994 . [ 5 ] M . Drummond . Situated control rules . In Pro - ceedings of the First International Conference on Knowledge Representation and Reasoning , May 1989 . [ 6 ] O . Etzioni , K . Golden , and D . Weld . Sound and e(cid:14)cient closed - world reasoning for planning . Ar - ti(cid:12)cial Intelligence , 1997 . ( To appear ) . [ 7 ] O . Etzioni and D . Weld . A softbot - based interface to the Internet . CACM , 37 ( 7 ) : 72 { 76 , 1994 . [ 8 ] Oren Etzioni , Keith Golden , and Dan Weld . Tractable closed - world reasoning with updates . In Proc . 4th Int . Conf . on Principles of Knowl - edge Representation and Reasoning , pages 178 { 189 , 1994 . [ 9 ] Oren Etzioni , Steve Hanks , Daniel Weld , Denise Draper , Neal Lesh , and Mike Williamson . An ap - proach to planning with incomplete information . In Proc . 3rd Int . Conf . on Principles of Knowl - edge Representation and Reasoning , pages 115 { 125 , 1992 . [ 10 ] K . Golden , O . Etzioni , and D . Weld . Planning with execution and incomplete information . Tech - nical Report 96 - 01 - 09 , University of Washington , Department of Computer Science and Engineer - ing , February 1996 . Available via FTP from pub / ai / at ftp . cs . washington . edu . [ 11 ] Keith Golden , Oren Etzioni , and Dan Weld . Om - nipotence without omniscience : Sensor manage - ment in planning . In Proc . 12th Nat . Conf . on AI , pages 1048 { 1054 , 1994 . [ 12 ] Robert P . Goldman and Mark S . Boddy . Ex - pressive Planning And Explicit Knowledge . In Proc . 3rd Intl . Conf . on AI Planning Systems , May 1996 . [ 13 ] Peter Haddawy and Steve Hanks . Utility Mod - els for Goal - Directed Decision - Theoretic Plan - ners . Technical Report 93 { 06 { 04 , Univ . of Wash - ington , Dept . of Computer Science and Engineer - ing , September 1993 . Submitted to Arti(cid:12)cial In - telligence . Available via FTP from pub / ai / at ftp . cs . washington . edu . [ 14 ] F . Ingrand , R . Chatila , R . Alami , and F Robert . PRS : A high level supervision and control lan - guage for autonomous mobile robots . In Proceed - ings of the 1996 IEEE International Conference On Robotics and Automation , 1996 . [ 15 ] S . Kambhampati , C . Knoblock , and Q . Yang . Planning as re(cid:12)nement search : A uni(cid:12)ed frame - work for evaluating design tradeo(cid:11)s in partial or - der planning . Arti(cid:12)cial Intelligence , 76 : 167 { 238 , 1995 . [ 16 ] N . Kushmerick , S . Hanks , and D . Weld . An Al - gorithm for Probabilistic Planning . Arti(cid:12)cial In - telligence , 76 : 239 { 286 , 1995 . [ 17 ] Hector Levesque . What is planning in the pres - ence of sensing ? In Proc . 14th Nat . Conf . on AI , 1996 . [ 18 ] J . McCarthy . Circumscription - a form of non - monotonic reasoning . Arti(cid:12)cial Intelligence , 13 ( 1 , 2 ) : 27 { 39 , April 1980 . [ 19 ] J . McCarthy and P . J . Hayes . Some philosophical problems from the standpoint of arti(cid:12)cial intelli - gence . In Machine Intelligence 4 , pages 463 { 502 . Edinburgh University Press , 1969 . [ 20 ] G . E . Monahan . A survey of partially observ - able markov decision processes : Theory , models , and algorithms . Management Science , 28 ( 1 ) : 1 { 16 , 1982 . [ 21 ] R . Moore . A Formal Theory of Knowledge and Action . In J . Hobbs and R . Moore , editors , For - mal Theories of the Commonsense World . Ablex , Norwood , NJ , 1985 . [ 22 ] Leora Morgenstern . Knowledge preconditions for actions and plans . In Proceedings of IJCAI - 87 , pages 867 { 874 , 1987 . [ 23 ] Leora Morgenstern . Foundations of a Logic of Knowledge , Action , and Communication . PhD thesis , New York University , 1988 . [ 24 ] E . Pednault . Toward a Mathematical Theory of Plan Synthesis . PhD thesis , Stanford University , December 1986 . [ 25 ] E . Pednault . Synthesizing plans that contain ac - tions with context - dependent e(cid:11)ects . Computa - tional Intelligence , 4 ( 4 ) : 356 { 372 , 1988 . [ 26 ] E . Pednault . ADL : Exploring the middle ground between STRIPS and the situation calculus . In Proc . 1st Int . Conf . on Principles of Knowledge Representation and Reasoning , pages 324 { 332 , 1989 . [ 27 ] M . Peot and D . Smith . Conditional Nonlinear Planning . In Proc . 1st Intl . Conf . on AI Planning Systems , pages 189 { 197 , June 1992 . [ 28 ] Martha Pollack . The uses of plans . Arti(cid:12)cial In - telligence , 57 ( 1 ) , 1992 . [ 29 ] L . Pryor and G . Collins . Planning for contingen - cies : A decision - based approach . Journal of Arti - (cid:12)cial Intelligence Research , 1996 . [ 30 ] Howard Rai(cid:11)a . Decision Analysis : Introductory Lectures on Choices Under Uncertainty . Addison - Wesley , 1968 . [ 31 ] R . Reiter . The frame problem in the situa - tion calculus : A simple solution ( sometimes ) and a completeness result for goal regression . In Vladimir Lifschitz , editor , Arti(cid:12)cial Intelligence and Mathematical Theory of Computation : Pa - pers in Honor of John McCarthy , pages 359 { 380 . Academic Press , 1991 . [ 32 ] R . Scherl and H . Levesque . The frame problem and knowledge producing actions . In Proc . 11th Nat . Conf . on AI , pages 689 { 695 , July 1993 . [ 33 ] Y . Shoham . Agent - oriented programming . Arti - (cid:12)cial Intelligence , 60 ( 1 ) : 51 { 92 , March 1993 . [ 34 ] D . Warren . Generating Conditional Plans and Programs . In Proceedings of AISB Summer Con - ference , pages 344 { 354 , University of Edinburgh , 1976 . [ 35 ] Dan Weld and Oren Etzioni . The (cid:12)rst law of robotics ( a call to arms ) . In Proc . 12th Nat . Conf . on AI , pages 1042 { 1047 , 1994 . [ 36 ] D . E . Wilkins . Practical Planning . Morgan Kauf - mann , San Mateo , CA , 1988 . [ 37 ] M . Williamson and S . Hanks . Optimal planning with a goal - directed utility model . In Proc . 2nd Intl . Conf . on AI Planning Systems , June 1994 .