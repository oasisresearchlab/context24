a r X i v : 2306 . 11951v1 [ c s . D S ] 21 J un 2023 1 On the Optimal Bounds for Noisy Computing Banghua Zhu , Ziao Wang , Nadim Ghaddar , Jiantao Jiao , Lele Wang Abstract We revisit the problem of computing with noisy information considered in Feige et al . ( 1994 ) , which includes computing the OR function from noisy queries , and computing the MAX , SEARCH , and SORT functions from noisy pairwise comparisons . For K given elements , the goal is to correctly recover the desired function with probability at least 1 − δ when the outcome of each query is ﬂipped with probability p . We consider both the adaptive sampling setting where each query can be adaptively designed based on past outcomes , and the non - adaptive sampling setting where the query cannot depend on past outcomes . The prior work provides tight bounds on the worst - case query complexity in terms of the dependence on K . However , the upper and lower bounds do not match in terms of the dependence on δ and p . We improve the lower bounds for all the four functions under both adaptive and non - adaptive query models . Most of our lower bounds match the upper bounds up to constant factors when either p or δ is bounded away from 0 , while the ratio between the best prior upper and lower bounds goes to inﬁnity when p → 0 or p → 1 / 2 . On the other hand , we also provide matching upper and lower bounds for the number of queries in expectation , improving both the upper and lower bounds for the variable - length query model . I . I NTRODUCTION The problem of computing with noisy information has been studied extensively since the seminal work ( Feige et al . , 1994 ) , which considers four problems : • Computing the OR function of K bits from noisy observations of the bits ; • Finding the largest ( or top - N ) element among K real - valued elements from noisy pairwise comparisons ; • Searching the rank of a new element in an ordered list of K elements from noisy pairwise comparisons ; • Sorting K elements from noisy pairwise comparisons . Feige et al . ( 1994 ) is based on a simple noise model where each observation goes through a binary symmetric channel BSC ( p ) , i . e . for each observation , with probability p we see its ﬂipped outcome , and with probability 1 − p we see its true value . They provide upper and lower bounds for the query complexity in terms of the total number of elements K , the noise probability p , and the desired conﬁdence level δ when adaptive querying is allowed . They establish the optimal query complexity in terms of dependence with respect to K . However , the exact sample / query complexity with respect to all parameters K , δ , and p is still not fully understood . In this paper , we revisit the problem of computing under noisy observations in both the adaptive sampling and non - adaptive sampling settings . We aim to close the gap in the existing bounds and illustrate the difference in query complexity between adaptive sampling and non - adaptive sampling . Taking the problem of computing the OR function of K bits as an example , assume that there are K bits ( X 1 , · · · , X K ) ∈ { 0 , 1 } K . The OR function is deﬁned as OR ( X 1 , · · · , X K ) = ( 1 , if ∃ k ∈ [ K ] , X k = 1 0 , otherwise . ( 1 ) The question is simple when we can query each bit noiselessly . In this case , K queries are both sufﬁcient and necessary since it sufﬁces to query each bit once . And thus there is no beneﬁt in applying adaptive querying Banghua Zhu is with the Department of Electrical Engineering and Computer Sciences , University of California , Berkeley , Berkeley , CA 94720 , USA ( email : banghua @ berkeley . edu ) . Ziao Wang is with the Department of Electrical and Computer Engineering , University of British Columbia , Vancouver , BC V6T1Z4 , Canada ( email : ziaow @ ece . ubc . ca ) . Nadim Ghaddar is with the Department of Electrical and Computer Engineering , University of California San Diego , La Jolla , CA 92093 , USA , ( email : nghaddar @ ucsd . edu ) . Jiantao Jiao is with the Department of Electrical Engineering and Computer Sciences , University of California , Berkeley , Berkeley , CA 94720 , USA ( email : jiantao @ berkeley . edu ) . Lele Wang is with the Department of Electrical and Computer Engineering , University of British Columbia , Vancouver , BC V6T1Z4 , Canada ( email : lelewang @ ece . ubc . ca ) . A shorter version of this work is accepted at the 2023 IEEE International Symposium on Information Theory . 2 compared to non - adaptive querying . When the observation of each query goes through a binary symmetric channel BSC ( p ) , we ask two questions : • How many queries ( samples ) do we need in the worst case to recover the true OR function value of any given sequences X 1 , · · · , X K with probability at least 1 − δ ? • Can adaptive sampling do better than non - adaptive sampling when noise is present ? Problem Fixed Length , Adaptive Sampling Upper Bound Lower Bound OR O ( K log ( 1 / δ ) 1 − H ( p ) ) ( Feige et al . , 1994 ) Ω ( K 1 − H ( p ) + K log ( 1 / δ ) D KL ( p k 1 − p ) ) ( Thm II . 1 ) MAX O ( K log ( 1 / δ ) 1 − H ( p ) ) ( Feige et al . , 1994 ) Ω ( K 1 − H ( p ) + K log ( 1 / δ ) D KL ( p k 1 − p ) ) ( Thm III . 1 ) SEARCH O ( log ( K / δ ) 1 − H ( p ) ) ( Feige et al . , 1994 ) Ω ( log ( K ) 1 − H ( p ) + log ( 1 / δ ) D KL ( p k 1 − p ) ) ( Thm IV . 1 ) SORT O ( K log ( K / δ ) 1 − H ( p ) ) ( Feige et al . , 1994 ) Ω ( K log ( K ) 1 − H ( p ) + K log ( K / δ ) D KL ( p k 1 − p ) ) ( Thm V . 1 ) Problem Fixed Length , Non - adaptive Sampling Upper Bound ( Appendix H ) Lower Bound OR O ( K log ( K / δ ) 1 − H ( p ) ) Ω ( max ( K , K log ( K ) p 1 − H ( p ) , K log ( K ) log ( ( 1 − p ) / p ) ) ) ( Thm II . 3 ) MAX O ( K 2 log ( K / δ ) 1 − H ( p ) ) Ω ( K 2 1 − H ( p ) + K 2 log ( 1 / δ ) D KL ( p k 1 − p ) ) ( Thm III . 2 ) SEARCH O ( K log ( 1 / δ ) 1 − H ( p ) ) Ω ( K 1 − H ( p ) + K log ( 1 / δ ) D KL ( p k 1 − p ) ) ( Thm IV . 3 ) SORT O ( K 2 log ( K / δ ) 1 − H ( p ) ) Ω ( K 2 + K 2 log ( K ) D KL ( p k 1 − p ) ) ) ( Thm V . 3 ) Problem Variable Length , Adaptive Sampling Matching Bound ( Thm VI . 1 ) OR Θ ( K 1 − H ( p ) + K log ( 1 / δ ) D KL ( p k 1 − p ) ) MAX Θ ( K 1 − H ( p ) + K log ( 1 / δ ) D KL ( p k 1 − p ) ) SEARCH Θ ( log ( K ) 1 − H ( p ) + log ( 1 / δ ) D KL ( p k 1 − p ) ) SORT Θ ( K log ( K ) 1 − H ( p ) + K log ( K / δ ) D KL ( p k 1 − p ) ) TABLE I : Summary of query complexity bounds of OR , MAX , SEARCH and SORT . Here , we assume δ < 0 . 49 . In Feige et al . ( 1994 ) , an adaptive tournament algorithm is proposed that achieves worst - case query complexity O ( K log ( 1 / δ ) / ( 1 − H ( p ) ) ) , and a corresponding lower bound Ω ( K log ( 1 / δ ) / log ( ( 1 − p ) / p ) ) for adaptive sampling is provided . A simple calculation tells us that their ratio log ( ( 1 − p ) / p ) / ( 1 − H ( p ) ) goes to inﬁnity as p → 0 or p → 1 / 2 , which indicates that there is still a gap between the upper and the lower bounds when the noise probability p is near the point 0 or 1 / 2 . This calls for tighter upper or lower bound for these cases . In our paper , we improve the lower bound to Ω ( K / ( 1 − H ( p ) ) + K log ( 1 / δ ) / D KL ( p k 1 − p ) ) , which matches the existing upper bound up to a constant factor when either p or δ is bounded away from 0 . One may wonder how many samples are needed when each query is not allowed to depend on previous outcomes . We provide a lower bound Ω ( max ( K , K log ( K ) p / ( 1 − H ( p ) ) ) , K log ( K ) / log ( ( 1 − p ) / p ) ) for this non - adaptive setting . On the other hand , a repetition - based upper bound O ( K log ( K / δ ) / ( 1 − H ( p ) ) ) matches the lower bound up to a constant factor when both p and δ are bounded from 0 . Similarly , we ask the same questions for computing MAX , SEARCH , and SORT . We defer the deﬁnitions and discussions of the problems to Section I - B . Here we summarize the best upper and lower bounds for the problems considered in this paper in Table I , either from previous art or from this paper . Our lower bounds take the form of f ( K ) / ( 1 − H ( p ) ) + g ( K , δ ) / D KL ( p k 1 − p ) . Here the ﬁrst term does not depend on δ for δ < 0 . 49 , representing the number of queries one must pay regardless of the target error probability δ . The second term grows logarithmically with 1 / δ , representing the price to pay for smaller target 3 error probability . Here we always have D KL ( p k 1 − p ) & 1 − H ( p ) for p ∈ ( 0 , 1 ) , with D KL ( p k 1 − p ) ≍ 1 − H ( p ) when p is bounded away from 0 . Technically , the ﬁrst term is usually from Fano’s inequality , which gives better dependence on p but worse dependence on δ . The second term is from a KL - divergence based lower bound , which gives better dependence on δ but worse dependence on p . We also extend our bounds from the ﬁxed length query model to variable length query model ( a . k . a . ﬁxed budget and ﬁxed conﬁdence in the bandit literature ( Kaufmann et al . , 2016 ) ) . In the ﬁxed length setting considered above , we ask for the worst - case deterministic number of queries required in the worst case to recover the true value with probability at least 1 − δ . In the variable length setting , the number of queries can be random and dependent on the past outcomes . And we ask for the expected number of queries to recover the true value with probability at least 1 − δ . We discuss the results for variable length in Section VI , where we give matching upper and lower bounds with respect to all parameters for computing all four functions , improving over both existing upper and lower bounds and closing the gap . A . Related Work The problems of noisy computation have been studied extensively before and after ( Feige et al . , 1994 ) . However , most of the existing research work focuses on tightening the dependence on K instead of p and δ , or extending the results to a more general framework where the noise follows a generalized model that includes BSC channel as a special case . Although worst - case upper and lower bounds are provided for the generalized model , most of the lower bounds are based on instances where the noise does not follow a BSC model . Thus the lower bounds do not apply to our case . a ) Noisy binary searching : The noisy searching problem was ﬁrst introduced by R´enyi ( R´enyi , 1961 ) and Ulam ( Ulam , 1976 ) and further developed by Berlekamp ( Berlekamp , 1964 ) and Horstein ( Horstein , 1963 ) in the context of coding for channels with feedback . The noisy searching algorithm in ( Burnashev and Zigangirov , 1974 ) by Burnashev and Zigangirov can be seen as a specialization of Horstein’s coding scheme , whereas the algorithms in ( Feige et al . , 1994 ; Pelc , 1989 ; Karp and Kleinberg , 2007 ) can be seen as an adaptation of the binary search algorithm to the noisy setting . The ﬁrst tight lower bound for variable - length adaptive sampling in noisy searching is given by ( Burnashev , 1976 ) . The recent concurrent work ( Gu and Xu , 2023 ) improves the dependence on constant when p is some constant that is bounded away from 0 and 1 / 2 . Our lower bounds are based on a different proof using Le Cam’s method . The results do not require p to be bounded , but are worse in terms of the constant dependence . ( Gu and Xu , 2023 ) also provides matching upper bound that is tight even with the constant in the variable - length setting . We provide more discussions in Section VI . Making the upper and lower bounds match in the ﬁxed - length query model still remains an important open problem . In terms of the bounds for non - adaptive sampling , the gap between O ( log ( K ) ) for adaptive sampling and O ( K ) can be seen from the noiseless case when p = 0 ( R´enyi , 1961 ) . Here we provide an improved bound for the noisy case that has explicit dependence on p and δ . b ) Noisy Sorting and max selection : The noisy sorting and max ( or Top - N ) selection problems have been usually studied together ( e . g . , ( Feige et al . , 1994 ) ) and later have been extended to a more general setting known as active ranking ( Mohajer et al . , 2017 ; Falahatgar et al . , 2017 ; Shah and Wainwright , 2018 ; Heckel et al . , 2019 ; Agarwal et al . , 2017 ) , where the noise p ij for the comparison of a pair of elements i and j is usually unknown and different for different pairs . Other related but different settings for noisy sorting in the literature include the cases when some distance metric for permutations is to be minimized ( rather than the the probability of error ) ( Ailon et al . , 2008 ; Braverman and Mossel , 2009 ; Ailon , 2011 ; Negahban et al . , 2012 ; Wauthier et al . , 2013 ; Rajkumar and Agarwal , 2014 ; Shah et al . , 2016 ; Shah and Wainwright , 2018 ; Mao et al . , 2018 ) , when the noise for each pairwise comparison is not i . i . d . and is determined by some noise model ( e . g . the Bradley – Terry – Luce model ( Bradley and Terry , 1952 ) ) ( Ajtai et al . , 2009 ; Negahban et al . , 2012 ; Rajkumar and Agarwal , 2014 ; Chen and Suh , 2015 ; Chen et al . , 2017 ; Ren et al . , 2018 ) , or when the ordering itself is restricted to some subset of all permutations ( Jamieson and Nowak , 2011 ; Ailon et al . , 2011 ) . For noisy sorting , the best upper and lower bounds have been provided in ( Feige et al . , 1994 ; Wang et al . , 2023 ) , which give an upper bound O ( K log ( K / δ ) / ( 1 − H ( p ) ) ) and lower bound Ω ( K log ( K ) / ( 1 − H ( p ) ) + log ( 1 / δ ) / D KL ( p k 1 − p ) ) . We tighten the lower bound to be Ω ( K log ( K ) / ( 1 − H ( p ) ) + K log ( K / δ ) / D KL ( p k 1 − p ) ) . On the other hand , ( Gu and Xu , 2023 ) shows that the query complexity is ( 1 + o ( 1 ) ) ( K log ( K ) / ( 1 − H ( p ) ) + K log ( K ) / D KL ( p k 1 − p ) ) , which does not scale with δ . Our lower bound for ﬁxed - length improves the dependence on δ . We also make the bounds match up to constant factors for all parameters in the variable - length 4 setting . However , making the δ dependence tight in upper bound for ﬁxed - length remains an open problem . In terms of the non - adaptive sampling scenario , we provide an upper and lower bound that matches in terms of the dependence on K , but is still loose when both p and δ go to 0 simultaneously . For max selection , the best known lower bound for ﬁxed - length adaptive sampling is Ω ( K log ( 1 / δ ) / log ( ( 1 − p ) / p ) ) ( Feige et al . , 1994 ) , while our result makes it tight when either p or δ goes to 0 and provides matching bounds for variable - length setting . On the other hand , ( Mohajer et al . , 2017 ; Shah and Wainwright , 2018 ) discuss the gap between adaptive sampling and non - adaptive sampling . However , the Ω ( K 3 log ( K ) ) lower bound for non - adaptive sampling in ( Shah and Wainwright , 2018 ) does not apply to our case since it is based on a generalized model where the noise probability is different . In our case , O ( K 2 log ( K ) ) is a natural upper bound . However , it is unclear whether our lower bound Ω ( K 2 ) can be improved to Ω ( K 2 log ( K ) ) . c ) OR and best arm identiﬁcation : The noisy computation of OR has been ﬁrst studied in the literature of circuit with noisy gates ( Dobrushin and Ortyukov , 1977a , b ; Von Neumann , 1956 ; Pippenger et al . , 1991 ; G´acs and G´al , 1994 ) and noisy decision trees ( Feige et al . , 1994 ; Evans and Pippenger , 1998 ; Reischuk and Schmeltz , 1991 ) . Different from the other three problems we consider , computing OR does not rely on pairwise comparisons , but instead directly queries the values of the bits . This is also related to the rich literature of best arm identiﬁcation , which queries real - valued arms and aims to identify the arm with largest value ( reward ) ( Bubeck et al . , 2009 ; Audibert et al . , 2010 ; Garivier and Kaufmann , 2016 ; Jamieson and Nowak , 2014 ; Gabillon et al . , 2012 ; Kaufmann et al . , 2016 ) . Indeed , any best - arm identiﬁcation algorithm can be converted to an OR computation by ﬁrst ﬁnding the maximum and then query the binary value of the maximum . This recovers the best existing upper bound O ( K log ( 1 / δ ) / ( 1 − H ( p ) ) ) for computation of OR under adaptive sampling scenario ( Audibert et al . , 2010 ) . However , the lower bound for best arm identiﬁcation does not apply to our case , since OR has a binary output , while the best arm identiﬁcation problem requires the arm index . And our lower bound for ﬁxed - length adaptive sampling improves over the best known lower bound Ω ( K log ( 1 / δ ) / log ( ( 1 − p ) / p ) ) ( Feige et al . , 1994 ) . We also provide matching bounds for variable - length . For non - adaptive sampling , ( Reischuk and Schmeltz , 1991 ; G´acs and G´al , 1994 ) provide a lower bound Ω ( K log ( K ) / log ( ( 1 − p ) / p ) ) . Our lower bound Ω ( K log ( K ) p / ( 1 − H ( p ) ) ) is tighter than the current lower bound when p → 1 / 2 , but looser when p → 0 . Thus the tightest bound is a maximum of K and the two lower bounds . B . Problem Deﬁnition and Preliminaries The OR function is deﬁned in Equation ( 1 ) . We deﬁne the rest of the problems here . Different from OR , the MAX , SEARCH and SORT problems are all based on noisy pairwise comparisons . Concretely , assume we have K distinct real - valued items X 1 , · · · , X K . Instead of querying the exact value of each element , we can only query a pair of elements and observe their noisy comparison . For any queried pair ( i , j ) , we will observe a sample from Bern ( 1 − p ) if X i > X j , and a sample from Bern ( p ) if X i < X j . We have MAX ( X 1 , · · · , X K ) = arg max i ∈ [ K ] X i , SORT ( X 1 , · · · , X K ) = σ , where σ : [ K ] 7→ [ K ] is the permutation function such that X σ ( 1 ) < X σ ( 2 ) < · · · < X σ ( K ) . And SEARCH ( X ; X 1 , · · · , X K ) = i , where i satisﬁes that X i < X < X i + 1 with X 0 = −∞ and X K + 1 = + ∞ . In the SEARCH problem , we assume that the ordering of X 1 , · · · , X K is given , and we are interested where the position of a new X is . Thus , in each round we compare the given X and any of the elements X i . We are interested in the probability of exact recovery of the functions . We consider both adaptive sampling and non - adaptive sampling . In adaptive sampling , the sampling distribution at each round can be dependent on the past queries and observations . In non - adaptive sampling , the sampling distribution in each round has to be determined at the beginning and cannot change with the ongoing queries or observations . Throughout the paper , we assume that the desired error probability δ satisﬁes δ < 0 . 49 . We use the terms “querying” and “sampling” interchangeably . II . C OMPUTING THE OR FUNCTION In this section , we provide the lower bounds for the query complexity of computing the OR function under both adaptive and non - adaptive sampling . The upper bound for adaptive sampling is from ( Feige et al . , 1994 ) . And the upper bound for non - adaptive sampling is omitted . Let θ ∈ { 0 , 1 } K be the K - bit sequence representing the true values . Let OR ( θ ) be the result of the OR function applied to the K - bit noiseless sequence . We also let ˆ µ be any algorithm that queries any noisy bit in T rounds , and outputs a ( possibly random ) decision from { 0 , 1 } . 5 A . Adaptive Sampling We have the following minimax lower bound . Theorem II . 1 . In the adaptive setting , we have inf ˆ µ sup θ ∈ { 0 , 1 } K P ( ˆ µ 6 = OR ( θ ) ) ≥ 1 4 · exp (cid:18) − T · D KL ( p k 1 − p ) K (cid:19) . Thus , the number of queries required to recover the true value with probability at least 1 − δ is lower bounded by Ω ( K / ( 1 − H ( p ) ) + K log ( 1 / δ ) / D KL ( p k 1 − p ) ) . We provide the proof here , which is based on Le Cam’s two point method ( see e . g . ( LeCam , 1973 ; Yu , 1997 ) ) . Proof of Theorem II . 1 . Our lower bound proof is mainly based on Le Cam’s two point method , which is also re - stated in Lemma A . 1 in Appendix A for reader’s convenience . Let θ 0 be the length - K all - zero sequence , and let θ j ∈ { 0 , 1 } K be such that θ jj = 1 and θ ji = 0 for i 6 = j . Here θ ji refers to the i - th element in the binary vector θ j . We can ﬁrst verify that for any ˆ µ , one has 1 ( ˆ µ 6 = OR ( θ 0 ) ) + 1 ( ˆ µ 6 = OR ( θ j ) ) ≥ 1 . By applying Le Cam’s two point lemma on θ 0 and θ j , we know that inf ˆ µ sup θ ∈ { 0 , 1 } K P ( ˆ µ 6 = OR ( θ ) ) ≥ 1 2 ( 1 − TV ( P θ 0 , P θ j ) ) . Here P θ j is the joint distribution of query - observation pairs in T rounds when the ground truth is θ j . By taking maximum over j on the right - hand side , we have inf ˆ µ sup θ ∈ { 0 , 1 } K P ( ˆ µ 6 = OR ( θ ) ) ≥ sup 1 ≤ j ≤ K 1 2 ( 1 − TV ( P θ 0 , P θ j ) ) ≥ sup 1 ≤ j ≤ K 1 4 exp ( − D KL ( P θ 0 , P θ j ) ) . Here the last inequality is due to Bretagnolle – Huber inequality ( Bretagnolle and Huber , 1979 ) ( Lemma A . 4 in Appendix A ) . Now we aim at computing D KL ( P θ 0 , P θ j ) . Let T j be the random variable that denotes the number of times the j - th element is queried among all T rounds . From divergence decomposition lemma ( Auer et al . , 1995 ) ( Lemma A . 3 in Appendix A ) , we have D KL ( P θ 0 , P θ j ) = E θ 0 [ T j ] · D KL ( p k 1 − p ) . Here E θ 0 [ T j ] denotes the expected number of times the j - th element is queried when the ground truth is θ 0 . Thus we have inf ˆ µ sup θ ∈ { 0 , 1 } K P ( ˆ µ 6 = OR ( θ ) ) ≥ sup 1 ≤ j ≤ K 1 4 exp ( − E θ 0 [ T j ] · D KL ( p k 1 − p ) ) . Now since P j E θ 0 [ T j ] = T , there must exists some j such that E θ 0 [ T j ] ≤ T / K . This gives inf ˆ µ sup θ ∈ { 0 , 1 } K P ( ˆ µ 6 = OR ( θ ) ) ≥ 1 4 · exp (cid:18) − T · D KL ( p k 1 − p ) K (cid:19) . On the other hand , K is naturally a lower bound for query complexity since one has to query each element at least once . Thus we arrive at a lower bound of Ω ( K + K log ( 1 / δ ) / D KL ( p k 1 − p ) ) . Note that this is equivalent to Ω ( K / ( 1 − H ( p ) ) + K log ( 1 / δ ) / D KL ( p k 1 − p ) ) up to a constant factor when δ < 0 . 49 . The reason is that when p is bounded away from 0 , ( 1 − H ( p ) ) / D KL ( p k 1 − p ) is always some constant . When p is close to 0 , 1 − H ( p ) is within constant factor of 1 . 6 Remark II . 2 . Compared with the existing tightest bound Ω ( K log ( 1 / δ ) / log ( ( 1 − p ) / p ) ) in Feige et al . ( 1994 ) , the rate is greatly improved as p → 0 or p → 1 / 2 . On the other hand , the best known upper bound from Feige et al . ( 1994 ) , which is O ( K log ( 1 / δ ) 1 − H ( p ) ) . We include its algorithm and analysis in Appendix I . Theorem II . 1 shows that when δ is bounded away from 0 , one needs at least C · K / ( 1 − H ( p ) ) samples , matching the upper bound . Similarly , when p is bounded away from 0 , the term D KL ( p k 1 − p ) is also within a constant factor of 1 − H ( p ) , thus the upper and lower bounds match . The only regime where the upper and lower bounds do not match is the case when both p and δ go to 0 . We conjecture that a better upper bound is needed in this case . B . Non - adaptive Sampling In the case of non - adaptive sampling , we show that O ( K ) queries are not enough . And one needs Ω ( K log ( K ) ) queries . Theorem II . 3 . In the non - adaptive sampling setting , where the sampling procedure is restricted to taking independent samples from a sequence of distributions p 1 , · · · , p T , we have inf ˆ µ sup θ ∈ { 0 , 1 } K P ( ˆ µ 6 = OR ( θ ) ) ≥ 12 · (cid:16) 1 − r 12 K (cid:16) (cid:16) 1 + ( 1 − 2 p ) 2 K ( 1 − p ) p (cid:17) T − 1 (cid:17)(cid:17) . This shows that the query complexity is at least Ω ( max ( K , K log ( K ) p / ( 1 − H ( p ) ) ) ) . We provide the proof below . Different from the case of adaptive sampling , for non - adaptive sampling we target for a rate of Ω ( K log ( K ) ) . And a standard Le Cam’s two point method is not sufﬁcient to give the extra logarithmic factor . Thus we provide a new proof based on a point versus mixture extension of Le Cam’s method . Proof of Theorem II . 3 . Consider the instance 0 which has all 0 as its elements . For instance 1 , we deﬁne it as a distribution q over K instances 1 k , k ∈ [ K ] which puts probability p k on the k - th element . We will determine the value of p k later . Here instance 1 k refers to the case when k - th element is 1 , and the rest elements are 0 . Now from Le Cam’s two point lemma , we have inf ˆ µ sup θ ∈ { 0 , 1 } K P ( ˆ µ 6 = OR ( θ ) ) ≥ 1 2 ( 1 − TV ( P 1 , P 0 ) ) = 1 2 ( 1 − TV ( E j ∼ q [ P 1 j ] , P 0 ) ) ≥ 1 2 1 − r 1 2 χ 2 ( E j ∼ q [ P 1 j ] , P 0 ) ! . Here the last inequality is based on the inequality TV ≤ q 12 χ 2 . Let π tm be the probability of querying the m - th 7 element in round t . We can further calculate the χ 2 divergence as χ 2 ( E [ P 1 j ( · ) ] , P 0 ( · ) ) ( a ) = E j , j ′ ∼ q " X x P 1 j ( x ) P 1 j ′ ( x ) P 0 ( x ) # − 1 ( b ) = E j , j ′ " T Y t = 1 K X m = 1 ( π tm ) 2 ( 1 − p ) 1 ( j 6 = m ) + 1 ( j ′ 6 = m ) p 1 ( j = m ) + 1 ( j ′ = m ) π tm ( 1 − p ) + ( π tm ) 2 ( 1 − p ) 1 ( j = m ) + 1 ( j ′ = m ) p 1 ( j 6 = m ) + 1 ( j ′ 6 = m ) π tm p ! # − 1 = E j , j ′ " T Y t = 1 K X m = 1 π tm ( 1 − p ) 1 ( j 6 = m ) + 1 ( j ′ 6 = m ) p 1 ( j = m ) + 1 ( j ′ = m ) 1 − p + ( 1 − p ) 1 ( j = m ) + 1 ( j ′ = m ) p 1 ( j 6 = m ) + 1 ( j ′ 6 = m ) p ! ! # − 1 = E j , j ′ " T Y t = 1 K X m = 1 π tm (cid:18) 1 + ( 1 − 2 p ) 2 · 1 ( j = j ′ = m ) ( 1 − p ) p (cid:19) ! # − 1 = 1 − K X j = 1 p 2 j + K X j = 1 p 2 j T Y t = 1 1 + π tj ( 1 − 2 p ) 2 ( 1 − p ) p ! − 1 = − K X j = 1 p 2 j + K X j = 1 p 2 j T Y t = 1 1 + π tj ( 1 − 2 p ) 2 ( 1 − p ) p ! , where ( a ) follows from Lemma A . 5 , and ( b ) follows from the tensorization property of χ 2 for tensor products , a direct result of Lemma A . 5 . Now denote T j = P Tt = 1 π tj . By Jensen’s inequality , we know that Q Tt = 1 ( 1 + π tj ( 1 − 2 p ) 2 ( 1 − p ) p ) ≤ ( 1 + T j ( 1 − 2 p ) 2 T ( 1 − p ) p ) T . Thus we have χ 2 ( P 0 ( · ) , E [ P 1 j ( · ) ] ) ≤ − K X j = 1 p 2 j + K X j = 1 p 2 j (cid:18) 1 + T j ( 1 − 2 p ) 2 T ( 1 − p ) p (cid:19) T . Now we take p j = ( ( 1 + T j ( 1 − 2 p ) 2 T ( 1 − p ) p ) T − 1 ) − 1 / 2 / ( P j ( ( 1 + T j ( 1 − 2 p ) 2 T ( 1 − p ) p ) T − 1 ) − 1 / 2 ) . We have χ 2 ( P 0 ( · ) , E [ P 1 j ( · ) ] ) ≤ K ( P Kj = 1 ( ( 1 + T j ( 1 − 2 p ) 2 T ( 1 − p ) p ) T − 1 ) − 1 / 2 ) 2 . Since P j T j = T , by Jensen’s inequality , we have P Kj = 1 ( ( 1 + T j ( 1 − 2 p ) 2 T ( 1 − p ) p ) T − 1 ) − 1 / 2 ≥ K ( ( 1 + ( 1 − 2 p ) 2 K ( 1 − p ) p ) T − 1 ) − 1 / 2 . Thus χ 2 ( P 0 ( · ) , E [ P 1 j ( · ) ] ) ≤ 1 K · (cid:18) 1 + ( 1 − 2 p ) 2 K ( 1 − p ) p (cid:19) T − 1 ! . This gives the desired result . Now solving the inequality δ ≥ 1 2 ·   1 − vuut 1 2 K (cid:18) 1 + ( 1 − 2 p ) 2 K ( 1 − p ) p (cid:19) T − 1 !   , we arrive at T ≥ log ( 1 + 2 K ( 1 − 2 δ 2 ) ) / log ( 1 + ( 1 − 2 p ) 2 K ( 1 − p ) p ) & K log ( K ) p / ( 1 − 2 p ) 2 . 8 Remark II . 4 . Compared with the bound Ω ( K log ( K ) log ( ( 1 − p ) / p ) ) in ( Reischuk and Schmeltz , 1991 ; G´acs and G´al , 1994 ) , our lower bound is tighter when p → 1 / 2 , but looser when p → 0 . Thus the tightest lower bound is a maximum of K and the two lower bounds . The corresponding repetition - based upper bound O ( K log ( K / δ ) 1 − H ( p ) ) can be derived by a union - bound based argument . Compared with the upper bound O ( K log ( K / δ ) 1 − H ( p ) ) , the lower bound is tight with respect to all parameters when both p and δ are bounded away from 0 . III . C OMPUTING THE MAX FUNCTION Let θ ∈ [ 0 , 1 ] K be a sequence of length K representing the true values of each element . MAX ( θ ) be the index of the maximum value in the sequence . We also let ˆ µ be any algorithm that ( possibly randomly ) queries any noisy comparison between two elements in T rounds , and output a ( possibly random ) decision from 0 , 1 . A . Adaptive Sampling We have the following minimax lower bound for the adaptive setting . The proof is deferred to Appendix B . Theorem III . 1 . In the adaptive setting , we have inf ˆ µ sup θ ∈ [ 0 , 1 ] K P ( ˆ µ 6 = MAX ( θ ) ) ≥ 12 · exp (cid:16) − T · D KL ( p k 1 − p ) K (cid:17) . Thus , the number of queries required to recover the true value with probability at least 1 − δ is lower bounded by Ω ( K / ( 1 − H ( p ) ) + K log ( 1 / δ ) / D KL ( p k 1 − p ) ) . The comparison with the existing lower bound ( Feige et al . , 1994 ) for computing MAX is similar as that for OR in Remark II . 2 , and is thus omitted here . B . Non - adaptive Sampling In the case of non - adaptive sampling , we show that O ( K ) samples are not enough . Instead , one needs at least Ω ( K 2 ) samples . We have the following result . The proof is deferred to Appendix C . Theorem III . 2 . In the non - adaptive setting , where the sampling procedure is restricted to taking independent samples from a sequence of distributions p 1 , · · · , p T , we have inf ˆ µ sup θ ∈ [ 0 , 1 ] K P ( ˆ µ 6 = MAX ( θ ) ) ≥ 14 · exp (cid:16) − T · D KL ( p k 1 − p ) K 2 (cid:17) . Thus the queries required to recover the true value with probability at least 1 − δ is lower bounded by Ω ( K 2 / ( 1 − H ( p ) ) + K 2 log ( 1 / δ ) / D KL ( p k 1 − p ) ) . Remark III . 3 . Compared with the repetition - based upper bound O ( K 2 log ( K ) ) , the lower bound has a log ( K ) gap . The tight dependence on K remains open . IV . C OMPUTING THE SEARCH FUNCTION A . Adaptive Sampling Recall that for any sorted sequence X 1 , · · · , X K , the SEARCH function for X is deﬁned as SEARCH ( X ; X 1 , · · · , X K ) = i , where i satisﬁes X i < X < X i + 1 . We start with adaptive setting . The proof is deferred to Appendix D . Theorem IV . 1 . In the adaptive setting , we have inf ˆ µ sup X P ( ˆ µ 6 = SEARCH ( X ) ) ≥ 14 · max (cid:16) exp ( − T · D KL ( p k 1 − p ) ) , 1 − T · ( 1 − H ( p ) ) log ( K ) (cid:17) . Thus the queries required to recover the true value with probability at least 1 − δ is lower bounded by Ω ( log ( K ) / ( 1 − H ( p ) ) + log ( 1 / δ ) / D KL ( p k 1 − p ) ) . Remark IV . 2 . The same lower bound is also proven in two different manners in the concurrent work ( Wang et al . , 2023 ; Gu and Xu , 2023 ) . And a matching upper bound that is tight with respect to all parameters when p is bounded away from 0 and 1 / 2 is provided in Gu and Xu ( 2023 ) . 9 B . Non - adaptive Sampling For non - adaptive sampling , we show Ω ( K ) queries are needed . The proof is deferred to Appendix E . Theorem IV . 3 . In the non - adaptive setting where the sampling procedure is restricted to taking independent samples from a sequence of distributions p 1 , · · · , p T , we have inf ˆ µ sup X P ( ˆ µ 6 = SEARCH ( X ) ) ≥ 14 · exp (cid:16) − T · D KL ( p k 1 − p ) K (cid:17) . Thus , the queries required to recover the true value with probability at least 1 − δ is lower bounded by Ω ( K / ( 1 − H ( p ) ) + K log ( 1 / δ ) / D KL ( p k 1 − p ) ) . Remark IV . 4 . We show in Appendix H the upper bound O ( K log ( 1 / δ ) / ( 1 − H ( p ) ) ) for computing SEARCH . Our lower bound matches with all parameters when either p or δ is bounded away from 0 . V . C OMPUTING THE SORT FUNCTION A . Adaptive Sampling Let θ ∈ [ 0 , 1 ] K be any sequence of K elements with distinct values in [ 0 , 1 ] , SORT ( θ ) be the result of the SORT function applied on the noiseless sequence . We also let ˆ µ be any algorithm that queries any noisy comparison between two elements in T rounds , and output a ( possibly random ) decision . We have the following result for computing the SORT function . The proof is deferred to Appendix F . Theorem V . 1 . In the adaptive setting , we have inf ˆ µ sup θ ∈ [ 0 , 1 ] K P ( ˆ µ 6 = SORT ( θ ) ) ≥ 1 4 max (cid:18) exp (cid:18) − T · D KL ( p k 1 − p ) K (cid:19) , 1 − T · ( 1 − H ( p ) ) K log ( K ) (cid:19) . Thus , the queries required to recover the true value with probability at least 1 − δ is lower bounded by Ω ( K log ( K ) / ( 1 − H ( p ) ) + K log ( K / δ ) / D KL ( p k 1 − p ) ) ) . Remark V . 2 . Compared with the upper bound O ( K log ( K / δ ) / ( 1 − H ( p ) ) ) , the bound is tight with all parameters when either p or δ is bounded away from 0 . B . Non - adaptive Sampling Here we provide the following minimax lower bound for non - adaptive learning . The proof is deferred to Appendix G . Theorem V . 3 . In the non - adaptive setting where the sampling procedure is restricted to taking independent samples from a sequence of distributions p 1 , · · · , p T , inf ˆ µ sup θ ∈ [ 0 , 1 ] K P ( ˆ µ 6 = SORT ( θ ) ) ≥ 1 2 · (cid:18) 1 − T · D KL ( p k 1 − p ) K 2 log ( K ) (cid:19) . Thus , the queries required to recover the true value with probability at least 1 − δ is lower bounded by Ω ( max ( K 2 , K 2 log ( K ) / D KL ( p k 1 − p ) ) ) . Remark V . 4 . Compared with the repetition - based upper bound O ( K 2 log ( K / δ ) / ( 1 − H ( p ) ) ) , the lower bound is tight with all parameters when p and δ are bounded away from 0 . VI . M ATCHING B OUNDS FOR V ARIABLE L ENGTH In this section , we provide matching upper and lower bounds for the variable - length setting . All the bounds here are the same as the lower bound for the ﬁxed - length setting , the proof of which can be directly adapted from the ﬁxed - length results . Theorem VI . 1 . In the adaptive setting , the number of queries in expectation to achieve at most δ error probability is 1 ) Θ ( K 1 − H ( p ) + K log ( 1 / δ ) D KL ( p k 1 − p ) ) for computing OR ; 10 2 ) Θ ( K 1 − H ( p ) + K log ( 1 / δ ) D KL ( p k 1 − p ) ) for computing MAX ; 3 ) Θ ( log ( K ) 1 − H ( p ) + log ( 1 / δ ) D KL ( p k 1 − p ) ) for computing SEARCH ; 4 ) Θ ( K log ( K ) 1 − H ( p ) + K log ( K / δ ) D KL ( p k 1 − p ) ) for computing SORT . The proof is deferred to Appendix J . The matching upper bound for SEARCH is given in ( Gu and Xu , 2023 ) in the regime when p is some constant that is bounded away from 0 and 1 / 2 , where they make it tight even for the dependency on the constant . Our results for the other three functions improve both existing upper and lower bounds , and provide tight query complexity with all parameters in the variable - length setting . ( Wang et al . , 2023 ) initiates the study for constant - wise matching bounds for SORT when δ = Ω ( K − 1 ) , which is achieved by ( Gu and Xu , 2023 ) . Our bounds are tight up to constant for arbitrarily small δ . Algorithm 1 Variable - length tournament for computing OR with noise 1 : Input : Target conﬁdence level δ . 2 : Set X = ( X 1 , X 2 , · · · , X K ) as the list of all bits with unknown value . 3 : for iteration i = 1 : ⌈ log 2 ( K ) ⌉ do 4 : for iteration j = 1 : ⌈ | X | / 2 ⌉ do 5 : Set a = 1 / 2 , ˜ δ i = δ 2 ( 2 i − 1 ) . 6 : while a ∈ ( ˜ δ i , 1 − ˜ δ i ) do 7 : Query the ( 2 j − 1 ) - th element once . 8 : If observe 1 , update a = ( 1 − p ) a ( 1 − p ) a + p ( 1 − a ) . Otherwise update a = pa pa + ( 1 − p ) ( 1 − a ) . 9 : If a ≤ ˜ δ i , remove the ( 2 j − 1 ) - th element , otherwise remove the 2 j - th element . 10 : Break when X only has one element left . 11 : Set a = 1 / 2 . 12 : while a ∈ ( δ , 1 − δ ) do 13 : Query the only left element in X element once . 14 : If observe 1 , update a = ( 1 − p ) a ( 1 − p ) a + p ( 1 − a ) . Otherwise update a = pa pa + ( 1 − p ) ( 1 − a ) . 15 : If a ≤ δ , return 0 , otherwise return 1 . We provide the upper bound algorithm for computing OR in Algorithm 1 . For the upper bound , one major difference is that to compare two elements with error probability at most δ , one needs O ( log ( 1 / δ ) / D KL ( p k 1 − p ) + 1 / ( 1 − 2 p ) ) queries in expectation , which can be achieved by keep comparing the two elements until the posterior distribution reaches the desired conﬁdence level ( see e . g . Lemma 13 of Gu and Xu ( 2023 ) ) . But the best known bound for ﬁxed - length is O ( log ( 1 / δ ) / ( 1 − H ( p ) ) ( Feige et al . , 1994 ) . This makes it simpler to achieve tight rate for variable length . In Algorithm 1 , we adapt the noisy comparison oracle in Gu and Xu ( 2023 ) to noisy query oracle on each element , and combine with the original ﬁxed - length algorithm in Feige et al . ( 1994 ) . In each round , the algorithm eliminates half of the elements in the current set by querying the elements with odd indices . If the ( 2 j − 1 ) - th element is determined to be 1 , the 2 j - th element will be removed without being queried . If the ( 2 j − 1 ) - th element is determined to be 0 , it will be removed from the list . Thus after O ( log ( K ) ) rounds , we have only one element left in the set , and it sufﬁces to query this element to determine the output of OR function . VII . C ONCLUSIONS AND F UTURE W ORK For four noisy computing tasks — the OR function from noisy queries , and the MAX , SEARCH , and SORT functions from noisy pairwise comparisons — we tighten the lower bounds for ﬁxed - length noisy computing and provide matching bounds for variable - length noisy computing . Making the bounds match exactly in the ﬁxed - length setting remains an important open problem . A CKNOWLEDGEMENTS The authors would like to thank Yanjun Han for insightful discussions on the information - theoretic lower bounds . This work was supported in part by the NSERC Discovery Grant No . RGPIN - 2019 - 05448 , the NSERC Collaborative Research and Development Grant CRDPJ 54367619 , NSF Grants IIS - 1901252 and CCF - 1909499 . 11 R EFERENCES A . Agarwal , S . Agarwal , S . Assadi , and S . Khanna . Learning with limited rounds of adaptivity : Coin tossing , multi - armed bandits , and ranking from pairwise comparisons . In Proceedings of the 2017 Conference on Learning Theory , volume 65 of Proceedings of Machine Learning Research , pages 39 – 75 . PMLR , 07 – 10 Jul 2017 . N . Ailon . Active learning ranking from pairwise preferences with almost optimal query complexity . In J . Shawe - Taylor , R . Zemel , P . Bartlett , F . Pereira , and K . Q . Weinberger , editors , Advances in Neural Information Processing Systems , volume 24 . Curran Associates , Inc . , 2011 . N . Ailon , M . Charikar , and A . Newman . Aggregating inconsistent information : Ranking and clustering . J . ACM , 55 ( 5 ) , nov 2008 . ISSN 0004 - 5411 . doi : 10 . 1145 / 1411509 . 1411513 . N . Ailon , R . Begleiter , and E . Ezra . A new active learning scheme with applications to learning to rank from pairwise preferences . 2011 . M . Ajtai , V . Feldman , A . Hassidim , and J . Nelson . Sorting and selection with imprecise comparisons . In ACM Trans . Algorithms , volume 12 , pages 37 – 48 , 07 2009 . doi : 10 . 1007 / 978 - 3 - 642 - 02927 - 1 5 . J . - Y . Audibert , S . Bubeck , and R . Munos . Best arm identiﬁcation in multi - armed bandits . In COLT , pages 41 – 53 , 2010 . P . Auer , N . Cesa - Bianchi , Y . Freund , and R . E . Schapire . Gambling in a rigged casino : The adversarial multi - armed bandit problem . In Proceedings of IEEE 36th annual foundations of computer science , pages 322 – 331 . IEEE , 1995 . E . R . Berlekamp . Block coding with noiseless feedback . Ph . D . thesis , MIT , Cambridge , MA , USA , 1964 . R . A . Bradley and M . E . Terry . Rank analysis of incomplete block designs : I . the method of paired comparisons . Biometrika , 39 ( 3 / 4 ) : 324 – 345 , 1952 . ISSN 00063444 . M . Braverman and E . Mossel . Sorting from noisy information . 2009 . URL https : / / arxiv . org / abs / 0910 . 1191 . J . Bretagnolle and C . Huber . Estimation des densit´es : risque minimax . Zeitschrift f¨ur Wahrscheinlichkeitstheorie und Verwandte Gebiete , 47 ( 2 ) : 119 – 137 , 1979 . doi : 10 . 1007 / BF00535278 . S . Bubeck , R . Munos , and G . Stoltz . Pure exploration in multi - armed bandits problems . In International conference on Algorithmic learning theory , pages 23 – 37 . Springer , 2009 . M . V . Burnashev . Data transmission over a discrete channel with feedback . random transmission time . Problemy Peredachi Informatsii , 12 ( 4 ) : 10 – 30 , 1976 . M . V . Burnashev and K . Zigangirov . An interval estimation problem for controlled observations . Problemy Peredachi Informatsii , 10 ( 3 ) : 51 – 61 , 1974 . X . Chen , Y . Chen , and X . Li . Asymptotically optimal sequential design for rank aggregation . Math . Oper . Res . , 10 2017 . doi : 10 . 1287 / moor . 2021 . 1209 . Y . Chen and C . Suh . Spectral MLE : Top - K rank aggregation from pairwise comparisons . In F . Bach and D . Blei , editors , Proceedings of the 32nd International Conference on Machine Learning , volume 37 of Proceedings of Machine Learning Research , pages 371 – 380 , Lille , France , 07 – 09 Jul 2015 . PMLR . R . L . Dobrushin and S . Ortyukov . Lower bound for the redundancy of self - correcting arrangements of unreliable functional elements . Problemy Peredachi Informatsii , 13 ( 1 ) : 82 – 89 , 1977a . R . L . Dobrushin and S . Ortyukov . Upper bound on the redundancy of self - correcting arrangements of unreliable functional elements . Problemy Peredachi Informatsii , 13 ( 3 ) : 56 – 76 , 1977b . W . Evans and N . Pippenger . Average - case lower bounds for noisy boolean decision trees . SIAM Journal on Computing , 28 ( 2 ) : 433 – 446 , 1998 . M . Falahatgar , A . Orlitsky , V . Pichapati , and A . T . Suresh . Maximum selection and ranking under noisy comparisons . In D . Precup and Y . W . Teh , editors , Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 1088 – 1096 . PMLR , 06 – 11 Aug 2017 . U . Feige , P . Raghavan , D . Peleg , and E . Upfal . Computing with noisy information . SIAM Journal on Computing , 23 ( 5 ) : 1001 – 1018 , 1994 . V . Gabillon , M . Ghavamzadeh , and A . Lazaric . Best arm identiﬁcation : A uniﬁed approach to ﬁxed budget and ﬁxed conﬁdence . Advances in Neural Information Processing Systems , 25 , 2012 . P . G´acs and A . G´al . Lower bounds for the complexity of reliable boolean circuits with noisy gates . IEEE Transactions on Information Theory , 40 ( 2 ) : 579 – 583 , 1994 . A . Garivier and E . Kaufmann . Optimal best arm identiﬁcation with ﬁxed conﬁdence . In Conference on Learning 12 Theory , 2016 . Y . Gu and Y . Xu . Optimal bounds for noisy sorting , 2023 . R . Heckel , N . B . Shah , K . Ramchandran , and M . J . Wainwright . Active ranking from pairwise comparisons and when parametric assumptions do not help . Ann . Stat . , 47 ( 6 ) : 3099 – 3126 , 2019 . M . Horstein . Sequential transmission using noiseless feedback . IEEE Trans . Inf . Theory , 9 ( 3 ) : 136 – 143 , 1963 . doi : 10 . 1109 / TIT . 1963 . 1057832 . Y . Ingster , J . I . Ingster , and I . Suslina . Nonparametric goodness - of - ﬁt testing under Gaussian models , volume 169 . Springer Science & Business Media , 2003 . K . Jamieson and R . Nowak . Best - arm identiﬁcation algorithms for multi - armed bandits in the ﬁxed conﬁdence setting . In 48th Annual Conference on Information Sciences and Systems , pages 1 – 6 , 2014 . K . G . Jamieson and R . D . Nowak . Active ranking using pairwise comparisons . In Proceedings of the 24th International Conference on Neural Information Processing Systems , NIPS’11 , page 2240 – 2248 , Red Hook , NY , USA , 2011 . Curran Associates Inc . ISBN 9781618395993 . R . M . Karp and R . Kleinberg . Noisy binary search and its applications . In Proceedings of the Eighteenth Annual ACM - SIAM Symposium on Discrete Algorithms , SODA ’07 , page 881 – 890 , USA , 2007 . Society for Industrial and Applied Mathematics . ISBN 9780898716245 . E . Kaufmann , O . Capp´e , and A . Garivier . On the complexity of best arm identiﬁcation in multi - armed bandit models . Journal of Machine Learning Research , 17 : 1 – 42 , 2016 . L . LeCam . Convergence of estimates under dimensionality restrictions . The Annals of Statistics , pages 38 – 53 , 1973 . C . Mao , J . Weed , and P . Rigollet . Minimax rates and efﬁcient algorithms for noisy sorting . In Proceedings of Algorithmic Learning Theory , volume 83 of Proceedings of Machine Learning Research , pages 821 – 847 . PMLR , 07 – 09 Apr 2018 . M . Mitzenmacher and E . Upfal . Probability and computing : Randomization and probabilistic techniques in algorithms and data analysis . Cambridge university press , 2017 . S . Mohajer , C . Suh , and A . Elmahdy . Active learning for top - k rank aggregation from noisy comparisons . In Proceedings of the 34th International Conference on Machine Learning - Volume 70 , ICML’17 , page 2488 – 2497 . JMLR . org , 2017 . S . Negahban , S . Oh , and D . Shah . Iterative ranking from pair - wise comparisons . In Advances in Neural Information Processing Systems , volume 25 , 2012 . A . Pelc . Searching with known error probability . Theoretical Computer Science , 63 ( 2 ) : 185 – 202 , 1989 . N . Pippenger , G . D . Stamoulis , and J . N . Tsitsiklis . On a lower bound for the redundancy of reliable networks with noisy gates . IEEE Transactions on Information Theory , 37 ( 3 ) : 639 – 643 , 1991 . A . Rajkumar and S . Agarwal . A statistical convergence perspective of algorithms for rank aggregation from pairwise data . In Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32 , page I – 118 – I – 126 , 2014 . R . Reischuk and B . Schmeltz . Reliable computation with noisy circuits and decision trees - a general n log n lower bound . In [ 1991 ] Proceedings 32nd Annual Symposium of Foundations of Computer Science , pages 602 – 611 . IEEE Computer Society , 1991 . W . Ren , J . Liu , and N . B . Shroff . Pac ranking from pairwise and listwise queries : Lower bounds and upper bounds . 2018 . A . R´enyi . On a problem of information theory . MTA Mat . Kut . Int . Kozl . B , 6 ( MR143666 ) : 505 – 516 , 1961 . N . B . Shah and M . J . Wainwright . Simple , robust and optimal ranking from pairwise comparisons . J . Mach . Learn . Res . , 18 ( 199 ) : 1 – 38 , 2018 . N . B . Shah , S . Balakrishnan , A . Guntuboyina , and M . J . Wainwright . Stochastically transitive models for pairwise comparisons : Statistical and computational issues . In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48 , ICML’16 , page 11 – 20 . JMLR . org , 2016 . A . B . Tsybakov . Introduction to nonparametric estimation . Springer , 9 ( 10 ) , 2004 . S . Ulam . Adventures of a mathematician . Charles Scribner’s Sons , New York , NY , USA , 1976 . J . Von Neumann . Probabilistic logics and the synthesis of reliable organisms from unreliable components . Automata studies , 34 : 43 – 98 , 1956 . Z . Wang , N . Ghaddar , and L . Wang . Noisy sorting capacity . In 2022 IEEE International Symposium on Information Theory ( ISIT ) , pages 2541 – 2546 . IEEE , 2022 . Z . Wang , N . Ghaddar , B . Zhu , and L . Wang . Noisy sorting capacity , 2023 . URL https : / / arxiv . org / abs / 2202 . 01446 . 13 F . Wauthier , M . Jordan , and N . Jojic . Efﬁcient ranking from pairwise comparisons . In Proceedings of the 30th International Conference on Machine Learning , volume 28 of Proceedings of Machine Learning Research , pages 109 – 117 , Atlanta , Georgia , USA , 17 – 19 Jun 2013 . B . Yu . Assouad , fano , and le cam . In Festschrift for Lucien Le Cam , pages 423 – 435 . Springer , 1997 . A PPENDIX A U SEFUL L EMMAS Here , we introduce several important lemmas . Lemma A . 1 ( Le Cam’s Two Point Lemma , see e . g . ( Yu , 1997 ) ) . For any θ 0 , θ 1 ∈ Θ , suppose that the following separation condition holds for some loss function L ( θ , a ) : Θ × A → R : ∀ a ∈ A , L ( θ 0 , a ) + L ( θ 1 , a ) ≥ ∆ > 0 . Then we have inf f sup θ E θ [ L ( θ , f ( X ) ) ] ≥ ∆ 2 ( 1 − TV ( P θ 0 , P θ 1 ) ) . Lemma A . 2 ( Point vs Mixture , see e . g . ( Ingster et al . , 2003 ) ) . For any θ 0 ∈ Θ , Θ 1 ⊂ Θ , suppose that the following separation condition holds for some loss function L ( θ , a ) : Θ × A → R : ∀ θ 1 ∈ Θ 1 , a ∈ A , L ( θ 0 , a ) + L ( θ 1 , a ) ≥ ∆ > 0 . Then for any probability measure µ supported on Θ 1 , we have inf f sup θ E θ [ L ( θ , f ( X ) ) ] ≥ ∆ 2 (cid:0) 1 − TV ( P θ 0 , E µ ( dθ ) [ P θ 1 ] ) (cid:1) . Lemma A . 3 ( Divergence Decomposition ( Auer et al . , 1995 ) ) . Let T i be the random variable denoting the number of times experiment i ∈ [ K ] is performed under some policy π , then for two distributions P π , Q π under policy π , D KL ( P π , Q π ) = X i ∈ [ K ] E P π [ T i ] D KL ( P πi , Q πi ) . Lemma A . 4 ( An upper Bound of Bretagnolle – Huber inequality ( ( Bretagnolle and Huber , 1979 ) , and Lemma 2 . 6 in Tsybakov ( 2004 ) ) ) . For any distribution P 1 , P 2 , one has TV ( P 1 , P 2 ) ≤ 1 − 1 2 exp ( − D KL ( P 1 , P 2 ) ) . Lemma A . 5 . ( Chi - Square divergence for mixture of distributions , see e . g . ( Ingster et al . , 2003 ) ) Let ( P θ ) θ ∈ Θ be a family of distributions parametrized by θ , and Q be any ﬁxed distribution . Then for any probability measure µ supported on Θ , we have χ 2 ( E θ ∼ µ [ P θ ] , Q ) = E θ ∼ µ , θ ′ ∼ µ " X x P θ ( x ) P θ ′ ( x ) Q ( x ) # − 1 . Here θ , θ ′ in the expectation are independent . Lemma A . 6 . [ Chernoff’s bound for Binomial random variables ( Mitzenmacher and Upfal , 2017 , Exercise 4 . 7 ) ] If X ∼ Bin ( n , λn ) , then for any η ∈ ( 0 , 1 ) , we have P ( X ≥ ( 1 + η ) λ ) ≤ (cid:18) e η ( 1 + η ) ( 1 + η ) (cid:19) λ ( 2 ) P ( X ≤ ( 1 − η ) λ ) ≤ (cid:18) e − η ( 1 − η ) ( 1 − η ) (cid:19) λ ≤ e − η 2 λ / 2 . ( 3 ) 14 A PPENDIX B P ROOF FOR T HEOREM III . 1 Proof . We ﬁrst select an arbitrary sequence 0 < X 1 < X 2 < · · · < X K < 1 . Let θ 0 = ( X 1 , X 2 , X 3 , · · · , X K ) be original sequence which has its largest value in the K - th element . Thus MAX ( θ 0 ) = K . Now for any i ∈ [ K − 1 ] , we design θ i = ( X 1 , · · · , X i − 1 , X K , X i , X i + 1 , · · · , X K − 1 ) , i . e . , we move the K - th element in θ 0 and insert it between X i − 1 and X i . Let T i , j be the random variable that represents the number of comparisons between the i - th item and j - th item in the T rounds . We know that MAX ( θ i ) = i for all i ∈ [ K − 1 ] . Following a similar proof as Theorem II . 1 , we know that inf ˆ µ sup θ ∈ { 0 , 1 } K P ( ˆ µ 6 = MAX ( θ ) ) ≥ sup 1 ≤ j ≤ K − 1 1 2 ( 1 − TV ( P θ 0 , P θ j ) ) ≥ sup 1 ≤ j ≤ K − 1 1 4 exp ( − D KL ( P θ 0 , P θ j ) ) ≥ sup 1 ≤ j ≤ K − 1 1 4 exp ( − K X l = j + 1 E θ 0 [ T j , l ] · D KL ( p k 1 − p ) ) . Now since P i , j ∈ [ K ] , i < j E θ 0 [ T i , j ] = T , there must exists some j such that P Kl = j + 1 E θ 0 [ T j , l ] ≤ T / K . This gives inf ˆ µ sup θ ∈ { 0 , 1 } K P ( ˆ µ 6 = MAX ( θ ) ) ≥ 1 4 · exp (cid:18) − T · D KL ( p k 1 − p ) K (cid:19) . On the other hand , K is naturally a lower bound for the query complexity since one has to query each element at least once . Thus we arrive at a lower bound of Ω ( max ( K , K log ( 1 / δ ) / D KL ( p k 1 − p ) ) ) . Note that this is equivalent to Ω ( K / ( 1 − H ( p ) ) + K log ( 1 / δ ) / D KL ( p k 1 − p ) ) up to a constant factor when δ < 0 . 49 . The reason is that when p is bounded away from 0 , ( 1 − H ( p ) ) / D KL ( p k 1 − p ) is always some constant . When p is close to 0 , 1 − H ( p ) is within constant factor of 1 . A PPENDIX C P ROOF FOR T HEOREM III . 2 Proof . Consider an arbitrary sequence 0 < X 1 < X 2 < · · · < X K < 1 . Since P i , j ∈ [ K ] , i < j E θ 0 [ T i , j ] = T , there must exists some pair ( i , j ) such that E θ 0 [ T i , j ] ≤ 2 T / K ( K − 1 ) . Now we construct θ 0 = ( X 1 , X 2 , X 3 , · · · , X K , · · · , X K − 1 , · · · , X K − 2 ) , where X K lies in the i - th position and X K − 1 lies in the j - th position , and θ 1 = ( X 1 , X 2 , X 3 , · · · , X K − 1 , · · · , X K , · · · , X K − 2 ) , where X K lies in the j - th position and X K − 1 lies in the i - th position . Thus MAX ( θ 0 ) = i , MAX ( θ 1 ) = j . From Le Cam’s two point lemma , we know that inf ˆ µ sup θ ∈ { 0 , 1 } K P ( ˆ µ 6 = MAX ( θ ) ) ≥ 1 2 ( 1 − TV ( P θ 0 , P θ 1 ) ) ≥ 1 4 exp ( − E θ 0 [ T i , j ] · D KL ( p k 1 − p ) ) ≥ 1 4 · exp (cid:18) − 2 T · D KL ( p k 1 − p ) K 2 (cid:19) . On the other hand , K 2 is naturally a lower bound for the query complexity since one has to query each element at least once . Thus we arrive at a lower bound of Ω ( max ( K 2 , K 2 log ( 1 / δ ) / D KL ( p k 1 − p ) ) ) . Note that this is equivalent to Ω ( K 2 / ( 1 − H ( p ) ) + K 2 log ( 1 / δ ) / D KL ( p k 1 − p ) ) up to a constant factor when δ < 0 . 49 . 15 A PPENDIX D P ROOF FOR T HEOREM IV . 1 Proof . We begin with the ﬁrst half , i . e . inf ˆ µ sup X P ( ˆ µ 6 = SEARCH ( X ) ) ≥ 1 4 · exp ( − T · D KL ( p k 1 − p ) ) . To see this , simply consider the case of K = 1 , and we need to determine whether X < X 1 or X > X 1 from their pairwise comparisons . We consider two instances X ( 0 ) , X ( 1 ) , where X ( 0 ) < X 1 < X ( 1 ) . From Le Cam’s lemma , we have inf ˆ µ sup X P ( ˆ µ 6 = SEARCH ( θ ) ) ≥ 1 2 ( 1 − TV ( P X ( 0 ) , P X ( 1 ) ) ) ≥ 1 4 exp ( − T · D KL ( p k 1 − p ) ) . Next , we aim to prove the second half , namely inf ˆ µ sup X P ( ˆ µ 6 = SEARCH ( X ) ) ≥ 1 2 · (cid:18) 1 − T · ( 1 − H ( p ) ) + log ( 2 ) log ( K ) (cid:19) . Now we design K instances X ( 0 ) , · · · , X ( K − 1 ) . We let X ( 0 ) < X 1 , and X ( l ) ∈ ( X l , X l + 1 ) . From Le Cam’s lemma , we have inf ˆ µ sup X P ( ˆ µ 6 = SEARCH ( θ ) ) ≥ inf ˆ µ sup X ∈ { X l } l ∈ [ K ] P ( ˆ µ 6 = SEARCH ( θ ) ) ≥ inf Ψ 1 2 K X l ∈ [ K ] P X ( l ) ( Ψ 6 = l ) . Now by Fano’s inequality , we have inf Ψ 1 2 K X l ∈ [ K ] P ( l ) ( Ψ 6 = l ) ≥ 1 2 · (cid:18) 1 − I ( V ; Y ) + log ( 2 ) log ( K ) (cid:19) . Here V ∼ Unif ( { 0 , 1 , · · · , K − 1 } ) ) and Y satisﬁes P Y | V = l = P X ( l ) . Following the same argument as the proof of Theorem 3 in Wang et al . ( 2022 ) , we know that I ( V ; Y ) ≤ T · ( 1 − H ( p ) ) . Thus overall we have inf ˆ µ sup X P ( ˆ µ 6 = SEARCH ( X ) ) ≥ 1 4 · (cid:18) 1 − T · ( 1 − H ( p ) ) log ( K ) (cid:19) . Thus the queries required to recover the true value with probability at least 1 − δ is lower bounded by Ω ( log ( K ) / ( 1 − H ( p ) ) + log ( 1 / δ ) / D KL ( p k 1 − p ) ) . A PPENDIX E P ROOF FOR T HEOREM IV . 3 Proof . Let T i be the random variable that denotes the number of times X is compared with X i . Since P i ∈ [ K ] E [ T i ] = T , there must exists some i such that E [ T i ] ≤ T / K . Now we construct the ﬁrst instance 16 X ( 0 ) ∈ ( X i − 1 , X i ) , and the second instance X ( 1 ) ∈ ( X i , X i + 1 ) . From Le Cam’s two point lemma , we know that inf ˆ µ sup X P ( ˆ µ 6 = SEARCH ( X ) ) ≥ 1 2 ( 1 − TV ( P X ( 0 ) , P X ( 1 ) ) ) ≥ 1 4 exp ( − E [ T i ] · D KL ( p k 1 − p ) ) ≥ 1 4 · exp (cid:18) − T · D KL ( p k 1 − p ) K (cid:19) . On the other hand , K is naturally a lower bound for query complexity since one has to query each element at least once . Thus we arrive at a lower bound of Ω ( max ( K , K log ( 1 / δ ) / D KL ( p k 1 − p ) ) ) . A PPENDIX F P ROOF FOR T HEOREM V . 1 Proof . From ( Wang et al . , 2023 ) , it is proven with Fano’s inequality that inf ˆ µ sup θ ∈ [ 0 , 1 ] K P ( ˆ µ 6 = SORT ( θ ) ) ≥ 1 2 · (cid:18) 1 − T · ( 1 − H ( p ) ) K log ( K ) (cid:19) . So it sufﬁces to prove that inf ˆ µ sup θ ∈ [ 0 , 1 ] K P ( ˆ µ 6 = SORT ( θ ) ) ≥ 1 4 · exp (cid:18) − T · D KL ( p k 1 − p ) K (cid:19) . To see this , consider an arbitrary sequence 0 < X 1 < X 2 < · · · < X K < 1 . Now we design K instances θ 0 , · · · , θ K − 1 . We let θ 0 = ( X 1 , · · · , X K ) , and θ l be the instance that switches the element X l with X l + 1 , where l ∈ [ K ] . From Le Cam’s two point lemma , we have inf ˆ µ sup θ ∈ [ 0 , 1 ] K P ( ˆ µ 6 = SORT ( θ ) ) ≥ sup 1 ≤ j ≤ K − 1 1 2 ( 1 − TV ( P θ 0 , P θ j ) ) ≥ sup 1 ≤ j ≤ K − 1 1 4 exp ( − D KL ( P θ 0 , P θ j ) ) ≥ sup 1 ≤ j ≤ K − 1 1 4 exp ( − E θ 0 [ T j , j + 1 ] · D KL ( p k 1 − p ) ) . Let T i , j be the random variable that represents the number of comparisons between the i - th item and j - th item in the T rounds . Now since P 1 ≤ j ≤ K − 1 E θ 0 [ T j , j + 1 ] ≤ T , there must exists some j such that E θ 0 [ T j , j + 1 ] ≤ T / K . This gives inf ˆ µ sup θ ∈ [ 0 , 1 ] K P ( ˆ µ 6 = SORT ( θ ) ) ≥ 1 4 · exp (cid:18) − T · D KL ( p k 1 − p ) K (cid:19) . Altogether , this shows a lower bound on the query complexity Ω ( K log ( K ) / ( 1 − H ( p ) ) + K log ( 1 / δ ) / D KL ( p k 1 − p ) ) ) . Note that this is equivalent to Ω ( K log ( K ) / ( 1 − H ( p ) ) + K log ( K / δ ) / D KL ( p k 1 − p ) ) ) since 1 / ( 1 − H ( p ) ) & 1 / D KL ( p k 1 − p ) . A PPENDIX G P ROOF FOR T HEOREM V . 3 Proof . We ﬁrst select an arbitrary sequence 0 < X 1 < X 2 < · · · < X K < 1 . Let σ i : [ K ] 7→ [ K ] be the i - th permutation of the sequence , where i ∈ [ K ! ] . Here σ i ( k ) refers to the k - th largest element under permutation 17 σ i . Now we consider a summation P i ∈ [ K ! ] , j ∈ [ K − 1 ] E [ T σ i ( j ) , σ i ( j + 1 ) ] . For each pair ( i , j ) , E [ T i , j ] is counted 2 ( K − 1 ) ! times in the summation . Furthermore , we know that P i < j E [ T i , j ] = T . Thus we have X i ∈ [ K ! ] , j ∈ [ K − 1 ] E [ T σ i ( j ) , σ i ( j + 1 ) ] = 2 T ( K − 1 ) ! We know that there must exists some i such that X j ∈ [ K ] E [ T σ i ( j ) , σ i ( j + 1 ) ] ≤ 2 T ( K − 1 ) ! K ! = 2 T K . Now we design K instances θ 0 , · · · , θ K − 1 . We let θ 0 = ( X σ i ( 1 ) , · · · , X σ i ( K ) ) , and θ l be the instance that switches the element X σ i ( l ) with X σ i ( l + 1 ) based on θ 0 , where l ∈ [ K − 1 ] . From Le Cam’s two point lemma , we have inf ˆ µ sup θ ∈ { 0 , 1 } K P ( ˆ µ 6 = SORT ( θ ) ) ≥ inf ˆ µ sup θ ∈ { θ l } l ∈ [ K ] P ( ˆ µ 6 = SORT ( θ ) ) ≥ inf Ψ 1 2 K X l ∈ [ K ] P θ l ( Ψ 6 = l ) . Now by Fano’s inequality , we have inf Ψ 1 2 K X l ∈ [ K ] P θ l ( Ψ 6 = l ) ≥ 1 2 · (cid:18) 1 − I ( V ; X ) + log ( 2 ) log ( K ) (cid:19) = 1 2 · 1 − P l ∈ [ K ] D KL ( P l , ¯ P ) / K + log ( 2 ) log ( K ) ! . Here ¯ P = 1 K P l ∈ [ K ] P l . We can compute D KL ( P l , ¯ P ) ≤ E [ T σ i ( l ) , σ i ( l + 1 ) ] D KL (cid:18) 1 − p k ( K − 1 ) p + ( 1 − p ) K (cid:19) + X m 6 = l E [ T σ i ( m ) , σ i ( m + 1 ) ] D KL (cid:18) p k ( K − 1 ) p + ( 1 − p ) K (cid:19) ≤ K − 1 K · E [ T σ i ( l ) , σ i ( l + 1 ) ] D KL ( p k 1 − p ) + 1 K X m 6 = l E [ T σ i ( m ) , σ i ( m + 1 ) ] D KL ( p k 1 − p ) . Now summing over all l , we have 1 K X l ∈ [ K ] D KL ( P l , ¯ P ) ≤ 2 D KL ( p k 1 − p ) K · X l ∈ [ K ] E [ T σ i ( l ) , σ i ( l + 1 ) ] ≤ 4 T D KL ( p k 1 − p ) K 2 . This gives inf ˆ µ sup θ ∈ { 0 , 1 } K P ( ˆ µ 6 = OR ( θ ) ) ≥ 1 2 · (cid:18) 1 − 4 T · D KL ( p k 1 − p ) K 2 log ( K ) (cid:19) . This shows that to output the correct answer with probability at least 2 / 3 , one needs at least C · K 2 log ( K ) / D KL ( p k 1 − p ) queries for some universal constant C . 18 A PPENDIX H U PPER B OUNDS FOR N ON - ADAPTIVE S AMPLING In this section , we present a theorem on the upper bounds for non - adaptive learning in the worst - case query model . Theorem H . 1 . One can design algorithm such that the worst - case query complexity is 1 ) O ( K log ( K / δ ) 1 − H ( p ) ) for computing OR ; 2 ) O ( K 2 log ( K / δ ) 1 − H ( p ) ) for computing MAX ; 3 ) O ( K log ( 1 / δ ) 1 − H ( p ) ) for computing SEARCH ; 4 ) O ( K 2 log ( K / δ ) 1 − H ( p ) ) for computing SORT ; The results for OR , MAX and SORT are based the simple algorithms of querying all possible elements equal number of times . And the analysis is a direct union bound argument . Here we only present the algorithm and analysis for SEARCH . Proof . Assume that the target X lies between X l and X l + 1 . Consider the non - adaptive learning algorithm which compares X with each element X i for T i = ⌊ T / K ⌋ = 4 log ( 1 / δ ) / ( 1 − H ( p ) ) times . Let N i be the number of observations of 1 among T i queries for element X i . Consider the following algorithm : ˆ l = arg max l ∈ [ K ] l X i = 1 N i + K X i = l + 1 ( T i − N i ) . We show that with high probability , ˆ l = l . We have P ( ˆ l 6 = l ) ≤ X j 6 = l P ( ˆ l = j ) ≤ X j 6 = l P (cid:16) l X i = 1 N i + K X i = l + 1 ( T i − N i ) − ( j X i = 1 N i + K X i = j + 1 ( T i − N i ) ) < 0 (cid:17) . Now we bound the above probability for each j . Without loss of generality , we assume that j > l . The above probability can be written as P (cid:16) l X i = 1 N i + K X i = l + 1 ( T i − N i ) < j X i = 1 N i + K X i = j + 1 ( T i − N i ) (cid:17) = P (cid:16) j X i = l + 1 ( T i − 2 N i ) < 0 (cid:17) = P (cid:16) j X i = l + 1 N i > 1 2 ( j − l ) ⌊ T / K ⌋ (cid:17) Note that P ji = l + 1 N i ∼ Bin ( ( j − l ) ⌊ T / K ⌋ , p ) . Let n = 12 ( j − l ) ⌊ T / K ⌋ . From Lemma A . 6 we have P ( j X i = l + 1 N i ≥ n / 2 ) ≤ e 1 − 2 p 2 p ( 1 / 2 p ) ( 1 / 2 p ) ! np = ( 2 p · exp ( 1 − 2 p ) ) n / 2 < exp (cid:18) log ( 1 / δ ) · 2 ( j − l ) ( 1 − 2 p + log ( 2 p ) ) ( 1 − 2 p ) 2 (cid:19) < δ j − l . Now by summing over the probability for different j ′ s , we get P ( ˆ l 6 = l ) ≤ X j 6 = l δ | j − l | < 2 δ 1 − δ . Rescaling δ ﬁnishes the proof . 19 A PPENDIX I U PPER B OUNDS FOR A DAPTIVE S AMPLING Here we present the tournament algorithm for computing OR introduced in ( Feige et al . , 1994 ) . Similar algorithm can also be applied to compute MAX . The main difference is that in MAX , we directly compare two elements ⌈ 4 ( 2 i − 1 ) log ( 1 / δ ) ( 1 − H ( p ) ) ⌉ times instead of comparing their number of 1 ′ s . Algorithm 2 Tournament for computing OR with noise 1 : Input : Target conﬁdence level δ . 2 : Set X = ( X 1 , X 2 , · · · , X K ) as the list of all bits with unknown value . 3 : for iteration i = 1 : ⌈ log 2 ( K ) ⌉ do 4 : Query ⌈ 4 ( 2 i − 1 ) log ( 1 / δ ) ( 1 − H ( p ) ) ⌉ times each of the element in X . 5 : for iteration j = 1 : ⌈ | X | / 2 ⌉ do 6 : Compare the number of 1 ’s in the queries from the ( 2 j − 1 ) - th element and ( 2 j ) - th element , remove the element with smaller number of 1 ’s from the list X . Ties are broken arbitrarily . ( If the ( 2 j ) - th element does not exist , we will not remove the ( 2 j − 1 ) - th element . ) 7 : Break when X only has one element left . 8 : Query ⌈ 6log ( 1 / δ ) ( 1 − H ( p ) ) ⌉ times the only left element in X . Return 1 if there is more than half 1 ’s , and 0 otherwise . The following theorem is due to ( Feige et al . , 1994 ) . We include it here for completeness . Theorem I . 1 . Algorithm 2 ﬁnishes within C · K log ( 1 / δ ) / ( 1 − H ( p ) ) queries , and outputs the correct value of OR ( X 1 , · · · , X K ) with probability at least 1 − 2 δ when δ < 1 / 2 . Proof . First , we compute the total number of queries of the algorithm . Without loss of generality , we may assume that K can be written as 2 m for some integer m . If not we may add no more than K extra dummy 0 ’s to the original list X to make sure K = 2 m . In each of the outer iteration i , the size of X is decreased half . We know that after ⌈ log 2 ( K ) ⌉ round , the set X will only contain one element . In round i , the number of queries we make for each element is ⌈ 4 ( 2 i − 1 ) log ( 1 / δ ) ( 1 − 2 p ) 2 ⌉ . The total number of queries we make is (cid:24) 4 log ( 1 / δ ) ( 1 − 2 p ) 2 (cid:25) + ⌈ log 2 ( K ) ⌉ X i = 1 (cid:24) 4 ( 2 i − 1 ) log ( 1 / δ ) ( 1 − 2 p ) 2 (cid:25) · K 2 i − 1 ≤ (cid:24) 4 log ( 1 / δ ) ( 1 − 2 p ) 2 (cid:25) + ⌈ log 2 ( K ) ⌉ X i = 1 (cid:18) 4 ( 2 i − 1 ) log ( 1 / δ ) ( 1 − 2 p ) 2 + 1 (cid:19) · K 2 i − 1 ≤ (cid:24) 4 log ( 1 / δ ) ( 1 − 2 p ) 2 (cid:25) + 2 K + K log ( 1 / δ ) ( 1 − 2 p ) 2 ⌈ log 2 ( K ) ⌉ X i = 1 4 ( 2 i − 1 ) 2 i − 1 ≤ (cid:24) 4 log ( 1 / δ ) ( 1 − 2 p ) 2 (cid:25) + 2 K + 28 K log ( 1 / δ ) ( 1 − 2 p ) 2 ≤ CK log ( 1 / δ ) 1 − H ( p ) Here in last inequality we use the fact below , which can be veriﬁed numerically : ∀ p ∈ [ 0 , 1 ] , ( 1 / 2 − p ) 2 1 − H ( p ) ∈ [ 1 / 4 , 1 / 2 ] . ( 4 ) Now we show that the failure probability of the algorithm is at most δ . Consider the ﬁrst case where all the elements are 0 . Then no matter which element is left in X , the probability that the algorithm fails is the probability that a Binomial random variable X ∼ B ( n , np ) has value larger or equal to n / 2 with n = l 4log ( 1 / δ ) ( 1 − 2 p ) 2 m . 20 Taking λ = np , η = 1 − 2 p 2 p in Equation ( 2 ) of Lemma A . 6 , we know that P ( X ≥ n / 2 ) ≤ e 1 − 2 p 2 p ( 1 / 2 p ) ( 1 / 2 p ) ! np = ( 2 p · exp ( 1 − 2 p ) ) n / 2 < exp (cid:18) log ( 1 / δ ) · 2 ( 1 − 2 p + log ( 2 p ) ) ( 1 − 2 p ) 2 (cid:19) < δ . Here the last inequality uses the fact that ( 1 − 2 p + log ( 2 p ) ) ( 1 − 2 p ) 2 < − 1 / 2 for all p ∈ [ 0 , 1 / 2 ) . This shows that the ﬁnal failure probability is bounded by δ when the true elements are all 0 . Consider the second case where there exists at least a 1 in the original elements X 1 , · · · , X K . Without loss of generality , we assume that X 1 = 1 . Let X i be the remaining list of elements at the beginning of i - th iteration . We let E i be the event that the ﬁrst element in X i is 1 while the ﬁrst element in X i + 1 is 0 . This event only happens when the second element in X i is 0 and gets more 1 ’s in the noisy queries than the ﬁrst element . Let A denote the event that the only left element is 1 in the last round , we have P ( A ) ≥ 1 − P   ⌈ log 2 ( K ) ⌉ [ i = 1 E i   ≥ 1 − ⌈ log 2 ( K ) ⌉ X i = 1 P ( E i ) ≥ 1 − ⌈ log 2 ( K ) ⌉ X i = 1 P ( Y i − X i ≥ 0 ) , where X i ∼ B ( n i , n i ( 1 − p ) ) and Y i ∼ B ( n i , n i p ) , with n i = ⌈ 4 ( 2 i − 1 ) log ( 1 / δ ) ( 1 − 2 p ) 2 ⌉ . Let Z i = Y i − X i + n i , we know that the random variable Z i ∼ B ( 2 n i , n i p ) . Thus we have P ( A ) ≥ 1 − ⌈ log 2 ( K ) ⌉ X i = 1 P ( Z i ≥ n i ) > 1 − ⌈ log 2 ( K ) ⌉ X i = 1 δ 2 ( 2 i − 1 ) > 1 − δ 2 1 − δ 2 . Following the same argument on Binomial distribution , we can upper bound the error probability under event A with δ . Thus the total failure probability is upper bounded by δ + δ 2 / ( 1 − δ 2 ) < 2 δ when δ < 1 / 2 . A PPENDIX J P ROOF OF T HEOREM VI . 1 A . Lower Bounds First , note that all our lower bounds for ﬁxed - length can be adapted to variable - length by replacing T with E [ T ] . The bound of mutual information in Fano’s inequality in Section D can be proven using the same argument as Lemma 27 in Gu and Xu ( 2023 ) , and the divergence decomposition lemma ( Lemma A . 3 ) still holds for variable length due to Lemma 15 in Kaufmann et al . ( 2016 ) . 21 B . Upper Bounds Now it sufﬁces to prove the upper bounds . For OR and MAX , we already know from Theorem I . 1 that O ( K log ( 1 / δ ) / ( 1 − H ( p ) ) ) is an upper bound for ﬁxed - length setting when comparing two elements with error probability at most δ requires ⌈ C log ( 1 / δ ) / ( 1 − H ( p ) ) ⌉ samples for some constant C . Now for variable - length setting , we know that comparing two elements with error probability at most δ only requires log ( 1 / δ ) / D KL ( p k 1 − p ) + 1 / ( 1 − 2 p ) queries in expectation via the variable - length comparison algorithm in Lemma 13 of Gu and Xu ( 2023 ) . Thus in Algorithm 1 , we replace the repetition - based comparisons in Algorithm 2 with the new variable - length comparison algorithm . This gives the query complexity O ( K / ( 1 − H ( p ) ) + K log ( 1 / δ ) / D KL ( p k 1 − p ) ) . Similar algorithm can also be applied to compute MAX . The main difference is that in MAX , we directly compare two elements instead of ﬁnding the number of 1 ′ s of the ﬁrst element . Theorem J . 1 . The expected number of total queries made by Algorithm 1 is upper bounded by C · (cid:16) K 1 − H ( p ) + K log ( 1 / δ ) D KL ( p k 1 − p ) (cid:17) . Furthermore , the algorithm outputs the correct value of OR ( X 1 , · · · , X K ) with probability at least 1 − 2 δ when δ < 1 / 2 . Proof . First , we compute the total number of queries of the algorithm . Without loss of generality , we may assume that K can be written as 2 m for some integer m . If not we may add no more than K extra dummy 0 ’s to the original list X to make sure K = 2 m . In each of the outer iteration i , the size of X is decreased half . We know that after ⌈ log 2 ( K ) ⌉ iterations , the set X will only contain one element . From Lemma 27 in Gu and Xu ( 2023 ) , the expected number of queries we make is O ( log ( 1 / ˜ δ i ) / D KL ( p k 1 − p ) + 1 / ( 1 − 2 p ) ) at round i . Thus the total number of queries we make is log ( 1 / δ ) D KL ( p k 1 − p ) + 1 1 − 2 p + C · ⌈ log 2 ( K ) ⌉ X i = 1 log ( 1 / ˜ δ i ) D KL ( p k 1 − p ) + 1 1 − 2 p ! · K 2 i − 1 ≤ C · ⌈ log 2 ( K ) ⌉ X i = 1 (cid:18) ( 4 i − 2 ) log ( 1 / δ ) D KL ( p k 1 − p ) + 1 1 − 2 p (cid:19) · K 2 i − 1 ≤ C ·   K 1 − 2 p + K log ( 1 / δ ) D KL ( p k 1 − p ) · ⌈ log 2 ( K ) ⌉ X i = 1 4 ( 2 i − 1 ) 2 i − 1   ≤ C · (cid:18) K 1 − 2 p + K log ( 1 / δ ) D KL ( p k 1 − p ) (cid:19) ≤ C · (cid:18) K 1 − H ( p ) + K log ( 1 / δ ) D KL ( p k 1 − p ) (cid:19) . Here in last inequality we use the fact below , which can be veriﬁed numerically : ∀ p ∈ [ 0 , 1 ] , ( 1 / 2 − p ) 2 1 − H ( p ) ∈ [ 1 / 4 , 1 / 2 ] . ( 5 ) Now we show that the failure probability of the algorithm is at most δ . Consider the ﬁrst case where all the elements are 0 . Then no matter which element is left in X , the probability that the algorithm fails is the probability that the last while loop gives wrong output . From Lemma 27 in Gu and Xu ( 2023 ) , we know that such probability is less than δ . Consider the second case where there exists at least a 1 in the original elements X 1 , · · · , X K . Without loss of generality , we assume that X 1 = 1 . Let X i be the remaining list of elements at the beginning of i - th iteration . We let E i be the event that the ﬁrst element in X i is 1 while the ﬁrst element in X i + 1 is 0 . This event only happens when the while loop ends with a < ˜ δ i for the ﬁrst element at the i - th round . Let A denote the event 22 that the only left element is 1 in the last round , we have P ( A ) ≥ 1 − P   ⌈ log 2 ( K ) ⌉ [ i = 1 E i   ≥ 1 − ⌈ log 2 ( K ) ⌉ X i = 1 P ( E i ) > 1 − ⌈ log 2 ( K ) ⌉ X i = 1 δ 2 ( 2 i − 1 ) > 1 − δ 2 1 − δ 2 . We can upper bound the error probability under event A with δ . Thus the total failure probability is upper bounded by δ + δ 2 / ( 1 − δ 2 ) < 2 δ when δ < 1 / 2 . The upper bound for SEARCH is due to Gu and Xu ( 2023 ) , and is thus omitted here . The upper bound for SORT is based on that for SEARCH . We can design an insertion - based sorting algorithm by adding elements sequentially to an initially empty sorted set via noisy searching , as in Algorithm 1 in ( Wang et al . , 2023 ) . Since the insertion step requires O ( log ( K ) / ( 1 − H ( p ) ) + log ( 1 / δ ) / D KL ( p k 1 − p ) ) queries . Overall we know that one needs O ( P Kk = 1 ( log ( k ) / ( 1 − H ( p ) ) + log ( 1 / δ ) / D KL ( p k 1 − p ) ) ) = O ( K log ( K ) / ( 1 − H ( p ) ) + K log ( 1 / δ ) / D KL ( p k 1 − p ) ) queries to achieve error probability at most Kδ . Rescaling δ gives the ﬁnal rate .