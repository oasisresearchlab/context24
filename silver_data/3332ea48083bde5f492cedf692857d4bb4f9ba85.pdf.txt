IEEE ROBOTICS AND AUTOMATION LETTERS . PREPRINT VERSION . ACCEPTED JULY , 2022 1 Model - Based Imitation Learning Using Entropy Regularization of Model and Policy Eiji Uchibe 1 Abstract â€”Approaches based on generative adversarial net - works for imitation learning are promising because they are sam - ple efï¬cient in terms of expert demonstrations . However , training a generator requires many interactions with the actual environ - ment because model - free reinforcement learning is adopted to update a policy . To improve the sample efï¬ciency using model - based reinforcement learning , we propose model - based Entropy - Regularized Imitation Learning ( MB - ERIL ) under the entropy - regularized Markov decision process to reduce the number of interactions with the actual environment . MB - ERIL uses two discriminators . A policy discriminator distinguishes the actions generated by a robot from expert ones , and a model discriminator distinguishes the counterfactual state transitions generated by the model from the actual ones . We derive structured discriminators so that the learning of the policy and the model is efï¬cient . Computer simulations and real robot experiments show that MB - ERIL achieves a competitive performance and signiï¬cantly improves the sample efï¬ciency compared to baseline methods . Index Terms â€”Imitation Learning , Reinforcement Learning , Machine Learning for Robot Control I . INTRODUCTION D EEP Reinforcement Learning ( RL ) using deep neural networks learns much better than manually designed policies ( action rules ) for problems where the environmental dynamics is completely described on a computer . Unfor - tunately , many problems remain in applying RL to robot control tasks , especially two signiï¬cant ones : ( 1 ) specifying reward functions and ( 2 ) the cost of collecting data in real environments . Imitation learning is a promising method for overcoming the ï¬rst problem because it ï¬nds a policy from expert demon - strations . In particular , recent imitation learning is related to Generative Adversarial Networks ( GANs ) [ 1 ] , [ 2 ] , and this framework has two components : a discriminator and a generator . Training the former , which distinguishes demon - strations generated by a robot from expert demonstrations , corresponds to inverse RL [ 3 ] , [ 4 ] . Training the latter , which produces expert - like demonstrations , corresponds to policy Manuscript received : February , 24 , 2022 ; Revised May , 20 , 2022 ; Accepted July , 11 , 2022 . This paper was recommended for publication by Associate Editor J . Garcia and Editor D . Kulic upon evaluation of the reviewersâ€™ comments . This work was supported by Innovative Science and Technology Initiative for Security Grant Number JPJ004596 , ATLA , Japan and partially based on results obtained from project JPNP20006 commissioned by the New Energy and Industrial Technology Development Organization ( NEDO ) . This work was partially supported by JSPS KAKENHI Grant Number JP21H03527 . 1 Eiji Uchibe is with the Department of Brain Robot Interface , ATR Computational Neuroscience Laboratories , Kyoto 619 - 0288 , Japan . uchibe @ atr . jp Digital Object Identiï¬er ( DOI ) : see top of this page . ( b ) ( a ) ğ· p o l i c y expertor learner ğ’Ÿ ğ¸ ğ’Ÿ ğ¿ expertpolicy learner policy actualenvironment ğ· m o d e l real or generated ğ’Ÿ ğ¸ âˆª ğ’Ÿ ğ¿ ğ’Ÿ ğº model expertpolicy learner policy actual environment ğ· p o l i c y expert or learner ğ’Ÿ ğ¸ ğ’Ÿ ğ¿ âˆª ğ’Ÿ ğº expertpolicy learner policy actual environment model Fig . 1 . Comparison of available datasets and discriminators between model - free and model - based imitation learning : ( a ) Model - free setting : Expert dataset D E and learnerâ€™s dataset D L are created through interactions with actual environments . Policy discriminator D policy exists . ( b ) Our proposed model - based setting : Simulated dataset D G is additionally generated from learnerâ€™s policy and model . Two discriminators exist : D model and D policy . improvement by forward RL 1 . Some previous studies [ 2 ] , [ 5 ] showed that such imitation learning approaches achieve high sample efï¬ciency in terms of the number of demonstrations . However , they also often suffer from sample inefï¬ciency in the generatorâ€™s training step because a model - free on - policy forward RL , which usually requires many environmental inter - actions , is adopted to update the policy . Several studies [ 6 ] , [ 7 ] , [ 8 ] , [ 9 ] employed model - free off - policy RL . However , these methods remain sample inefï¬cient as a method of real robot control because they require costly interactions with actual environments . Fig . 1 ( a ) illustrates the dataset and the discrim - inator in the model - free setting . Expert dataset D E is gathered by executing an expert policy in an actual environment . Note that the learner also interacts with the actual environment using its own policy to collect learnerâ€™s dataset D G , although its interaction is costly . Adopting a model - based forward RL is promising to reduce the number of costly interactions with the actual environment . To further reduce the number of interactions with the actual environment , we propose Model - Based Entropy - Regularized Imitation Learning ( MB - ERIL ) , which explicitly estimates a 1 Hereafter we refer to RL that ï¬nds an optimal policy from rewards as forward RL to distinguish it from inverse RL . a r X i v : 2206 . 10101v2 [ c s . L G ] 31 A ug 2022 2 IEEE ROBOTICS AND AUTOMATION LETTERS . PREPRINT VERSION . ACCEPTED JULY , 2022 model of the environment and generates simulated data by running the learnerâ€™s policy in the estimated model . MB - ERIL is formulated as the minimization problem of Kullback - Leibler ( KL ) divergence between the expert policy evaluated in the actual environment and the learnerâ€™s policy evaluated in the model environment . MB - ERIL regularizes the policy and model by Shannon entropy and KL divergence to derive the algorithm , where we assume that the expert policy and the actual environment are solutions of the regularized Bellman equation . The following are the contributions of this work : â€¢ It develops two novel discriminators . One is a policy discriminator that differentiates actions generated by the policy from expert actions . The other is that a model discriminator that distinguishes the state transitions gen - erated by the model from the actual transitions provided by an expert . â€¢ The discriminators are represented by the reward , state , and state - action value functions . The value functions are updated by training the discriminators , a step that makes the forward RL efï¬cient . â€¢ For model ï¬tting , MB - ERIL provides another objective alternative to maximum likelihood estimation . â€¢ The estimated model generates counterfactual data to up - date the state and state - action value functions ( Fig . 1 ( b ) ) . As a result , we can reduce the number of costly interac - tions with the actual environment . To evaluate MB - ERIL , we conducted two continuous con - trol benchmark tasks in the MuJoCo simulator [ 10 ] and a vision - based reaching task [ 9 ] using a real upper - body hu - manoid robot . MB - ERIL shows promising experimental results with competitive asymptotic performance and higher sample efï¬ciency than previous studies and empirically ensures low model bias , which is the gap between the actual environment and the model , by jointly updating the model and policy . II . LITERATURE REVIEW A . Model - free imitation learning Relative Entropy Inverse RL [ 11 ] is a sample - based method inspired by Maximum entropy inverse RL ( MaxEntIRL ) [ 12 ] . It provides an efï¬cient way of estimating the partition function of the probability distribution of expert trajectories . How - ever , one drawback is utilizing a ï¬xed sampling distribution , which is unhelpful in practice . Generative Adversarial Imi - tation Learning ( GAIL ) [ 2 ] , which can adapt the sampling distribution using policy optimization , is closely related to GAN . GAIL is more sample efï¬cient than Behavior Cloning ( BC ) with respect to the number of expert demonstrations . However , GAIL often requires many interactions with the environment because ( 1 ) an on - policy model - free forward RL is used for policy improvement and ( 2 ) the discriminator is not structured . Adversarial Inverse Reinforcement Learning ( AIRL ) [ 5 ] and Logistic Regression - based inverse RL [ 13 ] propose a structured discriminator represented by the reward and state - value functions , although an on - policy forward RL updates the policy . Adversarial Soft Advantage Fitting ( ASAF ) [ 14 ] exploits the discriminator inspired by the AIRL discrimi - nator , although the state transition is not considered explicitly . To reduce the number of environmental interactions , an off - policy model - free forward RL has been adopted [ 6 ] , [ 7 ] , [ 15 ] , [ 16 ] . We proposed Model - Free Entropy - Regularized Imitation Learning ( MF - ERIL ) based on entropy regularization that shares the network parameters between the discriminator and the generator . Even though MF - ERIL achieved better sample efï¬ciency than the above methods , there is room to improve the sample efï¬ciency by introducing model learning . MB - ERIL is one instantiation of this approach . Recent approaches corrected the issues around reusing data previously collected while training the discriminator [ 17 ] , [ 18 ] . GAN - like imitation learning often suffers from unstable learning from adversarial training . Therefore , some regular - ization terms are added to BCâ€™s loss function to stabilize the learning process . For example , Soft Q Imitation Learning ( SQIL ) is a regularized BC algorithm , where the squared soft Bellman error is used as a regularizer [ 19 ] . Interestingly , its algorithm can be implemented by assigning a reward of 1 to expert demonstrations and 0 to generated ones . Discriminator Soft Actor - Critic [ 20 ] is an extension of SQIL where its predeï¬ned reward value is replaced by the parameterized reward trained by the discriminator . B . Model - based imitation learning MaxEntIRL , which is a pioneer of an entropy regularized RL , estimates the reward function from expert trajectories based on the maximum entropy principle . MaxEnt IRL is a model - based approach , although how to train the model was not discussed . Although a method was proposed that simultaneously estimated the reward and the model [ 21 ] , maximum likelihood estimation was applied to model learning . A few studies introduced model learning to GAIL to propa - gate the discriminatorâ€™s gradient to the policy for a gradient up - date . Consequently , Model - based GAIL [ 22 ] , [ 23 ] and Model - based AIRL [ 24 ] achieved end - to - end training , although these methods do not sample state transitions from the model while unrolling the trajectories . However , these approaches do not use simulated experiences through interaction with the model , and therefore , they still require many actual interactions with a real environment [ 25 ] . Data - Efï¬cient Adversarial Learning for Imitation from Observation ( DEALIO ) , which is a GAIL - like algorithm [ 26 ] , adopts a model - based RL that is employed for training a policy from the trained discriminator . Since one drawback of DEALIO is that model learning is independent of discriminator training , it suffers from model bias . III . PROPOSED METHOD A . Objective function Consider Markov Decision Process ( MDP ) M = (cid:104)X , U , p , r , Î³ (cid:105) , where X denotes a continuous state space , U denotes an action space , p : X Ã— U Ã— X (cid:55)â†’ R â‰¥ 0 denotes an actual stochastic state transition probability , r : X (cid:55)â†’ R denotes an immediate reward function , and Î³ âˆˆ ( 0 , 1 ) denotes a discount factor that indicates how near and far future rewards are weighed . We chose the state - only function because more UCHIBE : MODEL - BASED IMITATION LEARNING USING ENTROPY REGULARIZATION OF MODEL AND POLICY 3 general reward functions like r : X Ã— U Ã— X (cid:55)â†’ R are often shaped as the result of training the discriminators . Let b : X Ã— U (cid:55)â†’ R â‰¥ 0 denote a stochastic policy to select action u âˆˆ U at state x âˆˆ X . Let Ï€ and q denote an expert policy and a model of p . Note that p and Ï€ are unknown , while q and b are maintained by the learner . MB - ERIL minimizes the following KL divergence : J ( q , b ) = E p L ( x , u , x (cid:48) ) (cid:20) ln p L ( x , u , x (cid:48) ) p E ( x , u , x (cid:48) ) (cid:21) , where p L and p E are respectively the joint density functions deï¬ned by p L ( x , u , x (cid:48) ) = q ( x (cid:48) | x , u ) b ( u | x ) p L ( x ) , p E ( x , u , x (cid:48) ) = p ( x (cid:48) | x , u ) Ï€ ( u | x ) p E ( x ) , where p L ( x ) and p E ( x ) are some initial distributions , and we assume p L ( x ) = p E ( x ) for simplicity . The difï¬culty is how to evaluate the log - ratio because it is unknown . B . Entropic regularization of policy and model The basic idea for estimating the log - ratio is to adopt the density ratio trick [ 27 ] . Then MB - ERIL updates the policy and the model by minimizing the estimated KL divergence . To derive the algorithm , we formulate the entropic regularization of the policy and the model . We add two regularization terms to the immediate reward : r ( x ) + Îº âˆ’ 1 H ( p ( Â· | x , u ) Ï€ ( Â· | x ) ) âˆ’ Î· âˆ’ 1 KL ( p ( Â· | x , u ) Ï€ ( Â· | x ) (cid:107) q ( Â· | x , u ) b ( Â· | x ) ) , where H represents the operator of Shannon entropy , and Îº and Î· denote positive hyperparameters . Then the Bellman optimality equation is given by V ( x ) = max Ï€ E Ï€ (cid:26) âˆ’ Îº âˆ’ 1 ln Ï€ ( u | x ) âˆ’ Î· âˆ’ 1 ln Ï€ ( u | x ) b ( u | x ) + max p E p (cid:20) r ( x ) + Î³V ( x (cid:48) ) âˆ’ Îº âˆ’ 1 ln p ( x (cid:48) | x , u ) âˆ’ Î· âˆ’ 1 ln p ( x (cid:48) | x , u ) q ( x (cid:48) | x , u ) (cid:21)(cid:27) , ( 1 ) where V ( x ) denotes a state - value function . To utilize the framework of entropic regularization , MB - ERIL assumes that Ï€ ( u | x ) and p ( x (cid:48) | x , u ) are the solutions of the Bellman equation ( 1 ) when the learner uses baseline policy b ( u | x ) and baseline state transition q ( x (cid:48) | x , u ) . Using the Lagrangian multiplier method , we obtain the following equations : p ( x (cid:48) | x , u ) = exp [ Î² ( r ( x ) + Î³V ( x (cid:48) ) + Î· âˆ’ 1 ln q ( x (cid:48) | x , u ) ) ] exp [ Î²Q ( x , u ) ] , ( 2 ) Ï€ ( u | x ) = exp [ Î² ( Q ( x , u ) + Î· âˆ’ 1 ln b ( u | x ) ) ] exp ( Î²V ( x ) ) , ( 3 ) where Î² is a positive hyperparameter deï¬ned by Î² (cid:44) ÎºÎ·Îº + Î· . Q ( x , u ) denotes the state - action value function , and the fol - lowing relation exists between V and Q : exp ( Î²Q ( x , u ) ) = (cid:90) exp [ Î² ( r ( x ) + Î³V ( x (cid:48) ) + Î· âˆ’ 1 ln q ( x (cid:48) | x , u ) (cid:3) d x (cid:48) , ( 4 ) exp ( Î²V ( x ) ) = (cid:90) exp (cid:2) Î² ( Q ( x , u ) + Î· âˆ’ 1 ln b ( u | x ) ) (cid:3) d u . ( 5 ) See Appendix A for the derivation . C . Derivation of MB - ERIL discriminators We obtain the following equations from ( 2 ) and ( 3 ) that represent the density ratio : Î² âˆ’ 1 ln p ( x (cid:48) | x , u ) q ( x (cid:48) | x , u ) = r ( x ) + Î³V ( x (cid:48) ) âˆ’ Q ( x , u ) âˆ’ Îº âˆ’ 1 ln q ( x (cid:48) | x , u ) , Î² âˆ’ 1 ln Ï€ ( u | x ) b ( u | x ) = Q ( x , u ) âˆ’ V ( x ) âˆ’ Îº âˆ’ 1 ln b ( u | x ) . Using the density ratio estimation [ 27 ] , we derive a model discriminator that distinguishes the state transition generated by the model from the real transitions provided by the expert : D model ( x (cid:48) | x , u ) = exp ( Î²f ( x , u , x (cid:48) ) ) exp ( Î²f ( x , u , x (cid:48) ) ) + exp ( Î²Îº âˆ’ 1 ln q ( x (cid:48) | x , u ) ) , where f ( x , u , x (cid:48) ) is deï¬ned by f ( x , u , x (cid:48) ) (cid:44) r ( x ) + Î³V ( x (cid:48) ) âˆ’ Q ( x , u ) . Similarly , the policy discriminator is given by D policy ( u | x ) = exp ( Î² ( Q ( x , u ) âˆ’ V ( x ) ) ) exp ( Î² ( Q ( x , u ) âˆ’ V ( x ) ) ) + exp ( Î²Îº âˆ’ 1 ln b ( u | x ) ) , which differentiates the action generated by the policy from the expert action . Note that D policy is the discriminator used by AIRL and MF - ERIL if Q ( x , u ) is replaced with r ( x ) + Î³V ( x (cid:48) ) . These discriminators have the same form as the optimal discriminator , also known as the Bradley - Terry model [ 28 ] . D . MB - ERIL learning In the model - free setting , the learner needs to interact with a real environment to generate trajectories like the expert , making the model - free setting sample inefï¬cient . On the other hand , the learner can generate trajectories by interacting with the real environment and the model . Learning from simulated trajectories is sometimes called Dyna architecture [ 29 ] . To improve the sample efï¬ciency , we adopt Dyna architecture , and therefore , MB - ERIL utilizes the following three datasets . 4 IEEE ROBOTICS AND AUTOMATION LETTERS . PREPRINT VERSION . ACCEPTED JULY , 2022 The ï¬rst is an expert dataset generated by an expert in a real environment : D E = { ( x i , u i , x (cid:48) i ) } N E i = 1 , x i âˆ¼ p E 0 ( Â· ) u i âˆ¼ Ï€ ( Â· | x i ) , x (cid:48) i âˆ¼ p ( Â· | x i , u i ) , where N E is the number of transitions in the dataset and p E 0 is the discounted state distribution [ 30 ] for ( Ï€ , p ) . The second and third are the learnerâ€™s datasets by running b ( u | x ) in the real environment and the model : D L = { ( x i , u i , x (cid:48) i ) } N L i = 1 , x i âˆ¼ p L 0 ( Â· ) u i âˆ¼ b ( Â· | x i ) , x (cid:48) i âˆ¼ p ( Â· | x i , u i ) , D G = { ( x i , u i , x (cid:48) i ) } N G i = 1 , x i âˆ¼ q 0 ( Â· ) u i âˆ¼ b ( Â· | x i ) , x (cid:48) i âˆ¼ q ( Â· | x i , u i ) , where N L and N G are the dataset sizes , and p L 0 and q 0 are the discounted state distributions for ( Ï€ L , p ) and ( Ï€ L , q ) . Normally , N G (cid:29) N L â‰¥ N E , because collecting data in the real environment is expensive . Here we show MB - ERILâ€™s learning rules that consist of ( 1 ) training the discriminators , ( 2 ) policy evaluation , and ( 3 ) policy improvement . First , we show the loss functions for training D model and D policy to estimate r , V , and Q : L model ( r , V , Q ) = âˆ’ E ( x , u , x (cid:48) ) âˆ¼D E âˆªD L [ ln D model ( x (cid:48) | x , u ) ] âˆ’ E ( x , u , x (cid:48) ) âˆ¼D G [ ln ( 1 âˆ’ D model ( x (cid:48) | x , u ) ) ] where E ( x , u , x (cid:48) âˆ¼D ) represents the expectation over a batch of data that is uniformly sampled from an experience replay buffer D . Similarly , the loss function for D policy is given by L policy ( V , Q ) = âˆ’ E ( x , u ) âˆ¼D E [ ln D policy ( u | x ) ] âˆ’ E ( x , u ) âˆ¼D L âˆªD G [ ln ( 1 âˆ’ D policy ( u | x ) ) ] . Note that r , V , and Q are all updated , and q and b remain ï¬xed while training the discriminators . These two loss functions are summarized : L dis ( r , V , Q ) = Î» model L model ( r , V , Q ) + Î» policy L policy ( V , Q ) , ( 6 ) where Î» model and Î» policy denote positive hyperparameters . Training the discriminators is interpreted by an inverse RL because the reward function is estimated . Next we show how the state and state - action value func - tions are updated . This corresponds to the policy evaluation step . Using ( 4 ) and ( 5 ) , the following loss function can be constructed : L ( V , Q ) = Î» QV E x , u (cid:20) (cid:0) Q âˆ’ Î² âˆ’ 1 ln E q (cid:2) exp ( Î² ( r + Î³V (cid:48) âˆ’ Îº âˆ’ 1 ln q ) ) (cid:3)(cid:1) 2 (cid:21) + Î» V Q E x (cid:104)(cid:0) V âˆ’ Î² âˆ’ 1 ln E b (cid:2) exp ( Î² ( Q âˆ’ Îº âˆ’ 1 ln b ) ) (cid:3)(cid:1) 2 (cid:105) , ( 7 ) where Î» QV and Î» V Q denote constant hyperparameters . We simplify the notation by omitting the arguments of the func - tions and setting V (cid:48) (cid:44) V ( x (cid:48) ) for readability . This loss function Algorithm 1 Model - Based Entropy - Regularized Imitation Learning ( MB - ERIL ) Require : Expert dataset D E and hyperparameters Îº and Î· Ensure : Learnerâ€™s policy b , model q , reward r , state - value V , and state - action value Q . 1 : Initialize all parameters of networks and replay buffers D L , D G . 2 : ( Pre - train the Regularized AutoEncoder using D E when raw images are used as inputs . ) 3 : for k = 0 , 1 , 2 , . . . do (cid:46) Collect state transitions in the real environment and the model . 4 : D L â† D L âˆª { ( x t , u t , x t + 1 ) } N L t = 0 with b and p . 5 : D G â† D G âˆª { ( x t , u t , x t + 1 ) } N G t = 0 with b and q . (cid:46) Train the discriminators 6 : Update r , V , and Q by minimizing ( 6 ) . (cid:46) Evaluate policy . 7 : D G â† D G âˆª { ( x t , u t , x t + 1 ) } N G t = 0 with b and q . 8 : Update V and Q by minimizing ( 7 ) . (cid:46) Improve the policy and the model . 9 : Update q and b . See ( 8 ) and ( 9 ) . 10 : end for contains the log - expected - exponent terms , which introduce biases in their gradients [ 31 ] . Empirically , using a biased estimate was sufï¬cient for our experiments , although applying Fenchel conjugates is also promising . Finally , we explain how the model and policy are updated based on the trained discriminators in the policy improvement step . As Soft Actor - Critic does [ 32 ] , they are directly learned by minimizing the expected KL divergence based on ( 2 ) and ( 3 ) : q ( x (cid:48) | x , u ) â† arg min q (cid:48) E x , u (cid:20) KL (cid:18) q (cid:48) (cid:107) exp [ Î² ( r + Î³V (cid:48) + Î· âˆ’ 1 ln q ) ] exp [ Î²Q ] (cid:19)(cid:21) , ( 8 ) b ( u | x ) â† arg min b (cid:48) E x (cid:20) KL (cid:18) b (cid:48) (cid:107) exp [ Î² ( Q + Î· âˆ’ 1 ln b ) ] exp ( Î²V ) (cid:19)(cid:21) . ( 9 ) MB - ERIL is summarized in Algorithm 1 . Lines 7 and 8 correspond to learning from simulated trajectories like Dyna architecture . E . Simpliï¬ed MB - ERIL Here we show two simpliï¬ed MB - ERIL implementations by removing some components . One is MB - ERIL without training the discriminators , which trains r , V , and Q by the maximum likelihood method instead of minimizing ( 6 ) . The loss function is given by L (cid:48) ( r , V , Q ) = âˆ’ E D E [ ln p ( x (cid:48) | x , u ) ] âˆ’ E D E [ ln b ( u | x ) ] + L ( V , Q ) . ( 10 ) The ï¬rst term of the right - hand side of ( 1 ) corresponds to model cloning , and the third term is the regularizer , which UCHIBE : MODEL - BASED IMITATION LEARNING USING ENTROPY REGULARIZATION OF MODEL AND POLICY 5 resembles SQIL . We call this approach Entropy - Regularized Model and Behavior Cloning ( ERMBC ) . The other is MB - ERIL without training the generator in - spired by ASAF . This approach simply skips lines 7 and 8 of Algorithm 1 , meaning that it does not learn from the additional simulated trajectories even though the model is maintained explicitly . We call this approach MB - ERIL without Policy Evaluation ( MB - ERIL \ PE ) . IV . MUJOCO BENCHMARK CONTROL TASKS A . Task description To verify our proposed method , we ï¬rst conducted two benchmark control tasks , Ant and Humanoid , provided by OpenAI gym [ 33 ] . Both use a physics engine called MuJoCo [ 10 ] . The goal of these tasks is to move forward as quickly as possible . First , an optimal policy was trained by the Trust Region Policy Optimization [ 34 ] , based on the original reward function provided by the simulator . Then it was used as an expert policy to collect expert data D E . We adopted the sparse sampling setup [ 8 ] in which we randomly sampled ( x , u , x (cid:48) ) triplets from each trajectory . The functions used by MB - ERIL need to be approximated by artiï¬cial neural networks , and r ( x ) , V ( x ) , Q ( x , u ) , and b ( u | x ) were represented by a two - layer neural network with a derivative of the sigmoid - weighted linear unit ( dSiLU ) [ 35 ] as an activation function , determined based on our previous study [ 9 ] . We deï¬ned the model to output a Gaus - sian distribution conditioned on the state and the action , i . e . : q ( x (cid:48) | x , u ) = N ( x (cid:48) | Âµ ( x , u ) , Î£ ( x , u ) ) , where Âµ ( x , u ) and Î£ ( x , u ) denote the mean vector and the diagonal covariance matrix for simplicity . Future work will investigate more com - plicated distributions , such as a mixture of Gaussians with full covariance . Each was represented by a neural network with two hidden layers , and each layer had 256 nodes with dSiLU activations . B . Comparative evaluation We evaluated the MB - ERIL , ERMBC , and MB - ERIL \ PE performances by comparing them with the following algo - rithms : â€¢ Dyna - MF - ERIL : MF - ERIL [ 9 ] with the Dyna framework [ 29 ] , which generates simulated trajectories by interacting with the model . The model , represented by a separate neural network , is trained by the simple maximum like - lihood method . Then MF - ERIL is trained from real and generated data . â€¢ MF - ERIL : We ignored the ï¬rst discriminator that esti - mates ln p E 0 ( x ) / p L 0 ( x ) for simplicity . â€¢ DAC : Discriminator - Actor - Critic ( DAC ) [ 7 ] , which is baseline model - free imitation learning . â€¢ BC : naive Behavior Cloning that minimizes the negative log - likelihood objective . Following Ho and Ermon [ 2 ] , a single trajectory contains 50 state - action transition pairs ( x , u , x (cid:48) ) , i . e . , 50 steps per episode . The number of trajectories sampled from expert policy Ï€ were set to 30 and 350 in the Ant and Humanoid environments . We set N L = 10 2 and N G = 10 4 . Fig . 2 compares the learning curves 2 . Note that the re - ward functions estimated by each method cannot be directly compared since inverse reinforcement learning is an ill - posed problem . Therefore , the method was evaluated by the mean normalized return : Â¯ R mean = 1 N N (cid:88) i = 1 R i âˆ’ R min R max âˆ’ R min , where i is the index of the experimental run and N is the number of experiments . R i is the raw return of the i - th experimental run , where the original reward of OpenAI gym was used . R min and R max are constants , where R max is given by the return of the expert while R min = 0 . In the Ant environment , the maximum normalized total reward was reached by MB - ERIL , MF - ERIL , DAC , ERMBC , and MB - ERIL \ PE in that order ; Dyna - MF - ERIL and BC did not reach the expert performance . MB - ERIL improved the learning efï¬ciency of the model - free methods ( MF - ERIL and DAC ) by about ten times . Note that the asymptotic performance of Dyna - MF - ERIL was worse than that of MF - ERIL even though the performing model did learn and its normalized return increased rapidly in the early stage of learning . Perhaps the model failed to learn the state transition because model learning by the maximum likelihood method cannot deal with a covariate shift . ERMBC and MB - ERIL \ PE also converged to the expert performance , although they learned much slower than MF - ERIL . Similar results were obtained in the Humanoid environment . MB - ERIL learned faster than the other methods , although MF - ERIL and DAC eventually achieved comparable performance . The performance of MB - ERIL \ PE was worst or inferior to BC . Next we evaluated the data efï¬ciency of the training dis - criminators by changing the number of samples in D E . Fig . 3 shows that MB - ERIL , MB - ERIL \ PE , MF - ERIL , and DAC found policies that exhibited higher control performance with fewer expert demonstrations . On the other hand , ERMBC and Dyna - MF - ERIL just exhibited slight performance degradation when the number of demonstrations was limited , implying that model learning did not improve the sample efï¬ciency with respect to the number of expert demonstrations . However , the MB - ERIl \ PE performance was degraded in the Humanoid environment ; it did not improve even though the number of demonstrations increased . The results indicate that V and Q did not satisfy the soft Bellman equations ( 4 ) and ( 5 ) when they were simply estimated by training the discriminators . We conclude that the policy evaluation of MB - ERIL was more crucial when the state space is large . To see the model and policy differences among MB - ERIL , Dyna - MF - ERIL , and MF - ERIL , we computed the negative log - likelihood ( NLL ) of the ï¬nally - obtained model and policy using test expert data . For example , the modelâ€™s NLL was calculated by NLL = âˆ’ 1 N E test N E test (cid:88) (cid:96) = 1 ln q ( x (cid:48) (cid:96) | x (cid:96) , u (cid:96) ) , 2 We conducted Hopper , Walker , Reacher , and HalfCheetah , all of which we previously evaluated [ 9 ] . Simulation results show that MB - ERIL outperformed MF - ERIL , although we omitted them due to space limitations . 6 IEEE ROBOTICS AND AUTOMATION LETTERS . PREPRINT VERSION . ACCEPTED JULY , 2022 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 number of interactions 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 n o r m a li z e d r e t u r n ( a ) Ant MB - ERIL ERMBC MB - ERIL \ PE Dyna - MF - ERIL MF - ERIL DAC BC 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 number of interactions 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 n o r m a li z e d r e t u r n ( b ) Humanoid MB - ERIL ERMBC MB - ERIL \ PE Dyna - MF - ERIL MF - ERIL DAC BC Fig . 2 . Normalized return versus number of interactions on MuJoCo bench - mark control tasks : Solid lines represent average values , and shaded areas correspond to Â± 1 standard deviation region . Note that horizontal axis is a log scale . where N E test is the number of samples in the test data . The policyâ€™s NLL was deï¬ned similarly . Figs . 4 ( a ) and ( c ) compare the NLLs of the policies on the MuJoCo experiments , showing no signiï¬cant difference between the MB - ERIL and MF - ERIL policies , although Dyna - MF - ERILâ€™s NLL was larger than the others , which led to its poor asymptotic performance . Figs . 4 ( b ) and ( d ) show that the NLL of the model obtained by MB - ERIL was the smallest among the other methods . V . REAL ROBOT CONTROL A . Task description Next we performed a vision - based reaching task [ 9 ] with an upper - body humanoid called Nextage developed by Kawada Industries Inc . Two colored blocks served as obstacles , and one switch button indicated the goal in the workspace . The aim is to move its left armâ€™s end - effector from a starting position ( Fig . 5 ( a ) ) to a target position ( Fig . 5 ( b ) ) as quickly as possible , where the starting and target positions were selected randomly from the pre - deï¬ned positions in Fig . 5 ( c ) . This task is more complicated than the FetchReach provided by OpenAI Gym because the target position must be detected from visual information . Nextage has a head with two cameras , a torso , two 6 - axis manipulators , and two cameras attached to its end - effectors . We mounted cameras on its left arm and head in this task . The head pose was ï¬xed during the experiments . The state was given by x = [ Î¸ 1 , . . . , Î¸ 6 , z (cid:62) ] (cid:62) , where Î¸ i is the i - th joint angle of the left arm and z represents the latent visual state obtained by a deterministic Regularized AutoEn - coder ( RAE ) [ 36 ] . The action was given by the changes in 5 10 15 20 25 30 number of demonstrations 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 n o r m a li z e d r e t u r n ( a ) Ant MB - ERIL ERMBC MB - ERIL \ PE Dyna - MF - ERIL MF - ERIL DAC BC 100 150 200 250 300 350 number of demonstrations 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 n o r m a li z e d r e t u r n ( b ) Humanoid MB - ERIL ERMBC MB - ERIL \ PE Dyna - MF - ERIL MF - ERIL DAC BC Fig . 3 . Normalized return versus number of trajectories provided by expert on MuJoCo benchmark control tasks MB - ERIL ERMBCMB - ERIL \ PEDyna - MF - ERILMF - ERIL DAC 0 1 2 3 4 n e g a t i v e l og - li k e li h oo d ( a ) Ant : policy MB - ERIL ERMBCMB - ERIL \ PEDyna - MF - ERIL 0 1 2 3 4 n e g a t i v e l og - li k e li h oo d ( b ) Ant : model MB - ERIL ERMBCMB - ERIL \ PEDyna - MF - ERILMF - ERIL DAC 0 1 2 3 4 5 n e g a t i v e l og - li k e li h oo d ( c ) Humanoid : policy MB - ERIL ERMBCMB - ERIL \ PEDyna - MF - ERIL 0 1 2 3 4 5 6 n e g a t i v e l og - li k e li h oo d ( d ) Humanoid : model Fig . 4 . Comparison of NLL in Ant and Humanoid environments : Smaller is better because vertical axis is negative log - likelihood . ( a ) and ( c ) NLL of policy . ( b ) and ( d ) NLL of model . Note that MF - ERIL and DAC did not estimate the model . the joint angle from previous position u = [ âˆ† Î¸ 1 , . . . , âˆ† Î¸ 6 ] (cid:62) , where âˆ† Î¸ i is the change in the joint angle from the previous position of the i - th joint ( although we clamped âˆ† Î¸ i in [ âˆ’ 1 , 1 ] . Figs . 5 ( d ) and ( e ) show the entire network architecture that represents the functions and the RAE encoder whose input came from two 160 Ã— 128 RGB images captured by Nextageâ€™s cameras . We prepared several environmental conï¬gurations by chang - UCHIBE : MODEL - BASED IMITATION LEARNING USING ENTROPY REGULARIZATION OF MODEL AND POLICY 7 ğ¼head ğ¼hand ( a ) ğ¼head ğ¼hand ( b ) encoder decoder MLP MLP MLP MLP MLP ğ‘§ captured images ğ¼head , ğ¼hand joint angles ğœƒ ğ‘– action Î”ğœƒ ğ‘– c o n c a t e n a t e c o n c a t e n a t e áˆšğ¼head , áˆšğ¼hand ğ‘Ÿ ğ‘¥ ğ‘‰ ( ğ’™ ) ğ‘ ( ğ’–âˆ£ğ’™ ) ğ‘„ ( ğ’™ , ğ’– ) ğ‘ ğ’™ â€² ğ’™ , ğ’– ( d ) ( e ) ğ¼ h ea d , ğ¼ h a n d I n s t a n t n o r m C o n v . , 3 Ã— 3 , 128 c , s t r i d e 2 R e L U I n s t a n t n o r m C o n v . , 3 Ã— 3 , 128 c , s t r i d e 2 R e L U C o n v . , 3 Ã— 3 , 128 c , s t r i d e 2 R e L U ğ‘§ button ( goal position ) Colored blocks ( obstacles ) ( c ) 700 mm 1000 mm : initial pos . for the expert : initial pos . for the learner : initial pos . for test : target pos . for the expert : target pos . for test Fig . 5 . Vision - based reaching task : ( a ) Starting position . ( b ) Target position . ( c ) Possible environmental conï¬guration . ( d ) Network architecture for reward , state value , policy , state - action value , and model . ( e ) Network architecture for encoder : Conv . denotes a convolutional neural network . â€œ n câ€ denotes â€œ n channels . â€ ing the armâ€™s initial pose , the buttonâ€™s location , and the height of the blocks . We designed three initial and two target poses for the expert conï¬guration ( Fig . 5 ( c ) ) . The learning conï¬gura - tion was given by ï¬ve initial and two target poses . In addition , we prepared blocks of two different heights , resulting in 3 Ã— 2 Ã— 2 = 12 expert conï¬gurations and 5 Ã— 2 Ã— 2 = 20 learning conï¬gurations . We created expert demonstrations with MoveIt ! [ 37 ] using the geometric information of the button and the colored blocks . Note that such information was unavailable for learning the algorithms . MoveIt ! generated 12 trajectories for every expert conï¬guration , and we sampled 50 transitions for each trajectory . Consequently , N E = 12 Ã— 50 expert transitions were obtained . Although the sequence of the joint angles was almost deterministic , the RGB images were stochastic and noisy due to the lighting conditions . In this experiment , N L and N G were set to 20 Ã— 50 and 10 4 . On the other hand , the test conï¬guration was constructed by one initial pose ( red circle ) and one target one ( red square ) that were not included in the expert and learnerâ€™s conï¬gurations . B . Experimental results We compared MB - ERIL with ERMBC , MB - ERIL \ PE , Dyna - MF - ERIL , MF - ERIL , DAC , ASAF , and BC . We mea - sured the performances by investigating a synthetic reward function : r = exp (cid:16) âˆ’ (cid:107) p t âˆ’ p g (cid:107) 22 Ïƒ 2 (cid:17) , where p t and p g denote the end - effectorâ€™s current and target position . Parameter Ïƒ was set to 5 . To evaluate the performance during training , we evaluated the policyâ€™s performance every ten episodes with a test set of initial and goal positions that were not included in the training demonstrations from the expert . Fig . 6 compares the performance , where the horizontal and vertical axes represent the number of environmental inter - actions and normalized returns . MB - ERIL achieved higher sample efï¬ciency than the other baselines . Unlike the MuJoCo experiments , Dyna - MF - ERIL was comparable to MF - ERIL , indicating that the model bias was not serious in this task . 0 50 100 150 200 250 300 number of interactions 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 n o r m a li z e d r e t u r n MB - ERIL ERMBC MB - ERIL \ PE Dyna - MF - ERIL MF - ERIL DAC ASAF BC Fig . 6 . Normalized return versus number of interactions on Nextage vision - based reaching task MB - ERIL ERMBCMB - ERIL \ PEDyna - MF - ERILMF - ERIL DAC ASAF 0 1 2 3 4 5 n e g a t i v e l og - li k e li h oo d ( a ) policy MB - ERILERMBCMB - ERIL \ PEDyna - MF - ERIL 0 1 2 3 4 5 n e g a t i v e l og - li k e li h oo d ( b ) model Fig . 7 . Comparison of NLL on Nextage vision - based reaching task : ( a ) Policyâ€™s NLL and ( b ) Modelâ€™s NLL However , Dyna - MF - ERIL did not exploit the model efï¬ciently because it did not improve the sample efï¬ciency . Fig . 7 shows that the NLL of MB - ERILâ€™s policy was comparable to those of ERMBC , MB - ERIL \ PE , MF - ERIL , and DAC , although the NLL of MB - ERILâ€™s model was the smallest . VI . CONCLUSIONS We proposed MB - ERIL , derived from RL theory , which regularized the policy and the model . The experimental results of the simulated and real robots showed that MB - ERIL im - proved the sample efï¬ciency over MF - ERIL , Dyna - MF - ERIL , DAC , and BC . A comparison between MB - ERIL and Dyna - MF - ERIL suggests that the simple use of data generated by the model was not useful due to the model bias . As explained in Section I , our formulation is based on the assumption that the expert policy and the actual environment are solutions of the regularized Bellman equation , although this assumption is not straightforward . In the future , we will provide a clear derivation of ERIL . Our experimental results on the Ant task show that MB - ERILâ€™s asymptotic performance was slightly worse than that of MF - ERIL due to model bias . One possible solution is to switch the algorithm from MB - ERIL to MF - ERIL based 8 IEEE ROBOTICS AND AUTOMATION LETTERS . PREPRINT VERSION . ACCEPTED JULY , 2022 on the loss function of the model discriminator . MB - ERIL updates V and Q alternatively with respect to two different objective functions . Although the update rules were derived from the Bellman equation ( 1 ) , the convergence and stability were not proved theoretically . In future work , we will apply the technique used in the convergence proof of GAIL [ 38 ] , [ 39 ] . APPENDIX A . Derivation of ( 2 ) - ( 5 ) Consider the maximization problem inside the right - hand side of ( 1 ) . It is a constrained optimization because (cid:82) p ( x (cid:48) | x , u ) d x (cid:48) = 1 . The Lagrangian is given by L 2 = (cid:90) p (cid:18) r âˆ’ Îº âˆ’ 1 p âˆ’ Î· âˆ’ 1 ln p q + Î³V (cid:48) (cid:19) d x (cid:48) + Î» 1 (cid:18) 1 âˆ’ (cid:90) p d x (cid:48) (cid:19) , where Î» 1 is the Lagrangian multipliers . Note that we simpli - ï¬ed the notation here to improve its readability . We set the derivative with respect to p to 0 and used the constraint to yield the following equation : p = exp [ Î² ( r + Î³V (cid:48) + Î· âˆ’ 1 ln q ) ] exp ( 1 + Î²Î» 1 ) , where the denominator is given by exp ( 1 + Î²Î» 1 ) = (cid:90) exp [ Î² ( r + Î³V (cid:48) + Î· âˆ’ 1 ln q ) d x (cid:48) . ( 11 ) By deï¬ning the right - hand side of ( 11 ) as exp Î²Q , we obtain ( 2 ) and ( 4 ) . Substituting the above results into ( 1 ) yields V = max Ï€ E Ï€ (cid:104) âˆ’ Îº âˆ’ 1 ln Ï€ âˆ’ Î· âˆ’ 1 ln Ï€ b + Q (cid:105) . Similarly , we can maximize the right - hand side using the Lagrangian multipliers method and obtain ( 3 ) and ( 5 ) . R EFERENCES [ 1 ] C . Finn , P . Christiano , P . Abbeel , and S . Levine , â€œA connection between generative adversarial networks , inverse reinforcement learning , and energy - based models , â€ in NIPS 2016 Workshop on Adversarial Training , 2016 . [ 2 ] J . Ho and S . Ermon , â€œGenerative adversarial imitation learning , â€ in NeurIPS , 2016 . [ 3 ] N . Ab Aza , A . Shahmansoorian , and M . Davoudi , â€œFrom inverse optimal control to inverse reinforcement learning : A historical review , â€ Annual Reviews in Control , vol . 50 , pp . 119 â€“ 138 , 2020 . [ 4 ] S . Arora and P . Doshi , â€œA survey of inverse reinforcement learning : Challenges , methods and progress , â€ Artiï¬cial Intelligence , 2021 . [ 5 ] J . Fu , K . Luo , and S . Levine , â€œLearning robust rewards with adversarial inverse reinforcement learning , â€ in Proc . of ICLR , 2018 . [ 6 ] L . BlondÃ© and A . Kalousis , â€œSample - efï¬cient imitation learning via generative adversarial nets , â€ in Proc . of AISTATS , 2019 . [ 7 ] I . Kostrikov , K . K . Agrawal , D . Dwibedi , S . Levine , and J . Tompson , â€œDiscriminator - actor - critic : Addressing sample inefï¬ciency and reward bias in adversarial imitation learning , â€ in Proc . of ICLR , 2019 . [ 8 ] F . Sasaki , T . Yohira , and A . Kawaguchi , â€œSample efï¬cient imitation learning for continuous control , â€ in Proc . of ICLR , 2019 . [ 9 ] E . Uchibe and K . Doya , â€œForward and inverse reinforcement learning sharing network weights and hyperparameters , â€ Neural Networks , vol . 144 , pp . 138 â€“ 153 , 2021 . [ 10 ] E . Todorov , T . Erez , and Y . Tassa , â€œMuJoCo : A physics engine for model - based control , â€ in Proc . of IROS , 2012 . [ 11 ] A . Boularias , J . Kober , and J . Peters , â€œRelative entropy inverse rein - forcement learning , â€ in Proc . of AISTATS , 2011 . [ 12 ] B . D . Ziebart , A . Maas , J . A . Bagnell , and A . K . Dey , â€œMaximum entropy inverse reinforcement learning , â€ in Proc . of AAAI , 2008 . [ 13 ] E . Uchibe , â€œModel - free deep inverse reinforcement learning by logistic regression , â€ Neural Processing Letters , vol . 47 , no . 3 , pp . 891 â€“ 905 , 2018 . [ 14 ] P . Barde , J . Roy , W . Jeon , J . Pineau , C . Pal , and D . Nowrouzezahrai , â€œAdversarial soft advantage ï¬tting : Imitation learning without policy optimization , â€ in NeurIPS , 2020 . [ 15 ] G . Zuo , K . Chen , J . Lu , and X . Huang , â€œDeterministic generative adversarial imitation learning , â€ Neurocomputing , pp . 60 â€“ 69 , 2020 . [ 16 ] G . Zuo , Q . Zhao , K . Chen , J . Li , and D . Gong , â€œOff - policy adversarial imitation learning for robotic tasks with low - quality demonstrations , â€ Applied Soft Computing , 2020 . [ 17 ] Z . Zhu , K . Lin , B . Dai , and J . Zhou , â€œOff - policy imitation learning from observations , â€ in NeurIPS , 2020 . [ 18 ] H . Hoshino , K . Ota , A . Kanezaki , and R . Yokota , â€œOPIRL : Sample efï¬cient off - policy inverse reinforcement learning via distribution match - ing , â€ in Proc . of ICRA , 2022 . [ 19 ] S . Reddy , A . D . Dragan , and S . Levine , â€œSQIL : Imitation learning via regularized behavioral cloning , â€ in Proc . of ICLR , 2020 . [ 20 ] D . Nishio , D . Kuyoshi , T . Tsuneda , and S . Yamane , â€œDiscriminator soft actor critic without extrinsic rewards , â€ in Proc . of the 9th IEEE Global Conference on Consumer Electronics , 2020 , pp . 327 â€“ 32 . [ 21 ] M . Herman , T . Gindele , J . Wagner , F . Schmitt , and W . Burgard , â€œInverse reinforcement learning with simultaneous estimation of rewards and dynamics , â€ in Proc . of AISTATS , 2016 . [ 22 ] N . Baram , O . Anschel , I . Caspi , and S . Mannor , â€œEnd - to - end differen - tiable adversarial imitation learning , â€ in Proc . of ICML , 2017 . [ 23 ] N . Rhinehart , K . M . Kitani , and P . Vernaza , â€œR2P2 : A ReparameteRized Pushforward Policy for diverse , precise generative path forecasting , â€ in Proc . of ECCV , 2018 . [ 24 ] J . Sun , L . Yu , P . Dong , B . Lu , and B . Zhou , â€œAdversarial inverse rein - forcement learning with self - attention dynamics model , â€ IEEE Robotics and Automation Letters , vol . 6 , no . 2 , pp . 1880 â€“ 1886 , 2021 . [ 25 ] V . Saxena , S . Sivanandan , and P . Mathur , â€œDyna - AIL : Adversarial imitation learning by planning , â€ in ICLR 2020 Workshop : Beyond Tabula Rasa in Reinforcement Learning , 2020 . [ 26 ] F . Torabi , G . Warnell , and P . Stone , â€œDEALIO : Data - efï¬cient adversarial learning for imitation from observation , â€ in Proc . of IROS , 2021 . [ 27 ] M . Sugiyama , T . Suzuki , and T . Kanamori , Density ratio estimation in machine learning . Cambridge University Press , 2012 . [ 28 ] R . A . Bradley and M . E . Terry , â€œRank analysis of incomplete block designs : I . the method of paired comparisons , â€ Biometrika , vol . 50 , no . 3 / 4 , pp . 324 â€“ 345 , 1952 . [ 29 ] R . S . Sutton , â€œIntegrated architecture for learning , planning , and reacting based on approximating dynamic programming , â€ in Proc . of ICML , 1990 . [ 30 ] R . S . Sutton , D . McAllester , S . Singh , and Y . Mansour , â€œPolicy gradient methods for reinforcement learning with function approximation , â€ in NeurIPS , 2000 , pp . 1057 â€“ 1063 . [ 31 ] I . Kostrikov , O . Nachum , and J . Tompson , â€œImitation learning via off - policy distribution matching , â€ in Proc . of ICLR , 2019 . [ 32 ] T . Haarnoja , A . Zhou , P . Abbeel , and S . Levine , â€œSoft Actor - Critic : Off - policy maximum entropy deep reinforcement learning with a stochastic actor , â€ in Proc . of ICML , 2018 . [ 33 ] G . Brockman , V . Cheung , L . Pettersson , J . Schneider , J . Schulman , J . Tang , and W . Zaremba , â€œOpenAI Gym , â€ CoRR , abs / 1606 . 01540 , 2016 . [ 34 ] J . Schulman , S . Levine , P . Abbeel , M . Jordan , and P . Moritz , â€œTrust region policy optimization , â€ in Proc . of ICML , 2015 . [ 35 ] S . Elfwing , E . Uchibe , and K . Doya , â€œSigmoid - weighted linear units for neural network function approximation in reinforcement learning , â€ Neural Networks , vol . 107 , pp . 3 â€“ 11 , 2018 . [ 36 ] P . Ghosh , M . S . M . Sajjadi , A . Vergari , M . Black , and B . Schaolkopf , â€œFrom variational to deterministic autoencoders , â€ in Proc . of ICLR , 2020 . [ 37 ] S . Chitta , I . Sucan , and S . Cousins , â€œMoveit ! [ ROS topics ] , â€ IEEE Robotics Automation Magazine , vol . 19 , no . 1 , pp . 18 â€“ 19 , 2012 . [ 38 ] Y . Zhang , Q . Cai , Z . Yang , and Z . Wang , â€œGenerative adversarial imita - tion learning with neural networks : Global optimality and convergence rate , â€ in Proc . of ICML , 2020 . [ 39 ] Z . Guan , X . Tengyu , and L . Yingbin , â€œWhen will generative adversarial imitation learning algorithms attain global convergence , â€ in Proc . of AISTATS , 2021 .