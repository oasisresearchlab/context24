IEEE ROBOTICS AND AUTOMATION LETTERS . PREPRINT VERSION . ACCEPTED JULY , 2022 1 Model - Based Imitation Learning Using Entropy Regularization of Model and Policy Eiji Uchibe 1 Abstract —Approaches based on generative adversarial net - works for imitation learning are promising because they are sam - ple efﬁcient in terms of expert demonstrations . However , training a generator requires many interactions with the actual environ - ment because model - free reinforcement learning is adopted to update a policy . To improve the sample efﬁciency using model - based reinforcement learning , we propose model - based Entropy - Regularized Imitation Learning ( MB - ERIL ) under the entropy - regularized Markov decision process to reduce the number of interactions with the actual environment . MB - ERIL uses two discriminators . A policy discriminator distinguishes the actions generated by a robot from expert ones , and a model discriminator distinguishes the counterfactual state transitions generated by the model from the actual ones . We derive structured discriminators so that the learning of the policy and the model is efﬁcient . Computer simulations and real robot experiments show that MB - ERIL achieves a competitive performance and signiﬁcantly improves the sample efﬁciency compared to baseline methods . Index Terms —Imitation Learning , Reinforcement Learning , Machine Learning for Robot Control I . INTRODUCTION D EEP Reinforcement Learning ( RL ) using deep neural networks learns much better than manually designed policies ( action rules ) for problems where the environmental dynamics is completely described on a computer . Unfor - tunately , many problems remain in applying RL to robot control tasks , especially two signiﬁcant ones : ( 1 ) specifying reward functions and ( 2 ) the cost of collecting data in real environments . Imitation learning is a promising method for overcoming the ﬁrst problem because it ﬁnds a policy from expert demon - strations . In particular , recent imitation learning is related to Generative Adversarial Networks ( GANs ) [ 1 ] , [ 2 ] , and this framework has two components : a discriminator and a generator . Training the former , which distinguishes demon - strations generated by a robot from expert demonstrations , corresponds to inverse RL [ 3 ] , [ 4 ] . Training the latter , which produces expert - like demonstrations , corresponds to policy Manuscript received : February , 24 , 2022 ; Revised May , 20 , 2022 ; Accepted July , 11 , 2022 . This paper was recommended for publication by Associate Editor J . Garcia and Editor D . Kulic upon evaluation of the reviewers’ comments . This work was supported by Innovative Science and Technology Initiative for Security Grant Number JPJ004596 , ATLA , Japan and partially based on results obtained from project JPNP20006 commissioned by the New Energy and Industrial Technology Development Organization ( NEDO ) . This work was partially supported by JSPS KAKENHI Grant Number JP21H03527 . 1 Eiji Uchibe is with the Department of Brain Robot Interface , ATR Computational Neuroscience Laboratories , Kyoto 619 - 0288 , Japan . uchibe @ atr . jp Digital Object Identiﬁer ( DOI ) : see top of this page . ( b ) ( a ) 𝐷 p o l i c y expertor learner 𝒟 𝐸 𝒟 𝐿 expertpolicy learner policy actualenvironment 𝐷 m o d e l real or generated 𝒟 𝐸 ∪ 𝒟 𝐿 𝒟 𝐺 model expertpolicy learner policy actual environment 𝐷 p o l i c y expert or learner 𝒟 𝐸 𝒟 𝐿 ∪ 𝒟 𝐺 expertpolicy learner policy actual environment model Fig . 1 . Comparison of available datasets and discriminators between model - free and model - based imitation learning : ( a ) Model - free setting : Expert dataset D E and learner’s dataset D L are created through interactions with actual environments . Policy discriminator D policy exists . ( b ) Our proposed model - based setting : Simulated dataset D G is additionally generated from learner’s policy and model . Two discriminators exist : D model and D policy . improvement by forward RL 1 . Some previous studies [ 2 ] , [ 5 ] showed that such imitation learning approaches achieve high sample efﬁciency in terms of the number of demonstrations . However , they also often suffer from sample inefﬁciency in the generator’s training step because a model - free on - policy forward RL , which usually requires many environmental inter - actions , is adopted to update the policy . Several studies [ 6 ] , [ 7 ] , [ 8 ] , [ 9 ] employed model - free off - policy RL . However , these methods remain sample inefﬁcient as a method of real robot control because they require costly interactions with actual environments . Fig . 1 ( a ) illustrates the dataset and the discrim - inator in the model - free setting . Expert dataset D E is gathered by executing an expert policy in an actual environment . Note that the learner also interacts with the actual environment using its own policy to collect learner’s dataset D G , although its interaction is costly . Adopting a model - based forward RL is promising to reduce the number of costly interactions with the actual environment . To further reduce the number of interactions with the actual environment , we propose Model - Based Entropy - Regularized Imitation Learning ( MB - ERIL ) , which explicitly estimates a 1 Hereafter we refer to RL that ﬁnds an optimal policy from rewards as forward RL to distinguish it from inverse RL . a r X i v : 2206 . 10101v2 [ c s . L G ] 31 A ug 2022 2 IEEE ROBOTICS AND AUTOMATION LETTERS . PREPRINT VERSION . ACCEPTED JULY , 2022 model of the environment and generates simulated data by running the learner’s policy in the estimated model . MB - ERIL is formulated as the minimization problem of Kullback - Leibler ( KL ) divergence between the expert policy evaluated in the actual environment and the learner’s policy evaluated in the model environment . MB - ERIL regularizes the policy and model by Shannon entropy and KL divergence to derive the algorithm , where we assume that the expert policy and the actual environment are solutions of the regularized Bellman equation . The following are the contributions of this work : • It develops two novel discriminators . One is a policy discriminator that differentiates actions generated by the policy from expert actions . The other is that a model discriminator that distinguishes the state transitions gen - erated by the model from the actual transitions provided by an expert . • The discriminators are represented by the reward , state , and state - action value functions . The value functions are updated by training the discriminators , a step that makes the forward RL efﬁcient . • For model ﬁtting , MB - ERIL provides another objective alternative to maximum likelihood estimation . • The estimated model generates counterfactual data to up - date the state and state - action value functions ( Fig . 1 ( b ) ) . As a result , we can reduce the number of costly interac - tions with the actual environment . To evaluate MB - ERIL , we conducted two continuous con - trol benchmark tasks in the MuJoCo simulator [ 10 ] and a vision - based reaching task [ 9 ] using a real upper - body hu - manoid robot . MB - ERIL shows promising experimental results with competitive asymptotic performance and higher sample efﬁciency than previous studies and empirically ensures low model bias , which is the gap between the actual environment and the model , by jointly updating the model and policy . II . LITERATURE REVIEW A . Model - free imitation learning Relative Entropy Inverse RL [ 11 ] is a sample - based method inspired by Maximum entropy inverse RL ( MaxEntIRL ) [ 12 ] . It provides an efﬁcient way of estimating the partition function of the probability distribution of expert trajectories . How - ever , one drawback is utilizing a ﬁxed sampling distribution , which is unhelpful in practice . Generative Adversarial Imi - tation Learning ( GAIL ) [ 2 ] , which can adapt the sampling distribution using policy optimization , is closely related to GAN . GAIL is more sample efﬁcient than Behavior Cloning ( BC ) with respect to the number of expert demonstrations . However , GAIL often requires many interactions with the environment because ( 1 ) an on - policy model - free forward RL is used for policy improvement and ( 2 ) the discriminator is not structured . Adversarial Inverse Reinforcement Learning ( AIRL ) [ 5 ] and Logistic Regression - based inverse RL [ 13 ] propose a structured discriminator represented by the reward and state - value functions , although an on - policy forward RL updates the policy . Adversarial Soft Advantage Fitting ( ASAF ) [ 14 ] exploits the discriminator inspired by the AIRL discrimi - nator , although the state transition is not considered explicitly . To reduce the number of environmental interactions , an off - policy model - free forward RL has been adopted [ 6 ] , [ 7 ] , [ 15 ] , [ 16 ] . We proposed Model - Free Entropy - Regularized Imitation Learning ( MF - ERIL ) based on entropy regularization that shares the network parameters between the discriminator and the generator . Even though MF - ERIL achieved better sample efﬁciency than the above methods , there is room to improve the sample efﬁciency by introducing model learning . MB - ERIL is one instantiation of this approach . Recent approaches corrected the issues around reusing data previously collected while training the discriminator [ 17 ] , [ 18 ] . GAN - like imitation learning often suffers from unstable learning from adversarial training . Therefore , some regular - ization terms are added to BC’s loss function to stabilize the learning process . For example , Soft Q Imitation Learning ( SQIL ) is a regularized BC algorithm , where the squared soft Bellman error is used as a regularizer [ 19 ] . Interestingly , its algorithm can be implemented by assigning a reward of 1 to expert demonstrations and 0 to generated ones . Discriminator Soft Actor - Critic [ 20 ] is an extension of SQIL where its predeﬁned reward value is replaced by the parameterized reward trained by the discriminator . B . Model - based imitation learning MaxEntIRL , which is a pioneer of an entropy regularized RL , estimates the reward function from expert trajectories based on the maximum entropy principle . MaxEnt IRL is a model - based approach , although how to train the model was not discussed . Although a method was proposed that simultaneously estimated the reward and the model [ 21 ] , maximum likelihood estimation was applied to model learning . A few studies introduced model learning to GAIL to propa - gate the discriminator’s gradient to the policy for a gradient up - date . Consequently , Model - based GAIL [ 22 ] , [ 23 ] and Model - based AIRL [ 24 ] achieved end - to - end training , although these methods do not sample state transitions from the model while unrolling the trajectories . However , these approaches do not use simulated experiences through interaction with the model , and therefore , they still require many actual interactions with a real environment [ 25 ] . Data - Efﬁcient Adversarial Learning for Imitation from Observation ( DEALIO ) , which is a GAIL - like algorithm [ 26 ] , adopts a model - based RL that is employed for training a policy from the trained discriminator . Since one drawback of DEALIO is that model learning is independent of discriminator training , it suffers from model bias . III . PROPOSED METHOD A . Objective function Consider Markov Decision Process ( MDP ) M = (cid:104)X , U , p , r , γ (cid:105) , where X denotes a continuous state space , U denotes an action space , p : X × U × X (cid:55)→ R ≥ 0 denotes an actual stochastic state transition probability , r : X (cid:55)→ R denotes an immediate reward function , and γ ∈ ( 0 , 1 ) denotes a discount factor that indicates how near and far future rewards are weighed . We chose the state - only function because more UCHIBE : MODEL - BASED IMITATION LEARNING USING ENTROPY REGULARIZATION OF MODEL AND POLICY 3 general reward functions like r : X × U × X (cid:55)→ R are often shaped as the result of training the discriminators . Let b : X × U (cid:55)→ R ≥ 0 denote a stochastic policy to select action u ∈ U at state x ∈ X . Let π and q denote an expert policy and a model of p . Note that p and π are unknown , while q and b are maintained by the learner . MB - ERIL minimizes the following KL divergence : J ( q , b ) = E p L ( x , u , x (cid:48) ) (cid:20) ln p L ( x , u , x (cid:48) ) p E ( x , u , x (cid:48) ) (cid:21) , where p L and p E are respectively the joint density functions deﬁned by p L ( x , u , x (cid:48) ) = q ( x (cid:48) | x , u ) b ( u | x ) p L ( x ) , p E ( x , u , x (cid:48) ) = p ( x (cid:48) | x , u ) π ( u | x ) p E ( x ) , where p L ( x ) and p E ( x ) are some initial distributions , and we assume p L ( x ) = p E ( x ) for simplicity . The difﬁculty is how to evaluate the log - ratio because it is unknown . B . Entropic regularization of policy and model The basic idea for estimating the log - ratio is to adopt the density ratio trick [ 27 ] . Then MB - ERIL updates the policy and the model by minimizing the estimated KL divergence . To derive the algorithm , we formulate the entropic regularization of the policy and the model . We add two regularization terms to the immediate reward : r ( x ) + κ − 1 H ( p ( · | x , u ) π ( · | x ) ) − η − 1 KL ( p ( · | x , u ) π ( · | x ) (cid:107) q ( · | x , u ) b ( · | x ) ) , where H represents the operator of Shannon entropy , and κ and η denote positive hyperparameters . Then the Bellman optimality equation is given by V ( x ) = max π E π (cid:26) − κ − 1 ln π ( u | x ) − η − 1 ln π ( u | x ) b ( u | x ) + max p E p (cid:20) r ( x ) + γV ( x (cid:48) ) − κ − 1 ln p ( x (cid:48) | x , u ) − η − 1 ln p ( x (cid:48) | x , u ) q ( x (cid:48) | x , u ) (cid:21)(cid:27) , ( 1 ) where V ( x ) denotes a state - value function . To utilize the framework of entropic regularization , MB - ERIL assumes that π ( u | x ) and p ( x (cid:48) | x , u ) are the solutions of the Bellman equation ( 1 ) when the learner uses baseline policy b ( u | x ) and baseline state transition q ( x (cid:48) | x , u ) . Using the Lagrangian multiplier method , we obtain the following equations : p ( x (cid:48) | x , u ) = exp [ β ( r ( x ) + γV ( x (cid:48) ) + η − 1 ln q ( x (cid:48) | x , u ) ) ] exp [ βQ ( x , u ) ] , ( 2 ) π ( u | x ) = exp [ β ( Q ( x , u ) + η − 1 ln b ( u | x ) ) ] exp ( βV ( x ) ) , ( 3 ) where β is a positive hyperparameter deﬁned by β (cid:44) κηκ + η . Q ( x , u ) denotes the state - action value function , and the fol - lowing relation exists between V and Q : exp ( βQ ( x , u ) ) = (cid:90) exp [ β ( r ( x ) + γV ( x (cid:48) ) + η − 1 ln q ( x (cid:48) | x , u ) (cid:3) d x (cid:48) , ( 4 ) exp ( βV ( x ) ) = (cid:90) exp (cid:2) β ( Q ( x , u ) + η − 1 ln b ( u | x ) ) (cid:3) d u . ( 5 ) See Appendix A for the derivation . C . Derivation of MB - ERIL discriminators We obtain the following equations from ( 2 ) and ( 3 ) that represent the density ratio : β − 1 ln p ( x (cid:48) | x , u ) q ( x (cid:48) | x , u ) = r ( x ) + γV ( x (cid:48) ) − Q ( x , u ) − κ − 1 ln q ( x (cid:48) | x , u ) , β − 1 ln π ( u | x ) b ( u | x ) = Q ( x , u ) − V ( x ) − κ − 1 ln b ( u | x ) . Using the density ratio estimation [ 27 ] , we derive a model discriminator that distinguishes the state transition generated by the model from the real transitions provided by the expert : D model ( x (cid:48) | x , u ) = exp ( βf ( x , u , x (cid:48) ) ) exp ( βf ( x , u , x (cid:48) ) ) + exp ( βκ − 1 ln q ( x (cid:48) | x , u ) ) , where f ( x , u , x (cid:48) ) is deﬁned by f ( x , u , x (cid:48) ) (cid:44) r ( x ) + γV ( x (cid:48) ) − Q ( x , u ) . Similarly , the policy discriminator is given by D policy ( u | x ) = exp ( β ( Q ( x , u ) − V ( x ) ) ) exp ( β ( Q ( x , u ) − V ( x ) ) ) + exp ( βκ − 1 ln b ( u | x ) ) , which differentiates the action generated by the policy from the expert action . Note that D policy is the discriminator used by AIRL and MF - ERIL if Q ( x , u ) is replaced with r ( x ) + γV ( x (cid:48) ) . These discriminators have the same form as the optimal discriminator , also known as the Bradley - Terry model [ 28 ] . D . MB - ERIL learning In the model - free setting , the learner needs to interact with a real environment to generate trajectories like the expert , making the model - free setting sample inefﬁcient . On the other hand , the learner can generate trajectories by interacting with the real environment and the model . Learning from simulated trajectories is sometimes called Dyna architecture [ 29 ] . To improve the sample efﬁciency , we adopt Dyna architecture , and therefore , MB - ERIL utilizes the following three datasets . 4 IEEE ROBOTICS AND AUTOMATION LETTERS . PREPRINT VERSION . ACCEPTED JULY , 2022 The ﬁrst is an expert dataset generated by an expert in a real environment : D E = { ( x i , u i , x (cid:48) i ) } N E i = 1 , x i ∼ p E 0 ( · ) u i ∼ π ( · | x i ) , x (cid:48) i ∼ p ( · | x i , u i ) , where N E is the number of transitions in the dataset and p E 0 is the discounted state distribution [ 30 ] for ( π , p ) . The second and third are the learner’s datasets by running b ( u | x ) in the real environment and the model : D L = { ( x i , u i , x (cid:48) i ) } N L i = 1 , x i ∼ p L 0 ( · ) u i ∼ b ( · | x i ) , x (cid:48) i ∼ p ( · | x i , u i ) , D G = { ( x i , u i , x (cid:48) i ) } N G i = 1 , x i ∼ q 0 ( · ) u i ∼ b ( · | x i ) , x (cid:48) i ∼ q ( · | x i , u i ) , where N L and N G are the dataset sizes , and p L 0 and q 0 are the discounted state distributions for ( π L , p ) and ( π L , q ) . Normally , N G (cid:29) N L ≥ N E , because collecting data in the real environment is expensive . Here we show MB - ERIL’s learning rules that consist of ( 1 ) training the discriminators , ( 2 ) policy evaluation , and ( 3 ) policy improvement . First , we show the loss functions for training D model and D policy to estimate r , V , and Q : L model ( r , V , Q ) = − E ( x , u , x (cid:48) ) ∼D E ∪D L [ ln D model ( x (cid:48) | x , u ) ] − E ( x , u , x (cid:48) ) ∼D G [ ln ( 1 − D model ( x (cid:48) | x , u ) ) ] where E ( x , u , x (cid:48) ∼D ) represents the expectation over a batch of data that is uniformly sampled from an experience replay buffer D . Similarly , the loss function for D policy is given by L policy ( V , Q ) = − E ( x , u ) ∼D E [ ln D policy ( u | x ) ] − E ( x , u ) ∼D L ∪D G [ ln ( 1 − D policy ( u | x ) ) ] . Note that r , V , and Q are all updated , and q and b remain ﬁxed while training the discriminators . These two loss functions are summarized : L dis ( r , V , Q ) = λ model L model ( r , V , Q ) + λ policy L policy ( V , Q ) , ( 6 ) where λ model and λ policy denote positive hyperparameters . Training the discriminators is interpreted by an inverse RL because the reward function is estimated . Next we show how the state and state - action value func - tions are updated . This corresponds to the policy evaluation step . Using ( 4 ) and ( 5 ) , the following loss function can be constructed : L ( V , Q ) = λ QV E x , u (cid:20) (cid:0) Q − β − 1 ln E q (cid:2) exp ( β ( r + γV (cid:48) − κ − 1 ln q ) ) (cid:3)(cid:1) 2 (cid:21) + λ V Q E x (cid:104)(cid:0) V − β − 1 ln E b (cid:2) exp ( β ( Q − κ − 1 ln b ) ) (cid:3)(cid:1) 2 (cid:105) , ( 7 ) where λ QV and λ V Q denote constant hyperparameters . We simplify the notation by omitting the arguments of the func - tions and setting V (cid:48) (cid:44) V ( x (cid:48) ) for readability . This loss function Algorithm 1 Model - Based Entropy - Regularized Imitation Learning ( MB - ERIL ) Require : Expert dataset D E and hyperparameters κ and η Ensure : Learner’s policy b , model q , reward r , state - value V , and state - action value Q . 1 : Initialize all parameters of networks and replay buffers D L , D G . 2 : ( Pre - train the Regularized AutoEncoder using D E when raw images are used as inputs . ) 3 : for k = 0 , 1 , 2 , . . . do (cid:46) Collect state transitions in the real environment and the model . 4 : D L ← D L ∪ { ( x t , u t , x t + 1 ) } N L t = 0 with b and p . 5 : D G ← D G ∪ { ( x t , u t , x t + 1 ) } N G t = 0 with b and q . (cid:46) Train the discriminators 6 : Update r , V , and Q by minimizing ( 6 ) . (cid:46) Evaluate policy . 7 : D G ← D G ∪ { ( x t , u t , x t + 1 ) } N G t = 0 with b and q . 8 : Update V and Q by minimizing ( 7 ) . (cid:46) Improve the policy and the model . 9 : Update q and b . See ( 8 ) and ( 9 ) . 10 : end for contains the log - expected - exponent terms , which introduce biases in their gradients [ 31 ] . Empirically , using a biased estimate was sufﬁcient for our experiments , although applying Fenchel conjugates is also promising . Finally , we explain how the model and policy are updated based on the trained discriminators in the policy improvement step . As Soft Actor - Critic does [ 32 ] , they are directly learned by minimizing the expected KL divergence based on ( 2 ) and ( 3 ) : q ( x (cid:48) | x , u ) ← arg min q (cid:48) E x , u (cid:20) KL (cid:18) q (cid:48) (cid:107) exp [ β ( r + γV (cid:48) + η − 1 ln q ) ] exp [ βQ ] (cid:19)(cid:21) , ( 8 ) b ( u | x ) ← arg min b (cid:48) E x (cid:20) KL (cid:18) b (cid:48) (cid:107) exp [ β ( Q + η − 1 ln b ) ] exp ( βV ) (cid:19)(cid:21) . ( 9 ) MB - ERIL is summarized in Algorithm 1 . Lines 7 and 8 correspond to learning from simulated trajectories like Dyna architecture . E . Simpliﬁed MB - ERIL Here we show two simpliﬁed MB - ERIL implementations by removing some components . One is MB - ERIL without training the discriminators , which trains r , V , and Q by the maximum likelihood method instead of minimizing ( 6 ) . The loss function is given by L (cid:48) ( r , V , Q ) = − E D E [ ln p ( x (cid:48) | x , u ) ] − E D E [ ln b ( u | x ) ] + L ( V , Q ) . ( 10 ) The ﬁrst term of the right - hand side of ( 1 ) corresponds to model cloning , and the third term is the regularizer , which UCHIBE : MODEL - BASED IMITATION LEARNING USING ENTROPY REGULARIZATION OF MODEL AND POLICY 5 resembles SQIL . We call this approach Entropy - Regularized Model and Behavior Cloning ( ERMBC ) . The other is MB - ERIL without training the generator in - spired by ASAF . This approach simply skips lines 7 and 8 of Algorithm 1 , meaning that it does not learn from the additional simulated trajectories even though the model is maintained explicitly . We call this approach MB - ERIL without Policy Evaluation ( MB - ERIL \ PE ) . IV . MUJOCO BENCHMARK CONTROL TASKS A . Task description To verify our proposed method , we ﬁrst conducted two benchmark control tasks , Ant and Humanoid , provided by OpenAI gym [ 33 ] . Both use a physics engine called MuJoCo [ 10 ] . The goal of these tasks is to move forward as quickly as possible . First , an optimal policy was trained by the Trust Region Policy Optimization [ 34 ] , based on the original reward function provided by the simulator . Then it was used as an expert policy to collect expert data D E . We adopted the sparse sampling setup [ 8 ] in which we randomly sampled ( x , u , x (cid:48) ) triplets from each trajectory . The functions used by MB - ERIL need to be approximated by artiﬁcial neural networks , and r ( x ) , V ( x ) , Q ( x , u ) , and b ( u | x ) were represented by a two - layer neural network with a derivative of the sigmoid - weighted linear unit ( dSiLU ) [ 35 ] as an activation function , determined based on our previous study [ 9 ] . We deﬁned the model to output a Gaus - sian distribution conditioned on the state and the action , i . e . : q ( x (cid:48) | x , u ) = N ( x (cid:48) | µ ( x , u ) , Σ ( x , u ) ) , where µ ( x , u ) and Σ ( x , u ) denote the mean vector and the diagonal covariance matrix for simplicity . Future work will investigate more com - plicated distributions , such as a mixture of Gaussians with full covariance . Each was represented by a neural network with two hidden layers , and each layer had 256 nodes with dSiLU activations . B . Comparative evaluation We evaluated the MB - ERIL , ERMBC , and MB - ERIL \ PE performances by comparing them with the following algo - rithms : • Dyna - MF - ERIL : MF - ERIL [ 9 ] with the Dyna framework [ 29 ] , which generates simulated trajectories by interacting with the model . The model , represented by a separate neural network , is trained by the simple maximum like - lihood method . Then MF - ERIL is trained from real and generated data . • MF - ERIL : We ignored the ﬁrst discriminator that esti - mates ln p E 0 ( x ) / p L 0 ( x ) for simplicity . • DAC : Discriminator - Actor - Critic ( DAC ) [ 7 ] , which is baseline model - free imitation learning . • BC : naive Behavior Cloning that minimizes the negative log - likelihood objective . Following Ho and Ermon [ 2 ] , a single trajectory contains 50 state - action transition pairs ( x , u , x (cid:48) ) , i . e . , 50 steps per episode . The number of trajectories sampled from expert policy π were set to 30 and 350 in the Ant and Humanoid environments . We set N L = 10 2 and N G = 10 4 . Fig . 2 compares the learning curves 2 . Note that the re - ward functions estimated by each method cannot be directly compared since inverse reinforcement learning is an ill - posed problem . Therefore , the method was evaluated by the mean normalized return : ¯ R mean = 1 N N (cid:88) i = 1 R i − R min R max − R min , where i is the index of the experimental run and N is the number of experiments . R i is the raw return of the i - th experimental run , where the original reward of OpenAI gym was used . R min and R max are constants , where R max is given by the return of the expert while R min = 0 . In the Ant environment , the maximum normalized total reward was reached by MB - ERIL , MF - ERIL , DAC , ERMBC , and MB - ERIL \ PE in that order ; Dyna - MF - ERIL and BC did not reach the expert performance . MB - ERIL improved the learning efﬁciency of the model - free methods ( MF - ERIL and DAC ) by about ten times . Note that the asymptotic performance of Dyna - MF - ERIL was worse than that of MF - ERIL even though the performing model did learn and its normalized return increased rapidly in the early stage of learning . Perhaps the model failed to learn the state transition because model learning by the maximum likelihood method cannot deal with a covariate shift . ERMBC and MB - ERIL \ PE also converged to the expert performance , although they learned much slower than MF - ERIL . Similar results were obtained in the Humanoid environment . MB - ERIL learned faster than the other methods , although MF - ERIL and DAC eventually achieved comparable performance . The performance of MB - ERIL \ PE was worst or inferior to BC . Next we evaluated the data efﬁciency of the training dis - criminators by changing the number of samples in D E . Fig . 3 shows that MB - ERIL , MB - ERIL \ PE , MF - ERIL , and DAC found policies that exhibited higher control performance with fewer expert demonstrations . On the other hand , ERMBC and Dyna - MF - ERIL just exhibited slight performance degradation when the number of demonstrations was limited , implying that model learning did not improve the sample efﬁciency with respect to the number of expert demonstrations . However , the MB - ERIl \ PE performance was degraded in the Humanoid environment ; it did not improve even though the number of demonstrations increased . The results indicate that V and Q did not satisfy the soft Bellman equations ( 4 ) and ( 5 ) when they were simply estimated by training the discriminators . We conclude that the policy evaluation of MB - ERIL was more crucial when the state space is large . To see the model and policy differences among MB - ERIL , Dyna - MF - ERIL , and MF - ERIL , we computed the negative log - likelihood ( NLL ) of the ﬁnally - obtained model and policy using test expert data . For example , the model’s NLL was calculated by NLL = − 1 N E test N E test (cid:88) (cid:96) = 1 ln q ( x (cid:48) (cid:96) | x (cid:96) , u (cid:96) ) , 2 We conducted Hopper , Walker , Reacher , and HalfCheetah , all of which we previously evaluated [ 9 ] . Simulation results show that MB - ERIL outperformed MF - ERIL , although we omitted them due to space limitations . 6 IEEE ROBOTICS AND AUTOMATION LETTERS . PREPRINT VERSION . ACCEPTED JULY , 2022 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 number of interactions 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 n o r m a li z e d r e t u r n ( a ) Ant MB - ERIL ERMBC MB - ERIL \ PE Dyna - MF - ERIL MF - ERIL DAC BC 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 number of interactions 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 n o r m a li z e d r e t u r n ( b ) Humanoid MB - ERIL ERMBC MB - ERIL \ PE Dyna - MF - ERIL MF - ERIL DAC BC Fig . 2 . Normalized return versus number of interactions on MuJoCo bench - mark control tasks : Solid lines represent average values , and shaded areas correspond to ± 1 standard deviation region . Note that horizontal axis is a log scale . where N E test is the number of samples in the test data . The policy’s NLL was deﬁned similarly . Figs . 4 ( a ) and ( c ) compare the NLLs of the policies on the MuJoCo experiments , showing no signiﬁcant difference between the MB - ERIL and MF - ERIL policies , although Dyna - MF - ERIL’s NLL was larger than the others , which led to its poor asymptotic performance . Figs . 4 ( b ) and ( d ) show that the NLL of the model obtained by MB - ERIL was the smallest among the other methods . V . REAL ROBOT CONTROL A . Task description Next we performed a vision - based reaching task [ 9 ] with an upper - body humanoid called Nextage developed by Kawada Industries Inc . Two colored blocks served as obstacles , and one switch button indicated the goal in the workspace . The aim is to move its left arm’s end - effector from a starting position ( Fig . 5 ( a ) ) to a target position ( Fig . 5 ( b ) ) as quickly as possible , where the starting and target positions were selected randomly from the pre - deﬁned positions in Fig . 5 ( c ) . This task is more complicated than the FetchReach provided by OpenAI Gym because the target position must be detected from visual information . Nextage has a head with two cameras , a torso , two 6 - axis manipulators , and two cameras attached to its end - effectors . We mounted cameras on its left arm and head in this task . The head pose was ﬁxed during the experiments . The state was given by x = [ θ 1 , . . . , θ 6 , z (cid:62) ] (cid:62) , where θ i is the i - th joint angle of the left arm and z represents the latent visual state obtained by a deterministic Regularized AutoEn - coder ( RAE ) [ 36 ] . The action was given by the changes in 5 10 15 20 25 30 number of demonstrations 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 n o r m a li z e d r e t u r n ( a ) Ant MB - ERIL ERMBC MB - ERIL \ PE Dyna - MF - ERIL MF - ERIL DAC BC 100 150 200 250 300 350 number of demonstrations 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 n o r m a li z e d r e t u r n ( b ) Humanoid MB - ERIL ERMBC MB - ERIL \ PE Dyna - MF - ERIL MF - ERIL DAC BC Fig . 3 . Normalized return versus number of trajectories provided by expert on MuJoCo benchmark control tasks MB - ERIL ERMBCMB - ERIL \ PEDyna - MF - ERILMF - ERIL DAC 0 1 2 3 4 n e g a t i v e l og - li k e li h oo d ( a ) Ant : policy MB - ERIL ERMBCMB - ERIL \ PEDyna - MF - ERIL 0 1 2 3 4 n e g a t i v e l og - li k e li h oo d ( b ) Ant : model MB - ERIL ERMBCMB - ERIL \ PEDyna - MF - ERILMF - ERIL DAC 0 1 2 3 4 5 n e g a t i v e l og - li k e li h oo d ( c ) Humanoid : policy MB - ERIL ERMBCMB - ERIL \ PEDyna - MF - ERIL 0 1 2 3 4 5 6 n e g a t i v e l og - li k e li h oo d ( d ) Humanoid : model Fig . 4 . Comparison of NLL in Ant and Humanoid environments : Smaller is better because vertical axis is negative log - likelihood . ( a ) and ( c ) NLL of policy . ( b ) and ( d ) NLL of model . Note that MF - ERIL and DAC did not estimate the model . the joint angle from previous position u = [ ∆ θ 1 , . . . , ∆ θ 6 ] (cid:62) , where ∆ θ i is the change in the joint angle from the previous position of the i - th joint ( although we clamped ∆ θ i in [ − 1 , 1 ] . Figs . 5 ( d ) and ( e ) show the entire network architecture that represents the functions and the RAE encoder whose input came from two 160 × 128 RGB images captured by Nextage’s cameras . We prepared several environmental conﬁgurations by chang - UCHIBE : MODEL - BASED IMITATION LEARNING USING ENTROPY REGULARIZATION OF MODEL AND POLICY 7 𝐼head 𝐼hand ( a ) 𝐼head 𝐼hand ( b ) encoder decoder MLP MLP MLP MLP MLP 𝑧 captured images 𝐼head , 𝐼hand joint angles 𝜃 𝑖 action Δ𝜃 𝑖 c o n c a t e n a t e c o n c a t e n a t e ሚ𝐼head , ሚ𝐼hand 𝑟 𝑥 𝑉 ( 𝒙 ) 𝑏 ( 𝒖∣𝒙 ) 𝑄 ( 𝒙 , 𝒖 ) 𝑞 𝒙 ′ 𝒙 , 𝒖 ( d ) ( e ) 𝐼 h ea d , 𝐼 h a n d I n s t a n t n o r m C o n v . , 3 × 3 , 128 c , s t r i d e 2 R e L U I n s t a n t n o r m C o n v . , 3 × 3 , 128 c , s t r i d e 2 R e L U C o n v . , 3 × 3 , 128 c , s t r i d e 2 R e L U 𝑧 button ( goal position ) Colored blocks ( obstacles ) ( c ) 700 mm 1000 mm : initial pos . for the expert : initial pos . for the learner : initial pos . for test : target pos . for the expert : target pos . for test Fig . 5 . Vision - based reaching task : ( a ) Starting position . ( b ) Target position . ( c ) Possible environmental conﬁguration . ( d ) Network architecture for reward , state value , policy , state - action value , and model . ( e ) Network architecture for encoder : Conv . denotes a convolutional neural network . “ n c” denotes “ n channels . ” ing the arm’s initial pose , the button’s location , and the height of the blocks . We designed three initial and two target poses for the expert conﬁguration ( Fig . 5 ( c ) ) . The learning conﬁgura - tion was given by ﬁve initial and two target poses . In addition , we prepared blocks of two different heights , resulting in 3 × 2 × 2 = 12 expert conﬁgurations and 5 × 2 × 2 = 20 learning conﬁgurations . We created expert demonstrations with MoveIt ! [ 37 ] using the geometric information of the button and the colored blocks . Note that such information was unavailable for learning the algorithms . MoveIt ! generated 12 trajectories for every expert conﬁguration , and we sampled 50 transitions for each trajectory . Consequently , N E = 12 × 50 expert transitions were obtained . Although the sequence of the joint angles was almost deterministic , the RGB images were stochastic and noisy due to the lighting conditions . In this experiment , N L and N G were set to 20 × 50 and 10 4 . On the other hand , the test conﬁguration was constructed by one initial pose ( red circle ) and one target one ( red square ) that were not included in the expert and learner’s conﬁgurations . B . Experimental results We compared MB - ERIL with ERMBC , MB - ERIL \ PE , Dyna - MF - ERIL , MF - ERIL , DAC , ASAF , and BC . We mea - sured the performances by investigating a synthetic reward function : r = exp (cid:16) − (cid:107) p t − p g (cid:107) 22 σ 2 (cid:17) , where p t and p g denote the end - effector’s current and target position . Parameter σ was set to 5 . To evaluate the performance during training , we evaluated the policy’s performance every ten episodes with a test set of initial and goal positions that were not included in the training demonstrations from the expert . Fig . 6 compares the performance , where the horizontal and vertical axes represent the number of environmental inter - actions and normalized returns . MB - ERIL achieved higher sample efﬁciency than the other baselines . Unlike the MuJoCo experiments , Dyna - MF - ERIL was comparable to MF - ERIL , indicating that the model bias was not serious in this task . 0 50 100 150 200 250 300 number of interactions 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 n o r m a li z e d r e t u r n MB - ERIL ERMBC MB - ERIL \ PE Dyna - MF - ERIL MF - ERIL DAC ASAF BC Fig . 6 . Normalized return versus number of interactions on Nextage vision - based reaching task MB - ERIL ERMBCMB - ERIL \ PEDyna - MF - ERILMF - ERIL DAC ASAF 0 1 2 3 4 5 n e g a t i v e l og - li k e li h oo d ( a ) policy MB - ERILERMBCMB - ERIL \ PEDyna - MF - ERIL 0 1 2 3 4 5 n e g a t i v e l og - li k e li h oo d ( b ) model Fig . 7 . Comparison of NLL on Nextage vision - based reaching task : ( a ) Policy’s NLL and ( b ) Model’s NLL However , Dyna - MF - ERIL did not exploit the model efﬁciently because it did not improve the sample efﬁciency . Fig . 7 shows that the NLL of MB - ERIL’s policy was comparable to those of ERMBC , MB - ERIL \ PE , MF - ERIL , and DAC , although the NLL of MB - ERIL’s model was the smallest . VI . CONCLUSIONS We proposed MB - ERIL , derived from RL theory , which regularized the policy and the model . The experimental results of the simulated and real robots showed that MB - ERIL im - proved the sample efﬁciency over MF - ERIL , Dyna - MF - ERIL , DAC , and BC . A comparison between MB - ERIL and Dyna - MF - ERIL suggests that the simple use of data generated by the model was not useful due to the model bias . As explained in Section I , our formulation is based on the assumption that the expert policy and the actual environment are solutions of the regularized Bellman equation , although this assumption is not straightforward . In the future , we will provide a clear derivation of ERIL . Our experimental results on the Ant task show that MB - ERIL’s asymptotic performance was slightly worse than that of MF - ERIL due to model bias . One possible solution is to switch the algorithm from MB - ERIL to MF - ERIL based 8 IEEE ROBOTICS AND AUTOMATION LETTERS . PREPRINT VERSION . ACCEPTED JULY , 2022 on the loss function of the model discriminator . MB - ERIL updates V and Q alternatively with respect to two different objective functions . Although the update rules were derived from the Bellman equation ( 1 ) , the convergence and stability were not proved theoretically . In future work , we will apply the technique used in the convergence proof of GAIL [ 38 ] , [ 39 ] . APPENDIX A . Derivation of ( 2 ) - ( 5 ) Consider the maximization problem inside the right - hand side of ( 1 ) . It is a constrained optimization because (cid:82) p ( x (cid:48) | x , u ) d x (cid:48) = 1 . The Lagrangian is given by L 2 = (cid:90) p (cid:18) r − κ − 1 p − η − 1 ln p q + γV (cid:48) (cid:19) d x (cid:48) + λ 1 (cid:18) 1 − (cid:90) p d x (cid:48) (cid:19) , where λ 1 is the Lagrangian multipliers . Note that we simpli - ﬁed the notation here to improve its readability . We set the derivative with respect to p to 0 and used the constraint to yield the following equation : p = exp [ β ( r + γV (cid:48) + η − 1 ln q ) ] exp ( 1 + βλ 1 ) , where the denominator is given by exp ( 1 + βλ 1 ) = (cid:90) exp [ β ( r + γV (cid:48) + η − 1 ln q ) d x (cid:48) . ( 11 ) By deﬁning the right - hand side of ( 11 ) as exp βQ , we obtain ( 2 ) and ( 4 ) . Substituting the above results into ( 1 ) yields V = max π E π (cid:104) − κ − 1 ln π − η − 1 ln π b + Q (cid:105) . Similarly , we can maximize the right - hand side using the Lagrangian multipliers method and obtain ( 3 ) and ( 5 ) . R EFERENCES [ 1 ] C . Finn , P . Christiano , P . Abbeel , and S . Levine , “A connection between generative adversarial networks , inverse reinforcement learning , and energy - based models , ” in NIPS 2016 Workshop on Adversarial Training , 2016 . [ 2 ] J . Ho and S . Ermon , “Generative adversarial imitation learning , ” in NeurIPS , 2016 . [ 3 ] N . Ab Aza , A . Shahmansoorian , and M . Davoudi , “From inverse optimal control to inverse reinforcement learning : A historical review , ” Annual Reviews in Control , vol . 50 , pp . 119 – 138 , 2020 . [ 4 ] S . Arora and P . Doshi , “A survey of inverse reinforcement learning : Challenges , methods and progress , ” Artiﬁcial Intelligence , 2021 . [ 5 ] J . Fu , K . Luo , and S . Levine , “Learning robust rewards with adversarial inverse reinforcement learning , ” in Proc . of ICLR , 2018 . [ 6 ] L . Blondé and A . Kalousis , “Sample - efﬁcient imitation learning via generative adversarial nets , ” in Proc . of AISTATS , 2019 . [ 7 ] I . Kostrikov , K . K . Agrawal , D . Dwibedi , S . Levine , and J . Tompson , “Discriminator - actor - critic : Addressing sample inefﬁciency and reward bias in adversarial imitation learning , ” in Proc . of ICLR , 2019 . [ 8 ] F . Sasaki , T . Yohira , and A . Kawaguchi , “Sample efﬁcient imitation learning for continuous control , ” in Proc . of ICLR , 2019 . [ 9 ] E . Uchibe and K . Doya , “Forward and inverse reinforcement learning sharing network weights and hyperparameters , ” Neural Networks , vol . 144 , pp . 138 – 153 , 2021 . [ 10 ] E . Todorov , T . Erez , and Y . Tassa , “MuJoCo : A physics engine for model - based control , ” in Proc . of IROS , 2012 . [ 11 ] A . Boularias , J . Kober , and J . Peters , “Relative entropy inverse rein - forcement learning , ” in Proc . of AISTATS , 2011 . [ 12 ] B . D . Ziebart , A . Maas , J . A . Bagnell , and A . K . Dey , “Maximum entropy inverse reinforcement learning , ” in Proc . of AAAI , 2008 . [ 13 ] E . Uchibe , “Model - free deep inverse reinforcement learning by logistic regression , ” Neural Processing Letters , vol . 47 , no . 3 , pp . 891 – 905 , 2018 . [ 14 ] P . Barde , J . Roy , W . Jeon , J . Pineau , C . Pal , and D . Nowrouzezahrai , “Adversarial soft advantage ﬁtting : Imitation learning without policy optimization , ” in NeurIPS , 2020 . [ 15 ] G . Zuo , K . Chen , J . Lu , and X . Huang , “Deterministic generative adversarial imitation learning , ” Neurocomputing , pp . 60 – 69 , 2020 . [ 16 ] G . Zuo , Q . Zhao , K . Chen , J . Li , and D . Gong , “Off - policy adversarial imitation learning for robotic tasks with low - quality demonstrations , ” Applied Soft Computing , 2020 . [ 17 ] Z . Zhu , K . Lin , B . Dai , and J . Zhou , “Off - policy imitation learning from observations , ” in NeurIPS , 2020 . [ 18 ] H . Hoshino , K . Ota , A . Kanezaki , and R . Yokota , “OPIRL : Sample efﬁcient off - policy inverse reinforcement learning via distribution match - ing , ” in Proc . of ICRA , 2022 . [ 19 ] S . Reddy , A . D . Dragan , and S . Levine , “SQIL : Imitation learning via regularized behavioral cloning , ” in Proc . of ICLR , 2020 . [ 20 ] D . Nishio , D . Kuyoshi , T . Tsuneda , and S . Yamane , “Discriminator soft actor critic without extrinsic rewards , ” in Proc . of the 9th IEEE Global Conference on Consumer Electronics , 2020 , pp . 327 – 32 . [ 21 ] M . Herman , T . Gindele , J . Wagner , F . Schmitt , and W . Burgard , “Inverse reinforcement learning with simultaneous estimation of rewards and dynamics , ” in Proc . of AISTATS , 2016 . [ 22 ] N . Baram , O . Anschel , I . Caspi , and S . Mannor , “End - to - end differen - tiable adversarial imitation learning , ” in Proc . of ICML , 2017 . [ 23 ] N . Rhinehart , K . M . Kitani , and P . Vernaza , “R2P2 : A ReparameteRized Pushforward Policy for diverse , precise generative path forecasting , ” in Proc . of ECCV , 2018 . [ 24 ] J . Sun , L . Yu , P . Dong , B . Lu , and B . Zhou , “Adversarial inverse rein - forcement learning with self - attention dynamics model , ” IEEE Robotics and Automation Letters , vol . 6 , no . 2 , pp . 1880 – 1886 , 2021 . [ 25 ] V . Saxena , S . Sivanandan , and P . Mathur , “Dyna - AIL : Adversarial imitation learning by planning , ” in ICLR 2020 Workshop : Beyond Tabula Rasa in Reinforcement Learning , 2020 . [ 26 ] F . Torabi , G . Warnell , and P . Stone , “DEALIO : Data - efﬁcient adversarial learning for imitation from observation , ” in Proc . of IROS , 2021 . [ 27 ] M . Sugiyama , T . Suzuki , and T . Kanamori , Density ratio estimation in machine learning . Cambridge University Press , 2012 . [ 28 ] R . A . Bradley and M . E . Terry , “Rank analysis of incomplete block designs : I . the method of paired comparisons , ” Biometrika , vol . 50 , no . 3 / 4 , pp . 324 – 345 , 1952 . [ 29 ] R . S . Sutton , “Integrated architecture for learning , planning , and reacting based on approximating dynamic programming , ” in Proc . of ICML , 1990 . [ 30 ] R . S . Sutton , D . McAllester , S . Singh , and Y . Mansour , “Policy gradient methods for reinforcement learning with function approximation , ” in NeurIPS , 2000 , pp . 1057 – 1063 . [ 31 ] I . Kostrikov , O . Nachum , and J . Tompson , “Imitation learning via off - policy distribution matching , ” in Proc . of ICLR , 2019 . [ 32 ] T . Haarnoja , A . Zhou , P . Abbeel , and S . Levine , “Soft Actor - Critic : Off - policy maximum entropy deep reinforcement learning with a stochastic actor , ” in Proc . of ICML , 2018 . [ 33 ] G . Brockman , V . Cheung , L . Pettersson , J . Schneider , J . Schulman , J . Tang , and W . Zaremba , “OpenAI Gym , ” CoRR , abs / 1606 . 01540 , 2016 . [ 34 ] J . Schulman , S . Levine , P . Abbeel , M . Jordan , and P . Moritz , “Trust region policy optimization , ” in Proc . of ICML , 2015 . [ 35 ] S . Elfwing , E . Uchibe , and K . Doya , “Sigmoid - weighted linear units for neural network function approximation in reinforcement learning , ” Neural Networks , vol . 107 , pp . 3 – 11 , 2018 . [ 36 ] P . Ghosh , M . S . M . Sajjadi , A . Vergari , M . Black , and B . Schaolkopf , “From variational to deterministic autoencoders , ” in Proc . of ICLR , 2020 . [ 37 ] S . Chitta , I . Sucan , and S . Cousins , “Moveit ! [ ROS topics ] , ” IEEE Robotics Automation Magazine , vol . 19 , no . 1 , pp . 18 – 19 , 2012 . [ 38 ] Y . Zhang , Q . Cai , Z . Yang , and Z . Wang , “Generative adversarial imita - tion learning with neural networks : Global optimality and convergence rate , ” in Proc . of ICML , 2020 . [ 39 ] Z . Guan , X . Tengyu , and L . Yingbin , “When will generative adversarial imitation learning algorithms attain global convergence , ” in Proc . of AISTATS , 2021 .