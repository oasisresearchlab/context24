Advice taking and decision - making : An integrative literature review , and implications for the organizational sciences q Silvia Bonaccio * , Reeshad S . Dalal Department of Psychological Sciences , 703 Third Street , West Lafayette , Indiana 47907 , Purdue University , USA Received 28 March 2005 Available online 21 August 2006 Communicated by Scott Highhouse Abstract This paper reviews the advice - giving and advice - taking literature . First , the central ﬁndings from this literature are catalogued . Topics include : advice utilization , conﬁdence , decision accuracy , and diﬀerences between advisors and decision - makers . Next , the implications of several variations of the experimental design are discussed . These variations include : the presence / absence of a pre - advice decision , the number of advisors , the amount of interaction between the decision - maker and the advisor ( s ) and also among advisors themselves , whether the decision - maker can choose if and when to access advice , and the type of decision - task . Several ways of measuring advice utilization are subsequently contrasted , and the conventional operationalization of ‘‘advice’’ itself is questioned . Finally , ways in which the advice literature can inform selected topics in the organizational sciences are discussed . (cid:2) 2006 Elsevier Inc . All rights reserved . Keywords : Judge – advisor system ; JAS ; Advisor ; Advice ; Advice giving ; Advice taking ; Utilization ; Discounting ; Token shift ; Decision - making Introduction Many ( if not most ) important decisions are not made by one person acting alone . A new college graduate , for example , is likely to consult his or her parents and peers about which job oﬀer to accept ; similarly , a personnel manager may well ask for colleagues’ advice prior to revamping the organization’s compensation system . Yet , the ﬁeld of judgment and decision - making has not systematically investigated the social context of deci - sions ( e . g . , Payne , Bettman , & Johnson , 1993 ) . One area that takes into account the fact that individ - uals do not make decisions in isolation is the ‘‘small groups’’ literature ( Kerr & Tindale , 2004 ) . However , this area typically assumes that group members’ roles are ‘‘undiﬀerentiated’’ ( Sniezek & Buckley , 1995 , p . 159 ) — i . e . , that all members have the same responsibilities vis - a ` - vis the decision task . Yet , leaders often emerge ( and , in general , status hierarchies materialize ) from originally undiﬀerentiated groups . In fact , one of the dimensions of individual performance often evaluated in the ‘‘leaderless group discussion’’ ( Bass , 1954 ) is lead - ership behavior ( Campbell , Simpson , Stewart , & Man - ning , 2003 ; Petty & Pryor , 1974 ; Waldman , Atwater , & Davidson , 2004 ) . In most real - world social organiza - tions , moreover , role structures are formalized and con - tributions to decisions are commonly unequal ( Katz & Kahn , 1966 ) . Numerous important decisions therefore www . elsevier . com / locate / obhdp Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 0749 - 5978 / $ - see front matter (cid:2) 2006 Elsevier Inc . All rights reserved . doi : 10 . 1016 / j . obhdp . 2006 . 07 . 001 q This paper is dedicated to Janet A . Sniezek . Her advice and mentorship are missed . We are grateful to David Budescu , Carolyn Jagacinski , Janice Kelly , and Charlie Reeve for their helpful comments on an earlier version of this paper . * Corresponding author . Present address : School of Management , University of Ottawa , 136 Jean - Jacques Lussier Street , Ottawa , Ont . , Canada K1N 6N5 . E - mail address : bonaccio @ management . uottawa . ca ( S . Bonaccio ) . appear to take place within a structure that is not well captured either by an individual acting alone or by all group members acting equally ( Brehmer & Hagafors , 1986 ; Sniezek & Buckley , 1995 ) . Speciﬁcally , decisions are often made by individuals after consulting with , and being inﬂuenced by , others . It is to model such deci - sion - making structures that research began to be con - ducted on advice - giving and advice - taking during decisions . Impetus for the review and organization of the current paper The impetus for this review is manifold . Although research on advice giving and taking is about two dec - ades old ( see Brehmer & Hagafors , 1986 , for the ﬁrst published paper ) , there has not yet been a comprehen - sive attempt to integrate the ﬁndings from , and identify the strengths and weaknesses of , the extant research . This paper attempts these tasks . The current review begins descriptively and then moves progressively toward greater evaluation . To this end , we ﬁrst describe the terminology used in the paper and outline a proto - typical study . Next , we review the central ﬁndings of the advice - giving and advice - taking literature . Follow - ing this section , we discuss several variations of the experimental design that have important implications for the questions posed and that may inﬂuence the con - clusions reached in a particular study . Next , various methods for calculating advice utilization are described and critiqued . After this , the dominant deﬁnition of ‘‘ad - vice’’ itself ( and hence , indirectly , of advice utilization ) is questioned . We moreover believe that the advice litera - ture is now mature enough to inform , and be informed by , other areas of research—particularly in the organi - zational sciences . To this end , we conclude this paper by discussing a number of research topics with connec - tions to advice taking and advice giving . However , one such topic—Hierarchical Decision - Making Teams ( HDT ; e . g . , Hollenbeck et al . , 1995 ; Humphrey , Hollen - beck , Meyer , & Ilgen , 2002 ) —is a subset of the larger ‘‘Judge – Advisor System’’ 1 ; relevant HDT ﬁndings will therefore be reviewed throughout the paper . An alternative approach would have been to struc - ture this review around a comprehensive theory of advice giving and taking . Unfortunately , no such theory exists—perhaps because of the breadth of research ques - tions addressed thus far ( see Hollenbeck et al . , 1995 , for a more narrowly focused theory applicable to HDTs ) , and , as mentioned previously , the relative youth of this research area . In fact , one of the motivations for this review was to aid in theory generation by summarizing relevant research ﬁndings and by raising questions that a comprehensive theory of advice will need to address . Terminology and description of prototypical study Before reviewing research ﬁndings , it is necessary to describe the terminology used in this paper . Following most of the advice - taking research ( e . g . , Harvey & Fischer , 1997 ; Yaniv , 2004b ) , the term ‘‘judge’’ refers to the decision - maker—the person who receives the advice and must decide what to do with it . The judge is the person responsible for making the ﬁnal decision . The ‘‘advisor’’ is , as the name implies , the source of advice or suggestions . 2 In addition , most studies have conceived of ‘‘advice’’ in terms of a recommendation , from the advisor , favoring a particular option . For instance , if the judge has to choose between three options , he or she would typically receive advice like : ‘‘Choose Option X . ’’ A few studies of advice have , in addition , allowed expressions of conﬁdence or ( un ) cer - tainty related to the recommendation—e . g . , ‘‘Choose Option X ; I am 85 % sure that it’s the best option . ’’ ( As we discuss later in the paper , there is reason to ques - tion the appropriateness of deﬁnitions of advice that focus solely on recommendations . ) In a ‘‘prototypical’’ Judge – Advisor System ( hereaf - ter , ‘‘JAS’’ ) study , participants enter the laboratory and are randomly assigned to the role of ‘‘judge’’ or ‘‘advisor . ’’ They are informed that the judge , not the advisor , must make the ﬁnal decision ( s ) ; as such , it is up to the judge to determine whether he or she should take the advice into consideration at all , and , if so , how much weight the advice should carry . Manipula - tions of independent variables ( expertise diﬀerences between judges and advisors , type of ﬁnancial incentives for JASs across conditions , etc . ) are then eﬀected—typ - ically in a between - subjects fashion . Next , both JAS members read information about the decision task . The judge makes an initial decision . He or she may also be asked to express a level of conﬁdence regarding the accuracy or eﬀectiveness of the initial decision . Simulta - 1 In fact , the HDT paradigm was speciﬁcally formulated to explain instances in which the decision - maker and multiple advisors always : share common outcomes , communicate with one another and with the decision - maker in real time , and work together over a number of trials on a quantitative judgment task on which they receive accurate performance - related feedback . Though the HDT literature regularly references the Judge – Advisor System , the converse is not true . Through this review , we hope to make Judge – Advisor System researchers more aware of the HDT literature ( including its roots stretching back to Brehmer & Hagafors , 1986 ) . 2 To avoid confusion , the terminology of authors ( e . g . , Budescu & Rantilla , 2000 ) who use the term ‘‘judge , ’’ ‘‘advisor , ’’ or ‘‘expert’’ interchangeably to refer to the person providing advice will not be employed in this paper . Furthermore , though some studies ( e . g . , Heath & Gonzalez , 1995 ) were not explicitly conducted with the JAS in mind , they are nonetheless highly informative and will therefore be included in this review . When this is the case , the JAS language is used to describe their manipulations and ﬁndings . 128 S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 neously , the advisor is asked to make a recommendation to the judge—accompanied , perhaps , by an expression of conﬁdence . Next , the advisor’s recommendation is conveyed to the judge ( the advisor , in contrast , is typi - cally unaware of the judge’s initial decision ) . The judge weighs his or her own initial decision and the advisor’s recommendation and arrives at a ﬁnal decision and , per - haps , a conﬁdence estimate . The judge’s ﬁnal decision can often be evaluated in terms of accuracy or eﬀective - ness . In many instances , the judge is required to make not one but a series of decisions ; therefore , after the judge makes a ﬁnal decision , he or she moves on to the next decision task . It should be noted that this ‘‘prototype’’ does not rep - resent any JAS study perfectly ; in fact , it represents some rather poorly . Note also that the JAS operates within the context of the speciﬁc decision task ( s ) employed by researchers . Both these issues are discussed later . We begin our review , however , with an explication of some of the important ﬁndings from the literature . Central ﬁndings of the advice literature To provide a framework for the central ﬁndings of the advice literature ( and the subsequent section on experimental design ) , we propose an input - process - out - put model for the JAS . In so doing , we borrow from the literature on ( undiﬀerentiated ) small groups ( e . g . , Hackman , 1987 ) . The ‘‘input’’ category in our model comprises individ - ual - level , JAS - level , and environment - level factors . Indi - vidual - level inputs include role diﬀerences ( e . g . , diﬀerences between the advisor and judge roles ) , individ - ual diﬀerences ( e . g . , characteristic levels of accuracy and conﬁdence ) , and individual task - related preferences ( e . g . , the advisor’s recommendation and conﬁdence , and the judge’s pre - advice opinion and conﬁdence , on the decision task at hand ) . JAS - level inputs include JAS structure ( e . g . , whether the judge is allowed to form a pre - advice opinion , and whether the judge has a choice about whether to solicit and / or access advice ) and JAS size ( e . g . , the number of advisors from whom a judge receives advice ) . Environment - level inputs include task type ( i . e . , the type of decision - task facing the JAS— e . g . , a choice among discrete alternatives or a quantita - tive judgment ) and reward structure ( e . g . , the presence and amount of ﬁnancial incentives , whether they are tied to decision accuracy , and their distribution among the judge and advisors ) . The ‘‘process’’ category comprises intra - JAS interac - tion ( i . e . , interaction between the judge and an advisor , or between two advisors ) . The ‘‘output’’ category com - prises advice utilization ( or discounting ) , the judge’s post - advice accuracy and conﬁdence , and JAS - member satisfaction and willingness to continue in the JAS . We begin our review of the ‘‘central ﬁndings’’ in the advice literature with advice utilization ( an output ) , per - haps the single most studied aspect of the JAS . Other central ﬁndings include those concerning advisor conﬁ - dence ( an input ) and judge conﬁdence ( an input in the case of judge pre - advice conﬁdence , and an output in the case of judge post - advice conﬁdence ) , the accuracy of the judge’s post - advice decision ( an output ) , and the diﬀerence between advisors and ‘‘personal decision makers’’ ( an input ) . Advice utilization or discounting Advice utilization refers to the extent to which the judge follows advice ; advice discounting , conversely , refers to the extent to which the judge does not follow advice . As described in this section , advice utilization ( or discounting ) is posited to inﬂuence the judge’s post - advice decision ; however , much research has stud - ied this construct as a criterion in its own right . Decision - makers seek out and attend to advice in order to share accountability for the outcome of the decision and to improve the probability that their deci - sions will be accurate or optimal ( Harvey & Fischer , 1997 ; Yaniv , 2004a , 2004b ) . For instance , interacting with others prior to making a decision forces decision - makers to think of the decision problem in new ways ( Schotter , 2003 ) and provides decision - makers with new information or alternatives not previously consid - ered ( Heath & Gonzalez , 1995 ) . In addition , receiving advice from a credible source reduces the eﬀects of fram - ing ( Druckman , 2001 ) . Many scholars maintain that there are indeed accuracy beneﬁts to be gained by inte - grating advice that comes from multiple—preferably independent / uncorrelated ( Soll , 1999 ; see also Johnson , Budescu , & Wallsten , 2001 ) —sources , and that perhaps as few as three to six sources would be suﬃcient ( Bude - scu & Rantilla , 2000 ; Yaniv , 2004a ; Yaniv & Kleinber - ger , 2000 ) . The aforementioned reasons pertain to decision quality . However , there are also social reasons for taking advice . Sniezek and Buckley ( 1995 ) indicated the possibility of social pressure not to reject freely oﬀered advice : such advice , if rejected , may not be prof - fered again in the future . Yet , one of the most robust ﬁndings in the JAS liter - ature is that of ‘‘egocentric advice discounting’’ ( e . g . , Yaniv , 2004b ; Yaniv & Kleinberger , 2000 ) . That is , most JAS researchers have noted that judges in their experi - ments did not follow their advisors’ recommendations nearly as much as they should have ( to truly have bene - ﬁted from them ) . Although advice generally helps improve judges’ accuracy , judges tend to overweigh their own opinion relative to that of their advisor ( e . g . , Gard - ner & Berry , 1995 ; Harvey & Fischer , 1997 ; Yaniv & Kleinberger , 2000 ) . In fact , Harvey and Fischer ( 1997 , see also Soll and Larrick , 1999 ) observed that judges S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 129 tended to shift a ‘‘token’’ amount—about 20 % to 30 % — toward their advisor’s initial estimate . Yaniv ( 2004a , 2004b ; Yaniv and Kleinberger , 2000 ) has contended that advice discounting occurs because judges have access to their internal justiﬁcations for arriving at a particular decision as well as to the strength of the supporting evidence for that decision . In contrast , they do not have access to the advisors’ reasoning , and , hence , have access to less evidence justifying the advis - ors’ decisions . Advice discounting could also occur because of the anchoring and adjustment strategy ( e . g . , Tversky & Kahneman , 1974 ) such that the judge’s initial decision serves as the anchor that is subsequently ( insuf - ﬁciently ) adjusted in response to the advisor’s recom - mendation ( Harvey & Fischer , 1997 ; Lim & O’Connor , 1995 ) . In contrast to these explanations , Krueger ( 2003 ) argues that discounting may occur because of an ego - centric bias . That is , judges may prefer their own opin - ions because they believe them to be superior to those of others—including the advisor . Krueger notes that decision - makers display this egocentric bias even when they are making judgments about novel situations or when they receive advice prior to seeing the decision task . In the former case , judges cannot rely on their own supporting evidence for a position ( Cadinu & Rothbart , 1996 ) ; in the latter case , there is no initial decision to serve as an ‘‘anchor’’ ( Clement & Krueger , 2000 ) . In addition , Harvey and Harries ( 2004 ) argue that egocentrism ( or conservatism , in their words ) is dif - ferent from anchoring . They argue that anchoring is a temporary and short - term process whereby an anchor is used in evaluation of the new stimulus . On the other hand , egocentrism / conservatism speciﬁcally refers to the long - term inﬂuence of one’s own opinion . This opin - ion exerts inﬂuence ‘‘without being disrupted by the appearance of intervening values that would displace anchors from working memory’’ ( Harvey & Harries , 2004 , p . 399 ) . Harvey and Harries’ results strongly favor the egocentrism / conservatism account of discounting rather than the anchoring and adjustment one . Speciﬁ - cally , their results demonstrated longer - term eﬀects of a presumed anchor than those usually associated with the anchoring and adjustment paradigm . In addition , their results showed that decision - makers gave greater weight to someone else’s forecasts incorrectly labeled as their own than to correctly labeled others’ forecasts . Given that the incorrectly labeled forecast was novel to decision - makers , it could not have served as an anchor ; yet , it could serve as the basis of an egocen - tric / conservative forecast . Although egocentric advice discounting is a robust phenomenon in the literature ( but see Gino , 2005 ; Schotter , 2003 , for possible exceptions ) , some aspects of the JAS context—all inputs in the input - process - out - put model—seem to ameliorate ( though typically not eliminate ) it . It appears that advice is perceived as more helpful and less intrusive if proﬀered by an expert source ( Goldsmith & Fitch , 1997 ) and that expert advice is more inﬂuential than novice advice is ( Jungermann & Fischer , 2005 ) . In essence , when advisors have greater task - relevant expertise or knowledge , they possess ‘‘ex - pert power’’ ( French & Raven , 1959 ; also see Birnbaum & Stegner’s , 1979 , discussion of source credibility ) . Con - sequently , less egocentric discounting is displayed by judges who are less experienced or knowledgeable rela - tive to their advisors ( e . g . , Harvey & Fischer , 1997 ; Sni - ezek , Schrah , & Dalal , 2004 ; also see Dalal , 2001 , for related ﬁndings ) and / or relative to other judges ( Harvey & Fischer , 1997 ; Yaniv , 2004b ; Yaniv & Kleinberger , 2000 ; Yaniv & Milyavsky , in press ) . These ﬁndings are in line with Yaniv’s ( 2004b ) reasoning that discounting occurs because of greater evidence retrieval for one’s own opinion relative to the advisor’s ( similar arguments were made by Koehler , 1994 , in discussing the diﬀerenc - es between generating one’s own hypotheses and evalu - ating the hypotheses of others ) . Indeed , Yaniv concluded that less knowledgeable judges could presum - ably retrieve less supporting information for their own opinion and therefore discount advice less than more knowledgeable judges . Given that Yaniv did not mea - sure information retrieval , future research should include such measures to test this conjecture . As might be expected from the ﬁndings on expertise , advice taking has also been linked theoretically ( Junger - mann , 1999 ) and empirically ( Yaniv & Kleinberger , 2000 ; Yaniv & Milyavsky , in press ) to the quality of the advice ( an input in the input - process - output model ) . Not surprisingly , judges discount poor ( inaccurate ) advice more than they discount good / accurate advice ( although they may also discount good advice : Gardner & Berry , 1995 ; Lim & O’Connor , 1995 ; Yaniv & Klein - berger , 2000 ) . In addition , judges’ weighting policies are sensitive to changes in the quality of advice : whereas a good reputation is gained with diﬃculty , it is easily lost when the quality of advice decreases . Therefore , judges quickly learn to discount poor advice ( Yaniv & Klein - berger , 2000 ) . The diﬀerential quality of a group of advisors’ past recommendations , moreover , may inﬂu - ence judges’ use of present recommendations and , ulti - mately , decision accuracy ( Redd , 2002 ) . Finally , judges use information and explanations about their advisors’ forecasting strategies as a way to infer advice quality ; yet , judges’ inferences of quality may not always be cor - rect ( Yates , Price , Lee , & Ramirez , 1996 ) . Note , in addi - tion , that whereas expertise has typically been construed as task - relevant expertise , judges also appear to be more responsive to advice from those with greater age , educa - tion , life experience , and wisdom than themselves ( Feng & MacGeorge , 2006 ) . Advice discounting is also aﬀected by the presence of performance - contingent ﬁnancial incentives / rewards , 130 S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 which functions as an input in the input - process - output model . Although numerous studies ( e . g . , Budescu & Rantilla , 2000 ; Dalal , 2001 ; Yaniv , 2004b ) have made incentives available to their JASs , not all of these studies manipulated their presence or amount . Most of those that did involve such a manipulation found that incen - tives reduce advice discounting ( Sniezek & Van Swol , 2001 ; Sniezek et al . , 2004 ; but see Dalal , 2001 ) . This is consistent with Camerer and Hogarth’s ( 1999 ) conclu - sion that incentives increase eﬀort ( though not necessar - ily accuracy ) . The JAS member responsible for allocating the ﬁnan - cial incentive is said to hold ‘‘reward power’’ ( French & Raven , 1959 ) . In general , results indicate that advice dis - counting is less pronounced when judges , rather than advisors , have reward power ( Sniezek & Van Swol , 2001 ; but see Van Swol & Sniezek , 2005 ) , or when judges allocate incentives to expert advisors prior to the advis - ing interaction ( such that they are , in essence , pre - pay - ing for expert advice ; Sniezek et al . , 2004 ) . Gino ( 2005 ) also found that judges weighed purchased advice more heavily than they weighed free advice . She argued that such behavior is consistent with the tendency to escalate commitment to a ‘‘sunk cost’’ ( a course of action in which one has previously invested time , money and / or eﬀort ; Arkes & Blumer , 1985 ) . In contrast , Dalal ( 2001 ) found increased advice dis - counting in the presence of ﬁnancial incentives for pairs of JASs playing the prisoner’s dilemma game against each other . Dalal concluded , based on exploratory anal - yses , that the presence of ﬁnancial incentives motivated both advisors and judges to decrease adherence to their respective sources of expertise : advisors departed from the Tit - for - Tat strategy ( Axelrod , 1984 ) they had been trained to use , whereas judges decreased advice utiliza - tion . In this case , the advisors’ behavior primarily repre - sented an attempt to break the cycle of mutual defection by recommending unilateral cooperation ; however , judges did not go along with these proposals—perhaps because their own analysis of previous trials suggested that the opposing JAS would exploit , rather than reward , unilateral cooperation . Financial incentives can also be used to manipulate the extent of ‘‘outcome interdependence’’ between judg - es and advisors . When provided , incentives have usually involved a monetary award to both the judge and the advisor contingent on the judge’s ﬁnal decision accura - cy . However , situations could easily be set up wherein the judge’s successful performance results in money for the judge but not for the advisor , or for the advisor but not for the judge . Alternatively , whereas the judge could be rewarded contingent on his or her successful performance , the advisor could be rewarded contingent on the judge’s unsuccessful performance . In such situa - tions , the goals of the judge and advisor are incongruent ( in the terminology of agency theory , there is an ‘‘agency problem’’ ; Eisenhardt , 1989 ) . Judges would be expected to discount advice to a greater extent if they knew , or even erroneously suspected , that their advisors’ goals diﬀered from their own . That is , one would predict that judges’ trust in their advisors will be positively related to advice taking ( see also Jungermann , 1999 ; Jungermann & Fischer , 2005 ) . Some empirical evidence indicates that this is indeed the case ( Sniezek , Heath , Van Swol , & Nochimowski , 1998 ; Sniezek & Van Swol , 2001 ; Van Swol & Sniezek , 2005 ) . Other aspects of the decision - making context also appear to have an impact on advice discounting . Judges who solicit advice are more likely to follow that recom - mendation than are judges who receive advice without requesting it ( Gibbons , Sniezek , & Dalal , 2003 ; see also Deelstra et al . , 2003 , for a discussion of potential detri - mental eﬀects of imposed help ) . Judges also appear to discount advice less when tasks are complex ( Gino & Moore , in press ; Schrah , Dalal , & Sniezek , 2006 ) . Judg - es’ individual diﬀerences in autonomy also seem to inﬂu - ence advice acceptance from expert sources ( Koestner et al . , 1999 ) . Moreover , judges are more likely to follow the advice when advisors make decisions for themselves in a manner consistent with their recommendations to the judge ( Schotter , 2003 ) . Finally , advice discounting has been linked to the persuasion and attitude change literature . For example , discounting increases as the distance between the judges’ initial opinions and the advisors’ recommendations increases ( Yaniv , 2004b ; Yaniv & Milyavsky , in press ) , an eﬀect similar to that found in classic studies of attitude change ( e . g . , Bochner & Insko , 1966 ; Sherif & Hovland , 1961 ) . Yaniv also noted that this eﬀect was particularly pronounced for more knowledgeable judges . In addition , Harries , Yaniv , and Harvey ( 2004 ) reported that judges tend to discount advisors whose recommendations are very diﬀerent from those of other advisors ( i . e . , judges discount outlying advice ) . Related - ly , advice taking is linked to the literature on belief updating . Thus , advice taking or discounting could operate according to Hogarth and Einhorn’s ( 1992 ) belief adjustment model . Over - reliance on the judges’ initial opinion could be a function of , among other things , how the information is encoded ( i . e . , relative to the pre - advice opinion or a constant ) , task length ( here , the number of pieces of advice received ) , the complexity of the information , and how the information is pro - cessed ( i . e . , if judges can process advice in a step - by - step manner , or if they have to wait until after all advice has been received prior to processing it ) . A full investigation of Hogarth and Einhorn’s model in the context of the JAS has not yet been conducted . In conclusion , and at the risk of oversimplifying , advice discounting may well be a function of the judge’s estimates of the potential costs and beneﬁts of advice ( Schrah et al . , 2006 ) . The potential beneﬁts of advice S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 131 are greater decision accuracy ( and its likely consequenc - es , monetary and otherwise ) and shared responsibility for the decision , whereas the potential cost is greater eﬀort ( as well as perhaps some monetary expenditure ) . Whether or not the judge utilizes advice depends on whether he or she believes that the potential beneﬁts outweigh the potential costs . Cost - beneﬁt judgments will be adjusted to match the situation ; in that sense judges , like individual decision - makers , are ‘‘adaptive’’ ( Payne et al . , 1993 ) even though they are only partially sensitive to changes in task characteristics relevant to beneﬁts and costs ( Connolly & Gilani , 1982 ; Harvey & Bolger , 2001 ) . For example , judges may make the eﬀort to avoid using only easily accessible information and may consciously ensure that they adjust to a greater extent than they nor - mally would from their pre - advice decisions in response to the advice if the stakes for making a correct decision are high ( e . g . , performance - contingent ﬁnancial incen - tives are available ) and if the advisor is known to be trustworthy ( i . e . , expert and well - intentioned ) . On the other hand , judges who are making routine , low - stakes decisions that fall squarely in their area of expertise are unlikely to take advice if they have to pay ( monetar - ily or otherwise ) for it and if the advisor was not previ - ously known to them . Thus , as Schrah et al . ( 2006 ) argue , a cost - beneﬁt perspective is likely to provide a useful framework ( though not a suﬃcient one ; Connolly & Gilani , 1982 ; Harvey & Bolger , 2001 ) for a theory of advice discounting . Judge and advisor conﬁdence Another characteristic of the JAS that has received considerable research attention is the amount of conﬁ - dence expressed by either the advisor or the judge . A judge’s pre - advice conﬁdence and the advisor’s conﬁ - dence function as inputs , whereas the judge’s post - ad - vice conﬁdence functions as an output . Conﬁdence has operationally been deﬁned as an expectation ( e . g . , via a probability estimate or a rating on a Likert - type scale ) of the extent to which a decision / opinion / recommenda - tion is correct , or as a range of values within which the correct answer should fall ( i . e . , a subjective conﬁdence interval ; Klayman , Soll , Gonzalez - Vallejo , & Barlas , 1999 ) . Most research on advisor conﬁdence has focused on conﬁdence vis - a ` - vis the decision task in question as opposed to habitual levels of ( i . e . , individual diﬀerences in ) conﬁdence . On a decision task , expressed or inferred advisor conﬁdence can serve as an important source of information . For instance , Price and Stone ( 2004 ) have argued that judges rely on a ‘‘conﬁdence heuristic . ’’ That is , judges use the advisors’ conﬁdence to infer their abil - ity , expertise , task - related knowledge , or accuracy ( also see Sniezek & Buckley , 1995 ; Sniezek & Van Swol , 2001 ) . Similarly , the width of the conﬁdence interval can be a cue to the advisor’s perception of his or her own knowledge ( Yaniv , 1997 ; Yaniv & Foster , 1997 ) . It has been found that recommendations given by more conﬁdent advisors are followed more often than those given by their less conﬁdent counterparts ( Law - rence & Warren , 2003 ; Phillips , 1999 ; Sniezek & Buck - ley , 1995 ; Sniezek & Van Swol , 2001 ; Van Swol & Sniezek , 2005 ; Yaniv , 1997 ) . Further , Price and Stone ( 2004 ) found that judges even preferred overconﬁdent advisors to appropriately conﬁdent advisors . This raises the interesting possibility that advisors can strategically use expressed conﬁdence levels as a means to inﬂuence judges ( Hollenbeck et al . , 1995 ; Sniezek & Buckley , 1995 ; Yates et al . , 1996 ) . However , there are probably limits to the extent to which advisors can inﬂate conﬁ - dence in order to wield greater inﬂuence . For instance , extreme advice ( e . g . , probabilistic forecasts close to 100 % on a two - option choice task , where the probability of a guess would be 50 % ) may be interpreted not as con - ﬁdence but rather as recklessness ( Yates et al . , 1996 ) . In addition , conﬁdence levels appear to be less inﬂuential than agreement between advisors ( Sniezek & Buckley , 1995 ) . It is interesting to note that advisors’ conﬁdence levels are not always a valid cue to advice accuracy . In other words , the correlation between an advisor’s reported con - ﬁdence level and his or her accuracy is sometimes low . Thus , while some studies have found that conﬁdence rat - ings were valid cues of advice quality ( e . g . , Sniezek & Van Swol , 2001 ; Van Swol & Sniezek , 2005 ) , others have not ( e . g . , Gibbons et al . , 2003 ; Phillips , 1999 ) . The diﬀerence may be due to the fact that Sniezek and Van Swol ( 2001 ; Van Swol and Sniezek , 2005 ) had speciﬁcally paired expert advisors with novice judges . Judges’ own conﬁdence has also been investigated . Again , the focus has typically been on judges’ pre - advice and post - advice conﬁdence on the decision task in ques - tion rather than on judges’ habitual levels of conﬁdence . The conﬁdence of judges in their pre - advice decisions is related to the number of advisors from whom they solic - it free advice , such that less conﬁdent judges seek greater amounts of advice ( Cooper , 1991 ) . In turn , judges’ con - ﬁdence in their post - advice decision appears to be inﬂu - enced by a number of variables . Post - advice conﬁdence increases with increasing advisor accuracy ( Budescu , Rantilla , Yu , & Karelitz , 2003 ) . Conﬁdence levels are also higher when judges receive recommendations from multiple advisors , when there is a greater amount of information on which advisors can base their recom - mendations , and when there is greater overlap in the information seen by the advisors ( Budescu & Rantilla , 2000 ; Budescu et al . , 2003 ) . However , the level of agreement among advisors appears to inﬂuence conﬁdence ratings such that when advisors disagree with each other , judges’ post - advice conﬁdence is low ( Budescu et al . , 2003 ; Savadori , Van 132 S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 Swol , & Sniezek , 2001 ; see also Sniezek & Buckley , 1995 ) —especially if judges believe that the disagreeing advisors had access to the same information ( Budescu & Rantilla , 2000 ) . Moreover , the eﬀort necessary to pro - cess and react to an advisor’s recommendation is related to judges’ post - advice conﬁdence in their decisions : higher eﬀort leads to higher post - advice conﬁdence ( Kuhn , Spurlock , & Sniezek , 1998 ; see also Paese & Sni - ezek , 1991 ) . Finally , judges’ conﬁdence in their decision is higher post - advice than pre - advice ( Heath & Gonz - alez , 1995 ; Savadori et al . , 2001 ) , perhaps because the process of interacting with the advisor ( s ) leads judges to construct a rationale for their decision ( Heath & Gonzalez , 1995 ) . Some researchers have also found that judges are overconﬁdent in their judgments . Although it has been the focus of much research attention in the decision - making literature as a whole ( e . g . , Klayman et al . , 1999 ) , overconﬁdence ( or , conversely , underconﬁdence ) has seldom been explicitly investigated in JASs . Sniezek and Buckley ( 1995 ) found that the judges who had access to the least amount of task - speciﬁc information , and consequently had to fully rely on their advisors’ rec - ommendations , were the most overconﬁdent in their choices ( underconﬁdence was not displayed ) . However , judges who received conﬂicting recommendations from their advisors were not overconﬁdent . Furthermore , Heath and Gonzalez ( 1995 ) demonstrated that interac - tion between decision - makers , in which advice could be exchanged , did not necessarily increase accuracy but did increase conﬁdence—and hence overconﬁdence . It is particularly important to note that the overcon - ﬁdence literature indicates that the prevalence of over - conﬁdence depends on the type of task used , with overconﬁdence being more likely in judgment than in choice tasks ( Klayman et al . , 1999 ; Soll & Klayman , 2004 ) . We return to task characteristics in a later sec - tion ; for the time being , we suggest only that JAS researchers interested in overconﬁdence ( and undercon - ﬁdence ) pay close attention to the type of task that they are employing . Accuracy of ﬁnal decisions The accuracy of judges’ post - advice ( ﬁnal ) decisions ( along with their own conﬁdence in these decisions—dis - cussed in the previous section ) , represents the end - prod - uct , or output , of the advising process . In general , using advice has been found to increase decision accuracy ( e . g . , Gardner & Berry , 1995 ; Sniezek et al . , 2004 ; Yaniv , 2004a ; but see Heath & Gonzalez , 1995 and Van Swol & Ludutsky , 2003 , for exceptions ) . This ﬁnding is consistent with Brehmer and Hagafors ( 1986 ) contention that relying on ( expert ) advisors should increase accuracy perhaps simply because relying on advice decreases the complexity of the overall deci - sion . This increase in accuracy should occur , according to these researchers , even if advice is slightly inaccurate . In fact , Yaniv ( 2004a , 2004b ) argues that combining the opinions of multiple , preferably uncorrelated , advisors increases decision accuracy because it reduces random error tied to each individual recommendation ( see also Stewart , 2001 ) . That is , aggregating across forecasts ensures that the resulting forecast has lower variability , lower random error , and converges towards the ‘‘true’’ forecast . In a related vein , optimal use of advice is improved by making the sources of advice more distin - guishable from each other . For example , Harvey , Har - ries , and Fischer ( 2000 ) found that judges made better use of advice when a subset of advisors gave advice revealing forecasting trends that were unusual for the task ( i . e . , with bias equal in magnitude but opposite in direction to what is normally observed in time series forecasts ) , thereby making these advisors stand out . Research has shown that judges’ post - advice decision accuracy is related to three core JAS - level variables : the average amount of decision - relevant information avail - able to advisors , the average accuracy of advisors’ rec - ommendations , and the weight the judge gives to each advisor’s recommendation ( Hedlund , Ilgen , & Hollen - beck , 1998 ; Hollenbeck , Ilgen , LePine , Colquitt , & Hedlund , 1998 ; Hollenbeck et al . , 1995 ; Humphrey et al . , 2002 ; see also Brehmer & Hagafors , 1986 ) . When advisors have more decision - relevant information , they are on average more accurate , which in turn aﬀects judg - es’ accuracy to the extent that the judges are capable of discriminating between good and bad advice ( and weighing the former more highly ) . Various types of feedback have also been linked to increased decision accuracy . First , providing feedback on judges’ own accuracy across trials improves their decision accuracy ( Fischer & Harvey , 1999 ) . Second , given the above ﬁndings on the antecedents to decision accuracy , it follows that providing feedback on these core JAS - level variables should also help with judges’ accuracy—and this has been shown to be the case ( Fischer & Harvey , 1999 ; Hollenbeck et al . , 1998 ; Phil - lips , 1999 ) . Furthermore , judges can learn to rely on out - lying advisors ( in spite of a natural tendency to discount dissenting opinions ) if they receive consistent feedback identifying the outlying advisor as accurate ( Harries et al . , 2004 ) . Decision accuracy is also inﬂuenced by task - related experience ( Hollenbeck et al . , 1998 , 1995 ; Phillips , 1999 ) . However , the eﬀects of both experience and feed - back on decision accuracy are at least partially mediated by the three core variables mentioned above ( Hollen - beck et al . , 1998 ) . Speciﬁcally , feedback inﬂuences judg - es’ post - advice accuracy by improving their weighting strategies . In contrast , experience inﬂuences judges’ post - advice accuracy by improving the accuracy of the advisors’ recommendations . S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 133 Note that it is somewhat surprising that feedback improves judges’ performance on advice - taking tasks . After all , decades of research on tasks such as multiple cue probability learning ( hereafter , ‘‘MCPL’’ ) tasks have demonstrated that improvements due to ( outcome ) feedback are at best modest and , even then , occur only after numerous trials ( Harvey & Fischer , 2005 ; Todd & Hammond , 1965 ) . In their review of the impact of feedback across various types of decision tasks , Harvey and Fischer ( 2005 ) speculate that feedback is helpful on tasks ( such as advice - taking tasks ) where it provides judges with easily interpretable information concerning how their performance can be improved . Interestingly , another feature that distinguishes between performance on advice - taking and MCPL tasks is ‘‘feedforward’’ ( i . e . , expectations based on information and instructions provided by the instructor ; Harries & Harvey , 2000 ; Harvey & Fischer , 2005 ) . For example , judges on an advice - taking task may reasonably conclude that the correct answer lies somewhere within the range of the advisor estimates ; thus , the advice ( and feedback ) is used eﬀectively . Decision - makers on MCPL tasks , on the other hand , realize that the correct answer is gener - ally not constrained to be within the range of the cue values ; thus , it is much more diﬃcult for judges to use the cue values ( and feedback ) eﬀectively . The accuracy of post - advice decisions is also aﬀect - ed by the precision with which advice is expressed ( e . g . , numerically versus verbally versus via ambiguous statements ) , such that more precise advice leads to better decisions ( Rantilla , 2000 ) . Other notable ﬁnd - ings involve the role of judge and advisor individual diﬀerences in determining judge post - advice decision accuracy ( LePine , Hollenbeck , Ilgen , & Hedlund , 1997 ) . Results indicate that accuracy is highest when both judges and advisors possess higher levels of intel - ligence ( i . e . , general mental ability ) , and that judges or advisors with high intelligence cannot compensate for the other party’s lower intelligence . Similarly , accuracy is highest with higher levels of judge and advisor con - scientiousness , whereas lower levels of conscientious - ness are detrimental regardless of the other party’s characteristics . Diﬀerences between advisors and ‘‘personal decision - makers’’ Whereas most JAS research has taken the perspective of the judge , fewer studies have chosen to take the per - spective of the advisor ( Jonas & Frey , 2003 ) . That is , most research has studied advice taking rather than advice giving . One important question that has been posed in the latter area , however , is whether people’s recommendations to others diﬀer from the choices they would make for themselves . It appears that they do : advisors tend to recommend options that most judges would prefer , whereas decision - makers choosing for themselves ( ‘‘personal decision - makers’’ ) make choices consistent with their own preferences ( Kray , 2000 ) . Spe - ciﬁcally , advisors favor the choice / alternative that has the best value on the most important attribute ; personal decision - makers , on the other hand , weigh the attributes more evenly ( Kray & Gonzalez , 1999 ) . After having made an initial decision , however , personal decision - makers appear to engage in biased information search ( i . e . , conﬁrmation bias ) , whereas advisors appear to engage in a more balanced information search ( Jonas & Frey , 2003 ) . These diﬀerences do not seem to be due to motiva - tional deﬁciencies in advisors . On the contrary , relative to personal decision - makers , advisors exhibit greater concern about the accuracy of their recommendations ( Jonas & Frey , 2003 ; Kray , 2000 ) and exert more task - related eﬀort ( Kray , 2000 ) . There is also some evidence that decision - makers want their expert advisors to employ decision - making strategies diﬀerent from the ones the decision - makers would use if they were to make the decision independently ( Kahn & Baron , 1995 ) . The aforementioned research on the diﬀerences between advisors and personal decision - makers suggests that studying situations in which people make decisions after viewing the decisions of others who had previously made the decisions for themselves is not equivalent to studying situations in which people make decisions after viewing the recommendations of advisors ( the typical JAS case ) . Still , the ways in which observers diﬀer from actors ( see , e . g . , Harvey , Koehler , & Ayton , 1997 ; Koeh - ler , 1994 ; Koehler & Harvey , 1997 ) are likely to provide insight into the ways in which advisors diﬀer from per - sonal decision - makers . Summary and conclusions An interesting parallel can be drawn between the cen - tral ﬁndings of the advice literature and Lasswell’s ( 1948 ) famous quotation—‘‘who says what to whom with what eﬀect’’ ( p . 37 ) —reminding researchers study - ing attitude change and persuasion of the importance of the source , message , recipient and outcome , respec - tively . Individual diﬀerences relating to the advisor ( s ) and judge ( e . g . , their reputations and their habitual lev - els of accuracy and conﬁdence ) would fall under the ‘‘who’’ and ‘‘to whom’’ rubrics , respectively . ‘‘What’’ pertains to the accuracy and conﬁdence of the advisor’s recommendation vis - a ` - vis the decision task in question . It also refers , however , to the type of advice provided ( discussed in a later section ) . Finally , ‘‘with what eﬀect’’ refers to the extent to which the judge utilizes versus dis - counts the proﬀered advice , the extent to which the judge’s ﬁnal ( post - advice ) decision is accurate , and the extent to which the judge is conﬁdent in his or her ﬁnal decision . 134 S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 Several studies have investigated advice discounting and have attempted to diminish it , presumably with an eye to increasing the accuracy of the judge’s ﬁnal ( post - advice ) decision . In particular , research has shown that the expertise diﬀerential between judges and advis - ors , advice quality , performance - contingent ﬁnancial rewards , trust in advisors , and advisors’ conﬁdence in their recommendations all inﬂuence advice utilization . Future research should assess which of these variables are more important in predicting advice utilization / dis - counting , and which are less important ( e . g . , Azen & Budescu , 2003 ) . Existing research has also examined factors responsi - ble for inﬂuencing judges’ post - advice accuracy and con - ﬁdence . In the case where multiple advisors are available , we see some evidence for why accuracy and conﬁdence do not move in tandem . Decision accuracy is increased when each recommendation , while ( relatively ) accurate , is uncorrelated with the other recommendations ( e . g . , because each advisor examined unique / non - shared infor - mation ) . In contrast , decision conﬁdence is increased when each recommendation is strongly correlated with the other recommendations , even if the recommendations are not accurate . Future research should continue to explore this disconnect . It is worthwhile mentioning that some of the afore - mentioned ﬁndings—for example , the tendency of judg - es to discount advice , and the beneﬁts to judges’ decision accuracy from combining multiple sources of advice— were anticipated by ﬁndings in the literatures on fore - casting and decision aids ( see Clemen , 1989 ; Collopy , Adya , & Armstrong , 2001 ; Harvey , 2001 ; Stewart , 2001 ) . Forecasting studies and studies involving the use of decision aids are not isomorphic with studies of advice utilization . Motives such as sharing responsibility for the decision and avoiding the appearance of rejecting help ( Harvey & Fischer , 1997 ) become salient only in the case of human advisors . 3 However , some JAS studies— e . g . , those involving simulated advisors ( and , conse - quently , no interaction or outcome interdependence between judges and advisors ) —do begin to resemble the forecasting and decision aid studies . More generally , one could question the extent to which experimental design has an impact on research ﬁndings . This issue is addressed further in the subsequent section of the paper . Finally , extant ﬁndings also suggest a proﬁtable ave - nue for future research . JAS research would beneﬁt from recognition of the dependencies between variables and an examination of the judge – advisor ‘‘system’’ as a whole . This would involve a model in which not only the direct eﬀects of the independent variables on judge ﬁnal accuracy and conﬁdence , but also the mediated eﬀects ( via judge initial accuracy and conﬁdence , advisor accuracy and conﬁdence , and advice utilization ) , are examined simultaneously—probably via path analysis . Research conducted under the rubric of hierarchical decision - making teams ( e . g . , Hollenbeck et al . , 1998 ) has adopted such an approach . Among JAS studies , Sni - ezek et al . ( 2004 ) attempted something along these lines ( though they did not discuss conﬁdence ) . Sniezek et al . found that : ( 1 ) the direct eﬀects of their independent variables ( advisor expertise , and ﬁnancial incentive allo - cation timing ) on judge ﬁnal accuracy were generally negligible , and ( 2 ) relationships between independent variables and judge ﬁnal accuracy were mediated to a much greater extent by advisor accuracy and judge ini - tial accuracy than by advice utilization . Experimental design There have been many variations on the basic exper - imental design described previously . To understand their potential eﬀects , we return to the input - process - output model described previously . Here , in the ‘‘input’’ category , we consider : ( 1 ) whether the judge is allowed to form a pre - advice opinion , ( 2 ) whether the judge has a choice about whether to solicit and / or access advice , ( 3 ) the number of advisors from whom the judge receives advice , and ( 4 ) the type of decision task facing the JAS . In the ‘‘process’’ category , we consider the extent of interaction between the judge and his or her advisors and also between the advisors themselves . As others ( e . g . , Van Swol & Sniezek , 2005 ) before us have also noted , variations in experimental design are important because they have the potential to inﬂuence the last category : ‘‘output’’ ( including , but not limited to , advice utilization and the judge’s post - advice accura - cy and conﬁdence ) . In fact , these variations may have implications for which outputs are studied . Although systematic investigations of the eﬀects of experimental design variations are unfortunately few and far between , some preliminary conclusions can be reached and some additional questions can be raised . Absence of a pre - advice judge decision The timing of advice acquisition should be an impor - tant consideration for researchers interested in studying advice taking and giving . The designs of several studies have precluded judges from forming pre - advice opinions ( e . g . , Budescu & Rantilla , 2000 ; Harvey et al . , 2000 ; Sni - ezek & Van Swol , 2001 ) . Most often , these experiments have asked judges to familiarize themselves with the decision task at the same time that they received their advisors’ recommendations . Here , judges do not have a chance to access their internal information regarding 3 Moreover , as discussed later , recommendations ( provided by decision aids or advisors ) are prescriptive whereas information ( used in forecasts ) is descriptive or , at the very least , is seen by the decision - maker as being descriptive . S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 135 the task before they receive the advice ; in other words , the advice ‘‘cues’’ judges toward the option ( s ) recom - mended ( Sniezek & Buckley , 1995 ) . Cueing has been shown to increase decision - makers’ overconﬁdence ( Ronis & Yates , 1987 ; Sniezek & Buckley , 1995 ) , per - haps because decision - makers process uncued alterna - tives more superﬁcially ( Sniezek , Paese , & Swithers , 1990 ; see also Koehler’s , 1994 , research showing that conﬁdence is lower when one’s own hypotheses are gen - erated than when others’ hypotheses are evaluated ) . Asking judges to make pre - advice decisions also allows for the possibility that judges might convey these pre - advice decisions to advisors before advisors provide their recommendations . To our knowledge , only one study that incorporated pre - advice decisions actually allowed for this possibility ( Gibbons et al . , 2003 ) . In that study , advisors were free to oﬀer unsolicited advice and often did so when their opinion conﬂicted with the judge’s initial opinion . It would be interesting to deter - mine whether , and if so how , advising interactions are altered when advisors have access to judges’ pre - advice opinions . For instance , one can ask whether advisors change their recommendations or conﬁdence after accessing their judges’ initial decisions . We believe that the question of whether a pre - advice judge decision should be incorporated in the experimen - tal design is an important one . Certainly , some research questions ( e . g . , investigating how judges aggregate mul - tiple pieces of advice in situations where the judges themselves do not have the capacity to make intelligent decisions ; see Budescu & Rantilla , 2000 ; Budescu et al . , 2003 ) would make the incorporation of judge initial opinions counterproductive . In other circumstances , however , it would be beneﬁcial for researchers to mea - sure judges’ initial opinions . As we discuss later , and as others ( e . g . , Van Swol & Sniezek , 2005 ) have also not - ed , judges’ pre - advice decisions can serve as the baseline for evaluating advice utilization ( or discounting ) . In addition , it is important to know whether the very act of asking judges to provide initial opinions makes them more resistant to advice . If advice utilization is inﬂu - enced by the mere presence of a judge’s pre - advice deci - sion ( as Sniezek & Buckley , 1995 , suggest ) , we should be cautious in comparing results from studies that contain pre - advice decisions with those that do not . Guaranteed versus solicited versus unsolicited advice In the traditional JAS , advice is guaranteed and is imposed on judges . In other words , advisors can decide neither whether to provide advice nor when to provide it . Similarly , judges can decide neither whether to view ( i . e . , access ) advice nor when to view it . Some experi - ments have , however , employed designs in which judges controlled when , if at all , they wanted to solicit advice . It appears that judges tend to seek out more advice from accurate advisors ( Yaniv & Kleinberger , 2000 ; Experi - ment 4 ) , and judges’ uncertainty regarding their initial decision predicts advice seeking ( Cooper , 1991 ; Gibbons et al . , 2003 ) . Also , if given the freedom to solicit advice at any stage during a complex decision problem , most judges opt to conduct a fairly substantial information search on their own ( acquire ‘‘internal information , ’’ in Sniezek & Buckley’s , 1995 , terms ) before obtaining advice ( ‘‘external information’’ ) ( Schrah et al . , 2006 ) . Finally , in one study , judges could decide from which advisor to seek information ( Van Swol & Ludutsky , 2003 ) . Results indicated that judges sought more task - related cues from the advisor who possessed some unique information compared to the advisor who only possessed information redundant with the judge’s . Fur - thermore , unique information was seen as more impor - tant and inﬂuential than shared information . 4 Yet , the evidence regarding judges’ propensity to seek advice is mixed . Some studies have found that judges do not always solicit free advice ( Gibbons et al . , 2003 ) even if they know that the advice is accurate ( Gardner & Ber - ry , 1995 , Experiment 3 ) . Conversely , other studies found that free advice from expert advisors was nearly always solicited ( Gino , 2005 ; Schrah et al . , 2006 ) , and that it was solicited more often than costly advice ( Gino , 2005 ) . The above research suggests that it is insuﬃcient sim - ply to attempt to predict the extent to which judges weigh their own opinion versus one or more advisor rec - ommendations that are automatically imposed as part of the laboratory experiment . Rather , a comprehensive theory of advice utilization should be able to predict the environmental conditions under which , the times at which , and the persons from whom judges will seek out advice , in addition to how heavily judges will weigh that advice ( once received ) . Such a theory should also be able to predict how judges react to unsolicited advice . Here , advice is not automatically provided to judges ; rather , advisors can choose if ( and when ) to provide recommendations . The research , though sparse , indicates rather unambig - uously that unsolicited advice is poorly received . It is discounted to a greater extent than explicitly solicited advice ( Gibbons et al . , 2003 ) . Moreover , whereas explicitly solicited advice is perceived as cooperative and helpful , unsolicited advice is considered to be intrusive ( i . e . , an attempt to ‘‘butt in’’ ) , a form of crit - icism ( Goldsmith , 2000 ; Goldsmith & Fitch , 1997 ) , and 4 As the authors state , these results appear to contradict the ( undiﬀerentiated ) ‘‘small groups’’ literature , which ﬁnds that group members tend to mention , and repeat , common rather than unique information ( Wittenbaum & Stasser , 1996 ) . Future research should therefore directly compare undiﬀerentiated groups and JASs to investigate whether the divergent results are due to diﬀering proce - dures , or , more interestingly , to the structure of the decision - making entity ( the ‘‘undiﬀerentiated’’ group versus the JAS ) . 136 S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 inappropriate ( Deelstra et al . , 2003 ) . Unsolicited help ( e . g . , advice and / or physical / material help ) , especially if oﬀered in a directive ( i . e . , controlling or imposing ) manner , may convey to judges that they are unable to cope with the problem autonomously and may con - sequently threaten their self - esteem ( cf . , Deelstra et al . , 2003 ; Harber , Schneider , Everard , & Fisher , 2005 ; Reinhardt , Boerner , & Horowitz , 2006 ) . Judges evalu - ate people providing such help unsympathetically , espe - cially when judges are not facing a problem for which help is necessary ( Deelstra et al . , 2003 ) . Advisors may sense this hostility on the part of their judges , and may be wary of providing unsolicited advice : less advice is provided unsolicited than is explicitly solicited ( Gibbons et al . , 2003 ) . Number of advisors What seems to be a natural extension of the extant advice - taking research , but yet is presently lacking , is an investigation of how advice taking is aﬀected by vari - ations in the number of advisors . Past research has used one real or computerized advisor ( e . g . , Harvey & Fischer , 1997 ) , two advisors ( e . g . , Yaniv , 1997 , Studies 1 and 2 ) , three advisors ( e . g . , Budescu et al . , 2003 ) , four advisors ( e . g . , Harvey et al . , 2000 ) , six advisors ( e . g . , Budescu & Rantilla , 2000 ; Study 1 ) and a maximum of ten advisors ( e . g . , Rantilla , 2000 ) . In spite of the var - iation in number of advisors across JAS experiments , to our knowledge only two studies varied the number of advisors within the same experiment in an eﬀort to determine how this would aﬀect advice taking ( Budescu & Rantilla , 2000 ; Yaniv & Milyavsky , in press ) . Budescu and Rantilla found that increasing the number of advis - ors made judges more conﬁdent in their ﬁnal decisions , but that , regardless of the number of advisors , the aggre - gation model that ﬁt judges’ estimates best was simply the arithmetic mean . In addition , when judges were ﬁrst asked to provide a pre - advice opinion , Yaniv and Mily - avsky found that , regardless of the number of advisors , judges tended to egocentrically prefer their initial opin - ion in nearly 40 % of decisions . When judges elected to give some weight to advisors’ opinions , they tended to egocentrically discount the one or two opinions furthest from their initial opinion , and average the remaining ones . It is interesting to note that varying the number of advisors can potentially have diﬀerent eﬀects , depending on the nature of the decision - making task . We discuss the characteristics of tasks in the next section . For now , however , we note only that Budescu and Rantilla ( 2000 ) and Yaniv and Milyavsky ( in press ) used tasks in which averaging across advisor recommendations was possible . Other tasks , such as the multiple choice task used by Sniezek and Buckley ( 1995 ) , do not permit taking the mean across recommendations because the alternatives are qualitatively diﬀerent . Consequently , judges are required to ﬁnd alternative ways ( such as , say , the modal recommendation ) of resolving discrepancies . It seems plausible that judges who are required to integrate advice from multiple sources are under greater cognitive pressure than judges who only have to attend to one source of advice . Further , increasing task com - plexity will inﬂuence the decision - maker’s strategies ( Payne et al . , 1993 ) . Thus , future JAS research should investigate how JAS dynamics and decision outcomes are aﬀected by changes in the number of advisors . Final - ly , it would be interesting to examine the shape of the functional relationship between the number of advisors and the dependent variables most often investigated in JAS studies ( e . g . , advice utilization , and judge ﬁnal / post - advice accuracy and conﬁdence ) . For example , is judge ﬁnal accuracy linearly related to the number of advisors , or are there diminishing returns to adding advisors beyond a certain number ? Type of task In his 1984 book on ( undiﬀerentiated ) ‘‘small groups’’ research , McGrath ( pp . 53 – 66 ) lamented the lack of attention paid to the type of tasks used in research stud - ies . He pointed out that research must either assume that all tasks are alike ( an absurdity ) , or else must account for diﬀerences in performance as a function of task diﬀerences . He further noted that the extant research had chosen tasks arbitrarily and / or for reasons of convenience . McGrath’s critique is equally relevant to JAS research . As previously mentioned , the operation of a JAS must be seen within the context of the speciﬁc deci - sion - task used . Some examples of decision tasks are multiple - choice questions ( e . g . , Sniezek & Buckley , 1995 ) or estimations of probabilities of occurrence of events ( e . g . , Budescu & Rantilla , 2000 ) . The eﬀects of the type of decision - task may take ( at least ) four forms . First , the task may directly inﬂuence the judge’s pre - ad - vice opinion , his or her post - advice decision , and / or the advisor’s recommendation . Second , two diﬀerent types of two - way interactions may take place : ( 1 ) the task may moderate the direct eﬀect of the judge’s pre - advice opinion on his or her post - advice decision , and ( 2 ) the task may moderate the direct eﬀect of the advice on the judge’s post - advice decision . Third , a three - way interaction may occur such that the moderating eﬀect of advice on the relationship between the judge’s pre - ad - vice opinion and his or her post - advice decision may itself be moderated by the task . Finally , as alluded to previously , the task may be such that judges are preclud - ed from forming a pre - advice opinion ( e . g . , Budescu & Rantilla , 2000 ) . We therefore think it important to attempt a classiﬁcation of JAS tasks . S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 137 One diﬀerence between tasks pertains to the measure - ment scale for the response expected from JAS mem - bers . That is , certain tasks require JAS members to choose among multiple qualitatively diﬀerent alterna - tives ; other tasks require them to make estimates that could vary in magnitude along a quantitative scale . This diﬀerence parallels the often - made distinction between ‘‘choice’’ and ‘‘judgment’’ ( e . g . , Billings & Scherer , 1988 ; Gigone & Hastie , 1997 ; Hinsz , 1999 ) . Choice tasks in JAS research have often taken the form of multiple - choice questions ( e . g . , Gibbons et al . , 2003 ; Sniezek & Van Swol , 2001 ) . For instance , partici - pants were asked to select which of four answers they believed to be correct in questions such as : ‘‘This ﬁle contains characters that are in machine readable form . A . DOS ﬁle B . hidden ﬁle * C . binary ﬁle D . root directory ﬁle’’ ( Sniezek & Van Swol , 2001 , p . 293 ; asterisk indicates correct answer ) . In JAS judgment tasks , on the other hand , judges are not given a set of qualitatively diﬀerent answers from which they must choose . Rather , judges must make quantitative forecasts or provide quantita - tive estimates based on their knowledge of the subject matter and / or advisors’ recommendations . For instance , JAS studies have used tasks in which judges were asked to estimate sales of a product ( Fischer & Harvey , 1999 ; Harvey et al . , 2000 ) , prices of various products ( Schrah et al . , 2006 ; Sniezek et al . , 2004 ) , the probability that an event will occur ( Budescu & Rantilla , 2000 ; Budescu et al . , 2003 ) , the number of cattle that would die follow - ing the outbreak of a disease ( Harvey & Fischer , 1997 ) , and the dates of certain historical events ( Yaniv , 2004b ; Yaniv & Kleinberger , 2000 ) . It is interesting that many studies did not provide a rationale for selecting one type of task over another ( e . g . , judgment over choice , or vice versa ) . This is consis - tent with Billings and Scherer’s ( 1988 ) statement , in rela - tion to choice versus judgment tasks , that ‘‘many empirical studies seem to assume that the response mode required is an incidental aspect of the task and does not aﬀect the conclusions’’ ( p . 1 ) . Yet , much judgment and decision - making research has indicated that response mode does inﬂuence decision processes ( Gigone & Has - tie , 1993 , 1997 ; Hinsz , 1999 ; Payne et al . , 1993 ) . For instance , Billings and Scherer ( 1988 ) found that choice tasks give rise to more non - compensatory decision strat - egies than do judgment tasks . Another distinction between types of tasks comes from Laughlin ( 1980 ) , who argued ( in the context of undiﬀerentiated small groups ) that tasks diﬀer in terms of whether a ‘‘correct’’ answer exists and , if so , in terms of the ease with which this answer’s correctness can be demonstrated to others . On tasks for which correct answers exist and are very easy to demonstrate ( e . g . , ‘‘eureka’’ tasks ) , the agreement between the advisor’s recommendation and the judge’s ﬁnal decision should be strongly predicted by whether or not the advisor has recommended the ‘‘correct’’ alternative . On the other hand , advisor correctness should not predict agreement as strongly on tasks for which correct answers exist either do not exist or are very diﬃcult to demonstrate . In conclusion , we reiterate our contention that more attention ought to be paid to task properties . Given that the aforementioned task types appear meaningfully dif - ferent , one can reasonably question whether they all elic - it the same strategies . Finally , as we discuss later , the choice of task has a direct bearing on the way advice uti - lization is computed . Amount of interaction between JAS members An important consideration in terms of ‘‘process’’ is the amount of interaction between judges and advisors . The amount of interaction , across JAS studies , varies along a continuum ranging from full interaction to absolutely no interaction between members . Only a few JAS experiments have allowed judges and advisors to interact verbally , either in person ( Savadori et al . , 2001 ; Van Swol & Ludutsky , 2003 ; Van Swol & Sniezek , 2005 ) or via a videoconferencing system ( Gibbons et al . , 2003 ) . A more restrictive interaction is found in Dalal ( 2001 ) and Sniezek and Van Swol ( 2001 , Experiment 2 ) . In both cases , judges and advisors were seated close to each other in the same experimental room , and com - municated only in writing . Even more restrictive are interactions in which judges and advisors are interacting in real - time , but cannot see each other ( Sniezek & Buck - ley , 1995 ; Sniezek et al . , 2004 ; Sniezek & Van Swol , 2001 , Experiment 1 ) . Communication in these studies was in writing , and was transported from one JAS mem - ber to another by an experimenter . Even if members of the JAS were located in diﬀerent rooms ( e . g . , Sniezek et al . , 2004 ) , the procedures were such that judges knew that they were interacting with real participants . In some studies , advisors and judges did not interact at all . Here , advice was collected prior to the judges’ decisions . Judges were ( truthfully ) told that the advice came from real advisors , either subject matter experts ( e . g . , Schrah et al . , 2006 ) , or peers ( e . g . , Yaniv , 2004b ; Yaniv & Kleinberger , 2000 , Experiments 1 – 3 ) . Finally , many JAS studies have used advice generated by the experimenter . Judges were told that the advice came from true advisors , most often subject matter experts ( e . g . , Brehmer & Hagafors , 1986 ; Budescu & Rantilla , 2000 ; Budescu et al . , 2003 ; Harvey & Fischer , 1997 ) ; however , in reality , there were no advisors . Other exper - 138 S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 iments also involved advice that was generated , but it does not appear from the descriptions that judges were explicitly told that the advice came from ‘‘real’’ people ( see , e . g . , Fischer & Harvey , 1999 ; Harvey et al . , 2000 ) . Research on hierarchical decision - making teams , moreover , has shown that the type of allowed interac - tion inﬂuences decision - making accuracy . Hedlund et al . ( 1998 ) found that JASs interacting face - to - face diﬀered from those interacting via a computer . In par - ticular , face - to - face interactions helped advisors give more accurate recommendations and helped JASs gath - er more of the task - relevant available information , whereas computer - mediation helped judges eﬀectively weigh their advisors’ recommendations in terms of their quality . The latter ﬁnding may be explained by the fact that computer mediation reduces judges’ reli - ance on cues extraneous to the accuracy of recommen - dations . However , face - to - face interactions led to overall better performance when the researchers statis - tically controlled for the amount of weight judges gave to advisors’ recommendations . In addition , Colquitt , Hollenbeck , Ilgen , LePine , and Sheppard ( 2002 ) found that JAS - level openness to experience ( i . e . , individual openness to experience aggregated to the JAS level ) moderated the eﬀects of communication type ( strictly verbal versus computer - mediated plus verbal ) on accu - racy , such that computer - assisted communication was most beneﬁcial for teams that were highly open to experience . The advisors in a JAS have typically not been allowed to interact with each other . But what if this were not the case ? Consider , for example , the two variations on the traditional JAS described by Lualhati ( 1992 ) . In the ﬁrst variant , N advisors interacted face - to - face and provided the judge with only one group recommendation . In the second variant , which bears some similarities to the Delphi technique in the nominal groups literature ( Linstone , 1978 ; Rowe & Wright , 1996 ) , advisors had access to each other’s recommendations but did not interact face - to - face : they used other advisors’ recom - mendations to update their own initial recommenda - tions , and then each of them provided the judge with an updated recommendation . The Vroom and Yetton ( 1973 ) model of leadership also allows for various forms of advisor – advisor interaction . Many additional interac - tion patterns could also be constructed . For example , the judge could be advised by advisors in competing coalitions with possibly conﬂicting goals and interaction within but not across coalitions . The above research clearly suggests that the amount and type of interaction present in JAS experiments can - not be overlooked as simply a secondary or incidental aspect of the experimental design . Researchers face a diﬃcult decision when designing JAS experiments , and many resolve it by taking the middle road : they trade - oﬀ some control to allow for a moderate amount of interaction . Unfortunately , limiting every JAS exper - iment to a particular level of interaction precludes us from investigating how social interaction aﬀects the giving and taking of advice or whether it is appropriate to generalize results found at one interaction level to other levels . Thus , an interesting avenue for future research involves varying the interaction between JAS members within the same study . However , rather than assuming that ( seemingly ) qualitatively diﬀerent ‘‘types’’ of interaction will necessarily lead to diﬀerent outcomes , it is preferable to adopt a theoretical approach that isolates the quantitative dimensions along which the various types of interaction may diﬀer . One can then assess the eﬀects of the dimensions rather than the types . The eﬀect of JAS members’ relationships prior to the decision - task should also be explored , especially given that the closeness of the interpersonal relationship between judges and advisors may determine whether the advice is received well ( Feng & MacGeorge , 2006 ; Goldsmith & Fitch , 1997 ) . To date , most JAS experi - ments have employed judges and advisors who were strangers . One exception has been the use of classmates ( Sniezek & Van Swol , 2001 , Experiment 2 ; Van Swol & Ludutsky , 2003 ) . Hollenbeck et al . ( 1995 ) , writing from the perspective of hierarchical decision - making teams , found that advisors in teams composed of familiar mem - bers initially proposed more valid recommendations but were later surpassed by their counterparts in teams com - posed of unfamiliar members under conditions of insta - bility in team - member composition ( manipulated by the experimenters ) . Results such as these underscore the importance of studying the prior familiarity of JAS members . Summary and conclusions As discussed above , many aspects of the experimen - tal design have the potential to inﬂuence decision pro - cesses and outcomes . In particular , the presence or absence of a judge pre - advice decision , whether the advice is imposed on judges , the number of advisors , the type of task employed , and the amount of interac - tion between JAS members all important variables in their own right . However , these variables ( and their eﬀects ) have generally not been studied in a systematic manner . For the moment , therefore , researchers should be aware of the potential problem ( and try to avoid it ) . Once a critical mass of studies pertaining to each of these variables comes into existence , meta - analytic investigations will be in order . Regression - based meta - analytic approaches ( e . g . , Hedges & Olkin , 1985 ) could be used to pit these potential moderator variables against each other . For example , is the rela - tionship between task complexity and advice utilization moderated to a greater extent by the number of advis - S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 139 ors or by the amount of interaction between judge and advisors ? One obstacle to such meta - analytic studies , however , is that the outcomes of interest ( advice utilization , and judge post - advice accuracy and conﬁdence ) have them - selves been measured very diﬀerently across diﬀerent studies . As can be seen in the next section , the amount of advice utilization , computed via diﬀerent methods ( e . g . , formula versus regression ) , cannot necessarily be equated without access to researchers’ raw data . Measure of advice utilization A number of measures of advice utilization have been developed by JAS researchers . Measures of advice utili - zation can be grouped according to whether the decision to be made is a choice or a judgment . Advice utilization on choice tasks As mentioned previously , choices are decisions involving qualitatively diﬀerent alternatives . Thus , deci - sion - makers may have to choose between investing in stocks or bonds or real estate , or between attending or missing work on a snowy day . On choice tasks , advice acceptance is most often operationalized as ‘‘matching , ’’ that is , a consistency between the judge’s ﬁnal ( post - advice ) decision and the advisor’s recom - mendation ( e . g . , Sniezek & Buckley , 1995 ; Sniezek & Van Swol , 2001 ) . Two observations can be raised with respect to advice acceptance measures based on matching . First , matching measures are insensitive to changes between judge’s pre - advice and post - advice choices when the latter do not match the advice . For instance , when three answer choices are available , a judge could select choice ‘‘A’’ prior to receiving a recommendation to select choice ‘‘B , ’’ only to select choice ‘‘C’’ as a ﬁnal answer . Presumably , this outcome is diﬀerent from that of a judge who retains his or her initial position ( ‘‘A , ’’ in the example ) ; yet , measures of matching would treat them both as instances of advice discounting . Second , matching can be examined in terms of ‘‘hold’’ versus ‘‘shift’’ decisions ( Gibbons et al . , 2003 ) . ‘‘Holds’’ and ‘‘shifts’’ both pertain to cases in which the judge’s ﬁnal choice matches the advisor’s recommendation ; they diﬀer , however , in terms of whether the judge’s initial choice also matches the advisor’s recommendation . ‘‘Holds’’ refer to situations where the judge’s initial choice , the advice , and the judge’s ﬁnal choice are all the same alternative . In other words , the judge initially chooses an alternative , receives advice that recommends the same alternative , and does not switch to a diﬀerent alternative . ‘‘Shifts , ’’ in contrast , refer to situations where the judge’s initial choice conﬂicts with the advice , and the judge ultimately decides to switch to the alternative rec - ommended by the advisor . Merely assessing whether the judge’s ﬁnal opinion matches the advice , therefore , does not distinguish between holds and shifts . Furthermore , although it may initially appear as though ‘‘advice taking’’ really only occurs in the case of shifts , the possibility of it occurring during holds should not be dismissed in such a cavalier manner . The advice may have succeeded in convincing judges that their initial choices , quite possibly made under a great deal of uncertainty , were likely to be correct . JAS researchers interested in choice tasks are there - fore encouraged to include initial ( pre - advice ) choices for judges , as long as the research questions permit this . Moreover , it may be the case that advice utilization is best measured by assessing not only changes in choice but also changes in the associated conﬁdence or uncertainty . Advice utilization on judgment tasks As mentioned previously , judgments are decisions involving estimates or forecasts of a quantitative nature . Thus , decision - makers may have to judge how much money to invest in stock , or how much earlier than usual they would need to leave for work on a snowy day in order to still make it in on time . Because judgment tasks produce data that are at least at an ordinal level ( compared to the nominal or categorical level of choice data ) , more options are available in terms of measuring advice taking . Gener - ally , two broad approaches have been taken . The ﬁrst approach measures advice utilization via one of sever - al related formulae . The second approach takes a regression - based approach . We discuss each of these approaches in turn . Formula - based approaches Advice taking in judgment tasks has been measured via several diﬀerent formulae . Each formula has advan - tages and disadvantages , and certain formulae cannot be applied in cases where judges were not required to pro - vide a pre - advice estimate . However , each formula essentially weighs the extent to which the judge’s ﬁnal decision is a function of his or her own initial estimate versus the advisor’s recommendation . In addition , as explained below in greater detail , the results obtained from the formulae are most meaningful when the judge’s ﬁnal estimate falls between his or her initial estimate and the advice . Harvey and Fischer ( 1997 ; see also Sniezek et al . , 2004 ) deﬁned advice taking as the ratio of two diﬀerenc - es : that between the judge’s post - advice and pre - advice estimates , and that between the advisor’s recommenda - tion and the judge’s pre - advice estimate . Expressed mathematically : 140 S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 Advice taking ¼ judge final estimate (cid:2) judge initial estimate advisor recommendation (cid:2) judge initial estimate : Yaniv ( 2004 ; see also Gino , 2005 ) subsequently gener - ated a similar measure of advice weighting ( weight of advice ; WOA ) . Mathematically , this formula is expressed as : WOA ¼ j judge final estimate (cid:2) judge initial estimate j j advisor recommendation (cid:2) judge initial estimate j : Apart from the use of absolute values , WOA is equiv - alent to Harvey and Fischer’s ( 1997 ) measure . Another advice - taking measure encountered in the JAS literature is that used by Yaniv and Kleinberger ( 2000 ) . Here , advice taking is measured as the ratio of two absolute diﬀerences : that between the advisor’s rec - ommendation and the judge’s ﬁnal estimate , and that between the recommendation and the judge’s initial esti - mate . Mathematically , the weight of own estimate ( WOE ) is expressed as : WOE ¼ j advisor recommendation (cid:2) judge final estimate j j advisor recommendation (cid:2) judge initial estimate j : Several observations can be made with regard to these formulae . First , all three yield undeﬁned values when the advice is equivalent to the judge’s initial esti - mate ( because the denominator is zero in such cases ) . As mentioned in the section on advice utilization on choice tasks , it is not necessarily true that such situa - tions represent instances of no advice taking . Though the formulae do not indicate that advice taking is zero in such situations , they are nonetheless unable to quan - tify it . Second , all three formulae do yield a value of zero under certain circumstances . Harvey and Fischer’s ( 1997 ) formula and the WOA yield a value of zero ( interpreted as no advice taking or a zero weight on advice ) when the judge’s ﬁnal estimate is identical to his or her initial estimate . Yet , if , in such a situation , the denominator ( i . e . , the diﬀerence between the advi - sor’s recommendation and the judge’s initial estimate ) were some very small ( albeit non - zero ) value , the advice might have served to conﬁrm to the judge that the initial opinion was approximately correct and should therefore be maintained . It may also have been the case that though the judge did not respond to the advice by changing his or her judgment , he or she did respond by altering his or her conﬁdence ( presumably by becoming more conﬁdent in response to a small gap between the recommendation and his or her own initial estimate , and by becoming less conﬁdent in response to a larger gap ) . The assumption of no advice taking under such circumstances may , thus , be unwarranted . The WOE , on the other hand , yields a value of zero ( interpreted as a zero weight on own estimate ) when the judge’s ﬁnal estimate is equivalent to the advice . Similar concerns may be raised about this assumption . In the instance of a small ( albeit non - zero ) diﬀerence between the judge’s initial estimate and the advice , the judge may have felt comfortable adopting the advice precisely because the diﬀerence was small . Also , here too the judge’s conﬁdence may change from initial to ﬁnal estimate . Third , given a particular judge initial estimate and advisor recommendation , the WOE does not distin - guish situations wherein the judge’s ﬁnal estimate approaches the advice from situations wherein it over - shoots the advice ; in contrast , the WOA does not dis - tinguish situations wherein the judge’s ﬁnal estimate approaches the advice from situations wherein it moves away from the advice . As an example , consider a judgment for which the judge’s initial estimate is 80 and the advisor’s recommendation is 100 . Here , WOE = 0 . 25 both when the judge’s ﬁnal estimate is 95 ( i . e . , when it approaches the advice ) and when it is 105 ( i . e . , when it overshoots the advice ) ; WOA = 0 . 75 both when the judge’s ﬁnal estimate is 95 ( i . e . , when it approaches the advice ) and when it is 65 ( i . e . , when it moves away from the advice ) . Thus , these formulae suﬀer from the potential prob - lems of undeﬁned or ambiguous values . Still , the observed data largely appear to cooperate with the for - mulae : in the overwhelming majority ( perhaps up to 95 % ) of observed decisions , the judge’s post - advice esti - mate appears to fall within the range of the pre - advice estimate and the advice . It is good practice for research - ers using these formulae to peruse their data to deter - mine what proportion of the decisions comprises either an ‘‘out of range’’ value ( i . e . , an instance in which the judge’s ﬁnal estimate overshot or moved away from the advisor’s recommendation ) or an undeﬁned value . Researchers should also analyze their data both with and without these problematic values in order to deter - mine whether the conclusions reached are identical in both cases . Fourth , WOE and WOA both have lower bounds of 0 . 00 ( because their numerators are absolute values ) but do not have upper bounds . Values greater than 1 . 00 occur when the judge’s ﬁnal estimate overshoots the advice ( WOA ) or moves away from it ( WOE ) . Harvey and Fischer’s ( 1997 ) formula , in contrast , is unbounded on either side . It takes on negative values when the judge’s ﬁnal estimate moves away from the advice , and values greater than 1 . 00 when the judge’s ﬁnal estimate overshoots the advice . However , Harvey and Fischer noted that the vast majority of actual shifts did fall between 0 . 00 and 1 . 00 . Fifth , the formulae treat shifts that are similar in their relative change as equivalent . For example , according to both Harvey and Fischer’s ( 1997 ) measure and WOA , a judge who shifts from 80 to 85 after receiving a recom - S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 141 mendation of 100 has shifted by the same proportion ( i . e . , 0 . 25 ) as another judge who shifts from 60 to 70 after receiving the same recommendation . Yet , research has shown that the distance between pre - advice esti - mates and advice has an inﬂuence on a judge’s propen - sity to accept advice ( Yaniv , 2004b ) . Thus , it is reasonable to question whether the two judges in the above example have been equally inﬂuenced by the advice . Finally , it should be noted that , as ratios of diﬀerenc - es , all three advice - utilization measures discussed above ( WOE , WOA , and Harvey & Fischer’s , 1997 , index ) are susceptible to the numerous ( and serious ) measurement problems associated with both diﬀerence scores ( Cronbach & Furby , 1970 ; Edwards , 1995 ) and ratios ( Cronbach , 1943 ; Firebaugh & Gibbs , 1985 ) . 5 Regression - based approaches An altogether diﬀerent way of measuring advice uti - lization is based on Brunswik’s ( e . g . , 1955 , 1956 ; see also Slovic and Lichtenstein , 1971 ) ‘‘lens model . ’’ Advice utilization is assessed by regressing the judge’s ﬁnal decision on all sources of advice simultaneously ( e . g . , Harvey et al . , 2000 ; Hedlund et al . , 1998 ; Hollen - beck et al . , 1995 ; Phillips , 1999 ; see also Brehmer & Hagafors , 1986 ) . If the judge were allowed to express a pre - advice opinion , this too could be included in the set of predictors ( e . g . , Lim & O’Connor , 1995 ) . If each judge faced several decisions , it is possible to com - pute utilization indices within , rather than between , judges ( for a discussion of utilization indices , see Cook - sey , 1996 ; Azen & Budescu , 2003 ) . One can use various methods of assessing the importance of each source of advice ( and the pre - advice opinion , if existent ) : correla - tion or regression coeﬃcients for each source of advice ( e . g . , Brehmer & Hagafors , 1986 ; Phillips , 1999 ) , or increases in the percentage of criterion variance explained ( i . e . , D R 2 ) when the focal source is added to a model containing the other sources ( e . g . , Harvey et al . , 2000 ; see also Cooksey , 1996 ; Azen & Budescu , 2003 ; Budescu & Azen , 2004 ) . Consider the application of a regression - based proce - dure like the ones described above to the standard ‘‘one Judge , one Advisor’’ setup , which has typiﬁed most JAS research and for which WOE , WOA , and Harvey and Fischer’s ( 1997 ) measure were created . Regression - based procedures , unlike these measures , would avoid the use of diﬀerence scores and ratios . In addition , their use is consistent with the suggestion of Edwards ( 1995 ) and Cronbach and Furby ( 1970 ) : if one component of a dif - ference score is endogenous , this component should be used as the criterion in an analysis that controls for the other ( exogenous ) component of the diﬀerence score . ( With regard to the JAS , we would typically assume that both the judge’s initial estimate and the advisor’s recom - mendation are exogenous , whereas the judge’s ﬁnal esti - mate is endogenous . ) It would also be possible to further extend this procedure by examining the extent to which the interaction between the judge’s initial estimate and the advisor’s recommendation explains incremental var - iance ( i . e . , beyond the main eﬀects of the variables them - selves ) in the judge’s ﬁnal estimate . Regression - based approaches therefore possess sever - al important advantages over the aforementioned for - mulae . One disadvantage of the former , however , is that advice utilization for each judge can only be mea - sured when he or she makes a series of decisions . In con - trast , the formulae can be used when each judge makes only one decision ; alternatively , if several decisions are made , advice utilization can be assessed for each decision within each judge ( and hence , if desired , intra - judge changes in advice utilization can be assessed ) . Regres - sion - based approaches may also be problematic because the advisors may not be independent ( and , consequent - ly , their advice may be correlated ) under a number of circumstances . For example , advisors will not be inde - pendent if they have access to the same information , or can discuss information prior to giving individual advice . Independence among advisors can often not be assumed ; in instances where advisors’ estimates are extremely highly correlated , multicollinearity may create problems if regression coeﬃcients are used to estimate the weight the judge gives to each advisor and his or her own pre - advice opinion ( if available ) . In these cases , it is recommended that researchers instead employ utili - zation indices to reﬂect the relative weight given to each source ( see Azen & Budescu , 2003 ; Cooksey , 1996 ) . Though several utilization indices exist , two in partic - ular bear mentioning . The ﬁrst , the ‘‘usefulness index’’ ( Darlington , 1968 ) , is the percentage increase in criteri - on variance explained ( i . e . , D R 2 ) when the focal predic - tor is added to a model containing all the other predictors . ( Here too , the criterion is advice utilization and the predictors are the estimates of each advisor and , if available , the judge’s own pre - advice estimate . ) The usefulness index is a special case of the second index , the dominance weight ( Azen & Budescu , 2003 ; Budescu & Azen , 2004 ) . The dominance weight ( or , more speciﬁcally , the general dominance weight ) is the average percentage increase in criterion variance explained ( i . e . , mean D R 2 ) when the focal predictor is added to models containing all possible subsets of the other predictors . The dominance weight therefore encompasses a predictor’s direct eﬀect ( i . e . , considered by itself ) , total eﬀect ( i . e . , conditional on all other pre - dictors—equivalent to the usefulness index ) , and partial eﬀect ( i . e . , conditional on subsets of predictors ) . From 5 Here , we are discussing advice utilization as a dependent variable . It can also be used , however , as an independent variable—for example , when predicting the accuracy of the judge’s ﬁnal decision . In such cases , too , these measures are problematic ( cf . Edwards , 2002 ) . 142 S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 both a conceptual and empirical standpoint , the domi - nance weight is the ‘‘gold standard’’ for predictor impor - tance indices ( LeBreton , Ployheart , & Ladd , 2004 ) . Another disadvantage of all the aforementioned regression - based approaches is that they cannot be used when the focus is on testing the ﬁt of various mathemat - ical models of advice utilization to actual data ( e . g . , Budescu & Rantilla , 2000 ; Budescu et al . , 2003 ) . In par - ticular , the ﬁt of diﬀerent advice - aggregation models on the part of the judge ( e . g . , median , unweighted mean , preference for one advisor only ) cannot be compared in this way . Results of studies that do adopt an explicit model - testing approach indicate that judges’ aggrega - tion policies approximate the unweighted mean of the advisors’ estimates ( Budescu & Rantilla , 2000 ) , or the weighted mean when judges can take additional infor - mation ( e . g . , the advisors’ accuracy rates and the num - ber of cues seen by each advisor ) into account ( Budescu et al . , 2003 ; see also Fischer & Harvey , 1999 ) . Summary and conclusions The assessment of advice utilization diﬀers somewhat , based on whether the decision task involves choice or judgment . In either case , no method is without drawbacks . Nonetheless , the inclusion of judge initial ( pre - advice ) choices or judgments is desirable unless pro - scribed by the nature of the research questions . Moreover , in the case of judgment tasks , regression - based approach - es seem preferable under most circumstances . Diverse as they are , the advice - utilization methods reviewed above nonetheless share an important com - monality : a particular deﬁnition of the term ‘‘advice . ’’ The next section questions this deﬁnition and therefore , indirectly , the deﬁnition of ‘‘advice utilization . ’’ What is advice ? In the English language , ‘‘advice’’ is deﬁned as a ‘‘rec - ommendation regarding a decision or course of conduct : counsel’’ ( Merriam - Webster’s collegiate dictionary ) . In the extant JAS research , the best explication of the role of the advisor is perhaps the one given by Sniezek and Buckley ( 1995 ) . According to them , advisors ‘‘formulate judgments or recommend alternatives and communicate these to the person in the role of the judge’’ ( p . 159 ) . Most studies , however , deﬁne advice not at the construct level , but at the operational level . The deﬁnition of advice has been determined by the experimental design , although , optimally , the converse would hold true . In other words , research on advice giving and taking has itself paid insuﬃcient attention to deﬁning the term ‘‘advice . ’’ It would be preferable to base the deﬁnition of ‘‘advice’’ on theory and empirical results , rather than simply adopting dictionary deﬁnitions of advice . A perusal of the advice literature , and related literatures , leads us to conclude that the primary operationalization of advice as a speciﬁc recommendation ( e . g . , ‘‘Choose X’’ ) is overly narrow . Speciﬁc recommendations may not always match judges’ goals for the advising exchange ( Gibbons , 2003 ; Horowitz et al . , 2001 ) , and are perhaps among the less preferred forms of advice by judges ( Gibbons , 2003 ) . Heath and Gonzalez ( 1995 ) found some evidence that advice occupies a much greater construct space than its conceptualization to date . They indicated that input from others was sought because it could help decision - makers make better decisions and avoid mistakes , help them think about new information , help them organize their thoughts , and help them become more conﬁdent in their decisions . Furthermore , advice could include the provision of social support needed for the decision . Cross , Borgatti , and Parker ( 2001 ) , too , issued a pow - erful appeal for a broader conceptualization of advice . According to these authors , ‘‘there has been little inves - tigation into what really ﬂows when organizational members go to each other for work - related advice , although it is often assumed that more than simple answers passes between the parties’’ ( p . 216 ) . These authors identiﬁed ﬁve types of advice : solutions , meta - knowledge , problem reformulation , validation , and legitimization . Based on multi - dimensional scaling , Cross et al . concluded that these types of advice were ordered along a single dimension in the aforementioned order , and that an advisor who provided one type of advice was also likely to provide all the ‘‘lower’’ types of advice . They speculated that the ordering might reﬂect the ‘‘semantic distance from the prototypical kind of response that we ordinarily expect when we ask oth - ers for information’’ ( p . 220 ) such that advisors might naturally provide solutions ﬁrst , and then the other , less concrete , types of advice . Gibbons ( 2003 ) also argued , and found supporting evidence , for expanding the con - struct space of advice . She proposed extending the deﬁ - nition of advice to include elements such as the provision of emotional support , the endorsement of the judge’s initially chosen alternative , the provision of information or reasoning regarding the decision , the suggestion of a new alternative not initially considered by the judge , the provision of assistance for the judge to gain greater self - insight , and / or the provision of assis - tance toward the decision process . Others ( Goldsmith & Fitch , 1997 ; Horowitz et al . , 2001 ; Schlosser & Gelso , 2001 ; Whittemore , Rankin , Callahan , Leder , & Carroll , 2000 ) have also conceptualized the advisor’s role as a provider of socio - emotional support in addition to task - related developmental recommendations , prob - lem - solving assistance or recommendations of speciﬁc courses of action . In addition to these types of advice , behavior such as recommending against one or more alternatives could also be considered advice . S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 143 Extant JAS research has also included some types of behavior that could be interpreted as forms of advice . For instance , advisors have been asked to provide esti - mates of how conﬁdent they are in their recommenda - tions ( Savadori et al . , 2001 ; Sniezek & Buckley , 1995 ; Sniezek & Van Swol , 2001 ) , or to provide conﬁdence intervals around their point estimates ( Yaniv , 2004b ; Yaniv & Kleinberger , 2000 ) . Similarly , advisors have been asked to provide written elaborations or justiﬁca - tions for their recommendations ( Sniezek & Van Swol , 2001 ; Van Swol & Sniezek , 2005 ; see also Jungermann , 1999 ) and also any additional information that the judg - es may speciﬁcally request ( Van Swol & Sniezek , 2005 ) . Having a clear understanding of the diverse types of behavior that are considered part of the advice construct will improve JAS research in a number of ways . Broader deﬁnitions of advice will require researchers to formu - late new measures of advice utilization / discounting and may even lead to new insights in terms of the central ﬁndings of the JAS literature . For instance , advice dis - counting could be less likely for emotional support , which Gibbons ( 2003 ) found to be one of the preferred forms of advice . Considering alternative forms of advice could also help uncover individual diﬀerences in prefer - ences for giving and / or receiving speciﬁc types of advice . Moreover , the extent to which a judge utilizes a speciﬁc recommendation may depend on the temporal order of the recommendation relative to other aspects of the advice interaction ( Goldsmith , 2000 ) —e . g . , a speciﬁc recommendation may be more palatable to the judge if it follows emotional support . Thus , future research should seek to further investigate the dimensionality of advice . In particular , we believe that taking a latent fac - tor approach to the study of the advice construct may provide this much needed clariﬁcation . Advice may best be conceptualized as a higher - order model , where the general ‘‘advice’’ factor subsumes a number of lower - or - der ( i . e . , narrower ) advice facets . These lower - order fac - ets could include , among others , the provision for a speciﬁc recommendation , the provision against a speciﬁc recommendation , and the provision of guidance on how to make a decision . We believe that the focused study of the entire construct space of ‘‘advice’’ is one of the most proﬁtable ( and needed ) avenues for future research . Finally , in considering what advice is and what it is not , it is important to delineate advice from information about the task obtained by other sources ( direct obser - vation , a textbook , a newspaper , an internet website , etc . ) . To the extent that there is similarity , as Harvey and Bolger ( 2001 ) noted , research on advice - taking should exploit what is known from research on ( task ) information acquisition . Schrah et al . ( 2006 ) attempted to contrast advice—deﬁned by them as a speciﬁc recom - mendation from an advisor—and task information on choice tasks . According to these authors , there are two main diﬀerences . First , there is a diﬀerence in granular - ity . Acquiring information concerning one attribute on one alternative , in itself , provides little clue as to the cor - rect alternative . That is , information is merely descrip - tive in nature . In contrast , advice constitutes a prescriptive or evaluative summary of the task informa - tion . We would caution that this distinction becomes much fuzzier when advice is construed more broadly and includes the provision of task information . Second , according to Schrah et al . , there is a diﬀerence in per - ceived trustworthiness . Advice is often greeted with uncertainty and mistrust due to questions about the advisor’s expertise and intentions . In contrast , task information is ( rightly or wrongly ) viewed as being fac - tually correct—perhaps because the judge does not asso - ciate another person quite as closely with task information as he or she does with advice . Judge – Advisor Systems and the organizational sciences We believe that the JAS research has great potential to inform , and be informed by , other areas of psycholo - gy . Thus , this section follows Naylor’s ( 1984 ) original and Highhouse’s ( 2001 ) renewed call for further integra - tion and ‘‘cross - fertilization’’ ( Highhouse , 2001 , p . 314 ) between judgment and decision - making research and the organizational sciences . To quote Naylor’s original words , both disciplines ‘‘have much to say to each other’’ ( p . 2 ) . Still , as Highhouse laments , this cross - fer - tilization has not yet occurred to the extent that it could . The ﬁrst few sections below consider areas of the organizational sciences that could beneﬁt from the application of ﬁndings from the JAS literature . The overarching theme here is that diﬃcult decisions are not made in a social vacuum . Employment decisions Whereas employment decisions have traditionally been studied from the organization’s perspective , researchers in the organizational sciences have increas - ingly begun to focus on the job candidate’s perspective ( Anderson , Born , & Cunningham - Snell , 2001 ) . A stream of research for which the advice literature is particularly relevant is that of job choice . Applicants’ employment decision - making has traditionally been studied in terms of content ( i . e . , which job attributes and other organiza - tion information cues interact with the job context and candidates’ individual diﬀerences to inﬂuence attraction to an organization and intent to apply ) and process ( i . e . , how job attributes are combined in order to arrive at a ﬁnal decision ; see Highhouse & Hoﬀman , 2001 , for a review ) . Important economic variables such as the labor market have been incorporated in some employment decision - making models ( e . g . , Anderson et al . , 2001 ) . Yet , much of the job choice research has looked at the 144 S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 job applicant as if he or she made decisions without the help of others . For instance , Slaugher and Highhouse ( 2003 ) argue that ‘‘choices among job alternatives are almost never made in isolation . Individuals choosing among jobs are likely to consult those with whom they have social contact , such as friends , and those individuals for whom the deci - sion will have indirect yet important consequences , such as family members’’ ( p . 12 ) . Thus , the advice - taking and advice - giving literature could be informative in the investigation of the social context of job choices . Important questions pertain to the individual diﬀerences that lead job seekers to ask for advice , how often job seekers ask for advice , who they consult for advice , at what stage in the job search / choice process they seek advice , and what type of advice they seek . For example , less conﬁdent job seek - ers might solicit advice to a greater extent than more conﬁdent job seekers ( cf . , Cooper , 1991 ) , or , relatedly , job seekers who ﬁnd the job choice task to be highly dif - ﬁcult may seek out more advice ( cf . , Schrah et al . , 2006 ) . Furthermore , less experienced job seekers might dis - count advice less than their more experienced counter - parts ( cf . , Harvey & Fischer , 1997 ) . Another important employment decision that may involve the use of advice is the decision to voluntarily quit from one’s current job ( see , e . g . , Griﬀeth & Hom , 1995 ; Lee & Mitchell , 1994 ) . Except when employees quit impulsively , they would presumably ﬁrst discuss the issue with spouses and perhaps other individuals whose opinion is valued and / or who may be aﬀected by the decision . Receiving advice might provide employ - ees with previously unconsidered aspects of the decision ( cf . , Heath & Gonzalez , 1995 ) . For example , talking to his or her spouse might lead an employee to realize that quitting would be very diﬃcult given the ﬁnancial costs associated with leaving the organization ( i . e . , continu - ance commitment ; Meyer , Stanley , Herscovitch , & Topolnytsky , 2002 ) , whereas advice from a coworker might make the employee realize that he or she ‘‘owes’’ it to the company to remain there because the company paid for his or her higher education ( i . e . , normative commitment ; Meyer et al . , 2002 ) . Thus , advice from oth - ers might lead an employee who wishes to quit to none - theless refrain from doing so ( the converse can also happen , of course ) . Mentoring The advice literature could also be relevant for men - tor - prote´ge´ interactions ( see Noe , Greenberger , & Wang , 2002 , for a review ) . In essence , mentors serve as the prote´ge´s’ advisors and provide two types of sup - port to prote´ge´s : psychological and career - related ( Kram , 1983 ; Noe et al . , 2002 ) . Some behavior posited to fall within these two categories certainly has an advi - sory nature and reﬂects the forms of advice discussed in a previous section of this paper . For instance , many forms of psychological mentoring approximate the social / emotional support aspect of advising . Similarly , some forms of career - related mentoring include behav - ior such as ‘‘sharing ideas , providing feedback , and sug - gesting strategies for accomplishing work objectives’’ ( Noe , 1988 , pp . 459 ) , which resemble advice in which a new course of action or a particular alternative are recommended . Given the expertise ( or at least experience ) diﬀerential inherent to traditional mentoring relationships , one would argue that prote´ge´s should not be overly inclined to egocentrically discount advice . However , mentors are often not prote´ge´s’ formal supervisors ( e . g . , Fagenson - Eland , Marks , & Amendola , 1997 ) , which implies that their recommendations may sometimes conﬂict with those of the supervisors . Consequently , the prote´ge´ has to decide whether to completely discount one of the options or , if not , how to weigh each of them relative to each other and , perhaps , relative to the prote´ge´’s own initial opinion . Advice might be preferred when it is proﬀered by the conﬁdent advisor ( cf . , Sniezek & Buckley , 1995 ) or the advisor who has most often been correct in the past ( cf . , Fischer & Harvey , 1999 ) . Alter - natively , the advice that was closest to the prote´ge´’s ini - tial opinion ( cf . , Yaniv , 2004a , 2004b ) might be preferred . Socialization Organizational socialization is the process by which employees learn the knowledge , attitudes and behavior needed to become an organizational member ( see Bauer , Morrison , & Callister , 1998 , for a review ; see also Van Maanen & Schein , 1979 ) . The information that new - comers acquire throughout this process falls into a num - ber of socialization domains ( see Chao , O’Leary - Kelly , Wolf , Klein , & Gardner , 1994 ; Cooper - Thomas & Anderson , 2002 ; Morrison , 1993 , 2002 for models of newcomer socialization ) . Though there is no overall consensus , models include ( among other domains ) : learning how to perform one’s job ( i . e . , task mastery ) , clarifying one’s role in the organization , becoming part of the workgroup ( i . e . , social integration ) , learning and adjusting to the organizational culture , learning the jar - gon used in the organization , and learning the organiza - tional politics . Many factors can aﬀect the socialization process and its outcomes , one of which is the newcom - er’s interaction with existing organizational members , or socialization agents . These agents can aid the new - comer’s learning process by providing guidance , instruc - tions , recommendations and social support ( Louis , Posner , & Powell , 1983 ) . Many of the interactions between the newcomer and his or her socialization S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 145 agents could thus fall into one or more broad types of advice that we identify above . Furthermore , newcomers engage in proactive socialization , and actively seek out information in order to better transition to their new environments ( e . g . , Griﬃn , Colella , & Goparauju , 2000 ) ; this is akin to judges deciding from whom , and at what time ( s ) , to seek advice . Bauer et al . ( 1998 ) indicate that there are still many unanswered questions in the socialization literature . The JAS literature could be particularly informative for one of these : determining how socialization out - comes are inﬂuenced when newcomers receive conﬂict - ing information from various socialization agents . For instance , newcomers might receive many ‘‘insider’’ tips on organizational politics ( e . g . , whom to trust , whom to ‘‘smooth - talk’’ ) or organizational culture ( e . g . , the power distance between employees and their supervi - sors ) . Here again , advice given by the more conﬁdent socialization agent ( cf . , Sniezek & Buckley , 1995 ) , or the socialization agent the newcomer trusts more ( cf . , Sniezek & Van Swol , 2001 ) should be attended to most . Similarly , JAS research could provide answers in terms of which type of newcomer is most likely to accept advice from his or her socialization agents . For instance , newcomers with more previous work experience might be less likely to accept advice about new or diﬀerent ways in which to perform their tasks than might novice newcomers ( cf . , Harvey & Fischer , 1997 ) . Thus , by being less resistant to advice , the latter group might socialize more quickly to their new organization . The above sections illustrate how the organizational sciences could beneﬁt from the literature on advice giv - ing and taking . However , the latter could also beneﬁt from the former . Two ways in which this could happen are via the investigation of topics that have been tradi - tionally studied in small groups research , as well as through the incorporation of more individual diﬀerence research in the advice literature . Small groups research The research literature on small groups ( e . g . , Kerr & Tindale , 2004 ) is a source of possible future research ideas for JAS scholars despite the fact that , as we have previously argued , the JAS is diﬀerent from the undiﬀer - entiated groups traditionally studied in that literature . For example , the literature on social loaﬁng ( i . e . , the reduction in eﬀort and motivation when individuals’ work is pooled compared to when it is not ; see Karau & Williams , 1993 , for a review ) could be informative in that it would indicate the conditions under which advisors would be likely to exert more versus less eﬀort in generating accurate advice . Based on this literature , we might expect advisors to exert less eﬀort when their performance is unidentiﬁable . Another interesting ave - nue for future research involves group mental models or shared cognitions ( see Mathieu , Heﬀner , Goodwin , Salas , & Cannon - Bowers , 2000 ; Salas & Cannon - Bow - ers , 2001 ) . When JASs interact across a long period of time ( e . g . , Hollenbeck et al . , 1995 ) , it is likely that shared cognitions emerge such that judges and advisors learn ( among other things ) which advisor holds which type of knowledge , which advisor is more accurate in which situation , and whether advisors’ conﬁdence ratings are valid cues for advice accuracy . One can argue that judg - es who develop a more accurate cognitive representation of their JAS will be better equipped to make use of their advisors’ recommendations , and , in turn , to have more accurate post - advice choices or judgments . Finally , Kerr and Tindale ( 2004 ) have observed that the egocentric advice - discounting eﬀect is similar to the group deci - sion - making ﬁnding that group members’ post - group discussion opinions tend to converge back toward indi - viduals’ pre - discussion opinions . Some JAS scholars have already started to make use of this literature . Harries et al . ( 2004 ) employed research on the eﬀects of deviant opinions in ( undiﬀerentiated ) small groups to shed light on the eﬀects of outlying advice ( i . e . , advice that is far removed from the other advisors’ recommendations ) . These authors drew on theories and empirical evidence from small groups research to interpret their ﬁnding that judges tend to dis - count outlying advice . Thus , although undiﬀerentiated small groups are not isomorphic with JASs , the former literature does serve as fertile ground for the latter . Equally , however , JAS research can inform small groups research ( see , e . g . , Luan , Sorkin , & Itzkowitz , 2004 ) . Individual diﬀerences To date , most research has studied situational factors helping or hindering the exchange of advice , leaving a class of variables relatively unexplored . The few studies that have included individual diﬀerences have found promising results ( e . g . , LePine et al . ’s , 1997 , ﬁndings concerning how decision accuracy is inﬂuenced by intel - ligence and conscientiousness ; Harvey & Fischer’s , 1997 , ﬁndings concerning task - related experience and advice discounting ; and Koestner et al . ’s , 1999 , ﬁndings con - cerning individual diﬀerences in autonomy and advice taking ) . Still , there are many more ways in which indi - vidual diﬀerences can inﬂuence the giving and taking of advice , and the organizational sciences abound with information on how individual diﬀerences inﬂuence behavior . Some of the ‘‘Big Five’’ factors of personality ( e . g . , Mount & Barrick , 1995 ) could be especially infor - mative to study in the context of advice discounting . For example , judges high in conscientiousness may be less likely to discount advice , especially if the recommenda - tion comes from an expert source . Furthermore , it is possible that advisor conscientiousness interacts with 146 S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 advisor conﬁdence in determining advice discounting : the relationship between advisor conﬁdence and advice discounting may be strengthened when the advisor is conscientious and attenuated when he or she is not . Sim - ilarly , highly agreeable judges might be less likely than less agreeable ones to reject freely oﬀered advice . In addition , some of the features of the ‘‘need for closure’’ ( Webster & Kruglanski , 1994 ) —e . g . , wanting to make quick decisions , and disliking having to deal with incon - sistent opinions or evidence—would appear to make this construct relevant to studies of advice discounting . Thus , individuals characterized by a high need for clo - sure may be less likely , compared to those characterized by a low need for closure , to take advice . Other variables worth investigating are individual diﬀerences in terms of preferences for giving or taking particular types of advice . For example , some individuals may appreciate advice on how to make a decision , whereas others may appreciate a recommendation on what to decide . Valid questionnaire measures of individuals’ preferences for giving and receiving particular types of advice have not yet been constructed ; this represents an important avenue for future research . Finally , individuals with frontal lobe brain lesions are likely to experience inter - ference with proper assessment and weighting of advice ( Gomez - Beldarrian , Harries , Garcia - Monco , Ballus , & Grafman , 2004 ) , and may therefore be especially likely to discount advice . More generally , research on the neu - ral bases for advice utilization represents a new and exciting frontier for advice research . Conclusions We conclude this review of the advice literature by reiterating our enthusiasm for the potential for future research it oﬀers . We heartily concur with Payne et al . ’s ( 1993 ) statement that ‘‘the social context of deci - sions has been a neglected part of decision research and . . . is an area worthy of much greater study’’ ( p . 255 ) . Research on the giving and taking of advice has begun to address this lacuna . It is our hope that , by con - solidating the literature and suggesting avenues for future inquiry , this review will provide a further impetus for such research . References Anderson , N . , Born , M . , & Cunningham - Snell , N . ( 2001 ) . Recruit - ment and selection : applicant perspectives and outcomes . In N . Anderson , D . S . Ones , H . K . Sinangil , & C . Visvesvaran ( Eds . ) . Handbook of industrial , work , and organizational psychology ( Vol . 1 , pp . 200 – 218 ) . London , UK : Sage Publications . Arkes , H . R . , & Blumer , C . ( 1985 ) . The psychology of sunk cost . Organizational Behavior and Human Decision Processes , 35 , 124 – 140 . Axelrod , R . ( 1984 ) . The evolution of cooperation . New York : Basic Books . Azen , R . , & Budescu , D . V . ( 2003 ) . The dominance analysis approach for comparing predictors in multiple regression . Psychological Methods , 8 , 129 – 148 . Bass , B . M . ( 1954 ) . The leaderless group discussion . Psychological Bulletin , 51 , 616 – 623 . Bauer , T . N . , Morrison , E . W . , & Callister , R . R . ( 1998 ) . Organiza - tional socialization : a review and directions for future research . In G . R . Ferris ( Ed . ) . Research in personnel and human resources management ( Vol . 16 , pp . 149 – 214 ) . Stamford , CT : JAI Press . Billings , R . S . , & Scherer , L . L . ( 1988 ) . The eﬀects of response mode and importance on decision - making strategies : judgment versus choice . Organizational Behavior and Human Decision Processes , 41 , 1 – 19 . Birnbaum , M . H . , & Stegner , S . E . ( 1979 ) . Source credibility in social judgment : bias , expertise , and the judge’s point of view . Journal of Personality and Social Psychology , 37 , 48 – 74 . Bochner , S . , & Insko , C . A . ( 1966 ) . Communicator discrepancy , source credibility , and opinion change . Journal of Personality and Social Psychology , 4 , 614 – 621 . Brehmer , B . , & Hagafors , R . ( 1986 ) . The use of experts in complex decision - making : a paradigm for the study of staﬀ work . Organi - zational Behavior and Human Decision Processes , 38 , 181 – 195 . Brunswik , E . ( 1955 ) . Representative design and probabilistic theory in functional psychology . Psychological Review , 62 , 193 – 217 . Brunswik , E . ( 1956 ) . Perception and the representative design of psychological experiments ( 2nd ed . ) . Berkeley , CA : University of California Press . Budescu , D . V . , & Azen , R . ( 2004 ) . Beyond global measures of relative importance : some insights from dominance analysis . Organization - al Research Methods , 7 , 341 – 350 . Budescu , D . V . , & Rantilla , A . K . ( 2000 ) . Conﬁdence in aggregation of expert opinions . Acta Psychologica , 104 , 371 – 398 . Budescu , D . V . , Rantilla , A . K . , Yu , H . , & Karelitz , T . K . ( 2003 ) . The eﬀects of asymmetry among advisors on the aggregation of their opinions . Organizational Behavior and Human Decision Processes , 90 , 178 – 194 . Cadinu , M . R . , & Rothbart , M . ( 1996 ) . Self - anchoring and diﬀeren - tiation processes in the minimal group setting . Journal of Person - ality and Social Psychology , 70 , 661 – 677 . Camerer , C . F . , & Hogarth , R . M . ( 1999 ) . The eﬀects of ﬁnancial incentives in experiments : a review and capital - labor - production framework . Journal of Risk and Uncertainty , 19 , 7 – 42 . Campbell , L . , Simpson , J . A . , Stewart , M . , & Manning , J . ( 2003 ) . Putting personality in social context : extraversion , emergent leadership , and the availability of rewards . Personality and Social Psychology Bulletin , 29 , 1547 – 1559 . Chao , G . T . , O’Leary - Kelly , A . M . , Wolf , S . , Klein , H . J . , & Gardner , P . ( 1994 ) . Organizational socialization : its content and conse - quences . Journal of Applied Psychology , 79 , 730 – 743 . Clemen , R . T . ( 1989 ) . Combining forecasts : a review and annotated bibliography . International Journal of Forecasting , 5 , 559 – 583 . Clement , R . W . , & Krueger , J . ( 2000 ) . The primacy of self - reference information in perceptions of social consensus . British Journal of Social Psychology , 39 , 279 – 299 . Collopy , F . , Adya , M . , & Armstrong , J . S . ( 2001 ) . Expert systems for forecasting . In J . A . Armstrong ( Ed . ) , Principles of forecasting : A handbook for researchers and practitioners ( pp . 285 – 302 ) . Boston , MA : Kluwer Academic Publishers . Colquitt , J . A . , Hollenbeck , J . R . , Ilgen , D . R . , LePine , J . A . , & Sheppard , L . ( 2002 ) . Computer - assisted communication and team decision - making performance : the moderating eﬀect of openness to experience . Journal of Applied Psychology , 87 , 402 – 410 . Connolly , T . , & Gilani , N . ( 1982 ) . Information search in judgment tasks : a regression model and some preliminary ﬁndings . Organi - zational Behavior and Human Performance , 30 , 330 – 350 . S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 147 Cooksey , R . W . ( 1996 ) . Judgment analysis : Theory , methods , and applications . San Diego : Academic Press . Cooper , R . S . ( 1991 ) . Information processing in the judge – adviser system of group decision - making . Unpublished master’s thesis , University of Illinois , Urbana - Champaign . Cooper - Thomas , H . , & Anderson , N . ( 2002 ) . Newcomer adjustment : the relationship between organizational socialization tactics , infor - mation acquisition and attitudes . Journal of Occupational and Organizational Psychology , 75 , 423 – 437 . Cronbach , L . J . ( 1943 ) . Note on the reliability of ratio scores . Educational and Psychological Measurement , 3 , 67 – 70 . Cronbach , L . J . , & Furby , L . ( 1970 ) . How we should measure ‘‘change’’—or should we ? Psychological Bulletin , 74 , 68 – 80 . Cross , R . , Borgatti , S . P . , & Parker , A . ( 2001 ) . Beyond answers : dimensions of the advice network . Social Networks , 23 , 215 – 235 . Dalal , R . S . ( 2001 ) . The eﬀect of expert advice and ﬁnancial incentives on cooperation . Unpublished master’s thesis , University of Illinois , Urbana - Champaign . Darlington , R . B . ( 1968 ) . Multiple regression in psychological research and practice . Psychological Bulletin , 69 , 161 – 182 . Deelstra , J . T . et al . ( 2003 ) . Receiving instrumental support at work : when help is not welcome . Journal of Applied Psychology , 88 , 324 – 331 . Druckman , J . N . ( 2001 ) . Using credible advice to overcome framing eﬀects . Journal of Law , Economics , and Organization , 17 , 62 – 82 . Edwards , J . R . ( 1995 ) . Alternatives to diﬀerence scores as dependent variables in the study of congruence in organizational research . Organizational Behavior and Human Decision Processes , 64 , 307 – 324 . Edwards , J . R . ( 2002 ) . Alternatives to diﬀerence scores : polynomial regression analysis and response surface methodology . In F . Drasgow & N . Schmitt ( Eds . ) , Measuring and analyzing behavior in organizations : Advances in measurement and data analysis ( pp . 350 – 400 ) . San Francisco , CA : Jossey - Bass . Eisenhardt , K . M . ( 1989 ) . Agency theory : an assessment and review . Academy of Management Review , 14 , 54 – 74 . Fagenson - Eland , E . A . , Marks , M . A . , & Amendola , K . L . ( 1997 ) . Perceptions of mentoring relationships . Journal of Vocational Behavior , 51 , 29 – 42 . Feng , B . , & MacGeorge , E . L . ( 2006 ) . Predicting receptiveness to advice : characteristics of the problem , the advice - giver , and the recipient . Southern Communication Journal , 71 , 67 – 85 . Firebaugh , G . , & Gibbs , J . P . ( 1985 ) . User’s guide to ratio variables . American Sociological Review , 50 , 713 – 722 . Fischer , I . , & Harvey , N . ( 1999 ) . Combining forecasts : what informa - tion do judges need to outperform the simple average ? International Journal of Forecasting , 15 , 227 – 246 . French , J . R . , & Raven , B . H . ( 1959 ) . The bases of social power . In D . Cartwright ( Ed . ) , Studies of social power ( pp . 150 – 167 ) . Ann Arbor : Institute for Social Research , University of Michigan . Gardner , P . H . , & Berry , D . C . ( 1995 ) . The eﬀect of diﬀerent forms of advice on the control of a simulated complex system . Applied Cognitive Psychology , 9 , S55 – S79 . Gibbons , A . M . ( 2003 ) . Alternative forms of advice in natural decision settings . Unpublished master’s thesis , University of Illinois , Urbana - Champaign . Gibbons , A . M . , Sniezek , J . A . , & Dalal , R . S . ( 2003 , November ) . Antecedents and consequences of unsolicited versus explicitly solicited advice . In D . Budescu ( Chair ) , Symposium in Honor of Janet Sniezek . Symposium presented at the annual meeting of the Society for Judgment and Decision Making , Vancouver , BC . Gigone , D . , & Hastie , R . ( 1993 ) . The common knowledge eﬀect : information sharing and group judgment . Journal of Personality and Social Psychology , 65 , 959 – 974 . Gigone , D . , & Hastie , R . ( 1997 ) . The impact of information on small group choice . Journal of Personality and Social Psychology , 72 , 132 – 140 . Gino , F . ( 2005 ) . Do we listen to advice just because we paid for it ? The impact of cost of advice on its use . Harvard Business School Working Paper Series , No . 05 - 017 . Gino , F . , & Moore , D . A . ( in press ) . Eﬀects of task diﬃculty on use of advice . Journal of Behavioral Decision Making . Goldsmith , D . J . ( 2000 ) . Solicitingadvice : theroleofsequentialplacement in mitigatingface threat . Communications Monographs , 67 , 1 – 19 . Goldsmith , D . J . , & Fitch , K . ( 1997 ) . The normative context of advice as social support . Human Communication Research , 23 , 454 – 476 . Gomez - Beldarrian , M . , Harries , C . , Garcia - Monco , J . C . , Ballus , E . , & Grafman , J . ( 2004 ) . Patients with right fontal lesions are unable to assess and use advice to make predictive judgments . Journal of Cognitive Neuroscience , 16 , 74 – 89 . Griﬀeth , R . W . , & Hom , P . W . ( 1995 ) . The employee turnover process . In G . R . Ferris & J . J . Martocchio ( Eds . ) . Research in personal and human resources management ( Vol . 13 , pp . 245 – 293 ) . Stamford , CT : JAI Press . Griﬃn , A . E . C . , Colella , A . , & Goparauju , S . ( 2000 ) . Newcomer and organizational socialization tactics : an interactionist perspective . Human Resource Management Review , 10 , 453 – 474 . Hackman , J . R . ( 1987 ) . The design of work teams . In J . W . Lorsch ( Ed . ) , Handbook of organizational behavior ( pp . 315 – 342 ) . Engle - wood Cliﬀs , NJ : Prentice Hall . Harber , K . D . , Schneider , J . K . , Everard , K . M . , & Fisher , E . B . ( 2005 ) . Directive support , nondirective support , and morale . Journal of Social and Clinical Psychology , 24 , 691 – 722 . Harries , C . , & Harvey , N . ( 2000 ) . Taking advice , using information and knowing what you are doing . Acta Psychologica , 104 , 399 – 416 . Harries , C . , Yaniv , I . , & Harvey , N . ( 2004 ) . Combining advice : the weight of a dissenting opinion in the consensus . Journal of Behavioral Decision Making , 17 , 333 – 348 . Harvey , N . , & Bolger , F . ( 2001 ) . Collecting information : optimizing outcomes , screening options or facilitating discrimination ? Quar - terly Journal of Experimental Psychology , 54A , 269 – 301 . Harvey , N . ( 2001 ) . Improving judgmental forecasts . In J . A . Arm - strong ( Ed . ) , Principles of forecasting : A handbook for researchers and practitioners ( pp . 59 – 80 ) . Boston , MA : Kluwer Academic Publishers . Harvey , N . , & Fischer , I . ( 1997 ) . Taking advice : accepting help , improving judgment , and sharing responsibility . Organizational Behavior and Human Decision Processes , 70 , 117 – 133 . Harvey , N . , & Fischer , I . ( 2005 ) . Development of experience - based judgement and decision making : the role of outcome feedback . In T . Betsch & S . Haberstroh ( Eds . ) , The routines of decision making ( pp . 119 – 137 ) . Mahwah , NJ : Lawrence Erlbaum . Harvey , N . , & Harries , C . ( 2004 ) . Eﬀects of judges’ forecasting on their later combination of forecasts for the some outcomes . International Journal of Forecasting , 20 , 391 – 409 . Harvey , N . , Harries , C . , & Fischer , I . ( 2000 ) . Using advice and assessing its quality . Organizational Behavior and Human Decision Processes , 81 , 252 – 273 . Harvey , N . , Koehler , D . J . , & Ayton , P . ( 1997 ) . Judgements of decision eﬀectiveness : actor – observer diﬀerences in overconﬁdence . Organizational Behavior and Human Decision Processes , 70 , 117 – 133 . Heath , C . , & Gonzalez , R . ( 1995 ) . Interaction with others increases decision conﬁdence but not decision quality : evidence against information collection views of interactive decision - making . Orga - nizational Behavior and Human Decision Processes , 61 , 305 – 326 . Hedges , L . V . , & Olkin , I . ( 1985 ) . Statistical methods for meta - analysis . New York : Academic Press . Hedlund , J . , Ilgen , D . R . , & Hollenbeck , J . R . ( 1998 ) . Decision accuracy in computer - mediated versus face - to - face decision - mak - ing teams . Organizational Behavior and Human Decision Processes , 76 , 30 – 47 . Highhouse , S . ( 2001 ) . Judgment and decision - making research : rele - vance to industrial and organizational psychology . In N . Ander - 148 S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 son , D . S . Ones , H . K . Sinangil , & C . Visvesvaran ( Eds . ) . Handbook of industrial , work , and organizational psychology ( Vol . 2 , pp . 314 – 332 ) . London , UK : Sage . Highhouse , S . , & Hoﬀman , J . ( 2001 ) . Organizational attraction and job choice . In C . L . Cooper & I . T . Robertson ( Eds . ) , International Review of Industrial and Organizational Psychology ( Vol . 16 , pp . 37 – 64 ) . Hinsz , V . B . ( 1999 ) . Group decision making with responses of a quantitative nature : the theory of social decision schemes for quantities . Organizational Behavior and Human Decision Processes , 80 , 28 – 49 . Hogarth , R . M . , & Einhorn , H . J . ( 1992 ) . Order eﬀects in belief updating : the belief - adjustment model . Cognitive Psychology , 24 , 1 – 55 . Hollenbeck , J . R . , Ilgen , D . R . , LePine , J . A . , Colquitt , J . A . , & Hedlund , J . ( 1998 ) . Extending the multilevel theory of team decision making : eﬀects of feedback and experience in hierarchical teams . Academy of Management Journal , 41 , 269 – 282 . Hollenbeck , J . R . , Ilgen , D . R . , Sego , D . J . , Hedlund , J . , Major , D . A . , & Phillips , J . ( 1995 ) . Multilevel theory of team decision making : decision performance in teams incorporating distributed expertise . Journal of Applied Psychology , 80 , 292 – 316 . Horowitz , L . M . , Krasnoperova , E . N . , Tatar , D . G . , Hansen , M . B . , Person , E . A . , Galvin , K . L . , et al . ( 2001 ) . The way to console may depend on the goal : experimental studies of social support . Journal of Experimental Social Psychology , 37 , 49 – 61 . Humphrey , S . E . , Hollenbeck , J . R . , Meyer , C . J . , & Ilgen , D . R . ( 2002 ) . Hierarchical team decision making . In G . R . Ferris & J . J . Martocchio ( Eds . ) . Research in personnel and human resources management ( Vol . 21 , pp . 175 – 213 ) . Stamford , CT : JAI Press . Johnson , T . R . , Budescu , D . V . , & Wallsten , T . S . ( 2001 ) . Averaging probability judgments : Monte Carlo analyses of diagnostic value . Journal of Behavioral Decision Making , 14 , 123 – 140 . Jonas , E . , & Frey , D . ( 2003 ) . Information search and presentation in advisor – client interactions . Organizational Behavior and Human Decision Processes , 91 , 154 – 168 . Jungermann , H . ( 1999 ) . Advice - giving and taking . In Proceedings of the 32nd Hawaii International Conference on System Sciences ( HICSS - 32 ) . Maui , HI : Institute of Electrical and Electronics Engineers , Inc . ( IEEE ) . [ CD - ROM ] . www . gp . tu - berlin . de / Users / j / jungermann / Publications / Advice % 20giving . html . Jungermann , H . , & Fischer , K . ( 2005 ) . Using expertise and experience for giving and taking advice . In T . Betsch & S . Haberstroh ( Eds . ) , The routines of decision making ( pp . 157 – 173 ) . Mahwah , NJ : Lawrence Erlbaum . Kahn , B . E . , & Baron , J . ( 1995 ) . An exploratory study of choice rules favored for high - stakes decisions . Journal of Consumer Psychology , 4 , 305 – 328 . Karau , S . J . , & Williams , K . D . ( 1993 ) . Social loaﬁng : a meta - analytic review and theoretical integration . Journal of Personality and Social Psychology , 65 , 681 – 706 . Katz , D . , & Kahn , R . L . ( 1966 ) . The social psychology of organizations . New York : Wiley . Kerr , N . L . , & Tindale , R . S . ( 2004 ) . Group performance and decision - making . Annual Review of Psychology , 55 , 623 – 655 . Klayman , J . , Soll , J . B . , Gonzalez - Vallejo , C . , & Barlas , S . ( 1999 ) . Overconﬁdence : it depends on how , what , and whom you ask . Organizational Behavior and Human Decision Processes , 79 , 216 – 247 . Koehler , D . J . ( 1994 ) . Hypothesis generation and conﬁdence in judgment . Journal of Experimental Psychology : Learning , Memory and Cognition , 20 , 461 – 469 . Koehler , D . J . , & Harvey , N . ( 1997 ) . Conﬁdence judgments by actors and observers . Journal of Behavioral Decision Making , 10 , 221 – 242 . Koestner , R . , Gingras , I . , Abutaa , R . , Losier , G . F . , DiDio , L . , & Gagne ´ , M . ( 1999 ) . To follow expert advice when making a decision : an examination of reactive versus reﬂective autonomy . Journal of Personality , 65 , 851 – 872 . Kram , K . E . ( 1983 ) . Phases of the mentoring relationship . Academy of Management Journal , 26 , 608 – 625 . Kray , L . , & Gonzalez , R . ( 1999 ) . Diﬀerential weighting in choice versus advice : I’ll do this , you do that . Journal of Behavioral Decision Making , 12 , 207 – 217 . Kray , L . J . ( 2000 ) . Contingent weighting in self - other decision making . Organizational Behavior and Human Decision Processes , 83 , 82 – 106 . Krueger , J . L . ( 2003 ) . Return of the ego—self - referent information as a ﬁlter for social prediction : comment on Karniol ( 2003 ) . Psycho - logical Review , 110 , 585 – 590 . Kuhn , K . M . , Spurlock , D . , & Sniezek , J . A . ( 1998 , November ) . Social inﬂuenceunderuncertainty : eﬀectsofevaluatingtypicalandextreme advice on conﬁdence . In D . J . Koehler ( Chair ) , Interactive Judgment and Decision Making . Symposium conducted at the annual meeting of the Society for Judgment and Decision Making , Dallas , TX . Laughlin , P . R . ( 1980 ) . Social combination processes of cooperative , problem - solving groups on verbal intellective tasks . In M . Fishbein ( Ed . ) . Progress in social psychology ( Vol . 1 , pp . 127 – 155 ) . Hillsdale , NJ : Lawrence Erlbaum . Lasswell , H . D . ( 1948 ) . The structure and function of communication in society . In L . Bryson ( Ed . ) , The communication of ideas : Religion and civilization series ( pp . 37 – 51 ) . New York : Harper & Row . Lawrence , M . & Warren , M . ( 2003 , November ) . Are judgmental forecasts impacted by the source of the advice ? Paper presented at the annual meeting of the Society for Judgment and Decision Making , Vancouver , BC . LeBreton , J . M . , Ployheart , R . E . , & Ladd , R . T . ( 2004 ) . A Monte Carlo comparison of relative importance methodologies . Organi - zational Research Methods , 7 , 258 – 282 . Lee , T . W . , & Mitchell , T . R . ( 1994 ) . An alternative approach : the unfolding model of voluntary employee turnover . Academy of Management Review , 19 , 51 – 89 . LePine , J . A . , Hollenbeck , J . R . , Ilgen , D . R . , & Hedlund , J . ( 1997 ) . Eﬀects of individual diﬀerences on the performance of hierarchical decision - making teams : much more thang . Journal of Applied Psychology , 82 , 803 – 811 . Lim , J . S . , & O’Connor , M . ( 1995 ) . Judgmental adjustment of initial forecasts : its eﬀectiveness and biases . Journal of Behavioral Deci - sion - Making , 8 , 149 – 168 . Linstone , H . A . ( 1978 ) . The Delphi technique . In R . B . Fowles ( Ed . ) , Handbook of futures research . Westport , CT : Greenwood Press . Louis , M . R . , Posner , B . Z . , & Powell , G . N . ( 1983 ) . The availability and helpfulness of socialization practices . Personnel Psychology , 36 , 857 – 866 . Lualhati , J . C . ( 1992 ) . Performance forecast accuracy and conﬁdence in a judge – adviser decision system . Unpublished master’s thesis , University of Illinois , Urbana - Champaign . Luan , S . , Sorkin , R . D . , & Itzkowitz , J . ( 2004 ) . Weighing information from outside sources : a biased process . Journal of Behavioral Decision Making , 17 , 45 – 116 . Mathieu , J . E . , Heﬀner , T . S . , Goodwin , G . F . , Salas , E . , & Cannon - Bowers , J . A . ( 2000 ) . The inﬂuence of shared mental models on team process and performance . Journal of Applied Psychology , 85 , 273 – 283 . McGrath , J . E . ( 1984 ) . Groups : Interaction and performance . Engle - wood Cliﬀs , NJ : Prentice - Hall ( pp . 51 – 66 ) . Merriam - Webster’s collegiate dictionary ( 11th ed . ) ( 2003 ) . [ Electronic version ] . Springﬁeld , MA : Merriam - Webster . Meyer , J . P . , Stanley , D . J . , Herscovitch , L . , & Topolnytsky , L . ( 2002 ) . Aﬀective , continuance , and normative commitment to the organi - zation : a meta - analysis of antecedents , correlates , and consequenc - es . Journal of Vocational Behavior , 61 , 20 – 52 . Morrison , E . W . ( 1993 ) . Longitudinal study of the eﬀects of informa - tion seeking on newcomer socialization . Journal of Applied Psychology , 78 , 173 – 183 . S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 149 Morrison , E . W . ( 2002 ) . Newcomers’ relationships : the role of social network ties during socialization . Academy of Management Jour - nal , 45 , 1149 – 1160 . Mount , M . K . , & Barrick , M . R . ( 1995 ) . The Big Five Personality Dimensions : implications for research and practice in human resources management . In G . R . Ferris ( Ed . ) . Research in personnel and human resources management ( Vol . 13 , pp . 153 – 200 ) . Stamford , CT : JAI Press . Naylor , J . ( 1984 ) . Editorial : a time of transition . Organizational Behavior and Human Performance , 34 , 1 – 4 . Noe , R . A . ( 1988 ) . An investigation of the determinants of successful assigned mentoring relationships . Personnel Psychology , 41 , 457 – 479 . Noe , R . A . , Greenberger , D . B . , & Wang , S . ( 2002 ) . Mentoring : what we know and where we might go . In G . R . Ferris & J . J . Martocchio ( Eds . ) . Research in personnel and human resources management ( Vol . 21 , pp . 129 – 173 ) . Stamford , CT : JAI Press . Paese , P . W . , & Sniezek , J . A . ( 1991 ) . Inﬂuences on the appropriateness of conﬁdence in judgment : practice , eﬀort , information , and decision - making . Organizational Behavior and Human Decision Processes , 48 , 100 – 130 . Payne , J . W . , Bettman , J . R . , & Johnson , E . J . ( 1993 ) . The adaptive decision maker . Cambridge , MA : Cambridge University Press . Petty , M . M . , & Pryor , N . M . ( 1974 ) . A note on the predictive validity of initiating structure and consideration in ROTC training . Journal of Applied Psychology , 59 , 383 – 385 . Phillips , J . M . ( 1999 ) . Antecedents of leader utilization of staﬀ input in decision - making teams . Organizational Behavior and Human Deci - sion Processes , 77 , 215 – 242 . Price , P . C . , & Stone , E . R . ( 2004 ) . Intuitive evaluation of likelihood judgment producers : evidence for a conﬁdence heuristic . Journal of Behavioral Decision Making , 17 , 39 – 57 . Rantilla , A . K . ( 2000 ) . The role of expert advice format on individual and group decision making . Unpublished doctoral dissertation , University of Illinois at Urbana - Champaign . Redd , S . B . ( 2002 ) . The inﬂuence of advisers on foreign policy decision making . Journal of Conﬂict Resolution , 46 , 335 – 364 . Reinhardt , J . P . , Boerner , K . , & Horowitz , A . ( 2006 ) . Good to have but not good to use : diﬀerential impact of perceived and received support on well - being . Journal of Social and Personal Relationships , 23 , 117 – 129 . Ronis , D . L . , & Yates , J . F . ( 1987 ) . Components of probability judgment accuracy : individual consistency and eﬀects of subject matter and assessment method . Organizational Behavior and Human Decision Processes , 40 , 193 – 218 . Rowe , G . , & Wright , G . ( 1996 ) . The impact of task characteristics on the performance of structured group forecasting techniques . International Journal of Forecasting , 12 , 73 – 89 . Salas , E . , & Cannon - Bowers , J . A . ( Eds . ) ( 2001 ) . Shared cognition [ Special Issue ] . Journal of Organizational Behavior , 22 ( 2 ) . Savadori , L . , Van Swol , L . M . , & Sniezek , J . A . ( 2001 ) . Information sampling and conﬁdence within groups and judge advisor systems . Communication Research , 28 , 737 – 771 . Schlosser , L . Z . , & Gelso , C . J . ( 2001 ) . Measuring the working alliance in advisor – advisee relationships in graduate school . Journal of Counseling Psychology , 48 , 157 – 167 . Schotter , A . ( 2003 ) . Decision - Making with naı¨ve advice . American Economic Review , 93 , 196 – 201 . Schrah , G . E . , Dalal , R . S . , & Sniezek , J . A . ( 2006 ) . Theadaptivedecision - maker is not an island : integrative expert advice with information search . Journal of Behavioral Decision - Making , 19 , 43 – 60 . Sherif , M . , & Hovland , C . I . ( 1961 ) . Social judgment : Assimilation and contrast eﬀects in communication and attitude change . New Haven , CT : Yale University Press . Slaugher , J . E . , & Highhouse , S . ( 2003 ) . Does matching up features mess up job choice ? Boundary conditions on attribute - salience eﬀects . Journal of Behavioral Decision Making , 16 , 1 – 15 . Slovic , P . , & Lichtenstein , S . ( 1971 ) . Comparison of Bayesian and regression approaches to the study of information processing in judgment . Organizational Behavior and Human Performance , 6 , 649 – 744 . Sniezek , J . A . , & Buckley , T . ( 1995 ) . Cueing and cognitive conﬂict in judge – advisor decision making . Organizational Behavior and Human Decision Processes , 62 , 159 – 174 . Sniezek , J . A . , Heath , C . , Van Swol , L . M . , & Nochimowski , M . ( 1998 , November ) . Suspicious minds . Paper presented at the annual meeting of the Society for Judgment and Decision Making , Dallas , TX . Sniezek , J . A . , Paese , P . W . , & Swithers , F . S . ( 1990 ) . The eﬀect of choosing on conﬁdence in choice . Organizational Behavior and Human Decision Processes , 46 , 264 – 282 . Sniezek , J . A . , & Van Swol , L . M . ( 2001 ) . Trust , conﬁdence , and expertise in a judge – advisor system . Organizational Behavior and Human Decision Processes , 84 , 288 – 307 . Sniezek , J . A . , Schrah , G . E . , & Dalal , R . S . ( 2004 ) . Improving judgment with prepaid expert advice . Journal of Behavioral Decision Making , 17 , 173 – 190 . Soll , J . B . ( 1999 ) . Intuitive theories of information : beliefs about the value of redundancy . Cognitive Psychology , 38 , 317 – 346 . Soll , J . B . , & Klayman , J . ( 2004 ) . Overconﬁdence in interval estimates . Journal of Experimental Psychology : Learning , Memory , and Cognition , 30 , 299 – 314 . Soll , J . B . , & Larrick , R . ( 1999 , November ) . The 80 / 20 rule and revision of judgment in light of another’s opinion : why do we believe in ourselves so much ? Paper presented at the annual meeting of the Society for Judgment and Decision Making , Los Angeles , CA . Stewart , T . R . ( 2001 ) . Improving reliability of judgmental forecasts . In J . A . Armstrong ( Ed . ) , Principles of forecasting : A handbook for researchers and practitioners ( pp . 81 – 106 ) . Boston , MA : Kluwer Academic Publishers . Todd , F . J . , & Hammond , K . R . ( 1965 ) . Diﬀerential feedback in two multiple - cue probability learning tasks . Behavioral Science , 10 , 429 – 435 . Tversky , A . , & Kahneman , D . ( 1974 ) . Judgment under uncertainty : heuristics and biases . Science , 185 , 1124 – 1131 . Van Maanen , J . , & Schein , E . ( 1979 ) . Toward a theory of organiza - tional socialization . In L . L . Cummings & B . Staw ( Eds . ) . Research in organizational behavior ( Vol . 1 , pp . 209 – 264 ) . Greenwich , CT : JAI Press . Van Swol , L . M . , & Ludutsky , C . ( 2003 ) . Tell me something I don’t know : Decision - makers’ preference for advisors with unshared information . In D . Budescu ( Chair ) , Symposium in Honor of Janet Sniezek . Symposium presented at the annual meeting of the Society for Judgment and Decision Making , Vancouver , BC . Van Swol , L . M . , & Sniezek , J . A . ( 2005 ) . Factors aﬀecting the acceptance of expert advice . British Journal of Social Psychology , 44 , 443 – 461 . Vroom , V . H . , & Yetton , P . W . ( 1973 ) . Leadership and decision making . Pittsburgh : University of Pittsburgh Press . Waldman , D . A . , Atwater , L . E . , & Davidson , R . A . ( 2004 ) . The role of individualism and the Five - Factor Model in the prediction of performance in a leaderless group discussion . Journal of Person - ality , 72 , 1 – 28 . Webster , D . M . , & Kruglanski , A . W . ( 1994 ) . Individual diﬀerences in need for cognitive closure . Journal of Personality and Social Psychology , 67 , 1049 – 1062 . Whittemore , R . , Rankin , S . L . , Callahan , C . D . , Leder , M . C . , & Carroll , D . L . ( 2000 ) . The peer advisor experience providing social support . Qualitative Health Research , 10 , 260 – 276 . Wittenbaum , G . M . , & Stasser , G . ( 1996 ) . Management of information in small groups . In J . L . Nye & A . M . Brower ( Eds . ) , What’s social about social cognition ? Research on socially shared cognition in small groups ( pp . 3 – 28 ) . Thousand Oaks , CA : Sage . 150 S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 Yaniv , I . ( 1997 ) . Weighting and trimming : heuristics for aggregation of judgments under uncertainty . Organizational Behavior and Human Decision Processes , 69 , 237 – 249 . Yaniv , I . ( 2004a ) . The Beneﬁt of additional opinions . Current Directions in Psychological Science , 13 , 75 – 78 . Yaniv , I . ( 2004b ) . Receivingotherpeople’sadvice : inﬂuenceandbeneﬁt . Organizational Behavior and Human Decision Processes , 93 , 1 – 13 . Yaniv , I . , & Foster , D . P . ( 1997 ) . Precision and accuracy of judgmental estimation . Journal of Behavioral Decision Making , 10 , 21 – 32 . Yaniv , I . , & Kleinberger , E . ( 2000 ) . Advice taking in decision making : egocentric discounting and reputation formation . Organizational Behavior and Human Decision Processes , 83 , 260 – 281 . Yaniv , I . , & Milyavsky , M . ( in press ) . Using advice from multiple sources to revise and improve judgment . Organizational Behavior and Human Decision Processes . Yates , J . F . , Price , P . C . , Lee , J . , & Ramirez , J . ( 1996 ) . Good probabilistic forecasters : the ‘‘consumer’s’’ perspective . Interna - tional Journal of Forecasting , 12 , 41 – 56 . S . Bonaccio , R . S . Dalal / Organizational Behavior and Human Decision Processes 101 ( 2006 ) 127 – 151 151