a r X i v : 1901 . 07786v1 [ c s . C L ] 23 J a n 2019 Self - Attentive Model for Headline Generation Daniil Gavrilov , Pavel Kalaidin , and Valentin Malykh VK , 191023 , Nevsky ave . , 28 , Saint - Petersburg , Russia { firstname . lastname } @ vk . com Abstract . Headline generation is a special type of text summarization task . While the amount of available training data for this task is al - most unlimited , it still remains challenging , as learning to generate head - lines for news articles implies that the model has strong reasoning about natural language . To overcome this issue , we applied recent Universal Transformer architecture paired with byte - pair encoding technique and achieved new state - of - the - art results on the New York Times Annotated corpus with ROUGE - L F1 - score 24 . 84 and ROUGE - 2 F1 - score 13 . 48 . We also present the new RIA corpus and reach ROUGE - L F1 - score 36 . 81 and ROUGE - 2 F1 - score 22 . 15 on it . Keywords : universal transformer · headline generation · BPE · sum - marization . 1 Introduction Headline writing style has broader applications than those used purely within the journalism community . So - called naming is one of the arts of journalism . Just as natural language processing techniques help people with tasks such as incoming message classiﬁcation ( see [ 5 ] or [ 6 ] ) , the naming problem could also be solved using modern machine learning and , in particular , deep learning techniques . In the ﬁeld of machine learning , the naming problem is formulated as headline generation , i . e . given the text it is needed to generate a title . Headline generation can also be seen as a special type of text summarization . The aim of summarization is to produce a shorter version of the text that cap - tures the main idea of the source version . We focus on abstractive summarization when the summary is generated on the ﬂy , conditioned on the source sentence , possibly containing novel words not used in the original text . The downside of traditional summarization is that ﬁnding a source of sum - maries for a large number of texts is rather costly . The advantage of headline generation over the traditional approach is that we have an endless supply of news articles since they are available in every major language and almost always have a title . This task could be considered language - independent due to the absence of the necessity of native speakers for markup and / or model development . While the task of learning to generate article headlines may seem to be easier than generating full summaries , it still requires that the learning algorithm be 2 Daniil Gavrilov , Pavel Kalaidin , and Valentin Malykh able to catch structure dependencies in natural language and therefore could be an interesting benchmark for testing various approaches . In this paper , we present a new approach to headline generation based on Universal Transformer architecture which explicitly learns non - local representa - tions of the text and seems to be necessary to train summarization model . We also present the test results of our model on the New York Times Annotated corpus and the RIA corpus . 2 Related Work Rush et al . [ 11 ] were the ﬁrst to apply an attention mechanism to abstractive text summarization . In the recent work of Hayashi [ 4 ] , an encoder - decoder approach was presented , where the ﬁrst sentence was reformulated to a headline . Our Encoder - Decoder baseline ( see section 6 . 1 ) follows their setup . The related approach was presented in [ 10 ] , where the approach of the ﬁrst sentence was expanded with a so - called topic sentence . The topic sentence is chosen to be the ﬁrst sentence containing the most important information from a news article ( so called 5W1H information , where 5W1H stands for who , what , where , when , why , how ) . Our Encoder - Decoder baseline could be considered to implement their approach in OF ( trained On First sentence ) setup . Tan et al . in [ 15 ] present an encoder - decoder approach based on a pregen - erated summary of the article . The summary is generated using a statistical summarization approach . The authors mention that the ﬁrst sentence approach is not enough for New York Times corpora , but they only use a summary for their approach instead of the whole text , thus relying on external tools of sum - marization . 3 Background Consider that we have dataset D = { ( title i , fulltext i ) } Ni of news articles and their titles . An approach for learning summarization is to deﬁne a conditional probability P ( y t | { y 1 , . . . , y t − 1 } , X , θ ) of some token y t ∈ V at time step t ∈ N , with respect to article text X = { x 1 , . . . , x N } ( x i ∈ V too ) and previous tokens of the title { y 1 , . . . , y t − 1 } , parameterized by a neural network with parameters θ . Then model parameters are found as θ MLE = argmax θ Q Ni P ( Y i | X i , θ ) We can then apply two methods for ﬁnding the most probable sentence under trained model : greedy , decoding token - by - token by ﬁnding the most probable token at each time step , and beam - search , where we ﬁnd the top - k most probable tokens at each step . The latter method yields better results though it is more computationally expensive . Sutskever et al . [ 14 ] proposed a model that deﬁnes P ( y t | { y 1 , . . . , y t − 1 } , X , θ ) by propagating initial sequence X through a Recurrent Neural Network ( RNN ) . Then last hidden state of RNN is used as context vector c and is then passed to the second RNN with y 1 , . . . , y t − 1 to obtain distribution over y t . Self - Attentive Model for Headline Generation 3 RNNs have a commonly known ﬂaw . They rapidly forget earlier timesteps , e . g . see [ 2 ] . To mitigate this issue , attention [ 1 ] was introduced to the Encoder - Decoder architecture . The attention mechanism makes a model able to obtain a new context vector at every decoding iteration from diﬀerent parts of an encoded sequence . It helps capture all the relevant information from the input sequence , removing the bottleneck of the ﬁxed size hidden vector of the decoder’s RNN . 4 Our Approach 4 . 1 Universal Transformer While RNNs could be easily used to deﬁne the Encoder - Decoder model , learning the recurrent model is very expensive from a computation perspective . The other drawback is that they use only local information while omitting a sequence of hidden states H = { h 1 , . . . , h N } . I . e . any two vectors from hidden state h i and h j are connected with j − i RNN computations that makes it hard to catch all the dependencies in them due to limited capacity . To train a rich model that would learn complex text structure , we have to deﬁne a model that relies on non - local dependencies in the data . In this work , we adopt the Universal Transformer model architecture [ 3 ] , which is a modiﬁed version of Transformer [ 16 ] . This approach has several bene - ﬁts over RNNs . First of all , it could be trained in parallel . Furthermore , all input vectors are connected to every other via the attention mechanism . It implies that Transformer architecture learns non - local dependencies between tokens regard - less of the distance between them , and thus it is able to learn a more complex representation of the text in the article , which proves to be necessary to eﬀec - tively solve the task of summarization . Also , unlike [ 4 , 15 ] , our model is trained end - to - end using the text and title of each news article . 4 . 2 Byte Pair Encoding We also adopt byte - pair encoding ( BPE ) , introduced by Sennrich for the ma - chine translation task in [ 13 ] . BPE is a data compression technique where often encountered pairs of bytes are replaced by additional extra - alphabet symbols . In the case of texts , like in the machine translation ﬁeld , the most frequent words are kept in the vocabulary , while less frequent words are replaced by a sequence of ( typically two ) tokens . E . g . , for morphologically rich languages , the word end - ings could be detached since each word form is deﬁnitely less frequent than its stem . BPE encoding allows us to represent all words , including the ones unseen during training , with a ﬁxed vocabulary . 5 Experiments In our experiments , we consider two corpora : one in Russian and another in English . It is important to mention that we have not done any additional pre - processing other than lower casing , unlike other approaches [ 4 , 10 ] . We apply 4 Daniil Gavrilov , Pavel Kalaidin , and Valentin Malykh BPE encoding , which allows us to avoid usage of the < UNK > token for out - of - vocabulary words . For our experiments , we withheld 20 , 000 random articles to form the test set . We have repeated our experiments 5 times with diﬀerent random seeds and report mean values . English Dataset We use the New York Times Annotated Corpus ( NYT ) as presented by the Linguistic Data Consortium in [ 12 ] . This dataset contains 1 . 8 million news articles from the New York Times news agency , written between the years 1987 and 2006 . For our experiments , we ﬁltered out news articles containing titles shorter than 3 words or longer than 15 words . We also ﬁltered articles with a body text shorter than 20 words or longer than 2000 words . In addition , we skipped obituaries in the dataset . After ﬁltering , we had 1444919 news available to us with a mean title length of 7 . 9 words and mean text length of 707 . 6 words . Russian Dataset Russian news agency “Rossiya Segodnya” provided us with a dataset ( RIA ) for research purposes 1 . It contains news documents from January , 2010 to December , 2014 . In total , there are 1003869 news articles in the provided corpus with a mean title length 9 . 5 words and mean text length of 315 . 6 words . 6 Experiments 6 . 1 Baseline models First Sentence This model takes the ﬁrst sentence of an article and uses it as its hypothesis for an article headline . This is a strong baseline for generating headlines from news articles . Encoder - Decoder Following [ 10 ] , we use the encoder - decoder architecture on the ﬁrst sentence of an article . The model itself is already described at recent works section as Seq - To - Seq with RNNs of Sutskever et al . [ 14 ] . For this ap - proach , we use the same preprocessing as we did for our model , including byte pair encoding . 6 . 2 Training For both datasets , NYT and RIA , we used the same set of hyper - parameters for the models , namely 4 layers in the encoder and decoder with 8 heads of attention . In addition , we added a Dropout of p = 0 . 3 before applying Layer Normalization [ 8 ] . The models were trained with the Adam optimizer using a scaled learning rate , as proposed by the authors of the original Transformer with the number 1 The dataset is available at https : / / vk . cc / 8W0l5P Self - Attentive Model for Headline Generation 5 of warmout steps equal to 4000 in both cases and β = ( 0 . 9 , 0 . 98 ) . Both models were trained until convergence . We trained the BPE tokenizator separately on the datasets . NYT data was tokenized with a vocabulary size of active tokens equal to 40000 , while RIA data was tokenized using 50000 token vocabulary . In addition , we have limited length of the documents with 3000 BPE tokens and 2000 BPE tokens for RIA and NYT datasets respectively . Any exceeding tokens were omitted . word2vec [ 9 ] embeddings were trained on each dataset with the size of each embedding equal to 512 . For headline generation , we adopted beam - search size of 10 . 7 Results Model R - 1 - f R - 1 - r R - 2 - f R - 2 - r R - L - f R - L - r New York Times First Sentence 11 . 64 34 . 67 2 . 28 7 . 43 7 . 19 31 . 39 Encoder - Decoder 23 . 02 21 . 90 11 . 84 11 . 44 21 . 23 21 . 31 summ - hieratt [ 15 ] - 29 . 60 - 8 . 17 - 26 . 05 Universal Transformer w / smoothing ( ours ) 25 . 60 23 . 90 12 . 92 12 . 42 23 . 66 25 . 27 Universal Transformer ( ours ) 26 . 86 25 . 33 13 . 48 13 . 01 24 . 84 24 . 38 Rossiya Segodnya First Sentence 24 . 08 45 . 58 10 . 57 21 . 30 16 . 70 41 . 67 Encoder - Decoder 39 . 10 38 . 31 22 . 13 21 . 75 36 . 34 36 . 34 Universal Transformer w / smoothing ( ours ) 39 . 31 37 . 10 21 . 82 20 . 66 36 . 32 35 . 37 Universal Transformer ( ours ) 39 . 75 37 . 62 22 . 15 21 . 04 36 . 81 35 . 91 Table 1 . ROUGE - 1 , 2 , L F 1 and recall scores , on NYT corpus and RIA corpus . In Tab . 1 we present results based on two corpora : the New York Times Annotated ( NYT ) corpus for English , and the Rossiya Segodnya ( RIA ) corpus for Russian . For the NYT corpus , we reached a new state of the art on ROUGE - 1 , ROUGE - 2 and ROUGE - L F 1 scores . For the RIA corpus , since it has no previous art , we present results for the baselines and our model . 2 For our model we also experimented with label smoothing following [ 7 ] . In our experiments , we noticed that some of the generated headlines are scored low by ROUGE metrics despite seeming reasonable , e . g . top sample in Tab . 3 . This lead us to a new series of experiments . We conducted human evalua - tion of obtained results for both NYT and RIA corpora . The results are presented 2 We are providing results from Tan et al . [ 15 ] , which were achieved using the NYT corpus . Unfortunately , the authors have not published all of their ﬁltering criteria and seed for random sampling for this corpus , so we could not follow their setup completely . Therefore , these results are presented here for reference . 6 Daniil Gavrilov , Pavel Kalaidin , and Valentin Malykh Dataset User Preference Human Tie Machine New York Times Annotated 57 . 4 27 . 4 15 . 2 Rossiya Segodnya 54 . 4 30 . 6 15 . 0 Table 2 . Human evaluation results for NYT and RIA datasets . in Tab . 2 . 5 annotators marked up 100 randomly sampled articles from a train set of each corpora . Each number shows the percentage of annotator prefer - ence over three possible options : original headline ( Human ) , generated headline ( Machine ) , no preference ( Tie ) . For the both corpora , we could see that our model is not reaching human parity yet , having 42 . 6 % and 45 . 6 % of ( Machine + Tie ) user preference for NYT and RIA datasets respectively , but this result is already close to human parity and leaves room for improvement . Original text , truncated : Unethical and irresponsible as the assertion that antidepressant medication , an ex - cellent treatment for some forms of depression , will turn a man into a ﬁsh . It does a disservice to psychoanalysis , which oﬀers rich and valuable insights into the human mind . . . . Homosexuality is not an illness by any of the usual criteria in medicine , such as an increased risk of morbidity or mortality , painful symptoms or social , interpersonal or occupational dysfunction as a result of homosexuality itself . . . Original headline : homosexuality , not an illness , can’t be cured Generated headline : why we can’t let gay therapy begin Original text , truncated : southwest airlines said yesterday that it would add 16 ﬂights a day from chicago midway airport , moving to protect a valuable hub amid the ﬁght breaking out over the assets of ata airlines , the airport’s biggest carrier . southwest said that beginning in january , it would add the ﬂights to 13 cities that it already served from midway . . . Original headline : southwest is adding ﬂights to protect its chicago hub Generated headline : southwest airlines to add 16 ﬂights from chicago Original text , truncated : москва , 1 апр - риа новости . количество сделок продажи элитных квартир в москве выросло в первом квартале этого года , по сравнению с аналогичным периодом предыдущего , в два раза , говорится в отчете компании intermarksavill s . при этом , также сообщается в нем , количество заключенных в столице первичных сделок в сегменте бизнес - класса в первом квартале 2010 года оказалось на 20 выше , чем в первом квартале прошлого года . . . Original headline : продажи элитного жилья в москве увеличились в 1 квартале в два раза Generated headline : продажи элитных квартир в москве в 1 квартале выросли вдвое Table 3 . Samples of headlines generated by our model . 8 Conclusion In this paper , we explore the application of Universal Transformer architecture to the task of abstractive headline generation and outperform the abstractive state - of - the - art result on the New York Times Annotated corpus . We also present a newly released Rossiya Segodnya corpus and results achieved by our model applied to it . Acknowledgments . Authors are thankful to Alexey Samarin for useful dis - cussions , David Prince for proofreading , Madina Kabirova for proofreading and human evaluation organization , Anastasia Semenyuk and Maria Zaharova for help obtaining the New York Times Annotated corpus , and Alexey Filippovskii for providing the Rossiya Segodnya corpus . Self - Attentive Model for Headline Generation 7 References 1 . Bahdanau , D . , Cho , K . , Bengio , Y . : Neural machine translation by jointly learning to align and translate . arXiv preprint arXiv : 1409 . 0473 ( 2014 ) 2 . Bengio , Y . , Simard , P . , Frasconi , P . : Learning long - term dependencies with gradient descent is diﬃcult . IEEE Transactions on Neural Networks 5 ( 2 ) , 157 – 166 ( Mar 1994 ) . https : / / doi . org / 10 . 1109 / 72 . 279181 3 . Dehghani , M . , Gouws , S . , Vinyals , O . , Uszkoreit , J . , Kaiser , L . : Universal trans - formers . arXiv preprint arXiv : 1807 . 03819 ( 2018 ) 4 . Hayashi , Y . , Yanagimoto , H . : Headline generation with recurrent neural network . In : New Trends in E - service and Smart Computing , pp . 81 – 96 . Springer ( 2018 ) 5 . Howard , J . , Ruder , S . : Fine - tuned language models for text classiﬁcation . arXiv preprint arXiv : 1801 . 06146 ( 2018 ) 6 . Joulin , A . , Grave , E . , Bojanowski , P . , Mikolov , T . : Bag of tricks for eﬃcient text classiﬁcation . In : Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 2 , Short Papers . vol . 2 , pp . 427 – 431 ( 2017 ) 7 . Kim , B . , Kim , H . , Kim , G . : Abstractive Summarization of Reddit Posts with Multi - level Memory Networks . In : arXiv ( 2018 ) 8 . Lei Ba , J . , Ryan Kiros , J . , Hinton , G . : Layer normalization ( 2016 ) 9 . Mikolov , T . , Sutskever , I . , Chen , K . , Corrado , G . , Dean , J . : Distributed represen - tations of words and phrases and their compositionality . In Proceedings of NIPS ( 2013 ) 10 . Putra , J . W . G . , Kobayashi , H . , Shimizu , N . : Experiment on using topic sentence for neural news headline generation ( 2018 ) 11 . Rush , A . M . , Chopra , S . , Weston , J . : A neural attention model for abstractive sentence summarization . In : Empirical Methods in Natural Language Processing . pp . 379 – 389 ( 2015 ) 12 . Sandhaus , E . : The new york times annotated corpus ldc2008t19 . dvd . Linguistic Data Consortium , Philadelphia ( 2008 ) 13 . Sennrich , R . , Haddow , B . , Birch , A . : Neural machine translation of rare words with subword units . https : / / arxiv . org / abs / 1508 . 07909 ( 2015 ) 14 . Sutskever , I . , Vinyals , O . , Le , Q . V . : Sequence to sequence learning with neural networks . https : / / arxiv . org / abs / 1409 . 3215 ( 2014 ) 15 . Tan , J . , Wan , X . , Xiao , J . : From neural sentence summarization to headline gen - eration : a coarse - to - ﬁne approach . In : Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence . pp . 4109 – 4115 . AAAI Press ( 2017 ) 16 . Vaswani , A . , Shazeer , N . , Parmar , N . , Uszkoreit , J . , Jones , L . , Gomez , A . N . , Kaiser , L . , Polosukhin , I . : Attention is all you need . In : Advances in Neural Information Processing Systems . pp . 5998 – 6008 ( 2017 )