Pipelined Backpropagation at Scale : Training Large Models without Batches Atli Kosson ∗† Vitaliy Chiley ∗† Abhinav Venigalla Joel Hestness Urs K¨oster † Cerebras Systems , Los Altos , California Abstract Parallelism is crucial for accelerating the train - ing of deep neural networks . Pipeline parallelism can provide an efﬁcient alternative to traditional data parallelism by allowing workers to specialize . Performing mini - batch SGD using pipeline paral - lelism has the overhead of ﬁlling and draining the pipeline . Pipelined Backpropagation updates the model parameters without draining the pipeline . This removes the overhead but introduces stale gradients and inconsistency between the weights used on the forward and backward passes , reduc - ing ﬁnal accuracy and the stability of training . We introduce Spike Compensation and Linear Weight Prediction to mitigate these effects . Analysis on a convex quadratic shows that both methods ef - fectively counteract staleness . We train multiple convolutional networks at a batch size of one , completely replacing batch parallelism with ﬁne - grained pipeline parallelism . With our methods , Pipelined Backpropagation achieves full accuracy on CIFAR - 10 and ImageNet without hyperparam - eter tuning . 1 . Introduction In recent years the compute requirements for training state of the art deep neural networks have rapidly increased ( Amodei & Hernandez , 2018 ) . To keep training times manageable , practitioners use increasingly parallel training setups where the workload is divided across multiple workers . This is most commonly done using data parallelism in the form of mini - batch training . This has been used to efﬁciently utilize individual accelerators as well as scaling training to large clusters of devices , usually in the form of synchronized distributed SGD ( Chen et al . , 2016 ) . Scaling training by increasing the batch size has drawbacks . Beyond a certain point , larger batch sizes do not reduce the number of steps required to train a model and thus cannot reduce training time further ( Shallue et al . , 2019 ) . * Equal contribution † Correspondence to : { atli , vitaliy , urs } @ cerebras . net . F 1 F 2 F 3 F 4 B 4 B 3 B 2 B 1 F 1 F 2 F 3 F 4 B 4 B 3 B 2 B 1 F 1 F 2 F 3 F 4 B 4 B 3 B 2 B 1 F 1 F 2 F 3 F 4 B 4 B 3 B 2 B 1 F 1 F 2 F 3 F 4 B 4 B 3 F 1 F 2 F 3 F 4 B 4 B 3 F 1 F 2 F 3 F 4 B 4 B 3 F 1 F 2 F 3 F 4 B 4 B 3 F 1 B 2 F 3 B 4 F 2 F 3 F 4 B 3 F 4 B 4 B 4 B 3 B 2 B 1 B 1 F 1 B 2 B 3 F 2 F 3 F 4 B 4 B 3 B 2 B 1 F 1 B 1 B 2 F 2 F 3 F 4 B 4 B 3 B 2 B 1 F 1 F 2 F 3 B 1 B 4 B 3 B 2 B 1 F 4 F 1 F 1 F 1 F 2 F 2 F 2 F 3 F 3 B 3 F 4 B 4 F 4 Worker Worker T i m e T i m e ⋮ ⋮ ⋮ ⋮ Figure 1 . Left : batch parallelism . Right : pipeline parallelism . We show four workers and a network that can be split into four sequen - tial transformations F 1 , F 2 , F 3 , F 4 with corresponding backwards operations B 1 , B 2 , B 3 , B 4 . The steady state is shown and for sim - plicity F and B are shown taking the same time . The processing of four inputs is highlighted in color . In pipeline parallelism workers can specialize to perform a subset of the transformations . Shallue et al . ( 2019 ) empirically show that this point varies depending on the network and dataset . Further , for a given compute budget , the range of acceptable hyperparameters ( learning rate , momentum ) can shrink with increased batch sizes . Finally , they show that tuning is necessary for good performance ; heuristically scaling the hyperparameters for large batch sizes does not always result in efﬁcient training . The cost of the tuning required for efﬁcient large batch size training could also be prohibitive . These downsides have sparked interest in alternative forms of parallelism . Pipeline parallelism is one such alternative where the model is divided sequentially into segments we call pipeline stages . Each worker is assigned to one stage and inputs proceed sequentially through the stages , similar to an assembly line ( Figure 1 ) . This form of parallelism has the advantage that each worker only performs a subset of the computation which can allow them to specialize . a r X i v : 2003 . 11666v1 [ c s . L G ] 25 M a r 2020 Pipelined Backpropagation at Scale An example of this can be seen in GPipe ( Huang et al . , 2018 ) where workers specialize by holding a subset of the model parameters . This allows GPipe to train larger models than can ﬁt on a single worker . Li & Pedram ( 2017 ) discuss other hardware advantages of such training , for example energy efﬁciency . Pipeline parallel training commonly uses a form of mini - batch SGD which sequentially feeds samples from a batch through the pipeline ( ﬁlling the pipeline ) and waits for the resulting gradients before updating the parameters ( draining the pipeline ) and processing the next batch . Filling and draining the pipeline for each update can sig - niﬁcantly lower hardware utilization when batch sizes are small compared to the number of pipeline stages ( Figure 2 ) . Pipelined backpropagation ( PB ) is a technique that avoids this overhead by updating the weights without draining the pipeline ( P´etrowski et al . , 1993 ) . This can result in an in - consistency between the weights used for the forward and backwards passes for a given sample . Even with Weight stashing ( Harlap et al . , 2018 ) , which saves the weights used on the forward pass for use on the backwards pass , the weights used to calculate the gradient may have been up - dated before the resulting gradient is applied , in which case the gradient is said to be stale . For these reasons PB may not match SGD training . Recent works have explored training networks through a combination of data parallelism and pipelined backpropaga - tion . SpecTrain ( Chen et al . , 2018 ) uses a form of weight prediction to mitigate both stale gradients and inconsistent weights . PipeMare ( Yang et al . , 2019 ) applies discrepancy correction ( a form of backward weight prediction ) to miti - gate for inconsistent weights and learning rate rescheduling ( a new form of learning rate warmup ) to help with stale gradients . Zhuang et al . ( 2019 ) propose Gradient Shrink - ing , which exponentially scales the gradients for each stage depending on the delay . Unlike prior work , we eliminate batch parallelism by having each stage process a single sample at a time . We attain parallelism using ﬁne - grained pipelined parallelism where each pipeline stage only consists of a single layer . This en - ables highly specialized workers which can have signiﬁcant hardware advantages . Our contributions are as follows : • We explore the use of ﬁne - grained pipelined backprop - agation without batch parallelism . We show that this could be a viable method for accelerating training with an update size of one . • We propose two methods , Spike Compensation and a new variant of weight prediction , Linear Weight Pre - diction , to mitigate the issues of pipelined backpropa - gation : inconsistent weights and stale gradients . • We analyze our methods and show how they can coun - teract the effects of stale gradients . We provide mathe - matically motivated settings for our methods removing Pipeline Step . . . . . . . . . P i pe li ne S t age P i pe li ne S t age P i pe li ne S t age Figure 2 . Utilization of different pipeline parallel training modes . Idle workers are depicted in red and fully utilized workers in green . Partially utilized workers ( only processing either the forward or backward pass while ﬁlling or draining the pipeline ) are shown in yellow . Top : Small batch size ﬁll and drain SGD . Middle : Large batch size ﬁll and drain SGD . Bottom : Pipelined Backpropaga - tion . The red and blue lines represent the forward and backward propagation of a single sample . The grey lines show the lengths of the delay for two of the stages . the need for hyperparameter tuning . We also show that the methods restore the beneﬁts of momentum for ill - conditioned problems with delay . • We show that with our mitigation strategies , ﬁne - grained pipelined backpropagation can be a viable al - ternative to mini - batch SGD training on standard im - age classiﬁcation benchmarks , CIFAR - 10 ( Krizhevsky et al . , 2009 ) and ImageNet ( Deng et al . , 2009 ) , without hyperparmeter tuning . 2 . Pipelined Backpropagation Pipeline parallelism is an interesting alternative or supple - ment to standard data parallelism 1 . To perform SGD training using pipeline parallelism , the same weights must be used on the forward and backwards passes . To satisfy this the pipeline needs to be empty before updating the weights . While the pipeline is ﬁlling or draining some workers sit idle which lowers utilization . The ﬁll and drain overhead is illustrated in Figure 2 . We assume our pipeline has S pipeline stages and that each stage performs a single forward and a single backward trans - formation at each time step . Each sample is processed in 2 S − 1 time steps . Performing a mini - batch SGD update with N samples takes roughly N + 2 S − 2 ≈ N + 2 S steps 2 . The work performed only corresponds to N fully utilized 1 Pipeline and data parallelism differences are analyzed in Ap - pendix A 2 This is assuming the workers are unable to speed up process - ing when they only perform one of the transformations , otherwise it may be about N + S . Pipelined Backpropagation at Scale steps so the overall utilization is upper bounded by : N N + 2 S ( 1 ) Unless N (cid:29) S this represents a signiﬁcant overhead . Pipelined backpropagation ( P´etrowski et al . , 1993 ) avoids the ﬁll and drain overhead by relaxing the constraint that the same weights must be used for the forward and backwards passes . In PB the pipeline is not drained before an update is applied , instead the parameters are updated as soon as N gradients have been obtained . This keeps all workers utilized after the pipeline is ﬁlled for the ﬁrst time ( Figure 2 ) . We assume an update size ( N ) of one 3 . We compare the weight updates of PB and SGD . We write SGD as : θ t + 1 = θ t − η ∇ L ( x t ; θ t ) ( 2 ) where θ is the set of all model weights , x t is the sample at time t , η is the learning rate , and L is the loss function . For PB we deﬁne w si to be the weights for pipeline stage s ∈ [ 0 , . . . , S − 1 ] as seen by the i th sample , x i , as it propa - gates backwards through the network . W i is deﬁned as the concatenation ( denoted by | | ) of w si for all stages 4 : W i = w 0 i | | w 1 i | | · · · | | w S − 1 i ( 3 ) The weight update for x i can then be written as : W i + 1 = W i − ηG ( x i ; F i , W i ) ( 4 ) where G approximates the gradient and F i is the network state used for the forward pass of the network 5 . For pipelined backpropagation with N = 1 : F i = w 0 i − 2 ( S − 1 ) | | w 1 i − 2 ( S − 2 ) | | · · · | | w S − 1 i ( 5 ) Equations 3 - 5 reveal that PB differs from SGD in two ways : inconsistent weights and stale gradients . Inconsistent Weights Different weight are used during the forward and backwards pass , W i (cid:54) = F i . The resulting sam - ple gradient is not the true sample gradient . The inconsis - tency is greater for earlier stages in the pipeline . If weight stashing ( Harlap et al . , 2018 ) is used to mitigate weight inconsistency the resulting update is : W i + 1 = W i − ηG ( x i ; F i , F i ) = W i − η ∇ L ( x i ; F i ) ( 6 ) Weight stashing requires the overhead of storing parameter versions along with the activations . 3 Alternatively N could be set to match some reference batch size for which known hyperparameters exist . We do not explore this . 4 This corresponds to the weights on the blue line in Figure 2 5 This corresponds to the weights on the red line in Figure 2 Stale Gradients In PB each gradient is obtained using weights from various time steps . When the gradient is ob - tained the weights have been updated . This results in stale gradients ( aka . delayed gradients ) , an issue that also occurs in asynchronous SGD training ( Lian et al . , 2015 ; Avron et al . , 2015 ) . The gradient staleness varies by stage , earlier stages suffer from a greater degree of staleness . The length of the grey lines in Figure 2 is proportional to the age of the weights , which is also a measure of the gradient delay for each stage . The depth of the pipeline determines the maximum delay . Weight stashing does not address gradient delay because F i in equation 6 is a delayed version of W i . 3 . Methods We introduce two compensation methods for pipelined back - propagation : weight prediction and spike compensation . We formulate them for SGD with momentum 6 ( SGDM ) which we write as : v t + 1 = mv t + g t ( 7 ) w t + 1 = w t − ηv t + 1 ( 8 ) where w t are weights ( parameters ) at time t , v t is the ve - locity ( sometimes called momentum ) , m is the momentum coefﬁcient , and η is the learning rate . We use g t to represent a gradient estimate for time t . The estimate can correspond to a delayed gradient , and is potentially calculated with inconsistent weights . We describe and analyze our methods for a constant delay , D , without modeling the pipeline or inconsistency . When we use the methods for PB we apply them to each stage separately , with the corresponding delay set to the number of steps between the forward and backwards passes for that stage . To simplify notation we drop the superscript s representing the stage index for w t , v t , and g t . We represent a delayed gradient with g t = G ( w t − D ) . We write the gradient as a function of the weights alone , in SGD the gradient may also depend on inputs , labels or other data . 3 . 1 . Small batch size training We deﬁne the per - worker batch size to be the number of samples that each pipeline stage processes at a time and the update - size to be the number of samples that contribute to the gradient in each update . We set both of these to one in our experiments . Larger values can potentially be used but this is outside the scope of this work . Since the optimal learning rate and momentum depend on the update size N , we scale the values used by the SGDM reference according to Chiley et al . ( 2019 ) . This corre - spond to scaling the expected update size linearly with the 6 Both methods require momentum . They can be adapted for other momentum based optimizers . Pipelined Backpropagation at Scale 0 Time 0 1 I m p u l s e R e s p o n s e 0 D Time 0 D Time 0 1 Figure 3 . Left : Momentum exponentially smooths gradients over time so the contribution of each gradient to future weights updates ( the impulse response ) is an exponentially decaying function from the time it arrives . Middle : A delayed gradient has an impulse response shifted by the delay D . The dotted line shows the baseline without delay . Right : With spike compensation ( SC D ) the impulse response has a spike ( denoted with an arrow ) and then matches the no - delay case . The size of the spike matches that of the missed updates compared to the baseline shown in light gray . batch size and scaling the momentum such that the decay per sample is the same . This allows for a fair comparison of techniques even though different update sizes are used ( Appendix H . 4 ) . The scaling rules are : m = m N / N r r , η = ( 1 − m ) N ( 1 − m r ) N r η r ( 9 ) where η r , m r and N r are the reference learning rate , mo - mentum coefﬁcient and batch size and η , m and N are the new values ( we use N = 1 ) . 3 . 2 . Spike Compensation We introduce spike compensation ( SC ) to mitigate the ef - fects of delayed gradients in pipelined backpropagation . The method uses a modiﬁed weight update which increases the contribution of the latest gradient relative to the velocity . For a delay of D this can generally be written as : g t = G ( w t − D ) ( 10 ) v t + 1 = mv t + g t ( 11 ) w t + 1 = w t − η · ( av t + 1 + bg t ) ( 12 ) where a and b are functions of the delay 7 . We refer to this form as generalized spike compensation ( GSC ) . To reason about sensible choices for a and b we can look at the contribution of each gradient over time in the no - delay case vs the delay case ( see Figure 3 ) . When a gradient g is obtained with some delay D , this gradient would already have contributed to D weight updates in the no - delay case . The total contribution of the gradient so far would have been : D − 1 (cid:88) t = 0 m t g = 1 − m D 1 − m g ( 13 ) 7 We could absorb either a or b into η but use this form to keep η consistent with other methods . This inspires our default choice of a and b for spike com - pensation which we will refer to as SC D : a = m D , b = 1 − m D 1 − m ( 14 ) For this choice , the missing weight update is applied imme - diately and the contribution of the gradient at later time steps will match that of the no - delay case . The total contribution of each gradient to the weights over the course of training is unchanged , this only changes how the gradients are applied over time . The modiﬁed weight update can equivalently be seen as approximating the velocity in the no - delay case with av t + 1 + bg t . This uses the latest gradient to estimate the gradient terms in the velocity that have not been observed yet due to the delay . Note that for a delay of zero , SC D reduces to standard SGD with momentum . 3 . 3 . Linear Weight Prediction Both the weight inconsistency and gradient delay arise from the fact that we can not access the ( future ) weights used on the backwards pass when we compute the forward pass . The goal of weight prediction is to estimate the backwards weights on the forward pass . The weights we want to esti - mate are : w t + D = w t − η D − 1 (cid:88) k = 0 v t + k + 1 ( 15 ) where D is the delay ( number of update steps between the forward and backwards passes ) . The future velocities are unknown but can be estimated by assuming a constant gradient ˆ g over the prediction horizon , i . e . the number of iterations over which the prediction is made . This gives : v t + k + 1 ≈ m k v t + 1 + ˆ g k − 1 (cid:88) i = 0 m i = m k v t + 1 + 1 − m k 1 − m ˆ g ( 16 ) which results in predicted weights : ˆ w t + D = w t − η 1 − m D 1 − m v t + 1 − η ˆ g 1 − m (cid:18) D − 1 − m D 1 − m (cid:19) ( 17 ) We have several good choices ˆ g including setting it to zero or estimating it based on recent gradients . In this work we focus on weight prediction where the direction of the velocity does not change , i . e . ˆ g is collinear with v t . We refer to this as linear weight prediction ( LWP ) . The estimate for the weights at time t and delay D can then be written in terms of past weights and velocities as : ˆ w ( t , D , T ) = w t − D − ηTv t − D = : ˆ w v ( t , D , T ) ( 18 ) Where T is a hyperparameter we call the horizon of the weight prediction . For SGDM without modiﬁcations , we Pipelined Backpropagation at Scale can equivalently write the estimate in terms of the previous weights alone : ˆ w ( t , D , T ) = w t − D + T · ( w t − D − w t − D − 1 ) = : ˆ w w ( t , D , T ) ( 19 ) When combined with spike compensation ( and potentially with other optimizers ) the predictions given by equations 18 and 19 differ . When this is the case we refer to the two types as LWP v ( velocity form ) and LWP w ( weight difference form ) , respectively . We can write the update step as : g t = G ( ˆ w ( t , D , T ) ) ( 20 ) v t + 1 = mv t + g t ( 21 ) w t + 1 = w t − ηv t + 1 ( 22 ) In the rest of this paper we use LWP D to denote LWP with our default choice of T = D . This is equivalent to choosing ˆ g = ( 1 − m ) v t + 1 in equation 17 which would result in a constant velocity . This form is closely related to the weight prediction used in SpecTrain ( Chen et al . , 2018 ) which extends the prediction horizon and also predicts weights on the backwards pass ( see Appendix C ) . 3 . 4 . Combined Mitigation Spike compensation and weight prediction can be combined resulting in the following update step : g t = G ( ˆ w ( t , D , T ) ) ( 23 ) v t + 1 = mv t + g t ( 24 ) w t + 1 = w t − η · ( av t + 1 + bg t ) ( 25 ) where , as before , T is the horizon of the weight prediction and a and b are the coefﬁcients for the spike compensation . When combined with spike compensation we have : ˆ w v ( t , D , T ) (cid:54) = ˆ w w ( t , D , T ) ( 26 ) In the combination ˆ w w ( t , D , T ) can be interpreted as using spike compensation to estimate the velocity used in the weight prediction 8 . 3 . 5 . Analysis for a Convex Quadratic In this section we analyze the optimization of a convex quadratic loss with gradient delay . We ﬁnd that our methods : • Improve convergence for large condition numbers • Allow higher learning rates for large momentum values • Restore the beneﬁts of momentum for poorly condi - tioned losses 8 This weight prediction also corresponds to a different choice of ˆ g in equation 17 using the most recent gradient estimate . We follow a similar approach as ( Odonoghue & Candes , 2015 ; Goh , 2017 ) and write the loss in terms of an eigenbasis of the quadratic as : L ( φ ) = φ T Λ φ , Λ = diag ( λ ( 1 ) , . . . , λ ( N ) ) ( 27 ) where φ = [ φ ( 1 ) , . . . , φ ( N ) ] T correspond to the parameters being optimized and λ ( 1 ) ≥ . . . ≥ λ ( N ) > 0 are the eigen - values of the quadratic . As shown in e . g . Goh ( 2017 ) , any positive deﬁnite quadratic can be written in this form through a coordinate transformation . Since Λ is diagonal , each coordinate of the gradient ∇ φ L ( φ ) = Λ φ is indepen - dent of other coordinates . This allows us to analyze the convergence for each coordinate separately . For simplicity we assume that the gradient is deterministic . A similar anal - ysis would hold for the expected values of φ if each gradient sample was assumed to be noisy but unbiased . In Appendix D we derive the state transition equations for SGDM with delay and our methods . Since the gradient here is linear , and the coordinates are independent , inserting it into the transition equations results in a linear recurrence relation for each coordinate . For component φ ( k ) , with as - sociated eigenvalue λ = λ ( k ) , the characteristic polynomial for the recurrence relation of each method is : GDM : p ( z ) = z τ + 1 − ( 1 + m ) z τ + mz τ − 1 − ηλ ( 28 ) GSC : p ( z ) = z τ + 2 − ( 1 + m ) z τ + 1 + mz τ + ηλ · ( a + b ) z − ηλmb ( 29 ) LWP : p ( z ) = z τ + 2 − ( 1 + m ) z τ + 1 + mz τ + ηλ · ( 1 + T ) z − ηλT ( 30 ) LWP w + GSC : p ( z ) = z τ + 3 − ( 1 + m ) z τ + 2 + mz τ + 1 + ηλ · ( a + b ) ( T + 1 ) z 2 − ηλ · ( ( T + 1 ) mb + T · ( a + b ) ) z + ηλTmb ( 31 ) where GDM stands for gradient descent with momentum , GSC is general spike compensation , LWP is linear weight prediction , z parameterizes the polynomials and other sym - bols have the same meaning as in Section 3 . 4 . Note that since the gradient is linear , GSC and LWP are equivalent for a certain choice of a , b and T as shown in Appendix D . Even though this is the case , the characteristic polynomial of the combination cannot be obtained from either method . Linear recurrence relations have a well known solution in terms of the roots of the corresponding characteristic equa - tion . The resulting sequence for component φ ( i ) , corre - sponding to the characteristic polynomial p ( z ) with roots r 1 , . . . , r n , can be written as : φ ( i ) t = n (cid:88) k = 1 q k ( t ) r tk ( 32 ) Pipelined Backpropagation at Scale 1 10 5 1 10 4 1 10 3 1 10 2 1 10 1 0 m o m e n t u m GDM for D = 0 GDM for D = 1 SC D for D = 1 10 9 10 6 10 3 10 0 1 10 5 1 10 4 1 10 3 1 10 2 1 10 1 0 m o m e n t u m Nesterov for D = 0 10 9 10 6 10 3 10 0 LWP D for D = 1 10 9 10 6 10 3 10 0 LWP wD + SC D for D = 1 0 1 10 1 1 10 2 1 10 3 1 10 4 1 10 5 1 10 6 | r m a x | Figure 4 . These plots show the magnitude of the dominant root of the characteristic polynomials given in equations 28 - 31 as a function of the normalized rate ηλ and the momentum m . The two leftmost plots show zero delay baselines and the other plots use a delay of D = 1 . The blacked out region has roots with magnitudes larger than one and is therefore unstable . For a delay of one , Nesterov momentum is equivalent to spike compensation , but for larger delays this does not hold and Nesterov is only marginally better than GDM . where q k ( t ) is a polynomial . The order of the polynomial is one less than the multiplicity of the corresponding root r k . The coefﬁcients of the polynomials are determined by the initial conditions . For our analysis we assume that all components start with some error and look at the rate of convergence in the limit t → ∞ . A component φ ( i ) converges to the optimal value of 0 if | r max | = max k ( | r k | ) < 1 . In the limit , the slowest term of equation 32 will dominate so the error for this component , ε ( i ) will be : ε ( i ) t = | φ ( i ) t − 0 | ∝ | r max | t ( 33 ) The overall rate of convergence is determined by the slowest component . The slowest component can depend on the roots of high order polynomials , which are difﬁcult to determine analytically , so we turn to computational analysis . For a given delay , we can compute the roots of the characteristic polynomials 28 - 31 , including | r max | , as a function of the normalized rate λη and the momentum m . Figure 4 shows heatmaps of | r max | for each method for a delay of one and our default values of a , b and T . Note that the region of stability is signiﬁcantly reduced by the delay , especially for large momentum values . Our compensation methods counteract this , allowing larger learning rates to be used for high momentum values . SC D in particular strictly increases the region of stability , the other methods slightly decrease it for small momentum coefﬁcients . Figure 4 also allow us to reason about more than a single component at a time . Let’s assume that we have multiple components , a condition number κ = λ 1 / λ N and a dense spectrum of eigenvalues between λ 1 and λ N . The same learning rate η and momentum m are used for all compo - nents . The overall convergence rate is determined by the 10 0 10 1 10 2 10 3 10 4 10 5 10 6 Condition Number 10 0 10 1 10 2 10 3 10 4 10 5 10 6 M i n i m u m H a l f li f e GDM D = 1 SC D D = 1 LWP D D = 1 LWP wD + SC D D = 1 GDM D = 0 Figure 5 . The half - life of the error as a function of the conditioning number when optimizing a convex quadratic with delay D . All methods improve the convergence rate , LWP wD + SC D performs best . component with the largest | r max | . This corresponds to the largest value in a horizontal line segment between ηλ N and ηλ 1 on the root heatmaps . With a log scale the line segment has a constant length determined by κ . Figure 5 shows the convergence speed as a function of κ for the different methods . We measure the half - life − ln 2 / ln | r ∗ | where | r ∗ | is obtained by ﬁnding the low - est max magnitude over all intervals of sufﬁcient length . The methods improve the rate of convergence compared to the delayed baseline . The combination performs the best which also holds for larger delays as is shown in Figure 6 . As mentioned earlier , GSC and LWP can be equivalent for a convex quadratic . The fact that LWP D slightly outperforms SC D indicates that our selection of T = D is better than the selection of a and b as given in equation 14 in this case . Fig - ure 7 shows the effect of different values of T . It shows that values close to T = 2 D are optimal but do not outperform the combination LWP wD + SC D . This seems to indicate that “overcompensating” for the delays , by predicting weights further out in LWP or equivalently by using larger spikes in Pipelined Backpropagation at Scale 0 2 4 6 8 10 12 14 16 Delay 10 0 10 1 10 2 10 3 10 4 M i n i m u m H a l f li f e GDM LWP D LWP wD + SC D Figure 6 . The optimal half - life of the error for different delays when optimizing a convex quadratic with κ = 10 3 . 1 10 5 1 10 4 1 10 3 1 10 2 1 10 1 1 10 0 momentum 10 2 10 3 10 4 M i n i m u m H a l f li f e LWP T = 0 LWP T = 3 LWP T = 5 LWP T = 10 LWP T = 20 LWP wD + SC D Figure 7 . The effect of momentum and the horizon T for weight prediction on the optimal half - life when optimizing a convex quadratic with κ = 10 3 for a delay D = 5 . SC , seems to produce better optimization trajectories . The resulting root heatmaps resemble the ones for the no - delay Nesterov baseline ( see LWP wD + SC D in Figure 4 , LWP with T = 2 D looks similar ) . Note that adding Nesterov to the delay is not sufﬁcient to get this effect . In Appendix E we show the effect of extended horizons for both the convex quadratic and a neural network . Figure 7 also reveals that without mitigation ( T = 0 is equal to GDM with delay ) , the optimal momentum is zero . In the no - delay case the optimal momentum is given by m = ( ( √ κ − 1 ) / ( √ κ + 1 ) ) 2 ( Zhang & Mitliagkas , 2017 ) which increases with the condition number . Our compensa - tion methods restore the beneﬁts of momentum for high con - dition numbers . Overall the combined mitigation performs the best . Extended horizons for LWP or the equivalent coef - ﬁcients for GSC also outperform our default choice in this case but are unable to match the combination LWP wD + SC D . 4 . Experiments To efﬁciently run small batch , ﬁne - grained pipelined back - propagation on a GPU , we developed a framework described in Appendix G . 1 . The majority of experiments are done with the pre - activation residual networks proposed by He et al . ( 2016b ) . To enable training at a batch size of one we replace batch normalization ( Ioffe & Szegedy , 2015 ) with 0 50 100 150 200 250 Epoch 82 84 86 88 90 V a li d a t i o n A cc u r a c y ( % ) Training Method Val Accuracy SGDM 90 . 6 % PB 90 . 4 % PB + LWP D 90 . 7 % PB + SC D 90 . 8 % PB + LWP v D + SC D 90 . 9 % Figure 8 . CIFAR10 ResNet20 validation accuracy ( ﬁve run mean ) . 0 10 20 30 40 50 60 70 80 90 Epoch 50 55 60 65 70 75 V a li d a t i o n A cc u r a c y ( % ) Training Method Val Accuracy SGDM 75 . 7 % PB 75 . 1 % PB + LWP D 75 . 2 % PB + SC D 75 . 6 % PB + LWP vD + SC D 75 . 8 % Figure 9 . ImageNet ResNet50 validation accuracy ( single run ) . group normalization ( Wu & He , 2018 ) 9 . Hyperparameters are adopted from He et al . ( 2016a ) and scaled for batch size one training ( Section 3 . 1 ) . We combine each convolu - tion layer and its associated normalization and non - linearity into a single pipeline stage . In our implementation the sum nodes between residual blocks also become pipeline stages . For our mitigation methods we use the default hyperparam - eters for LWP D and SC D without further tuning . The results can potentially be improved with a hyperparameter search . Other experiment details as well as run to run variability can be seen in Appendix H . Pipelined backpropogation without mitigation suffers from a loss of accuracy compared to the SGDM baseline ( Figures 8 and 9 , Table 1 ) . The size of degradation depends on the depth of the pipeline ( Table 1 ) . This is expected given longer pipelines produce larger delays . Mitigating for the delay improves the performance of PB training . For relatively shallow networks , PB training has minimal degradation . All mitigation methods tested fully recover the SGDM baseline accuracy ( Figure 8 ) . For CI - FAR10 ResNet20 training , PB + LPW vD + SC D produces the best accuracy 10 . 9 In ResNets , batch normalization slightly outperforms group normalization so the results are not directly comparable to the baselines found in ( He et al . , 2016b ; a ) . 10 In our training setup LPW vD + SC D outperformed LPW wD + SC D ( Appendix H . 5 ) . Pipelined Backpropagation at Scale Table 1 . CIFAR10 ﬁnal validation accuracy ( ﬁve run mean ) for ResNet ( RN ) and VGG training . N ETWORK S TAGES SGDM PB PB + LPW vD + SC D VGG11 29 91 . 2 90 . 8 91 . 1 VGG13 33 92 . 6 92 . 6 92 . 6 VGG16 39 92 . 2 92 . 1 92 . 4 RN20 34 90 . 6 90 . 4 90 . 9 RN32 52 91 . 7 91 . 5 92 . 0 RN44 70 92 . 2 91 . 7 92 . 2 RN56 88 92 . 4 91 . 9 92 . 5 RN110 169 92 . 8 91 . 8 92 . 4 When training deeper networks , such as ImageNet ResNet50 with 78 pipeline stages ( Figure 9 ) , PB training incurs an ac - curacy loss of 0 . 6 % 11 . LPW D and SC D are not able to fully recover the baseline accuracy but LPW vD + SC D produces competitive results . PB training of CIFAR10 ResNet110 leads to an accuracy which is 1 . 0 % worse than SGD training . Although LPW vD + SC D recovers most of the accuracy loss it does not fully close the gap . Weight stashing does not help with PB training in our setting ( Table 2 in Appendix B ) . While SpecTrain works well in training CIFAR networks , it still exhibits a 0 . 4 % accuracy degradation on ImageNet Training ( Appendix C . 1 ) . Even without any hyperparameter tuning , PB + LPW vD + SC D mostly produces results which are competitive to SGD train - ing for both CIFAR and ImageNet . Where LPW vD + SC D is not sufﬁcient , hyperparameter tuning , a learning rate warmup , or additional delay mitigation methods can po - tentially help recover full accuracy . 5 . Discussion Pipelined backpropagation training works well for shallow networks but does not perform as well as SGD for deeper networks without mitigation . The pipeline geometry deter - mines the number of steps between the forward and back - ward passes which cause gradient delay and weight incon - sistency . In Appendix B we explore the effects of weight inconsistency and ﬁnd that it is insigniﬁcant in our setting . In cases where weight inconsistency is an issue , weight stashing or similar techniques ( e . g . discrepancy correction from Yang et al . 2019 ) can be applied . The effect of the delays depends on the total change in the local loss surface over the course of the delay . For a small change , a delayed gradient is roughly equal to the true non - delayed gradient and is therefore unlikely to have an adverse impact . The change in the model parameters , which also causes the weight inconsistency , is indicative of the change in the local loss surface . The effects of the delay may 11 Wu & He ( 2018 ) report an accuracy of 75 . 9 % . They do this by extending and modifying the learning rate schedule we used which we adopted from ( He et al . , 2016a ) . therefore depend on the learning rate , phase of training , etc . Since the model parameters usually change most rapidly at the start of training , a learning rate warmup may help stabilize PB training . Such methods can be combined with our mitigation strategies to improve performance . Using a small per - worker batch size decreases the length of the delays when measured in number of samples . If the learning rate is adjusted to keep the contribution of each sample the same , this reduces the total change in model parameters over the course of the delay and thus the adverse effects of the delay . The use of small batch size training therefore helps mitigate the delays of ﬁne - grained PB . Small batch size training prevents the use of batch normal - ization ( BN ) so we opted to use group normalization ( GN ) . In additional exploratory experiments ( not shown ) we ob - served that BN seems to signiﬁcantly decrease the effects of delayed gradients compared to GN . The use of other small batch size alternatives to BN such as Online Normal - ization ( Chiley et al . , 2019 ) , Weight Standardization ( Qiao et al . , 2019 ) or Filter Response Normalization ( Singh & Kr - ishnan , 2019 ) may boost delay tolerance . Optimizers such as ADAM may also increase delay tolerance . We introduced spike compensation and linear weight predic - tion to mitigate the effects of delays in PB . These methods require momentum to be effective . When we scale the hy - perparameters for small batch size training , we keep the half - life of the momentum the same when measured in the number of samples . This results in a very high momentum coefﬁcient which we ﬁnd works well and boosts the per - formance of our mitigation methods ( Appendix F ) . Other works claim that momentum is not necessary for small batch size training ( Smith & Le , 2018 ) . However momentum may still make it easier to mitigate the effects of the delays and enable the use of existing hyper - parameter settings . In Sec - tion 3 . 5 we show our methods restore some of the traditional advantages of momentum in the delayed setting . We ﬁnd overcompensating for the delays can result in better optimization trajectories ( Section 3 . 5 , Appendix E ) . One way to do this is to combine spike compensation and weight prediction . We show this combination enables training mod - erately deep neural networks such as ResNet50 for Ima - geNet without a loss of accuracy . Overcompensating for large delays , like those in ResNet110 , can adversely impact performance . In such cases using one method can work better than the combination ( Appendix E ) . With mitigation , PB is a promising alternative to batch par - allel training . It overcomes the ﬁll and drain overhead of traditional pipeline parallel SGD training . This could en - able the design of highly efﬁcient pipeline parallel hardware accelerators that beneﬁt from specialized workers . Pipelined Backpropagation at Scale Acknowledgments We are grateful to Vithu Thangarasa and Ron Estrin for their feedback on the manuscript . We thank Min Xu for his help with the dataloader used in our GProp experiments and Chuan - Yung Tsai for insightful discussions . References Amodei , D . and Hernandez , D . AI and com - pute , 2018 . URL https : / / openai . com / blog / ai - and - compute / . Avron , H . , Druinsky , A . , and Gupta , A . Revisiting asynchronous linear solvers : Provable convergence rate through randomization . Journal of the ACM ( JACM ) , 62 ( 6 ) : 51 , 2015 . Chen , C . - C . , Yang , C . - L . , and Cheng , H . - Y . Efﬁcient and robust parallel dnn training through model parallelism on multi - gpu platform . ArXiv , abs / 1809 . 02839 , 2018 . Chen , J . , Monga , R . , Bengio , S . , and J ´ ozefowicz , R . Revisiting distributed synchronous SGD . CoRR , abs / 1604 . 00981 , 2016 . Chetlur , S . , Woolley , C . , Vandermersch , P . , Cohen , J . , Tran , J . , Catanzaro , B . , and Shelhamer , E . cuDNN : Efﬁcient primitives for deep learning . arXiv preprint arXiv : 1410 . 0759 , 2014 . Chiley , V . , Sharapov , I . , Kosson , A . , Koster , U . , Reece , R . , Samaniego de la Fuente , S . , Subbiah , V . , and James , M . Online normalization for training neural networks . In Advances in Neural Information Processing Systems 32 , pp . 8431 – 8441 . Curran Associates , Inc . , 2019 . Dauphin , Y . N . and Schoenholz , S . MetaInit : Initializing learning by learning to initialize . In Advances in Neural Information Processing Systems , pp . 12624 – 12636 , 2019 . Deng , J . , Dong , W . , Socher , R . , Li , L . - J . , Li , K . , and Fei - Fei , L . ImageNet : A Large - Scale Hierarchical Image Database . In CVPR09 , 2009 . Fu , C . - Y . pytorch - vgg - cifar10 , May 2019 . URL https : / / github . com / chengyangfu / pytorch - vgg - cifar10 . Giladi , N . , Nacson , M . S . , Hoffer , E . , and Soudry , D . At stability’s edge : How to adjust hyperparameters to pre - serve minima selection in asynchronous training of neural networks ? arXiv preprint arXiv : 1909 . 12340 , 2019 . Goh , G . Why momentum really works . Distill , 2017 . doi : 10 . 23915 / distill . 00006 . Hakimi , I . , Barkai , S . , Gabel , M . , and Schuster , A . Taming momentum in a distributed asynchronous environment . arXiv preprint arXiv : 1907 . 11612 , 2019 . Harlap , A . , Narayanan , D . , Phanishayee , A . , Seshadri , V . , Devanur , N . R . , Ganger , G . R . , and Gibbons , P . B . PipeDream : Fast and efﬁcient pipeline parallel dnn train - ing . ArXiv , abs / 1806 . 03377 , 2018 . He , K . , Zhang , X . , Ren , S . , and Sun , J . Deep residual learning for image recognition . In The IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , June 2016a . He , K . , Zhang , X . , Ren , S . , and Sun , J . Identity mappings in deep residual networks . In European conference on computer vision , pp . 630 – 645 . Springer , 2016b . Huang , Y . , Cheng , Y . , Chen , D . , Lee , H . , Ngiam , J . , Le , Q . V . , and Chen , Z . GPipe : Efﬁcient training of gi - ant neural networks using pipeline parallelism . ArXiv , abs / 1811 . 06965 , 2018 . Ioffe , S . and Szegedy , C . Batch normalization : Accelerating deep network training by reducing internal covariate shift . In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37 , ICML15 , pp . 448456 . JMLR . org , 2015 . Krizhevsky , A . , Hinton , G . , et al . Learning multiple layers of features from tiny images , 2009 . Li , Y . and Pedram , A . CATERPILLAR : coarse grain recon - ﬁgurable architecture for accelerating the training of deep neural networks . CoRR , abs / 1706 . 00517 , 2017 . Lian , X . , Huang , Y . , Li , Y . , and Liu , J . Asynchronous parallel stochastic gradient for nonconvex optimization . In Advances in Neural Information Processing Systems , pp . 2737 – 2745 , 2015 . Odonoghue , B . and Candes , E . Adaptive restart for accel - erated gradient schemes . Foundations of computational mathematics , 15 ( 3 ) : 715 – 732 , 2015 . Paszke , A . , Gross , S . , Massa , F . , Lerer , A . , Bradbury , J . , Chanan , G . , Killeen , T . , Lin , Z . , Gimelshein , N . , Antiga , L . , et al . Pytorch : An imperative style , high - performance deep learning library . In Advances in Neural Information Processing Systems , pp . 8024 – 8035 , 2019 . P´etrowski , A . , Dreyfus , G . , and Girault , C . Performance analysis of a pipelined backpropagation parallel algo - rithm . IEEE transactions on neural networks , 4 6 : 970 – 81 , 1993 . Qiao , S . , Wang , H . , Liu , C . , Shen , W . , and Yuille , A . L . Weight standardization . CoRR , abs / 1903 . 10520 , 2019 . Pipelined Backpropagation at Scale Shallue , C . J . , Lee , J . , Antognini , J . , Sohl - Dickstein , J . , Frostig , R . , and Dahl , G . E . Measuring the effects of data parallelism on neural network training . Journal of Machine Learning Research , 20 ( 112 ) : 1 – 49 , 2019 . Simonyan , K . and Zisserman , A . Very deep convolu - tional networks for large - scale image recognition . arXiv preprint arXiv : 1409 . 1556 , 2014 . Singh , S . and Krishnan , S . Filter response normalization layer : Eliminating batch dependence in the training of deep neural networks . arXiv preprint arXiv : 1911 . 09737 , 2019 . Smith , S . L . and Le , Q . V . A bayesian perspective on gener - alization and stochastic gradient descent . In International Conference on Learning Representations , 2018 . Wu , Y . and He , K . Group normalization . In The Euro - pean Conference on Computer Vision ( ECCV ) , September 2018 . Yang , B . , Zhang , J . , Li , J . , R ´ e , C . , Aberger , C . R . , and De Sa , C . Pipemare : Asynchronous pipeline parallel DNN training . arXiv preprint arXiv : 1910 . 05124 , 2019 . Zhang , H . , Dauphin , Y . N . , and Ma , T . Fixup initialization : Residual learning without normalization . In 7th Inter - national Conference on Learning Representations , ICLR 2019 , pp . 1 – 16 , 2019 . Zhang , J . and Mitliagkas , I . Yellowﬁn and the art of mo - mentum tuning . arXiv preprint arXiv : 1706 . 03471 , 2017 . Zhuang , H . , Wang , Y . , Liu , Q . , and Lin , Z . Fully decoupled neural network learning using delayed gradients . ArXiv , abs / 1906 . 09108 , 2019 . Pipelined Backpropagation at Scale A . Batch Parallel vs Pipeline Parallel Computation Pipeline parallelism differs from batch parallelism in several ways : • The training memory requirements differ . In both cases we assume an L layer network trained with W work - ers . During neural network training , the activations of many layers must be stored for the gradient calculation . For batch parallelism the activation memory required is O ( LW ) . To compute the backwards pass , each worker has to store activations for roughly every layer . In the pipeline parallel setting , each worker is responsible for storing the activations of approximately L / W lay - ers . The ﬁrst worker must store its activations for 2 W steps . The second worker needs to keep activations for 2 ( W − 1 ) steps and so on . The total activation mem - ory comes out to be approximately the same , O ( LW ) , however the per worker memory requirements can be very different . Pipeline parallelism generally requires less memory for storing model parameters potentially requiring only a single copy of each parameter . Unless special methods are used , batch parallelism may need to keep W copies of the model . • The communication pattern is different . In pipeline parallelism each worker sends activations and the cor - responding gradients to their neighbors . In distributed mini - batch training every worker must send the gra - dients for all model parameters and receive updated values after every batch . The bandwidth requirements in each case depend on the exact model used , the batch size , as well as other factors . • Both pipeline parallel training and synchronized dis - tributed batch parallel training can suffer from worker balancing bottlenecks . When using pipeline paral - lelism , care must be taken to balance the throughput of all workers since the overall speed is determined by the slowest worker . This load balancing issue could be handled in software ( Harlap et al . , 2018 ) without requiring users to manually specify the model division . In synchronized distributed SGD care must be taken to balance the throughput and master node communica - tion of all workers since the overall speed is determined by the slowest worker . • Batch normalization ( Ioffe & Szegedy , 2015 ) requires batch parallelism . In our work we are interested in replacing batch parallelism with ﬁne - grained pipeline parallelism . We therefore operate at a per - worker batch size of one which does not work well with Batch Nor - malization . Newer normalization techniques such as Group Normalization ( Wu & He , 2018 ) , Weight Stan - dardization ( Qiao et al . , 2019 ) , Filter Response Normal - ization ( Singh & Krishnan , 2019 ) and Online Normal - ization ( Chiley et al . , 2019 ) are alternative normaliza - 0 1 2 3 4 5 6 7 8 Delay [ batches of 32 ] 76 78 80 82 84 86 88 90 92 F i n a l V a li d a t i o n A cc u r a c y Forward Delay Only Consistent Delay Figure 10 . The effect of weight inconsistency on the ﬁnal valida - tion accuracy of CIFAR10 ResNet20 ( with Group Normalization ) for different delays . Consistent Delay uses the same old version of the weights for both the forward and backward passes . This pro - duces delayed gradients . Forward Delay Only uses old versions of the weights on the forward pass and current weights on the back - wards pass , resulting in weight inconsistency . Delayed gradients result in a loss of ﬁnal accuracy . Adding weight inconsistency only incurs additional degradation for large delays . tion techniques which work well and can be used with small batch sizes . Alternatively initialization methods can be used to enable training without normalization ( Zhang et al . , 2019 ; Dauphin & Schoenholz , 2019 ) . B . Inconsistent Weights vs Stale Gradients In pipelined backpropagations gradients are delayed and computed with inconsistent weights . This can lead to ac - curacy degradation and instability . In this section we in - vestigate the relative importance of the effects . We do this by comparing training with delayed gradients using either inconsistent or consistent weights . In Appendix G . 2 we describe how we can simulate this in PyTorch ( Paszke et al . , 2019 ) without using pipelined backpropagation . Figure 10 shows the effects of delay on the ﬁnal accuracy of CIFAR10 ResNet20 training with or without inconsis - tent weights . As can be seen , even modest delays affect the ﬁnal accuracy of training . Weight inconsistency does not cause an additional loss of accuracy for small delays but causes a rapid loss of accuracy beyond a certain delay . This transition point where weight inconsistency starts to affect training will depend on the dataset and architecture . Harlap et al . ( 2018 ) and Chen et al . ( 2018 ) make oppos - ing claims about the effect of weight inconsistency . Harlap et al . ( 2018 ) introduce weight stashing to ﬁx weight in - consistency and claim its use is necessary for convergence . Chen et al . ( 2018 ) show that weight stashing has no effect on training in their experiments so it should not be used to avoid memory overhead . Our results suggest that the effects of weight inconsistency depend on the magnitude of delays reconciling the two claims . Pipelined Backpropagation at Scale Table 2 . CIFAR10 ﬁnal validation accuracy ( mean ± std . dev of ﬁve runs ) with and without weight stashing for ResNet ( RN ) and VGG training . N ETWORK SGDM PB PB + WS VGG11 91 . 16 ± 0 . 19 90 . 83 ± 0 . 20 90 . 93 ± 0 . 12 VGG13 92 . 57 ± 0 . 15 92 . 59 ± 0 . 15 92 . 30 ± 0 . 24 VGG16 92 . 24 ± 0 . 19 92 . 06 ± 0 . 21 59 . 31 ± 45 . 01 12 RN20 90 . 63 ± 0 . 31 90 . 44 ± 0 . 24 90 . 36 ± 0 . 06 RN32 91 . 68 ± 0 . 23 91 . 46 ± 0 . 09 91 . 40 ± 0 . 28 RN44 92 . 19 ± 0 . 14 91 . 71 ± 0 . 25 91 . 72 ± 0 . 14 RN56 92 . 39 ± 0 . 20 91 . 89 ± 0 . 40 91 . 82 ± 0 . 19 RN110 92 . 77 ± 0 . 22 91 . 81 ± 0 . 15 91 . 92 ± 0 . 33 We also investigate the effect of weight inconsistency in our ﬁne - grained pipelined backpropagation setup . Table 2 compares PB training with and without weight stashing . The results suggest that weight stashing is not beneﬁcial in our setup so we do not use it in other experiments . This indicates that weight inconsistency is likely not an issue and the accuracy losses of PB primarily stem from the gradient delay . As mentioned in the discussion section , the small batch sizes we use combined with the hyperparameter scal - ing may reduce the effects of the delay . For larger batch sizes weight inconsistency may be a bigger issue . C . Forms of Weight Prediction The goal of weight prediction is to estimate future weights to combat gradient delay and weight inconsistency . Linear Weight Prediction ( LWP ) gives a general form for predicting the network state T steps into the future by using the velocity . In Pipelined Backpropagation the delay varies for different stages . By default ( LPW D ) we set T equal to the delay for every stage ( see red arrows in Figure 11 ) . Other works have proposed related forms of weight prediction . LWP is closely related to the weight prediction proposed in SpecTrain ( Chen et al . , 2018 ) . SpecTrain extends the pre - diction horizon such that all stages predict to the same time step . This form of time synchronization is ﬁrst described by Harlap et al . ( 2018 ) as Vertical Sync . The forward prediction horizon is depicted in green in Figure 11 . With the extended prediction horizon , SpecTrain must also predicts weights on the backwards pass to address inconsistency . The prediction horizon for the backward pass weights is depicted in blue in Figure 11 . This can be seen as using a stage dependent extended prediction horizon ( Appendix E ) . Discrepancy correction ( Yang et al . , 2019 ) can be seen as a form of weight prediction . Whereas LWP and SpecTrain predict weights into the future to mitigate for gradient delay and weight inconsistency , PipeMare estimates the weights 12 Unstable training . Pipeline Step P i pe li ne S t age Figure 11 . Pipelined Backpropagation creates a discrepancy be - tween the forward and backwards weights . We use weight pre - diction to estimate the backwards weights for use during the for - ward pass . Our prediction horizon is shown in red for two of the stages . SpecTrain ( Chen et al . , 2018 ) uses an extended horizon ( Appendix E ) and re - predicts on the backwards pass . SpecTrain’s forward and backward weight predictions are depicted in green and blue respectively . used on the forward pass during the backward pass . This can only deal with weight inconsistency , but potentially provides a more accurate prediction . Discrepancy correction uses a separate exponential tracker for their prediction . LWP uses the optimizer velocity directly . In Appendix B we show that weight inconsistency is not a signiﬁcant issue in our setting so we primarily focus on mitigating the effects of gradient delay . DANA ( Hakimi et al . , 2019 ) is another variant of weight prediction that has been used in the ASGD setting but is not directly applicable to Pipelined Backpropagation . C . 1 . SpecTrain Experimental Results Table 3 compares the ﬁnal validation accuracy of CIFAR10 training using SpecTrain and our methods . Although Spec - Train does very well in these settings , it is not able to recover SGDM reference accuracy on ImageNet training unlike the combined method LPW vD + SC D . D . State Transition Equations In order to analyze and compare our methods , we view the optimization as a dynamical system in terms of its state tran - sition equation . A similar approach is used in ( Odonoghue & Candes , 2015 ; Goh , 2017 ; Giladi et al . , 2019 ) . We assume that ¯ L ( w t ) is the underlying loss function we are trying to minimize where w t are the weights at time t . For neural networks , ¯ L could be the mean training loss , the expected loss over all training samples . We assume that for a given sample or time step , the gradient with respect to the weights is ∇ ¯ L ( w t ) + R where R = R ( w t ) is a random variable . The expectation of R ( over all samples ) is assumed to be zero . We are interested in comparing the dynamics of delayed SGDM , weight prediction , spike compensation and the com - Pipelined Backpropagation at Scale Table 3 . CIFAR10 ( C10 ) validation accuracy ( mean ± std . dev of ﬁve runs ) and ImageNet ( I1k ) validation accuracy ( single run ) comparing SpecTrain and our methods for ResNet ( RN ) and VGG training . N ETWORKS ( D ATASET ) SGDM PB PB + LPW vD + SC D PB + S PEC T RAIN VGG13 ( C10 ) 92 . 57 ± 0 . 15 92 . 59 ± 0 . 15 92 . 56 ± 0 . 14 92 . 49 ± 0 . 12 RN20 ( C10 ) 90 . 63 ± 0 . 31 90 . 44 ± 0 . 24 90 . 92 ± 0 . 25 90 . 93 ± 0 . 09 RN56 ( C10 ) 92 . 39 ± 0 . 20 91 . 89 ± 0 . 40 92 . 48 ± 0 . 11 92 . 72 ± 0 . 10 RN50 ( I1 K ) 75 . 7 75 . 1 75 . 8 75 . 3 bined mitigation . These can all be seen as special cases of the combined mitigation given in Section 3 . 4 for the ap - propriate choice of a , b and T . The velocity form of the combined mitigation , LWP v + SC , results in a complicated state transition equation which can not be easily analyzed without further simpliﬁcations . The velocity form can be approximated with the weight difference form , LWP w + SC . This form is simple to analyze so we use it for the rest of the analysis . We analyze the systems in expectation and do not try to estimate the variance . Let ¯ w t and ¯ v t be the expected weights and velocity at time t . We can then write the expected state update for the combined mitigation at time t in terms of previous expected values as : ¯ v t + 1 = E [ m ¯ v t + g t ] = m ¯ v t + ¯ g t ( 34 ) ¯ w t + 1 = E [ ¯ w t − η · ( a ¯ v t + 1 + bg t ) ] = ¯ w t − η · ( a ¯ v t + 1 + b ¯ g t ) ( 35 ) where a , b are the coefﬁcients for general spike compensa - tion and ¯ g t : = E [ g t ] is the expected gradient arriving at time t . This gradient is calculated using weight prediction with horizon T from weights delayed by D time steps : ¯ g t = E [ ∇ ¯ L ( ¯ w t − D + T · ( ¯ w t − D − ¯ w t − D − 1 ) ) + R ] = ∇ ¯ L ( ¯ w t − D + T · ( ¯ w t − D − ¯ w t − D − 1 ) ) ( 36 ) We can isolate ¯ v t + 1 from equation 35 : ¯ v t + 1 = − 1 ηa ( ¯ w t + 1 − ¯ w t ) − b a ¯ g t ( 37 ) Shifting the time index we obtain an expression for ¯ v t which we can insert into equation 34 : ¯ v t + 1 = − m ηa ( ¯ w t − ¯ w t − 1 ) − bm a ¯ g t − 1 + ¯ g t ( 38 ) Combining equations 35 , 36 and 38 we obtain a state transi - tion equation in terms of the expected weights without the velocity : ¯ w t + 1 = ( 1 + m ) ¯ w t − m ¯ w t − 1 ( 39 ) − η · ( a + b ) ∇ ¯ L ( ( T + 1 ) ¯ w t − D − T ¯ w t − D − 1 ) ) + ηmb ∇ ¯ L ( ( T + 1 ) ¯ w t − D − 1 − T ¯ w t − D − 2 ) ) By inserting appropriate values for T , a and b we can obtain the state transition equations for general spike compensation ( GSC , T = 0 ) , linear weight prediction ( LWP , a = 1 , b = 0 ) and SGDM with delay ( a = 1 , b = 0 , T = 0 ) : SGDM : ¯ w t + 1 = ( 1 + m ) ¯ w t − m ¯ w t − 1 ( 40 ) − η ∇ ¯ L ( ¯ w t − D ) GSC : ¯ w t + 1 = ( 1 + m ) ¯ w t − m ¯ w t − 1 ( 41 ) − η · ( a + b ) ∇ ¯ L ( ¯ w t − D ) + ηmb ∇ ¯ L ( ¯ w t − D − 1 ) LWP : ¯ w t + 1 = ( 1 + m ) ¯ w t − m ¯ w t − 1 ( 42 ) − η ∇ ¯ L ( ( T + 1 ) ¯ w t − D − T ¯ w t − D − 1 ) We note that unlike state transition equation of SGDM the equations for LWP and GSC both contain ¯ w t − D − 1 . This means that the mitigation methods generally do not corre - spond to a simple change in the hyperparameter values of SGDM . Similarly , the combination of GSC and LWP has an additional ¯ w t − D − 2 term and thus does not simply corre - spond to a different setting of a , b or T for either method . The equations for LWP and GSC contain the same weight terms which could indicate that they operate in similar ways . If the gradient is well approximated as a linear function on the line segment : { ¯ w t − D − 1 + α ( T + 1 ) ( ¯ w t − D − ¯ w t − D − 1 ) | α ∈ [ 0 ; 1 ] } we have : ∇ ¯ L ( ( T + 1 ) ¯ w t − D − T ¯ w t − D − 1 ) ≈ ( T + 1 ) ∇ ¯ L ( ¯ w t − D ) − T ∇ ¯ L ( ¯ w t − D − 1 ) ( 43 ) In this case GSC and LWP are equivalent for the same learning rate and momentum if : a + b = 1 + T ( 44 ) mb = T ( 45 ) When the approximation in equation 43 holds , LWP is equiv - alent to our default choice of a and b ( see equation 14 ) if : T = m 1 − m D 1 − m ( 46 ) Pipelined Backpropagation at Scale This is equivalent to assuming zero future gradient over the prediction horizon in equation 17 instead of a constant velocity . GSC is equivalent to LWP with horizon T for the same learning rate if the approximation in 43 holds and : a = 1 − 1 − m m T , b = T m ( 47 ) This shows that LWP and GSC are closely related . Both methods compensate for a delay but at different points in time . Weight prediction changes how the gradient is com - puted , spike compensation changes how it is applied . Each method has its advantages . Spike compensation has minimal overhead and doesn’t require an estimate of the delay ahead of time . Weight prediction might introduce memory over - head by adding a new copy of the weights ( depending on the implementation and hardware ) , but may help reduce weight inconsistency . The combination of the two methods can be useful in cases where we want to overcompensate for the de - lay . A similar effect can be achieved with either method by changing the horizon but their combination offers increased weight consistency without requiring an additional weight prediction on the backwards pass . E . Extended Weight Prediction Horizons In Section 3 . 5 we discuss how overcompensating for delays can help improve convergence speed . One way to do this is to predict weights more than D ( the delay ) steps into the future with linear weight prediction . Figure 12 shows the effect of scaling the weight prediction horizon on the convergence rate when optimizing a convex quadratic . We see that horizon lengths of around T = 2 D seem to give the best results . We repeated this experiment for ResNet20 ( with group nor - malization ) trained on CIFAR10 . We used a delay D = 4 for all layers with consistent weights and a batch size of 32 for a total delay of 128 samples ( which is in the range of many of our CP experiments ) . The learning rate and momen - tum were scaled according to equation 9 using the default reference values referenced in the experiments section . The results can be seen in Figure 13 . We can see that the training loss curve looks somewhat similar to the convergence speed for the convex quadratic , with the lowest loss obtained for T ≈ 2 D . The validation accuracy also peaks for T ≈ 2 D . We also test this hypothesis in the Pipelined Backpropaga - tion setting . We explore the use of weight prediction with a horizon which is double that of the delay ( LWP 2D ) . We also experiment with overcompensating for the delay by doubling the effect of Spike Compensation ( SC 2D which replaces D with 2D in equation 14 ) . We observe that over - compensating can improve the ﬁnal accuracy in most cases ( Table 4 ) . We note that in these networks weight inconsis - tency does not seem to be an issue ( see Appendix B ) . In 0 2 4 6 8 10 Prediction Scale 2 . 0 2 . 5 3 . 0 3 . 5 4 . 0 4 . 5 5 . 0 M i n i nu m H a l f li f e ( l o g 10 ) = 10 3 , D = 4 = 10 3 , D = 10 = 10 5 , D = 4 Figure 12 . The convergence speed for a convex quadratic with dif - ferent condition numbers ( κ ) and delays ( D ) . A weight prediction with horizon T = αD is used where α is the prediction scale shown on the horizontal axis . 0 2 4 6 8 10 Prediction Scale 89 . 00 89 . 25 89 . 50 89 . 75 90 . 00 90 . 25 90 . 50 F i n a l V a li d a t i o n A cc u r a c y [ % ] 0 . 025 0 . 030 0 . 035 0 . 040 0 . 045 0 . 050 0 . 055 0 . 060 F i n a l T r a i n i n g L o ss Figure 13 . The effects of different weight prediction horizons on the ﬁnal loss and accuracy when training ResNet20 on CIFAR10 . A prediction scale of α scales the horizon to be T = αD where D = 4 is the delay . The delay is the same for all layers and consistent weights are used . Each point is the mean of multiple runs , 25 for 1 . 75 ≤ α ≤ 2 . 5 , and 10 for other α values . cases where weight inconsistency is an issue , doubling the prediction horizon can reduce training stability . The same may apply to networks with large delays . One such example may be training ResNet110 on CIFAR10 ( Table 4 ) where standard weight prediction outperforms methods which over - compensate for delay . F . Effects of Momentum Scaling Throughout this work we heuristically scale the momentum and learning rate for small batch size training according to equation 9 . This enables us to use pipelined backpropaga - tion without hyperparameter tuning for existing networks which is important for the practicality of PB training . These rules increase the momentum signiﬁcantly compared to other heuristics which might keep it constant or lower it . In Section 3 . 5 we show that momentum loses some of its beneﬁts with delays . However our compensation methods , Spike Compensation and Linear Weight Prediction , likely beneﬁt from high momentum . In this section we look at the effects of different momentum values , while keeping the total contribution from each gradient the same . We do this Pipelined Backpropagation at Scale Table 4 . CIFAR10 validation accuracy ( mean ± std . dev of ﬁve runs ) for ResNet ( RN ) and VGG training with overcompensation . N ETWORK SGDM PB PB + LPW D PB + LPW 2D PB + SC D PB + SC 2D VGG11 91 . 16 ± 0 . 19 90 . 83 ± 0 . 20 91 . 05 ± 0 . 11 91 . 27 ± 0 . 14 91 . 08 ± 0 . 19 91 . 03 ± 0 . 22 VGG13 92 . 57 ± 0 . 15 92 . 59 ± 0 . 15 92 . 51 ± 0 . 11 92 . 57 ± 0 . 21 92 . 38 ± 0 . 27 92 . 60 ± 0 . 17 VGG16 92 . 24 ± 0 . 19 92 . 06 ± 0 . 21 92 . 22 ± 0 . 24 92 . 28 ± 0 . 18 92 . 45 ± 0 . 30 92 . 42 ± 0 . 21 RN20 90 . 63 ± 0 . 31 90 . 44 ± 0 . 24 90 . 68 ± 0 . 30 91 . 05 ± 0 . 10 90 . 80 ± 0 . 29 90 . 95 ± 0 . 40 RN32 91 . 68 ± 0 . 23 91 . 46 ± 0 . 09 91 . 66 ± 0 . 10 91 . 98 ± 0 . 22 91 . 55 ± 0 . 14 91 . 96 ± 0 . 24 RN44 92 . 19 ± 0 . 14 91 . 71 ± 0 . 25 92 . 00 ± 0 . 14 92 . 29 ± 0 . 09 92 . 13 ± 0 . 16 92 . 21 ± 0 . 21 RN56 92 . 39 ± 0 . 20 91 . 89 ± 0 . 40 92 . 31 ± 0 . 14 92 . 41 ± 0 . 17 92 . 33 ± 0 . 16 92 . 68 ± 0 . 23 RN110 92 . 77 ± 0 . 22 91 . 81 ± 0 . 15 92 . 76 ± 0 . 05 71 . 83 ± 36 . 91 13 92 . 28 ± 0 . 29 92 . 35 ± 0 . 85 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 3 . 5 4 . 0 - log10 ( 1 - m ) 86 87 88 89 90 91 F i n a l V a li d a t i o n A cc u r a c y No Delay Baseline D = 12 SC D , D = 12 LWP D , D = 12 LWP vD + SC D , D = 12 ( a ) Using consistent weights . 0 . 0 0 . 5 1 . 0 1 . 5 2 . 0 2 . 5 3 . 0 3 . 5 4 . 0 - log10 ( 1 - m ) 84 86 88 90 F i n a l V a li d a t i o n A cc u r a c y No Delay Baseline D = 12 SC D , D = 12 LWP D , D = 12 LWP vD + SC D , D = 12 ( b ) Using inconsistent weights . Figure 14 . Effect of momentum on CIFAR10 ResNet20 training with delay . Showing the mean of three runs ( six for the no - delay case ) . by selecting a speciﬁc value of m in equation 9 ( ignoring the ﬁrst expression ) and then scaling the learning rate according to the second expression . The experiments involve training ResNet20 ( with group nor - malization ) on CIFAR10 . We use a batch size of 8 and a delay of 12 for all layers for a total delay of 96 samples ( which is in the range of many of our CP experiments ) . Fig - ure 14a shows this when consistent weights are used . We can see that for the baseline with no delay a wide range of momentum values can be used , including no momentum , but very large values cause accuracy loss . With delay , small values of momentum are better and the accuracy falls off relatively quickly for larger values . With our compensation methods the best accuracy is obtained for large momentum values . Spike compensation has no effect for low ( zero ) mo - mentum values and therefore matches the delayed baseline for small momentum values . Weight prediction for small momentum values tries to predict future weights based on recent gradients without sufﬁcient smoothing and performs worse than the baseline . The combined mitigation exceeds the best results for the no - delay baseline for a range of large momentum values . Figure 14b shows the same experiment performed with inconsistent weights ( using the most recent weights on the backwards pass instead of the delayed weights used on the forward pass ) . Most of the observations from the previous 13 Unstable training . experiment hold in this case as well . The most notable difference is the poor performance of all methods when low momentum is used . This suggests that small momentum values adversely affect weight consistency . These runs do not use a tuned learning rate or a learning rate warmup which could likely help stabilize lower momentum values . Using our formulation of momentum causes a warmup in the step size while the velocity is building up . This effect could contribute to larger momentum values performing better . Another factor may be the exponential smoothing of weight updates with momentum . Without this , a couple of relatively large gradients could cause a large weight inconsistency for some time steps , potentially destabilize training . G . Computational Setup G . 1 . Simulating Pipelined Backpropagation on GPUs One of the goals of this work is to explore PB training of modern deep networks such as ResNet50 . In particular we are interested in simulating fully pipeline parallel training with a maximal number of pipeline stages and no batch parallelism . For ResNet50 this results in about 150 stages if we naively make every convolution , normalization , and non - linearity into a stage . Combining convolution , normal - ization , and relu into one stage still makes ResNet50 a 50 stage network . Most modern deep learning frameworks are not well suited for such experimentation . To enable efﬁ - cient simulation of fully pipeline parallel training , we built Pipelined Backpropagation at Scale a mini - framework , GProp . GProp is implemented in C + + using cuDNN ( Chetlur et al . , 2014 ) kernels and Thrust . Overall ﬁne - grained pipeline parallelism is not very efﬁcient on GPUs . While speed is a consideration , the goal is not to be competitive with data parallel training on GPUs . We only aim to simulate pipeline parallel training and evaluate its potential as an alternative to data parallel training . As discussed before , other compute architectures could reap signiﬁcant beneﬁts from pipeline parallelism . In this section we discuss some of the implementation details and some of the potential limitations of GPUs at batch size one training . Compute Utilization At small batch sizes , the amount of computation per kernel might be insufﬁcient to utilize all compute resources . With pipeline parallelism a large num - ber of kernels can run in parallel . Launching multiple ker - nels in parallel can signiﬁcantly increase compute utiliza - tion . Kernel Launch Rate The compute throughput of the GPU is equal to the rate at which kernels are launched multiplied by the work done by each kernel . As the work per kernel is decreased , the kernel launch rate must be increased to main - tain compute throughput . Among other factors , the work depends on the batch size . As described previously , launch - ing kernels in parallel can mitigate for decreased in work due to batch size one training . For smaller networks the work done per kernel is generally less , therefore the kernel launch rate must be higher for good utilization . GPUs have a kernel launch rate limit which can become the training bottleneck . This is an issue for smaller networks such as ResNet20 for CIFAR10 . Bandwidth Limitations Without signiﬁcant weight reuse , GPU’s become memory bandwidth limited . For convolu - tional layers the weights are reused over the spatial and batch dimensions . Weight reuse increases as the spatial dimensions of the inputs increase . This makes bandwidth less of an issue for ImageNet ( i1k ) scale networks when compared to CIFAR10 scale networks . There are a few other challenges to small batch sized train - ing . At small batch sizes optimizer overheads become sig - niﬁcant . Each optimizer step requires loading the entire model , consuming signiﬁcant memory bandwidth . At large batch sizes this is amortized over the batch size . For a batch size of one the optimizer steps consume a large fraction of the total memory bandwidth . Similarly , the time required for any new memory allocations cannot be amortized over the batch size . In GProp the network is split into structures we call stages that act as pipeline stages . Each stage manages all resources needed to compute the forward and backward passes for the corresponding part of the network ( Figure 15 ) . In our experiments we sometimes group several components to - Stage K Temp Deltas Current Deltas Current Output Temp Output dW W FWD BWD GRAD Current OutputK - 1 Current DeltasK + 1 Stage K + 1 Stage K - 1 Figure 15 . GProp Stage . GProp is divided into stages that corre - spond to pipeline stages . Each stage contains logic for both the forward and backward pass for the corresponding part of the net - work . The stages use buffers to enable parallel execution , one stage can be computing an output while another stage is using the previous output . gether into a single stage . One example of this is grouping convolution , normalization , and ReLU into a stage . GProp uses CUDA streams to run the stages in parallel . GProp also supports splitting the network over multiple GPUs and uses a different thread to launch the stages on each GPU . We found that using multiple threads to launch kernels on a single GPU did not raise the kernel launch rate limit . We suspect this is potentially due to some sort of locking mechanism in cuDNN . G . 2 . Simulating Delayed Gradients Weight inconsistency and delayed gradients are potential issues in pipelined backpropagation . To better understand the issues we simulated weight inconsistency and delayed gradients in a PyTorch ( Paszke et al . , 2019 ) environment using a modiﬁed optimizer . The modiﬁed optimizer has a buffer of old parameter values . To apply a delay D , the model is loaded with parameters from D time steps ago , a forward and backward pass is performed . The resulting gra - dients are then used to update a master copy of the weights . Weight inconsistency is simulated by loaded the model with parameters from D time steps ago , doing the forward pass then loading the model with the master weights before do - ing the backwards pass . While this was not an exact model of PB , this setup allows for the simulation of PB’s issues and fast iterate of potential methods to overcome the issues . This technique can also be used to simulate PB by having different delays for different layers based on the depth of the layer . This simulation method does not allow simul - taneously launching multiple kernels and is therefore not efﬁcient for small batch sizes . Our simulations are done using a constant delay across layers . This upper bounds the effect of weight inconsistency and delayed gradients . This setup can also be used to simulated ASGD training by making D a random variable which models the distribution of GPU communications with the master node in ASGD . Pipelined Backpropagation at Scale Table 5 . CIFAR10 ﬁnal validation accuracy ( mean ± std . dev of ﬁve runs ) for ResNet ( RN ) and VGG training . N ETWORK S TAGES SGDM PB PB + LPW vD + SC D VGG11 29 91 . 16 ± 0 . 19 90 . 83 ± 0 . 20 91 . 12 ± 0 . 18 VGG13 33 92 . 57 ± 0 . 15 92 . 59 ± 0 . 15 92 . 56 ± 0 . 14 VGG16 39 92 . 24 ± 0 . 19 92 . 06 ± 0 . 21 92 . 38 ± 0 . 27 RN20 34 90 . 63 ± 0 . 31 90 . 44 ± 0 . 24 90 . 92 ± 0 . 25 RN32 52 91 . 68 ± 0 . 23 91 . 46 ± 0 . 09 92 . 04 ± 0 . 13 RN44 70 92 . 19 ± 0 . 14 91 . 71 ± 0 . 25 92 . 16 ± 0 . 26 RN56 88 92 . 39 ± 0 . 20 91 . 89 ± 0 . 40 92 . 48 ± 0 . 11 RN110 169 92 . 77 ± 0 . 22 91 . 81 ± 0 . 15 92 . 41 ± 0 . 16 0 . 0 2 . 5 5 . 0 7 . 5 10 . 0 12 . 5 15 . 0 17 . 5 20 . 0 Epoch 10 20 30 40 50 60 70 80 90 A c c u r a c y PyTorchSGD SGD Fill & Drain _ SGD ( a ) Training accuracy . 2 . 5 5 . 0 7 . 5 10 . 0 12 . 5 15 . 0 17 . 5 20 . 0 Epoch 30 40 50 60 70 80 A c c u r a c y PyTorchSGD SGD Fill & Drain _ SGD ( b ) Validation accuracy . Figure 16 . Validation the GProp framework using CIFAR10 VGG11 . Showing mean ( shading is standard deviation ) of ten runs . H . Experiment Details Table 5 shows the run to run variability of Table 1 . H . 1 . VGG Experiments Simonyan & Zisserman ( 2014 ) do not provide a setup for training VGG on CIFAR10 . We adopt the VGG architecture , hyperparameters , and data preprocessing from Fu ( 2019 ) . H . 2 . GProp validation To validate our framework implementation , we compare batch parallel SGD , and ﬁll & drain SGD training . We trained each setting , as well as the same network in PyTorch , 10 times to validate similar behavior . Figure 16 shows the optimization of the different SGD training modes for the ﬁrst 20 epochs . Numerical precision , network initialization , and data loading / augmentation randomness makes a nu - merical comparison for distinct runs impractical . Instead we show the mean and standard deviation of 10 runs . The dif - ferent SGD modes in GProp are consistent and also match PyTorch’s SGD convergence . H . 3 . ResNetv2 He et al . ( 2016b ) modiﬁed the original ResNet formulation given by He et al . ( 2016a ) by introducing the ResNet pre - activation block . We adopt the hyperparameters and data preprocessing from He et al . ( 2016a ) . Our experiments are done at batch size one where Batch Normalization is not effective . We replace Batch Normalization with Group Nor - malization . For ImageNet ResNet50 training , we used an initial group size of two as outlined in the Group Normal - ization paper . Wu & He ( 2018 ) do not tune Group Normal - ization for CIFAR10 training . We use the same initial group size of two for our CIFAR10 experiments . H . 4 . Hyperparameter comparison As mentioned in Section 3 . 1 we use the hyperparameters published in ( He et al . , 2016a ) and scale them using the rules described by Chiley et al . ( 2019 ) . Figures 17a and 17b shows that the hyperparameters produced using these scal - ing rule result in training curves similar to the reference when training VGG11 on the CIFAR10 dataset . Pipelined Backpropagation at Scale 0 . 0 2 . 5 5 . 0 7 . 5 10 . 0 12 . 5 15 . 0 17 . 5 20 . 0 Epoch 10 20 30 40 50 60 70 80 A cc u r a c y 1 128 ( a ) Training accuracy . 2 . 5 5 . 0 7 . 5 10 . 0 12 . 5 15 . 0 17 . 5 20 . 0 Epoch 30 40 50 60 70 80 A cc u r a c y 1 128 ( b ) Validation accuracy . Figure 17 . Hyperparameter comparison using CIFAR10 VGG11 . Showing mean ( shading is standard deviation ) of ten runs . Table 6 . CIFAR10 validation accuracy ( mean ± std . dev of ﬁve runs ) comparing LPW v D and LPW w D on ResNet ( RN ) and VGG training . N ETWORK SGDM PB PB + LPW vD + SC D PB + LPW wD + SC D VGG11 91 . 16 ± 0 . 19 90 . 83 ± 0 . 20 91 . 12 ± 0 . 18 90 . 93 ± 0 . 15 VGG13 92 . 57 ± 0 . 15 92 . 59 ± 0 . 15 92 . 56 ± 0 . 14 92 . 55 ± 0 . 08 VGG16 92 . 24 ± 0 . 19 92 . 06 ± 0 . 21 92 . 38 ± 0 . 27 92 . 09 ± 0 . 10 RN20 90 . 63 ± 0 . 31 90 . 44 ± 0 . 24 90 . 92 ± 0 . 25 90 . 85 ± 0 . 41 RN32 91 . 68 ± 0 . 23 91 . 46 ± 0 . 09 92 . 04 ± 0 . 13 91 . 99 ± 0 . 16 RN44 92 . 19 ± 0 . 14 91 . 71 ± 0 . 25 92 . 16 ± 0 . 26 92 . 20 ± 0 . 36 RN56 92 . 39 ± 0 . 20 91 . 89 ± 0 . 40 92 . 48 ± 0 . 11 92 . 32 ± 0 . 06 RN110 92 . 77 ± 0 . 22 91 . 81 ± 0 . 15 92 . 41 ± 0 . 16 91 . 85 ± 0 . 16 H . 5 . LPW vD vs LPW wD Table 6 shows the results of using the two variants of LWP . When combined with SC , LPW vD outperforms LPW wD . When the weight form is used the most recent gradient has a large effect on the velocity estimate used for the weight prediction . For small batch sizes this estimate might be noisy decreasing the effectiveness of LWP . A similar effect can be observed for LWP in general ( Appendix F ) when very small momentum values are used which also leads to noisy predictions .