Scheduled Restart Momentum for Accelerated Stochastic Gradient Descent Bao Wang ∗ Scientiﬁc Computing and Imaging ( SCI ) Institute University of Utah , Salt Lake City , UT , USA Tan M . Nguyen ∗ Department of ECE Rice University , Houston , USA Tao Sun College of Computer National University of Defense Technology Changsha , China Andrea L . Bertozzi Department of Mathematics University of California , Los Angeles Richard G . Baraniuk † Department of ECE Rice University , Houston , USA Stanley J . Osher † Department of Mathematics University of California , Los Angeles April 28 , 2020 Abstract Stochastic gradient descent ( SGD ) with constant momentum and its variants such as Adam are the optimization algorithms of choice for training deep neural networks ( DNNs ) . Since DNN training is incredibly computationally expensive , there is great interest in speeding up the convergence . Nesterov accelerated gradient ( NAG ) improves the convergence rate of gradient descent ( GD ) for convex optimization using a specially designed momentum ; however , it accumulates error when an inexact gradient is used ( such as in SGD ) , slowing convergence at best and diverging at worst . In this paper , we propose Scheduled Restart SGD ( SRSGD ) , a new NAG - style scheme for training DNNs . SRSGD replaces the constant momentum in SGD by the increasing momentum in NAG but stabilizes the iterations by resetting the momentum to zero according to a schedule . Using a variety of models and benchmarks for image classiﬁcation , we demonstrate that , in training DNNs , SRSGD signiﬁcantly improves convergence and generalization ; for instance in training ResNet200 for ImageNet classiﬁcation , SRSGD achieves an error rate of 20 . 93 % vs . the benchmark of 22 . 13 % . These improvements become more signiﬁcant as the network grows deeper . Furthermore , on both CIFAR and ImageNet , SRSGD reaches similar or even better error rates with signiﬁcantly fewer training epochs compared to the SGD baseline . 1 Introduction Training many machine learning ( ML ) models reduces to solving the following ﬁnite - sum optimization problem min w f ( w ) : = min w 1 N N X i = 1 f i ( w ) , w ∈ R d , ( 1 ) where f i ( w ) : = L ( g ( x i , w ) , y i ) is the loss between the ground - truth label y i and the prediction by the model g ( · , w ) , parametrized by w . This training loss is typically a cross - entropy loss for classiﬁcation and a root mean square error for regression . Here { x i , y i } Ni = 1 are the training samples , and problem ( 1 ) is known as empirical risk minimization ( ERM ) . For many practical applications , f ( w ) is highly non - convex , and g ( · , w ) is ∗ Co - ﬁrst author . Please correspond to : wangbaonj @ gmail . com or mn15 @ rice . edu † Co - last author 1 a r X i v : 2002 . 10583v2 [ c s . L G ] 26 A p r 2020 chosen among deep neural networks ( DNNs ) due to their preeminent performance across various tasks . These deep models are heavily overparametrized and require large amounts of training data . Thus , both N and the dimension of w can scale up to millions or even billions . These complications pose serious computational challenges . One of the simplest algorithms to solve ( 1 ) is gradient descent ( GD ) , which updates w according to : w k + 1 = w k − s k 1 N N X i = 1 ∇ f i ( w k ) , ( 2 ) where s k > 0 is the step size at the k - th iteration . Computing ∇ f ( w k ) on the entire training set is memory intensive and often cannot ﬁt on devices with limited random access memory ( RAM ) such as graphics processing units ( GPUs ) typically used for deep learning ( DL ) . In practice , we sample a subset of the training set , of size m with m (cid:28) N , to approximate ∇ f ( w k ) by the mini - batch gradient 1 / m P mj = 1 ∇ f i j ( w k ) . This results in the stochastic gradient descent ( SGD ) update w k + 1 = w k − s k 1 m m X j = 1 ∇ f i j ( w k ) . ( 3 ) SGD and its accelerated variants are among the most used optimization algorithms in ML practice [ 6 ] . These gradient - based algorithms have a number of beneﬁts . Their convergence rate is usually independent of the dimension of the underlying problem [ 6 ] ; their computational complexity is low and easy to parallelize , which makes them suitable to large scale and high dimensional problems [ 56 , 57 ] . They have achieved , so far , the best performance in training DNNs [ 16 ] . Nevertheless , GD and SGD have convergence issues , especially when the problem is ill - conditioned . There are two common approaches to accelerate GD : adaptive step size [ 12 , 22 , 55 ] and momentum [ 43 ] . The integration of both adaptive step size and momentum with SGD leads to Adam [ 26 ] , which is one of the most used optimizers for DNNs . Many recent developments have improved Adam [ 11 , 32 , 33 , 44 ] . GD with constant momentum leverages previous step information to accelerate GD according to : v k + 1 = w k − s k ∇ f ( w k ) , w k + 1 = v k + 1 + µ ( v k + 1 − v k ) , ( 4 ) where µ > 0 is a constant . A similar acceleration can be achieved by the heavy - ball ( HB ) method [ 43 ] . Both momentum update in ( 4 ) and HB enjoy the same convergence rate of O ( 1 / k ) as GD for convex smooth optimization . A breakthrough due to Nesterov [ 39 ] replaces the constant momentum µ with ( k − 1 ) / ( k + 2 ) ( aka , Nesterov accelerated gradient ( NAG ) momentum ) , and it can accelerate the convergence rate to O ( 1 / k 2 ) , which is optimal for convex and smooth loss functions [ 39 , 51 ] . Jin et al . showed that NAG can also speed up escaping saddle point [ 25 ] . In practice , NAG momentum and its variants such as Katyusha momentum [ 1 ] can also accelerate GD for nonconvex optimization , especially when the underlying loss function is poorly conditioned [ 15 ] . However , Devolder et al . [ 10 ] has recently showed that NAG accumulates error when an inexact gradient is used , thereby slowing convergence at best and diverging at worst . Until now , only constant momentum has been successfully used in training DNNs in practice [ 52 ] . Since NAG momentum has achieved a much better convergence rate than constant momentum methods with exact gradient oracle , in this paper we study the following question : Can we leverage NAG momentum to accelerate SGD and to improve generalization in training DNNs ? Contributions . We answer the above question by proposing the ﬁrst algorithm that integrates scheduled restart ( SR ) NAG momentum with plain SGD . We name the resulting algorithm scheduled restart SGD ( SRSGD ) . Theoretically , we present the error accumulation of Nesterov accelerated SGD ( NASGD ) and the convergence of SRSGD . The major practical beneﬁts of SRSGD are fourfold : • SRSGD can signiﬁcantly speed up DNN training . For image classiﬁcation , SRSGD can signiﬁcantly reduce the number of training epochs while preserving or even improving the network’s accuracy . In 2 SGD SRSGD T e s t E rr o r Number of Layers CIFAR10 CIFAR100 ImageNet Figure 1 : Error vs . depth of ResNet models trained with SRSGD and the baseline SGD with constant momemtum . Advantage of SRSGD continues to grow with depth . particular , on CIFAR10 / 100 , the number of training epochs can be reduced by half with SRSGD while on ImageNet the reduction in training epochs is also remarkable . • DNNs trained by SRSGD generalize signiﬁcantly better than the current benchmark optimizers . The improvement becomes more signiﬁcant as the network grows deeper as shown in Fig . 1 . • SRSGD reduces overﬁtting in very deep networks such as ResNet - 200 for ImageNet classiﬁcation , enabling the accuracy to keep increasing with depth . • SRSGD is straightforward to implement and only requires changes in a few lines of the SGD code . There is also no additional computational or memory overhead . We focus on DL for image classiﬁcation , in which SGD with constant momentum is the choice . Organization . In Section 2 , we review and discuss momentum for accelerating GD in convex smooth optimization . In Section 3 , we present scheduled restart NAG momentum to accelerate SGD , namely SRSGD algorithm and its theoretical guarantees . In Section 4 , we verify the eﬃcacy of the proposed SRSGD in training DNNs for image classiﬁcation on CIFAR and ImageNet . In Section 5 , we perform some empirical analysis of SRSGD . In Section 6 , we brieﬂy review some representative works that utilize momentum to accelerate SGD and study the restart techniques in NAG . We end with concluding remarks . Technical proofs and some more experimental details and results , in particular training RNNs and GANs , are provided in the appendix . Notation . We denote scalars and vectors by lower case and lower case bold face letters , respectively , and matrices by upper case bold face letters . For a vector x = ( x 1 , · · · , x d ) ∈ R d , we denote its ‘ p norm ( p ≥ 1 ) by k x k p = (cid:16)P di = 1 | x i | p (cid:17) 1 / p , the ‘ ∞ norm of x by k x k ∞ = max di = 1 | x i | . For a matrix A , we used k A k p to denote its induced norm by the vector ‘ p norm . Given two sequences { a n } and { b n } , we write a n = O ( b n ) if there exists a positive constant s . t . 0 < C < + ∞ such that a n ≤ Cb n . We denote the interval a to b ( included ) as ( a , b ] . For a function f ( w ) : R d → R , we denote its gradient and Hessian as ∇ f ( w ) and ∇ 2 f ( w ) , respectively . 2 Review : Momentum in Gradient Descent 2 . 1 Gradient Descent Perhaps the simplest algorithm to solve ( 1 ) is GD ( 2 ) , which dates back to [ 7 ] . If the objective f ( w ) is convex and L - smooth ( i . e . , k∇ 2 f ( w ) k 2 ≤ L ) , then GD converges with rate O ( 1 / k ) by letting s k ≡ 1 / L ( we use this 3 s k in all the discussion below ) , which is independent of the dimension of w . 2 . 2 Gradient Descent with Momentum – Heavy Ball HB scheme ( 5 ) [ 43 ] accelerates GD by using the momentum w k − w k − 1 , which gives w k + 1 = w k − s k ∇ f ( w k ) + µ ( w k − w k − 1 ) , ( 5 ) where µ > 0 is a constant . Alternatively , we can accelerate GD by using the Nesterov momentum ( aka , lookahead momentum ) , which leads to the scheme in ( 4 ) . Both HB and ( 4 ) have the same convergence rate of O ( 1 / k ) for solving convex smooth problems . Recently , several variants of ( 4 ) have been proposed for DL , e . g . , [ 52 ] and [ 5 ] . 2 . 3 Nesterov Accelerated Gradient NAG [ 4 , 39 ] replaces the constant µ with ( t k − 1 ) / t k + 1 , where t k + 1 = ( 1 + p 1 + 4 t 2 k ) / 2 with t 0 = 1 , v k + 1 = w k − s k ∇ f ( w k ) , w k + 1 = v k + 1 + t k − 1 t k + 1 ( v k + 1 − v k ) . ( 6 ) NAG achieves a convergence rate O ( 1 / k 2 ) with the step size s k = 1 / L , which is the optimal rate for general convex smooth optimization problems . Remark 1 . Su et al . [ 51 ] showed that ( k − 1 ) / ( k + 2 ) is the asymptotic limit of ( t k − 1 ) / t k + 1 . In the following presentation of NAG with restart , for the ease of notation , we will replace the momentum coeﬃcient ( t k − 1 ) / t k + 1 with the form of ( k − 1 ) / ( k + 2 ) . 2 . 4 Adaptive Restart NAG ( ARNAG ) The sequences , { f ( w k ) − f ( w ∗ ) } where w ∗ is the minimum of f ( w ) , generated by GD and GD with constant momentum ( GD + Momentum ) converge monotonically to zero . However , that sequence generated by NAG oscillates , as illustrated in Fig . 2 ( a ) when f ( w ) is a quadratic function . [ 41 ] proposes ARNAG ( 7 ) to alleviate this oscillatory phenomenon v k + 1 = w k − s k ∇ f ( w k ) , w k + 1 = v k + 1 + m ( k ) − 1 m ( k ) + 2 ( v k + 1 − v k ) , ( 7 ) where m ( 1 ) = 1 ; m ( k + 1 ) = m ( k ) + 1 if f ( w k + 1 ) ≤ f ( w k ) , and m ( k + 1 ) = 1 otherwise . 2 . 5 Scheduled Restart NAG ( SRNAG ) SR is another strategy to restart NAG . We ﬁrst divide the total iterations ( 0 , T ] ( integers only ) into a few intervals { I i } mi = 1 = ( T i − 1 , T i ] , such that ( 0 , T ] = S mi = 1 I i . In each I i we restart the momentum after every F i , and the iteration is according to : v k + 1 = w k − s k ∇ f ( w k ) , w k + 1 = v k + 1 + ( k mod F i ) ( k mod F i ) + 3 ( v k + 1 − v k ) . ( 8 ) Both AR and SR accelerate NAG to linear convergence for convex problems with PL condition [ 49 ] . 4 2 . 6 Case Study – Quadratic Function Consider the following quadratic optimization 1 min x f ( x ) = 1 2 x T L x − x T b , ( 9 ) where L ∈ R d × d is the Laplacian of a cycle graph . and b is a d - dimensional vector whose ﬁrst entry is 1 and all the other entries are 0 . It is easy to see that f ( x ) is convex with Lipschitz constant 4 . In particular , we set d = 1K ( 1K : = 10 3 ) . We run T = 50K iterations with step size 1 / 4 . In SRNAG , we restart , i . e . , we set the momentum to 0 , after every 1K iterations . As shown in Fig . 2 ( a ) , GD + Momentum converges faster than GD , while NAG speeds up GD + Momentum dramatically and converges to the minimum in an oscillatory fashion . Both AR and SR accelerate NAG signiﬁcantly . f ( x k ) – f ( x * ) Iteration ( a ) ( b ) ( c ) GD GD + Momentum NAG ARNAG SRNAG Figure 2 : Comparison between diﬀerent schemes in optimizing the quadratic function , ( 9 ) , with ( a ) exact gradient , ( b ) gradient with constant variance Gaussian noise , and ( c ) gradient with decaying variance Gaussian noise . NAG , ARNAG , and SRNAG can speed up convergence remarkably when exact gradient is used . Also , SRNAG is more robust to noisy gradient than NAG and ARNAG . 3 Scheduled Restart SGD ( SRSGD ) Computing gradient for ERM , ( 1 ) , can be computational costly and memory intensive , especially when the training set is large . In many applications , such as training DNNs , SGD ( 3 ) is used . In this section , we will ﬁrst analyze whether NAG and restart techniques can still speed up SGD . Then we formulate our new SRSGD as a solution to accelerate convergence of SGD using NAG momentum . 3 . 1 Uncontrolled Bound of Nesterov Accelerated SGD ( NASGD ) Replacing ∇ f ( w k ) : = 1 / N P N i = 1 ∇ f i ( w k ) in ( 6 ) with the stochastic gradient 1 / m P m j = 1 ∇ f i j ( w k ) for ( 1 ) will accumulate error even for convex function . We formulate this fact in Theorem 1 . Theorem 1 . Let f ( w ) be a convex and L - smooth function . The sequence { w k } k ≥ 0 generated by ( 6 ) , with mini - batch stochastic gradient using any constant step size s k ≡ s ≤ 1 / L , satisﬁes E (cid:0) f ( w k ) − f ( w ∗ ) (cid:1) = O ( k ) , ( 10 ) where w ∗ is the minimum of f , and the expectation is taken over the random mini - batch samples . In Appendix A , we provide the proof of Theorem 1 . In [ 10 ] , Devolder et al . proved a similar error accumulation result for the δ - inexact gradient . In Appendix B , we provide a brief review of NAG with δ - inexact gradient . We consider three diﬀerent inexact gradients , namely , Gaussian noise with constant and 1 We take this example from [ 18 ] . 5 decaying variance corrupted gradients for the quadratic optimization ( 9 ) , and training logistic regression model for MNIST [ 30 ] classiﬁcation . The detailed settings and discussion are provided in the Appendix B . We denote SGD with NAG momentum as NASGD , and denote NASGD with AR and SR as ARSGD and SRSGD , respectively . The results shown in Fig . 2 ( b ) and ( c ) ( iteration vs . optimal gap for quadratic optimization ( 9 ) ) , and Fig . 3 ( iteration vs . loss for training logistic regression model ) conﬁrm Theorem 1 . Moreover , for these cases SR can improve the performance of NAG with inexact gradients . When inexact gradient is used , GD performs almost the same as ARNAG asymptotically because ARNAG restarts too often and almost degenerates to GD . L o ss Iteration SGD SGD + Momentum NASGD ARSGD SRSGD Figure 3 : Training loss comparison between diﬀerent schemes in training logistic regression for MNIST classiﬁcation . NASGD is not robust to noisy gradient , ARSGD almost degenerates to SGD , and SRSGD performs the best in this case . 3 . 2 SRSGD and Its Convergence For ERM ( 1 ) , SRSGD replaces ∇ f ( w ) in ( 8 ) with the stochastic gradient with batch size m , gives v k + 1 = w k − s k 1 m m X j = 1 ∇ f i j ( w k ) , w k + 1 = v k + 1 + ( k mod F i ) ( k mod F i ) + 3 ( v k + 1 − v k ) , ( 11 ) where F i is the restart frequency used in the interval I i . We implemented SRSGD in both PyTorch [ 42 ] and Keras [ 8 ] , by changing just a few lines on top of the existing SGD optimizer . We provide a snippet of SRSGD code in Appendix J and K . We formulate the convergence of SRSGD for general nonconvex problems in Theorem 2 and we provide its proof in Appendix C . Theorem 2 . Suppose f ( w ) is L - smooth . Consider the sequence { w k } k ≥ 0 generated by ( 11 ) with mini - batch stochastic gradient and any restart frequency F using any constant step size s k : = s ≤ 1 / L . Assume that the set A : = { k ∈ Z + | E f ( w k + 1 ) ≥ E f ( w k ) } is ﬁnite , then we have min 1 ≤ k ≤ K (cid:8) E k∇ f ( w k ) k 22 (cid:9) = O ( s + 1 sK ) . ( 12 ) Therefore for ∀ (cid:15) > 0 , to get (cid:15) error , we just need to set s = O ( (cid:15) ) and K = O ( 1 / (cid:15) 2 ) . 6 Table 1 : Classiﬁcation test error ( % ) on CIFAR10 using the SGD , SGD + NM , and SRSGD . We report the results of SRSGD with two diﬀerent restarting schedules : linear ( lin ) and exponential ( exp ) . The numbers of iterations after which we restart the momentum in the lin schedule are 30 , 60 , 90 , 120 for the 1st , 2nd , 3rd , and 4th stage . Those numbers for the exp schedule are 40 , 50 , 63 , 78 . We also include the reported results from [ 21 ] ( in parentheses ) in addition to our reproduced results . Network # Params SGD ( baseline ) SGD + NM SRSGD SRSGD Improve over Improve over ( lin ) ( exp ) SGD ( lin / exp ) SGD + NM ( lin / exp ) Pre - ResNet - 110 1 . 1M 5 . 25 ± 0 . 14 ( 6 . 37 ) 5 . 24 ± 0 . 16 4 . 93 ± 0 . 13 4 . 93 ± 0 . 13 4 . 93 ± 0 . 13 5 . 00 ± 0 . 47 0 . 32 0 . 32 0 . 32 / 0 . 25 0 . 31 0 . 31 0 . 31 / 0 . 24 Pre - ResNet - 290 3 . 0M 5 . 05 ± 0 . 23 5 . 04 ± 0 . 12 4 . 37 ± 0 . 15 4 . 37 ± 0 . 15 4 . 37 ± 0 . 15 4 . 50 ± 0 . 18 0 . 68 0 . 68 0 . 68 / 0 . 55 0 . 67 0 . 67 0 . 67 / 0 . 54 Pre - ResNet - 470 4 . 9M 4 . 92 ± 0 . 10 4 . 97 ± 0 . 15 4 . 18 ± 0 . 09 4 . 18 ± 0 . 09 4 . 18 ± 0 . 09 4 . 49 ± 0 . 19 0 . 74 0 . 74 0 . 74 / 0 . 43 0 . 79 0 . 79 0 . 79 / 0 . 48 Pre - ResNet - 650 6 . 7M 4 . 87 ± 0 . 14 4 . 80 ± 0 . 14 4 . 00 ± 0 . 07 4 . 00 ± 0 . 07 4 . 00 ± 0 . 07 4 . 40 ± 0 . 13 0 . 87 0 . 87 0 . 87 / 0 . 47 0 . 80 0 . 80 0 . 80 / 0 . 40 Pre - ResNet - 1001 10 . 3M 4 . 84 ± 0 . 19 ( 4 . 92 ) 4 . 62 ± 0 . 14 3 . 87 ± 0 . 07 3 . 87 ± 0 . 07 3 . 87 ± 0 . 07 4 . 13 ± 0 . 10 0 . 97 0 . 97 0 . 97 / 0 . 71 0 . 75 0 . 75 0 . 75 / 0 . 49 Table 2 : Classiﬁcation test error ( % ) on CIFAR100 using the SGD , SGD + NM , and SRSGD . We report the results of SRSGD with two diﬀerent restarting schedules : linear ( lin ) and exponential ( exp ) . The numbers of iterations after which we restart the momentum in the lin schedule are 50 , 100 , 150 , 200 for the 1st , 2nd , 3rd , and 4th stage . Those numbers for the exp schedule are 45 , 68 , 101 , 152 . We also include the reported results from [ 21 ] ( in parentheses ) in addition to our reproduced results . Network # Params SGD ( baseline ) SGD + NM SRSGD SRSGD Improve over Improve over ( lin ) ( exp ) SGD ( lin / exp ) SGD + NM ( lin / exp ) Pre - ResNet - 110 1 . 2M 23 . 75 ± 0 . 20 23 . 65 ± 0 . 36 23 . 49 ± 0 . 23 23 . 49 ± 0 . 23 23 . 49 ± 0 . 23 23 . 50 ± 0 . 39 0 . 26 0 . 26 0 . 26 / 0 . 25 0 . 16 0 . 16 0 . 16 / 0 . 15 Pre - ResNet - 290 3 . 0M 21 . 78 ± 0 . 21 21 . 68 ± 0 . 21 21 . 49 ± 0 . 27 21 . 49 ± 0 . 27 21 . 49 ± 0 . 27 21 . 58 ± 0 . 20 0 . 29 0 . 29 0 . 29 / 0 . 20 0 . 19 0 . 19 0 . 19 / 0 . 10 Pre - ResNet - 470 4 . 9M 21 . 43 ± 0 . 30 21 . 21 ± 0 . 30 20 . 71 ± 0 . 32 20 . 64 ± 0 . 18 20 . 64 ± 0 . 18 20 . 64 ± 0 . 18 0 . 72 / 0 . 79 0 . 79 0 . 79 0 . 50 / 0 . 57 0 . 57 0 . 57 Pre - ResNet - 650 6 . 7M 21 . 27 ± 0 . 14 21 . 04 ± 0 . 38 20 . 36 ± 0 . 25 20 . 36 ± 0 . 25 20 . 36 ± 0 . 25 20 . 41 ± 0 . 21 0 . 91 0 . 91 0 . 91 / 0 . 86 0 . 68 0 . 68 0 . 68 / 0 . 63 Pre - ResNet - 1001 10 . 4M 20 . 87 ± 0 . 20 ( 22 . 71 ) 20 . 13 ± 0 . 16 19 . 75 ± 0 . 11 19 . 53 ± 0 . 19 19 . 53 ± 0 . 19 19 . 53 ± 0 . 19 1 . 12 / 1 . 34 1 . 34 1 . 34 0 . 38 / 0 . 60 0 . 60 0 . 60 4 Experimental Results We evaluate SRSGD on a variety of DL benchmarks for image classiﬁcation , including CIFAR10 , CIFAR100 , and ImageNet . In all experiments , we show the advantage of SRSGD over the widely used and well - calibrated SGD baselines with a constant momentum of 0 . 9 and decreasing learning rate at certain epochs , and we denote this optimizer as SGD . We also compare SRSGD with the well - calibrated SGD but switch momentum to the Nesterov momentum of 0 . 9 , and we denoted this optimizer as SGD + NM . We ﬁne tune the SGD / SGD + NM baselines to obtain the best performance , and we then adopt the same set of parameters for training with SRSGD . In the SRSGD experiments , we tune the restart frequencies on small DNNs and apply the tuned restart frequencies to large DNNs . We provide the detailed description of datasets and experimental settings in Appendix D . 4 . 1 CIFAR10 and CIFAR100 We summarize our results for CIFAR in Table 1 and 2 . We also explore two diﬀerent restarting frequency schedules for SRSGD : linear and exponential schedule . These schedules are governed by two parameters : the initial restarting frequency F 1 and the growth rate r . In both scheduling schemes , during training , the restarting frequency at the 1st learning rate stage is set to F 1 . Then the restarting frequency at the ( k + 1 ) - th learning rate stage is determined by : F k + 1 = ( F 1 × r k , exponential schedule F 1 × ( 1 + ( r − 1 ) × k ) , linear schedule . We have conducted a hyper - parameter search for F 1 and r for both scheduling schemes . For CIFAR10 , ( F 1 = 40 , r = 1 . 25 ) and ( F 1 = 30 , r = 2 ) are good initial restarting frequencies and growth rates for the exponential and linear schedules , respectively . For CIFAR100 , those values are ( F 1 = 45 , r = 1 . 5 ) for the exponential schedule and ( F 1 = 50 , r = 2 ) for the linear schedule . 7 Improvement in Accuracy Increases with Depth : We observe that the linear schedule of restart yields better test error on CIFAR than the exponential schedule for most of the models except for Pre - ResNet - 470 and Pre - ResNet - 1001 on CIFAR100 ( see Table 1 and 2 ) . SRSGD with either linear or exponential restart schedule outperforms the SGD . Furthermore , the advantage of SRSGD over SGD is greater for deeper networks . This observation holds strictly when using the linear schedule ( see Fig . 1 ) and is overall true when using the exponential schedule with only a few exceptions . SGD SRSGD T r a i n L o ss Epoch CIFAR10 ImageNet Figure 4 : Training loss vs . training epoch of ResNet models trained with SRSGD ( blue ) and the SGD baseline with momentum ( red ) . Faster Convergence Reduces the Training Time by Half : SRSGD also converges faster than SGD . This is expected since SRSGD can avoid the error accumulation with inexact oracle and converges faster than SGD + Momentum in our MNIST case study in Section 3 . For CIFAR , Fig . 4 ( left ) shows that SRSGD yields smaller training loss than SGD during the training . Interestingly , SRSGD converges quickly to good loss values at the 2nd and 3rd stages . This suggests that the model can be trained with SRSGD in many fewer epochs compared to SGD while achieving similar error rate . Our numerical results in Table 3 conﬁrm the hypothesis above . We train Pre - ResNet models with SRSGD in only 100 epochs , decreasing the learning rate by a factor of 10 at the 80th , 90th , and 95th epoch while using the same linear schedule for restarting frequency as before with ( F 1 = 30 , r = 2 ) for CIFAR10 and ( F 1 = 50 , r = 2 ) for CIFAR100 . We compare the test error of the trained models with those trained by the SGD baseline in 200 epochs . We observe that SRSGD trainings consistently yield lower test errors than SGD except for the case of Pre - ResNet - 110 even though the number of training epochs of our method is only half of the number of training epochs required by SGD . For Pre - ResNet - 110 , SRSGD training in 110 epochs with learning rate decreased at the 80th , 90th , and 100th epoch achieves the same error rate as the 200 - epoch SGD training on CIFAR10 . On CIFAR100 , SRSGD training for Pre - ResNet - 110 needs 140 epochs with learning rate decreased at the 80th , 100th and 120th epoch to achieve an 0 . 02 % improvement in error rate over the 200 - epoch SGD . 4 . 2 ImageNet Next we discuss our experimental results on the 1000 - way ImageNet classiﬁcation task [ 50 ] . We conduct our experiments on ResNet - 50 , 101 , 152 , and 200 with 5 diﬀerent seeds . We use the oﬃcial Pytorch implementation 2 for all of our ResNet models [ 42 ] . Following common practice , we train each model for 90 epochs and decrease the learning rate by a factor of 10 at the 30th and 60th epoch . We use an initial learning 2 Implementation available at https : / / github . com / pytorch / examples / tree / master / imagenet 8 Table 3 : Comparison of classiﬁcation errors on CIFAR10 / 100 ( % ) between SRSGD training with only 100 epochs and SGD baseline training with 200 epochs . Using only half the number of training epochs , SRSGD achieves comparable results to SGD . CIFAR10 CIFAR100 Network SRSGD Improvement SRSGD Improvement Pre - ResNet - 110 5 . 43 ± 0 . 18 − 0 . 18 23 . 85 ± 0 . 19 − 0 . 10 Pre - ResNet - 290 4 . 83 ± 0 . 11 0 . 22 21 . 77 ± 0 . 43 0 . 01 Pre - ResNet - 470 4 . 64 ± 0 . 17 0 . 28 21 . 42 ± 0 . 19 0 . 01 Pre - ResNet - 650 4 . 43 ± 0 . 14 0 . 44 21 . 04 ± 0 . 20 0 . 23 Pre - ResNet - 1001 4 . 17 ± 0 . 20 0 . 67 20 . 27 ± 0 . 11 0 . 60 Pre - ResNet - 110 5 . 25 ± 0 . 10 ( 110 epochs ) 0 . 00 23 . 73 ± 0 . 23 ( 140 epochs ) 0 . 02 rate of 0 . 1 , momentum value of 0 . 9 , and weight decay value of 0 . 0001 . Additional details and comparison between SRSGD and SGD + NM are given in Appendix E . We report single crop validation errors of ResNet models trained with SGD and SRSGD on ImageNet in Table 4 . In contrast to our CIFAR experiments , we observe that for ResNets trained on ImageNet with SRSGD , linearly decreasing the restarting frequency to 1 at the last learning rate ( i . e . , after the 60th epoch ) helps improve the generalization of the models . Thus , in our experiments , we set the restarting frequency to a linear schedule until epoch 60 . From epoch 60 to 90 , the restarting frequency is linearly decreased to 1 . We use ( F 1 = 40 , r = 2 ) . Table 4 : Single crop validation errors ( % ) on ImageNet of ResNets trained with SGD baseline and SRSGD . We report the results of SRSGD with the increasing restarting frequency in the ﬁrst two learning rates . In the last learning rate , the restarting frequency is linearly decreased from 70 to 1 . For baseline results , we also include the reported single - crop validation errors [ 20 ] ( in parentheses ) . Network # Params SGD SRSGD Improvement top - 1 top - 5 top - 1 top - 5 top - 1 top - 5 ResNet - 50 25 . 56M 24 . 11 ± 0 . 10 ( 24 . 70 ) 7 . 22 ± 0 . 14 ( 7 . 80 ) 23 . 85 ± 0 . 09 23 . 85 ± 0 . 09 23 . 85 ± 0 . 09 7 . 10 ± 0 . 09 7 . 10 ± 0 . 09 7 . 10 ± 0 . 09 0 . 26 0 . 12 ResNet - 101 44 . 55M 22 . 42 ± 0 . 03 ( 23 . 60 ) 6 . 22 ± 0 . 01 ( 7 . 10 ) 22 . 06 ± 0 . 10 22 . 06 ± 0 . 10 22 . 06 ± 0 . 10 6 . 09 ± 0 . 07 6 . 09 ± 0 . 07 6 . 09 ± 0 . 07 0 . 36 0 . 13 ResNet - 152 60 . 19M 22 . 03 ± 0 . 12 ( 23 . 00 ) 6 . 04 ± 0 . 07 ( 6 . 70 ) 21 . 46 ± 0 . 07 21 . 46 ± 0 . 07 21 . 46 ± 0 . 07 5 . 69 ± 0 . 03 5 . 69 ± 0 . 03 5 . 69 ± 0 . 03 0 . 57 0 . 35 ResNet - 200 64 . 67M 22 . 13 ± 0 . 12 6 . 00 ± 0 . 07 20 . 93 ± 0 . 13 20 . 93 ± 0 . 13 20 . 93 ± 0 . 13 5 . 57 ± 0 . 05 5 . 57 ± 0 . 05 5 . 57 ± 0 . 05 1 . 20 0 . 43 Advantage of SRSGD continues to grow with depth : Similar to the CIFAR experiments , we observe that SRSGD outperforms the SGD baseline for all ResNet models that we study . As shown in Fig . 1 , the advantage of SRSGD over SGD grows with network depth , just as in our CIFAR experiments with Pre - ResNet architectures . Avoiding Overﬁtting in ResNet - 200 : ResNet - 200 is an interesting model that demonstrates that SRSGD is better than the SGD baseline at avoiding overﬁtting . 3 The ResNet - 200 trained with SGD has a top - 1 error of 22 . 18 % , higher than the ResNet - 152 trained with SGD , which achieves a top - 1 error of 21 . 9 % ( see Table 4 ) . As pointed out in [ 21 ] , it is because ResNet - 200 suﬀers from overﬁtting . The ResNet - 200 trained with our SRSGD has a top - 1 error of 21 . 08 % , which is 1 . 1 % lower than the ResNet - 200 trained with the SGD baseline and also lower than the ResNet - 152 trained with both SRSGD and SGD , an improvement by 0 . 21 % and 0 . 82 % , respectively . Table 5 : Comparison of single crop validation errors on ImageNet ( % ) between SRSGD training with fewer epochs and SGD training with full 90 epochs . Network SRSGD Reduction Improvement Network SRSGD Reduction Improvement ResNet - 50 24 . 30 ± 0 . 21 10 − 0 . 19 ResNet - 152 21 . 79 ± 0 . 07 15 0 . 24 ResNet - 101 22 . 32 ± 0 . 06 10 0 . 1 ResNet - 200 21 . 92 ± 0 . 17 30 0 . 21 Training ImageNet in Fewer Number of Epochs : As in the CIFAR experiments , we note that when training on ImageNet , SRSGD converges faster than SGD at the ﬁrst and last learning rate while 3 By overﬁtting , we mean that the model achieves low training error but high test error . 9 quickly reaching a good loss value at the second learning rate ( see Fig . 4 ) . This observation suggests that ResNets can be trained with SRSGD in fewer epochs while still achieving comparable error rates to the same models trained by the SGD baseline using all 90 epochs . We summarize the results in Table 5 . On ImageNet , we note that SRSGD helps reduce the number of training epochs for very deep networks ( ResNet - 101 , 152 , 200 ) . For smaller networks like ResNet - 50 , training with fewer epochs slightly decreases the accuracy . 5 Empirical Analysis T e s t E rr o r Number of Epoch Reduction Pre - ResNet - 101 Pre - ResNet - 290 Pre - ResNet - 470 Pre - ResNet - 650 Pre - ResNet - 1001 S i n g l e C r o p V a li d a t i o n E rr o r ResNet - 50 ResNet - 101 ResNet - 152 ResNet - 200 CIFAR10 ImageNet Figure 5 : Test error vs . number of epoch reduction in CIFAR10 and ImageNet training . The dashed lines are test errors of the SGD baseline . For CIFAR , SRSGD training with fewer epochs can achieve comparable results to SRSGD training with full 200 epochs . For ImageNet , training with less epochs slightly decreases the performance of SRSGD but still achieves comparable results to the SGD baseline training . Error Rate vs . Reduction in Epochs . We ﬁnd that SRSGD training using fewer epochs yield comparable error rate to both the SGD baseline and the SRSGD full training with 200 epochs on CIFAR . We conduct an ablation study to understand the impact of reducing the number of epochs on the ﬁnal error rate when training with SRSGD on CIFAR10 and ImageNet . In the CIFAR10 experiments , we reduce the number of epochs from 15 to 90 while in the ImageNet experiments , we reduce the number of epochs from 10 to 30 . We summarize our results in Fig . 5 and provide detailed results in Appendix F . For CIFAR10 , we can train with 30 epochs less while still maintaining a comparable error rate to the full SRSGD training , and with a better error rate than the SGD baseline . For ImageNet , SRSGD training with fewer epochs decreases the accuracy but still obtains comparable results to the 90 - epoch SGD baseline as shown in Table 5 . Impact of Restarting Frequency We examine the impact of restarting frequency on the network training . We choose a case study of training Pre - ResNet - 290 on CIFAR10 using SRSGD with a linear schedule scheme for the restarting frequency . We ﬁx the growth rate r = 2 and vary the initial restarting frequency F 1 from 1 to 80 in increments of 10 . As shown in Fig . 6 , SRSGD with large F 1 , e . g . F 1 = 80 , approximates NASGD ( yellow ) . As discussed in Section 3 , it suﬀers from error accumulation due to stochastic gradients and converges slowly . SRSGD with small F 1 , e . g . F 1 = 1 , approximates SGD without momentum ( green ) . It converges faster initially but reaches a worse local minimum ( i . e . greater loss ) . Typical SRSGD ( blue ) converges faster 10 than NASGD and to a better local minimum than both NASGD and SGD without momentum . It also achieves the best test error . We provide more results in Appendix G and H . T r a i n L o ss Epoch T e s t E rr o r Initial Restarting Frequency ( F 1 ) Approximate SGD without momentum Approximate NASGD Approximate SGD without momentum Approximate NASGD Figure 6 : Training loss and test error of Pre - ResNet - 290 trained on CIFAR10 with diﬀerent initial restarting frequencies F 1 ( linear schedule ) . SRSGD with small F 1 approximates SGD without momentum , while SRSGD with large F 1 approximates NASGD . 6 Additional Related Work Momentum has long been used to accelerate SGD . [ 52 ] showed that SGD with scheduled momentum and a good initialization can handle the curvature issues in training DNNs and enable the trained models to generalize well . [ 11 , 26 ] integrated momentum with adaptive step size to accelerate SGD . These works all leverage constant momentum , while our work utilizes NAG momentum with restart . AR and SR have been used to accelerate NAG with exact gradient [ 13 , 14 , 24 , 31 , 36 , 37 , 41 , 45 , 48 , 51 ] . These studies of restart NAG momentum are for convex optimization with exact gradient . Our work focuses on SGD for nonconvex optimization . Many eﬀorts have also been devoted to accelerating ﬁrst - order algorithms with noise - corrupted gradients [ 3 , 9 ] . 7 Conclusions We propose the Scheduled Restart SGD ( SRSGD ) , with two major changes from the widely used SGD with constant momentum ( without ambiguity we call it SGD ) . First , we replace the momentum in SGD with the increasing momentum in Nesterov accelerated gradient ( NAG ) . Second , we restart the momentum according to a schedule to prevent error accumulation when the stochastic gradient is used . For image classiﬁcation , SRSGD can signiﬁcantly improve the accuracy of the trained DNNs . Also , compared to the SGD baseline , SRSGD requires fewer training epochs to reach to the same trained model’s accuracy . There are numerous avenues for future work : 1 ) deriving the optimal restart scheduling and the corresponding convergence rate of SRSGD , 2 ) integrating the scheduled restart NAG momentum with adaptive learning rate algorithms , e . g . Adam , and 3 ) integrating SRSGD with optimizers that remove noise on the ﬂy , e . g . , Laplacian smoothing SGD [ 40 ] . Acknowledgments This material is based on research sponsored by the National Science Foundation under grant number DMS - 1924935 and DMS - 1554564 ( STROBE ) . 11 References [ 1 ] Zeyuan Allen - Zhu . Katyusha : The ﬁrst direct acceleration of stochastic gradient methods . The Journal of Machine Learning Research , 18 ( 1 ) : 8194 – 8244 , 2017 . [ 2 ] Martin Arjovsky , Soumith Chintala , and L´eon Bottou . Wasserstein generative adversarial networks . In Doina Precup and Yee Whye Teh , editors , Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 214 – 223 , International Convention Centre , Sydney , Australia , 06 – 11 Aug 2017 . PMLR . [ 3 ] Necdet Serhat Aybat , Alireza Fallah , Mert Gurbuzbalaban , and Asuman Ozdaglar . Robust accelerated gradient methods for smooth strongly convex functions . arXiv preprint arXiv : 1805 . 10579 , 2018 . [ 4 ] Amir Beck and Marc Teboulle . A fast iterative shrinkage - thresholding algorithm for linear inverse problems . SIAM journal on imaging sciences , 2 ( 1 ) : 183 – 202 , 2009 . [ 5 ] Yoshua Bengio , Nicolas Boulanger - Lewandowski , and Razvan Pascanu . Advances in optimizing recurrent networks . In 2013 IEEE International Conference on Acoustics , Speech and Signal Processing , pages 8624 – 8628 . IEEE , 2013 . [ 6 ] L´eon Bottou , Frank E Curtis , and Jorge Nocedal . Optimization methods for large - scale machine learning . Siam Review , 60 ( 2 ) : 223 – 311 , 2018 . [ 7 ] Augustin Cauchy . M´ethode g´en´erale pour la r´esolution des systemes dâĂŹ´equations simultan´ees . Comp . Rend . Sci . Paris , 1847 . [ 8 ] Fran¸cois Chollet et al . Keras . https : / / keras . io , 2015 . [ 9 ] Michael B Cohen , Jelena Diakonikolas , and Lorenzo Orecchia . On acceleration with noise - corrupted gradients . arXiv preprint arXiv : 1805 . 12591 , 2018 . [ 10 ] Olivier Devolder , Fran¸cois Glineur , and Yurii Nesterov . First - order methods of smooth convex optimiza - tion with inexact oracle . Mathematical Programming , 146 ( 1 - 2 ) : 37 – 75 , 2014 . [ 11 ] Timothy Dozat . Incorporating nesterov momentum into adam . 2016 . [ 12 ] John Duchi , Elad Hazan , and Yoram Singer . Adaptive subgradient methods for online learning and stochastic optimization . Journal of machine learning research , 12 ( Jul ) : 2121 – 2159 , 2011 . [ 13 ] Robert M Freund and Haihao Lu . New computational guarantees for solving convex optimization problems with ﬁrst order methods , via a function growth condition measure . Mathematical Programming , 170 ( 2 ) : 445 – 477 , 2018 . [ 14 ] Pontus Giselsson and Stephen Boyd . Monotonicity and restart in fast gradient methods . In 53rd IEEE Conference on Decision and Control , pages 5058 – 5063 . IEEE , 2014 . [ 15 ] Gabriel Goh . Why momentum really works . Distill , 2 ( 4 ) : e6 , 2017 . [ 16 ] Ian Goodfellow , Yoshua Bengio , and Aaron Courville . Deep learning . MIT press , 2016 . [ 17 ] Ishaan Gulrajani , Faruk Ahmed , Martin Arjovsky , Vincent Dumoulin , and Aaron C Courville . Improved training of wasserstein gans . In Advances in neural information processing systems , pages 5767 – 5777 , 2017 . [ 18 ] Moritz Hardt . Robustness versus acceleration . http : / / blog . mrtz . org / 2014 / 08 / 18 / robustness - versus - acceleration . html , 2014 . [ 19 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual learning for image recognition . In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770 – 778 , 2016 . 12 [ 20 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Deep residual networks . https : / / github . com / KaimingHe / deep - residual - networks , 2016 . [ 21 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . Identity mappings in deep residual networks . In European conference on computer vision , pages 630 – 645 . Springer , 2016 . [ 22 ] Geoﬀrey Hinton , Nitish Srivastava , and Kevin Swersky . Neural networks for machine learning lecture 6a overview of mini - batch gradient descent . [ 23 ] W Ronny Huang , Zeyad Emam , Micah Goldblum , Liam Fowl , Justin K Terry , Furong Huang , and Tom Goldstein . Understanding generalization through visualizations . arXiv preprint arXiv : 1906 . 03291 , 2019 . [ 24 ] Anatoli Iouditski and Yuri Nesterov . Primal - dual subgradient methods for minimizing uniformly convex functions . arXiv preprint arXiv : 1401 . 1792 , 2014 . [ 25 ] Chi Jin , Praneeth Netrapalli , and Michael I Jordan . Accelerated gradient descent escapes saddle points faster than gradient descent . arXiv preprint arXiv : 1711 . 10456 , 2017 . [ 26 ] Diederik P Kingma and Jimmy Ba . Adam : A method for stochastic optimization . arXiv preprint arXiv : 1412 . 6980 , 2014 . [ 27 ] Alex Krizhevsky , Geoﬀrey Hinton , et al . Learning multiple layers of features from tiny images . 2009 . [ 28 ] Quoc V Le , Navdeep Jaitly , and Geoﬀrey E Hinton . A simple way to initialize recurrent networks of rectiﬁed linear units . arXiv preprint arXiv : 1504 . 00941 , 2015 . [ 29 ] Y . LECUN . The mnist database of handwritten digits . http : / / yann . lecun . com / exdb / mnist / . [ 30 ] Yann LeCun and Corinna Cortes . MNIST handwritten digit database . 2010 . [ 31 ] Qihang Lin and Lin Xiao . An adaptive accelerated proximal gradient method and its homotopy continuation for sparse optimization . In International Conference on Machine Learning , pages 73 – 81 , 2014 . [ 32 ] Liyuan Liu , Haoming Jiang , Pengcheng He , Weizhu Chen , Xiaodong Liu , Jianfeng Gao , and Jiawei Han . On the variance of the adaptive learning rate and beyond . In International Conference on Learning Representations , 2020 . [ 33 ] Ilya Loshchilov and Frank Hutter . Fixing weight decay regularization in adam . 2018 . [ 34 ] Laurens van der Maaten and Geoﬀrey Hinton . Visualizing data using t - sne . Journal of machine learning research , 9 ( Nov ) : 2579 – 2605 , 2008 . [ 35 ] Boris S Mordukhovich . Variational analysis and generalized diﬀerentiation I : Basic theory , volume 330 . Springer Science & Business Media , 2006 . [ 36 ] Arkaddii S Nemirovskii and Yu E Nesterov . Optimal methods of smooth convex minimization . USSR Computational Mathematics and Mathematical Physics , 25 ( 2 ) : 21 – 30 , 1985 . [ 37 ] Yu Nesterov . Gradient methods for minimizing composite functions . Mathematical Programming , 140 ( 1 ) : 125 – 161 , 2013 . [ 38 ] Yurii Nesterov . Introductory lectures on convex programming volume i : Basic course . 1998 . [ 39 ] Yurii E Nesterov . A method for solving the convex programming problem with convergence rate o ( 1 / kˆ 2 ) . In Dokl . akad . nauk Sssr , volume 269 , pages 543 – 547 , 1983 . [ 40 ] Stanley Osher , Bao Wang , Penghang Yin , Xiyang Luo , Farzin Barekat , Minh Pham , and Alex Lin . Laplacian smoothing gradient descent . arXiv preprint arXiv : 1806 . 06317 , 2018 . [ 41 ] Brendan OâĂŹdonoghue and Emmanuel Candes . Adaptive restart for accelerated gradient schemes . Foundations of computational mathematics , 15 ( 3 ) : 715 – 732 , 2015 . 13 [ 42 ] Adam Paszke , Sam Gross , Francisco Massa , Adam Lerer , James Bradbury , Gregory Chanan , Trevor Killeen , Zeming Lin , Natalia Gimelshein , Luca Antiga , et al . Pytorch : An imperative style , high - performance deep learning library . In Advances in Neural Information Processing Systems , pages 8024 – 8035 , 2019 . [ 43 ] Boris T Polyak . Some methods of speeding up the convergence of iteration methods . USSR Computational Mathematics and Mathematical Physics , 4 ( 5 ) : 1 – 17 , 1964 . [ 44 ] Sashank J Reddi , Satyen Kale , and Sanjiv Kumar . On the convergence of adam and beyond . arXiv preprint arXiv : 1904 . 09237 , 2019 . [ 45 ] James Renegar . Eﬃcient ﬁrst - order methods for linear programming and semideﬁnite programming . arXiv preprint arXiv : 1409 . 5832 , 2014 . [ 46 ] R Tyrrell Rockafellar . Convex analysis . Number 28 . Princeton university press , 1970 . [ 47 ] R Tyrrell Rockafellar and Roger J - B Wets . Variational analysis , volume 317 . Springer Science & Business Media , 2009 . [ 48 ] Vincent Roulet , Nicolas Boumal , and Alexandre d’Aspremont . Computational complexity versus statistical performance on sparse recovery problems . arXiv preprint arXiv : 1506 . 03295 , 2015 . [ 49 ] Vincent Roulet and Alexandre d’Aspremont . Sharpness , restart and acceleration . In Advances in Neural Information Processing Systems , pages 1119 – 1129 , 2017 . [ 50 ] Olga Russakovsky , Jia Deng , Hao Su , Jonathan Krause , Sanjeev Satheesh , Sean Ma , Zhiheng Huang , Andrej Karpathy , Aditya Khosla , Michael Bernstein , et al . Imagenet large scale visual recognition challenge . International journal of computer vision , 115 ( 3 ) : 211 – 252 , 2015 . [ 51 ] Weijie Su , Stephen Boyd , and Emmanuel Candes . A diﬀerential equation for modeling nesterovâĂŹs accelerated gradient method : Theory and insights . In Advances in Neural Information Processing Systems , pages 2510 – 2518 , 2014 . [ 52 ] Ilya Sutskever , James Martens , George Dahl , and Geoﬀrey Hinton . On the importance of initialization and momentum in deep learning . In International conference on machine learning , pages 1139 – 1147 , 2013 . [ 53 ] Svante Wold , Kim Esbensen , and Paul Geladi . Principal component analysis . Chemometrics and intelligent laboratory systems , 2 ( 1 - 3 ) : 37 – 52 , 1987 . [ 54 ] Wei Yang . Pytorch classiﬁcation . https : / / github . com / bearpaw / pytorch - classification , 2017 . [ 55 ] Matthew D Zeiler . Adadelta : an adaptive learning rate method . arXiv preprint arXiv : 1212 . 5701 , 2012 . [ 56 ] Sixin Zhang , Anna E Choromanska , and Yann LeCun . Deep learning with elastic averaging sgd . In Advances in neural information processing systems , pages 685 – 693 , 2015 . [ 57 ] Martin Zinkevich , Markus Weimer , Lihong Li , and Alex J Smola . Parallelized stochastic gradient descent . In Advances in neural information processing systems , pages 2595 – 2603 , 2010 . 14 A Uncontrolled Bound of NASGD Consider the following optimization problem min w = f ( w ) , ( 13 ) where f ( w ) is L - smooth and convex . Start from w k , GD update , with step size 1 r , can be obtained based on the minimization of the functional min v Q r ( v , w k ) : = h v − w k , ∇ f ( w k ) i + r 2 k v − w k k 22 . ( 14 ) With direct computation , we can get that Q r ( v k + 1 , w k ) − min Q r ( v , w k ) = k g k − ∇ f ( w k ) k 2 2 r , where g k : = 1 m P mj = 1 ∇ f i j ( w k ) . We assume the variance is bounded , which gives The stochastic gradient rule , R s , satisﬁes E [ Q r ( v k + 1 , w k ) − min Q r ( v , w k ) | χ k ] ≤ δ , with δ being a constant and χ k being the sigma algebra generated by w 1 , w 2 , · · · , w k , i . e . , χ k : = σ ( w 1 , w 2 , · · · , w k ) . NASGD can be reformulated as v k + 1 ≈ min v Q r ( v , w k ) with rule R s , w k + 1 = v k + 1 + t k − 1 t k + 1 ( v k + 1 − v k ) , ( 15 ) where t 0 = 1 and t k + 1 = ( 1 + p 1 + 4 t 2 k ) / 2 . A . 1 Preliminaries To proceed , we introduce several deﬁnitions and some useful properties in variational and convex analysis . More detailed background can be found at [ 35 , 38 , 46 , 47 ] . Let f be a convex function , we say that f is L - smooth ( gradient Lipschitz ) if f is diﬀerentiable and k∇ f ( v ) − ∇ f ( w ) k 2 ≤ L k v − w k 2 , and we say f is ν - strongly convex if for any w , v ∈ dom ( J ) f ( w ) ≥ f ( v ) + h∇ f ( v ) , w − v i + ν 2 k w − v k 22 . Below of this subsection , we list several basic but useful lemmas , the proof can be found in [ 38 ] . Lemma 1 . If f is ν - strongly convex , then for any v ∈ dom ( J ) we have f ( v ) − f ( v ∗ ) ≥ ν 2 k v − v ∗ k 22 , ( 16 ) where v ∗ is the minimizer of f . Lemma 2 . If f is L - smooth , for any w , v ∈ dom ( f ) , f ( w ) ≤ f ( v ) + h∇ f ( v ) , w − v i + L 2 k w − v k 22 . 15 A . 2 Uncontrolled Bound of NASGD In this part , we denote ˜ v k + 1 : = min v Q r ( v , w k ) . ( 17 ) Lemma 3 . If the constant r > 0 , then E (cid:0) k v k + 1 − ˜ v k + 1 k 22 | χ k (cid:1) ≤ 2 δ r . ( 18 ) Proof . Note that Q r ( v , w k ) is strongly convex with constant r , and ˜ v k + 1 in ( 17 ) is the minimizer of Q r ( v , w k ) . With Lemma 1 we have Q r ( v k + 1 , w k ) − Q r ( ˜ v k + 1 , w k ) ≥ r 2 k v k + 1 − ˜ v k + 1 k 22 . ( 19 ) Notice that E (cid:2) Q r ( v k + 1 , w k ) − Q r ( ˜ v k + 1 , w k ) (cid:3) = E h Q r ( v k + 1 , w k ) − min v Q r ( v , w k ) i ≤ δ . The inequality ( 18 ) can be established by combining the above two inequalities . Lemma 4 . If the constant satisfy r > L , then we have E (cid:16) f ( ˜ v k + 1 ) + r 2 k ˜ v k + 1 − w k k 22 − ( f ( v k + 1 ) + r 2 k v k + 1 − w k k 22 ) (cid:17) ( 20 ) ≥ − τδ − r − L 2 E [ k w k − ˜ v k + 1 k 22 ] , where τ = L 2 r ( r − L ) . Proof . The convexity of f gives us 0 ≤ h∇ f ( v k + 1 ) , v k + 1 − ˜ v k + 1 i + f ( ˜ v k + 1 ) − f ( v k + 1 ) , ( 21 ) From the deﬁnition of the stochastic gradient rule R s , we have − δ ≤ E (cid:0) Q r ( ˜ v k + 1 , w k ) − Q r ( v k + 1 , w k ) (cid:1) ( 22 ) = E h h ˜ v k + 1 − w k , ∇ f ( w k ) i + r 2 k ˜ v k + 1 − w k k 22 i − E h h v k + 1 − w k , ∇ f ( w k ) i + r 2 k v k + 1 − w k k 22 i . With ( 21 ) and ( 22 ) , we have − δ ≤ (cid:16) f ( v k + 1 ) + r 2 k v k + 1 − w k k 22 (cid:17) − (cid:16) f ( ˜ v k + 1 ) + r 2 k ˜ v k + 1 − w k k 22 (cid:17) + ( 23 ) E h∇ f ( w k ) − ∇ f ( ˜ v k + 1 ) , v k + 1 − ˜ v k + 1 i . With the Schwarz inequality h a , b i ≤ k a k 22 2 µ + µ 2 k b k 22 with µ = L 2 r − L , a = ∇ f ( v k + 1 ) − ∇ f ( ˜ v k + 1 ) and b = w k − ˜ v k + 1 , h∇ f ( w k ) − ∇ f ( ˜ v k + 1 ) , v k + 1 − ˜ v k + 1 i ( 24 ) ≤ ( r − L ) 2 L 2 k∇ f ( w k ) − ∇ f ( ˜ v k + 1 ) k 22 + L 2 2 ( r − L ) k v k + 1 − ˜ v k + 1 k 22 ≤ ( r − L ) 2 k w k − ˜ v k + 1 k 22 + L 2 2 ( r − L ) k v k + 1 − ˜ v k + 1 k 22 . 16 Combining ( 23 ) and ( 24 ) , we have − δ ≤ E (cid:16) f ( v k + 1 ) + r 2 k v k + 1 − w k k 22 (cid:17) − E (cid:16) f ( ˜ v k + 1 ) + r 2 k ˜ v k + 1 − w k k 22 (cid:17) ( 25 ) + L 2 2 ( r − L ) E k v k + 1 − ˜ v k + 1 k 22 + r − L 2 E k w k − ˜ v k + 1 k 22 . By rearrangement of the above inequality ( 25 ) and using Lemma 3 , we obtain the result . Lemma 5 . If the constants satisfy r > L , then we have the following bounds E (cid:0) f ( v k ) − f ( v k + 1 ) (cid:1) ≥ r 2 E k w k − v k + 1 k 22 + r E h w k − v k , ˜ v k + 1 − w k i − τδ , ( 26 ) E (cid:0) f ( v ∗ ) − f ( v k + 1 ) (cid:1) ≥ r 2 E k w k − v k + 1 k 22 + r E h w k − v ∗ , ˜ v k + 1 − w k i − τδ , ( 27 ) where τ : = L 2 r ( r − L ) and v ∗ is the minimum . Proof . With Lemma 2 , we have − f ( ˜ v k + 1 ) ≥ − f ( w k ) − h ˜ v k + 1 − w k , ∇ f ( w k ) i − L 2 k ˜ v k + 1 − w k k 22 . ( 28 ) Using the convexity of f , we have f ( v k ) − f ( w k ) ≥ h v k − w k , ∇ f ( w k ) i , i . e . , f ( v k ) ≥ f ( w k ) + h v k − w k , ∇ f ( w k ) i . ( 29 ) According to the deﬁnition of ˜ v k + 1 in ( 14 ) , i . e . , ˜ v k + 1 = min v Q r ( v , w k ) = min v h v − w k , ∇ f ( w k ) i + r 2 k v − w k k 22 , and the optimization condition gives ˜ v k + 1 = w k − 1 r ∇ f ( w k ) . ( 30 ) Substituting ( 30 ) into ( 29 ) , we obtain f ( v k ) ≥ f ( w k ) + h v k − w k , r ( w k − ˜ v k + 1 ) i . ( 31 ) Direct summation of ( 28 ) and ( 31 ) gives f ( v k ) − f ( ˜ v k + 1 ) ≥ (cid:18) r − L 2 (cid:19) k ˜ v k + 1 − w k k 22 + r h w k − v k , ˜ v k + 1 − w k i . ( 32 ) Summing ( 32 ) and ( 20 ) , we obtain the inequality ( 26 ) E (cid:2) f ( v k ) − f ( v k + 1 ) (cid:3) ≥ r 2 E k w k − v k + 1 k 22 + r E h w k − v k , ˜ v k + 1 − w k i − τδ . ( 33 ) On the other hand , with the convexity of f , we have f ( v ∗ ) − f ( w k ) ≥ h v ∗ − w k , ∇ f ( w k ) i = h v ∗ − w k , r ( w k − ˜ v k + 1 ) i . ( 34 ) The summation of ( 28 ) and ( 34 ) resulting in f ( v ∗ ) − f ( ˜ v k + 1 ) ≥ (cid:18) r − L 2 (cid:19) k w k − ˜ v k + 1 k 22 + r h w k − v ∗ , ˜ v k + 1 − w k i . ( 35 ) Summing ( 35 ) and ( 20 ) , we obtain E (cid:0) f ( v ∗ ) − f ( v k + 1 ) (cid:1) ≥ r 2 E k w k − v k + 1 k 22 + r E h w k − v ∗ , ˜ v k + 1 − w k i − τδ , ( 36 ) which is the same as ( 27 ) . 17 Theorem 3 ( Uncontrolled Bound of NASGD ( Theorem 1 restate ) ) . Let the constant satisﬁes r < L and the sequence { v k } k ≥ 0 be generated by NASGD , then we have E [ f ( v k ) − min v f ( v ) ] = O ( k ) . ( 37 ) Proof . We denote F k : = E ( f ( v k ) − f ( v ∗ ) ) . By ( 26 ) × ( t k − 1 ) + ( 27 ) , we have 2 [ ( t k − 1 ) F k − t k F k + 1 ] r ≥ t k E k v k + 1 − w k k 22 ( 38 ) + 2 E h ˜ v k + 1 − w k , t k w k − ( t k − 1 ) v k − v ∗ i − 2 τt k δ r . With t 2 k − 1 = t 2 k − t k , ( 38 ) × t k yields 2 [ t 2 k − 1 F k − t 2 k F k + 1 ] r ≥ E k t k v k + 1 − t k w k k 22 ( 39 ) + 2 t k E h ˜ v k + 1 − w k , t k w k − ( t k − 1 ) v k − v ∗ i − 2 τt 2 k δ r Substituting a = t k v k + 1 − ( t k − 1 ) v k − v ∗ and b = t k w k − ( t k − 1 ) v k − v ∗ into identity k a − b k 22 + 2 h a − b , b i = k a k 22 − k b k 22 . ( 40 ) It follows that E k t k v k + 1 − t k w k k 22 + 2 t k E h ˜ v k + 1 − w k , t k w k − ( t k − 1 ) v k − v ∗ i ( 41 ) = E k t k v k + 1 − t k w k k 22 + 2 t k E h v k + 1 − w k , t k w k − ( t k − 1 ) v k − v ∗ i + 2 t k E h ˜ v k + 1 − v k + 1 , t k w k − ( t k − 1 ) v k − v ∗ i = | { z } ( 40 ) E k t k v k + 1 − ( t k − 1 ) v k − v ∗ k 22 − k t k w k − ( t k − 1 ) v k − v ∗ k 22 + 2 t k E h ˜ v k + 1 − v k + 1 , t k w k − ( t k − 1 ) v k − v ∗ i = E k t k v k + 1 − ( t k − 1 ) v k − v ∗ k 22 − E k t k − 1 v k − ( t k − 1 − 1 ) v k − 1 − v ∗ k 22 + 2 t k E h ˜ v k + 1 − v k + 1 , t k − 1 v k − ( t k − 1 − 1 ) v k − 1 − v ∗ i . In the third identity , we used the fact t k w k = t k v k + ( t k − 1 − 1 ) ( v k − v k − 1 ) . If we denote u k = E k t k − 1 v k − ( t k − 1 − 1 ) v k − 1 − v ∗ k 2 2 , ( 39 ) can be rewritten as 2 t 2 k F k + 1 r + u k + 1 ≤ 2 t 2 k − 1 F k r + u k + 2 τt 2 k δ r ( 42 ) + 2 t k E h v k + 1 − ˜ v k + 1 , t k − 1 v k − ( t k − 1 − 1 ) v k − 1 − v ∗ i ≤ 2 t 2 k F k r + u k + 2 τt 2 k δ r + t 2 k − 1 R 2 , where we used 2 t k E h v k + 1 − ˜ v k + 1 , t k − 1 v k − ( t k − 1 − 1 ) v k − 1 − v ∗ i ≤ t 2 k E k v k + 1 − ˜ v k + 1 k 2 2 + E k t k − 1 v k − ( t k − 1 v k − ( t k − 1 − 1 ) v k − 1 − v ∗ ) k 2 2 = 2 t 2 k δ / r + t 2 k − 1 R 2 . Denoting ξ k : = 2 t 2 k − 1 F k r + u k , 18 then , we have ξ k + 1 ≤ ξ 0 + ( 2 τδ r + R 2 ) k X i = 1 t 2 i = O ( k 3 ) . ( 43 ) With the fact , ξ k : = 2 t 2 k − 1 F k r ≥ Ω ( k 2 ) F k , we then proved the result . B NAG with δ - Inexact Oracle & Experimental Settings in Section 3 . 1 In [ 10 ] , the authors deﬁnes δ - inexact gradient oracle for convex smooth optimization as follows : Deﬁnition 1 ( δ - Inexact Oracle ) . [ 10 ] For a convex L - smooth function f : R d → R . For ∀ w ∈ R d and exact ﬁrst - order oracle returns a pair ( f ( w ) , ∇ f ( w ) ) ∈ R × R d so that for ∀ v ∈ R d we have 0 ≤ f ( v ) − (cid:0) f ( w ) + h∇ f ( w ) , v − w i (cid:1) ≤ L 2 k w − v k 22 . A δ - inexact oracle returns a pair (cid:0) f δ ( w ) , ∇ f δ ( w ) (cid:1) ∈ R × R d so that ∀ v ∈ R d we have 0 ≤ f ( v ) − (cid:0) f δ ( w ) + h∇ f δ ( w ) , v − w i (cid:1) ≤ L 2 k w − v k 22 + δ . We have the following convergence results of GD and NAG under a δ - Inexact Oracle for convex smooth optimization . Theorem 4 . [ 10 ] 4 Consider min f ( w ) , w ∈ R d , where f ( w ) is convex and L - smooth with w ∗ being the minimum . Given access to δ - inexact oracle , GD with step size 1 / L returns a point w k after k steps so that f ( w k ) − f ( w ∗ ) = O (cid:18) L k (cid:19) + δ . On the other hand , NAG , with step size 1 / L returns f ( w k ) − f ( w ∗ ) = O (cid:18) L k 2 (cid:19) + O ( kδ ) . Theorem 4 says that NAG may not robust to a δ - inexact gradient . In the following , we will study the numerical behavior of a variety of ﬁrst - order algorithms for convex smooth optimizations with the following diﬀerent inexact gradients . Constant Variance Gaussian Noise : We consider the inexact oracle where the true gradient is contaminated with a Gaussian noise N ( 0 , 0 . 001 2 ) . We run 50K iterations of diﬀerent algorithms . For SRNAG , we restart after every 200 iterations . Fig . 2 ( b ) shows the iteration vs . optimal gap , f ( x k ) − f ( x ∗ ) , with x ∗ being the minimum . NAG with the inexact gradient due to constant variance noise does not converge . GD performs almost the same as ARNAG asymptotically , because ARNAG restarts too often and almost degenerates into GD . GD with constant momentum outperforms the three schemes above , and SRNAG slightly outperforms GD with constant momentum . Decaying Variance Gaussian Noise : Again , consider minimizing ( 9 ) with the same experimental setting as before except that ∇ f ( x ) is now contaminated with a decaying Gaussian noise N ( 0 , ( 0 . 1 b t / 100 c + 1 ) 2 ) . For SRNAG , we restart every 200 iterations in the ﬁrst 10 k iterations , and restart every 400 iterations in the remaining 40K iterations . Fig . 3 ( c ) shows the iteration vs . optimal gap by diﬀerent schemes . ARNAG still performs almost the same as GD . The path of NAG is oscillatory . GD with constant momentum again outperforms the previous three schemes . Here SRNAG signiﬁcantly outperforms all the other schemes . 4 We adopt the result from [ 18 ] . 19 Logisitic Regression for MNIST Classiﬁcation : We apply the above schemes with stochastic gradient to train a logistic regression model for MNIST classiﬁcation [ 30 ] . We consider ﬁve diﬀerent schemes , namely , SGD , SGD + ( constant ) momentum , NASGD , ASGD , and SRSGD . In ARSGD , we perform restart based on the loss value of the mini - batch training data . In SRSGD , we restart the NAG momentum after every 10 iterations . We train the logistic regression model with a ‘ 2 weight decay of 10 − 4 by running 20 epochs using diﬀerent schemes with batch size of 128 . The step sizes for all the schemes are set to 0 . 01 . Fig . 3 plots the training loss vs . iteration . In this case , NASGD does not converge , and SGD with momentum does not speed up SGD . ARSGD’s performance is on par with SGD’s . Again , SRSGD gives the best performance with the smallest training loss among these ﬁve schemes . C Convergence of SRSGD We prove the convergence of Nesterov accelerated SGD with scheduled restart , i . e . , the convergence of SRSGD . We denote that θ k : = t k − 1 t k + 1 in the Nesterov iteration and ˆ θ k is its use in the restart version , i . e . , SRSGD . For any restart frequency F ( positive integer ) , we have ˆ θ k = θ k −b k / F c∗ F . In the restart version , we can see that ˆ θ k ≤ θ F = : ¯ θ < 1 . Lemma 6 . Let the constant satisﬁes r > L and the sequence { v k } k ≥ 0 be generated by the SRSGD with restart frequency F ( any positive integer ) , we have k X i = 1 k v i − v i − 1 k 22 ≤ r 2 kR 2 ( 1 − ¯ θ ) 2 , ( 44 ) where ¯ θ : = θ F < 1 and R : = sup x { k∇ f ( x ) k } . Proof . It holds that k v k + 1 − w k k 2 = k v k + 1 − v k + v k − w k k 2 ( 45 ) ≥ k v k + 1 − v k k 2 − k v k − w k k 2 ≥ k v k + 1 − v k k 2 − ¯ θ k v k − v k − 1 k 2 . Thus , k v k + 1 − w k k 22 ≥ (cid:0) k v k + 1 − v k k − ¯ θ k v k − v k − 1 k (cid:1) 2 ( 46 ) = k v k + 1 − v k k 22 − 2¯ θ k v k − v k − 1 k 2 k v k − v k − 1 k 2 + ¯ θ 2 k v k − v k − 1 k 22 ≥ ( 1 − ¯ θ ) k v k + 1 − v k k 22 − ¯ θ ( 1 − ¯ θ ) k v k + 1 − v k k 22 . Summing ( 46 ) from k = 1 to K , we get ( 1 − ¯ θ ) 2 K X k = 1 k v k − v k − 1 k 22 ≤ K X k = 1 k v k + 1 − w k k ≤ r 2 KR 2 . ( 47 ) In the following , we denote A : = { k ∈ Z + | E f ( v k ) ≥ E f ( v k − 1 ) } . Theorem 5 ( Convergence of SRSGD ) . ( Theorem 2 restate ) For any L - smooth function f , let the constant satisﬁes r > L and the sequence { v k } k ≥ 0 be generated by the SRSGD with restart frequency F ( any positive integer ) . Assume that A is ﬁnite , then we have min 1 ≤ k ≤ K { E k∇ f ( w k ) k 22 } = O ( 1 r + 1 rK ) . ( 48 ) Therefore for ∀ (cid:15) > 0 , to get (cid:15) error bound , we just need to set r = O ( 1 (cid:15) ) and K = O ( 1 (cid:15) 2 ) . 20 Proof . L - smoothness of f , i . e . , Lipschitz gradient continuity , gives us f ( v k + 1 ) ≤ f ( w k ) + h∇ f ( w k ) , v k + 1 − w k i + L 2 k v k + 1 − w k k 22 ( 49 ) Taking expectation , we get E f ( v k + 1 ) ≤ E f ( w k ) − r E k∇ f ( w k ) k 22 + r 2 LR 2 2 . ( 50 ) On the other hand , we have f ( w k ) ≤ f ( v k ) + ˆ θ k h∇ f ( v k ) , v k − v k − 1 i + L ( ˆ θ k ) 2 2 k v k − v k − 1 k 22 . ( 51 ) Then , we have E f ( v k + 1 ) ≤ E f ( v k ) + ˆ θ k E h∇ f ( v k ) , v k − v k − 1 i ( 52 ) + L ( ˆ θ k ) 2 2 E k v k − v k − 1 k 22 − r E k∇ f ( w k ) k 22 + r 2 LR 2 2 . We also have ˆ θ k h∇ f ( v k ) , v k − v k − 1 i ≤ ˆ θ k (cid:18) f ( v k ) − f ( v k − 1 ) + L 2 k v k − v k − 1 k 22 (cid:19) . ( 53 ) We then get that E f ( v k + 1 ) ≤ E f ( v k ) + ˆ θ k (cid:0) E f ( v k ) − E f ( v k − 1 ) (cid:1) − r E k∇ f ( w k ) k 22 + A k , ( 54 ) where A k : = E L 2 k v k − v k − 1 k 22 + E L ( ˆ θ k ) 2 2 E k v k − v k − 1 k 22 + r 2 LR 2 2 . Summing the inequality gives us E f ( v K + 1 ) ≤ E f ( v 0 ) + ˜ θ X k ∈A (cid:0) E f ( v k ) − E f ( v k − 1 ) (cid:1) ( 55 ) − r K X k = 1 E k∇ f ( w k ) k 22 + K X k = 1 A k . It is easy to see that X k ∈A (cid:0) E f ( v k ) − E f ( v k − 1 ) (cid:1) = ¯ R < + ∞ , for the ﬁniteness of A , and K X k = 1 A k = O ( r 2 K ) . D Datasets and Implementation Details D . 1 CIFAR The CIFAR10 and CIFAR100 datasets [ 27 ] consist of 50K training images and 10K test images from 10 and 100 classes , respectively . Both training and test data are color images of size 32 × 32 . We run our CIFAR experiments on Pre - ResNet - 110 , 290 , 470 , 650 , and 1001 with 5 diﬀerent seeds [ 21 ] . We train each model for 200 epochs with batch size of 128 and initial learning rate of 0 . 1 , which is decayed by a factor of 10 at the 21 80th , 120th , and 160th epoch . The weight decay rate is 5 × 10 − 5 and the momentum for the SGD baseline is 0 . 9 . Random cropping and random horizontal ﬂipping are applied to training data . Our code is modiﬁed based on the Pytorch classiﬁcation project [ 54 ] , 5 which was also used by Liu et al . [ 32 ] . We provide the restarting frequencies for the exponential and linear scheme for CIFAR10 and CIFAR100 in Table 6 below . Using the same notation as in the main text , we denote F i as the restarting frequency at the i - th learning rate . Table 6 : Restarting frequencies for CIFAR10 and CIFAR100 experiments CIFAR10 CIFAR100 Linear schedule F 1 = 30 , F 2 = 60 , F 3 = 90 , F 4 = 120 ( r = 2 ) F 1 = 50 , F 2 = 100 , F 3 = 150 , F 4 = 200 ( r = 2 ) Exponential schedule F 1 = 40 , F 2 = 50 , F 3 = 63 , F 4 = 78 ( r = 1 . 25 ) F 1 = 45 , F 2 = 68 , F 3 = 101 , F 4 = 152 ( r = 1 . 50 ) D . 2 ImageNet The ImageNet dataset contains roughly 1 . 28 million training color images and 50K validation color images from 1000 classes [ 50 ] . We run our ImageNet experiments on ResNet - 50 , 101 , 152 , and 200 with 5 diﬀerent seeds . Following [ 19 , 21 ] , we train each model for 90 epochs with a batch size of 256 and decrease the learning rate by a factor of 10 at the 30th and 60th epoch . The initial learning rate is 0 . 1 , the momentum is 0 . 9 , and the weight decay rate is 1 × 10 − 5 . Random 224 × 224 cropping and random horizontal ﬂipping are applied to training data . We use the oﬃcial Pytorch ResNet implementation [ 42 ] , 6 and run our experiments on 8 Nvidia V100 GPUs . We report single - crop top - 1 and top - 5 errors of our models . In our experiments , we set F 1 = 40 at the 1st learning rate , F 2 = 80 at the 2nd learning rate , and F 3 is linearly decayed from 80 to 1 at the 3rd learning rate ( see Table 7 ) . Table 7 : Restarting frequencies for ImageNet experiments ImageNet Linear schedule F 1 = 40 , F 2 = 80 , F 3 : linearly decayed from 80 to 1 in the last 30 epochs D . 3 Training ImageNet in Fewer Number of Epochs : Table 8 contains the learning rate and restarting frequency schedule for our experiments on training ImageNet in fewer number of epochs , i . e . the reported results in Table 5 in the main text . Other settings are the same as in the full - training ImageNet experiments described in Section D . 2 above . Table 8 : Learning rate and restarting frequency schedule for ImageNet short training , i . e . Table 5 in the main text . ImageNet ResNet - 50 Decrease the learning rate by a factor of 10 at the 30th and 56th epoch . Train for a total of 80 epochs . F 1 = 60 , F 2 = 105 , F 3 : linearly decayed from 105 to 1 in the last 24 epochs ResNet - 101 Decrease the learning rate by a factor of 10 at the 30th and 56th epoch . Train for a total of 80 epochs . F 1 = 40 , F 2 = 80 , F 3 : linearly decayed from 80 to 1 in the last 24 epochs ResNet - 152 Decrease the learning rate by a factor of 10 at the 30th and 51th epoch . Train for a total of 75 epochs . F 1 = 40 , F 2 = 80 , F 3 : linearly decayed from 80 to 1 in the last 24 epochs ResNet - 200 Decrease the learning rate by a factor of 10 at the 30th and 46th epoch . Train for a total of 60 epochs . F 1 = 40 , F 2 = 80 , F 3 : linearly decayed from 80 to 1 in the last 14 epochs 5 Implementation available at https : / / github . com / bearpaw / pytorch - classiﬁcation 6 Implementation available at https : / / github . com / pytorch / examples / tree / master / imagenet 22 Additional Implementation Details : Implementation details for the ablation study of error rate vs . reduction in epochs and the ablation study of impact of restarting frequency are provided in Section F and G below . E SRSGD vs . SGD and SGD + NM on ImageNet Classiﬁcation and Other Tasks E . 1 Comparing with SGD with Nesterov Momentum on ImageNet Classiﬁca - tion In this section , we compare SRSGD with SGD with Nesterov constant momentum ( SGD + NM ) in training ResNets for ImageNet classiﬁcation . All hyper - parameters of SGD with constant Nesterov momentum used in our experiments are the same as those of SGD described in section D . 2 . We list the results in Table 9 . Again , SRSGD remarkably outperforms SGD + NM in training ResNets for ImageNet classiﬁcation , and as the network goes deeper the improvement becomes more signiﬁcant . Table 9 : Single crop validation errors ( % ) on ImageNet of ResNets trained with SGD + NM and SRSGD . We report the results of SRSGD with the increasing restarting frequency in the ﬁrst two learning rates . In the last learning rate , the restarting frequency is linearly decreased from 70 to 1 . For baseline results , we also include the reported single - crop validation errors [ 20 ] ( in parentheses ) . Network # Params SGD + NM SRSGD Improvement top - 1 top - 5 top - 1 top - 5 top - 1 top - 5 ResNet - 50 25 . 56M 24 . 27 ± 0 . 07 7 . 17 ± 0 . 07 23 . 85 ± 0 . 09 23 . 85 ± 0 . 09 23 . 85 ± 0 . 09 7 . 10 ± 0 . 09 7 . 10 ± 0 . 09 7 . 10 ± 0 . 09 0 . 42 0 . 07 ResNet - 101 44 . 55M 22 . 32 ± 0 . 05 6 . 18 ± 0 . 05 22 . 06 ± 0 . 10 22 . 06 ± 0 . 10 22 . 06 ± 0 . 10 6 . 09 ± 0 . 07 6 . 09 ± 0 . 07 6 . 09 ± 0 . 07 0 . 26 0 . 09 ResNet - 152 60 . 19M 21 . 77 ± 0 . 14 5 . 86 ± 0 . 09 21 . 46 ± 0 . 07 21 . 46 ± 0 . 07 21 . 46 ± 0 . 07 5 . 69 ± 0 . 03 5 . 69 ± 0 . 03 5 . 69 ± 0 . 03 0 . 31 0 . 17 ResNet - 200 64 . 67M 21 . 98 ± 0 . 22 5 . 99 ± 0 . 20 20 . 93 ± 0 . 13 20 . 93 ± 0 . 13 20 . 93 ± 0 . 13 5 . 57 ± 0 . 05 5 . 57 ± 0 . 05 5 . 57 ± 0 . 05 1 . 05 0 . 42 E . 2 Long Short - Term Memory ( LSTM ) Training for Pixel - by - Pixel MNIST In this task , we examine the advantage of SRSGD over SGD and SGD with Nesterov Momentum in training recurrent neural networks . In our experiments , we use an LSTM with diﬀerent numbers of hidden units ( 128 , 256 , and 512 ) to classify samples from the well - known MNIST dataset [ 29 ] . We follow the implementation of [ 28 ] and feed each pixel of the image into the RNN sequentially . In addition , we choose a random permutation of 28 × 28 = 784 elements at the beginning of the experiment . This ﬁxed permutation is applied to training and testing sequences . This task is known as permuted MNIST classiﬁcation , which has become standard to measure the performance of RNNs and their ability to capture long term dependencies . Implementation and Training Details : For the LSTM model , we initialize the forget bias to 1 and other biases to 0 . All weights matrices are initialized orthogonally except for the hidden - to - hidden weight matrices , which are initialized to be identity matrices . We train each model for 350 epochs with the initial learning rate of 0 . 01 . The learning rate was reduced by a factor of 10 at epoch 200 and 300 . The momentum is set to 0 . 9 for SGD with standard and Nesterov constant momentum . The restart schedule for SRSGD is set to 90 , 30 , 90 . The restart schedule changes at epoch 200 and 300 . In all experiments , we use batch size 128 and the gradients are clipped so that their L2 norm are at most 1 . Our code is based on the code from the exponential RNN’s Github . 7 Results : Our experiments corroborate the superiority of SRSGD over the two baselines . SRSGD yields much smaller test error and converges faster than SGD with standard and Nesterov constant momentum across all settings with diﬀerent number of LSTM hidden units . We summarize our results in Table 10 and Figure 7 . 7 Implementation available at https : / / github . com / Lezcano / expRNN 23 Table 10 : Test errors ( % ) on Permuted MNIST of trained with SGD , SGD + NM and SRSGD . The LSTM model has 128 hidden units . In all experiments , we use the initial learning rate of 0 . 01 , which is reduced by a factor of 10 at epoch 200 and 300 . All models are trained for 350 epochs . The momentum for SGD and SGD + NM is set to 0 . 9 . The restart schedule in SRSGD is set to 90 , 30 , and 90 . Network No . Hidden Units SGD SGD + NM SRSGD Improvement over SGD / SGD + NM LSTM 128 10 . 10 ± 0 . 57 9 . 75 ± 0 . 69 8 . 61 ± 0 . 30 8 . 61 ± 0 . 30 8 . 61 ± 0 . 30 1 . 49 / 1 . 14 LSTM 256 10 . 42 ± 0 . 63 10 . 09 ± 0 . 61 9 . 03 ± 0 . 23 9 . 03 ± 0 . 23 9 . 03 ± 0 . 23 1 . 39 / 1 . 06 LSTM 512 10 . 04 ± 0 . 35 9 . 55 ± 1 . 09 8 . 49 ± 1 . 59 8 . 49 ± 1 . 59 8 . 49 ± 1 . 59 1 . 55 / 1 . 06 SGD + Momentum SRSGD SGD + Nesterov Momentum T r a i n l o ss Iteration Figure 7 : Training loss vs . training iterations of LSTM trained with SGD ( red ) , SGD + NM ( green ) , and SRSGD ( blue ) for PMNIST classiﬁcation tasks . E . 3 Wasserstein Generative Adversarial Networks ( WGAN ) Training on MNIST We investigate the advantage of SRSGD over SGD with standard and Nesterov momentum in training deep generative models . In our experiments , we train a WGAN with gradient penalty [ 17 ] on MNIST . We evaluate our models using the discriminator’s loss , i . e . the Earth Moving distance estimate , since in WGAN lower discriminator loss and better sample quality are correlated [ 2 ] . Implementation and Training Details : The detailed implementations of our generator and discriminator are given below . For the generator , we set latent dim to 100 and d to 32 . For the discriminator , we set d to 32 . We train each model for 350 epochs with the initial learning rate of 0 . 01 . The learning rate was reduced by a factor of 10 at epoch 200 and 300 . The momentum is set to 0 . 9 for SGD with standard and Nesterov constant momentum . The restart schedule for SRSGD is set to 60 , 120 , 180 . The restart schedule changes at epoch 200 and 300 . In all experiments , we use batch size 64 . Our code is based on the code from the Pytorch WGAN - GP Github . 8 import torch import torch . nn as nn class Generator ( nn . Module ) : def i n i t ( self , latent dim , d = 32 ) : super ( ) . i n i t ( ) s e l f . net = nn . Sequential ( nn . ConvTranspose2d ( latent dim , d ∗ 8 , 4 , 1 , 0 ) , nn . BatchNorm2d ( d ∗ 8 ) , nn . ReLU ( True ) , 8 Implementation available at https : / / github . com / arturml / pytorch - wgan - gp 24 nn . ConvTranspose2d ( d ∗ 8 , d ∗ 4 , 4 , 2 , 1 ) , nn . BatchNorm2d ( d ∗ 4 ) , nn . ReLU ( True ) , nn . ConvTranspose2d ( d ∗ 4 , d ∗ 2 , 4 , 2 , 1 ) , nn . BatchNorm2d ( d ∗ 2 ) , nn . ReLU ( True ) , nn . ConvTranspose2d ( d ∗ 2 , 1 , 4 , 2 , 1 ) , nn . Tanh ( ) ) def forward ( self , x ) : return s e l f . net ( x ) class Discriminator ( nn . Module ) : def i n i t ( self , d = 32 ) : super ( ) . i n i t ( ) s e l f . net = nn . Sequential ( nn . Conv2d ( 1 , d , 4 , 2 , 1 ) , nn . InstanceNorm2d ( d ) , nn . LeakyReLU ( 0 . 2 ) , nn . Conv2d ( d , d ∗ 2 , 4 , 2 , 1 ) , nn . InstanceNorm2d ( d ∗ 2 ) , nn . LeakyReLU ( 0 . 2 ) , nn . Conv2d ( d ∗ 2 , d ∗ 4 , 4 , 2 , 1 ) , nn . InstanceNorm2d ( d ∗ 4 ) , nn . LeakyReLU ( 0 . 2 ) , nn . Conv2d ( d ∗ 4 , 1 , 4 , 1 , 0 ) , ) def forward ( self , x ) : outputs = s e l f . net ( x ) return outputs . squeeze ( ) Results : Our SRSGD is still better than both the baselines . SRSGD achieves smaller discriminator loss , i . e . Earth Moving distance estimate , and converges faster than SGD with standard and Nesterov constant momentum . We summarize our results in Table 11 and Figure 8 . We also demonstrate the digits generated by the trained WGAN in Figure 9 . By visually evaluation , we observe that samples generated by the WGAN trained with SRSGD look slightly better than those generated by the WGAN trained with SGD with standard and Nesterov constant momentum . Table 11 : Discriminator loss ( i . e . Earth Moving distance estimate ) of the WGAN with gradient penalty trained on MNIST with SGD , SGD + NM and SRSGD . In all experiments , we use the initial learning rate of 0 . 01 , which is reduced by a factor of 10 at epoch 200 and 300 . All models are trained for 350 epochs . The momentum for SGD and SGD + NM is set to 0 . 9 . The restart schedule in SRSGD is set to 60 , 120 , and 180 . Task SGD SGD + NM SRSGD Improvement over SGD / SGD + NM MNIST 0 . 71 ± 0 . 10 0 . 58 ± 0 . 03 0 . 44 ± 0 . 06 0 . 44 ± 0 . 06 0 . 44 ± 0 . 06 0 . 27 / 0 . 14 F Error Rate vs . Reduction in Training Epochs F . 1 Implementation Details CIFAR10 ( Figure 5 , left , in the main text ) and CIFAR100 ( Figure 10 in this Appendix ) : Except for learning rate schedule , we use the same setting described in Section D . 1 above and Section 4 . 1 in the main text . Table 12 contains the learning rate schedule for each number of epoch reduction in Figure 10 ( left ) in the main text and Figure 10 below . ImageNet ( Figure 10 , right , in the main text ) : Except for the total number of training epochs , other settings are similar to experiments for training ImageNet in fewer number of epochs described in 25 SGD + Momentum SRSGD SGD + Nesterov Momentum Epoch E a r t h M o v i n g D i s t a n c e E s t i m a t e Figure 8 : Earth Moving distance estimate ( i . e . discriminator loss ) vs . training epochs of WGAN with gradient penalty trained with SGD ( red ) , SGD + NM ( green ) , and SRSGD ( blue ) on MNIST . SGD SGD + NM SRSGD Figure 9 : MNIST digits generated by WGAN trained with gradient penalty by SGD ( left ) , SGD + NM ( middle ) , and SRSGD ( right ) . Section D . 3 . In particular , the learning rate and restarting frequency schedule still follow those in Table 8 above . We examine diﬀerent numbers of training epochs : 90 ( 0 epoch reduction ) , 80 ( 10 epochs reduction ) , 75 ( 15 epochs reduction ) , 70 ( 20 epochs reduction ) , 65 ( 25 epochs reduction ) , and 60 ( 30 epochs reduction ) . F . 2 Additional Experimental Results Table 13 and Table 14 provide detailed test errors vs . number of training epoch reduction reported in Figure 10 in the main text . We also conduct an additional ablation study of error rate vs . reduction in epochs for CIFAR100 and include the results in Figure 10 and Table 15 below . 26 Table 12 : Learning rate ( LR ) schedule for the ablation study of error rate vs . reduction in training epochs for CIFAR10 experiments , i . e . Figure 10 ( left ) in the main text and for CIFAR100 experiments , i . e . Figure 10 in this Appendix . # of Epoch Reduction LR Schedule 0 Decrease the LR by a factor of 10 at the 80th , 120th and 160th epoch . Train for a total of 200 epochs . 15 Decrease the LR by a factor of 10 at the 80th , 115th and 150th epoch . Train for a total of 185 epochs . 30 Decrease the LR by a factor of 10 at the 80th , 110th and 140th epoch . Train for a total of 170 epochs . 45 Decrease the LR by a factor of 10 at the 80th , 105th and 130th epoch . Train for a total of 155 epochs . 60 Decrease the LR by a factor of 10 at the 80th , 100th and 120th epoch . Train for a total of 140 epochs . 75 Decrease the LR by a factor of 10 at the 80th , 95th and 110th epoch . Train for a total of 125 epochs . 90 Decrease the LR by a factor of 10 at the 80th , 90th and 100th epoch . Train for a total of 110 epochs . Table 13 : Test error vs . number of training epochs for CIFAR10 Network 110 ( 90 less ) 125 ( 75 less ) 140 ( 60 less ) 155 ( 45 less ) 170 ( 30 less ) 185 ( 15 less ) 200 ( full trainings ) Pre - ResNet - 110 5 . 37 ± 0 . 11 5 . 27 ± 0 . 17 5 . 15 ± 0 . 09 5 . 09 ± 0 . 14 4 . 96 ± 0 . 14 4 . 96 ± 0 . 13 4 . 93 ± 0 . 13 4 . 93 ± 0 . 13 4 . 93 ± 0 . 13 Pre - ResNet - 290 4 . 80 ± 0 . 14 4 . 71 ± 0 . 13 4 . 58 ± 0 . 11 4 . 45 ± 0 . 09 4 . 43 ± 0 . 09 4 . 44 ± 0 . 11 4 . 37 ± 0 . 15 4 . 37 ± 0 . 15 4 . 37 ± 0 . 15 Pre - ResNet - 470 4 . 52 ± 0 . 16 4 . 43 ± 0 . 12 4 . 29 ± 0 . 11 4 . 33 ± 0 . 07 4 . 23 ± 0 . 12 4 . 18 ± 0 . 09 4 . 18 ± 0 . 09 4 . 18 ± 0 . 09 4 . 18 ± 0 . 09 4 . 18 ± 0 . 09 4 . 18 ± 0 . 09 Pre - ResNet - 650 4 . 35 ± 0 . 10 4 . 24 ± 0 . 05 4 . 22 ± 0 . 15 4 . 10 ± 0 . 15 4 . 12 ± 0 . 14 4 . 02 ± 0 . 05 4 . 00 ± 0 . 07 4 . 00 ± 0 . 07 4 . 00 ± 0 . 07 Pre - ResNet - 1001 4 . 23 ± 0 . 19 4 . 13 ± 0 . 12 4 . 08 ± 0 . 15 4 . 10 ± 0 . 09 3 . 93 ± 0 . 11 4 . 06 ± 0 . 14 3 . 87 ± 0 . 07 3 . 87 ± 0 . 07 3 . 87 ± 0 . 07 Table 14 : Top 1 single crop validation error vs . number of training epochs for ImageNet Network 60 ( 30 less ) 65 ( 25 less ) 70 ( 20 less ) 75 ( 15 less ) 80 ( 10 less ) 90 ( full trainings ) ResNet - 50 25 . 42 ± 0 . 42 25 . 02 ± 0 . 15 24 . 77 ± 0 . 14 24 . 38 ± 0 . 01 24 . 30 ± 0 . 21 23 . 85 ± 0 . 09 23 . 85 ± 0 . 09 23 . 85 ± 0 . 09 ResNet - 101 23 . 11 ± 0 . 10 22 . 79 ± 0 . 01 22 . 71 ± 0 . 21 22 . 56 ± 0 . 10 22 . 44 ± 0 . 03 22 . 06 ± 0 . 10 22 . 06 ± 0 . 10 22 . 06 ± 0 . 10 ResNet - 152 22 . 28 ± 0 . 20 22 . 12 ± 0 . 04 21 . 97 ± 0 . 04 21 . 79 ± 0 . 07 21 . 70 ± 0 . 07 21 . 46 ± 0 . 07 21 . 46 ± 0 . 07 21 . 46 ± 0 . 07 ResNet - 200 21 . 92 ± 0 . 17 21 . 69 ± 0 . 20 21 . 64 ± 0 . 03 21 . 45 ± 0 . 06 21 . 30 ± 0 . 03 20 . 93 ± 0 . 13 20 . 93 ± 0 . 13 20 . 93 ± 0 . 13 Table 15 : Test error vs . number of training epochs for CIFAR100 Network 110 ( 90 less ) 125 ( 75 less ) 140 ( 60 less ) 155 ( 45 less ) 170 ( 30 less ) 185 ( 15 less ) 200 ( full trainings ) Pre - ResNet - 110 24 . 06 ± 0 . 26 23 . 82 ± 0 . 24 23 . 82 ± 0 . 28 23 . 58 ± 0 . 18 23 . 69 ± 0 . 21 23 . 73 ± 0 . 34 23 . 49 ± 0 . 23 23 . 49 ± 0 . 23 23 . 49 ± 0 . 23 Pre - ResNet - 290 21 . 96 ± 0 . 45 21 . 77 ± 0 . 21 21 . 67 ± 0 . 37 21 . 56 ± 0 . 33 21 . 38 ± 0 . 44 21 . 38 ± 0 . 44 21 . 38 ± 0 . 44 21 . 47 ± 0 . 32 21 . 49 ± 0 . 27 Pre - ResNet - 470 21 . 35 ± 0 . 17 21 . 25 ± 0 . 17 21 . 21 ± 0 . 18 21 . 09 ± 0 . 28 20 . 87 ± 0 . 28 20 . 81 ± 0 . 32 20 . 71 ± 0 . 32 20 . 71 ± 0 . 32 20 . 71 ± 0 . 32 Pre - ResNet - 650 21 . 18 ± 0 . 27 20 . 95 ± 0 . 13 20 . 77 ± 0 . 31 20 . 61 ± 0 . 19 20 . 57 ± 0 . 13 20 . 47 ± 0 . 07 20 . 36 ± 0 . 25 20 . 36 ± 0 . 25 20 . 36 ± 0 . 25 Pre - ResNet - 1001 20 . 27 ± 0 . 17 20 . 03 ± 0 . 13 20 . 05 ± 0 . 22 19 . 74 ± 0 . 18 19 . 71 ± 0 . 22 19 . 67 ± 0 . 22 19 . 67 ± 0 . 22 19 . 67 ± 0 . 22 19 . 75 ± 0 . 11 G Impact of Restarting Frequency for ImageNet and CIFAR100 G . 1 Implementation Details For the CIFAR10 experiments on Pre - ResNet - 290 in Figure 6 in the main text , as well as the CIFAR100 and ImageNet experiments in Figure 11 and 12 in this Appendix , we vary the initial restarting frequency F 1 . Other settings are the same as described in Section D above . G . 2 Additional Experimental Results To complete our study on the impact of restarting frequency in Section 5 . 2 in the main text , we examine the case of CIFAR100 and ImageNet in this section . We summarize our results in Figure 11 and 12 below . 27 T e s t E rr o r Number of Epoch Reduction Pre - ResNet - 101 Pre - ResNet - 290 Pre - ResNet - 470 Pre - ResNet - 650 Pre - ResNet - 1001 CIFAR100 Figure 10 : Test error vs . number of epoch reduction in CIFAR100 training . The dashed lines are test errors of the SGD baseline . For CIFAR100 , SRSGD training with fewer epochs can achieve comparable results to SRSGD training with full 200 epochs . In some cases , such as with Pre - ResNet - 290 and 1001 , SRSGD training with fewer epochs achieves even better results than SRSGD training with full 200 epochs . T r a i n L o ss Epoch T e s t E rr o r Initial Restarting Frequency ( F 1 ) Approximate SGD without momentum Approximate NASGD Approximate SGD without momentum Approximate NASGD CIFAR100 Figure 11 : Training loss and test error of Pre - ResNet - 290 trained on CIFAR100 with diﬀerent initial restarting frequencies F 1 ( linear schedule ) . SRSGD with small F 1 approximates SGD without momentum , while SRSGD with large F 1 approximates NASGD . H Full Training with Less Epochs at the Intermediate Learning Rates We explore SRSGD full training ( 200 epochs on CIFAR and 90 epochs on ImageNet ) with less number of epochs at the intermediate learning rates and report the results in Table 16 , 17 , 18 and Figure 13 , 14 , 15 below . The settings and implementation details here are similar to those in Section F of this Appendix , but using all 200 epochs for CIFAR experiments and 90 epochs for ImageNet experiments . 28 T r a i n L o ss Epoch T e s t E rr o r Initial Restarting Frequency ( F 1 ) Approximate SGD without momentum Approximate NASGD Approximate SGD without momentum Approximate NASGD ImageNet Figure 12 : Training loss and test error of ResNet - 101 trained on ImageNet with diﬀerent initial restarting frequencies F 1 . We use linear schedule and linearly decrease the restarting frequency to 1 at the last learning rate . SRSGD with small F 1 approximates SGD without momentum , while SRSGD with large F 1 approximates NASGD . T e s t E rr o r Number of Epoch Reduction at the 2 nd and 3 rd learning rate Pre - ResNet - 101 Pre - ResNet - 290 Pre - ResNet - 470 Pre - ResNet - 650 Pre - ResNet - 1001 Figure 13 : Test error when using new learning rate schedules with less training epochs at the 2nd and 3rd learning rate for CIFAR10 . We still train in full 200 epochs in this experiment . On the x - axis , 10 , for example , means we reduce the number of training epochs by 10 at each intermediate learning rate , i . e . the 2nd and 3rd learning rate . The dashed lines are test errors of the SGD baseline . Table 16 : Test error when using new learning rate schedules with less training epochs at the 2nd and 3rd learning rate for CIFAR10 . We still train in full 200 epochs in this experiment . In the table , 80 - 90 - 100 , for example , means we reduce the learning rate by factor of 10 at the 80th , 90th , and 100th epoch . Network 80 - 90 - 100 80 - 95 - 110 80 - 100 - 120 80 - 105 - 130 80 - 110 - 140 80 - 115 - 150 80 - 120 - 160 Pre - ResNet - 110 5 . 32 ± 0 . 14 5 . 24 ± 0 . 17 5 . 11 ± 0 . 13 5 . 04 ± 0 . 15 4 . 92 ± 0 . 15 4 . 92 ± 0 . 15 4 . 92 ± 0 . 15 4 . 95 ± 0 . 12 4 . 93 ± 0 . 13 Pre - ResNet - 290 4 . 73 ± 0 . 13 4 . 67 ± 0 . 12 4 . 53 ± 0 . 10 4 . 40 ± 0 . 11 4 . 42 ± 0 . 09 4 . 42 ± 0 . 10 4 . 37 ± 0 . 15 4 . 37 ± 0 . 15 4 . 37 ± 0 . 15 Pre - ResNet - 470 4 . 48 ± 0 . 16 4 . 34 ± 0 . 10 4 . 25 ± 0 . 12 4 . 28 ± 0 . 10 4 . 19 ± 0 . 10 4 . 14 ± 0 . 07 4 . 14 ± 0 . 07 4 . 14 ± 0 . 07 4 . 18 ± 0 . 09 Pre - ResNet - 650 4 . 25 ± 0 . 13 4 . 12 ± 0 . 06 4 . 13 ± 0 . 09 4 . 03 ± 0 . 11 4 . 04 ± 0 . 11 4 . 04 ± 0 . 04 4 . 00 ± 0 . 07 4 . 00 ± 0 . 07 4 . 00 ± 0 . 07 Pre - ResNet - 1001 4 . 14 ± 0 . 18 4 . 06 ± 0 . 12 4 . 04 ± 0 . 15 4 . 08 ± 0 . 09 3 . 92 ± 0 . 13 4 . 05 ± 0 . 14 3 . 87 ± 0 . 07 3 . 87 ± 0 . 07 3 . 87 ± 0 . 07 29 Table 17 : Test error when using new learning rate schedules with less training epochs at the 2nd and 3rd learning rate for CIFAR100 . We still train in full 200 epochs in this experiment . In the table , 80 - 90 - 100 , for example , means we reduce the learning rate by factor of 10 at the 80th , 90th , and 100th epoch . Network 80 - 90 - 100 80 - 95 - 110 80 - 100 - 120 80 - 105 - 130 80 - 110 - 140 80 - 115 - 150 80 - 120 - 160 Pre - ResNet - 110 23 . 65 ± 0 . 14 23 . 96 ± 0 . 26 23 . 97 ± 0 . 31 23 . 53 ± 0 . 13 23 . 57 ± 0 . 36 23 . 68 ± 0 . 24 23 . 49 ± 0 . 23 23 . 49 ± 0 . 23 23 . 49 ± 0 . 23 Pre - ResNet - 290 21 . 94 ± 0 . 44 21 . 71 ± 0 . 27 21 . 55 ± 0 . 40 21 . 44 ± 0 . 31 21 . 37 ± 0 . 45 21 . 37 ± 0 . 45 21 . 37 ± 0 . 45 21 . 47 ± 0 . 32 21 . 49 ± 0 . 27 Pre - ResNet - 470 21 . 29 ± 0 . 11 21 . 21 ± 0 . 14 21 . 17 ± 0 . 18 20 . 99 ± 0 . 28 20 . 81 ± 0 . 22 20 . 80 ± 0 . 31 20 . 71 ± 0 . 32 20 . 71 ± 0 . 32 20 . 71 ± 0 . 32 Pre - ResNet - 650 21 . 11 ± 0 . 24 20 . 91 ± 0 . 17 20 . 66 ± 0 . 33 20 . 52 ± 0 . 18 20 . 51 ± 0 . 16 20 . 43 ± 0 . 10 20 . 36 ± 0 . 25 20 . 36 ± 0 . 25 20 . 36 ± 0 . 25 Pre - ResNet - 1001 20 . 21 ± 0 . 15 20 . 00 ± 0 . 11 19 . 86 ± 0 . 19 19 . 55 ± 0 . 19 19 . 55 ± 0 . 19 19 . 55 ± 0 . 19 19 . 69 ± 0 . 21 19 . 60 ± 0 . 17 19 . 75 ± 0 . 11 T e s t E rr o r Number of Epoch Reduction at the 2 nd and 3 rd learning rate Pre - ResNet - 101 Pre - ResNet - 290 Pre - ResNet - 470 Pre - ResNet - 650 Pre - ResNet - 1001 CIFAR100 Figure 14 : Test error when using new learning rate schedules with less training epochs at the 2nd and 3rd learning rate for CIFAR100 . We still train in full 200 epochs in this experiment . On the x - axis , 10 , for example , means we reduce the number of training epochs by 10 at each intermediate learning rate , i . e . the 2nd and 3rd learning rate . The dashed lines are test errors of the SGD baseline . Table 18 : Top 1 single crop validation error when using new learning rate schedules with less training epochs at the 2nd learning rate for ImageNet . We still train in full 90 epochs in this experiment . In the table , 30 - 40 , for example , means we reduce the learning rate by factor of 10 at the 30th and 40th epoch . Network 30 - 40 30 - 45 30 - 50 30 - 55 30 - 60 ResNet - 50 24 . 44 ± 0 . 16 24 . 06 ± 0 . 15 24 . 05 ± 0 . 09 23 . 89 ± 0 . 14 23 . 85 ± 0 . 09 23 . 85 ± 0 . 09 23 . 85 ± 0 . 09 ResNet - 101 22 . 49 ± 0 . 09 22 . 51 ± 0 . 05 22 . 24 ± 0 . 01 22 . 20 ± 0 . 01 22 . 06 ± 0 . 10 22 . 06 ± 0 . 10 22 . 06 ± 0 . 10 ResNet - 152 22 . 02 ± 0 . 01 21 . 84 ± 0 . 03 21 . 65 ± 0 . 14 21 . 55 ± 0 . 06 21 . 46 ± 0 . 07 21 . 46 ± 0 . 07 21 . 46 ± 0 . 07 ResNet - 200 21 . 65 ± 0 . 02 21 . 27 ± 0 . 14 21 . 12 ± 0 . 02 21 . 07 ± 0 . 01 20 . 93 ± 0 . 13 20 . 93 ± 0 . 13 20 . 93 ± 0 . 13 I Visualization of SRSGD’s trajectory Here we visualize the training trajectory through bad minima of SRSGD , SGD with constant momentum , and SGD . In particular , we train a neural net classiﬁer on a swiss roll data as in [ 23 ] and ﬁnd bad minima along its training . Each red dot in Figure 16 represents the trained model after each 10 epochs in the training . From each red dot , we search for nearby bad local minima , which are the blue dots . Those bad local minima achieve good training error but bad test error . We plots the trained models and bad local minima using PCA [ 53 ] and t - SNE [ 34 ] embedding . The blue color bar is for the test accuracy of bad local minima ; the red color 30 Number of Epoch Reduction at the 2 nd learning rate S i n g l e C r o p V a li d a t i o n E rr o r ResNet - 50 ResNet - 101 ResNet - 152 ResNet - 200 ImageNet Figure 15 : Test error when using new learning rate schedules with less training epochs at the 2nd learning rate for ImageNet . We still train in full 90 epochs in this experiment . On the x - axis , 10 , for example , means we reduce the number of training epochs by 10 at the 2nd learning rate . The dashed lines are test errors of the SGD baseline . bar is for the number of training epochs . ( CONTINUED NEXT PAGE ) 31 SGD + Momentum SGD SRSGD PCA Embedding of the Trajectory SGD + Momentum SGD SRSGD t - SNE Embedding of the Trajectory Figure 16 : Trajectory through bad minima of SGD , SGD with constant momentum , and SRSGD during the training : we train a neural net classiﬁer and plot the iterates of SGD after each ten epoch ( red dots ) . We also plot locations of nearby âĂĲbadâĂİ minima with poor generalization ( blue dots ) . We visualize these using PCA and t - SNE embedding . The blue color bar is for the test accuracy of bad local minima while the red color bar is for the number of training epochs . All blue dots for SGD with constant momentum and SRSGD achieve near perfect train accuracy , but with test accuracy below 59 % . All blue dots for SGD achieves average train accuracy of 73 . 11 % and with test accuracy also below 59 % . The ﬁnal iterate ( yellow star ) of SGD , SGD with constant momentum , and SRSGD achieve 73 . 13 % , 99 . 25 % , and 100 . 0 % test accuracy , respectively . J SRSGD Implementation in Pytorch import torch from . optimizer import Optimizer , required class SRSGD ( Optimizer ) : ”””Scheduled Restart SGD . Args : params ( iterable ) : iterable of parameters to optimize or dicts defining parameter groups . l r ( float ) : learning rate . weight decay ( float , optional ) : weight decay ( L2 penalty ) ( default : 0 ) iter count ( integer ) : count the iterations mod 200 Example : > > > optimizer = torch . optim . SRSGD ( model . parameters ( ) , l r = 0 . 1 , weight decay = 5e − 4 , iter count = 1 ) > > > optimizer . zero grad ( ) > > > loss fn ( model ( input ) , target ) . backward ( ) > > > optimizer . step ( ) > > > iter count = optimizer . update iter ( ) Formula : v { t + 1 } = p t − l r ∗ g t p { t + 1 } = v { t + 1 } + ( iter count ) / ( iter count + 3 ) ∗ ( v { t + 1 } − v t ) ”””def i n i t ( self , params , l r = required , weight decay = 0 . , iter count = 1 , r e s t a r t i n g i t e r = 100 ) : i f l r is not required and l r < 0 . 0 : 32 raise ValueError ( ” Invalid learning rate : { } ” . format ( l r ) ) i f weight decay < 0 . 0 : raise ValueError ( ” Invalid weight decay value : { } ” . format ( weight decay ) ) i f iter count < 1 : raise ValueError ( ” Invalid i t e r count : { } ” . format ( iter count ) ) i f r e s t a r t i n g i t e r < 1 : raise ValueError ( ” Invalid i t e r total : { } ” . format ( r e s t a r t i n g i t e r ) ) defaults = dict ( lr = lr , weight decay = weight decay , iter count = iter count , r e s t a r t i n g i t e r = r e s t a r t i n g i t e r ) super ( SRSGD , s e l f ) . i n i t ( params , defaults ) def s e t s t a t e ( self , state ) : super ( SRSGD , s e l f ) . s e t s t a t e ( state ) def update iter ( s e l f ) : idx = 1 for group in s e l f . param groups : i f idx = = 1 : group [ ’ iter count ’ ] + = 1 i f group [ ’ iter count ’ ] > = group [ ’ r e s t a r t i n g i t e r ’ ] : group [ ’ iter count ’ ] = 1 idx + = 1 return group [ ’ iter count ’ ] , group [ ’ r e s t a r t i n g i t e r ’ ] def step ( self , closure = None ) : ”””Perform a single optimization step . Arguments : closure ( callable , optional ) : A closure that reevaluates the model and returns the loss . ”””loss = None i f closure is not None : loss = closure ( ) for group in s e l f . param groups : weight decay = group [ ’ weight decay ’ ] momentum = ( group [ ’ iter count ’ ] − 1 . ) / ( group [ ’ iter count ’ ] + 2 . ) for p in group [ ’params ’ ] : i f p . grad is None : continue d p = p . grad . data i f weight decay ! = 0 : d p . add ( weight decay , p . data ) param state = s e l f . state [ p ] i f ’ momentum buffer ’ not in param state : buf0 = param state [ ’ momentum buffer ’ ] = torch . clone ( p . data ) . detach ( ) else : buf0 = param state [ ’ momentum buffer ’ ] buf1 = p . data − group [ ’ l r ’ ] ∗ d p p . data = buf1 + momentum ∗ ( buf1 − buf0 ) param state [ ’ momentum buffer ’ ] = buf1 iter count , i t e r t o t a l = s e l f . update iter ( ) return loss K SRSGD Implementation in Keras import numpy as np import tensorflow as tf from keras import backend as K from keras . optimizers import Optimizer 33 from keras . legacy import interfaces i f K . backend ( ) = = ’ tensorflow ’ : import tensorflow as tf class SRSGD ( Optimizer ) : ””” Scheduled Restart Stochastic gradient descent optimizer . Includes support for Nesterov momentum and learning rate decay . # Argumentslearning rate : float > = 0 . Learning rate . ””” def i n i t ( self , learning rate = 0 . 01 , iter count = 1 , r e s t a r t i n g i t e r = 40 , ∗∗ kwargs ) : learning rate = kwargs . pop ( ’ l r ’ , learning rate ) s e l f . initial decay = kwargs . pop ( ’ decay ’ , 0 . 0 ) super ( SRSGD , s e l f ) . i n i t ( ∗∗ kwargs ) with K . name scope ( s e l f . c l a s s . name ) : s e l f . iterations = K . variable ( 0 , dtype = ’ int64 ’ , name = ’ iterations ’ ) s e l f . learning rate = K . variable ( learning rate , name = ’ learning rate ’ ) s e l f . decay = K . variable ( s e l f . initial decay , name = ’ decay ’ ) # for srsgd s e l f . iter count = K . variable ( iter count , dtype = ’ int64 ’ , name = ’ iter count ’ ) s e l f . r e s t a r t i n g i t e r = K . variable ( restarting iter , dtype = ’ int64 ’ , name = ’ r e s t a r t i n g i t e r ’ ) s e l f . nesterov = nesterov @ interfaces . legacy get updates support @ K . symbolic def get updates ( self , loss , params ) : grads = s e l f . get gradients ( loss , params ) s e l f . updates = [ K . update add ( s e l f . iterations , 1 ) ] momentum = ( K . cast ( s e l f . iter count , dtype = K . dtype ( s e l f . decay ) ) − 1 . ) / ( K . cast ( s e l f . iter count , dtype = K . dtype ( s e l f . decay ) ) + 2 . ) l r = s e l f . learning rate i f s e l f . initial decay > 0 : l r = l r ∗ ( 1 . / ( 1 . + s e l f . decay ∗ K . cast ( s e l f . iterations , K . dtype ( s e l f . decay ) ) ) ) # momentum shapes = [ K . int shape ( p ) for p in params ] moments = [ K . variable ( value = K . get value ( p ) , dtype = K . dtype ( s e l f . decay ) , name = ’moment ’ + str ( i ) ) for ( i , p ) in enumerate ( params ) ] s e l f . weights = [ s e l f . iterations ] + moments + [ s e l f . iter count ] for p , g , m in zip ( params , grads , moments ) : v = p − l r ∗ g new p = v + momentum ∗ ( v − m ) s e l f . updates . append ( K . update ( m , v ) ) # Apply constraints . i f getattr ( p , ’ constraint ’ , None ) is not None : new p = p . constraint ( new p ) s e l f . updates . append ( K . update ( p , new p ) ) condition = K . a l l ( K . l e s s ( s e l f . iter count , s e l f . r e s t a r t i n g i t e r ) ) new iter count = K . switch ( condition , s e l f . iter count + 1 , s e l f . iter count − s e l f . r e s t a r t i n g i t e r + 1 ) s e l f . updates . append ( K . update ( s e l f . iter count , new iter count ) ) return s e l f . updates def get config ( s e l f ) : config = { ’ learning rate ’ : float ( K . get value ( s e l f . learning rate ) ) , ’ decay ’ : float ( K . get value ( s e l f . decay ) ) , 34 ’ iter count ’ : int ( K . get value ( s e l f . iter count ) ) , ’ r e s t a r t i n g i t e r ’ : int ( K . get value ( s e l f . r e s t a r t i n g i t e r ) ) } base config = super ( SRSGD , s e l f ) . get config ( ) return dict ( l i s t ( base config . items ( ) ) + l i s t ( config . items ( ) ) ) 35