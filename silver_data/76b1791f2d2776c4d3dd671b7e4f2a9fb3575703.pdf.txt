Advances in Very Deep Convolutional Neural Networks for LVCSR Tom Sercu , Vaibhava Goel Multimodal Algorithms and Engines Group , IBM T . J Watson Research Center , USA tsercu @ us . ibm . com , vgoel @ us . ibm . com Abstract Very deep CNNs with small 3 × 3 kernels have recently been shown to achieve very strong performance as acoustic models in hybrid NN - HMM speech recognition systems . In this paper we investigate how to efﬁciently scale these models to larger datasets . Speciﬁcally , we address the design choice of pooling and padding along the time dimension which renders convolu - tional evaluation of sequences highly inefﬁcient . We propose a new CNN design without timepadding and without timepool - ing , which is slightly suboptimal for accuracy , but has two sig - niﬁcant advantages : it enables sequence training and deploy - ment by allowing efﬁcient convolutional evaluation of full ut - terances , and , it allows for batch normalization to be straight - forwardly adopted to CNNs on sequence data . Through batch normalization , we recover the lost peformance from removing the time - pooling , while keeping the beneﬁt of efﬁcient convo - lutional evaluation . We demonstrate the performance of our models both on larger scale data than before , and after sequence training . Our very deep CNN model sequence trained on the 2000h switch - board dataset obtains 9 . 4 word error rate on the Hub5 test - set , matching with a single model the performance of the 2015 IBM system combination , which was the previous best published re - sult . Index Terms : Convolutional Networks , Acoustic Modeling , Speech Recognition , Neural Networks 1 . Introduction We present advances and results on using very deep convolu - tional networks ( CNNs ) as acoustic model for Large Vocabu - lary Continuous Speech Recognition ( LVCSR ) , extending our earlier work [ 1 ] . In [ 1 ] , we introduced very deep convolutional network ar - chitectures to LVCSR , in the hybrid NN - HMM framework : the input to our CNN is a window of ( 1 + 2 ctx ) frames , each frame a 40 - D logmel feature vector , as output we produce a vector of probabilities over CD states for the center frame of the window . We presented strong results on the Babel low - resource ASR task [ 2 ] and the 300 - hour switchboard - 1 dataset ( SWB - 1 ) after cross - entropy training only . The very deep convolutional net - works are inspired by the “VGG Net” architecture introduced in [ 3 ] for the 2014 ImageNet classiﬁcation challenge . The central idea of VGG networks is to replace layers with large convolu - tional kernels by a stack of layers with small 3 × 3 kernels . This way , the same receptive ﬁeld is created with less parameters and more nonlinearity . Furthermore , by applying zero - padding throughout and only reducing spatial resolution through strided pooling , the networks in [ 3 ] were simple and elegant . We fol - lowed this design in the acoustic model CNNs [ 1 ] . However , an important design choice remained unexplored in [ 1 ] : the accuracy and computational efﬁciency impact of time - pooling and time - padding ( i . e . zero - padding at the borders along the time dimension ) . The networks from [ 1 ] pool in time with stride 2 on the higher layers of the network , and applied time - padding throughout . This allowed for the elegant design analogous to the VGG networks for computer vision . However , it leads to inefﬁcient convolution over full sequences , making sequence training infesible on large data sets . In this paper , we explore alternatives to that design choice . Most importantly , we focus on designs that allow for efﬁcient convolutional evalua - tion of full utterances , which is essential during sequence train - ing and test time ( or deployment in a production system ) . The key contributions of this paper are : • We demonstrate that convolutional models without time - pooling or time - padding , while being slightly subopti - mal in recognition accuracy , allow for fast convolutional evaluation over sequences and enable sequence training over large data sets ( sections 2 and 3 ) . • We demonstrate the merit of batch normalization ( BN ) [ 4 ] for CNN acoustic models ( section 4 ) . BN is a tech - nique to accelerate training and improve generalization by normalizing the internal representations inside the network . We show that in order to use batch normaliza - tion for CNNs during sequence training , it is important to train on several utterances at the same time . Since this is feasible for the efﬁcient architectures only , batch normalization gives us essentially a way to compensate the lost performance from following the no time - padding design principle . • We present accuracy results of the very deep networks’ performance after CE and sequence training on the full SWB ( 2000h ) corpus , achieving a 9 . 4 WER on Hub5 ( 1 . 0 WER better than the classical CNN ) . 2 . Exploring pooling and padding in time Pooling with stride is an essential element of CNNs in computer vision , as the downsampling reduces the grid size while build - ing invariance to local geometric transformations . In acoustic CNNs , pooling can be readily applied along the frequency di - mension , which can help build invariance against small spectral variation . In our deepest 10 - layer CNN in [ 1 ] , we reduce the frequency - dimension size from 40 to 20 , 10 , 4 , 2 through pool - ing after the second , fourth , seventh and tenth convolutional layer , while the convolutions are zero - padded in order to pre - serve size in the frequency dimension . Along the time dimension the application of pooling is less straightforward . It was argued in [ 5 ] that downsampling in time should be avoided , rather pooling with stride 1 was used , which does not downsample in time . However , in [ 1 ] we did pool with stride 2 in the two top pooling layers . This ﬁts in the design of our CNNs , where zero - padding is applied both along the time and frequency directions , so pooling is the only operation which reduces the context window from its original size ( e . g . 16 ) to the ﬁnal size ( e . g . 4 ) . This design is directly analogous to VGG a r X i v : 1604 . 01792v2 [ c s . C L ] 25 J un 2016 FC FC Time pooling Time pooling FC ( b ) ( a ) ( c ) Input features Conv layer output Zero padding Pooling output All fully connected layers incl . output layer and softmax Target probabilities Target probabilities Target probabilities 64x40x16 3x40x16 64x40x16 128x20x16 128x20x16 256x10x16 256x10x16 256x8x16 256x4x8 512x4x8 512x4x8 512x4x8 512x2x4 64 x 40 x ( time ) 256 x 10 x ( time ) 128 x 20 x ( time ) 512 x 4 x ( time ) 64 x 40 x T 256 x 10 x T 512 x 4 x T 128 x 20 x T Figure 1 : Different versions of the 10 - layer CNN from [ 1 ] . The time - dimension is shown horizontally , CNN depth is shown vertically , the frequency dimension and number of feature maps are indicated with color shades . Pooling in frequency is implicitly understood at transitions between color shades . ( a ) This is the original ( WDX ) architecture from [ 1 ] , starting from a 16 - frame window . ( b ) Here we do not pool in time , but rather we leave out time - padding on the top layers . This reduces the size along the time - direction from 15 ( ctx = 7 ) to size 3 , using the 6 highest convolutional layers . ( c ) If we want to remove the time - padding and pooling altogether , we need to start with a larger context window ( ctx = 11 ) . This architecture is the only one that allows for efﬁcient convolutional evaluation of full utterances ( section 3 ) and batch normalization ( section 4 ) . SWB - 1 ( 300h ) SWB ( 2000h ) CE ST CE ST Classic 512 CNN [ 6 ] 13 . 2 11 . 8 12 . 6 10 . 4 Classic + AD + Maxout [ 7 ] 12 . 6 11 . 2 11 . 7 * 9 . 9 * DNN + RNN + CNN [ 7 ] - - 11 . 1 9 . 4 ( a ) Pool 11 . 8 10 . 5 10 . 2 9 . 4 ( b ) No pool 11 . 5 10 . 8 10 . 7 9 . 7 ( c ) No pool , no pad 11 . 9 10 . 8 10 . 8 9 . 7 Table 1 : WER on the SWB part of the Hub5’00 testset , for ar - chitectures ( a ) , ( b ) and ( c ) . Column titles show training dataset and method ( cross - entropy or sequence trained ) . For SWB ( 2000h ) cross - entropy training we initialize with networks that are cross - entropy trained on SWB - 1 ( 300h ) . * New results [ 8 ] . AD refers to Annealed Dropout . networks in computer vision . Apart from this practical reason , we did not justify this de - sign choice in [ 1 ] . We hypothesize that downsampling in time has both an advantage and a disadvantage . The advantage is that higher layers in the network are able to access more context and can learn useful invariants in time . As argued in [ 9 ] , once a fea - ture is detected in the lower layers , its exact location does not matter that much anymore and can be blurred out , as long as its approximate relative position is conserved . The disadvantage is that the resolution is reduced with which neighboring but differ - ent CD states can be distinguished , which could possibly hurt performance . In this section we empirically investigate whether pooling in time is justiﬁed . Figure 1 summarizes three variations of the 10 - layer archi - tecture , ( a ) being the original version of [ 1 ] , Figure 1 ( b ) shows an alternative to pooling in time . To reduce the context from its original size to the size we want to absorb in the fully connected layer , we simply omit time - padding on top layers as needed to achieve the desired reduction . In table 1 row ( a ) and ( b ) we compare results with and with - out timepooling . We see that architecture ( a ) with time - pooling outperforms architecture ( b ) after sequence training and train - ing on 2000 hours . The result after 2000 hours and sequence training matches the system combination of a classical CNN , DNN and RNN from [ 7 ] , which was the state of the art sys - tem combination result . Also note that the CE number on SWB ( 2000h ) is far better than the baselines , but the gains from ST are less . This can be explained by the fact that we do stochastic rather than HF sequence training ( see section 5 ) , which leaves an opportunity for improvement . From comparing results for model variants ( a ) with ( b ) , we conclude that better WER accuracy is achieved by models with strided pooling along the time dimension . 3 . Efﬁcient convolution over full utterances During sequence training and at test time , it is desirable to pro - cess an utterance at once in a convolutional way as originally described in the foundational CNN papers [ 9 , 10 ] . For classi - cal CNNs this has never been a problem . However , our best performing very deep CNN ( ﬁgure 1 ( a ) ) introduced padding and pooling along the time dimension , different than prior CNN architectures for LVCSR . This destroys the desirable property of being able to process a full utterance at once , outputting a “dense prediction” , i . e . a CD state probability for each frame in the utterance : • When we pool in time with stride of 2 , the number of frames for which we have an output is reduced by a fac - tor of 2 ; pooling p times ( in p layers ) results in reduc - tion by a factor 2 p . In our ﬁrst network ( a ) this means that for an utterance of length uttlen the number of output frames will be uttlen / 4 . This is obviously ➔ y t ➔ y t + 1 ➔ y t + 2 ➔ … , y t , y t + 1 , y t + 2 , . . . ( more frames ) ( more utterances ) A B Figure 2 : Two ways of evaluating a full utterance : ( A ) by splic - ing as different samples in a minibatch , ( B ) efﬁcient convolu - tional evaluation , treating the full utterance as a single sample . Spliced ( A ) duplicates the amount of input ( and computation ) with a factor context size . Efﬁcient convolutional evalu - ation ( B ) does not duplicate input and computation , but rather takes the full utterance as a single input sample and produces correspondingly large feature maps on the intermediate convo - lutional layers . Variant CE ST Full utterance eval ? ( b ) No Pool 306 207 Spliced ( c ) No Pool , no pad 282 622 Efﬁcient convolution Table 2 : Training speed of the very deep CNNs in frames per second ( higher is better ) . Numbers are average over experi - ments , for our CuDNNv3 - based torch implementation executed on a single NVIDIA Tesla K40 , including all overhead of data loading and host - device transfer . Architecture ( a ) is omitted be - cause the computational cost is very similar to architecture ( b ) . problematic since the HMM and decoder expect CD state probabilities for each frame . • The problem with time padding is more subtle . Consider the ﬁrst convolutional layer : the zero padding in time makes the edges dependent on the location of the win - dow . Shifting one frame to predict the next timestep , the values on the edges are now truncated by zero padding and have to be recomputed . On the second convolutional layer , the zero padding changes the outer two edge val - ues , on the third layer the three outer edge values are modiﬁed , etc . This is illustrated in ﬁgure 1 ( b ) . The light red squares at the edges are zero - padding in time . The red dashed line indicates which output values in the CNN are modiﬁed by the timepadding , as compared to the same network without timepadding . The modiﬁcation travels inwards deeper in the network . Everything outside the dashed lines is modiﬁed by the timepadding which is speciﬁc to the window location . The one obvious way to still obtain an output for each frame for architecture ( a ) with pooling and padding in time is what we call “splicing” the utterance into uttlen different samples in a minibatch , one sample per window for which we want an output . This is depicted in ﬁgure 2 A , with regular full efﬁcient convolution in ﬁgure 2 B . This way the amount of computation is multiplied by a factor ( 1 + 2 ctx ) when sequence training or testing the network . To avoid the need to splice an utterance during sequence training , we propose the CNN design from ﬁgure 1 ( c ) , which does not have any timepadding and pooling , and therefore takes a larger temporal context window . During sequence training , this CNN model can efﬁciently convolve over a full utterance . For ( c ) the increased context on the lower layers gives a slightly increased computational cost during CE training ( table 2 , left column ) . However architecture ( c ) is the only architecture that allows for efﬁcient sequence training and deployment ( table 2 , right column ) . The WER results of network ( c ) is in the bottom row of table 1 . The results of ( c ) are not signiﬁcantly different from ( b ) after 300h - ST and 2000h . As we will discuss in the next section , next to computa - tional efﬁciency , another advantage to architecture ( c ) is the fact that this architecture allows for a modiﬁed version of batch nor - malization at sequence training time . 4 . Batch Normalization Batch normalization ( BN ) [ 4 ] is a technique to accelerate train - ing and improve generalization which gained a lot of traction in the deep learning community . The idea of BN is to standard - ize the internal representations inside the network ( i . e . the layer outputs ) , which helps the network to converge faster and gen - eralize better , inspired by the way whitening the network input improves performance . BN is implemented by standardizing the output of a layer before applying the nonlinearity , using the local mean and variance computed over the minibatch , then cor - recting with a learned variance and bias term ( γ and β resp ) : BN ( x ) = γ x − E [ x ] ( Var [ x ] + (cid:15) ) 1 / 2 + β ( 1 ) The mean and variance ( computed over the minibatch ) are a cheap and simple - to - implement stochastic approximation of the data statistics at this speciﬁc layer , for the current network weights . Since at test time we want to be able to do inference without presence of a minibatch , the prescribed method is to accumulate a running average of the mean and variance during training to be used at test time . For CNNs , the mean and vari - ance is computed over samples and spatial location . The standard formulation of BN for CNNs can be readily applied to cross - entropy training during which the minibatch contains samples from different utterances , different targets and different speakers . However , during sequence training , spliced evaluation as in ﬁgure 2 A is problematic . If we construct a minibatch from consecutive windows of the same utterance , the minibatch mean and variance will be bad for two reasons : • The consecutive samples are identical except the shift and border frame . Thus the different samples in the mini - batch will be highly correlated . • The GPU memory will limit the number of samples in the batch ( to around 512 samples on our system ) . There - fore the mean and variance can typically only be com - puted over few utterances or even just a chunk of an ut - terance . Both these reasons cause the mean and variance estimate to be a poor approximation of the true data statistics , and to ﬂuctuate strongly between minibatches . We can drastically improve the mean and variance if we have an architecture that allows for efﬁcient convolutional pro - cessing of a full utterance like ﬁgure 2 B . In this case both of the issues are solved : ﬁrstly there is no duplication from splicing . Secondly , several utterances can be processed in one minibatch , since now they ﬁt in GPU memory . Version ( Fig 1 ) SWB - 1 ( 300h ) SWB ( 2000h ) CE ST CE ST ( a ) 11 . 8 10 . 5 10 . 2 9 . 4 ( b ) 11 . 5 10 . 8 10 . 7 9 . 7 ( b ) + BN 11 . 7 11 . 3 10 . 5 10 . 4 ( c ) 11 . 9 10 . 8 10 . 8 9 . 7 ( c ) + BN 11 . 8 10 . 5 10 . 8 9 . 5 Table 3 : WER on the SWB part of the Hub5’00 testset . We aim to maximize the number of frames being processed in a minibatch , in order for the mean and variance to become a better estimate . We achieve this by matching the number of utterances in a minibatch with the utterance length , such that ( number of utterances ) × ( max utterance length ) = ( con - stant number of frames ) . The algorithm for batch assembly can be expressed in pseudo - code as : • choose numFrames to maximize GPU usage • while ( training ) : – targUttLen ← sample from p ( uttLen ) ∼ f ( uttLen ) × uttLen – numUtts ← ﬂoor ( numFrames / uttLen ) – minibatch ← sample ( numUtts ) utterances with length close to targUttLen With our implementation of the 10 - layer network of ﬁgure 1 ( c ) , and a 12 GB Nvidia Tesla K40 GPU , we found numFrames = 6000 to be optimal , taking up about 11GB of memory on the device . Table 3 shows the results of the architectural variants ( b ) and ( c ) with and without BN . As expected , for architecture ( b ) with batch normalization we do not obtain good performance from sequence training , since we have to resort to spliced ( in - efﬁcient ) evaluation . The performance is worse with than with - out BN . In contrast , using the efﬁcient convolutional evaluation with architecture ( c ) , using batch normalization improves per - formance from 10 . 8 to 10 . 5 on SWB - 1 ( 300h ) , matching the performance of the superior architecture ( a ) . On SWB ( 2000h ) adding BN brings the WER down to 9 . 5 , almost matching the result of ( a ) . We conclude that model ( c ) with BN recovers the lost performance from model ( a ) . 5 . Training details We follow the same hybrid setup as [ 7 ] , with 32k CD states ( decision tree leaves ) , 40D logmel features with window size described in ﬁgure 1 . All our work was done using the torch environment [ 11 ] . Both CE and sequence training are SGD - based with batch size 128 ( except ( c ) ST as described in section 4 ) , and L 2 weight penalty of 1 e − 6 . During CE training , we found SGD ( learning rate 0 . 03 ) to work best for networks without BN , and nesterov acceler - ated gradient ( NAG ) with learning rate 0 . 003 and momentum 0 . 99 for networks with BN . We use a ﬁxed learning rate decay scheme which divides the learning rate by 3 after 150M , 250M , and 350M frames . To deal with class imbalance , we adopt the balanced sampling from [ 1 ] , by sampling from context depen - dent state CD i with probability p i = f γi (cid:80) j f γj . We keep γ = 0 . 8 throughout the experiments . We found two elements essential to make sequence training work well in the stochastic setting : • NAG with momentum 0 . 99 , which we dropped to 0 . 95 after 100M frames ( inspired by [ 12 ] ) . • Regularization of ST by adding the gradient of cross - entropy loss , as proposed in [ 13 ] . 6 . Related Work CNNs [ 10 ] have become a dominant approach for solving large - scale machine learning problems on natural data , for example in computer vision [ 14 , 15 , 16 ] , speech recognition [ 17 , 5 ] , and more recently also on character - level text classiﬁcation [ 18 ] and language modeling [ 19 ] . Very deep nets with small 3 × 3 ﬁlters ( VGG net ) excel not only on ImageNet classiﬁcation , but have shown to transfer well to different tasks like neural image cap - tioning [ 20 ] , object detection [ 21 ] , semantic segmentation [ 22 ] , etc . Pooling in time has been applied as early as the work on Time Delay NNs ( TDNNs ) [ 23 ] . Prior work on CNNs for LVCSR has explored non - strided pooling in time [ 24 ] , while in contrast our strided pooling does subsample in time and ob - tains superior performance . CNNs with strided pooling in time have been succesfully used for small - footprint keyword spot - ting [ 25 ] , learning ﬁlterbanks from raw signal [ 26 ] , and the end - to - end CTC - based model in [ 27 ] . Sequence training was introduced to neural network train - ing in [ 28 ] . We performed stochastic sequence training as op - posed to Hessian - Free sequence training [ 29 ] which was used in our baselines [ 6 , 7 ] . As mentioned in section 5 , we smoothed the ST loss with CE loss as in [ 13 ] , and used Nesterov Acceler - ated Gradient ( nag ) as optimization method [ 12 ] . Batch normalization ( BN ) was introduced in [ 4 ] , and is closely related to prior work aimed at whitening the activations inside the network [ 30 ] . BN was shown to improve the Ima - genet classiﬁcation performance in the GoogLeNet architecture [ 31 ] and residual networks [ 32 ] , which were the top two sub - missions in 2015 to the ImageNet classiﬁcation competition . When applying batch normalization to sequence data , stacking multiple utterances as one batch for computing the mean and variance stastistics , is identical to how BN was applied to recur - rent neural networks in [ 33 , 27 ] . 7 . Discussion In this paper we demonstrated the strength of very deep con - volutional networks applied to speech recognition in the hybrid NN - HMM framework . We obtain a WER of 9 . 4 after sequence training on the 2000 hour switchboard dataset , which as a sin - gle model matches the performance of the state of the art model combination DNN + RNN + CNN from [ 7 ] . This model , when combined with a state of the art RNN acoustic model and better language models , obtains signiﬁcantly better performance on Hub5 than any other published model , see [ 8 ] . We compared three model variants , and discussed the im - portance of time - padding and time - pooling . • Architecture ( a ) with pooling performs better then ( b ) and ( c ) without pooling . • Only architecture ( c ) without padding or pooling allows for batch normalization and efﬁcient convolutional pro - cessing of full utterances . This naturally raises the question whether we can combine the best of both : pool in time like architecture ( a ) and efﬁcient convolutional evaluation like architecture ( c ) . This is possible with a CNN architecture that does pool but does not pad in time , and compensates for the lost resolution by either applying ∆ offsets as proposed in [ 15 ] , or by using non - strided pooling with sparse kernels along the time dimension , an elegant technique independently proposed in both [ 34 ] and [ 35 ] . We leave this for future work . 8 . References [ 1 ] T . Sercu , C . Puhrsch , B . Kingsbury , and Y . LeCun , “Very deep multilingual convolutional neural networks for lvcsr , ” Proc . ICASSP , 2016 . [ 2 ] “Iarpa babel , ” http : / / www . iarpa . gov / index . php / research - programs / babel . [ 3 ] K . Simonyan and A . Zisserman , “Very deep convolutional net - works for large - scale image recognition , ” CoRR arXiv : 1409 . 1556 , 2014 . [ 4 ] S . Ioffe and C . Szegedy , “Batch normalization : Accelerating deep network training by reducing internal covariate shift , ” Proc . ICML , 2015 . [ 5 ] T . N . Sainath , A . - r . Mohamed , B . Kingsbury , and B . Ramabhad - ran , “Deep convolutional neural networks for lvcsr , ” in Acous - tics , Speech and Signal Processing ( ICASSP ) , 2013 IEEE Inter - national Conference on . IEEE , 2013 , pp . 8614 – 8618 . [ 6 ] H . Soltau , G . Saon , and T . N . Sainath , “Joint training of convolu - tional and non - convolutional neural networks , ” to Proc . ICASSP , 2014 . [ 7 ] G . Saon , H . - K . J . Kuo , S . Rennie , and M . Picheny , “The ibm 2015 english conversational telephone speech recognition sys - tem , ” Proc . Interspeech , 2015 . [ 8 ] G . Saon , T . Sercu , S . Rennie , and H . - K . J . Kuo , “The ibm 2016 english conversational telephone speech recognition system , ” - , 2016 . [ 9 ] Y . LeCun and Y . Bengio , “Convolutional networks for images , speech , and time series , ” The handbook of brain theory and neural networks , vol . 3361 , no . 10 , p . 1995 , 1995 . [ 10 ] Y . LeCun , L . Bottou , Y . Bengio , and P . Haffner , “Gradient - based learning applied to document recognition , ” Proceedings of the IEEE , vol . 86 , no . 11 , pp . 2278 – 2324 , 1998 . [ 11 ] R . Collobert , K . Kavukcuoglu , and C . Farabet , “Torch7 : A matlab - like environment for machine learning , ” in BigLearn , NIPS Workshop , no . EPFL - CONF - 192376 , 2011 . [ 12 ] I . Sutskever , J . Martens , G . Dahl , and G . Hinton , “On the impor - tance of initialization and momentum in deep learning , ” in Proc . ICML , 2013 , pp . 1139 – 1147 . [ 13 ] H . Su , G . Li , D . Yu , and F . Seide , “Error back propagation for sequence training of context - dependent deep networks for con - versational speech transcription , ” in Acoustics , Speech and Signal Processing ( ICASSP ) , 2013 IEEE International Conference on . IEEE , 2013 , pp . 6664 – 6668 . [ 14 ] A . Krizhevsky , I . Sutskever , and G . E . Hinton , “Imagenet classi - ﬁcation with deep convolutional neural networks , ” in Advances in neural information processing systems , 2012 , pp . 1097 – 1105 . [ 15 ] P . Sermanet , D . Eigen , X . Zhang , M . Mathieu , R . Fergus , and Y . LeCun , “Overfeat : Integrated recognition , localization and detection using convolutional networks , ” arXiv preprint arXiv : 1312 . 6229 , 2013 . [ 16 ] C . Farabet , C . Couprie , L . Najman , and Y . LeCun , “Learning hi - erarchical features for scene labeling , ” Pattern Analysis and Ma - chine Intelligence , IEEE Transactions on , vol . 35 , no . 8 , pp . 1915 – 1929 , 2013 . [ 17 ] O . Abdel - Hamid , A . - r . Mohamed , H . Jiang , and G . Penn , “Ap - plying convolutional neural networks concepts to hybrid nn - hmm model for speech recognition , ” in Acoustics , Speech and Signal Processing ( ICASSP ) , 2012 IEEE International Conference on . IEEE , 2012 , pp . 4277 – 4280 . [ 18 ] X . Zhang , J . Zhao , and Y . LeCun , “Character - level con - volutional networks for text classiﬁcation , ” arXiv preprint arXiv : 1509 . 01626 , 2015 . [ 19 ] Y . Kim , Y . Jernite , D . Sontag , and A . M . Rush , “Character - aware neural language models , ” arXiv preprint arXiv : 1508 . 06615 , 2015 . [ 20 ] K . Xu , J . Ba , R . Kiros , A . Courville , R . Salakhutdinov , R . Zemel , and Y . Bengio , “Show , attend and tell : Neural image caption gen - eration with visual attention , ” arXiv preprint arXiv : 1502 . 03044 , 2015 . [ 21 ] R . Girshick , “Fast r - cnn , ” in Proceedings of the IEEE Interna - tional Conference on Computer Vision , 2015 , pp . 1440 – 1448 . [ 22 ] J . Long , E . Shelhamer , and T . Darrell , “Fully convolu - tional networks for semantic segmentation , ” arXiv preprint arXiv : 1411 . 4038 , 2014 . [ 23 ] A . Waibel , T . Hanazawa , G . Hinton , K . Shikano , and K . J . Lang , “Phoneme recognition using time - delay neural networks , ” Acoustics , Speech and Signal Processing , IEEE Transactions on , vol . 37 , no . 3 , pp . 328 – 339 , 1989 . [ 24 ] T . N . Sainath , B . Kingsbury , G . Saon , H . Soltau , A . - r . Mohamed , G . Dahl , and B . Ramabhadran , “Deep convolutional neural net - works for large - scale speech tasks , ” Neural Networks , 2014 . [ 25 ] T . Sainath and C . Parada , “Convolutional neural networks for small - footprint keyword spotting , ” in Proc . Interspeech , 2015 . [ 26 ] D . Palaz , R . Collobert , and M . M . Doss , “Estimating phoneme class conditional probabilities from raw speech signal using con - volutional neural networks , ” arXiv preprint arXiv : 1304 . 1018 , 2013 . [ 27 ] D . Amodei , R . Anubhai , E . Battenberg , C . Case , J . Casper , B . Catanzaro , J . Chen , M . Chrzanowski , A . Coates , G . Diamos et al . , “Deep speech 2 : End - to - end speech recognition in english and mandarin , ” CoRR arXiv : 1512 . 02595 , 2015 . [ 28 ] B . Kingsbury , “Lattice - based optimization of sequence classiﬁ - cation criteria for neural - network acoustic modeling , ” in Proc . ICASSP . IEEE , 2009 , pp . 3761 – 3764 . [ 29 ] B . Kingsbury , T . N . Sainath , and H . Soltau , “Scalable minimum bayes risk training of deep neural network acoustic models using distributed hessian - free optimization , ” in Thirteenth Annual Con - ference of the International Speech Communication Association , 2012 . [ 30 ] S . Wiesler , A . Richard , R . Schluter , and H . Ney , “Mean - normalized stochastic gradient for large - scale deep learning , ” in proc . ICASSP . IEEE , 2014 , pp . 180 – 184 . [ 31 ] C . Szegedy , V . Vanhoucke , S . Ioffe , J . Shlens , and Z . Wojna , “Re - thinking the inception architecture for computer vision , ” CoRR arXiv : 1512 . 00567 , 2015 . [ 32 ] K . He , X . Zhang , S . Ren , and J . Sun , “Deep residual learning for image recognition , ” CoRR arXiv : 1512 . 03385 , 2015 . [ 33 ] C . Laurent , G . Pereyra , P . Brakel , Y . Zhang , and Y . Bengio , “Batch normalized recurrent neural networks , ” Proc . ICASSP , 2016 . [ 34 ] H . Li , R . Zhao , and X . Wang , “Highly efﬁcient forward and back - ward propagation of convolutional neural networks for pixelwise classiﬁcation , ” arXiv preprint arXiv : 1412 . 4526 , 2014 . [ 35 ] F . Yu and V . Koltun , “Multi - scale context aggregation by dilated convolutions , ” arXiv preprint arXiv : 1511 . 07122 , 2015 .