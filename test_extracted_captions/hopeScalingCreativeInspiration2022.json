{"CAPTION FIG4.png": "\"Figure 4: Comparing our GCN model predictions (right) to human annotations (left). Interestingly, our model managed to correct some annotator errors (\u201cit's\u201d, \u201cheated\u201d, \u201ccoffee warm\u201d, \u201cbeverages\u201d). Purposes in pink, mechanisms in green.\\n\\n\"", "CAPTION FIG3.png": "'Figure 3: Left: Precision\\\\(0\\\\)K results for the best performing model (GCN + self-training). Right: Raw extraction accuracy evaluation. All approaches use CRF loss. GCN with syntactic edges outperforms baselines. Self-training further improves results. Random-label achieves only \\\\(16.01\\\\)\\\\(\\\\mathbf{F}_{1}\\\\).\\n\\n'", "CAPTION FIG1.png": "'Figure 1: Extracting fine-grained purposes and mechanisms at scale enables mapping the landscape of ideas. Suppose an inventor is looking for a way to wash clothes without water. On the left we see a snippet from the graph of purposes. Each node in the graph represents a cluster of similar purpose spans extracted from documents (labels are manually generated for illustration purposes). Edges reflect abstract structural similarity, capturing co-occurrence patterns of spans in the corpus (see Section 5.1 and Figure 9). On the right we see example documents containing purposes from the four clusters (corresponding cluster numbers appear in boxes). Purpose/mechanism spans in documents are shown in pink/green, respectively. One could find direct matches to the query \u2013 i.e., documents with purpose from cluster 1 and mechanism not \u201cwater\u201d (e.g., waterless washing machine using dry ice), or explore neighboring purpose clusters, reformulating the problem as removing odor, removing dirt, or getting rid of the dirty clothes, each resulting in a different set of inspirations.\\n\\n'", "CAPTION FIG2.png": "'Figure 2: Crowdsourcing interface for fine-grained purposes and mechanisms.\\n\\n'", "CAPTION FIG10.png": "'Figure 10: **Inspiration user study results. Left: Proportion of inspirations selected by at least \\\\(2\\\\) raters, per condition. Right: Proportion of boxes (clusters) with at least \\\\(2\\\\) spans marked by \\\\(\\\\geq 2\\\\) raters.**\\n\\n'", "CAPTION FIG11.png": "'\\n\\n**Figure 11:** Schematic of our GCN model'", "CAPTION FIG5.png": "'Figure 5: Applications for light where light is not in the purpose. **Left: Interface. Right: Two of the results and their automatic annotations (purposes in pink, mechanisms in green).**\\n\\n'", "CAPTION FIG6.png": "'Figure 6: Results for search evaluation test case. Mean average precision (MAP) and Normalized Discounted Cumulative Gain (NDCG) by method, averaged across queries. Methods in bold use our model.\\n\\n'", "CAPTION TAB2.png": "'\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{|c|c|} \\\\hline \\\\hline \\\\multicolumn'", "CAPTION FIG9.png": "'Figure 9: Excerpt from our Functional Concept Graph. Nodes represent concepts (clusters of purposes). To give intuition for how edges were created, they are annotated with example products containing spans from both concepts. All nodes and edges in this figure were automatically constructed and used to create the user-fixing inspirations shown in Figure 8. This figure provides a graphic illustration (not shown to users) explaining how the boxes in Figure 8 are generated, with node names provided here by us for readability. The problem of \"medicine morning reminder\" is mapped (via embedding) to the _Alert/remind_ concept cluster (as named by us), which is linked to the concepts of medical monitoring and making hot drinks through products such as \"smart medicine injector\" and \"coffee machine alarm\" (among others, not displayed in the figure). These links serve as the source for inspirations in our study, as seen in Figure 8.\\n\\n'", "CAPTION TAB1.png": "'\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l l} \\\\hline \\\\hline  & \\\\\\\\ \\\\end{tabular}\\n\\\\end{table}\\nTable 1: Scenarios and example results retrieved by our FineGrained-AVG method. Queries reflect non-trivial uses of mechanisms (e.g., using water not for drinking/cleaning, retrieving a lighter running on hydrogen from water and sunlight).\\n\\n'", "CAPTION FIG7.png": "'Figure 7: An example of our learned functional concept graph extracted from texts. Mechanism in green, purpose in pink. Titles are tags nearest to cluster centroids (redicted to fit).\\n\\n'", "CAPTION FIG8.png": "'Figure 8: A snippet from our ideation interface for \\\\({}^{4}\\\\)morning medicine reminder\u201d. Users see inspirations grouped into boxes. Each box is supposed to represent a concept \u2013 a cluster of related spans as found by our method or by the baselines (see \u00a75.1). Users indicate which inspirations were useful, and what ideas they inspired. For example, seeing \"real time health checker\" inspired one user to suggest a monitoring device for finding the best time for reminding to take the medicine.\\n\\n'"}